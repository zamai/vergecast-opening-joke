1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 13cd01d0-d281-11ed-b4d4-e1af67e95bfb
Status: Done
Stage: Done
Title: The swaggy AI Pope and Apple headset rumors
Audio URL: https://jfe93e.s3.amazonaws.com/4837357641121187084/8584808035190189892/s93290-US-5280s-1680568730.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and David Pierce discuss AI image generators getting better, Apple WWDC 2023 rumors, and the other big stories in tech this week.

2
00:00:00,910 --> 00:00:04,280
Speaker 1:  This episode is brought to you by kpmg. At

3
00:00:04,470 --> 00:00:08,280
Speaker 1:  kpmg Innovation is the go-to state of mind. Their visionary

4
00:00:08,280 --> 00:00:12,160
Speaker 1:  thinkers and advanced technology help you see beyond the now, uncover

5
00:00:12,160 --> 00:00:16,000
Speaker 1:  new insights and turn them into opportunities. KPMG

6
00:00:16,000 --> 00:00:19,920
Speaker 1:  can help you leverage the value of data and drive transformational outcomes

7
00:00:20,200 --> 00:00:23,600
Speaker 1:  through innovation To explore their thinking, go to

8
00:00:24,030 --> 00:00:25,560
Speaker 1:  kpmg.us.

9
00:00:26,580 --> 00:00:30,480
Speaker 2:  The Supreme Court is hearing a case this week about art and

10
00:00:30,480 --> 00:00:34,440
Speaker 2:  who gets to remix an artistic idea. But this fight

11
00:00:34,770 --> 00:00:35,520
Speaker 2:  is not new.

12
00:00:35,750 --> 00:00:39,160
Speaker 3:  Olivia Rodrigo was not doing anything different from

13
00:00:39,180 --> 00:00:43,080
Speaker 3:  Michaelangelo or Leonardo da Vinci. Honestly, those guys were

14
00:00:43,080 --> 00:00:46,920
Speaker 3:  looking around seeing what other people did at the time in Florence

15
00:00:46,920 --> 00:00:49,880
Speaker 3:  and Rome and wherever, and copying some of it.

16
00:00:50,390 --> 00:00:54,160
Speaker 2:  SCOTUS copyright law. And you this week

17
00:00:54,160 --> 00:00:56,800
Speaker 2:  on Intuit Vultures Pop Culture podcast.

18
00:01:10,140 --> 00:01:13,940
Speaker 5:  Hello and welcome to Rich Chest, the flagship podcast with the swag

19
00:01:13,940 --> 00:01:17,900
Speaker 5:  out Pope. God, I love the swag out Pope so much. If

20
00:01:17,900 --> 00:01:21,740
Speaker 5:  human, if AI ends humanity, but we got swag Pope,

21
00:01:22,140 --> 00:01:25,860
Speaker 5:  it'll been fine. Like, honestly, like it's worth it.

22
00:01:26,080 --> 00:01:28,780
Speaker 5:  Put a sign on earth that's like, here lies swag, Pope

23
00:01:29,790 --> 00:01:33,220
Speaker 5:  passing aliens. Like, just send that message out. Like, you know, there's

24
00:01:33,220 --> 00:01:37,020
Speaker 5:  other civilizations in the universe where like, why have we never heard from

25
00:01:37,020 --> 00:01:39,940
Speaker 5:  anyone? And there's that theory that every civilization gets to at this point

26
00:01:39,940 --> 00:01:43,140
Speaker 5:  and collapses. We just need to send the message

27
00:01:43,640 --> 00:01:47,180
Speaker 5:  now. Swag pop. If you see swag, Pope, you

28
00:01:47,580 --> 00:01:49,020
Speaker 5:  probably have 25 years left.

29
00:01:50,170 --> 00:01:51,700
Speaker 6:  That was us. We did that.

30
00:01:52,150 --> 00:01:53,980
Speaker 5:  We, we know something.

31
00:01:55,330 --> 00:01:56,700
Speaker 7:  That's what we leave behind

32
00:01:57,250 --> 00:02:01,060
Speaker 5:  Drippy Pope, man. It's like, just shut it down once you get there. Hi,

33
00:02:01,060 --> 00:02:03,620
Speaker 5:  I'm Eli. I'm your friend Alex Grant is here.

34
00:02:03,800 --> 00:02:07,180
Speaker 7:  I'm your friend too. You don't know if I'm real unless you check how many

35
00:02:07,180 --> 00:02:08,300
Speaker 7:  fingers I have in the photos.

36
00:02:09,460 --> 00:02:10,300
Speaker 5:  David Pierce

37
00:02:10,300 --> 00:02:14,100
Speaker 6:  Is here. Hi. I'm suddenly realizing I have a puffy jacket upstairs and that

38
00:02:14,100 --> 00:02:16,340
Speaker 6:  like, maybe I should have been wearing it for this podcast. Oh,

39
00:02:16,600 --> 00:02:19,100
Speaker 5:  You should have. You absolutely shouldn't. Now that we've moved into video,

40
00:02:19,120 --> 00:02:22,660
Speaker 5:  I'm so sorry. We all, we gotta take, we gotta start taking style cues from

41
00:02:22,900 --> 00:02:26,740
Speaker 5:  the Pope. Yeah. Just in general. We should, I mean, honestly, why have I

42
00:02:26,740 --> 00:02:30,220
Speaker 5:  not been dress dressing like the head of a church this whole time? Let's

43
00:02:30,220 --> 00:02:34,020
Speaker 5:  talk about the Pope. An amazing sentence to say on this. Our

44
00:02:34,020 --> 00:02:37,740
Speaker 5:  technology podcast this week, mid Journey, which is a generative

45
00:02:37,910 --> 00:02:41,860
Speaker 5:  AI tool that makes images, at least an update, the images can

46
00:02:41,860 --> 00:02:45,820
Speaker 5:  now be vastly more photorealistic. And the first thing

47
00:02:45,820 --> 00:02:49,620
Speaker 5:  that happened was people made images of the Pope

48
00:02:49,620 --> 00:02:53,420
Speaker 5:  just absolutely swag out in a gigantic white puffy coat.

49
00:02:53,420 --> 00:02:56,780
Speaker 6:  Well, actually, the, the best part of this is, that's the second thing that

50
00:02:56,900 --> 00:03:00,440
Speaker 6:  happened. Okay? The first thing that happened was somebody tweeted a

51
00:03:00,440 --> 00:03:04,240
Speaker 6:  generated picture of Donald Trump being arrested that no one fell

52
00:03:04,240 --> 00:03:06,960
Speaker 6:  for. And actually it sparked this really interesting thing about like, oh,

53
00:03:06,960 --> 00:03:09,960
Speaker 6:  look at this fake picture. Someday people are gonna fall for these. But it

54
00:03:09,960 --> 00:03:13,880
Speaker 6:  didn't become a thing. And then literally a couple of days later, the

55
00:03:13,880 --> 00:03:17,760
Speaker 6:  Pope swag out in a giant puffy coat gets everybody

56
00:03:18,430 --> 00:03:21,920
Speaker 5:  That got. Yeah. So I, and, and this is like the heart of this story. This

57
00:03:21,920 --> 00:03:25,440
Speaker 5:  is like the most verge casty thing. Why did the information environment

58
00:03:25,630 --> 00:03:29,320
Speaker 5:  know not to fall for Trump being arrested, even though he's

59
00:03:29,320 --> 00:03:33,200
Speaker 5:  tweeting I'm about to be arrested. Right? And then everyone I think had the

60
00:03:33,200 --> 00:03:36,840
Speaker 5:  same reaction, which is they were scrolling on whatever feeds somewhere and

61
00:03:36,840 --> 00:03:40,760
Speaker 5:  like, you know, that swag out Pope showed up and everyone

62
00:03:40,760 --> 00:03:44,720
Speaker 5:  was like, oh, that rules and like moved on. Right? And like, there are

63
00:03:44,720 --> 00:03:48,600
Speaker 5:  no stakes to falling for it, right? And so no one applied

64
00:03:48,600 --> 00:03:52,520
Speaker 5:  their like, critical thinking skills until later when various

65
00:03:52,520 --> 00:03:56,400
Speaker 5:  writers and people were like, Hey, the, the fake pope is important of

66
00:03:56,400 --> 00:04:00,360
Speaker 5:  doom. That we all had to like, take a step back. You should go look at these

67
00:04:00,360 --> 00:04:03,240
Speaker 5:  images if you haven't seen, I would, I, I can't imagine you're listening

68
00:04:03,240 --> 00:04:07,080
Speaker 5:  to the show. We, James Vincent wrote a great piece. The headline is The

69
00:04:07,080 --> 00:04:11,040
Speaker 5:  Swag Out Pope is an AI fake and an early glimpse of the new reality. A

70
00:04:11,040 --> 00:04:14,160
Speaker 5:  perfect Verge headline. I love it. You should go read that piece. You should

71
00:04:14,160 --> 00:04:17,960
Speaker 5:  go look the images, they are remarkably realistic. There's some pieces of

72
00:04:17,960 --> 00:04:21,840
Speaker 5:  them that make it easy to spot, like this new version

73
00:04:21,840 --> 00:04:25,800
Speaker 5:  of Mid Journey. It's kind of like everything's like a little too hdr. Everything

74
00:04:25,800 --> 00:04:29,120
Speaker 5:  kind of looks like the clarity slider. And Lightroom has been turned up too

75
00:04:29,120 --> 00:04:32,880
Speaker 5:  far. So they have a style, which is interesting in and of

76
00:04:32,880 --> 00:04:36,520
Speaker 5:  itself. Like you tell Mid Journey make a thing that looks like a real photo

77
00:04:36,700 --> 00:04:40,640
Speaker 5:  and it is like, okay, we've mined Zillow for the

78
00:04:40,640 --> 00:04:44,560
Speaker 5:  most whacked down HDR real estate photos that can exist. And that's the

79
00:04:44,560 --> 00:04:45,400
Speaker 5:  style we're using for,

80
00:04:45,580 --> 00:04:48,240
Speaker 6:  One of the things I thought was really interesting is there have been people

81
00:04:48,240 --> 00:04:51,000
Speaker 6:  like with some of these Trump photos that are starting to come out where

82
00:04:51,000 --> 00:04:54,520
Speaker 6:  you can say things like, make it look like it came out of a TV camera. And

83
00:04:54,540 --> 00:04:58,040
Speaker 6:  the people making these images are getting better and better at teaching

84
00:04:58,040 --> 00:05:02,000
Speaker 6:  the system how to replicate the weirdness of photos that we

85
00:05:02,000 --> 00:05:05,320
Speaker 6:  see in the world. Because in a weird way, like Instagram has trained us to

86
00:05:05,320 --> 00:05:08,640
Speaker 6:  not notice that a photo isn't real anymore. Like first we had filters. Yeah.

87
00:05:08,640 --> 00:05:12,200
Speaker 6:  And now we have all these incredibly nuanced tools that people can use to

88
00:05:12,200 --> 00:05:15,720
Speaker 6:  change things. And it's generally true that if you look really, really, really

89
00:05:15,720 --> 00:05:19,080
Speaker 6:  hard, you can figure it out, right? Like there are, there are telltale signs

90
00:05:19,080 --> 00:05:23,000
Speaker 6:  in like the, the dynamic range and the contrast and often the

91
00:05:23,000 --> 00:05:26,520
Speaker 6:  hands. But like the argument that you should zoom this photo to

92
00:05:26,520 --> 00:05:30,480
Speaker 6:  200% and inspect every pixel in order to see if this is real is

93
00:05:30,480 --> 00:05:33,640
Speaker 6:  ridiculous. And we're way past that point. Like the idea that you can just

94
00:05:33,640 --> 00:05:36,560
Speaker 6:  sort of quickly look at it and be like, that's not what faces are, which

95
00:05:36,560 --> 00:05:40,400
Speaker 6:  is what these things were six months ago. Yeah. We're so far down the

96
00:05:40,400 --> 00:05:44,240
Speaker 6:  road of like, this looks like a regular photo that a normal human might have

97
00:05:44,240 --> 00:05:47,640
Speaker 6:  edited to have this kind of style and then posted on the internet. Like we've

98
00:05:47,640 --> 00:05:48,640
Speaker 6:  crossed some Rubicon there.

99
00:05:48,640 --> 00:05:52,320
Speaker 5:  The pop one is really interesting cause his glasses are melting onto his

100
00:05:52,320 --> 00:05:56,000
Speaker 5:  face. Like it's it's right in front of you. Yeah. Like, you don't have to

101
00:05:56,000 --> 00:05:58,960
Speaker 5:  ne you should go, go look at his hands. Everyone should go look at this picture.

102
00:05:58,960 --> 00:06:02,360
Speaker 5:  There's a bunch of stuff in it that would tell you. But the first one

103
00:06:02,810 --> 00:06:05,560
Speaker 5:  is that his glasses aren't melting into his face. Well,

104
00:06:05,560 --> 00:06:05,840
Speaker 7:  You're too

105
00:06:06,080 --> 00:06:09,080
Speaker 5:  Distracted and you just sort of overlook it Unscr, you're like, I don't know,

106
00:06:09,080 --> 00:06:11,040
Speaker 5:  look at that jacket and you're just like, miss it. Yeah.

107
00:06:11,040 --> 00:06:14,840
Speaker 7:  The jacket is really, really good. Like, I want that jacket. I wish the

108
00:06:14,840 --> 00:06:18,040
Speaker 7:  jacket existed. The one that makes it look like it's got like the little

109
00:06:18,040 --> 00:06:22,000
Speaker 7:  Pope Cumber bun built in. I was like, yes. Beautiful. No

110
00:06:22,000 --> 00:06:25,480
Speaker 7:  notes. I don't even care that his glasses are melting into his face. That's

111
00:06:25,480 --> 00:06:27,080
Speaker 7:  just an aesthetic choice for him, I assume.

112
00:06:27,790 --> 00:06:31,200
Speaker 6:  Yeah. And, and as James points out in his story, there's a couple of other

113
00:06:31,200 --> 00:06:34,880
Speaker 6:  things too, right? Like the, the cross he's wearing on a necklace is, is

114
00:06:34,880 --> 00:06:38,840
Speaker 6:  sort of warped and melted. Like Salvador Ali style, his hand

115
00:06:38,840 --> 00:06:42,440
Speaker 6:  looks crazy with the, the thing that he's like, if you spend

116
00:06:42,810 --> 00:06:46,280
Speaker 6:  15 seconds with this photo, you can pretty quickly figure out, it's not real,

117
00:06:46,300 --> 00:06:49,560
Speaker 6:  but like, how many photos do you spend 15 seconds staring at going? Is this

118
00:06:49,560 --> 00:06:52,120
Speaker 6:  a thing? Like, are we supposed to do that with every picture we see from

119
00:06:52,120 --> 00:06:53,200
Speaker 6:  now on for the rest of our lives?

120
00:06:53,650 --> 00:06:56,360
Speaker 5:  So bring that back around to Donald Trump. Right? I didn't even think of

121
00:06:56,360 --> 00:06:59,480
Speaker 5:  it as the first thing that happened. Like of course the first thing that

122
00:06:59,680 --> 00:07:03,040
Speaker 5:  somebody made with this tool was a photo of Trump getting arrested. Sure.

123
00:07:03,140 --> 00:07:07,000
Speaker 5:  And it, but it didn't hit like it wasn't resonant in a really unique

124
00:07:07,000 --> 00:07:09,600
Speaker 5:  way. Yeah. In the, the swag Pope was

125
00:07:10,620 --> 00:07:11,560
Speaker 5:  Why do you think that is?

126
00:07:11,680 --> 00:07:15,360
Speaker 6:  James makes a couple of arguments that I think are really compelling. His

127
00:07:15,360 --> 00:07:17,720
Speaker 6:  piece is really good. Everybody should read it. But the, the first thing

128
00:07:17,720 --> 00:07:21,600
Speaker 6:  I think is there is this like hyper-realism I think

129
00:07:21,600 --> 00:07:25,480
Speaker 6:  is the phrase he he used that others have used before around celebrities

130
00:07:25,480 --> 00:07:29,120
Speaker 6:  we're just like used to seeing these people in these odd places with odd

131
00:07:29,120 --> 00:07:32,720
Speaker 6:  lighting doing weird stuff. The Pope in particular is the kind of person

132
00:07:32,720 --> 00:07:36,560
Speaker 6:  who is like massively photographed. He is unusually, I think

133
00:07:36,560 --> 00:07:40,040
Speaker 6:  for a pope around like swag all the time.

134
00:07:40,360 --> 00:07:40,720
Speaker 6:  Like

135
00:07:41,170 --> 00:07:41,840
Speaker 5:  There's

136
00:07:41,840 --> 00:07:45,240
Speaker 6:  Just a picture of the man like signing a Lamborghini. And so it's like, it's

137
00:07:45,240 --> 00:07:49,040
Speaker 6:  just in this realm of possibility enough. And then you add

138
00:07:49,040 --> 00:07:52,360
Speaker 6:  on top of that this kind of weird veneer that is all

139
00:07:52,360 --> 00:07:56,160
Speaker 6:  celebrity stuff now. And then attach again, like you said, sort of

140
00:07:56,160 --> 00:07:59,880
Speaker 6:  the low stakes of it, right? Like Trump getting arrested is like objectively

141
00:08:00,070 --> 00:08:03,320
Speaker 6:  it's a big deal if it happens. And so it's gonna cause most

142
00:08:03,320 --> 00:08:06,840
Speaker 6:  discerning people to at least take a half a beat and be like, is this

143
00:08:07,600 --> 00:08:10,760
Speaker 6:  anything? But like, why, why would they lie to you about like this sick jacket

144
00:08:10,760 --> 00:08:13,680
Speaker 6:  the Pope is wearing? Right? Like, we're just that conditioned to ask that

145
00:08:13,920 --> 00:08:16,880
Speaker 6:  question. And so I think you, you put all these things together and also

146
00:08:16,920 --> 00:08:20,800
Speaker 6:  something like Mid Journey is gonna do really well with a photo

147
00:08:21,050 --> 00:08:24,840
Speaker 6:  of the Pope or Donald Trump as opposed to like a picture of your neighbor

148
00:08:24,840 --> 00:08:28,760
Speaker 6:  because it has the training data to do this. So it's just, it is able to

149
00:08:28,760 --> 00:08:32,680
Speaker 6:  do these things in such a more convincing way because of kind

150
00:08:32,680 --> 00:08:36,440
Speaker 6:  of both what we're primed to see and what it actually has in its system to

151
00:08:36,440 --> 00:08:37,120
Speaker 6:  work with. I

152
00:08:37,120 --> 00:08:40,880
Speaker 5:  Have a related theory, which is that Trump getting arrested is a sort of

153
00:08:40,880 --> 00:08:42,680
Speaker 5:  thing that is immediately reinforced.

154
00:08:43,280 --> 00:08:45,440
Speaker 6:  Hmm. Yeah. Like you'd be seeing at other places,

155
00:08:45,830 --> 00:08:49,680
Speaker 5:  Like if that photo existed, the person who runs push alerts, the

156
00:08:49,680 --> 00:08:52,200
Speaker 5:  New York Times would be losing their mind,

157
00:08:53,240 --> 00:08:56,520
Speaker 5:  Right? They're like, just beat the phone again. Just hit him again. Just

158
00:08:56,520 --> 00:08:59,880
Speaker 5:  tell him again, live updates. There's a, there's a story stream thing on

159
00:08:59,880 --> 00:09:03,600
Speaker 5:  the top. Hit him again. Trump sneezed in handcuffs, hit him again, right?

160
00:09:03,600 --> 00:09:07,360
Speaker 5:  Like, and the Post would be doing it and we would have it, like for some

161
00:09:07,360 --> 00:09:11,280
Speaker 5:  reason we at Tech site would be like, we need to have a, like the information

162
00:09:11,280 --> 00:09:15,240
Speaker 5:  environment would instantly reinforce that image as being

163
00:09:15,240 --> 00:09:19,000
Speaker 5:  real. And I think we're trained for that to happen, right?

164
00:09:19,000 --> 00:09:22,800
Speaker 5:  Like we know if something of that scale happens, it will happen

165
00:09:22,800 --> 00:09:26,640
Speaker 5:  a lot like at massive scale. And so you see that

166
00:09:26,640 --> 00:09:29,960
Speaker 5:  picture and you're like, eh, like there's not a, there's none of the other

167
00:09:29,960 --> 00:09:33,440
Speaker 5:  signaling, right? And so then you're like, wait, is this real, the pope

168
00:09:33,700 --> 00:09:36,920
Speaker 5:  the Pope wearing a puffer jacket with a built-in conman And Alex, I just

169
00:09:36,920 --> 00:09:39,600
Speaker 5:  looked at that picture again. I had noticed that con button. It's like real

170
00:09:39,600 --> 00:09:40,400
Speaker 6:  Good. It's, it's so good.

171
00:09:40,510 --> 00:09:43,800
Speaker 5:  It's real good. The Pope wearing a like what is

172
00:09:43,800 --> 00:09:45,440
Speaker 5:  objectively a sick jacket?

173
00:09:46,790 --> 00:09:47,280
Speaker 7:  It's,

174
00:09:47,370 --> 00:09:50,440
Speaker 5:  There's like, actually the New York Times might send a push alert about it,

175
00:09:50,440 --> 00:09:53,240
Speaker 5:  like at this point they just can't help themselves.

176
00:09:54,100 --> 00:09:56,880
Speaker 5:  But it, it doesn't, it just doesn't come with a signaling.

177
00:09:56,880 --> 00:10:00,560
Speaker 7:  I think you're right. That's, that's exactly what it is. Like we're se we're,

178
00:10:00,560 --> 00:10:03,960
Speaker 7:  we're used to kinda looking for that misinformation. We've been primed by

179
00:10:03,960 --> 00:10:07,840
Speaker 7:  like the shark is in the, is in the highway flood, the

180
00:10:07,840 --> 00:10:11,760
Speaker 7:  flooded highway. Like we see that video every single time

181
00:10:11,760 --> 00:10:14,640
Speaker 7:  and somebody has to be like, it's fake. So you're used to kind being like,

182
00:10:14,640 --> 00:10:18,000
Speaker 7:  this is the coolest thing ever. Why is no one talking about it? Oh cuz it's

183
00:10:18,000 --> 00:10:21,600
Speaker 7:  fake. Whereas Trump, you're gonna go and you're gonna

184
00:10:21,600 --> 00:10:25,440
Speaker 7:  immediately look for it because if he's being arrested, you're gonna wanna

185
00:10:25,440 --> 00:10:29,120
Speaker 7:  like message your parents and be like, ha got him or whatever. Maybe you're

186
00:10:29,120 --> 00:10:32,160
Speaker 7:  gonna arrest him. Like be happy about it. It was all really bad.

187
00:10:32,160 --> 00:10:36,120
Speaker 5:  Here's my question. I posted a, a video of the swag

188
00:10:36,120 --> 00:10:40,000
Speaker 5:  Pope, it's just a TikTok of different, I mean it's, it's so I can't get enough

189
00:10:40,000 --> 00:10:43,400
Speaker 5:  of it's is the disinformation problem way lower stakes?

190
00:10:43,640 --> 00:10:46,960
Speaker 5:  Right? It's not everyone's worried about the election that's coming up. No,

191
00:10:46,960 --> 00:10:47,440
Speaker 5:  I think it's,

192
00:10:47,870 --> 00:10:51,680
Speaker 7:  I think it's still high stakes because right now we're, we're,

193
00:10:51,680 --> 00:10:55,600
Speaker 7:  we're seeing, we see the same thing with deep fakes. We saw it with Photoshop.

194
00:10:55,600 --> 00:10:59,120
Speaker 7:  Every time we get these new tools that allow us to kind of recreate

195
00:10:59,120 --> 00:11:02,760
Speaker 7:  reality, the initial things are like fun. It's like

196
00:11:02,760 --> 00:11:06,680
Speaker 7:  deep fake Keanu Reeves on TikTok just eating a sandwich. Like that's funny.

197
00:11:06,680 --> 00:11:09,960
Speaker 7:  Yeah. But then somebody's like, okay, well now I'm gonna use a really cool

198
00:11:09,960 --> 00:11:13,800
Speaker 7:  deep fake to make it look like Biden said something really,

199
00:11:13,800 --> 00:11:17,640
Speaker 7:  really horrible to, to get my bass really upset because my

200
00:11:17,640 --> 00:11:21,520
Speaker 7:  base isn't as online as the rest of us are. So they're not as primed

201
00:11:21,690 --> 00:11:25,320
Speaker 7:  to look for that disinformation and react to it appropriately.

202
00:11:25,490 --> 00:11:29,160
Speaker 7:  So it's a lot easier to manipulate those like other groups who aren't as

203
00:11:29,160 --> 00:11:31,320
Speaker 7:  primed as the majority of us looking at the Pope.

204
00:11:31,320 --> 00:11:35,000
Speaker 6:  Yeah. Yeah. And I think there's also the question of where the

205
00:11:35,000 --> 00:11:38,880
Speaker 6:  line is kind of for us as consumers

206
00:11:38,880 --> 00:11:41,720
Speaker 6:  is really fascinating to me. Cuz I think about the, like do you remember

207
00:11:41,720 --> 00:11:45,640
Speaker 6:  that story from a couple of years ago when there were lions on the street

208
00:11:45,640 --> 00:11:49,160
Speaker 6:  in Russia during the pandemic that one had a full,

209
00:11:49,160 --> 00:11:52,880
Speaker 6:  everybody believed it cycle before it became something that was like

210
00:11:53,060 --> 00:11:56,900
Speaker 6:  roundly debunked because it wasn't true. And so that's still kind of

211
00:11:56,900 --> 00:12:00,780
Speaker 6:  under the, like we immediately learned to question it. Whereas

212
00:12:00,840 --> 00:12:04,020
Speaker 6:  the former president of the United States is about to get arrested. Obviously

213
00:12:04,020 --> 00:12:07,780
Speaker 6:  above the line of, I immediately need to scrutinize whether this is real

214
00:12:07,780 --> 00:12:11,260
Speaker 6:  or not, right? Yeah. Yeah. But the, the best disinformation we've seen

215
00:12:11,800 --> 00:12:15,500
Speaker 6:  has always been the stuff that exists just barely underneath that line.

216
00:12:15,500 --> 00:12:17,780
Speaker 6:  Right? And I think especially as we get to the election, I don't know where

217
00:12:17,780 --> 00:12:21,500
Speaker 6:  the line is, but this question of like, what is higher stakes than, yo, look

218
00:12:21,500 --> 00:12:25,260
Speaker 6:  at the jacket this Pope has on but lower stakes than the former president

219
00:12:25,260 --> 00:12:28,820
Speaker 6:  of the United States got arrested. And all of these very smart actors from

220
00:12:28,820 --> 00:12:32,740
Speaker 6:  around the world are gonna find ways to live just right a smidge

221
00:12:32,740 --> 00:12:36,460
Speaker 6:  underneath that line. And I think it just gets easier and easier to do

222
00:12:36,730 --> 00:12:38,460
Speaker 6:  with some of these tools. Now

223
00:12:38,840 --> 00:12:42,420
Speaker 5:  Let me rephrase that question. I think both of you're right in that way.

224
00:12:42,890 --> 00:12:46,700
Speaker 5:  I meant like is it lower stakes if you did this to a

225
00:12:46,700 --> 00:12:49,340
Speaker 5:  school board member, right? If you

226
00:12:49,340 --> 00:12:50,620
Speaker 6:  Swag down a school board member, is that

227
00:12:50,680 --> 00:12:54,140
Speaker 5:  Say Yeah. Yeah. If I was like, all right, let's, we're gonna swag David out,

228
00:12:54,780 --> 00:12:58,620
Speaker 5:  right? And like put him and like put you out there like we're, you're not

229
00:12:58,620 --> 00:13:01,620
Speaker 5:  gonna get the signal. It's a thing you're talking about. It's like just below

230
00:13:01,620 --> 00:13:05,460
Speaker 5:  the line of instant scrutiny, right? Is is it that where the

231
00:13:05,460 --> 00:13:08,300
Speaker 5:  problem is headed? Because that's what it feels like to me like this

232
00:13:08,570 --> 00:13:09,340
Speaker 7:  Yeah, I

233
00:13:09,340 --> 00:13:13,100
Speaker 5:  Trippy post it's is hilarious. But like I'm thinking about like high school

234
00:13:13,100 --> 00:13:16,380
Speaker 5:  students. Yes. And maybe it's just there's not enough training data

235
00:13:16,750 --> 00:13:20,540
Speaker 5:  to make weird celebrity photos of your principal, but you I you can just

236
00:13:20,540 --> 00:13:22,060
Speaker 5:  see where it's going, right?

237
00:13:22,450 --> 00:13:26,020
Speaker 7:  Yeah. Yeah. There was, I was trying to look up some old

238
00:13:26,020 --> 00:13:29,620
Speaker 7:  celebrity and like an old photo of a celebrity I had in my head that I remember

239
00:13:29,620 --> 00:13:33,020
Speaker 7:  being real. And I was like, I wish I could remember who it was now I was

240
00:13:33,020 --> 00:13:35,500
Speaker 7:  like Judy Garland or something. I was like, I'm gonna go find this photo.

241
00:13:35,500 --> 00:13:39,180
Speaker 7:  I've seen I've, I can see it in my head. I know it's real. And I went searching

242
00:13:39,180 --> 00:13:42,260
Speaker 7:  for it and it was actually a Photoshop and I couldn't find an original photo.

243
00:13:42,320 --> 00:13:46,100
Speaker 7:  And I was like, oh, this like idea I've had in my head all of this time

244
00:13:46,100 --> 00:13:49,300
Speaker 7:  was just some dumb misinformation and I just felt bumped for a little while.

245
00:13:49,320 --> 00:13:52,860
Speaker 7:  And I think that's probably gonna be the most common kind of thing. Like

246
00:13:52,860 --> 00:13:56,780
Speaker 7:  maybe it's old celebrities or new celebrities, just a little twist

247
00:13:56,780 --> 00:13:59,620
Speaker 7:  just so you're like, you love that photo of 'em so much. They're the swag

248
00:13:59,620 --> 00:14:03,260
Speaker 7:  out pope. And, but it'll also be like, okay, well I can go and make

249
00:14:03,470 --> 00:14:07,060
Speaker 7:  my best friend who I'm no longer friends with cuz I'm 16 and and

250
00:14:07,060 --> 00:14:10,980
Speaker 7:  everything's in the world is ending look really ugly in this dress even though

251
00:14:10,980 --> 00:14:13,540
Speaker 7:  she never owned that dress. And then everybody's gonna make fun of her.

252
00:14:13,540 --> 00:14:15,420
Speaker 5:  That's very specific. Alex,

253
00:14:15,420 --> 00:14:19,260
Speaker 7:  Look I was a teenage girl. We know how to bully better than anyone else

254
00:14:19,260 --> 00:14:23,220
Speaker 7:  on the planet. But I think like we're gonna see a lot more of that

255
00:14:23,510 --> 00:14:26,660
Speaker 7:  as those tools become easier to use for regular people.

256
00:14:26,950 --> 00:14:30,580
Speaker 5:  So in the middle of this, right, right. The mid journey is not

257
00:14:30,580 --> 00:14:33,380
Speaker 5:  photorealistic. It is remarkable. You should go look at the images, you should

258
00:14:33,380 --> 00:14:37,340
Speaker 5:  go play with it. They are starting to close it down. I think they're

259
00:14:37,340 --> 00:14:40,780
Speaker 5:  realizing like they need to put some rules around it. So that's happening.

260
00:14:40,780 --> 00:14:43,620
Speaker 5:  There's a big push for that. Then there's the voices,

261
00:14:44,300 --> 00:14:48,220
Speaker 5:  which are pretty accurate. Again, my favorite tos of all are

262
00:14:48,220 --> 00:14:52,000
Speaker 5:  presidents talking about mixing music and arguing about it. It's like the

263
00:14:52,000 --> 00:14:54,120
Speaker 5:  funniest shit in the entire world. It's, I'm gonna end up posting one every

264
00:14:54,120 --> 00:14:57,760
Speaker 5:  week for the rest of my life cuz they're so funny. But the voices are good.

265
00:14:57,760 --> 00:15:01,680
Speaker 5:  There's one I'll, I'll just post it. But the voices are good.

266
00:15:01,680 --> 00:15:05,560
Speaker 5:  What they can't do yet is make a move. Right? So if you got

267
00:15:05,560 --> 00:15:08,440
Speaker 5:  really great voices and you can generate an image of the president, you are

268
00:15:08,440 --> 00:15:12,320
Speaker 5:  pretty close to a video of a president saying something bad. And we

269
00:15:12,320 --> 00:15:15,520
Speaker 5:  are just starting to see, okay, we're making videos out of this. Right? Like

270
00:15:15,630 --> 00:15:19,560
Speaker 5:  that part of the deep fake cycle where you just type in President Biden

271
00:15:19,560 --> 00:15:23,360
Speaker 5:  saying he hates everyone generates an accurate looking video

272
00:15:23,450 --> 00:15:26,520
Speaker 5:  of President Biden saying he he hates everyone without the sort of like current

273
00:15:26,590 --> 00:15:30,240
Speaker 5:  deep fake remodeling that you have to do. We're not there yet. And we've

274
00:15:30,240 --> 00:15:34,160
Speaker 5:  seen some examples now of AI generated video, which I think James referred

275
00:15:34,160 --> 00:15:35,680
Speaker 5:  to as being in its demonic phase.

276
00:15:35,680 --> 00:15:36,440
Speaker 7:  He's accurate.

277
00:15:36,440 --> 00:15:40,240
Speaker 6:  Yeah. It's, it's, it's either wonderful or horrifying

278
00:15:40,400 --> 00:15:43,880
Speaker 6:  and I genuinely can't, like I'm, I, every time I watch it, I like watch it

279
00:15:43,880 --> 00:15:47,400
Speaker 6:  again and also don't wanna ever see it again as long as I live. But there's

280
00:15:47,400 --> 00:15:51,360
Speaker 6:  like, yeah you can, you can type what is it? Will Smith Eating Pasta

281
00:15:51,360 --> 00:15:54,880
Speaker 6:  and it will show you a video of Will Smith eating pasta. But it's, it's horrifying,

282
00:15:54,880 --> 00:15:58,000
Speaker 6:  right? Like it's, yeah, it just makes me think of salad Fingers every time

283
00:15:58,000 --> 00:16:01,320
Speaker 6:  for reasons I can't explain. But that's a reference most of our listeners

284
00:16:01,320 --> 00:16:04,880
Speaker 6:  won't get anyway, so it's fine. But it's really funny cuz this is

285
00:16:05,220 --> 00:16:08,040
Speaker 6:  the kind of thing you look at and it's like, oh this is just silly, right?

286
00:16:08,040 --> 00:16:12,000
Speaker 6:  Like this is, these are like goofy videos of Joe Rogan

287
00:16:12,400 --> 00:16:14,800
Speaker 6:  fighting a bear that is like essentially a cartoon. Yes.

288
00:16:15,020 --> 00:16:18,960
Speaker 5:  And then he turns into a bear. Yeah. Like it's not, it's

289
00:16:18,960 --> 00:16:22,720
Speaker 5:  not like, like they're cra like Will Smith eating go watch the video

290
00:16:22,720 --> 00:16:23,720
Speaker 5:  Will Smith eating pasta. It's

291
00:16:23,720 --> 00:16:26,760
Speaker 6:  Yeah, we'll post it in the shoutout. It's like the, they're all great, but

292
00:16:26,760 --> 00:16:29,640
Speaker 6:  you look at them and it's like, okay, this is, this is ultimately harmless.

293
00:16:29,640 --> 00:16:33,440
Speaker 6:  Like nobody's gonna look at this and be like, oh, Joe Rogan actually fought

294
00:16:33,440 --> 00:16:37,200
Speaker 6:  and then became a bear. But also like last summer we were talking

295
00:16:37,200 --> 00:16:41,040
Speaker 6:  about Dolly and Mid Journey in this same way where everything was

296
00:16:41,040 --> 00:16:44,480
Speaker 6:  very abstract and it was like, oh, you can see how

297
00:16:44,810 --> 00:16:48,640
Speaker 6:  it thinks this looks like a person, but it doesn't really, and no one would

298
00:16:48,640 --> 00:16:52,280
Speaker 6:  be confused by it. And now here we are. Like that took, that took

299
00:16:52,380 --> 00:16:56,240
Speaker 6:  six months essentially to go from, no one would ever confuse this for a

300
00:16:56,240 --> 00:17:00,120
Speaker 6:  person to like, you have to work pretty hard to not confuse this for

301
00:17:00,120 --> 00:17:04,080
Speaker 6:  a person. Yeah. And I think a video's gonna come the same way. And that's

302
00:17:04,080 --> 00:17:08,000
Speaker 6:  where I think like, I don't know, we either enter this place of total nihilism

303
00:17:08,000 --> 00:17:10,800
Speaker 6:  where it's like nothing matters. Don't believe anything. The only real thing

304
00:17:10,800 --> 00:17:14,720
Speaker 6:  is like the stuff right in front of your face and maybe not even that.

305
00:17:14,810 --> 00:17:18,400
Speaker 6:  Or we're gonna have to figure out some really complicated systems for how

306
00:17:18,400 --> 00:17:19,800
Speaker 6:  we deal with all of this stuff. Well

307
00:17:19,800 --> 00:17:23,520
Speaker 7:  I think it's, it's education, right? Like video obviously is more

308
00:17:23,520 --> 00:17:27,000
Speaker 7:  complex than photos, but we've been dealing with this issue as long as kind

309
00:17:27,000 --> 00:17:30,000
Speaker 7:  of photos have been existing, people have been faking photos. And we've even

310
00:17:30,000 --> 00:17:33,840
Speaker 7:  seen it with video too. People were making fake videos all the time and

311
00:17:33,900 --> 00:17:37,440
Speaker 7:  the only thing that's changed is that the tools are way, way more accessible

312
00:17:37,440 --> 00:17:41,320
Speaker 7:  now. So there's gonna be a lot more content rather than just

313
00:17:41,320 --> 00:17:45,160
Speaker 7:  a few really good people doing it. And I think the way you counter

314
00:17:45,160 --> 00:17:49,000
Speaker 7:  that is to just educate people, make people the rest. Most

315
00:17:49,150 --> 00:17:52,910
Speaker 7:  of us listening, most of us here talking right now can pretty much

316
00:17:52,940 --> 00:17:56,030
Speaker 7:  pick this stuff out. Like we take a moment, we look at it, we know how to

317
00:17:56,030 --> 00:17:58,590
Speaker 7:  do it because we've educated ourselves, we've learned how to,

318
00:17:58,770 --> 00:18:02,430
Speaker 5:  I'm sorry, the premise of this conversation is that all of us got taken by

319
00:18:02,430 --> 00:18:03,270
Speaker 5:  a dripped out pope.

320
00:18:03,380 --> 00:18:07,030
Speaker 7:  Yeah because, but then you stop and you pay take a moment

321
00:18:07,090 --> 00:18:10,870
Speaker 7:  and like once you apply rigor like we did with, with the fake

322
00:18:10,870 --> 00:18:14,310
Speaker 7:  Trump photos, we immediately were like, no, those are fake. And so I think

323
00:18:14,310 --> 00:18:18,230
Speaker 7:  like once people start applying that rigor more frequently with

324
00:18:18,230 --> 00:18:21,550
Speaker 7:  this stuff, we're gonna like, it won't be quite as bad.

325
00:18:22,170 --> 00:18:24,870
Speaker 7:  And I think that's certainly one of the better ways to deal with it than

326
00:18:24,870 --> 00:18:28,350
Speaker 7:  like ban the tools altogether or some of the other weird

327
00:18:28,590 --> 00:18:32,310
Speaker 6:  Plans. But I think that's, that's asking a lot of people, right? And we've,

328
00:18:32,310 --> 00:18:35,350
Speaker 6:  we've been through this a few times where like we went through this with

329
00:18:35,410 --> 00:18:38,270
Speaker 6:  misinformation on the internet in general, right? And we have a bunch of

330
00:18:38,270 --> 00:18:42,150
Speaker 6:  tools that are making it easier to be smarter. And yet everybody gets

331
00:18:42,150 --> 00:18:45,910
Speaker 6:  taken by most stuff because most of us are primed to think, most people

332
00:18:45,910 --> 00:18:49,710
Speaker 6:  are not. Like it's deliberately lying to us, right? And so yeah, the

333
00:18:49,710 --> 00:18:53,550
Speaker 6:  getting past that thing has been very hard. And then you talk about like

334
00:18:53,550 --> 00:18:57,030
Speaker 6:  the chat G P T stuff that people believe, even though it is, these things

335
00:18:57,030 --> 00:18:59,430
Speaker 6:  are known to lie, they say it confidently and like they know what they're

336
00:18:59,430 --> 00:19:01,510
Speaker 6:  talking about. And I was talking to somebody the other day who was saying,

337
00:19:02,270 --> 00:19:06,230
Speaker 6:  he was like, I've done more Googling than ever since chat G P T because I

338
00:19:06,230 --> 00:19:09,750
Speaker 6:  have to Google every fact that this thing gives me. And like maybe that's

339
00:19:09,750 --> 00:19:12,750
Speaker 6:  sneakily what Google's doing. They're like, if we make a shitty chatbot,

340
00:19:12,860 --> 00:19:16,830
Speaker 6:  searches will go way up. But that's, it's a lot

341
00:19:16,830 --> 00:19:20,390
Speaker 6:  of work and I think the idea that I'm gonna like scroll through my TikTok

342
00:19:20,390 --> 00:19:24,350
Speaker 6:  feed or my Instagram feed or whatever and sort of inquire as to the

343
00:19:24,350 --> 00:19:27,790
Speaker 6:  veracity of every single thing in it is just too much to ask of people. And

344
00:19:27,790 --> 00:19:30,190
Speaker 6:  so you get people like Adobe is saying they're gonna come out with these

345
00:19:30,190 --> 00:19:34,030
Speaker 6:  tools for identifying this stuff and one of open AI's businesses is

346
00:19:34,030 --> 00:19:37,990
Speaker 6:  gonna be showing you what was made by open ai, which is like bonkers

347
00:19:37,990 --> 00:19:41,710
Speaker 6:  and backwards and insane. That's good. But it does feel like that

348
00:19:41,710 --> 00:19:45,590
Speaker 6:  stuff is gonna come eventually, but slowly. And we're we're

349
00:19:45,590 --> 00:19:49,390
Speaker 6:  due for a really weird phase of being tricked by drippy popes. Wait,

350
00:19:49,510 --> 00:19:50,470
Speaker 6:  between now and then

351
00:19:50,660 --> 00:19:54,230
Speaker 7:  I figured it out. Snoops can go public and we all

352
00:19:54,230 --> 00:19:57,150
Speaker 7:  invest in Snoops and they just spend forever

353
00:19:57,630 --> 00:19:59,590
Speaker 7:  debunking every single fake

354
00:19:59,660 --> 00:20:03,510
Speaker 6:  Just call it Snoops G P T. Yeah. And you just send it everything

355
00:20:03,510 --> 00:20:06,470
Speaker 6:  and it says, is this real? Yeah, this is great idea. Come into this play.

356
00:20:06,470 --> 00:20:06,750
Speaker 6:  Yeah.

357
00:20:06,840 --> 00:20:09,550
Speaker 5:  So the Adobe thing is really interesting, right? They call it the content

358
00:20:09,910 --> 00:20:13,830
Speaker 5:  authenticity initiative. We've covered it. I talked to

359
00:20:13,830 --> 00:20:17,470
Speaker 5:  Scott Beski from Adobe about it before. Their whole thing is okay,

360
00:20:17,880 --> 00:20:21,550
Speaker 5:  we can verify an image from creation to

361
00:20:21,550 --> 00:20:25,350
Speaker 5:  ingest to distribution. So you take a photo, your camera like sports, this

362
00:20:25,350 --> 00:20:29,190
Speaker 5:  thing, you put it through Photoshop, it'll get tagged and we'll

363
00:20:29,190 --> 00:20:32,870
Speaker 5:  basically, it's basically like a very complicated metadata system. And

364
00:20:32,870 --> 00:20:36,670
Speaker 5:  various social networks will support it so that when Twitter

365
00:20:36,670 --> 00:20:40,550
Speaker 5:  or Facebook or Instagram or whatever displays an image, it

366
00:20:40,550 --> 00:20:43,670
Speaker 5:  will tell you it's real because it's been verified as authentic all the way

367
00:20:43,670 --> 00:20:47,440
Speaker 5:  through the capture chain. I, I want to believe this will work. I do do I,

368
00:20:47,440 --> 00:20:51,080
Speaker 5:  it's like it's a very verge solution to the problem. Like it's a nice idea

369
00:20:51,080 --> 00:20:52,360
Speaker 5:  what if we did good metadata?

370
00:20:54,220 --> 00:20:57,480
Speaker 5:  But like those are the kinds of solutions that the industry can come up with.

371
00:20:57,480 --> 00:21:00,680
Speaker 5:  And they've even, Adobe's been plugging away at that for a minute now. Then

372
00:21:00,680 --> 00:21:04,480
Speaker 5:  there's the sort of policy solutions, a group called The Future of Life

373
00:21:04,480 --> 00:21:08,360
Speaker 5:  Institute published an open letter calling on AI labs around the world

374
00:21:08,360 --> 00:21:12,000
Speaker 5:  to pause development. Elon Musk signed it. Alex, I think you were saying

375
00:21:12,000 --> 00:21:15,760
Speaker 5:  Steve Wazniak signed it. Yeah, Andrew Yang showed up and

376
00:21:15,760 --> 00:21:19,680
Speaker 5:  signed it. You know, Elon signing it. There's a lot

377
00:21:19,680 --> 00:21:23,040
Speaker 5:  of history with Elon and open AI that's coming out now. It's maybe seems

378
00:21:23,040 --> 00:21:26,760
Speaker 5:  like he just wants him to stop, but we're at that phase,

379
00:21:26,760 --> 00:21:30,520
Speaker 5:  right? Yeah. Where there's an open letter there. There's another

380
00:21:30,520 --> 00:21:34,400
Speaker 5:  group has called on the ftc, the Center for AI and Digital

381
00:21:34,400 --> 00:21:38,360
Speaker 5:  Policy filed a complaint with the FTC saying Open

382
00:21:38,360 --> 00:21:42,120
Speaker 5:  AI should stop launching new models of G P T because of

383
00:21:42,120 --> 00:21:45,680
Speaker 5:  unfair and deceptive trade practices. Which is seems like a stretch.

384
00:21:45,750 --> 00:21:46,240
Speaker 5:  Yeah.

385
00:21:46,240 --> 00:21:50,160
Speaker 6:  Their, their big theory was like, because these things

386
00:21:50,160 --> 00:21:54,040
Speaker 6:  can put out false information that falls under the auspices of

387
00:21:54,040 --> 00:21:57,520
Speaker 6:  the FTC to regulate. It's, it's drawing a pretty tenuous

388
00:21:57,710 --> 00:22:01,400
Speaker 6:  line. Yeah. Like you're saying there is a lot of momentum for this idea that

389
00:22:01,530 --> 00:22:04,520
Speaker 6:  we just have to stop it because it is getting out of control and we don't

390
00:22:04,520 --> 00:22:06,280
Speaker 6:  know what to do with it. Make it stop. Yeah.

391
00:22:06,680 --> 00:22:10,560
Speaker 5:  The UK rejected the idea of a dedicated AI regulator. So and

392
00:22:10,560 --> 00:22:14,200
Speaker 5:  then there's some sort of antitrust fight. Here's a question, a very vir

393
00:22:14,600 --> 00:22:17,960
Speaker 5:  question. Do robots have free speech? Because cuz we do,

394
00:22:18,310 --> 00:22:19,560
Speaker 5:  I think for

395
00:22:19,560 --> 00:22:21,080
Speaker 7:  A while I, people who program the robots

396
00:22:21,080 --> 00:22:24,760
Speaker 5:  Have until President DeSantis rides herd over these United States

397
00:22:26,390 --> 00:22:30,240
Speaker 5:  like we do. Right? It's like legal to lie in most cases, right?

398
00:22:30,240 --> 00:22:32,880
Speaker 5:  It's not legal to like do a fraud and like get money for lying. But like

399
00:22:33,010 --> 00:22:36,120
Speaker 5:  in general, if I wanted to tell you that Aaron Rodgers is gonna have six

400
00:22:36,120 --> 00:22:38,760
Speaker 5:  great seasons with the New York chats, like that's a thing I can say no one

401
00:22:38,760 --> 00:22:39,320
Speaker 5:  can arrest before

402
00:22:42,140 --> 00:22:43,040
Speaker 5:  I'm just lying to you.

403
00:22:43,040 --> 00:22:45,120
Speaker 6:  Aaron Rodgers is outside of your door right now.

404
00:22:46,190 --> 00:22:50,120
Speaker 5:  He's showing up, right? He's the, he's the law. Like

405
00:22:50,120 --> 00:22:53,800
Speaker 5:  it's legal to lie to people. Is it legal for robots to lie to people? Like

406
00:22:53,800 --> 00:22:56,600
Speaker 5:  I don't know the answer to that question in any way, shape or form. Well,

407
00:22:56,600 --> 00:23:00,440
Speaker 5:  no. So I'm not sure that the government is presently constituted as the ability

408
00:23:00,440 --> 00:23:03,560
Speaker 5:  to answer that question in a rational, coherent, or stable way.

409
00:23:03,610 --> 00:23:07,080
Speaker 7:  So the Writer's Guild of America is currently renegotiating their contract

410
00:23:07,080 --> 00:23:10,440
Speaker 7:  with the producers in Hollywood so that they can get a better deal

411
00:23:10,440 --> 00:23:14,200
Speaker 7:  primarily on streaming rights. That's like a whole big deal.

412
00:23:14,360 --> 00:23:17,840
Speaker 7:  We'll do a plenty of verge cast about it, don't worry about it. But

413
00:23:17,840 --> 00:23:21,480
Speaker 7:  they're also talking about AI tools because there's a potential for somebody

414
00:23:21,480 --> 00:23:24,280
Speaker 7:  to just be like, I don't need writers, I'm just gonna have the ai. Right?

415
00:23:24,280 --> 00:23:28,120
Speaker 7:  The whole thing. And they said no. The same way that you don't just

416
00:23:28,120 --> 00:23:31,800
Speaker 7:  like use a Wikipedia entry for a

417
00:23:31,800 --> 00:23:35,680
Speaker 7:  story. We still are the ones writing itis are just

418
00:23:35,680 --> 00:23:39,520
Speaker 7:  tools and you're allowed to use those tools to maybe enhance, maybe

419
00:23:39,520 --> 00:23:43,360
Speaker 7:  make suggestions, but they cannot be credited. They cannot be

420
00:23:43,470 --> 00:23:47,280
Speaker 7:  like treated the same, same as a writer because they're not because they're

421
00:23:47,280 --> 00:23:51,160
Speaker 7:  tools. And they also called it kind of inherently plagiarism

422
00:23:51,370 --> 00:23:55,360
Speaker 7:  because of the data it's trained on, which I thought was really interesting,

423
00:23:55,480 --> 00:23:58,840
Speaker 7:  maybe wrong, but certainly like very, very interesting.

424
00:23:59,100 --> 00:24:02,760
Speaker 7:  And I think that approach of like, these are tools, they're

425
00:24:02,920 --> 00:24:06,600
Speaker 7:  inherently cannot lie. They cannot be credited because

426
00:24:06,770 --> 00:24:10,520
Speaker 7:  they're tools. You don't credit Photoshop when you go and you do a really

427
00:24:10,520 --> 00:24:13,720
Speaker 7:  cool edit. Like we don't credit Photoshop whenever we do illustrations on

428
00:24:13,720 --> 00:24:17,680
Speaker 7:  the website, you wouldn't credit minefield in the same way. So I

429
00:24:17,680 --> 00:24:18,720
Speaker 7:  thought that was kind of interesting.

430
00:24:18,970 --> 00:24:22,680
Speaker 5:  So I should disclose the writer's skill of America is the

431
00:24:22,720 --> 00:24:26,680
Speaker 5:  union that the Vox Media Union is part of. So our newsroom is

432
00:24:26,680 --> 00:24:29,800
Speaker 5:  represented by the writer Guild of America. We're management. So we're not

433
00:24:29,910 --> 00:24:33,760
Speaker 5:  Yeah, we're we're just the boot also, Comcast is an investor in

434
00:24:33,760 --> 00:24:36,960
Speaker 5:  Vox Media. Oh my god. Yeah. And they Netflix own nbc,

435
00:24:37,840 --> 00:24:40,880
Speaker 5:  which yeah, we talked about Hollywood here. Its, here's the block. Comcast

436
00:24:40,880 --> 00:24:44,720
Speaker 5:  is investor in Vox, it's NBC universal division. I'm the

437
00:24:44,720 --> 00:24:47,280
Speaker 5:  executive register of Netflix show which rules you should go watch it on

438
00:24:47,280 --> 00:24:50,880
Speaker 5:  Netflix. That's part of Hollywood. And our company has six

439
00:24:50,880 --> 00:24:54,840
Speaker 5:  unions and writers of America is one of them. Alex Heat lives in la. Do

440
00:24:54,840 --> 00:24:58,640
Speaker 5:  we have to disclose that? Does that, is that anything? I have seen the

441
00:24:58,640 --> 00:24:59,440
Speaker 5:  Hollywood sign.

442
00:25:01,510 --> 00:25:05,400
Speaker 5:  It's not, it's remark. It's remarkably unremarkable. I would say. I

443
00:25:05,400 --> 00:25:08,400
Speaker 5:  did not get stars in my eyes. It didn't happen.

444
00:25:08,510 --> 00:25:09,440
Speaker 7:  Just got the bug.

445
00:25:09,610 --> 00:25:13,200
Speaker 5:  So that's fascinating, right? Like this idea of attribution Yeah. To the

446
00:25:13,200 --> 00:25:17,120
Speaker 5:  robot it it like, can a monkey take a selfie is like a real thing. Like we're

447
00:25:17,120 --> 00:25:20,000
Speaker 5:  gonna do it again but with robots at scale and then there's this contingent

448
00:25:20,000 --> 00:25:23,160
Speaker 5:  of people who are like, actually we need to stop it. Yes. Right. We, we are

449
00:25:23,160 --> 00:25:26,960
Speaker 5:  not ready for this thing to happen. And then I think that

450
00:25:26,960 --> 00:25:30,120
Speaker 5:  conversation entirely flattens out the fact that these systems are all different,

451
00:25:30,120 --> 00:25:33,800
Speaker 5:  right? And you can get an open AI in a Microsoft, in a Google

452
00:25:34,210 --> 00:25:37,920
Speaker 5:  to agree to some safety standards, but there's a lot of like

453
00:25:38,200 --> 00:25:42,080
Speaker 5:  slightly worse ones or weird open source projects that do not have to agree

454
00:25:42,080 --> 00:25:45,680
Speaker 5:  to anything. And they can still be used to do bad things and they will,

455
00:25:45,680 --> 00:25:49,520
Speaker 5:  they'll be run by criminals. So like won't do what the laws

456
00:25:49,520 --> 00:25:52,840
Speaker 5:  of various countries tell them to do. And that I, this is just a big

457
00:25:53,040 --> 00:25:57,000
Speaker 5:  swirl of a mess right now. Yeah. And it at the

458
00:25:57,000 --> 00:25:59,880
Speaker 5:  heart of it again is just like a speech question. Like what are you allowed

459
00:25:59,880 --> 00:26:02,880
Speaker 5:  to do with a computer? Yep. Are you allowed to instruct the computer to lie

460
00:26:02,880 --> 00:26:05,640
Speaker 5:  to people? Yes. Yes. Like it seems like do

461
00:26:05,640 --> 00:26:06,560
Speaker 7:  It on Twitter all the time,

462
00:26:07,780 --> 00:26:11,440
Speaker 5:  But I think there's gonna be some amount of, all right, the

463
00:26:11,440 --> 00:26:14,800
Speaker 5:  companies, the big companies, the ones that truly democratize the tools

464
00:26:15,180 --> 00:26:18,720
Speaker 5:  are gonna be either at a company market level,

465
00:26:18,890 --> 00:26:22,760
Speaker 5:  they're gonna put some restrictions in, right? Like I think mid journey in

466
00:26:22,760 --> 00:26:26,280
Speaker 5:  the rest of the image generators would do well to say we're gonna start ruining

467
00:26:26,480 --> 00:26:30,440
Speaker 5:  faces again. Hmm. Cause that was a conscious decision

468
00:26:30,440 --> 00:26:33,920
Speaker 5:  that they had made before that they wouldn't do faces and they undid it and

469
00:26:33,920 --> 00:26:37,880
Speaker 5:  like you can see how as just an industry, they could say, you know what,

470
00:26:37,880 --> 00:26:40,920
Speaker 5:  we're gonna start being bad at faces again because this is getting a little

471
00:26:40,920 --> 00:26:44,280
Speaker 5:  weird until there's like a regulatory system that we, we understand. But

472
00:26:44,280 --> 00:26:46,280
Speaker 5:  then you can also see how the government might be like, you know what, we

473
00:26:46,280 --> 00:26:50,240
Speaker 5:  want you to shut it down or like register all your users or worse like D

474
00:26:50,240 --> 00:26:52,960
Speaker 5:  r m, all the pictures so we know where they all came from and like, there's

475
00:26:52,960 --> 00:26:55,480
Speaker 5:  just a range of options on the table I think is, I mean this is what the

476
00:26:55,480 --> 00:26:58,760
Speaker 5:  verges made for. Yeah. Like one publication can talk about all these things

477
00:26:58,760 --> 00:26:59,320
Speaker 5:  that happening

478
00:26:59,320 --> 00:27:03,200
Speaker 6:  At once. But I do think you're right that to a pretty real extent, like the,

479
00:27:03,200 --> 00:27:06,760
Speaker 6:  the whatever the horse is out of the barn, the toothpaste is out of the tube,

480
00:27:07,040 --> 00:27:10,600
Speaker 6:  whatever you want to pick on that one. That like the, this antitrust argument

481
00:27:10,600 --> 00:27:14,320
Speaker 6:  that is starting to happen is like people like Lenahan and Jonathan Canter

482
00:27:14,320 --> 00:27:16,840
Speaker 6:  are basically saying these things are really expensive to build. They're

483
00:27:16,840 --> 00:27:20,760
Speaker 6:  really expensive to train. And inherently what that does is give too

484
00:27:20,760 --> 00:27:23,840
Speaker 6:  much of the power and control and sort of the ability to make this stuff

485
00:27:24,010 --> 00:27:27,400
Speaker 6:  to these big companies. Right? And it's why you look at open ai, which got

486
00:27:27,400 --> 00:27:30,760
Speaker 6:  10 billion from Microsoft and it's why Microsoft is doing it and it's why

487
00:27:30,760 --> 00:27:33,640
Speaker 6:  Google is doing it. There's just not that many out there. But that's changing

488
00:27:33,640 --> 00:27:36,400
Speaker 6:  really, really, really, really fast. And some of this stuff is starting to

489
00:27:36,400 --> 00:27:39,240
Speaker 6:  get open sourced and the training data is more available and we're hitting

490
00:27:39,240 --> 00:27:42,920
Speaker 6:  this point where building one of these things from scratch is hard, but you

491
00:27:42,920 --> 00:27:46,680
Speaker 6:  kind of don't have to anymore. Yep. Like there's just so much infrastructure

492
00:27:46,700 --> 00:27:50,360
Speaker 6:  out there now that if you, like you're saying if I wanted to just go build

493
00:27:50,360 --> 00:27:54,200
Speaker 6:  a large language model chatbot and have it be horrible, it's a hell of a

494
00:27:54,200 --> 00:27:56,840
Speaker 6:  lot easier than it was 12 months ago and it's gonna be a hell of a lot easier

495
00:27:56,930 --> 00:28:00,800
Speaker 6:  in 12 months than it is right now. Like I don't see how we can rewind that

496
00:28:00,800 --> 00:28:01,120
Speaker 6:  from here.

497
00:28:01,140 --> 00:28:04,840
Speaker 7:  See, I don't know if that's actually true because of

498
00:28:05,000 --> 00:28:08,640
Speaker 7:  Getty Getty's a good example of this. They're the professional image library.

499
00:28:09,090 --> 00:28:12,840
Speaker 7:  We use them, almost every single publication pays

500
00:28:12,840 --> 00:28:16,680
Speaker 7:  Getty so that we can use their photos. They are

501
00:28:16,680 --> 00:28:20,560
Speaker 7:  currently suing some of these ais because they've trained their data

502
00:28:20,570 --> 00:28:24,560
Speaker 7:  on Getty Yeah. Images and didn't pay licenses for it. And

503
00:28:24,560 --> 00:28:28,360
Speaker 7:  what can very easily happen there is Getty can say, you have to license

504
00:28:28,380 --> 00:28:32,280
Speaker 7:  our entire data, like all of our images to train your data

505
00:28:32,280 --> 00:28:35,720
Speaker 7:  set and we're gonna charge you thousands and thousands and probably millions

506
00:28:35,720 --> 00:28:39,480
Speaker 7:  of dollars to do so. Or actually we're just gonna build

507
00:28:39,480 --> 00:28:43,400
Speaker 7:  our own generator using our data set. And now we have

508
00:28:43,720 --> 00:28:47,520
Speaker 7:  like our photo monopoly has gotten even larger. And I think

509
00:28:47,520 --> 00:28:50,880
Speaker 7:  that's gonna be, we're seeing that with others too. Like people are starting

510
00:28:50,880 --> 00:28:54,840
Speaker 7:  to say, Hey, you can't use my artwork to train your data because

511
00:28:54,840 --> 00:28:58,440
Speaker 7:  of plagiarism. We saw that with the W G A accusing, like

512
00:28:58,440 --> 00:29:01,800
Speaker 7:  just inherently using what's publicly available as

513
00:29:01,800 --> 00:29:05,680
Speaker 7:  plagiarism. So I think that fight's still out there and

514
00:29:05,680 --> 00:29:09,600
Speaker 7:  is going to continue to happen primarily in the court. And

515
00:29:09,600 --> 00:29:12,960
Speaker 7:  that's gonna be a pretty significant component of this, I think.

516
00:29:13,310 --> 00:29:17,120
Speaker 5:  Yeah. I mean if there's a more verge story than AI copyright, I, I don't

517
00:29:17,120 --> 00:29:18,200
Speaker 5:  know what it is. I'm lit

518
00:29:18,200 --> 00:29:20,760
Speaker 7:  Up. Yeah, you just got very like the big grin on your face. You

519
00:29:20,760 --> 00:29:24,640
Speaker 5:  Got another hour. Let's do it again. The the Copyright act was

520
00:29:24,880 --> 00:29:28,360
Speaker 5:  like barely functional on the internet, right? Yeah. Like it barely

521
00:29:29,000 --> 00:29:31,920
Speaker 5:  describes what people do on the internet. It is the thing that governs most

522
00:29:31,920 --> 00:29:35,200
Speaker 5:  of our lives on the internet. It's like the default law. It's the only law

523
00:29:35,200 --> 00:29:36,920
Speaker 5:  that counts. It's like a cliche in our newsroom.

524
00:29:36,940 --> 00:29:39,680
Speaker 6:  Can I ask a question out of a really tortured metaphor that might not work

525
00:29:39,680 --> 00:29:40,800
Speaker 6:  and we can cut it out

526
00:29:41,000 --> 00:29:42,640
Speaker 5:  Shelf? What show do you think this is? Of course you should.

527
00:29:43,650 --> 00:29:47,280
Speaker 6:  Is this or is this like the Napster moment of

528
00:29:47,390 --> 00:29:51,120
Speaker 6:  this where it's like everything is free, everything is chaos. Take all this

529
00:29:51,120 --> 00:29:53,720
Speaker 6:  stuff that belongs to somebody else, shove it into a pile. You can have it

530
00:29:53,720 --> 00:29:56,440
Speaker 6:  any way you want. And like what you're talking about with Getty, it just

531
00:29:56,440 --> 00:29:59,840
Speaker 6:  made me think of like the R I A A coming after people saying, you owe 60 billion

532
00:30:00,380 --> 00:30:03,960
Speaker 6:  for the songs that you downloaded. And they successfully like

533
00:30:04,030 --> 00:30:07,800
Speaker 6:  sued enough people to make them afraid and enough companies to run them outta

534
00:30:08,000 --> 00:30:11,040
Speaker 6:  business that it sort of reverted back. And then it's like, okay, now we

535
00:30:11,040 --> 00:30:14,520
Speaker 6:  need a different kind of model for that. Like are we in that first phase

536
00:30:14,520 --> 00:30:16,480
Speaker 6:  now or is this just totally different not making that?

537
00:30:16,480 --> 00:30:20,440
Speaker 7:  I, I think so. Yeah. I mean nea you, you, you feel you have stronger feelings

538
00:30:20,440 --> 00:30:22,280
Speaker 7:  about copyright than I do and know more about

539
00:30:22,280 --> 00:30:25,920
Speaker 5:  It. It's all I care about. All of my emotions are pointed. Copy.

540
00:30:27,550 --> 00:30:28,480
Speaker 7:  What do you think?

541
00:30:28,990 --> 00:30:31,560
Speaker 5:  I think that that there's a parallel there. I don't know if it's a direct

542
00:30:31,560 --> 00:30:35,520
Speaker 5:  comparison. Like the music industry for all of its idiocy had a

543
00:30:35,520 --> 00:30:39,480
Speaker 5:  point, right? Like you were making copies of the songs and

544
00:30:39,480 --> 00:30:43,160
Speaker 5:  then giving those copies to other people, all right? Like at some point

545
00:30:43,160 --> 00:30:47,080
Speaker 5:  they should have something was amiss there, right? Like

546
00:30:47,290 --> 00:30:50,920
Speaker 5:  no one was capturing the value. The artists were working for free I think

547
00:30:51,030 --> 00:30:54,600
Speaker 5:  many, many years later. My my position on should artists work for free

548
00:30:54,780 --> 00:30:58,560
Speaker 5:  has changed from the time when I was like, music Napster the doorman. And

549
00:30:58,560 --> 00:31:02,160
Speaker 5:  that was different, right? The the economic condition of the music

550
00:31:02,480 --> 00:31:06,440
Speaker 5:  industry made them unsympathetic at that point in time, right? I was

551
00:31:06,440 --> 00:31:10,000
Speaker 5:  like, Metallica's already rich. Like I'm just taking inner sand from them.

552
00:31:10,000 --> 00:31:13,800
Speaker 5:  Like screw you Lars, right now. I think in 2023 there's

553
00:31:13,800 --> 00:31:17,640
Speaker 5:  a lot more sympathy towards people who make things and whether

554
00:31:17,640 --> 00:31:21,120
Speaker 5:  they should be compensated for those things and what's free and how you give

555
00:31:21,360 --> 00:31:24,920
Speaker 5:  credit. And like the younger generations on the internet

556
00:31:25,840 --> 00:31:29,740
Speaker 5:  are just much more attuned to like, oh, there's value extraction that happens

557
00:31:29,740 --> 00:31:32,900
Speaker 5:  on the internet every single day and we should, artists should be rich. Like

558
00:31:32,900 --> 00:31:36,020
Speaker 5:  we're mad at Spotify in general even though we won't stop using it because

559
00:31:36,020 --> 00:31:38,700
Speaker 5:  the artists are always grumbling about how they don't get paid enough. And

560
00:31:38,700 --> 00:31:42,260
Speaker 5:  just that sort of economic literacy of the internet is vastly different than

561
00:31:42,500 --> 00:31:45,580
Speaker 5:  the Napster moment. That's, and so I think that's one piece of it. The second

562
00:31:45,580 --> 00:31:49,180
Speaker 5:  piece of it, one more technical and I think will result in a number of

563
00:31:49,180 --> 00:31:52,740
Speaker 5:  disastrously hilarious rulings from courts.

564
00:31:52,750 --> 00:31:56,620
Speaker 5:  Because at the end of the day, like yes, it's training data but

565
00:31:56,620 --> 00:31:58,220
Speaker 5:  the thing they make is not a copy.

566
00:31:58,220 --> 00:32:02,180
Speaker 6:  Right? Yeah. It's a much less direct line from you burned a CD

567
00:32:02,180 --> 00:32:04,860
Speaker 6:  and gave it to people on the internet. Like that's a very straight line.

568
00:32:05,000 --> 00:32:08,620
Speaker 6:  How you get from, I wrote a screenplay to it, generated a different

569
00:32:08,620 --> 00:32:12,260
Speaker 6:  screenplay maybe potentially with some knowledge of my screenplay

570
00:32:12,310 --> 00:32:13,260
Speaker 6:  is different.

571
00:32:13,380 --> 00:32:17,100
Speaker 5:  There is a copy that's being made in there. Like I, one of the things that

572
00:32:17,100 --> 00:32:20,980
Speaker 5:  I think we forget about Cut all the time is like, it's still rooted in, in

573
00:32:21,220 --> 00:32:24,740
Speaker 5:  Adams, right? It's rooted in the idea of like making copies.

574
00:32:25,130 --> 00:32:28,540
Speaker 5:  It's, that's what it's called. And so there was a line of

575
00:32:28,780 --> 00:32:32,420
Speaker 5:  cases that literally had to say every copy that a computer

576
00:32:32,420 --> 00:32:36,260
Speaker 5:  made, it was legal. Like literally copying a program from

577
00:32:36,260 --> 00:32:40,200
Speaker 5:  a disc onto the ram of a computer was a lawsuit, right? Cause

578
00:32:40,200 --> 00:32:43,960
Speaker 5:  it's making a copy, right? And so we've kind of just like abstracted away

579
00:32:43,960 --> 00:32:47,200
Speaker 5:  what computers do, which is just constantly make billions of copies all the

580
00:32:47,200 --> 00:32:51,080
Speaker 5:  time for all kinds of reasons. And now we're gonna come back, I think to

581
00:32:51,550 --> 00:32:55,520
Speaker 5:  okay, to use our images as your trading data. You have to copy

582
00:32:55,520 --> 00:32:59,440
Speaker 5:  them all onto your disks. Just step one. Like was that

583
00:32:59,440 --> 00:33:03,200
Speaker 5:  copy authorized? Did? Is that a, is that allowed? Is that legal?

584
00:33:03,420 --> 00:33:07,320
Speaker 5:  And like we just haven't had to ask that question in the context of

585
00:33:07,480 --> 00:33:11,120
Speaker 5:  computers for about 20 years and like we're just, that's gonna be Getty's

586
00:33:11,120 --> 00:33:13,960
Speaker 5:  first argument. It's like, here's what open AI did. They went to Best Buy,

587
00:33:14,030 --> 00:33:17,440
Speaker 5:  they bought all the Western digital hard drives that existed and

588
00:33:17,600 --> 00:33:21,240
Speaker 5:  copied the, the Getty database onto them, right? And it's like,

589
00:33:21,240 --> 00:33:25,200
Speaker 5:  wait, hold on, we're doing that because that's their opening salvo

590
00:33:25,200 --> 00:33:28,920
Speaker 5:  best theory of the case. Like just making that database is a

591
00:33:28,920 --> 00:33:32,120
Speaker 5:  copyright infringement and like we're just gonna, I think we're gonna go

592
00:33:32,120 --> 00:33:35,560
Speaker 5:  back there in a weird way and maybe never get all the way to, and then we

593
00:33:35,560 --> 00:33:38,680
Speaker 5:  created derivative works of those images

594
00:33:38,960 --> 00:33:42,440
Speaker 5:  violating a computer, predict the next pixel in a sequence. I can talk about

595
00:33:42,440 --> 00:33:46,320
Speaker 5:  this all day. Can we keep, we should probably move on. Addie and

596
00:33:46,320 --> 00:33:49,720
Speaker 5:  Sarah, John and I are like lit up with the copyright story cuz it's so complicated

597
00:33:49,720 --> 00:33:53,480
Speaker 5:  and so weird and like so many old internet laws

598
00:33:53,700 --> 00:33:57,560
Speaker 5:  are coming back around, right? Foundational copyright cases of the internet

599
00:33:57,560 --> 00:34:01,120
Speaker 5:  are kind of like up for grabs in really weird ways.

600
00:34:01,300 --> 00:34:05,080
Speaker 5:  And I think that's gonna be, again, because the economic understanding

601
00:34:05,080 --> 00:34:08,960
Speaker 5:  of how the internet works is so different now. They could go another

602
00:34:08,960 --> 00:34:12,880
Speaker 5:  way. Hmm. Like the, the copyright cases that allowed Google to

603
00:34:12,880 --> 00:34:16,680
Speaker 5:  exist and make image search happened in a wildly different economic

604
00:34:16,680 --> 00:34:17,600
Speaker 5:  climate. Yeah.

605
00:34:17,600 --> 00:34:20,160
Speaker 6:  The last time we were doing this, everybody was shouting about how, you know,

606
00:34:20,160 --> 00:34:24,080
Speaker 6:  the internet wants to be free and information wants to be free. And we

607
00:34:24,080 --> 00:34:26,400
Speaker 6:  don't look at it quite like that anymore, generally

608
00:34:26,400 --> 00:34:30,000
Speaker 5:  Speaking. Yeah. Not, not even a little bit. Right? And like Google got rich

609
00:34:30,250 --> 00:34:34,080
Speaker 5:  at, at the end of the day Google captured all the value and photographers

610
00:34:34,080 --> 00:34:37,280
Speaker 5:  and musicians and whoever didn't. And like this is another one of those moments.

611
00:34:37,280 --> 00:34:41,160
Speaker 5:  And I, I just think that economic understanding of the trade off of

612
00:34:41,160 --> 00:34:44,680
Speaker 5:  information wants to be free is a lot, is a lot clearer for people.

613
00:34:44,790 --> 00:34:48,280
Speaker 5:  Yeah. Last we should just note this, there was a really well reported piece

614
00:34:48,290 --> 00:34:52,280
Speaker 5:  in the information this week claiming that Bard was birthed

615
00:34:52,280 --> 00:34:56,160
Speaker 5:  in conflict at Google, including a researcher leaving the

616
00:34:56,160 --> 00:34:59,640
Speaker 5:  team because he claimed that Bard was trained

617
00:34:59,720 --> 00:35:03,480
Speaker 5:  using chat G p T data through a tool called Shared G P T.

618
00:35:04,480 --> 00:35:07,400
Speaker 5:  Google has denied it. They gave a statement to us, we have it on the site.

619
00:35:07,470 --> 00:35:10,920
Speaker 5:  Bard is not trained on any data from shared G P T or chat G P T. It's a very

620
00:35:10,920 --> 00:35:14,240
Speaker 5:  clear denial, but I think it is just worth pointing out

621
00:35:14,470 --> 00:35:17,520
Speaker 5:  even throughout this conversation. We've mostly talked about startups, right?

622
00:35:17,520 --> 00:35:21,160
Speaker 5:  There's Microsoft on the side, but open AI is a new company, mid Journey

623
00:35:21,160 --> 00:35:24,920
Speaker 5:  is a new company. The big companies are still finding their footing,

624
00:35:24,920 --> 00:35:27,760
Speaker 5:  right? Or they've just bought into it in the case of Microsoft, right? Yeah.

625
00:35:28,610 --> 00:35:32,320
Speaker 5:  Or Bard. Okay, here's let's wrap, I'm gonna wrap this up with two

626
00:35:32,320 --> 00:35:35,600
Speaker 5:  instructions. One, if you work at Google and you're on the Bard team, let

627
00:35:35,600 --> 00:35:39,400
Speaker 5:  that shit be horny. It'll work. Just horn it up.

628
00:35:39,400 --> 00:35:43,360
Speaker 5:  Just let part flirt man it it's got swag W RZ

629
00:35:43,360 --> 00:35:46,400
Speaker 5:  from Bard. It's like that's put that on the on the board.

630
00:35:47,020 --> 00:35:51,000
Speaker 5:  And then two, if you operate a radio telescope of any kind or

631
00:35:51,200 --> 00:35:54,040
Speaker 5:  anything that can send signals in a deep space, I'm gonna give you this audio

632
00:35:54,040 --> 00:35:57,920
Speaker 5:  clip. Just broadcast it as much as you can. Swag Pope

633
00:35:57,920 --> 00:36:01,660
Speaker 5:  is important of doom. You have 25 years

634
00:36:01,660 --> 00:36:05,460
Speaker 5:  left. If you see a drippy pope, citizens of the

635
00:36:05,460 --> 00:36:09,020
Speaker 5:  Galaxy, galaxy swag Pope is a signal of doom.

636
00:36:09,640 --> 00:36:12,700
Speaker 5:  End message. And if you could just send that to space, I think,

637
00:36:14,050 --> 00:36:17,740
Speaker 5:  I think we'll be doing a surface to all the, it's so weird a time traveler

638
00:36:17,740 --> 00:36:21,500
Speaker 5:  just showed up and said, I heard he's straight here. Swag.

639
00:36:21,500 --> 00:36:22,620
Speaker 5:  Pope is doomed.

640
00:36:24,680 --> 00:36:25,940
Speaker 5:  All right, we gotta take a break.

641
00:36:31,220 --> 00:36:34,940
Speaker 7:  Sport for today's show comes from Deloitte. What does a future look like?

642
00:36:35,130 --> 00:36:38,980
Speaker 7:  It's a question that drives progress, but more importantly, how do you

643
00:36:39,200 --> 00:36:43,140
Speaker 7:  get there? It's curiosity, resourcefulness and bold ideas that

644
00:36:43,140 --> 00:36:47,060
Speaker 7:  can drive us even further than new technology. Because while technology can

645
00:36:47,060 --> 00:36:50,860
Speaker 7:  take you far human exploration could take you even farther. Deloitte

646
00:36:50,860 --> 00:36:54,740
Speaker 7:  helps businesses build the future only they can imagine by melding deep business

647
00:36:54,740 --> 00:36:58,540
Speaker 7:  acumen and innovative technology with a vast team of tech savvy

648
00:36:58,540 --> 00:37:02,420
Speaker 7:  professionals. Unlock technology as powerful as your vision and push

649
00:37:02,420 --> 00:37:05,740
Speaker 7:  the boundaries of the possible so that you can stay a step ahead with the

650
00:37:05,740 --> 00:37:09,500
Speaker 7:  thinking to help you transform what's next into what's now helping

651
00:37:09,500 --> 00:37:12,860
Speaker 7:  you see the extraordinary potential in the seemingly ordinary while blending

652
00:37:12,860 --> 00:37:16,660
Speaker 7:  the possible with the practical is what Deloitte does. See how you can

653
00:37:16,900 --> 00:37:18,700
Speaker 7:  engineer advantage with Deloitte at

654
00:37:18,980 --> 00:37:22,860
Speaker 7:  deloitte.com/us/engineering advantage.

655
00:37:27,830 --> 00:37:31,770
Speaker 9:  The all new Chevy Colorado is made for more stacked with the latest

656
00:37:31,780 --> 00:37:35,290
Speaker 9:  in vehicle technologies like a class leading 11 inch diagonal center

657
00:37:35,290 --> 00:37:38,970
Speaker 9:  touchscreen and an extra large wireless charging pad. Plus it

658
00:37:38,970 --> 00:37:42,810
Speaker 9:  features wireless apple CarPlay and Android auto compatibility to make staying

659
00:37:42,810 --> 00:37:46,610
Speaker 9:  connected easy wherever your adventure takes you. Chevy Colorado

660
00:37:46,960 --> 00:37:49,210
Speaker 9:  made for more. Learn more at

661
00:37:49,210 --> 00:37:53,170
Speaker 9:  chevrolet.com/truck/colorado claims based on latest

662
00:37:53,170 --> 00:37:54,010
Speaker 9:  competitive data.

663
00:37:59,050 --> 00:38:02,920
Speaker 5:  Alright, we're back. Let's talk about some gadgets or promised

664
00:38:02,920 --> 00:38:06,680
Speaker 5:  gadgets soon to be gadgets. Gadgets that might also be important of

665
00:38:06,680 --> 00:38:09,640
Speaker 5:  doom but not for us. For Apple.

666
00:38:09,640 --> 00:38:10,360
Speaker 7:  Just apple.

667
00:38:11,210 --> 00:38:15,080
Speaker 5:  So WW C was announced this week, June 5th. We will

668
00:38:15,240 --> 00:38:19,120
Speaker 5:  probably send an army of people, we could do the preview like WW C is

669
00:38:19,120 --> 00:38:22,200
Speaker 5:  kinda like a known quantity in the base level, right? Yeah. They announced

670
00:38:22,200 --> 00:38:25,440
Speaker 5:  the new versions of iOS, Mac os, t V os, home os all

671
00:38:25,780 --> 00:38:29,440
Speaker 5:  oss watch os and then maybe we usually see a previous

672
00:38:29,960 --> 00:38:32,360
Speaker 5:  hardware this year. That piece of the puzzle

673
00:38:33,620 --> 00:38:37,560
Speaker 5:  is loaded with meaning, right? Yes. Because people think it's gonna be the

674
00:38:37,560 --> 00:38:40,880
Speaker 5:  big mixed reality headset and there's like a lot of

675
00:38:40,880 --> 00:38:44,520
Speaker 5:  reporting about it in a way that is unusual in

676
00:38:44,520 --> 00:38:46,880
Speaker 5:  tone and unusual in volume I would say.

677
00:38:46,880 --> 00:38:49,440
Speaker 6:  Okay. I'm really glad you said that cuz I was actually trying to remember

678
00:38:49,610 --> 00:38:53,400
Speaker 6:  in prepping for this, if there has ever been a sort of run

679
00:38:53,640 --> 00:38:57,120
Speaker 6:  up to an Apple product quite like this one because like when the Apple Watch

680
00:38:57,120 --> 00:39:00,680
Speaker 6:  launched there was a lot of smoke that there was gonna be a watch. That's

681
00:39:00,680 --> 00:39:04,480
Speaker 6:  kind of all we knew about it, right? But it feels like as Apple gets ready

682
00:39:04,530 --> 00:39:08,440
Speaker 6:  to come out with its mixed reality headset, which we think is likely to

683
00:39:08,440 --> 00:39:12,360
Speaker 6:  come at WWDC that we should get into why suddenly there's questions about

684
00:39:12,360 --> 00:39:15,880
Speaker 6:  that. We know a shocking amount about what the thing is gonna cost, what

685
00:39:15,880 --> 00:39:19,800
Speaker 6:  it's gonna do, what it looks like, how it works, how people at Apple feel

686
00:39:19,800 --> 00:39:23,680
Speaker 6:  about it. Like is this, can you remember another time when it feels like

687
00:39:23,680 --> 00:39:27,360
Speaker 6:  a product Apple is working on this much feels like it has been sort of out

688
00:39:27,360 --> 00:39:29,240
Speaker 6:  there in the world this much ahead of time.

689
00:39:29,240 --> 00:39:30,400
Speaker 5:  The iPad, was it?

690
00:39:30,400 --> 00:39:30,720
Speaker 6:  Yeah.

691
00:39:30,820 --> 00:39:31,240
Speaker 5:  Did

692
00:39:31,240 --> 00:39:34,920
Speaker 7:  The iPad have that same like sense of just the

693
00:39:34,920 --> 00:39:38,680
Speaker 7:  grimace emoji that seems to be surrounding the mixed reality headset?

694
00:39:38,710 --> 00:39:41,600
Speaker 6:  Well it did right after because everybody thought the name iPad was stupid,

695
00:39:41,600 --> 00:39:43,560
Speaker 6:  but I actually don't remember if it was there before.

696
00:39:44,260 --> 00:39:48,010
Speaker 5:  No, it definitely had more hopes and dreams, but if you just remember,

697
00:39:48,480 --> 00:39:51,690
Speaker 5:  they had to like go to the media like Steve Jobs and Asie iPad and he was

698
00:39:51,690 --> 00:39:54,530
Speaker 5:  like, it's, this is the future of newspapers and magazines. So like, and

699
00:39:54,730 --> 00:39:57,450
Speaker 5:  then remember like Rippert Murdoch made the daily Oh

700
00:39:57,450 --> 00:39:57,610
Speaker 6:  Yeah.

701
00:39:57,680 --> 00:40:00,370
Speaker 5:  R i p The Daily, which was actually great. And the staff of the Daily has

702
00:40:00,370 --> 00:40:03,610
Speaker 5:  gone on to do amazing things around the hundred. Yeah. Because he assembled

703
00:40:03,610 --> 00:40:07,210
Speaker 5:  like an amazing team to make that, what was it? It was like an iPad app that

704
00:40:07,210 --> 00:40:09,290
Speaker 5:  updated with news every day and then it failed because

705
00:40:09,290 --> 00:40:13,210
Speaker 6:  You could turn pages. It was the most like anachronistic thing I think Apple

706
00:40:13,210 --> 00:40:16,090
Speaker 6:  has ever done. It was amazing. They made a print magazine that you could

707
00:40:16,090 --> 00:40:17,370
Speaker 6:  only read on the iPad. It

708
00:40:17,370 --> 00:40:20,530
Speaker 5:  Was like truly strange. Right? But like, because they had made all those

709
00:40:20,530 --> 00:40:24,330
Speaker 5:  deals, like a lot of people had awareness of the iPad. Yeah. Yeah. And, and

710
00:40:24,330 --> 00:40:28,250
Speaker 5:  like pre-baked good thoughts about the iPad. Like they were rooted in

711
00:40:28,250 --> 00:40:31,410
Speaker 5:  actual experiences. So like I would just like remind you like yes, the iPad

712
00:40:31,410 --> 00:40:35,130
Speaker 5:  was that thing where the hype around it was so high and

713
00:40:35,130 --> 00:40:39,050
Speaker 5:  enough people had like, sort of used it that there was like a a

714
00:40:39,050 --> 00:40:42,850
Speaker 5:  level of doubts, right? If you remember it's just a big iPhone was like

715
00:40:42,850 --> 00:40:46,810
Speaker 5:  the Totally. And that was correct. Yeah. It turns out

716
00:40:47,010 --> 00:40:50,730
Speaker 5:  years later it's a, it's a big iPhone with some features of the Mac and it

717
00:40:50,730 --> 00:40:54,530
Speaker 5:  should just be a Mac remains the, the gestalt of the iPad. Yep.

718
00:40:54,750 --> 00:40:58,530
Speaker 5:  But this one is different. It's more than that. Right? And I think

719
00:40:58,530 --> 00:41:02,410
Speaker 5:  it's because the iPad was obvious, like in the best possible way. Sure.

720
00:41:02,410 --> 00:41:06,250
Speaker 5:  You should make a really big iPhone. People love iPhones. People will

721
00:41:06,250 --> 00:41:08,610
Speaker 5:  love this thing. Yeah. And there's all kinds of stuff you could do. You can

722
00:41:08,610 --> 00:41:12,210
Speaker 5:  just imagine where it will go. You can imagine how to make it right, like

723
00:41:12,210 --> 00:41:14,610
Speaker 5:  make the screen bigger and like whatever.

724
00:41:14,610 --> 00:41:17,970
Speaker 6:  Right. You made a thing that's all screened do more Screen

725
00:41:18,010 --> 00:41:21,530
Speaker 5:  Apple now with the mixed reality headset is operating in the context of,

726
00:41:21,960 --> 00:41:25,690
Speaker 5:  well we have them, they exist as products. Most of,

727
00:41:25,690 --> 00:41:29,330
Speaker 5:  there's the meta quests pro sitting right behind me on that shelf there is

728
00:41:29,330 --> 00:41:32,930
Speaker 5:  not been charged in weeks, months. You know, like

729
00:41:33,240 --> 00:41:33,360
Speaker 5:  whatever.

730
00:41:34,000 --> 00:41:36,120
Speaker 6:  You're you're out on the supernatural. You're not boxing

731
00:41:36,120 --> 00:41:39,960
Speaker 5:  Anymore. I, no, I use the Quest two for that. Oh. Oh. It's lighter and simpler

732
00:41:39,960 --> 00:41:43,360
Speaker 5:  and that's fair. Right? And it doesn't have 90 cameras. It's not trying to

733
00:41:43,360 --> 00:41:46,880
Speaker 5:  do a thing that no one wants to do. Yeah. Right. It's just, it's a game console.

734
00:41:46,990 --> 00:41:50,760
Speaker 5:  I have a p I have a review unit or the PSVR two in the other room

735
00:41:50,760 --> 00:41:54,400
Speaker 5:  and I spend all my chase points on a Logitech racing wheel.

736
00:41:54,840 --> 00:41:58,520
Speaker 5:  And I gotta tell you, grand Tomo in the PSVR two is the single best

737
00:41:58,520 --> 00:42:02,440
Speaker 5:  experience you can have. It makes me sick after 20 minutes, but boy

738
00:42:02,440 --> 00:42:05,560
Speaker 5:  do I wanna not be sick again so I can go spend another 20 minutes in it.

739
00:42:05,710 --> 00:42:09,680
Speaker 5:  It's so fun. Right? And like as game consoles, like that's really cool.

740
00:42:09,680 --> 00:42:13,240
Speaker 5:  I I cannot recommend that experience. Now Sean Holster wrote about it and

741
00:42:13,240 --> 00:42:17,040
Speaker 5:  I immediately figured out how to have this experience of like VR racing

742
00:42:17,040 --> 00:42:20,920
Speaker 5:  game cuz it's so cool. That's not a market that's big enough for Apple.

743
00:42:20,920 --> 00:42:24,600
Speaker 5:  They've gotta do a thing here. So Alex Heath, when he was at the information

744
00:42:24,600 --> 00:42:28,440
Speaker 5:  broke the price like $3,000. That was his scoop. It's held up so far.

745
00:42:28,500 --> 00:42:32,040
Speaker 5:  Yep. And then there's this stack of problems that we just know

746
00:42:32,320 --> 00:42:36,080
Speaker 5:  really well because of the meta quests too. And just the reality of what

747
00:42:36,080 --> 00:42:39,320
Speaker 5:  these problems are. Like you've gotta build a headset that doesn't look

748
00:42:39,510 --> 00:42:43,120
Speaker 5:  ridiculous. Very few people have accomplished this. Maybe no one has,

749
00:42:43,190 --> 00:42:45,000
Speaker 6:  Okay, zero people have accomplished this yet.

750
00:42:45,310 --> 00:42:49,200
Speaker 5:  Yeah. They're just familiar now. They're not not ridiculous. Right? It's

751
00:42:49,200 --> 00:42:52,120
Speaker 5:  like clowns, like, you know what a clown looks like. You're not like, oh

752
00:42:52,120 --> 00:42:56,080
Speaker 5:  my God, a click no, that's ridiculous, but like a familiar way just there.

753
00:42:56,080 --> 00:42:59,640
Speaker 5:  Yeah. You've gotta, it's gotta have a processor in it. It's gotta have a

754
00:42:59,640 --> 00:43:02,680
Speaker 5:  display system that doesn't suck all the power. It's gotta have a bunch of

755
00:43:02,680 --> 00:43:05,160
Speaker 5:  cameras. You've gotta process the cameras in real time and augment the information.

756
00:43:05,160 --> 00:43:08,640
Speaker 5:  Like we've done this stack on the ver has a hundred times the problems here

757
00:43:08,640 --> 00:43:12,320
Speaker 5:  at every level are novel computing problems. Yes. And then you, you're

758
00:43:12,320 --> 00:43:15,840
Speaker 5:  constrained in power and battery life and all this stuff. And so Apple's

759
00:43:15,840 --> 00:43:19,480
Speaker 5:  solutions, as far as we understand are like external battery pack.

760
00:43:20,070 --> 00:43:23,960
Speaker 5:  They don't want you to wear it all day. Really high priced to make it clear

761
00:43:23,960 --> 00:43:27,920
Speaker 5:  this is not a mass consumer product. And then this reporting

762
00:43:27,920 --> 00:43:30,360
Speaker 5:  that we that's come out over the past few weeks, we're like, they're searching

763
00:43:30,360 --> 00:43:34,040
Speaker 5:  for a killer app. Mark Gurman said this will have the same curve as the watch.

764
00:43:34,240 --> 00:43:37,480
Speaker 5:  Like that's their hope. And if you'll recall, the watch launched, it was

765
00:43:37,480 --> 00:43:39,960
Speaker 5:  really messy. The battery life was constrained and then they figured out

766
00:43:39,960 --> 00:43:43,320
Speaker 5:  as a fitness tracker with notifications and now people love it. And also

767
00:43:43,320 --> 00:43:46,840
Speaker 5:  they use their ecosystem lock in ruthlessly to make sure it has no

768
00:43:46,840 --> 00:43:50,600
Speaker 5:  competition for iOS owners, whatever. But here it's like

769
00:43:50,680 --> 00:43:54,200
Speaker 5:  every layer, there's a novel technology problem and there's no killer app

770
00:43:54,440 --> 00:43:58,080
Speaker 5:  because they're not gonna do Grand Tomo VR on a $3,000 headset.

771
00:43:58,130 --> 00:44:01,720
Speaker 5:  They're just not gonna do it. And also they suck at games like

772
00:44:01,940 --> 00:44:04,640
Speaker 5:  yes they have Apple Arcade, but you're, you're, you are just not gonna buy

773
00:44:04,640 --> 00:44:07,240
Speaker 5:  a $3,000 headset to play Monument Valley. You know? It's like, it's

774
00:44:07,240 --> 00:44:10,200
Speaker 6:  Not gonna happen. Well apparently the thing that they're talking the most

775
00:44:10,200 --> 00:44:14,000
Speaker 6:  about is, I think Tim Cook keeps calling it co-presence has been the,

776
00:44:14,000 --> 00:44:17,920
Speaker 6:  the thing that's been reported is basically really great video

777
00:44:17,920 --> 00:44:21,600
Speaker 6:  chat and this feeling like, you know, I can see through your eyes

778
00:44:21,600 --> 00:44:24,280
Speaker 6:  because you're wearing the headset and it's looking out and that's,

779
00:44:24,470 --> 00:44:28,120
Speaker 5:  That shit's gonna make people vomit all over everything all the time. Can

780
00:44:28,120 --> 00:44:30,760
Speaker 6:  You imagine? Yes, a hundred percent. And even to the extent that like that

781
00:44:30,760 --> 00:44:34,720
Speaker 6:  is, is cool as far as it goes. Like I think we've all arrived

782
00:44:34,720 --> 00:44:38,440
Speaker 6:  at a point where we've decided that Google Meet and Zoom

783
00:44:38,740 --> 00:44:42,160
Speaker 6:  are pretty good for what they do. And the bar for

784
00:44:42,390 --> 00:44:46,280
Speaker 6:  like, let me get dedicated hardware to have a meeting every single

785
00:44:46,280 --> 00:44:50,120
Speaker 6:  time is, is so much higher than anybody realizes. And like,

786
00:44:50,120 --> 00:44:52,520
Speaker 6:  this is what Meta has been trying to do for a while. They're like, what if

787
00:44:52,520 --> 00:44:56,400
Speaker 6:  instead of having a Zoom meeting, you had a meeting in Horizons. Yeah. And

788
00:44:56,400 --> 00:44:59,360
Speaker 6:  you all sat around a table and didn't have legs and made weird faces at each

789
00:44:59,360 --> 00:45:02,480
Speaker 6:  other. And it's like, actually no one wants that. Well

790
00:45:02,480 --> 00:45:06,440
Speaker 5:  I will say Horizons is so hard to use that I don't

791
00:45:06,440 --> 00:45:09,960
Speaker 5:  even know if I don't want that. Right. I know for a fact

792
00:45:10,390 --> 00:45:11,960
Speaker 5:  I do not want to use their software.

793
00:45:12,100 --> 00:45:12,520
Speaker 6:  Yes.

794
00:45:12,520 --> 00:45:16,160
Speaker 5:  That's the, like without question the idea, I have the remote

795
00:45:16,160 --> 00:45:19,960
Speaker 5:  desktop server on my Mac and they're constantly updating it like every

796
00:45:19,960 --> 00:45:23,760
Speaker 5:  day. It's like a new version of the meta quests remote desktop. It's like,

797
00:45:23,940 --> 00:45:27,400
Speaker 5:  why like, shouldn't you spend all your engineering effort? I'm like

798
00:45:27,490 --> 00:45:30,040
Speaker 5:  joining a meeting and not having it crash.

799
00:45:30,390 --> 00:45:33,440
Speaker 6:  Well, so that's kind of the thing. Right? And this is what you're talking

800
00:45:33,440 --> 00:45:36,760
Speaker 6:  about, about having to solve all these problems simultaneously. Like, have

801
00:45:36,760 --> 00:45:39,920
Speaker 6:  you ever seen the movie The Kingsman? Yeah. Yes. So do you know the thing

802
00:45:39,920 --> 00:45:43,200
Speaker 6:  in the Kingsman where to have a meeting, you sit down and you basically put

803
00:45:43,200 --> 00:45:47,000
Speaker 6:  on a pair of like Wayfair sunglasses and immediately

804
00:45:47,000 --> 00:45:50,360
Speaker 6:  around you, you sort of sit at your chair at the table and everybody else

805
00:45:50,360 --> 00:45:53,760
Speaker 6:  is sitting at the same table in other places in the world and their, their

806
00:45:53,760 --> 00:45:56,640
Speaker 6:  holograms show up with you. So it appears roughly as if you're all sitting

807
00:45:56,640 --> 00:46:00,560
Speaker 6:  at the table super into that. That sounds great. Short of that, I

808
00:46:00,560 --> 00:46:03,800
Speaker 6:  suspect almost every experience that isn't that

809
00:46:04,080 --> 00:46:08,000
Speaker 6:  sucks. Okay. And will be more work than it's worth to just talk to your

810
00:46:08,200 --> 00:46:08,880
Speaker 6:  coworkers or friends.

811
00:46:09,510 --> 00:46:12,760
Speaker 7:  What if the mixed reality headset let you have feet?

812
00:46:13,680 --> 00:46:14,680
Speaker 5:  That's the big winner.

813
00:46:14,710 --> 00:46:15,400
Speaker 7:  That's the

814
00:46:15,400 --> 00:46:16,720
Speaker 6:  Winner. Right? $5,000 I

815
00:46:17,460 --> 00:46:21,160
Speaker 5:  Has already promised feet or the oh, the, the brightest mind technology,

816
00:46:21,290 --> 00:46:21,640
Speaker 5:  to

817
00:46:21,640 --> 00:46:25,000
Speaker 6:  Be fair, he feet, legs, we have not talked about feet yet. Feet is like,

818
00:46:25,070 --> 00:46:26,240
Speaker 6:  that's a apple's

819
00:46:26,240 --> 00:46:29,080
Speaker 7:  Gonna go. Apple's gonna skip legs and go straight to feet and you're gonna

820
00:46:29,080 --> 00:46:29,240
Speaker 7:  be like,

821
00:46:29,240 --> 00:46:30,560
Speaker 6:  You have no legs but feet.

822
00:46:30,830 --> 00:46:31,320
Speaker 7:  Yeah.

823
00:46:31,630 --> 00:46:32,240
Speaker 5:  You're

824
00:46:32,480 --> 00:46:33,760
Speaker 6:  Just feet in hands. Nothing else.

825
00:46:34,630 --> 00:46:38,440
Speaker 5:  It's like this emoji it's that, you know, like the the emoji that's just

826
00:46:38,440 --> 00:46:39,600
Speaker 5:  the the face with the hands.

827
00:46:40,080 --> 00:46:40,760
Speaker 6:  Yeah, exactly.

828
00:46:41,150 --> 00:46:42,080
Speaker 7:  You're now feet.

829
00:46:42,250 --> 00:46:45,960
Speaker 5:  So the Kingsman thing is like, that's what everybody wants. Yes. Right. That's,

830
00:46:45,960 --> 00:46:48,560
Speaker 5:  that's a useful analogy for this, that movies also really funny. You should

831
00:46:48,560 --> 00:46:48,960
Speaker 5:  go, it's a

832
00:46:48,960 --> 00:46:50,000
Speaker 6:  Great movie. Everybody should watch it. Yeah.

833
00:46:50,740 --> 00:46:54,440
Speaker 5:  But every time I see that scene, I'm like, okay, how do you know what chair

834
00:46:54,720 --> 00:46:57,120
Speaker 5:  everyone's in? How do they know it? Like they're not, are they all in the

835
00:46:57,120 --> 00:46:58,360
Speaker 5:  same room but the room is empty?

836
00:46:58,450 --> 00:47:02,440
Speaker 6:  No. They all, yes, they all have the same room. I think, I could be wrong

837
00:47:02,440 --> 00:47:03,000
Speaker 6:  about this, I haven't seen,

838
00:47:03,000 --> 00:47:06,080
Speaker 5:  But do they all sit like, what if you screw up and you everyone sits at the

839
00:47:06,080 --> 00:47:08,560
Speaker 5:  head of the table? Well, like, just the amount of,

840
00:47:08,560 --> 00:47:10,200
Speaker 6:  There's a lot of laps sitting happening, like

841
00:47:10,940 --> 00:47:11,520
Speaker 5:  The future,

842
00:47:11,520 --> 00:47:13,520
Speaker 6:  Like sitting on each other's laps by accident. Who's,

843
00:47:13,520 --> 00:47:17,000
Speaker 5:  That's what it's, and then you need a picture of them, of their faces.

844
00:47:17,120 --> 00:47:20,800
Speaker 5:  Right. And if you're wearing this Apple headset, you've gotta recreate that

845
00:47:20,800 --> 00:47:24,680
Speaker 5:  face. So like you're just creating this like stack of

846
00:47:24,680 --> 00:47:28,560
Speaker 5:  problems that is vastly more expensive and vastly more

847
00:47:28,560 --> 00:47:32,280
Speaker 5:  complicated and vastly less reliable than getting on a plane.

848
00:47:32,540 --> 00:47:36,480
Speaker 6:  Yes. And I think my, my running theory is if you solve 60%

849
00:47:36,480 --> 00:47:40,120
Speaker 6:  of that problem, you've made everything worse. And that, that actually

850
00:47:40,500 --> 00:47:44,120
Speaker 6:  the only way to do it is to fully all the way

851
00:47:44,120 --> 00:47:47,560
Speaker 6:  completely do it. And I, I think like

852
00:47:48,100 --> 00:47:52,040
Speaker 6:  one of the things that it seems possible that Apple is gonna do is basically

853
00:47:52,190 --> 00:47:56,040
Speaker 6:  render big screens as you use these things that I think rather than

854
00:47:56,040 --> 00:47:58,960
Speaker 6:  try to do some of the AR stuff, it'll just kind of look like you have a big

855
00:47:59,120 --> 00:48:02,960
Speaker 6:  floating screen in front of your face. And that is substantially less exciting,

856
00:48:02,960 --> 00:48:06,800
Speaker 6:  but mitigates some of the like crappy, uncanny valley of all of this, I think.

857
00:48:07,020 --> 00:48:10,920
Speaker 6:  But yeah, I just, I, I have such a hard time imagining that Apple has

858
00:48:10,920 --> 00:48:14,520
Speaker 6:  solved every single one of these problems in its fullest way

859
00:48:14,690 --> 00:48:18,280
Speaker 6:  in order to be able to do this. And I think it's a real bummer for Apple.

860
00:48:19,200 --> 00:48:22,440
Speaker 6:  Like if I work at Apple, I'm really, really upset that all of this is coming

861
00:48:22,440 --> 00:48:25,040
Speaker 6:  out because even to the extent that it's gonna dull the hype. So

862
00:48:25,040 --> 00:48:27,520
Speaker 5:  This is the reporting, we should bring this all the way back around. This

863
00:48:27,520 --> 00:48:31,400
Speaker 5:  is the reporting, right? This is what you and I are both noticing. One of

864
00:48:31,400 --> 00:48:34,920
Speaker 5:  the stories is the design team didn't wanna ship it and Tim Cook overruled

865
00:48:34,920 --> 00:48:38,720
Speaker 5:  them. Ooh. Now obviously I think every story like this is,

866
00:48:38,720 --> 00:48:41,920
Speaker 5:  people will tell you it's too simple. That's not how it actually, like it's

867
00:48:41,920 --> 00:48:45,120
Speaker 5:  a big company, lots of process, but like that's a story that somebody wanted

868
00:48:45,120 --> 00:48:48,080
Speaker 5:  told, right? There's a group of people who thought this isn't ready yet.

869
00:48:48,080 --> 00:48:51,680
Speaker 5:  Yep. And we shouldn't do it. There's other stories about people being worried

870
00:48:51,680 --> 00:48:54,640
Speaker 5:  about the price. There's other stories about people saying there's not a

871
00:48:54,640 --> 00:48:57,880
Speaker 5:  killer app and like co-presence like this isn't gonna do it. I don't wanna

872
00:48:57,880 --> 00:49:01,560
Speaker 5:  knock this. Right? Like it's, if you can get to AR glasses, which is where

873
00:49:01,560 --> 00:49:05,400
Speaker 5:  Apple wants to go, which is where Meadow wants to go, I, that is the

874
00:49:05,400 --> 00:49:07,400
Speaker 5:  dream. Like I was,

875
00:49:07,810 --> 00:49:09,360
Speaker 6:  Is awesome. I really, really, really do

876
00:49:09,360 --> 00:49:13,120
Speaker 5:  Well, I know it's a killer app for AR glasses is it's faces and names. Like

877
00:49:13,120 --> 00:49:16,960
Speaker 5:  I was just at the ASME awards where the Verge won an award for

878
00:49:16,960 --> 00:49:20,240
Speaker 5:  best print design. It was great. And I was sitting next to Anna Wintour when

879
00:49:20,240 --> 00:49:23,800
Speaker 5:  we won the award for best print design and I screamed, it's a website and

880
00:49:23,800 --> 00:49:27,080
Speaker 5:  that will go down. I mean it truly one of the most hilarious achievements

881
00:49:27,080 --> 00:49:30,840
Speaker 5:  of our careers. And I mean that in both senses. It's an achievement, right?

882
00:49:30,840 --> 00:49:34,040
Speaker 5:  The Homeland Series was very complicated. There's a lot of incredible reporting

883
00:49:34,040 --> 00:49:37,560
Speaker 5:  on it. We made a zine that was really difficult. It's beautiful.

884
00:49:37,560 --> 00:49:40,840
Speaker 5:  Christian Raki or Art Tractor like did a beautiful job with it. But we're,

885
00:49:40,840 --> 00:49:44,160
Speaker 5:  we won best print award. Like that's hilarious.

886
00:49:45,300 --> 00:49:48,880
Speaker 5:  And so like have it announced or we're sitting next to like Conde Nastro,

887
00:49:48,880 --> 00:49:52,800
Speaker 5:  like hilarious. It was so good. But I was like at a thing and I was like,

888
00:49:52,800 --> 00:49:55,840
Speaker 5:  look at all these people. And I was like, man, if I could just put names

889
00:49:55,840 --> 00:49:59,640
Speaker 5:  to these faces, I'd be like king of the universe. I'd be honest to vote Patel

890
00:49:59,640 --> 00:50:03,080
Speaker 5:  man. Like let's do it. And like I want that. I

891
00:50:03,460 --> 00:50:06,960
Speaker 5:  insurmountable problems all the way to, you need a worldwide database of

892
00:50:06,960 --> 00:50:10,360
Speaker 5:  faces and names, which seems bad. Facebook has it though,

893
00:50:10,940 --> 00:50:14,520
Speaker 5:  but boy would I buy that product in one second. But to your point, David,

894
00:50:14,520 --> 00:50:18,240
Speaker 5:  that's a hundred percent, 60% of that product is a $3,000 headset

895
00:50:18,240 --> 00:50:21,320
Speaker 5:  with an external battery pack that doesn't actually do augmented reality

896
00:50:21,320 --> 00:50:22,720
Speaker 5:  does pass through video mixed

897
00:50:22,720 --> 00:50:25,000
Speaker 6:  Reality. And it just says like, I think that guy's name is Bill.

898
00:50:25,320 --> 00:50:28,840
Speaker 5:  And it's like, because it's Apple and their privacy thing, it's like super

899
00:50:28,840 --> 00:50:32,480
Speaker 5:  wrong all the time. Exactly. Like

900
00:50:33,260 --> 00:50:37,120
Speaker 5:  you can see why there's drama around this product and why, you know,

901
00:50:37,120 --> 00:50:40,240
Speaker 5:  to some extent they're, they're actively lowering expectations for

902
00:50:40,240 --> 00:50:43,800
Speaker 7:  It. This is also like probably the most ambitious

903
00:50:43,800 --> 00:50:47,720
Speaker 7:  Apple's been in a while, right? Like Apple's whole mo, I think they even

904
00:50:47,720 --> 00:50:51,200
Speaker 7:  mention it in the New York Times piece about people being

905
00:50:51,200 --> 00:50:54,640
Speaker 7:  nervous and not really wanting this thing to go out was that Apple

906
00:50:54,640 --> 00:50:58,600
Speaker 7:  usually waits until the markets kind of mature. Like when the Apple Watch

907
00:50:58,600 --> 00:51:02,200
Speaker 7:  came out, Fitbit was already huge. People were wearing these bands all the

908
00:51:02,200 --> 00:51:05,160
Speaker 7:  time. So people were like, oh yeah, it's a new band but it's better. The

909
00:51:05,160 --> 00:51:08,000
Speaker 7:  iPhone was the same thing. Smartphones had been around for a while. They

910
00:51:08,000 --> 00:51:11,840
Speaker 7:  just figured out pension zoom better laptops, everything. And this time they're

911
00:51:11,840 --> 00:51:15,640
Speaker 7:  like, yeah, this market is not mature. It has not proven itself

912
00:51:15,690 --> 00:51:19,440
Speaker 7:  in any way, shape or form. We're gonna do it anyway. And that's just super

913
00:51:19,440 --> 00:51:20,240
Speaker 7:  unlike Apple.

914
00:51:20,490 --> 00:51:23,600
Speaker 5:  So I would make the comparison to the watch, right? They did create the smartwatch

915
00:51:23,600 --> 00:51:27,560
Speaker 5:  market like the Pebble existed, but everyone kind of knew that wasn't

916
00:51:27,560 --> 00:51:31,360
Speaker 5:  it? You know? Yeah. They created a modern smartphone market like

917
00:51:31,360 --> 00:51:34,960
Speaker 5:  Noki existed and I'm sure people are gonna start sending us any angry emails,

918
00:51:34,960 --> 00:51:38,840
Speaker 5:  but like those phones were not mainstream in this country. They were in some

919
00:51:38,840 --> 00:51:39,680
Speaker 5:  cases mainstream in

920
00:51:39,680 --> 00:51:42,840
Speaker 7:  Europe. They, they took something that was already deeply flawed. Like they,

921
00:51:42,840 --> 00:51:45,840
Speaker 7:  these were, these were other places where it was super, super deeply flawed

922
00:51:45,840 --> 00:51:49,480
Speaker 7:  and Apple said, we're gonna make it perfect. And they did.

923
00:51:49,640 --> 00:51:53,560
Speaker 5:  Right? So the Steve Jobs used to be able to put up the photos of the competitor

924
00:51:53,560 --> 00:51:57,120
Speaker 5:  products and be like, look at this shit. Right? I fixed it right With the

925
00:51:57,120 --> 00:52:00,360
Speaker 5:  watch, they were not able to do that. Right? And this is kind of like Tim

926
00:52:00,470 --> 00:52:04,360
Speaker 5:  Cook's legacy is he's pushing 'em into categories that have not yet been

927
00:52:04,360 --> 00:52:08,240
Speaker 5:  defined. And the watch was a real mess. And the thing with the

928
00:52:08,240 --> 00:52:11,960
Speaker 5:  watch was that it had conviction behind it. This is what Johnny

929
00:52:11,960 --> 00:52:15,880
Speaker 5:  Ive wanted to do. We, we had Trip Mickel on the show. He wrote the great

930
00:52:15,880 --> 00:52:19,520
Speaker 5:  book about Johnny and Tim and the legacy of after Steve there.

931
00:52:19,990 --> 00:52:23,520
Speaker 5:  This is like Johnny, ive wanted to do it. It's a product that made sense.

932
00:52:23,520 --> 00:52:26,240
Speaker 5:  Like intuitively you're like, do I want a computer on my wrist? And everyone

933
00:52:26,240 --> 00:52:27,640
Speaker 5:  thinks about Dick Tracy and the answer is yes.

934
00:52:27,950 --> 00:52:29,040
Speaker 7:  Yeah, a hundred percent.

935
00:52:29,310 --> 00:52:33,000
Speaker 5:  They didn't know what it should do. And the battery life was a mess and the

936
00:52:33,000 --> 00:52:36,960
Speaker 5:  screen was all this stuff. But they knew that like this category should definitely

937
00:52:36,960 --> 00:52:40,720
Speaker 5:  exist and it, there's enough value

938
00:52:40,720 --> 00:52:44,600
Speaker 5:  that is like articulable, like you can say what the value will be if we get

939
00:52:44,600 --> 00:52:45,520
Speaker 5:  everything right along the way.

940
00:52:45,520 --> 00:52:49,040
Speaker 6:  Well, and even to the extent that they were wrong about what the watch was

941
00:52:49,040 --> 00:52:52,360
Speaker 6:  for, which is I think the, the like legacy of the watch and as we've talked

942
00:52:52,360 --> 00:52:55,520
Speaker 6:  about is that Apple was right about the hardware and totally wrong about

943
00:52:55,520 --> 00:52:58,440
Speaker 6:  what people were gonna use it for. And it took a few years for them to realize,

944
00:52:58,440 --> 00:53:01,000
Speaker 6:  oh this is just fitness and health, right? Like we can stack out their stuff

945
00:53:01,000 --> 00:53:04,080
Speaker 6:  on top of it, but fundamentally what people really want is a better Fitbit.

946
00:53:04,080 --> 00:53:07,440
Speaker 6:  So they just built a better Fitbit and now here we are. But it, they were

947
00:53:07,440 --> 00:53:11,320
Speaker 6:  right to your point in Eli that like what people wanted was on their

948
00:53:11,320 --> 00:53:13,760
Speaker 6:  wrist, what they wanted on their wrist, apple was wrong about, but they were

949
00:53:13,760 --> 00:53:17,720
Speaker 6:  like, we can, we can focus on this space and figure it out. What is not at

950
00:53:17,720 --> 00:53:20,880
Speaker 6:  all clear is if people want stuff on their face Yeah.

951
00:53:21,740 --> 00:53:22,880
Speaker 5:  For any amount of time.

952
00:53:22,880 --> 00:53:24,640
Speaker 6:  Like not even a little bit clearer. Yeah.

953
00:53:25,030 --> 00:53:28,440
Speaker 5:  Back in the watch days I had this, I like a formula

954
00:53:29,180 --> 00:53:33,000
Speaker 5:  for thinking about it, which is like the more it touches your body, the less

955
00:53:33,000 --> 00:53:36,000
Speaker 5:  you, the more value it has to provide and the less you should care for it.

956
00:53:36,100 --> 00:53:39,240
Speaker 5:  And you can just like, that's a good monkey with the variables. And so my,

957
00:53:39,650 --> 00:53:43,560
Speaker 5:  my best example was glasses, right? Glasses are pretty finicky. You have

958
00:53:43,760 --> 00:53:46,440
Speaker 5:  to care for them. You can't break them If you break them like you're really

959
00:53:46,440 --> 00:53:50,040
Speaker 5:  screwed. Like I do not drive anywhere. I worry about losing my contact lenses

960
00:53:50,040 --> 00:53:52,600
Speaker 5:  all the time. Like right? So there's a lot of care in there on your body,

961
00:53:52,600 --> 00:53:54,960
Speaker 5:  but the amount of value they provide to you is immense. So you just like

962
00:53:54,960 --> 00:53:58,120
Speaker 5:  put up with it and you don't think about it too much. And then there's like

963
00:53:58,330 --> 00:54:02,160
Speaker 5:  clothes where depending on your relationship to clothes, if you're the swag

964
00:54:02,160 --> 00:54:05,680
Speaker 5:  pope or you're just like me, you know, there's a big spectrum of how much

965
00:54:05,680 --> 00:54:08,280
Speaker 5:  you might care about your clothes. We actually care for them a lot, but they

966
00:54:08,280 --> 00:54:11,200
Speaker 5:  provide value. They make you feel good about your solid stuff. And the watch

967
00:54:11,200 --> 00:54:14,320
Speaker 5:  was like, it was wrong and a bunch of the variables in the beginning, right?

968
00:54:14,320 --> 00:54:17,640
Speaker 5:  It didn't, wasn't useful enough. It was like kind of hard to use you to care

969
00:54:17,640 --> 00:54:20,200
Speaker 5:  for it a lot. You charge battery every day and it was like on your wrist

970
00:54:20,200 --> 00:54:23,160
Speaker 5:  and like they just got it wrong in the beginning. Like they really increased

971
00:54:23,160 --> 00:54:26,760
Speaker 5:  the value and helped fitness. They made it easier to care for, right? The

972
00:54:26,760 --> 00:54:30,680
Speaker 5:  batteries started lasting longer and like they, they aligned that

973
00:54:31,160 --> 00:54:34,560
Speaker 5:  equation such that the watch is like a really good pro. Like I really like

974
00:54:34,560 --> 00:54:37,880
Speaker 5:  my Apple Watch Ultra. Do I worry that everyone can read every notification

975
00:54:37,880 --> 00:54:40,840
Speaker 5:  that I ever read from like a hundred meters away like I do, but like I still

976
00:54:40,840 --> 00:54:44,680
Speaker 5:  love it. The face computer zone, those equations are just completely

977
00:54:44,680 --> 00:54:48,480
Speaker 5:  outta whack, right? They're like almost all universally hard to use. You

978
00:54:48,480 --> 00:54:52,400
Speaker 5:  have to care for them a lot. You have to put them on your face and they like

979
00:54:52,400 --> 00:54:55,360
Speaker 5:  don't do anything. The best thing that you can do in them is play games.

980
00:54:55,500 --> 00:54:59,440
Speaker 5:  And like I, again, if you are a Grando person, go

981
00:54:59,440 --> 00:55:03,360
Speaker 5:  spend all over the money on Psvr two and play grand. Like it is great.

982
00:55:03,360 --> 00:55:06,720
Speaker 5:  It is an incredible experience, but that's it.

983
00:55:06,980 --> 00:55:10,920
Speaker 5:  You know, like none of the other stuff is quite there yet. And

984
00:55:10,920 --> 00:55:14,760
Speaker 5:  I like, I just, I I should, we should come back to this equation around headset

985
00:55:14,760 --> 00:55:17,680
Speaker 5:  time and like try to lay it out. Yeah. If people have thoughts on like what

986
00:55:17,680 --> 00:55:20,920
Speaker 5:  that equation is like, what the variables are, but like it's a really helpful

987
00:55:20,920 --> 00:55:24,880
Speaker 5:  way of understanding like the second you strap it to your body, like

988
00:55:24,880 --> 00:55:28,080
Speaker 5:  you sort of have to, the amount of value you have to deliver starts going

989
00:55:28,080 --> 00:55:29,440
Speaker 5:  up in a way that is just enormous.

990
00:55:29,470 --> 00:55:33,040
Speaker 6:  Yeah. Yeah. Well and I think one of the interesting things about

991
00:55:33,220 --> 00:55:36,800
Speaker 6:  all of this is that I think Apple has been so

992
00:55:36,800 --> 00:55:40,200
Speaker 6:  successful that it's gonna have a really hard time launching this thing

993
00:55:40,510 --> 00:55:44,440
Speaker 6:  even as it knows and will say as many times as it can. This

994
00:55:44,440 --> 00:55:48,360
Speaker 6:  thing is $3,000, it's new, we're still learning, it's an

995
00:55:48,360 --> 00:55:51,920
Speaker 6:  experiment, whatever. It's like Apple doesn't get to do that

996
00:55:52,050 --> 00:55:54,760
Speaker 6:  anymore, right? Like when was the last time Apple launched something that

997
00:55:54,760 --> 00:55:57,920
Speaker 6:  they didn't claim was gonna like change the course of your life in history

998
00:55:58,240 --> 00:56:01,880
Speaker 5:  Apple TV see drives like is hobby and they're still kinda like,

999
00:56:01,900 --> 00:56:03,960
Speaker 5:  ugh. Yeah.

1000
00:56:04,030 --> 00:56:07,600
Speaker 6:  I mean that's, that's a really good example. And I would say if the, if

1001
00:56:07,600 --> 00:56:10,440
Speaker 6:  Apples mixed reality plans go the way of Apple tv,

1002
00:56:11,280 --> 00:56:12,720
Speaker 6:  that's, that's a real rough look.

1003
00:56:13,190 --> 00:56:17,000
Speaker 5:  Yeah. They are not allowed to say that this is a hobby. No. Even though absolutely

1004
00:56:17,000 --> 00:56:19,280
Speaker 6:  Not. I don't, I don't think it's a hobby. I think it's as so much as it is

1005
00:56:19,280 --> 00:56:22,040
Speaker 6:  like this is a science experiment that they're shipping to you, right? They're

1006
00:56:22,040 --> 00:56:24,880
Speaker 6:  like, some people will buy this, this, it'll be good for some things. But

1007
00:56:24,880 --> 00:56:28,320
Speaker 6:  Tim Cook is gonna get up and he's gonna talk about the reality dial,

1008
00:56:28,360 --> 00:56:32,320
Speaker 6:  which is an unbelievable name for a thing. It's how you control how

1009
00:56:32,320 --> 00:56:35,760
Speaker 6:  much of the world you can see versus how much of a VR thing. And he's gonna

1010
00:56:35,820 --> 00:56:38,920
Speaker 6:  say things about it being a revolutionary input mechanism on par with the

1011
00:56:38,920 --> 00:56:42,480
Speaker 6:  mouse and the touchscreen. Like it's, this is just what Apple does. And I

1012
00:56:42,480 --> 00:56:46,320
Speaker 6:  have such a hard time imagining any other pitch and if

1013
00:56:46,320 --> 00:56:49,680
Speaker 6:  they make the classic Apple pitch, I think there's a pretty good chance this

1014
00:56:49,680 --> 00:56:51,320
Speaker 6:  ends up in a really weird place.

1015
00:56:51,390 --> 00:56:55,080
Speaker 5:  Yeah. I don't think that they can, I even the fact that we're hearing this

1016
00:56:55,080 --> 00:56:58,920
Speaker 5:  much dissent now in their reporting is like I, they have to know they

1017
00:56:58,920 --> 00:56:59,760
Speaker 5:  can't make the, like it

1018
00:56:59,760 --> 00:57:02,760
Speaker 6:  Does make you wonder if it's strategic. If there's like, if they're like,

1019
00:57:02,760 --> 00:57:06,280
Speaker 6:  oh you think this is dumb, let me give you Trip Michael's phone number.

1020
00:57:08,150 --> 00:57:12,080
Speaker 5:  I dunno about that. That's very unlike them. But it No, that's like three

1021
00:57:12,080 --> 00:57:15,880
Speaker 5:  takes too clever, you know, like let's undercut ourselves before. But I do

1022
00:57:15,880 --> 00:57:19,320
Speaker 5:  think it's reflective of the volume of dissent inside the company. Yeah.

1023
00:57:19,320 --> 00:57:23,200
Speaker 5:  Like go back, I said this so many times, but go back and watch

1024
00:57:23,540 --> 00:57:27,200
Speaker 5:  the Apple Watch introduction. They were so high in their own

1025
00:57:27,200 --> 00:57:30,080
Speaker 5:  fumes. Oh yeah, yeah, right. Like Bono was there,

1026
00:57:31,850 --> 00:57:34,760
Speaker 5:  do you remember this saying like, Christie Turton was there to talk about

1027
00:57:34,760 --> 00:57:38,600
Speaker 5:  running marathons in Africa. Like it was crazy. And

1028
00:57:38,600 --> 00:57:41,920
Speaker 5:  they, they, I'm not saying this product is not successful again, I really

1029
00:57:41,920 --> 00:57:45,520
Speaker 5:  like mine. I'm just saying like their belief in

1030
00:57:45,520 --> 00:57:49,480
Speaker 5:  could they just reassemble the pieces of past success. It

1031
00:57:49,480 --> 00:57:52,360
Speaker 5:  was like a Marvel phase three movie. They're like, remember the Avengers,

1032
00:57:52,960 --> 00:57:56,520
Speaker 5:  here's that movie again. But like with different worse actors

1033
00:57:57,590 --> 00:58:01,520
Speaker 5:  like, and like they, they cannot

1034
00:58:01,520 --> 00:58:05,480
Speaker 5:  do that again. And because this product is, it can't support

1035
00:58:05,480 --> 00:58:08,520
Speaker 5:  the weight of those expectations. The watch could not support the weight

1036
00:58:08,520 --> 00:58:08,760
Speaker 5:  of those

1037
00:58:08,760 --> 00:58:11,840
Speaker 6:  Expectations. But on the flip side, what Apple needs desperately to do is

1038
00:58:12,120 --> 00:58:14,720
Speaker 6:  convince people that this is the future. And especially right now we're in

1039
00:58:14,720 --> 00:58:18,440
Speaker 6:  this moment where I think a lot of people are souring on the whole idea of

1040
00:58:18,440 --> 00:58:22,400
Speaker 6:  the metaverse and you know, ar and VR as the next big thing. And

1041
00:58:22,400 --> 00:58:25,600
Speaker 6:  we're seeing all these companies like, didn't Disney just lay off its whole

1042
00:58:25,600 --> 00:58:26,720
Speaker 6:  Metaverse division got

1043
00:58:26,720 --> 00:58:27,240
Speaker 7:  Rid of the whole thing.

1044
00:58:27,350 --> 00:58:31,240
Speaker 6:  Yeah. Like that. That's not nothing. Right. And so now the, the weight of

1045
00:58:31,240 --> 00:58:35,160
Speaker 6:  all this has kind of always been and feels even more so on Apple to

1046
00:58:35,160 --> 00:58:38,520
Speaker 6:  say this, this is the thing, right? Like this is where we are headed.

1047
00:58:38,520 --> 00:58:41,680
Speaker 6:  Developers come build for this content, people come make content for this.

1048
00:58:41,690 --> 00:58:45,640
Speaker 6:  So Apple, apple kind of has to and can't sandbag

1049
00:58:45,640 --> 00:58:46,600
Speaker 6:  itself at the same time.

1050
00:58:46,600 --> 00:58:50,240
Speaker 7:  Yeah. They're, they're the same position meta's in where they've

1051
00:58:50,240 --> 00:58:54,000
Speaker 7:  invested too much time and energy into this thing to give

1052
00:58:54,000 --> 00:58:57,880
Speaker 7:  up. They just have to like close their eyes and keep going and hope

1053
00:58:57,880 --> 00:59:01,160
Speaker 7:  it turns out fine. Probably not. Cuz that seems like it'd be bad for their

1054
00:59:01,160 --> 00:59:01,920
Speaker 7:  bottom line. But,

1055
00:59:01,940 --> 00:59:04,360
Speaker 5:  But what is the pressure to do this? I

1056
00:59:04,360 --> 00:59:07,960
Speaker 7:  Think it's because they, they've put so much time and effort into it already.

1057
00:59:08,110 --> 00:59:08,880
Speaker 7:  Like I think

1058
00:59:08,880 --> 00:59:11,920
Speaker 5:  They, they they have started and stopped a car company

1059
00:59:12,360 --> 00:59:12,920
Speaker 5:  40

1060
00:59:12,920 --> 00:59:13,840
Speaker 7:  Times. Yeah,

1061
00:59:14,000 --> 00:59:16,840
Speaker 5:  Yeah. Right. Like they don't, they're, they've

1062
00:59:16,880 --> 00:59:19,680
Speaker 7:  Actually done twice in the recording of this podcast by the way.

1063
00:59:19,710 --> 00:59:23,200
Speaker 5:  Yeah. Project Titan has been shut down and restarted twice.

1064
00:59:23,390 --> 00:59:27,000
Speaker 5:  Doug Field has gone from Ford to Apple and Tesla five more times.

1065
00:59:27,970 --> 00:59:31,720
Speaker 5:  No, I'm just like for real. Like the yeah, like what is the pressure?

1066
00:59:31,720 --> 00:59:32,920
Speaker 5:  It's not meta beating them.

1067
00:59:32,950 --> 00:59:36,800
Speaker 7:  Like I think it's this grand desire to have the next iPhone to have the

1068
00:59:36,800 --> 00:59:40,640
Speaker 7:  next hardware device that can drive computing forward the same

1069
00:59:40,640 --> 00:59:44,280
Speaker 7:  way the iPhone did. Right? Like the iPhone was a c change. It it, it

1070
00:59:44,280 --> 00:59:48,120
Speaker 7:  changed how, how we use our devices. There's a whole generation of people

1071
00:59:48,120 --> 00:59:51,600
Speaker 7:  who don't know how to search files on their computer because they've only

1072
00:59:51,600 --> 00:59:54,000
Speaker 7:  used their smartphone all this time and

1073
00:59:54,110 --> 00:59:55,320
Speaker 5:  I told you this would happen.

1074
00:59:55,380 --> 00:59:58,520
Speaker 7:  You, you like totally vindicated and

1075
00:59:58,860 --> 01:00:00,560
Speaker 5:  The kids are so confused.

1076
01:00:01,010 --> 01:00:04,440
Speaker 7:  So confused. Do not tell them what a file tree is.

1077
01:00:04,880 --> 01:00:08,840
Speaker 7:  They will not know. But I like that was, that was like

1078
01:00:08,840 --> 01:00:12,360
Speaker 7:  the, the major impetus behind this, right? Like they wanted to find the next

1079
01:00:12,360 --> 01:00:16,280
Speaker 7:  iPhone and then they've invested in it. And then Silicon Valley as we've

1080
01:00:16,280 --> 01:00:20,200
Speaker 7:  seen is really prone to groupthink and I think it kind of, I

1081
01:00:20,200 --> 01:00:23,880
Speaker 7:  don't wanna say infected, but it did it like infected Silicon Valley. Everybody

1082
01:00:23,880 --> 01:00:27,760
Speaker 7:  was like we have to have the next mixed reality thing. And then it just so

1083
01:00:27,800 --> 01:00:30,520
Speaker 7:  happened that Tim Cook and Mark Zuckerberg were two of the people totally

1084
01:00:30,520 --> 01:00:34,080
Speaker 7:  convinced by that. And now we're going off on this like

1085
01:00:34,480 --> 01:00:38,240
Speaker 7:  weird path into spacial computing when we're not quite ready

1086
01:00:38,240 --> 01:00:39,720
Speaker 7:  for it yet. That's

1087
01:00:39,720 --> 01:00:43,600
Speaker 6:  The thing, you know what's the move is Tim Cook should come out at WW C

1088
01:00:43,600 --> 01:00:46,720
Speaker 6:  and be like, look we thought this was the moment we thought it was gonna

1089
01:00:46,720 --> 01:00:50,480
Speaker 6:  happen. It's not, we're not the world we're

1090
01:00:50,480 --> 01:00:53,360
Speaker 6:  this isn't ready, it's not done. Cuz I think if, if you rewind a couple years

1091
01:00:53,360 --> 01:00:56,480
Speaker 6:  ago when we first started hearing about this, it was very clear to everyone

1092
01:00:56,670 --> 01:01:00,120
Speaker 6:  that by now every other company on earth was going to have made a headset,

1093
01:01:00,120 --> 01:01:03,160
Speaker 6:  right? Yes. And we were gonna, in this giant race, Google was gonna be out

1094
01:01:03,160 --> 01:01:06,720
Speaker 6:  there, HTC was building stuff, HoloLens was gonna be huge. There was this

1095
01:01:06,720 --> 01:01:10,640
Speaker 6:  big race and now it is the opposite. It's like we're back in the phase

1096
01:01:10,640 --> 01:01:14,400
Speaker 6:  of like, is this actually anything? And it feels like Apple

1097
01:01:14,690 --> 01:01:18,440
Speaker 6:  is about to finish a race. It's kind of not needing to

1098
01:01:18,440 --> 01:01:19,680
Speaker 6:  run at the moment. And

1099
01:01:19,680 --> 01:01:22,560
Speaker 5:  Yeah, that's what I mean. Like what is the wouldn't it be better if Apple

1100
01:01:22,560 --> 01:01:24,960
Speaker 5:  was like we've built generative AI tools into iOS.

1101
01:01:25,030 --> 01:01:28,600
Speaker 6:  I think at this point it can, yeah for a lot of reasons.

1102
01:01:28,600 --> 01:01:32,000
Speaker 6:  Presumably like production and all that stuff. But even just like from a

1103
01:01:32,000 --> 01:01:35,720
Speaker 6:  sort of pers like the world's perspective of Apple thing, like

1104
01:01:35,720 --> 01:01:38,960
Speaker 6:  there's too much out there now. Like the perception is already that this

1105
01:01:38,960 --> 01:01:42,880
Speaker 6:  thing is coming late and it's been a problem and it kind of like,

1106
01:01:43,110 --> 01:01:45,960
Speaker 6:  I believe it needs to do what it did with air power, which is be like nevermind

1107
01:01:45,960 --> 01:01:49,800
Speaker 6:  this doesn't work, we'll come back to you later. Or it has to ship

1108
01:01:49,800 --> 01:01:53,560
Speaker 6:  the thing and just like bully everyone into believing in it

1109
01:01:53,560 --> 01:01:56,240
Speaker 6:  until there's actually something good here. Like I don't, I don't necessarily

1110
01:01:56,240 --> 01:01:58,680
Speaker 6:  know that it has another move at this point. Even though I think it would

1111
01:01:58,680 --> 01:02:01,280
Speaker 6:  be badass if Tim Cook came out and he was like, look we built this headset,

1112
01:02:01,390 --> 01:02:05,160
Speaker 6:  it's rad. I don't wanna talk about it cuz it's not done yet. Like that would

1113
01:02:05,160 --> 01:02:05,560
Speaker 6:  be amazing.

1114
01:02:05,920 --> 01:02:07,160
Speaker 7:  Chuck it off stage.

1115
01:02:07,160 --> 01:02:09,840
Speaker 5:  When I think about things Tim Cook is likely to

1116
01:02:09,840 --> 01:02:10,040
Speaker 6:  Do, listen,

1117
01:02:10,040 --> 01:02:13,640
Speaker 5:  I'm not saying it's likely canceling a product live on stage. Yes.

1118
01:02:13,640 --> 01:02:16,200
Speaker 5:  After showing it to you that's high on the list.

1119
01:02:16,610 --> 01:02:17,480
Speaker 6:  Be badass though.

1120
01:02:17,800 --> 01:02:18,560
Speaker 7:  It'd be really cool.

1121
01:02:19,110 --> 01:02:22,040
Speaker 5:  He's like, all right Jeff, take over. I'm out.

1122
01:02:23,490 --> 01:02:26,600
Speaker 6:  He just sits down, puts it on and just kicks back on stage.

1123
01:02:26,860 --> 01:02:29,720
Speaker 5:  I'm just gonna read the rest of the Apple things and we're gonna take a break,

1124
01:02:29,720 --> 01:02:33,320
Speaker 5:  do a lighting round. iOS 16.4 is here, it has a bunch of new emoji.

1125
01:02:33,500 --> 01:02:37,320
Speaker 5:  Importantly for the six people who care, Google P now

1126
01:02:37,320 --> 01:02:39,880
Speaker 5:  supports 5G on the iPhone as of 16.4

1127
01:02:40,050 --> 01:02:42,320
Speaker 6:  To you as one of those six people. Congratulations.

1128
01:02:42,410 --> 01:02:45,280
Speaker 5:  It was great. I mean now it's, it's basically now I have at and t and T-Mobile

1129
01:02:45,280 --> 01:02:47,960
Speaker 5:  on my phone but I can turn the T-Mobile on and off whenever I want. I love

1130
01:02:47,960 --> 01:02:50,240
Speaker 5:  that. Which is actually really useful if you're the sort of person who like

1131
01:02:50,440 --> 01:02:53,160
Speaker 5:  constantly has phones like floating through your life, right? So very cool.

1132
01:02:53,160 --> 01:02:56,600
Speaker 5:  Thank you finally for solving this extremely stupid problem. I love it.

1133
01:02:57,240 --> 01:03:01,080
Speaker 5:  Also the new architecture for the home app back again for round two. We'll

1134
01:03:01,080 --> 01:03:04,600
Speaker 5:  see you guys are gonna have Jen on the Wednesday show to talk about sidewalk.

1135
01:03:04,600 --> 01:03:06,240
Speaker 5:  You should talk about what's going on with Matter too.

1136
01:03:06,370 --> 01:03:10,240
Speaker 6:  We are, that's we, we teased that this week because we got, we got a

1137
01:03:10,240 --> 01:03:12,880
Speaker 6:  hotline question that was like a bunch of other stuff and then it was like

1138
01:03:12,880 --> 01:03:15,200
Speaker 6:  what's going on with Matter? And I was like we can't do this here Jen. Like

1139
01:03:15,200 --> 01:03:17,760
Speaker 6:  we have to not but we're gonna do it next week so stick around.

1140
01:03:17,760 --> 01:03:21,520
Speaker 5:  Yeah because Amazon launched its sidewalk, low range IOT

1141
01:03:21,520 --> 01:03:25,280
Speaker 5:  network that's like every Alexa is now a little little wireless node.

1142
01:03:25,910 --> 01:03:29,760
Speaker 5:  It's very cool. But the new architecture and

1143
01:03:29,760 --> 01:03:33,320
Speaker 5:  home is out to so little smartphone, little smart home stuff going on in

1144
01:03:33,320 --> 01:03:36,640
Speaker 5:  the background there. Apple Buy now pay later is finally launching, which

1145
01:03:36,640 --> 01:03:40,320
Speaker 5:  took them forever. It seems like that moment of

1146
01:03:40,320 --> 01:03:42,920
Speaker 5:  unrestrained pandemic spending might be

1147
01:03:42,920 --> 01:03:43,840
Speaker 6:  Over. Yes.

1148
01:03:44,930 --> 01:03:45,760
Speaker 5:  So whatever.

1149
01:03:46,080 --> 01:03:48,960
Speaker 6:  And then And also the whole buy now pay later industry might be over.

1150
01:03:48,960 --> 01:03:49,640
Speaker 5:  Yeah that

1151
01:03:51,380 --> 01:03:54,080
Speaker 5:  and then Apple Music Classical also come

1152
01:03:54,080 --> 01:03:57,040
Speaker 6:  Out. Yeah. Wait, crayons. I just wanna know your thoughts on classical before

1153
01:03:57,040 --> 01:04:00,040
Speaker 6:  we take a break because Yeah, I a lot of enthusiasm about this

1154
01:04:00,040 --> 01:04:03,760
Speaker 7:  App. I've been using it since it came out. I don't consider

1155
01:04:03,760 --> 01:04:06,400
Speaker 7:  myself a classical music fan, but then when we were all talking about it,

1156
01:04:06,400 --> 01:04:08,320
Speaker 7:  I guess I am one who knew.

1157
01:04:08,430 --> 01:04:10,840
Speaker 6:  Yeah. You like classical music name five composers.

1158
01:04:11,830 --> 01:04:14,960
Speaker 7:  I can, but I'm not gonna do that because I don't trust my pronunciation of

1159
01:04:14,960 --> 01:04:16,000
Speaker 7:  their names. That's,

1160
01:04:16,070 --> 01:04:17,480
Speaker 6:  That was a trick. Well

1161
01:04:17,550 --> 01:04:21,320
Speaker 7:  I really, really like the app. I wish the regular Apple Music

1162
01:04:21,320 --> 01:04:24,960
Speaker 7:  app was actually this just because I like all the different

1163
01:04:25,160 --> 01:04:28,520
Speaker 7:  metadata. I like that I can just look things up by composer and not have

1164
01:04:28,520 --> 01:04:31,280
Speaker 7:  a bunch of garbage that I don't care about. Like every time I look up Steven

1165
01:04:31,280 --> 01:04:34,880
Speaker 7:  Sondheim in the regular app, it's like do you wanna listen to Steven

1166
01:04:34,880 --> 01:04:38,440
Speaker 7:  Sondheim talk? And I'm like no, I just wanna see a list of

1167
01:04:38,440 --> 01:04:42,240
Speaker 7:  musicals he wrote. And I can do that with Apple classical

1168
01:04:42,240 --> 01:04:46,000
Speaker 7:  music. Not with Sondheim but with lots of other people like Gershwin and

1169
01:04:46,000 --> 01:04:49,800
Speaker 7:  and Rachmaninov. And I'm so close to naming a few more for you

1170
01:04:49,800 --> 01:04:52,760
Speaker 7:  David, but I'm not gonna do it. You'll just have to stay tuned. I'm writing

1171
01:04:52,760 --> 01:04:54,160
Speaker 7:  about it on the website. It's gonna be great.

1172
01:04:54,160 --> 01:04:57,480
Speaker 5:  Yeah, I would say it's like a pure strategy credit to Apple, right? Like

1173
01:04:57,670 --> 01:05:01,440
Speaker 5:  they don't need Apple music to make money. It's just

1174
01:05:01,680 --> 01:05:05,600
Speaker 5:  like not what it does for them. So they can spit out another app and be like,

1175
01:05:05,600 --> 01:05:08,120
Speaker 5:  look classical music lovers, we made you a thing. Like don't you love it?

1176
01:05:08,120 --> 01:05:11,720
Speaker 5:  And like se target ever smaller segments of music lovers

1177
01:05:12,050 --> 01:05:15,520
Speaker 5:  in a way that like Spotify kind of can't cuz it can't spread all the costs.

1178
01:05:15,550 --> 01:05:15,960
Speaker 7:  I cannot

1179
01:05:15,960 --> 01:05:19,680
Speaker 5:  Wait. So that's cool. And then Apple is really high on spatial for

1180
01:05:19,680 --> 01:05:22,320
Speaker 5:  classical as far as I can tell, where it actually makes sense.

1181
01:05:22,450 --> 01:05:24,120
Speaker 7:  It sounds fine

1182
01:05:24,120 --> 01:05:27,200
Speaker 5:  Because you have like an orchestra, you know all around you.

1183
01:05:27,470 --> 01:05:30,680
Speaker 7:  It's fine. I'd rather just listen on a really nice pair of headphones

1184
01:05:31,190 --> 01:05:35,160
Speaker 7:  than like spatial audio. But everything about it, I'm

1185
01:05:35,160 --> 01:05:39,120
Speaker 7:  really, really digging. I've had way, way, way too much fun with it

1186
01:05:39,120 --> 01:05:43,080
Speaker 7:  this week and like forgot how much I liked classical music. So

1187
01:05:43,080 --> 01:05:44,200
Speaker 7:  it's a nice pre-discovery.

1188
01:05:44,590 --> 01:05:47,760
Speaker 5:  I should mention here by the way, Chris Welch reviewed the Sonos era 100

1189
01:05:47,760 --> 01:05:51,720
Speaker 5:  and the Sonos era 300 era 100. Just a, a stereo speaker. It's

1190
01:05:51,720 --> 01:05:55,400
Speaker 5:  great. He likes it says it's superior to the Sonos one era 300,

1191
01:05:55,700 --> 01:05:59,640
Speaker 5:  the only other real standalone spatial speaker apart from

1192
01:05:59,640 --> 01:06:03,360
Speaker 5:  the HomePod and whatever Amazon thing. He said it's great, it's spatial.

1193
01:06:03,360 --> 01:06:07,080
Speaker 5:  Was it for spatial not as great on stereo tracks.

1194
01:06:07,080 --> 01:06:10,800
Speaker 5:  This satisfy, which is a real, it's a real trade off in there.

1195
01:06:10,800 --> 01:06:14,200
Speaker 5:  Yeah. But one of the reasons that I think Apples so high on

1196
01:06:14,560 --> 01:06:18,280
Speaker 5:  classical for spatial is cuz they can do the thing where the guy sneaks up

1197
01:06:18,280 --> 01:06:19,680
Speaker 5:  behind you with a symbol and goes crash.

1198
01:06:19,680 --> 01:06:23,600
Speaker 7:  It's gonna be the OBO player slowly coming up behind

1199
01:06:23,600 --> 01:06:23,960
Speaker 7:  you.

1200
01:06:25,600 --> 01:06:28,280
Speaker 5:  Go over those reviews. Chris did a great job. There's just a

1201
01:06:28,280 --> 01:06:31,320
Speaker 6:  Lot I'm very excited about the Arrow 100. Very excited about the Arrow 100.

1202
01:06:31,380 --> 01:06:34,920
Speaker 5:  I'm, I'm thinking I might buy a 300 just to like, I feel like I need to give

1203
01:06:34,920 --> 01:06:38,800
Speaker 5:  spatial a shot. I've been so down on it for so long and now

1204
01:06:38,960 --> 01:06:42,640
Speaker 5:  everyone's like the new mixes are good and I'm just like, eh, like

1205
01:06:43,160 --> 01:06:45,480
Speaker 5:  maybe adding some new hardware to the mix

1206
01:06:45,480 --> 01:06:49,280
Speaker 6:  Makes when, when Mariah Carey does a spatial audio mix of, of

1207
01:06:49,280 --> 01:06:52,320
Speaker 6:  baby, then Eli will buy an arrow three the

1208
01:06:52,320 --> 01:06:53,480
Speaker 7:  Way ne Eli's face changed.

1209
01:06:53,570 --> 01:06:56,760
Speaker 5:  If Mariah Carey would sneak up behind me, I'd be so happy.

1210
01:06:57,310 --> 01:07:00,960
Speaker 5:  Mariah, I dunno if you're listening, if anyone has a radio telescope point

1211
01:07:00,960 --> 01:07:01,520
Speaker 5:  at Mariah

1212
01:07:01,520 --> 01:07:02,000
Speaker 7:  Carey,

1213
01:07:03,750 --> 01:07:06,400
Speaker 5:  I love you. I loved you my whole life.

1214
01:07:07,380 --> 01:07:11,160
Speaker 5:  All right, we gotta take a break. If you see the swag up

1215
01:07:11,160 --> 01:07:13,240
Speaker 5:  Pope, let Mariah Carey know I love her.

1216
01:07:14,270 --> 01:07:16,800
Speaker 6:  Tell her to get away before doom happens.

1217
01:07:18,180 --> 01:07:22,040
Speaker 5:  She needs to know we gotta take a break. We'll be right

1218
01:07:22,040 --> 01:07:22,840
Speaker 5:  back in the lightning round

1219
01:07:27,580 --> 01:07:31,450
Speaker 10:  As if the mc crispy couldn't get any better. Bacon and ranch

1220
01:07:31,560 --> 01:07:35,370
Speaker 10:  just entered the chat. The Bacon Ranch Mc Crispy

1221
01:07:35,450 --> 01:07:38,810
Speaker 10:  available at participating McDonald's for a limited time.

1222
01:07:42,640 --> 01:07:46,370
Speaker 11:  Well folks, here we are. Former president Donald Trump

1223
01:07:46,470 --> 01:07:50,370
Speaker 11:  has been indicted by a Manhattan grand jury. I'm Preet

1224
01:07:50,370 --> 01:07:53,930
Speaker 11:  Barra, the former US attorney in Manhattan. My podcast

1225
01:07:54,120 --> 01:07:57,890
Speaker 11:  stay tuned is about law, justice, power and democracy.

1226
01:07:58,930 --> 01:08:02,290
Speaker 11:  Recently I broke down the indictment with a group of former federal prosecutors

1227
01:08:02,510 --> 01:08:05,370
Speaker 11:  who understand how the justice system really works.

1228
01:08:05,610 --> 01:08:08,970
Speaker 11:  Joyce Vance, Barb McQuaid and Ellie Honig.

1229
01:08:09,340 --> 01:08:12,250
Speaker 11:  We discuss the questions on everyone's mind. Like,

1230
01:08:12,710 --> 01:08:16,320
Speaker 12:  Can you directly tie Donald Trump to the way these

1231
01:08:16,320 --> 01:08:17,640
Speaker 12:  payments were booked and

1232
01:08:17,640 --> 01:08:21,280
Speaker 13:  Logged? Are prosecutors considering additional defendants or

1233
01:08:21,280 --> 01:08:22,240
Speaker 13:  additional charges?

1234
01:08:22,450 --> 01:08:25,920
Speaker 11:  Is this the kind of conduct that merits a charge of a former president of

1235
01:08:25,920 --> 01:08:27,080
Speaker 11:  the United States? I

1236
01:08:27,080 --> 01:08:29,960
Speaker 14:  Think this is a serious crime pre and I think it's one that I would charge.

1237
01:08:30,380 --> 01:08:31,680
Speaker 11:  And where do we go from here?

1238
01:08:32,100 --> 01:08:35,880
Speaker 13:  The presidency from prison, right? I mean add to the crazy.

1239
01:08:36,440 --> 01:08:40,200
Speaker 11:  Add to the crazy to listen. Just search. Stay tuned

1240
01:08:40,400 --> 01:08:43,720
Speaker 11:  wherever you get your podcasts. New episodes drop every Thursday.

1241
01:08:45,430 --> 01:08:46,320
Speaker 11:  Stay tuned.

1242
01:08:51,450 --> 01:08:54,390
Speaker 5:  All right, we're back. We gotta do a lightning round. There's a lot of news

1243
01:08:54,390 --> 01:08:57,870
Speaker 5:  this week. A lot of news. Just a lot of things. Yeah. Franz, why don't you

1244
01:08:57,870 --> 01:08:58,030
Speaker 5:  start.

1245
01:08:58,210 --> 01:09:02,030
Speaker 7:  All right. My big thing this week is Ike Perlmutter is no longer at

1246
01:09:02,030 --> 01:09:05,590
Speaker 7:  Disney. He was the guy who owned Marvel

1247
01:09:05,690 --> 01:09:09,630
Speaker 7:  for years and years and years. When Marvel got acquired by Disney,

1248
01:09:09,840 --> 01:09:13,350
Speaker 7:  he stayed on. He notoriously was like, people don't want

1249
01:09:13,670 --> 01:09:17,510
Speaker 7:  superhero movies with girls in them. We don't need toys with girls

1250
01:09:17,510 --> 01:09:18,590
Speaker 7:  on 'em. That's dumb.

1251
01:09:18,590 --> 01:09:20,830
Speaker 5:  That was a perfect impression by the way. A plus

1252
01:09:20,830 --> 01:09:24,590
Speaker 7:  Impression. Thank you. I worked really, really hard on it. He had to be slowly

1253
01:09:24,590 --> 01:09:28,430
Speaker 7:  forced out because Kevin Faggy and him just didn't get along. And like he

1254
01:09:28,430 --> 01:09:31,830
Speaker 7:  was trying to make Black Panther and Captain Marvel and Perlmutter was like,

1255
01:09:31,830 --> 01:09:35,350
Speaker 7:  no, we only want white guys. That's, that's the only thing people

1256
01:09:35,630 --> 01:09:39,110
Speaker 7:  will buy at a superhero movie. He was quickly proven wrong with both of those.

1257
01:09:39,280 --> 01:09:43,110
Speaker 7:  So it was kind of shocking that he was still at Disney and Marvel and now

1258
01:09:43,110 --> 01:09:45,670
Speaker 7:  he no longer is Goodbye Ike.

1259
01:09:45,670 --> 01:09:48,910
Speaker 5:  Yeah, he also tried to do a coup with an actor.

1260
01:09:48,920 --> 01:09:52,350
Speaker 7:  He did try to do a coup didn't go over well for him. So like

1261
01:09:53,040 --> 01:09:56,870
Speaker 7:  he, you know, it's, it's time he, he can go off and do other things and have

1262
01:09:56,870 --> 01:09:59,950
Speaker 7:  strong feelings about women and people of color on his own time.

1263
01:10:00,180 --> 01:10:04,110
Speaker 5:  Yeah. Can I tell you my favorite Disney story of the week? Yes. Do you,

1264
01:10:04,110 --> 01:10:07,190
Speaker 5:  do you, have you guys heard about the King Charles clause? No. Yes.

1265
01:10:07,200 --> 01:10:08,110
Speaker 7:  Oh that was so

1266
01:10:08,110 --> 01:10:11,270
Speaker 5:  Good. This is so good. So Governor DeSantis in Florida

1267
01:10:11,890 --> 01:10:15,630
Speaker 5:  mad about Disney. Don't say gay passes a bill to take away

1268
01:10:15,630 --> 01:10:19,350
Speaker 5:  Disney's like 50 or 60 year old weird tax exception

1269
01:10:19,670 --> 01:10:23,550
Speaker 5:  district. The REI Creek Improvement district. So like, Disney was like basically

1270
01:10:23,550 --> 01:10:26,790
Speaker 5:  a city, right? Yeah. In this like huge area. And they got to, they ran everything

1271
01:10:26,790 --> 01:10:30,110
Speaker 5:  and they were exempted from laws and like, it's weird, it's weird to give

1272
01:10:30,110 --> 01:10:33,590
Speaker 5:  corporations cities to America, but they had it, they had it for a long time.

1273
01:10:34,270 --> 01:10:37,190
Speaker 5:  Whatever Florida they come out against the, don't say gay bell in Florida.

1274
01:10:37,190 --> 01:10:40,870
Speaker 5:  DeSantis does his thing. Tries to punish them for it. So he passes law,

1275
01:10:40,960 --> 01:10:44,910
Speaker 5:  he replaces the people who run this district with like five of his own handpicked

1276
01:10:44,910 --> 01:10:48,510
Speaker 5:  people. It comes out yesterday day before, right

1277
01:10:48,510 --> 01:10:52,430
Speaker 5:  before those people took power, Disney along with like

1278
01:10:52,620 --> 01:10:55,550
Speaker 5:  Vanguard and BlackRock passed a bunch of

1279
01:10:56,150 --> 01:10:59,910
Speaker 5:  bills giving itself permanent power over the

1280
01:11:00,110 --> 01:11:03,550
Speaker 5:  district and completely disempowering the new board.

1281
01:11:03,550 --> 01:11:04,390
Speaker 7:  That's so

1282
01:11:04,390 --> 01:11:07,670
Speaker 5:  Good. And so the new board is literally like we have no power. Like all we

1283
01:11:07,670 --> 01:11:11,110
Speaker 5:  have is the power to maintain the roads in Disney is like

1284
01:11:11,340 --> 01:11:14,880
Speaker 5:  giving itself a 30 year development agreement. It no longer needs approval

1285
01:11:14,880 --> 01:11:18,760
Speaker 5:  to build buildings of any height or assign development rights. And

1286
01:11:18,760 --> 01:11:22,480
Speaker 5:  then it says if the court finds this agreement is

1287
01:11:22,560 --> 01:11:26,240
Speaker 5:  illegally in perpetuity, we will extend its term by the

1288
01:11:26,240 --> 01:11:30,040
Speaker 5:  following. This declaration shall continue in, in effect until 21

1289
01:11:30,040 --> 01:11:34,000
Speaker 5:  years after the death of the last survivor of the descendants of King

1290
01:11:34,000 --> 01:11:37,800
Speaker 5:  Charles iii, king of England living as of the date of this

1291
01:11:37,800 --> 01:11:41,640
Speaker 5:  declaration. So, and they did it all. And their

1292
01:11:41,640 --> 01:11:45,440
Speaker 5:  thing is like we held meetings, no one came, like the board held meetings

1293
01:11:45,440 --> 01:11:49,240
Speaker 5:  and we passed the resolutions and no one came or paid

1294
01:11:49,240 --> 01:11:52,000
Speaker 5:  attention or told Governor DeSantis. It

1295
01:11:52,000 --> 01:11:52,520
Speaker 7:  Was a real,

1296
01:11:52,530 --> 01:11:52,880
Speaker 5:  So

1297
01:11:52,880 --> 01:11:56,280
Speaker 7:  They Bart losing to Martin in the Simpsons

1298
01:11:56,280 --> 01:11:56,880
Speaker 7:  moment.

1299
01:11:57,190 --> 01:12:00,400
Speaker 5:  Yeah. Wow. I mean, and so the board is like, they,

1300
01:12:01,050 --> 01:12:04,800
Speaker 5:  we have to like sue Disney to get the power that we're supposed to have back.

1301
01:12:05,830 --> 01:12:09,680
Speaker 5:  It's really good. I mean it is like corporate trickery and it's

1302
01:12:09,680 --> 01:12:13,520
Speaker 5:  finest and it's like hard to root for. Yeah. But I'm like rooting for Disney

1303
01:12:13,520 --> 01:12:16,080
Speaker 5:  cuz it's fun. Like you, you pick the funnier one and you pick that

1304
01:12:16,080 --> 01:12:19,400
Speaker 6:  One. Yeah. It's like if, if succession had two more seasons, like that would

1305
01:12:19,400 --> 01:12:22,960
Speaker 6:  for sure be a plot line in one of the episodes. Yeah. That's fantastic.

1306
01:12:23,360 --> 01:12:26,240
Speaker 5:  Lemme just read this quote. This essentially makes Disney, the government,

1307
01:12:26,420 --> 01:12:30,280
Speaker 5:  Ron Perry, one of the new board members as appointed by DeSantis said the

1308
01:12:30,280 --> 01:12:33,640
Speaker 5:  board loses for all practical purposes, the majority of its ability to do

1309
01:12:33,840 --> 01:12:37,280
Speaker 5:  anything beyond maintain the roads and maintain basic infrastructure.

1310
01:12:37,910 --> 01:12:41,240
Speaker 5:  It's like, oh, they just wrote themselves a contract and signed it.

1311
01:12:41,950 --> 01:12:43,200
Speaker 6:  Said, does anyone object

1312
01:12:43,200 --> 01:12:46,400
Speaker 5:  To an empty room? And that was it because it's like so fundamentally

1313
01:12:46,400 --> 01:12:49,920
Speaker 5:  unserious no one paid any attention and they just got away with it. It's

1314
01:12:49,920 --> 01:12:53,880
Speaker 5:  really good. It's so good. That's like the i promo thing is funny in its

1315
01:12:53,880 --> 01:12:57,120
Speaker 5:  way cuz they just allowed like the cranky old man to stay at Disney. He just

1316
01:12:57,120 --> 01:13:01,080
Speaker 5:  had off, laid off after he tried to do a coup. This is like deeply

1317
01:13:01,270 --> 01:13:04,880
Speaker 5:  like you went up against Disney's lawyers and you thought they wouldn't

1318
01:13:04,900 --> 01:13:07,560
Speaker 5:  and they didn't and they were suspiciously quiet the whole time.

1319
01:13:07,840 --> 01:13:10,840
Speaker 7:  Yeah. They were just like, yeah, whatever. It's gonna be fine. It's very,

1320
01:13:10,840 --> 01:13:11,920
Speaker 7:  and it was and

1321
01:13:11,920 --> 01:13:14,520
Speaker 6:  They and they pinged it to King Charles King.

1322
01:13:14,520 --> 01:13:17,640
Speaker 5:  The king Charles Claus is like icing on the cake. Right. It's

1323
01:13:17,640 --> 01:13:21,000
Speaker 6:  Amazing. Yeah, it's very good. I have, I have no notes. That's perfect.

1324
01:13:21,760 --> 01:13:25,320
Speaker 6:  Mine is that Elon Musk is now the most followed person on Twitter,

1325
01:13:25,360 --> 01:13:29,280
Speaker 6:  which is like fine. Great. Congratulations Elon Musk. Everyone should

1326
01:13:29,280 --> 01:13:32,760
Speaker 6:  quit Twitter. It's a cesspool and a horrible place and it's only getting

1327
01:13:32,760 --> 01:13:33,800
Speaker 6:  worse and everyone should leave.

1328
01:13:34,320 --> 01:13:36,440
Speaker 5:  Did you listen to the Macon episode as decoder?

1329
01:13:36,440 --> 01:13:40,080
Speaker 6:  It's terrific. It's very, very good. Everyone should listen to it. Well

1330
01:13:40,080 --> 01:13:42,080
Speaker 5:  Thank you for complimenting me. Do you think Maidan is gonna

1331
01:13:42,080 --> 01:13:45,960
Speaker 6:  Survive? No, Maidan is not the answer. I think Macedon is gonna be fine,

1332
01:13:45,960 --> 01:13:49,840
Speaker 6:  but I think I, I have a story on activity pub that's coming in the next couple

1333
01:13:49,840 --> 01:13:52,960
Speaker 6:  of weeks because it's this, it's it's typical call everybody is betting on,

1334
01:13:52,960 --> 01:13:55,680
Speaker 6:  right? It's like they're betting on the open web instead of the, instead

1335
01:13:55,680 --> 01:13:58,680
Speaker 6:  of aol And like that's essentially where we're headed or at least a lot of

1336
01:13:58,680 --> 01:14:01,160
Speaker 6:  people think we're headed. And what I've heard from a bunch of people about

1337
01:14:01,160 --> 01:14:05,000
Speaker 6:  Mastodon is that Mastodon is probably not going to be

1338
01:14:05,000 --> 01:14:08,440
Speaker 6:  like the largest platform in this space, but it's, it was really, really,

1339
01:14:08,440 --> 01:14:12,240
Speaker 6:  really important that Mastodon was there for the last like

1340
01:14:12,240 --> 01:14:15,640
Speaker 6:  six months or so, that like, that there was a thing that proved this point

1341
01:14:15,640 --> 01:14:19,440
Speaker 6:  that it can and should exist and we can still have what we want in a better

1342
01:14:19,440 --> 01:14:23,080
Speaker 6:  way. Even if Macedon doesn't win, it will have like earned its place in

1343
01:14:23,080 --> 01:14:26,600
Speaker 6:  history just by being here at this like pivotal moment. Yeah. Which I think

1344
01:14:26,600 --> 01:14:27,280
Speaker 6:  is really interesting.

1345
01:14:27,760 --> 01:14:31,440
Speaker 5:  Activity probably in general is fascinating to me. It's the sort of,

1346
01:14:31,460 --> 01:14:33,720
Speaker 5:  how would you describe, it's like RSS but good.

1347
01:14:33,720 --> 01:14:36,680
Speaker 6:  That's one way to think about it. Yeah, I mean it's, it's essentially it's,

1348
01:14:36,680 --> 01:14:40,480
Speaker 6:  it's just a very simple communication tool. Like you can

1349
01:14:40,480 --> 01:14:43,760
Speaker 6:  build all kinds of stuff on top of it, but fundamentally it's a way of, it's

1350
01:14:43,760 --> 01:14:47,600
Speaker 6:  a structure for taking a type of content and distributing that type of content.

1351
01:14:47,600 --> 01:14:50,960
Speaker 6:  Like that's really all it is. And then people are building all kinds of social

1352
01:14:50,960 --> 01:14:54,120
Speaker 6:  networks on top of it and messaging platforms on top of it. And the idea

1353
01:14:54,120 --> 01:14:57,680
Speaker 6:  is I can make a thing and anywhere that can read that thing, it's closer

1354
01:14:57,680 --> 01:15:01,560
Speaker 6:  to email for social media really than anything else that I, I make a thing

1355
01:15:01,560 --> 01:15:04,680
Speaker 6:  and it can be read and interacted with from lots of places in a standardized

1356
01:15:04,680 --> 01:15:08,320
Speaker 6:  way. And that's a better vision than one platform

1357
01:15:08,320 --> 01:15:12,000
Speaker 6:  owned by a guy who just wanted to be the most followed person. But the real

1358
01:15:12,000 --> 01:15:15,520
Speaker 6:  reason I love this is because this happened right after

1359
01:15:16,400 --> 01:15:19,960
Speaker 6:  our friends Casey and Zoe over at Platformer reported that there is a secret

1360
01:15:20,230 --> 01:15:24,080
Speaker 6:  list of I think 35 people on Twitter who are being

1361
01:15:24,080 --> 01:15:27,960
Speaker 6:  artificially boosted by the algorithm in order to I guess

1362
01:15:27,960 --> 01:15:31,400
Speaker 6:  like keep them happy. And the list is hilarious. It's like Mr. Beast

1363
01:15:31,580 --> 01:15:35,360
Speaker 6:  and Elon Musk, but also drill and cat turd

1364
01:15:36,620 --> 01:15:37,040
Speaker 5:  And

1365
01:15:37,040 --> 01:15:40,880
Speaker 6:  It's just this random list of like relatively influential

1366
01:15:40,880 --> 01:15:44,840
Speaker 6:  tweeters who are given extra shine on the platform presumably in

1367
01:15:44,840 --> 01:15:48,640
Speaker 6:  order to keep them happy and using the platform. So it's like Elon Musk

1368
01:15:48,780 --> 01:15:52,680
Speaker 6:  put his thumb on the scale such that he is now the most followed person on

1369
01:15:52,680 --> 01:15:56,320
Speaker 6:  Twitter. Like, congratulations, I suppose you did it. Yeah, 44 billion

1370
01:15:56,360 --> 01:15:58,560
Speaker 6:  later you beat Barack Obama.

1371
01:15:59,040 --> 01:16:02,440
Speaker 5:  Good job. Twitter is rolling on its API

1372
01:16:02,450 --> 01:16:06,240
Speaker 5:  payment structures this week. We're all gonna lose our blue check mark

1373
01:16:06,240 --> 01:16:07,920
Speaker 5:  soon. Yep. Alright, whatever.

1374
01:16:08,680 --> 01:16:11,240
Speaker 6:  Do you guys remember a few months ago when we had a big argument about whether

1375
01:16:11,240 --> 01:16:15,160
Speaker 6:  we were gonna pay $8 a month for Twitter Blue and

1376
01:16:15,830 --> 01:16:19,560
Speaker 6:  Alex Heath Vociferously said, we're all going to, I don't remember what you

1377
01:16:19,560 --> 01:16:22,760
Speaker 6:  two said. And I said there's absolutely no chance in hell if I didn't say

1378
01:16:22,760 --> 01:16:24,800
Speaker 6:  that. Please nobody find the clip and send it to me. Yeah.

1379
01:16:24,800 --> 01:16:25,600
Speaker 5:  I'm definitely not gonna pay

1380
01:16:25,600 --> 01:16:29,080
Speaker 6:  For it. No, of course not. Who like it's now it literally has reached the

1381
01:16:29,080 --> 01:16:32,640
Speaker 6:  point actually Kranz, I think this was your case back then and you were right.

1382
01:16:32,640 --> 01:16:36,280
Speaker 6:  So kudos. It's embarrassing now to be the person who paid for Twitter Blue

1383
01:16:36,280 --> 01:16:39,520
Speaker 6:  to the point where they're gonna let you hide your check mark even if you

1384
01:16:39,520 --> 01:16:42,720
Speaker 6:  pay for Twitter blue. Like that's how far around this has come. Yeah,

1385
01:16:42,720 --> 01:16:46,640
Speaker 5:  I love it so much. The other piece of the puzzle is the API

1386
01:16:46,640 --> 01:16:50,320
Speaker 5:  limits. Yeah. So you, there's a free tier, I think it's

1387
01:16:50,320 --> 01:16:51,320
Speaker 5:  1500 tweets a month,

1388
01:16:52,000 --> 01:16:55,720
Speaker 6:  Which is nothing for most most things apps using this. That's nothing

1389
01:16:56,590 --> 01:17:00,200
Speaker 5:  Like we have an auto tweeter built into our cms that is not enough

1390
01:17:00,460 --> 01:17:04,400
Speaker 5:  for us. Just us little inbox media. So like a bunch of

1391
01:17:04,400 --> 01:17:08,240
Speaker 5:  publications have to figure out like are they gonna pay to tweet their stories?

1392
01:17:08,620 --> 01:17:12,560
Speaker 5:  Ooh. Right. Like that's just a weird imposition of a

1393
01:17:12,560 --> 01:17:15,760
Speaker 5:  cost. I think the New York Times said it was definitely not gonna pay for

1394
01:17:15,760 --> 01:17:19,680
Speaker 5:  Twitter blue French reporters. We, I I doubt that we are, I think we have

1395
01:17:19,680 --> 01:17:21,960
Speaker 5:  better things to spend our money on. Like I'd rather spend money on reporting

1396
01:17:22,110 --> 01:17:25,920
Speaker 7:  Also it's charging an additional fee for like

1397
01:17:25,920 --> 01:17:28,720
Speaker 7:  publications and stuff, right? To be verified.

1398
01:17:28,720 --> 01:17:32,680
Speaker 6:  Well you can get your own special verification badge for even more

1399
01:17:32,680 --> 01:17:33,800
Speaker 6:  money. So you can,

1400
01:17:33,950 --> 01:17:37,080
Speaker 5:  I mean the thing that is like whatever is being said today is not the thing

1401
01:17:37,080 --> 01:17:40,840
Speaker 5:  that is true tomorrow. Correct. Like the, the the who

1402
01:17:40,840 --> 01:17:44,360
Speaker 5:  knows, like whatever I say today on this podcast about what is happening

1403
01:17:44,450 --> 01:17:48,000
Speaker 5:  is definitely not what will be true next week. The big idea is that Twitter,

1404
01:17:48,000 --> 01:17:51,800
Speaker 5:  Twitter's gonna, it's gonna start costing money to post Right.

1405
01:17:51,800 --> 01:17:55,720
Speaker 5:  In a variety of ways. Whether it's you want to be, have your replies

1406
01:17:55,720 --> 01:17:58,880
Speaker 5:  seen and you need Twitter blue and that's gonna cost money. Or whether you

1407
01:17:58,940 --> 01:18:02,320
Speaker 5:  are an app developer, a publisher, and you tweet enough that you need API

1408
01:18:02,320 --> 01:18:06,200
Speaker 5:  access that's gonna cost money and imposing the cost on that side

1409
01:18:06,610 --> 01:18:10,400
Speaker 5:  is super weird cuz there's no value on the other side.

1410
01:18:10,690 --> 01:18:14,480
Speaker 5:  So if I pay money onto Twitter, what am I, how do I make the money

1411
01:18:14,480 --> 01:18:15,800
Speaker 5:  back? No one knows.

1412
01:18:15,950 --> 01:18:18,960
Speaker 6:  I've seen a bunch of creators asking that recently this, they're like, okay,

1413
01:18:18,960 --> 01:18:22,280
Speaker 6:  let me get this straight. You're gonna charge me money to post so that you

1414
01:18:22,280 --> 01:18:24,080
Speaker 6:  can put ads next to my post that you take

1415
01:18:24,870 --> 01:18:25,360
Speaker 5:  This.

1416
01:18:27,430 --> 01:18:27,920
Speaker 7:  It's

1417
01:18:27,920 --> 01:18:30,920
Speaker 5:  Not a bunch of craters. It's Elon's number one guy.

1418
01:18:30,920 --> 01:18:31,800
Speaker 6:  Yeah, that's

1419
01:18:31,800 --> 01:18:32,680
Speaker 7:  Right. Old capture.

1420
01:18:33,200 --> 01:18:37,080
Speaker 6:  Yeah. And, and so it's like I'm you're, you're asking me to pay to do

1421
01:18:37,080 --> 01:18:41,000
Speaker 6:  a bunch of work from which you accrue all of the value. Like that's a bad

1422
01:18:41,000 --> 01:18:43,840
Speaker 6:  deal. I'm no economist, but like that's a bad deal.

1423
01:18:44,340 --> 01:18:48,120
Speaker 5:  You got like there are like cookie shop owners that can do this

1424
01:18:48,120 --> 01:18:48,400
Speaker 5:  math.

1425
01:18:48,800 --> 01:18:49,040
Speaker 6:  Yeah.

1426
01:18:49,390 --> 01:18:53,320
Speaker 5:  Like no pH like a six-year-old lemonade sands. Like lemme get it, lemme get

1427
01:18:53,320 --> 01:18:57,280
Speaker 5:  this straight. I gotta pay to set up shop and then there's no one to buy

1428
01:18:57,280 --> 01:18:57,600
Speaker 5:  the lemonade.

1429
01:18:57,600 --> 01:19:00,520
Speaker 6:  You're gonna give me $10 and then I'm gonna drink the lemonade.

1430
01:19:00,920 --> 01:19:01,880
Speaker 7:  That's how this works.

1431
01:19:05,410 --> 01:19:08,960
Speaker 5:  So we'll see. I I, I know. All right, here's, okay, so that's Twitter. We'll

1432
01:19:08,960 --> 01:19:12,840
Speaker 5:  see I think the next inflection point is when they start charging for API

1433
01:19:12,840 --> 01:19:16,640
Speaker 5:  access and they take away the check marks. Agreed. And I think that the

1434
01:19:16,640 --> 01:19:20,280
Speaker 5:  line on the curve goes a different Get ready mastodon is what I'm saying.

1435
01:19:20,790 --> 01:19:24,640
Speaker 5:  Even if it doesn't survive all tumble. Okay, I have two for

1436
01:19:24,640 --> 01:19:28,480
Speaker 5:  lightning iron. One is very fast. The new Sony Z V

1437
01:19:28,480 --> 01:19:31,760
Speaker 5:  E one came out this week. It's like an RX 100 with a full frame sensor in

1438
01:19:31,760 --> 01:19:34,280
Speaker 5:  it. There's no other way to describe it. Like it's that model of camera,

1439
01:19:34,280 --> 01:19:37,360
Speaker 6:  Which is literally ne Eli's dream camera. Like what you just described is

1440
01:19:37,360 --> 01:19:38,680
Speaker 6:  everything you've ever wanted your whole life.

1441
01:19:38,680 --> 01:19:41,920
Speaker 5:  Yeah. So I have a zv one, right? So the camera I'm looking into right now

1442
01:19:41,920 --> 01:19:45,280
Speaker 5:  is a Sony Zv one. I have two of them as webcams. They're just like

1443
01:19:45,290 --> 01:19:49,000
Speaker 5:  cheaper RX 100 class cameras. They have APSC sensors in them.

1444
01:19:49,000 --> 01:19:52,560
Speaker 5:  They're great. I love them. Zv one little bigger, but same

1445
01:19:53,280 --> 01:19:57,120
Speaker 5:  physical concept, full frame sensor, interchangeable lenses. Just go

1446
01:19:57,120 --> 01:20:01,000
Speaker 5:  watch Becca's video with it. It's so good. It's great. It's

1447
01:20:01,000 --> 01:20:04,800
Speaker 5:  such a good idea. What if we make a full frame camera that kind of just

1448
01:20:04,800 --> 01:20:07,760
Speaker 5:  acts like an iPhone, right? It's just like does it all all for you? It's

1449
01:20:07,760 --> 01:20:11,680
Speaker 5:  just like lets you f like just run and gun shoot with it. It's so expensive.

1450
01:20:11,680 --> 01:20:15,360
Speaker 5:  It's like $2,200 for body only And I'm like oh would I want it really bad?

1451
01:20:17,390 --> 01:20:18,880
Speaker 6:  Yeah, that's how they get you.

1452
01:20:19,140 --> 01:20:22,000
Speaker 5:  But go watch that video cause it's really cool and like for all of our, what

1453
01:20:22,000 --> 01:20:25,960
Speaker 5:  is a photo conversations and ai, video conversations,

1454
01:20:26,200 --> 01:20:29,840
Speaker 5:  whatever. Cool. Cameras are still really cool, you know And like

1455
01:20:29,890 --> 01:20:33,840
Speaker 5:  there's something about this one that just makes me want to go start a

1456
01:20:33,840 --> 01:20:37,520
Speaker 5:  YouTube channel. Like it's a piece of hardware that is like inspiring

1457
01:20:37,530 --> 01:20:41,280
Speaker 5:  to think about using and I think that's really cool. So that's one. Go watch

1458
01:20:41,280 --> 01:20:44,080
Speaker 5:  that video also. Becca's good. And she might have sold me the camera. Yeah,

1459
01:20:44,080 --> 01:20:47,000
Speaker 5:  go watch the video. Okay, here's my last one. I just think this is like a

1460
01:20:47,000 --> 01:20:50,400
Speaker 5:  very, we talked about Matter in Smart Home, it's taken three years

1461
01:20:50,760 --> 01:20:54,440
Speaker 5:  since Google invested like hundreds of millions of dollars into a d t. The

1462
01:20:54,440 --> 01:20:56,600
Speaker 5:  the security company to compete with Ring

1463
01:20:57,440 --> 01:21:00,040
Speaker 6:  Straight up. Forgot that happened. Like until I saw this headline, I absolutely

1464
01:21:00,040 --> 01:21:00,600
Speaker 6:  forgot that happened

1465
01:21:00,760 --> 01:21:04,440
Speaker 5:  Three years, 450 million. They invested in

1466
01:21:04,440 --> 01:21:08,000
Speaker 5:  ADT because they're like, ring is killing us and they finally

1467
01:21:08,390 --> 01:21:12,280
Speaker 5:  have rolled something out. It's called the self setup system. And it just

1468
01:21:12,280 --> 01:21:16,160
Speaker 5:  like integrates Google Nest video doorbells and cameras into a D

1469
01:21:16,240 --> 01:21:20,160
Speaker 5:  t and gives you a bunch of sensors. You gotta pay $25 a month,

1470
01:21:20,160 --> 01:21:23,720
Speaker 5:  24 9 9 a month. And then you also need a Nest Aware subscription, which is

1471
01:21:23,720 --> 01:21:27,520
Speaker 5:  $35 a month. Ooh. And the starter

1472
01:21:27,520 --> 01:21:31,440
Speaker 5:  package is like nobody knows how much it costs. So Jen had to run around

1473
01:21:31,440 --> 01:21:34,680
Speaker 5:  like asking executives at Google and a D t like how much does this cost?

1474
01:21:34,680 --> 01:21:38,520
Speaker 5:  Because you keep saying it's gonna be $220 but that's just

1475
01:21:38,520 --> 01:21:41,920
Speaker 5:  the introductory price and soon it'll be $480.

1476
01:21:42,330 --> 01:21:46,320
Speaker 5:  No. And it's like why? Wow. Just all like the

1477
01:21:46,320 --> 01:21:50,120
Speaker 5:  whole nest story landed in this place where three years after they

1478
01:21:50,120 --> 01:21:53,760
Speaker 5:  spent almost half a billion dollars to invest in this company. They're like,

1479
01:21:53,780 --> 01:21:57,520
Speaker 5:  now your Nest cameras can alert the cops

1480
01:21:57,520 --> 01:22:00,280
Speaker 5:  through your AD T subscription. I just dunno what they're

1481
01:22:00,280 --> 01:22:04,200
Speaker 6:  Doing. It's like the most expensive, very simple a p i of all time. Like

1482
01:22:04,200 --> 01:22:07,480
Speaker 6:  congratulations to everybody involved there. Yeah. How did that take three

1483
01:22:07,480 --> 01:22:11,000
Speaker 6:  years? Nothing in any of this announcement sounds like three years of work.

1484
01:22:11,000 --> 01:22:14,840
Speaker 6:  There's just, it sounds very much like they, there's like one person at Google

1485
01:22:14,840 --> 01:22:18,320
Speaker 6:  and one person at AD T who talked to each other and they like lost each other's

1486
01:22:18,320 --> 01:22:20,960
Speaker 6:  phone numbers for two years and then were like, should we do it now? And

1487
01:22:20,960 --> 01:22:22,280
Speaker 6:  then they did it and now here we are.

1488
01:22:22,280 --> 01:22:23,800
Speaker 5:  Yeah. Look where's the stuff?

1489
01:22:23,950 --> 01:22:25,200
Speaker 6:  Yeah, yeah.

1490
01:22:25,270 --> 01:22:29,200
Speaker 5:  Like yeah where are the new products? Where's the, it's

1491
01:22:29,200 --> 01:22:31,520
Speaker 5:  just like the reason you would invest in a company like ADT is cause they

1492
01:22:31,520 --> 01:22:35,440
Speaker 5:  like have sales and service and dealers and da da da and it's like nah,

1493
01:22:35,440 --> 01:22:38,360
Speaker 5:  it's just your Google cameras. You already have to hang up by yourself. Like

1494
01:22:38,630 --> 01:22:41,920
Speaker 5:  I don't get it. It's super weird. It's just completely emblematic of like

1495
01:22:41,920 --> 01:22:43,680
Speaker 5:  Google lack of focus. Yeah.

1496
01:22:43,680 --> 01:22:44,000
Speaker 6:  Yeah.

1497
01:22:44,380 --> 01:22:48,280
Speaker 5:  But it's out. Jen actually says pricing-wise it's like compelling. Once you

1498
01:22:48,280 --> 01:22:51,320
Speaker 5:  add up all the services, it's just, you gotta add up all the services and

1499
01:22:51,320 --> 01:22:54,400
Speaker 5:  the point is you should have a pu No thanks anyhow. Look, I, I started on

1500
01:22:54,400 --> 01:22:58,240
Speaker 5:  the high of the CBE one. Yes. I hope spend $2,200 on a body

1501
01:22:58,240 --> 01:23:01,880
Speaker 5:  only full frame handled camera. That's exciting.

1502
01:23:01,970 --> 01:23:05,760
Speaker 5:  No, I will not spend $50 a month on that to wear wear subscription.

1503
01:23:05,990 --> 01:23:07,000
Speaker 6:  Sounds about right

1504
01:23:07,490 --> 01:23:09,560
Speaker 5:  To, to protect my family. But

1505
01:23:09,560 --> 01:23:12,480
Speaker 6:  If you put a full frame sensor in that nest cam and you eyes back in

1506
01:23:13,700 --> 01:23:17,000
Speaker 5:  For sure. Alright, we're over as always.

1507
01:23:17,880 --> 01:23:21,240
Speaker 5:  This is a delight. We should just have like wide-ranging philosophical Wait,

1508
01:23:21,240 --> 01:23:25,080
Speaker 5:  that's our show. That's what we do every week. Surprise. We

1509
01:23:25,080 --> 01:23:27,400
Speaker 5:  have actually a lot of coverage. We didn't talk about the TikTok band this

1510
01:23:27,400 --> 01:23:30,200
Speaker 5:  week. We talked about a lot last week. There's a lot more coverage on the

1511
01:23:30,200 --> 01:23:33,840
Speaker 5:  side of that. Yep. The political alliances are actually shifting. Like Rand

1512
01:23:33,840 --> 01:23:37,280
Speaker 5:  Paul came out against it today. Siding with a bunch of progressive democrats.

1513
01:23:37,280 --> 01:23:41,160
Speaker 5:  Like that's weird. It's weird. Weird. That's about there. Yeah. And his

1514
01:23:41,320 --> 01:23:45,120
Speaker 5:  argument is like, this is a weird imposition on the First Amendment. It's

1515
01:23:45,120 --> 01:23:48,960
Speaker 5:  weird. It's weird. Me and Rand Paul together at last. So

1516
01:23:48,970 --> 01:23:52,720
Speaker 5:  go look at that coverage cuz it's important. I think I will just say this

1517
01:23:52,720 --> 01:23:56,440
Speaker 5:  in response to some of the like Instagram reels comments I saw from our clips

1518
01:23:56,440 --> 01:24:00,360
Speaker 5:  last week. It's not about China or not China. It's like if the United States

1519
01:24:00,360 --> 01:24:03,920
Speaker 5:  government wants to do something like this, they have to make a good case.

1520
01:24:05,110 --> 01:24:09,040
Speaker 5:  They didn't. It's just like pretty flat out. Like they're

1521
01:24:09,040 --> 01:24:12,880
Speaker 5:  not doing that right now. And like yeah, I feel bad that the wifi guy

1522
01:24:12,950 --> 01:24:15,880
Speaker 5:  didn't know how to ask the question about the wifi and there's like 500 people

1523
01:24:15,880 --> 01:24:19,320
Speaker 5:  trying to explain to us what he really meant. No, but like he wields the

1524
01:24:19,320 --> 01:24:23,240
Speaker 5:  power of the state so like he has to do a good job and

1525
01:24:23,240 --> 01:24:26,440
Speaker 5:  that's like, there's just like a disconnect there where like I think we're

1526
01:24:26,440 --> 01:24:29,680
Speaker 5:  all too sympathetic to people who are making bad decisions about technology.

1527
01:24:29,680 --> 01:24:32,360
Speaker 5:  It's like you're gonna shut down an app that millions of people use anyway.

1528
01:24:32,360 --> 01:24:35,680
Speaker 5:  It's all on the site. Addie wrote a great piece. The TikTok ban is a betrayal

1529
01:24:35,680 --> 01:24:38,520
Speaker 5:  of the open internet. Go read it. Also very virgie. This has been a very

1530
01:24:38,680 --> 01:24:42,560
Speaker 5:  virgie episode. You should listen to decoder y Eugene Roco, the CEO of Macedon.

1531
01:24:42,560 --> 01:24:46,120
Speaker 5:  There's a lot of activity pub conversation there, which I to you is important.

1532
01:24:46,120 --> 01:24:49,680
Speaker 5:  Yeah. Next week we're wrapping up our centennial series with the CEO of Hasbro.

1533
01:24:49,680 --> 01:24:53,080
Speaker 5:  We talk a lot about magic, the gathering in Dungeons and Dragons and like

1534
01:24:53,080 --> 01:24:55,960
Speaker 5:  there's a lot of controversy in those communities. It's very good.

1535
01:24:56,900 --> 01:25:00,720
Speaker 5:  But here's the thing that I learned, hasbro hundred years old started

1536
01:25:00,720 --> 01:25:04,040
Speaker 5:  in 1923 by the Hassenfeld Brothers. It's just the

1537
01:25:04,090 --> 01:25:06,800
Speaker 5:  Hasbros. Wait, really? It's

1538
01:25:06,800 --> 01:25:10,680
Speaker 6:  So good. It's the has. It's like how Adidas is Adi and Doss. They're just

1539
01:25:10,680 --> 01:25:12,360
Speaker 6:  two people. It's the Hasbros.

1540
01:25:12,360 --> 01:25:16,160
Speaker 5:  Yeah. But think about 1923. They're like, oh, we're bros. Like it's just

1541
01:25:16,160 --> 01:25:18,960
Speaker 5:  like, it blows your mind. Like this language didn't exist in 1923.

1542
01:25:19,280 --> 01:25:22,560
Speaker 6:  Also the Hasbros is like an ice cream shop that I would go to every single

1543
01:25:22,560 --> 01:25:23,760
Speaker 6:  day. I'm just gonna throw that out there.

1544
01:25:24,030 --> 01:25:27,600
Speaker 5:  Yeah, it's very good. That's coming next week and Wednesday show what's on

1545
01:25:27,600 --> 01:25:28,080
Speaker 5:  the Wednesday show.

1546
01:25:28,080 --> 01:25:31,040
Speaker 6:  Wednesday show. We're gonna talk about Matter and Sidewalk. We're gonna talk

1547
01:25:31,040 --> 01:25:35,000
Speaker 6:  about a bunch of Microsoft AI stuff with Tom. Part of the sort of

1548
01:25:35,000 --> 01:25:38,800
Speaker 6:  ongoing question of like, what is AI actually good for? Microsoft's doing

1549
01:25:38,800 --> 01:25:41,600
Speaker 6:  a bunch of interesting stuff, so we're gonna dig into that with him. And

1550
01:25:41,600 --> 01:25:45,360
Speaker 6:  then we sent Monica Chin out to Times Square to test a bunch of laptop

1551
01:25:45,360 --> 01:25:48,920
Speaker 6:  microphones and it went so differently than I expected,

1552
01:25:48,920 --> 01:25:52,800
Speaker 6:  which is fascinating and really fun. And it's gonna be one of those episodes

1553
01:25:52,800 --> 01:25:56,640
Speaker 6:  that like occasionally sounds terrible, but in the service of good

1554
01:25:56,640 --> 01:25:57,680
Speaker 6:  things. So it's gonna be,

1555
01:25:57,680 --> 01:25:58,720
Speaker 5:  It's a good one. Journalism.

1556
01:25:58,830 --> 01:25:59,760
Speaker 6:  Yeah. All

1557
01:25:59,760 --> 01:26:02,200
Speaker 5:  Right. I love it. That's it. It's for chest background

1558
01:26:06,820 --> 01:26:10,440
Speaker 15:  And that's a wrap for Vergecast this week. Thanks for listening. If you

1559
01:26:10,440 --> 01:26:13,880
Speaker 15:  enjoy the show, subscribe in the podcast app of your choice or tell a friend,

1560
01:26:13,980 --> 01:26:17,920
Speaker 15:  you can send us feedback at vergecast@theverge.com. This show is

1561
01:26:18,120 --> 01:26:21,560
Speaker 15:  produced by me, Liam James, and our senior audio director, Andrew Marino.

1562
01:26:21,800 --> 01:26:25,520
Speaker 15:  This episode was edited and mixed by Amanda Rose Smith. Our

1563
01:26:25,520 --> 01:26:29,360
Speaker 15:  editorial director is Brooke Min, and our executive producer is Eleanor

1564
01:26:29,360 --> 01:26:33,040
Speaker 15:  Donovan. The Verge Cast is a production of the Verge and Box Media

1565
01:26:33,040 --> 01:26:36,160
Speaker 15:  podcast network. And that's it. We'll see you next week.

1566
01:26:40,950 --> 01:26:44,720
Speaker 16:  What one seemed improbable is happening now? Insurance is

1567
01:26:44,720 --> 01:26:48,040
Speaker 16:  front and center for tech companies who are finally commanding the space

1568
01:26:48,040 --> 01:26:52,000
Speaker 16:  they occupy in every other industry. Behind the disruption is Cover

1569
01:26:52,000 --> 01:26:55,640
Speaker 16:  Genius, the InsureTech for embedded insurance that protects customers of

1570
01:26:55,640 --> 01:26:59,320
Speaker 16:  the world's largest digital companies available at Amazon. Intuit,

1571
01:26:59,350 --> 01:27:03,000
Speaker 16:  Flipcart, eBay, booking.com. Skyscanner, Ryan Air

1572
01:27:03,240 --> 01:27:06,920
Speaker 16:  and Southeast Asia's largest company. Shoppy cover Genius's

1573
01:27:07,200 --> 01:27:10,480
Speaker 16:  platform makes it easy for their global partners to embed insurance and

1574
01:27:10,680 --> 01:27:14,440
Speaker 16:  warranty bundles, and especially RUS within their booking path or

1575
01:27:14,440 --> 01:27:17,680
Speaker 16:  signup, connecting customers with the protection they need and with global

1576
01:27:17,680 --> 01:27:21,640
Speaker 16:  licensing and end-to-end capabilities across all industries. From property

1577
01:27:21,640 --> 01:27:25,320
Speaker 16:  to travel, FinTech logistics. The gig economy in Retail

1578
01:27:25,360 --> 01:27:29,120
Speaker 16:  Cover Genius can build and distribute any kind of insurance and process

1579
01:27:29,120 --> 01:27:33,000
Speaker 16:  claims in all 50 states and in more than 60 countries, all through a

1580
01:27:33,000 --> 01:27:36,720
Speaker 16:  single integration. Because your customers are at the center of everything.

1581
01:27:36,720 --> 01:27:40,400
Speaker 16:  Cover Genius Pays claims instantly and maintains an NPS of

1582
01:27:40,400 --> 01:27:44,280
Speaker 16:  65 plus the highest in an industry where traditional insurers

1583
01:27:44,480 --> 01:27:48,440
Speaker 16:  rely on paper forms, missed calls and mail checks. To deliver NPS

1584
01:27:48,440 --> 01:27:52,240
Speaker 16:  below Zero, give your customers the peace of mind they deserve. Visit cover

1585
01:27:52,240 --> 01:27:55,720
Speaker 16:  genius.com/fox today to learn more. Cover Genius,

1586
01:27:55,820 --> 01:27:57,720
Speaker 16:  the intro tech for Embedded Insurance.

