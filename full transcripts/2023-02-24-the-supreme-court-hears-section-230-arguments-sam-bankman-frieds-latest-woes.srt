1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: eb579430-b41d-11ed-9702-d5b2ebc31101
Status: Done
Stage: Done
Title: The Supreme Court hears Section 230 arguments, Sam Bankman-Fried's latest woes
Audio URL: https://jfe93e.s3.amazonaws.com/7184822667862162072/1326342453828877326/s93290-US-4454s-1677227607.mp3
Description: The Verge's Nilay Patel, Alex Cranz, T.C. Sottek, and Adi Robertson discuss the Supreme Court cases that could reshape the future of the internet.
Later, Verge policy reporter Makena Kelly joins the show to discuss new charges against FTX founder Sam Bankman-Fried.

2
00:01:44,860 --> 00:01:45,420
Speaker 2:  Hello

3
00:01:45,420 --> 00:01:48,940
Speaker 4:  And welcome to first the flagship podcast of Supreme Court Argument.

4
00:01:48,940 --> 00:01:49,740
Speaker 4:  Interpretation.

5
00:01:50,250 --> 00:01:50,540
Speaker 1:  I

6
00:01:50,540 --> 00:01:53,700
Speaker 4:  Love that. It's good. Yeah, it's, it's not what people expect of us, but

7
00:01:53,700 --> 00:01:56,820
Speaker 4:  it's what we deliver week after week for some reason Here for you. I'm your

8
00:01:56,820 --> 00:01:58,620
Speaker 4:  friend Eli. Alex Trans is here. I'm

9
00:01:58,620 --> 00:02:02,460
Speaker 1:  Here for all of your judicial needs is, am

10
00:02:02,620 --> 00:02:03,340
Speaker 1:  I allowed to say that?

11
00:02:03,340 --> 00:02:06,980
Speaker 4:  That sounds like you do filing and like you're like you, you do

12
00:02:07,340 --> 00:02:09,780
Speaker 4:  you like service a process like you show up, you're like you've been

13
00:02:09,780 --> 00:02:13,140
Speaker 1:  So No, no. I wanna do like the law stuff, but like the law stuff, I don't,

14
00:02:13,140 --> 00:02:16,780
Speaker 1:  I don't don't, don't actually ask me to do it. I don't have a bar card.

15
00:02:17,410 --> 00:02:17,900
Speaker 1:  I'll

16
00:02:17,900 --> 00:02:19,380
Speaker 4:  Be fine. TC Soak is here. Yes.

17
00:02:19,400 --> 00:02:23,220
Speaker 5:  I'm here as the unwilling subject of human experimentation

18
00:02:23,220 --> 00:02:26,500
Speaker 5:  on putting bloggers in Supreme Court for six hours for

19
00:02:26,500 --> 00:02:30,300
Speaker 4:  Two days. Yeah, it's good. And Addie Robertson is here. Hey, so there's a

20
00:02:30,300 --> 00:02:33,260
Speaker 4:  lot of news this week. This is be a strange episode of the ro. I'm here for

21
00:02:33,260 --> 00:02:33,820
Speaker 4:  the first part.

22
00:02:33,850 --> 00:02:35,020
Speaker 1:  Yeah. And then I'm kicking you

23
00:02:35,020 --> 00:02:38,540
Speaker 4:  Out, Alex kicking me out and taking over. I've gotta go do our podcast

24
00:02:38,540 --> 00:02:41,700
Speaker 4:  conference, hot pod summit, which we'll talk about next week. Very cool.

25
00:02:41,700 --> 00:02:45,380
Speaker 4:  Very excited about that. But I wanted to make sure I was on the first segment

26
00:02:45,380 --> 00:02:49,180
Speaker 4:  here and especially with TC and Addie because there were Supreme

27
00:02:49,180 --> 00:02:52,620
Speaker 4:  Court arguments this week on two cases that

28
00:02:53,340 --> 00:02:55,260
Speaker 4:  honestly, depending on how this goes, it could be the story of the year.

29
00:02:55,290 --> 00:02:58,660
Speaker 4:  Yeah. Story of the decade. This could be like the week the internet changed

30
00:02:58,660 --> 00:03:02,040
Speaker 4:  forever because of how the Supreme Court might decide in these two cases.

31
00:03:02,340 --> 00:03:06,320
Speaker 4:  One is Gonzalez v Google, which is Eddie, correct me if I'm wrong. That's

32
00:03:06,320 --> 00:03:10,000
Speaker 4:  the one that is the boundary of two 30 where Google Correct.

33
00:03:10,000 --> 00:03:13,760
Speaker 4:  Might be liable for the YouTube recommendation system in a very

34
00:03:13,760 --> 00:03:17,200
Speaker 4:  narrow way. And the other one is Twitter v taney,

35
00:03:17,680 --> 00:03:21,520
Speaker 4:  where Twitter allowing terrorists

36
00:03:21,520 --> 00:03:24,720
Speaker 4:  to use Twitter might make it liable under the

37
00:03:24,720 --> 00:03:25,800
Speaker 4:  Anti-Terrorism Act.

38
00:03:26,040 --> 00:03:27,840
Speaker 6:  Right. Aiding in a vetting terrorism,

39
00:03:27,960 --> 00:03:28,720
Speaker 4:  Right? Yeah.

40
00:03:29,460 --> 00:03:33,160
Speaker 5:  Any terrorism act is like the nexus of both lawsuits. But

41
00:03:33,420 --> 00:03:36,960
Speaker 5:  two 30 immunity is being implicated in Gonzalez v Google

42
00:03:38,080 --> 00:03:41,900
Speaker 5:  and in Twitterverse Taney. It's just, they're just looking

43
00:03:41,900 --> 00:03:45,140
Speaker 5:  at Yeah. Whether they aided in a better terrorist, but

44
00:03:45,140 --> 00:03:47,780
Speaker 1:  It's about free speech on the internet. Basically. It's about what you are.

45
00:03:47,840 --> 00:03:51,580
Speaker 1:  You can and cannot say. No, no, it's not. It's about like

46
00:03:51,580 --> 00:03:52,500
Speaker 1:  the algorithms.

47
00:03:52,530 --> 00:03:56,420
Speaker 4:  It's a, yeah. So I'll get, these are very strange cases I think

48
00:03:56,870 --> 00:04:00,820
Speaker 4:  at a very fundamental level. And I think

49
00:04:00,820 --> 00:04:04,700
Speaker 4:  Addie has written about this. Sarah Jong slacked me a couple days ago

50
00:04:04,760 --> 00:04:08,420
Speaker 4:  and just asked me this out of the blue, I have found myself just like

51
00:04:08,420 --> 00:04:11,980
Speaker 4:  staring into the night sky wondering this no one knows why the Supreme Court

52
00:04:11,980 --> 00:04:15,660
Speaker 4:  took up these cases. They're not great examples of the problem.

53
00:04:16,090 --> 00:04:19,780
Speaker 4:  Yeah. With big services and particularly the Google

54
00:04:19,780 --> 00:04:23,660
Speaker 4:  case around two 30, the law is like pretty settled, right? That

55
00:04:23,770 --> 00:04:27,540
Speaker 4:  this recommendation system does not make Google liable for the

56
00:04:27,540 --> 00:04:31,140
Speaker 4:  contents of the, and then the facts of this case Addie are, are particularly

57
00:04:31,140 --> 00:04:33,540
Speaker 4:  weird. Go through the Gonzalez case really quickly just so people understand

58
00:04:33,540 --> 00:04:36,140
Speaker 4:  what, what happened here and why Google found itself in front of the Supreme

59
00:04:36,140 --> 00:04:36,300
Speaker 4:  Court.

60
00:04:36,780 --> 00:04:40,650
Speaker 6:  So both these cases are about, there were Islamic state attacks

61
00:04:40,650 --> 00:04:44,610
Speaker 6:  and someone died in that attack and the families sued the

62
00:04:44,880 --> 00:04:48,250
Speaker 6:  A full sort of host of services. There's meta, YouTube, Twitter

63
00:04:48,840 --> 00:04:52,610
Speaker 6:  that saying that they aided in a vetted terrorism. And

64
00:04:52,610 --> 00:04:56,330
Speaker 6:  with Gonzalez, the claim is YouTube is not necessarily

65
00:04:56,330 --> 00:04:59,690
Speaker 6:  responsible for just the fact that terrorists were on the platform, but

66
00:05:00,090 --> 00:05:04,050
Speaker 6:  their recommendations served up terrorist videos and ended up

67
00:05:04,050 --> 00:05:07,690
Speaker 6:  then promoting the, their claim is promoting this and

68
00:05:08,090 --> 00:05:11,410
Speaker 6:  actually creating content instead of just hosting third party content, which

69
00:05:11,410 --> 00:05:13,490
Speaker 6:  they maybe wouldn't be liable for under section two 30.

70
00:05:13,820 --> 00:05:17,290
Speaker 4:  So that two 30 is the one that says, okay, you're in the Verge comments.

71
00:05:17,320 --> 00:05:21,210
Speaker 4:  Yeah. And you're like, I hate Google. And Google's like, you defamed us

72
00:05:21,210 --> 00:05:25,010
Speaker 4:  and they sue you. They can't sue us. Right? Right. So

73
00:05:25,010 --> 00:05:28,170
Speaker 4:  we're not liable for what you publish on our internet. But

74
00:05:28,170 --> 00:05:29,090
Speaker 1:  This is about like also

75
00:05:29,090 --> 00:05:32,010
Speaker 5:  How Google objectively true statements are defense against Definitely

76
00:05:32,730 --> 00:05:35,450
Speaker 4:  Whatever. If Google wants to sue people, Google sue people, that's not an

77
00:05:35,450 --> 00:05:38,010
Speaker 4:  invitation to Google at this point in time. I'm just saying you post something

78
00:05:38,010 --> 00:05:41,770
Speaker 4:  in our comments that somebody doesn't like, yeah. Two 30 is just like flat

79
00:05:41,770 --> 00:05:45,690
Speaker 4:  out bars you from suing us. Right? In many cases, Addie, in this case,

80
00:05:45,690 --> 00:05:49,610
Speaker 4:  what the distinction here is that YouTube's

81
00:05:49,610 --> 00:05:53,290
Speaker 4:  recommendation system like took a video for

82
00:05:53,490 --> 00:05:57,170
Speaker 4:  terrorist recruitment and showed it to someone else and that undid

83
00:05:57,360 --> 00:05:58,480
Speaker 4:  the two 30 protection.

84
00:05:58,480 --> 00:06:02,320
Speaker 6:  Right? The claim is that they are producing their own content. I

85
00:06:02,320 --> 00:06:06,280
Speaker 6:  am hesitant to make any metaphors because metaphors are where everything

86
00:06:06,280 --> 00:06:10,080
Speaker 6:  goes to die here. But it's more, it's sort of like if

87
00:06:10,210 --> 00:06:14,200
Speaker 6:  YouTube had published a separate series of videos and was like,

88
00:06:14,200 --> 00:06:18,160
Speaker 6:  Hey, these things are all really great and we endorse them is a

89
00:06:18,160 --> 00:06:19,440
Speaker 6:  little bit where they're going with this.

90
00:06:19,560 --> 00:06:23,200
Speaker 4:  Right? So YouTube feeds your video into the recommendation system and

91
00:06:23,200 --> 00:06:27,080
Speaker 4:  merely by showing a video to someone, they've made

92
00:06:27,080 --> 00:06:29,720
Speaker 4:  some sort of choice. They've created some sort of new content, right?

93
00:06:29,720 --> 00:06:31,200
Speaker 5:  Because they've put content in front of you.

94
00:06:31,350 --> 00:06:32,760
Speaker 4:  This is a whole argument. Yeah.

95
00:06:32,760 --> 00:06:36,560
Speaker 6:  But as I think many Supreme Court justices have decided,

96
00:06:37,050 --> 00:06:40,680
Speaker 6:  as far as we can tell from the questions this week, the way in which

97
00:06:40,680 --> 00:06:42,880
Speaker 6:  they're making that argument is really weird.

98
00:06:43,340 --> 00:06:46,240
Speaker 4:  Yes. So we'll come to that. So that's, I just wanna say that's the first

99
00:06:46,240 --> 00:06:50,040
Speaker 4:  case. That's the Google case with two 30 in the middle of it.

100
00:06:50,040 --> 00:06:52,320
Speaker 4:  Yes. What's going on in the Twitter case?

101
00:06:52,740 --> 00:06:56,640
Speaker 6:  The Twitter case is one that was basically taken up

102
00:06:56,840 --> 00:07:00,120
Speaker 6:  as a sort of, if you take up Gonzalez, please, please, please take this too.

103
00:07:00,600 --> 00:07:04,400
Speaker 6:  Which is arguing that even if two 30

104
00:07:04,570 --> 00:07:08,480
Speaker 6:  is falls here, even if two 30 weren't around, Twitter

105
00:07:08,490 --> 00:07:12,320
Speaker 6:  is just providing a basic service. And terrorists happen to have used

106
00:07:12,320 --> 00:07:16,280
Speaker 6:  that service. And you cannot say that somebody aided an embedded terrorism

107
00:07:16,570 --> 00:07:20,440
Speaker 6:  if they're just, there's somebody there that is, happens to

108
00:07:20,440 --> 00:07:24,080
Speaker 6:  be on your platform and you weren't trying to get them there.

109
00:07:24,140 --> 00:07:27,040
Speaker 6:  You were opposed to them being there, but you failed to kick them off.

110
00:07:27,960 --> 00:07:31,910
Speaker 4:  So Twitter's saying, okay, look, we know the Islamic State uses

111
00:07:31,910 --> 00:07:32,150
Speaker 4:  Twitter

112
00:07:32,910 --> 00:07:36,190
Speaker 5:  And, and like you said, nei, the facts of both cases are super weird because

113
00:07:36,720 --> 00:07:40,590
Speaker 5:  there's no, nobody alleged that there was like

114
00:07:40,590 --> 00:07:44,350
Speaker 5:  a specific video or like a specific like that one of the

115
00:07:44,350 --> 00:07:48,310
Speaker 5:  attackers even used the service that's not alleged in. So is

116
00:07:48,310 --> 00:07:52,190
Speaker 5:  it like super broad then? Yeah. It's basically like you contributed to

117
00:07:52,330 --> 00:07:55,990
Speaker 5:  the existence of ISIS as an enterprise and therefore you're responsible for

118
00:07:56,420 --> 00:08:00,350
Speaker 4:  This is what I mean when I say like many smart people are like, why did

119
00:08:00,350 --> 00:08:01,870
Speaker 4:  these cases end up at the Supreme Court?

120
00:08:02,190 --> 00:08:05,830
Speaker 5:  And even the Supreme Court justices seemed maybe not to fully understand

121
00:08:05,830 --> 00:08:07,590
Speaker 5:  what they had done past

122
00:08:07,590 --> 00:08:08,790
Speaker 4:  Years, why they accepted the

123
00:08:08,790 --> 00:08:12,230
Speaker 5:  Cases, the level of confusion that they expressed, which was partly the plaintiff's

124
00:08:12,230 --> 00:08:12,790
Speaker 5:  lawyers fault.

125
00:08:12,910 --> 00:08:16,630
Speaker 6:  Confusion is often Supreme Court justices saying, wow, you,

126
00:08:16,630 --> 00:08:17,550
Speaker 6:  you suck. Yeah.

127
00:08:17,550 --> 00:08:18,110
Speaker 4:  Until

128
00:08:18,110 --> 00:08:18,830
Speaker 6:  Years as far.

129
00:08:19,280 --> 00:08:23,150
Speaker 4:  So I, I just wanna put out the stakes here, right? So if

130
00:08:23,300 --> 00:08:27,190
Speaker 4:  Google loses the Google case in the worst possible way and the

131
00:08:27,190 --> 00:08:31,110
Speaker 4:  Supreme Court says, alright, your recommendation system

132
00:08:31,440 --> 00:08:35,350
Speaker 4:  on a platform is a decision you have made

133
00:08:35,350 --> 00:08:39,070
Speaker 4:  that creates a new content that you are responsible for, that means platforms

134
00:08:39,070 --> 00:08:41,270
Speaker 4:  will become liable for what they're

135
00:08:41,300 --> 00:08:42,150
Speaker 6:  Algorithms,

136
00:08:42,150 --> 00:08:46,070
Speaker 4:  Algorithms do. And that is insane. Like just fully,

137
00:08:46,070 --> 00:08:49,830
Speaker 4:  like there's no way to make a user maybe anything at

138
00:08:49,830 --> 00:08:53,030
Speaker 4:  scale without some sort of sorting algorithm

139
00:08:53,030 --> 00:08:55,440
Speaker 5:  In the mix. Google's council is actually really funny on this this cuz it

140
00:08:55,440 --> 00:08:59,240
Speaker 5:  she, she said both like simultaneously in a

141
00:08:59,240 --> 00:09:02,280
Speaker 5:  charitable, in a very arrogant way. Well Google may will survive,

142
00:09:03,500 --> 00:09:05,680
Speaker 5:  but everyone else is, is gonna die.

143
00:09:05,830 --> 00:09:09,680
Speaker 4:  Yeah. Yeah. Because Google got the money, they got

144
00:09:09,680 --> 00:09:11,800
Speaker 5:  The back. At least they know. At least they know. But that's

145
00:09:11,800 --> 00:09:14,880
Speaker 4:  The, these are the stakes, right. Algorithms become illegal on the internet.

146
00:09:15,280 --> 00:09:19,240
Speaker 4:  Right. And like at some level, everything that happens on

147
00:09:19,240 --> 00:09:22,120
Speaker 4:  a computer is an exercise in sorting and prioritization.

148
00:09:22,600 --> 00:09:25,960
Speaker 6:  Right. Which Atlantic Kagan said, which was a really great moment in this.

149
00:09:26,210 --> 00:09:29,440
Speaker 4:  Oh right. I wanna come to the fact that the courts seemed to understand

150
00:09:30,110 --> 00:09:34,000
Speaker 4:  what the stakes were, which is very, like all of us were worried.

151
00:09:34,690 --> 00:09:37,980
Speaker 5:  Yeah. On both sides. They seem to understand what the stakes are.

152
00:09:38,070 --> 00:09:41,980
Speaker 4:  So the Google case algorithms illegal question mark. Right. Those

153
00:09:41,980 --> 00:09:45,620
Speaker 4:  are the stakes or what are the limits of an algorithm? Like any sort of

154
00:09:45,620 --> 00:09:48,180
Speaker 4:  algorithmic system that makes reliable of them. Yeah. And you get the sense

155
00:09:48,490 --> 00:09:52,340
Speaker 4:  that like the, these cases are outta the Supreme Court cuz there's isis

156
00:09:52,640 --> 00:09:56,500
Speaker 4:  and so like everyone can agree ISIS is bad. Yeah. Right. And you

157
00:09:56,500 --> 00:10:00,340
Speaker 4:  kind of like start from there and that's like maybe the only

158
00:10:00,340 --> 00:10:04,300
Speaker 4:  thing in America left that we can all agree is bad. Right. Like you,

159
00:10:04,320 --> 00:10:08,140
Speaker 4:  you could, if you were like, there's tweets about the Russian war in Ukraine,

160
00:10:08,250 --> 00:10:11,980
Speaker 4:  like some percentage of Americans be like, I don't know. That's fine. Yeah.

161
00:10:11,980 --> 00:10:15,740
Speaker 4:  Right. ISIS is the one where like, yep. Bad across the board, bad

162
00:10:15,740 --> 00:10:16,500
Speaker 5:  Noah liked it.

163
00:10:16,500 --> 00:10:19,420
Speaker 6:  So that's a good, I mean just maybe this is clear to everyone, but on a technical

164
00:10:19,420 --> 00:10:22,820
Speaker 6:  level it's also that there are basically sanctions against isis. Right? That

165
00:10:22,820 --> 00:10:26,460
Speaker 6:  that is, this is the rare example where there is a group where it is literally

166
00:10:26,460 --> 00:10:27,660
Speaker 6:  illegal to deal with

167
00:10:27,860 --> 00:10:30,860
Speaker 4:  'em. Yeah. So, and and that brings me to the Twitter case where the stakes

168
00:10:30,860 --> 00:10:34,420
Speaker 4:  are, do you have to be perfect in offering your

169
00:10:34,950 --> 00:10:36,660
Speaker 4:  at scale internet service?

170
00:10:36,770 --> 00:10:40,540
Speaker 5:  Yeah. Well interestingly, even in Gonzalez versus Google,

171
00:10:40,960 --> 00:10:44,780
Speaker 5:  the justices seemed to, I think by way of understanding what the

172
00:10:44,780 --> 00:10:47,740
Speaker 5:  stakes were, maybe some of them suddenly understanding what the stakes are.

173
00:10:47,990 --> 00:10:50,780
Speaker 5:  It seemed like they were trying to give themselves a little bit of an escape

174
00:10:50,780 --> 00:10:53,900
Speaker 5:  hatch. And Justice Barrett at some point suggested they might not have to

175
00:10:53,900 --> 00:10:57,380
Speaker 5:  decide the Google case at all. Because if the

176
00:10:57,700 --> 00:11:01,380
Speaker 5:  Tamana case fails, that means there's

177
00:11:01,380 --> 00:11:04,980
Speaker 5:  two 30 is moot the immunity, what would the immunity even apply to? Cause

178
00:11:04,980 --> 00:11:08,950
Speaker 5:  there's no liability for the terrorism stuff. So they

179
00:11:08,950 --> 00:11:10,470
Speaker 5:  could just say

180
00:11:11,070 --> 00:11:11,870
Speaker 4:  We, sorry.

181
00:11:11,900 --> 00:11:14,430
Speaker 5:  They could just say, we don't wanna deal with this since then, you know,

182
00:11:14,550 --> 00:11:18,150
Speaker 5:  after they've discovered now that they've opened up a big world. So

183
00:11:18,150 --> 00:11:20,550
Speaker 1:  They really can just be like, nevermind.

184
00:11:21,100 --> 00:11:24,590
Speaker 4:  Well, so we'll see. I just wanted, I wanted to start here with these very

185
00:11:24,590 --> 00:11:26,790
Speaker 4:  basic stake. Yeah. Here are the facts of the case. Here's what happened,

186
00:11:26,790 --> 00:11:30,110
Speaker 4:  here's here's why we're in front of the Supreme Court. Yeah. And then what

187
00:11:30,110 --> 00:11:33,550
Speaker 4:  happens if things go wrong? And what happens if things go wrong are

188
00:11:33,710 --> 00:11:37,310
Speaker 4:  basically every platform has to stop operating

189
00:11:37,600 --> 00:11:41,230
Speaker 4:  because there's just no way to, to open up a user account

190
00:11:41,230 --> 00:11:45,110
Speaker 4:  signup form and guarantee that ISIS won't

191
00:11:45,110 --> 00:11:47,070
Speaker 4:  use it. Right. Like you, you

192
00:11:47,070 --> 00:11:48,750
Speaker 1:  Can't, are you in ISIS checkings

193
00:11:48,750 --> 00:11:52,630
Speaker 4:  Or No? Like you have to do some work. And most

194
00:11:52,870 --> 00:11:56,860
Speaker 4:  platforms it's illegal to deal with them. So they do do some work to make

195
00:11:56,860 --> 00:12:00,340
Speaker 4:  sure that platforms aren't being used in this way. Yeah. But if the

196
00:12:00,340 --> 00:12:04,220
Speaker 4:  Supreme Court is like anything, like they sign up for Twitter and

197
00:12:04,220 --> 00:12:08,100
Speaker 4:  now Twitter is liable for an act of terrorism right down

198
00:12:08,100 --> 00:12:11,860
Speaker 4:  the line. Like that just changes the entire way that

199
00:12:11,970 --> 00:12:13,580
Speaker 4:  user accounts work on the

200
00:12:13,580 --> 00:12:17,140
Speaker 5:  Well they can, they can choose to get really stupid with it and be like,

201
00:12:17,200 --> 00:12:20,980
Speaker 5:  you cannot create thumbnails of videos anymore. Yeah. Because they may

202
00:12:20,980 --> 00:12:24,820
Speaker 5:  lead to terrorist content while still like having Right. So other things

203
00:12:24,820 --> 00:12:25,380
Speaker 5:  be functional.

204
00:12:25,380 --> 00:12:29,220
Speaker 4:  So I just wanna Those are the stakes. Yeah. Right. One is you have to be

205
00:12:29,220 --> 00:12:32,740
Speaker 4:  perfect invalidating all the users in your platform. And the other one is

206
00:12:32,740 --> 00:12:33,460
Speaker 1:  Algorithms

207
00:12:33,460 --> 00:12:37,380
Speaker 4:  Go by your algorithm recommendation algorithms make you liable for the

208
00:12:37,380 --> 00:12:41,340
Speaker 4:  content of the video, even if there's no, and I this

209
00:12:41,340 --> 00:12:45,180
Speaker 4:  is true. Right? There was no direct evidence that's showing anyone

210
00:12:45,180 --> 00:12:47,260
Speaker 4:  any videos on YouTube led to terrorist sex.

211
00:12:47,590 --> 00:12:51,140
Speaker 1:  So I have a question about that particular case and I would love for you

212
00:12:51,140 --> 00:12:54,900
Speaker 1:  guys to help my brain understand that. So they're saying like

213
00:12:55,020 --> 00:12:58,620
Speaker 1:  basically the, the, the potential stakes here are any algorithm that does

214
00:12:58,620 --> 00:13:02,020
Speaker 1:  recommendations is gone, which would mean like seo,

215
00:13:02,090 --> 00:13:03,620
Speaker 1:  Google searching,

216
00:13:04,010 --> 00:13:07,900
Speaker 4:  Google's argument. Right? This was not Addie, this was not supposed to

217
00:13:07,900 --> 00:13:10,580
Speaker 4:  be the plaintiff's argument. Right. Like the, because

218
00:13:10,600 --> 00:13:14,500
Speaker 1:  Theoretically I'd be like, okay, if I'm watching YouTube and YouTube's algorithm

219
00:13:14,500 --> 00:13:17,140
Speaker 1:  is like here's an ISIS video and I go join this isis,

220
00:13:17,140 --> 00:13:20,380
Speaker 4:  Well now, now we're just gonna talk about how how bad these, the Casey seemed

221
00:13:20,380 --> 00:13:24,060
Speaker 4:  to go and how surprisingly this is. TC mentioned this how surprisingly

222
00:13:24,060 --> 00:13:26,900
Speaker 4:  intelligent the Supreme Court seemed about the stakes. Okay. What they were

223
00:13:26,900 --> 00:13:30,660
Speaker 4:  early on. So what you would want, just in an abstract way, you're a lawyer

224
00:13:30,660 --> 00:13:34,260
Speaker 4:  in front of the Supreme Court, you're arguing in your position, you're kind

225
00:13:34,260 --> 00:13:37,780
Speaker 4:  of dancing into I want the narrowest interpretation of this thing

226
00:13:38,340 --> 00:13:41,780
Speaker 4:  because that is the most likely outcome. Yeah. Right. Like every

227
00:13:41,780 --> 00:13:45,140
Speaker 4:  Supreme Court opinion is like literally increasingly like this is a narrow,

228
00:13:45,140 --> 00:13:47,740
Speaker 4:  like they pat themselves in the back, they're like, we threaded the needle.

229
00:13:47,740 --> 00:13:50,860
Speaker 4:  We did it once again, the Supreme Court has saved the day in America right

230
00:13:51,730 --> 00:13:55,420
Speaker 4:  here. It, the dude blew it. Right. Addie, they were like,

231
00:13:55,420 --> 00:13:57,860
Speaker 4:  here's a narrow interpretation of this. And he is like, I don't know, maybe

232
00:13:57,860 --> 00:13:58,940
Speaker 4:  Google search should be legal.

233
00:13:59,320 --> 00:14:03,100
Speaker 6:  Yes. People just like justices just kept asking him things

234
00:14:03,720 --> 00:14:07,660
Speaker 6:  and they were just clearly like trying to get him to draw any

235
00:14:07,660 --> 00:14:11,300
Speaker 6:  level of lines that would not just blow up the internet.

236
00:14:12,370 --> 00:14:15,980
Speaker 6:  Like, okay, well what if you know thumbnails? What if the, instead of

237
00:14:15,980 --> 00:14:19,060
Speaker 6:  thumbnails you posted screenshot, he is like, eh. Yeah. I mean maybe

238
00:14:19,830 --> 00:14:23,620
Speaker 6:  there's a Google search question that I think this was

239
00:14:23,620 --> 00:14:26,900
Speaker 6:  Kagan again who was talking about, okay, well

240
00:14:27,190 --> 00:14:30,540
Speaker 6:  there's search. You're clearly if you have a top result, maybe you're saying

241
00:14:30,540 --> 00:14:34,300
Speaker 6:  this is really good and he just would not say, would not draw any of those

242
00:14:34,300 --> 00:14:34,660
Speaker 6:  lines.

243
00:14:35,650 --> 00:14:36,620
Speaker 1:  I love this. Yeah.

244
00:14:36,620 --> 00:14:40,420
Speaker 5:  Yeah. He was really incapable of like defining even fundamental things

245
00:14:40,420 --> 00:14:44,340
Speaker 5:  about how these systems work. Which makes it hard to then figure

246
00:14:44,340 --> 00:14:45,060
Speaker 5:  out how to,

247
00:14:45,160 --> 00:14:48,100
Speaker 1:  Was that like that was his point though, right? Like to just

248
00:14:48,210 --> 00:14:51,880
Speaker 5:  Well, no, because they'll, the whole point is trying to define

249
00:14:52,220 --> 00:14:55,880
Speaker 5:  how the service works. If your goal is show that the search

250
00:14:56,120 --> 00:14:59,800
Speaker 5:  provider or YouTube or something is, is creating speech, right? Like that's

251
00:14:59,800 --> 00:15:03,600
Speaker 5:  the whole point is if you're, or or the attack on two 30 is if

252
00:15:03,600 --> 00:15:06,200
Speaker 5:  you're recommending something to someone or you're like putting in a box

253
00:15:06,200 --> 00:15:09,640
Speaker 5:  or selecting a thumbnail, you're making some kind of judgment or pushing

254
00:15:09,640 --> 00:15:10,720
Speaker 5:  someone in front of people.

255
00:15:11,190 --> 00:15:14,880
Speaker 6:  I should also clarify on the thumb thumbnail thing, it is so much weirder

256
00:15:14,880 --> 00:15:18,760
Speaker 6:  than it sounds as anyone who has made a YouTube video knows YouTube

257
00:15:18,760 --> 00:15:22,240
Speaker 6:  does not choose a thumbnail for you. I have made enough thumbnail selections

258
00:15:23,160 --> 00:15:26,920
Speaker 6:  to know that's not true. And so the way that he's defining thumbnail is basically

259
00:15:26,940 --> 00:15:30,880
Speaker 6:  the entire box that shows up when Google or when

260
00:15:30,880 --> 00:15:34,840
Speaker 6:  YouTube recommends something, which is the actual image people

261
00:15:34,840 --> 00:15:38,280
Speaker 6:  call a thumbnail. And then the url, the idea that

262
00:15:38,470 --> 00:15:42,160
Speaker 6:  URLs are creating a specific new kind of content is

263
00:15:42,190 --> 00:15:46,120
Speaker 6:  a thing that has been a running argument in the Gonzalez case. And

264
00:15:46,120 --> 00:15:49,920
Speaker 6:  it is just hard to overstate how disruptive that would be. That if you

265
00:15:50,240 --> 00:15:53,520
Speaker 6:  generated in theory, like if you generated a link shortener to something

266
00:15:53,520 --> 00:15:57,000
Speaker 6:  or you just generated any kind of web address, then suddenly

267
00:15:57,580 --> 00:16:01,520
Speaker 6:  you are the provider of and speaker of that content. Which is just

268
00:16:01,520 --> 00:16:03,760
Speaker 6:  Yeah. A wild way to organize the web.

269
00:16:03,910 --> 00:16:07,120
Speaker 4:  When I say that I stare into the night sky wondering why the Supreme Court

270
00:16:07,120 --> 00:16:10,360
Speaker 4:  took this case. It's this stuff. Yeah. Right. Where it's like,

271
00:16:11,210 --> 00:16:15,040
Speaker 4:  oh, maybe you've got a fact pattern. Right. Which is ISIS

272
00:16:15,040 --> 00:16:18,920
Speaker 4:  exists, they used your service to recruit, some people saw

273
00:16:18,920 --> 00:16:22,520
Speaker 4:  the videos and they went off and did terrorism. Right. And like

274
00:16:22,690 --> 00:16:26,320
Speaker 4:  in America in 2023, that might be the only fact pattern

275
00:16:26,320 --> 00:16:29,000
Speaker 4:  where everyone agrees that's bad. Yeah.

276
00:16:29,000 --> 00:16:32,160
Speaker 5:  And there that's, it's, it's especially tragic because there have been so

277
00:16:32,160 --> 00:16:36,000
Speaker 5:  many other good cases that have, you know, well none that have reached the

278
00:16:36,000 --> 00:16:39,880
Speaker 5:  Supreme Court for two 30, but like Snap got sued because Right. They did

279
00:16:39,880 --> 00:16:43,640
Speaker 5:  that thing where they, they created a function that showed you how

280
00:16:43,640 --> 00:16:47,000
Speaker 5:  fast you were going and some kids got killed because they were

281
00:16:47,070 --> 00:16:50,080
Speaker 5:  trying to go in real fast, go as fast as they could because of the thing

282
00:16:50,080 --> 00:16:52,080
Speaker 5:  that Snap created. That wasn't something that,

283
00:16:52,220 --> 00:16:56,080
Speaker 4:  The one that I always think about is Herrick fee grinder. Right.

284
00:16:56,080 --> 00:16:57,240
Speaker 4:  Addie is not the caption,

285
00:16:57,380 --> 00:16:58,800
Speaker 6:  The harassment grinder. Yeah.

286
00:16:58,800 --> 00:17:02,400
Speaker 4:  Yeah. Where Grinder being used to harass someone and they get Grinder got

287
00:17:02,400 --> 00:17:05,880
Speaker 4:  sued not on two 30 grounds, but on product liability grounds, your product

288
00:17:05,930 --> 00:17:08,880
Speaker 4:  is helping this harassment occur. Yeah. And they lost because of two 30.

289
00:17:08,880 --> 00:17:09,240
Speaker 4:  But the

290
00:17:09,240 --> 00:17:09,760
Speaker 6:  Google thing is,

291
00:17:10,260 --> 00:17:13,880
Speaker 4:  But like, but like what we're saying is like those cases, like those things,

292
00:17:14,130 --> 00:17:18,000
Speaker 4:  they're like rich fact patterns where you can say, okay,

293
00:17:18,000 --> 00:17:21,400
Speaker 4:  we can dive into the difference between speech and conduct. Right. We can

294
00:17:21,400 --> 00:17:25,360
Speaker 4:  say the design of this product has choices embedded in it that lead

295
00:17:25,360 --> 00:17:29,280
Speaker 4:  to these outcomes. They just never made it here. Instead

296
00:17:29,280 --> 00:17:33,240
Speaker 4:  this goofball set of facts made it here. Right. Where there's not

297
00:17:33,240 --> 00:17:36,440
Speaker 4:  even a direct connection. Like

298
00:17:36,440 --> 00:17:37,800
Speaker 6:  There's not a specific video.

299
00:17:37,890 --> 00:17:41,880
Speaker 4:  It was, yeah. It wasn't this terrorist saw this video and went and did this

300
00:17:41,880 --> 00:17:43,040
Speaker 4:  act of terrorism. Because

301
00:17:43,040 --> 00:17:43,320
Speaker 6:  That would be

302
00:17:43,400 --> 00:17:47,050
Speaker 4:  Clear. Generally people have used this platform,

303
00:17:47,520 --> 00:17:51,160
Speaker 4:  some of them were terrorists and some of them maybe

304
00:17:51,740 --> 00:17:55,400
Speaker 4:  are the ones who did the terrorists. There's like, like not a direct connection

305
00:17:55,400 --> 00:17:55,560
Speaker 4:  there.

306
00:17:55,590 --> 00:17:59,520
Speaker 1:  Yeah. Cuz then like if this one goes through, then everyone

307
00:17:59,520 --> 00:18:03,120
Speaker 1:  who's ever lost someone to QAN on can go ensue Google.

308
00:18:03,170 --> 00:18:05,200
Speaker 5:  Oh yeah. That's another consequence. They don't have to have

309
00:18:05,200 --> 00:18:08,800
Speaker 6:  They. They can't. Yeah. Oh they can't. So just to be clear, this is not any

310
00:18:08,800 --> 00:18:12,320
Speaker 6:  extremist group that uses YouTube could suddenly produce this host of

311
00:18:12,320 --> 00:18:16,280
Speaker 6:  lawsuits, ISIS and their sort of other specific foreign

312
00:18:16,680 --> 00:18:19,960
Speaker 6:  terrorist groups. Think of it like the equivalent of sanctioning a country.

313
00:18:20,070 --> 00:18:23,680
Speaker 6:  Okay. So that's what makes this illegal speech. It's, this is really sort

314
00:18:23,680 --> 00:18:27,400
Speaker 6:  of, it is a narrow subset even though it does introduce all of these

315
00:18:27,640 --> 00:18:30,840
Speaker 6:  big liability questions. And this is all, all this stuff we've been talking

316
00:18:30,840 --> 00:18:34,680
Speaker 6:  about is what tamay hinges on a lot. Which is do you

317
00:18:34,680 --> 00:18:38,240
Speaker 6:  have to have an actual connection to a specific act of terrorism or is the

318
00:18:38,430 --> 00:18:42,240
Speaker 6:  enough that you are providing a megaphone that lets

319
00:18:42,240 --> 00:18:46,040
Speaker 6:  ISIS recruit. Okay. So this is all at least someone unsettled.

320
00:18:46,040 --> 00:18:47,080
Speaker 4:  Yeah. And and

321
00:18:47,080 --> 00:18:50,600
Speaker 5:  Like the speech arena, it's even weirder because with the foreign terrorist

322
00:18:50,600 --> 00:18:53,840
Speaker 5:  organizations, it's not about, it's not really the speech itself that's illegal.

323
00:18:53,840 --> 00:18:57,440
Speaker 5:  It's the person who's saying it. Like cuz they've banned these organizations

324
00:18:57,440 --> 00:19:01,200
Speaker 4:  From Yeah, well I'm just saying in the United States there is

325
00:19:01,200 --> 00:19:05,080
Speaker 4:  vanishing little speech that is illegal and ISIS is some of that

326
00:19:05,080 --> 00:19:08,960
Speaker 4:  speech. Right. And that's and that. But it opens the door to, okay, we

327
00:19:08,960 --> 00:19:12,800
Speaker 4:  have now said YouTube's recommendation system

328
00:19:12,800 --> 00:19:16,120
Speaker 4:  gets rid of two 30 cuz it is YouTube making some sort of direct editorial

329
00:19:16,120 --> 00:19:19,520
Speaker 4:  choice that exposes them to liability. And the thing they have chosen to

330
00:19:19,520 --> 00:19:23,080
Speaker 4:  do is promote the illegal speech of isis. Yeah. If you can get

331
00:19:23,130 --> 00:19:26,520
Speaker 4:  there, there's no reason you can't replace the illegal speech of ISIS with

332
00:19:26,520 --> 00:19:29,920
Speaker 4:  this illegal defamatory speech about my

333
00:19:30,030 --> 00:19:32,960
Speaker 4:  company. Yeah. Right. And then maybe you'll win and maybe you won't, but

334
00:19:32,960 --> 00:19:34,520
Speaker 4:  that door right now is slam shut

335
00:19:34,520 --> 00:19:35,520
Speaker 1:  And this just cracks

336
00:19:35,520 --> 00:19:39,400
Speaker 4:  It open. It just cracks it open to say the recommendations are create

337
00:19:39,400 --> 00:19:43,320
Speaker 4:  liability for YouTube for the content of the speech. Okay. Even though the

338
00:19:43,320 --> 00:19:46,840
Speaker 4:  speech is like a bunch of thumbnails in a url. Yeah. And you just see like,

339
00:19:46,840 --> 00:19:48,200
Speaker 4:  oh, this is not, I

340
00:19:48,200 --> 00:19:52,040
Speaker 5:  Went for the record, like to make the shocked face thumbnails illegal speech.

341
00:19:52,890 --> 00:19:56,680
Speaker 4:  So the, the again, we're we're kind of still, you can see like these cases

342
00:19:56,680 --> 00:19:59,400
Speaker 4:  are just weird. Yeah. But we're still just talking about like

343
00:19:59,400 --> 00:20:03,200
Speaker 6:  Justice. The spirit also raised the question of section two 30 also

344
00:20:03,200 --> 00:20:07,040
Speaker 6:  protects users. So if you hit thumbs up on a YouTube video, is that

345
00:20:07,040 --> 00:20:09,720
Speaker 6:  you then trying to promote and repeat the speech too?

346
00:20:11,250 --> 00:20:12,200
Speaker 6:  Oh, that's great.

347
00:20:12,200 --> 00:20:14,680
Speaker 4:  So this is what I wanna get to. We've now, we've mostly been talking about

348
00:20:14,680 --> 00:20:18,240
Speaker 4:  how weird the cases are. Yeah. But you two actually listened to the arguments

349
00:20:18,240 --> 00:20:21,640
Speaker 4:  for six hours and add this point from Justice Barrett,

350
00:20:21,700 --> 00:20:25,560
Speaker 4:  surprisingly cogent. Right. Vince is an understanding about

351
00:20:25,560 --> 00:20:29,440
Speaker 4:  YouTube works, which I feel like most people who

352
00:20:29,440 --> 00:20:33,400
Speaker 4:  have phones have encountered YouTube, but you cannot take for

353
00:20:33,400 --> 00:20:36,520
Speaker 4:  granted our court system's ability to understand internet platforms. Yeah.

354
00:20:36,840 --> 00:20:40,500
Speaker 4:  But it seemed like at least in the Google case, there was an

355
00:20:40,500 --> 00:20:43,940
Speaker 4:  understanding and not only the stakes, but what questions they should ask

356
00:20:43,940 --> 00:20:47,590
Speaker 4:  and what the answers at least should be. Which I found surprising.

357
00:20:47,980 --> 00:20:50,110
Speaker 6:  I was surprised and relieved. Okay.

358
00:20:50,780 --> 00:20:51,790
Speaker 4:  What stood out to

359
00:20:51,790 --> 00:20:55,710
Speaker 6:  You? I think that they just went through and asked a bunch of

360
00:20:55,710 --> 00:20:59,270
Speaker 6:  the questions we have been asking at just the broadest level that

361
00:20:59,460 --> 00:21:03,350
Speaker 6:  Clarence Thomas opens up with. Okay. So is there an actual

362
00:21:03,350 --> 00:21:07,270
Speaker 6:  YouTube algorithm that promotes terrorism? Or is there just a single large

363
00:21:07,270 --> 00:21:11,230
Speaker 6:  recommendation algorithm that happens to give you what you

364
00:21:11,230 --> 00:21:14,870
Speaker 6:  want? And maybe some of that is terrorism, his example used Bryce Poff.

365
00:21:15,650 --> 00:21:19,310
Speaker 6:  But I think that there are all these questions that are actually digging

366
00:21:19,310 --> 00:21:23,150
Speaker 6:  into, first of all how the services work and not just taking it

367
00:21:23,150 --> 00:21:26,550
Speaker 6:  for granted that there is something called the algorithm that is evil.

368
00:21:27,270 --> 00:21:31,190
Speaker 6:  They recognize computers have to sort information somehow. And so

369
00:21:31,190 --> 00:21:34,590
Speaker 6:  you're trying to make these very fine green distinctions. Second of all,

370
00:21:34,590 --> 00:21:38,470
Speaker 6:  there's a real understanding that there are a bunch of services that

371
00:21:38,470 --> 00:21:42,390
Speaker 6:  do rely on these things and these services have become enmeshed in our lives

372
00:21:42,810 --> 00:21:46,750
Speaker 6:  and any decision is going to affect them greatly. But Kavanaugh

373
00:21:46,750 --> 00:21:50,390
Speaker 6:  made a really big deal out of, okay, what are the economic implications of

374
00:21:50,390 --> 00:21:53,550
Speaker 6:  this? Is this going to tear down a huge section of the internet while we're

375
00:21:53,550 --> 00:21:56,910
Speaker 6:  trying to figure out what the law is? The third part is just that there were

376
00:21:56,910 --> 00:22:00,750
Speaker 6:  these weird little moments where they actually recognized things that I think

377
00:22:00,750 --> 00:22:04,670
Speaker 6:  just many average people wouldn't, which is things like ai, text generation

378
00:22:04,970 --> 00:22:08,630
Speaker 6:  and images that they were talking about algorithms and can algorithms produce

379
00:22:08,630 --> 00:22:12,350
Speaker 6:  content justice? Corset says, okay, yeah, well there are these

380
00:22:12,350 --> 00:22:15,990
Speaker 6:  tools that can make images and texts so clearly the algorithms can

381
00:22:16,230 --> 00:22:19,350
Speaker 6:  generate things and that opens up this whole other can of worms. It was just,

382
00:22:19,480 --> 00:22:23,230
Speaker 6:  it was smart, but none of which means they'll rule in a way that I like.

383
00:22:23,930 --> 00:22:27,670
Speaker 6:  But it was just a far cry from a bunch of the really bizarre,

384
00:22:28,060 --> 00:22:31,430
Speaker 6:  sweeping, clearly politically motivated

385
00:22:31,440 --> 00:22:35,350
Speaker 6:  rulings on opinions that we have gotten from a lot of courts,

386
00:22:35,350 --> 00:22:37,190
Speaker 6:  including the Supreme Court sometimes,

387
00:22:37,540 --> 00:22:41,270
Speaker 4:  Well in particular there has been right wing

388
00:22:41,460 --> 00:22:45,070
Speaker 4:  sort of animus towards big tech and the algorithm in free speech and

389
00:22:45,070 --> 00:22:48,870
Speaker 4:  censorship. We have seen Clarence Thomas, right. In dissent,

390
00:22:48,960 --> 00:22:52,430
Speaker 4:  we should reevaluate two 30, we should make these platforms common

391
00:22:52,430 --> 00:22:55,590
Speaker 4:  carriers. None of that appeared to come up in this argument.

392
00:22:55,660 --> 00:22:59,350
Speaker 6:  Yeah. There was also surprisingly little, and I wish I, I think, I mean almost

393
00:22:59,350 --> 00:23:03,150
Speaker 6:  none of the sort of, this is big tech, clearly we must stop protecting

394
00:23:03,150 --> 00:23:03,670
Speaker 6:  big tech.

395
00:23:03,740 --> 00:23:07,310
Speaker 5:  Yeah. But that's also coming up later this year with the net choice suits

396
00:23:07,310 --> 00:23:09,390
Speaker 4:  With the moderation suits the moderations in Florida,

397
00:23:09,790 --> 00:23:11,270
Speaker 6:  Which haven't been taken up yet.

398
00:23:11,270 --> 00:23:15,070
Speaker 5:  Yeah. But the common carrier stuff is insanely argued

399
00:23:15,070 --> 00:23:17,430
Speaker 5:  and I think at least the Texas one. And that's,

400
00:23:17,650 --> 00:23:19,750
Speaker 6:  And those are both in state courts?

401
00:23:19,750 --> 00:23:22,070
Speaker 5:  Those are, yeah, those are like states trying to

402
00:23:22,400 --> 00:23:24,790
Speaker 4:  No, those are in federal courts. They're in federal court. Yeah.

403
00:23:24,970 --> 00:23:28,550
Speaker 6:  The net choice cases went through appeals courts and then the Supreme Court

404
00:23:28,550 --> 00:23:31,990
Speaker 6:  asked for the Biden administration to submit its opinion on them basically.

405
00:23:31,990 --> 00:23:35,870
Speaker 6:  And so it seems very likely they're gonna get taken up, but they're not currently

406
00:23:35,870 --> 00:23:36,830
Speaker 6:  in play. Okay.

407
00:23:36,830 --> 00:23:39,750
Speaker 4:  But, and those are just like kind of like dead ahead First amendment cases.

408
00:23:39,820 --> 00:23:43,470
Speaker 4:  Yeah. Like it's, should this government make speech regulations?

409
00:23:43,720 --> 00:23:47,440
Speaker 4:  No. The, the answer should be no. But like we live in a topsy-turvy time

410
00:23:47,440 --> 00:23:48,720
Speaker 4:  in America, we,

411
00:23:48,720 --> 00:23:52,080
Speaker 1:  You never know what what should actually be killed because somebody really

412
00:23:52,080 --> 00:23:52,600
Speaker 1:  wants it to be

413
00:23:52,600 --> 00:23:55,360
Speaker 4:  Killed. All right. So we talked about the argument in the Gonzalez case,

414
00:23:55,360 --> 00:23:58,120
Speaker 4:  which is how does a recommendation algorithm work? Are you liable for these

415
00:23:58,120 --> 00:23:59,720
Speaker 4:  thumbnails? What was the argument in the Twitter case like

416
00:24:00,300 --> 00:24:04,230
Speaker 6:  Much more metaphor heavy, which is one of my

417
00:24:04,230 --> 00:24:06,830
Speaker 6:  least favorite types of Supreme court hearing.

418
00:24:07,710 --> 00:24:11,510
Speaker 6:  There's just a huge amount of, it turns on. Okay, so there is this

419
00:24:11,510 --> 00:24:15,430
Speaker 6:  test that defines what aiding and abetting is for

420
00:24:15,430 --> 00:24:18,550
Speaker 6:  specifically civil cases, like the kinds of lawsuits that are getting brought

421
00:24:18,550 --> 00:24:22,390
Speaker 6:  here. And part of it hinges on, all right, well what does

422
00:24:22,390 --> 00:24:26,270
Speaker 6:  it mean for Twitter to have known, does Twitter have

423
00:24:26,270 --> 00:24:29,630
Speaker 6:  to have known that there was this really specific act that was being planned

424
00:24:29,630 --> 00:24:33,070
Speaker 6:  and that ISIS was planning it and that there were these specific ISIS accounts

425
00:24:33,070 --> 00:24:36,870
Speaker 6:  that they had the band to stop recruiting? Or did it just have to

426
00:24:37,120 --> 00:24:40,430
Speaker 6:  vaguely know that there were terrorists on the platform? And then there's

427
00:24:40,430 --> 00:24:44,310
Speaker 6:  also, alright, how much of a specific

428
00:24:44,310 --> 00:24:48,150
Speaker 6:  relationship do they have with these terrorists? Is this like going into

429
00:24:48,150 --> 00:24:52,070
Speaker 6:  a bank? Is it like selling them a gun? What other is it

430
00:24:52,070 --> 00:24:55,390
Speaker 6:  like stealing sheep? They're just a million different metaphors. Stealing,

431
00:24:55,390 --> 00:24:58,350
Speaker 6:  play, stealing sheep. Yes. This was one of the metaphors

432
00:24:58,510 --> 00:25:01,430
Speaker 1:  Who's the farmer on, on the court on the bench.

433
00:25:02,350 --> 00:25:06,090
Speaker 6:  The opening metaphor in the case was, is this more like

434
00:25:06,180 --> 00:25:10,090
Speaker 6:  if you have a friend who's a murderer and a thief and you give them

435
00:25:10,090 --> 00:25:14,050
Speaker 6:  a gun but you just happen to not know exactly when they'll use the gun. Or

436
00:25:14,050 --> 00:25:17,370
Speaker 6:  is it you doing something like opening a gate

437
00:25:17,710 --> 00:25:21,130
Speaker 6:  and then it turns out that the person you're opening the gate for is trying

438
00:25:21,130 --> 00:25:24,330
Speaker 6:  to steal sheep. So that's like the level of argumentation we're talking about.

439
00:25:24,840 --> 00:25:28,730
Speaker 4:  Wait, actually someone trying to steal your sheep and all you have

440
00:25:28,730 --> 00:25:32,410
Speaker 4:  is a gun is fairly close to the average Twitter day,

441
00:25:32,410 --> 00:25:36,050
Speaker 4:  right? It's like, I don't know, I, I've gotta protect this situation.

442
00:25:36,800 --> 00:25:40,410
Speaker 4:  It's like everyone's panicking. There's a sheep thief. I'm just

443
00:25:40,410 --> 00:25:43,010
Speaker 4:  firing indiscriminately. Stealing your neighbor's eggs would've been the

444
00:25:43,010 --> 00:25:45,530
Speaker 4:  better manner. Tuesday on, there's 30 to 40 feral bags.

445
00:25:47,070 --> 00:25:50,810
Speaker 6:  The best pick that I've heard on this from Blake Reed from

446
00:25:50,920 --> 00:25:54,650
Speaker 6:  University of Colorado Boulder is that this is an example

447
00:25:54,650 --> 00:25:58,450
Speaker 6:  of how hard things are gonna get if two 30 isn't around. That you have all

448
00:25:58,450 --> 00:26:02,210
Speaker 6:  these really super fine lines about what counts as aiding and abetting. What

449
00:26:02,210 --> 00:26:05,570
Speaker 6:  counts as something that's really valuable to Twitter, really valuable to

450
00:26:05,570 --> 00:26:09,330
Speaker 6:  terrorists. What does it mean to actually know what's

451
00:26:09,330 --> 00:26:13,090
Speaker 6:  happening on your platform? That right now section two 30 means you

452
00:26:13,090 --> 00:26:16,330
Speaker 6:  don't really have to deal with these things if it goes away. Suddenly you're

453
00:26:16,330 --> 00:26:20,210
Speaker 6:  the making these long, long lists of metaphors and long lists of

454
00:26:20,450 --> 00:26:23,170
Speaker 6:  determinations about what is and is not supporting terrorism.

455
00:26:23,270 --> 00:26:27,090
Speaker 4:  And to be clear, this is like one of like the classic example of

456
00:26:27,380 --> 00:26:31,050
Speaker 4:  they're doing the right thing and getting no credit for it. Like Twitter

457
00:26:31,050 --> 00:26:34,890
Speaker 4:  does not want ISIS on its platform. It activ

458
00:26:34,890 --> 00:26:35,730
Speaker 4:  or at least until

459
00:26:36,020 --> 00:26:37,890
Speaker 1:  As far as we know, Twitter does not want

460
00:26:37,890 --> 00:26:40,250
Speaker 4:  Until least the recent changes. Who knows now

461
00:26:40,390 --> 00:26:43,560
Speaker 6:  The tweet about 2015 and 2022, Twitter did not.

462
00:26:43,630 --> 00:26:47,200
Speaker 4:  Yeah. Right. Like Twitter probably had a department of ISIS prevention,

463
00:26:47,530 --> 00:26:48,760
Speaker 1:  They're gone now. They

464
00:26:48,760 --> 00:26:52,320
Speaker 4:  Got laid off like who knows what is happening in Twitter now but

465
00:26:52,500 --> 00:26:56,040
Speaker 4:  act during the relevant time of this lawsuit, Twitter

466
00:26:56,460 --> 00:27:00,200
Speaker 4:  was taking steps like active engaged

467
00:27:00,200 --> 00:27:03,720
Speaker 4:  steps to remove these accounts. They just weren't perfect.

468
00:27:03,940 --> 00:27:05,200
Speaker 4:  Did that come up at all?

469
00:27:05,950 --> 00:27:09,880
Speaker 5:  They spent a lot of time both days talking about the

470
00:27:09,880 --> 00:27:13,720
Speaker 5:  extent of, first of all, what is even knowledge, how much

471
00:27:13,720 --> 00:27:17,600
Speaker 5:  knowledge did Twitter have of you know, or Twitter or Google who was

472
00:27:17,600 --> 00:27:20,960
Speaker 5:  on, who's using their service? What kind of content is on the service?

473
00:27:21,220 --> 00:27:25,120
Speaker 5:  And there's kind of a a weird paradox here because

474
00:27:25,120 --> 00:27:29,080
Speaker 5:  the more that you know about what's on your platform, arguably the more

475
00:27:29,300 --> 00:27:32,600
Speaker 5:  responsibility you have to deal with it. Whereas the less you know, the more

476
00:27:32,600 --> 00:27:36,440
Speaker 5:  you can kind of plausibly say I had no relationship with these people

477
00:27:36,440 --> 00:27:38,760
Speaker 5:  who use the service cuz I didn't know they were there. Yeah.

478
00:27:38,760 --> 00:27:42,480
Speaker 4:  This sort of like strategic ignorance actually comes up with

479
00:27:42,680 --> 00:27:46,040
Speaker 4:  platforms a lot. Like this is the argument they will all make to you. Right?

480
00:27:46,040 --> 00:27:49,160
Speaker 4:  Like if you make us run a copyright enforcement system, then we will know

481
00:27:49,160 --> 00:27:52,440
Speaker 4:  and we will take more stuff down and more people will be at, or it could

482
00:27:52,440 --> 00:27:52,760
Speaker 4:  be a free

483
00:27:52,760 --> 00:27:56,680
Speaker 5:  For all I think. I think a lot of people though do maybe assume that the

484
00:27:56,920 --> 00:27:59,960
Speaker 5:  platforms know more than what they actually know about the content that's

485
00:28:00,080 --> 00:28:01,200
Speaker 5:  actually on the platform. Well I think

486
00:28:01,200 --> 00:28:04,440
Speaker 1:  They, they assume that because they, they do have all the data, right? Like,

487
00:28:04,440 --> 00:28:08,240
Speaker 1:  like Twitter and Google have all the data theoretically system,

488
00:28:08,260 --> 00:28:11,800
Speaker 1:  the computers know that ISIS is on the platform but that doesn't mean a human

489
00:28:11,800 --> 00:28:12,280
Speaker 1:  knows.

490
00:28:12,280 --> 00:28:16,080
Speaker 6:  Right? Now a lot of these cases hinge on the idea that there were government

491
00:28:16,080 --> 00:28:19,560
Speaker 6:  officials or there was media coverage that when you say they know often what

492
00:28:19,560 --> 00:28:22,840
Speaker 6:  they mean is the B D C said Hey there are all these terrorists on your platform

493
00:28:23,330 --> 00:28:27,040
Speaker 6:  or that somebody in the US government pointed this out to them

494
00:28:27,060 --> 00:28:30,800
Speaker 6:  and that they didn't take enough action often to

495
00:28:30,800 --> 00:28:33,880
Speaker 6:  sort of play the game of whack-a-mole of keeping people off once they knew.

496
00:28:34,030 --> 00:28:37,640
Speaker 4:  Yeah. Okay. Well and there's also just like the inherent

497
00:28:37,670 --> 00:28:41,400
Speaker 4:  nature of data and like this comes up with YouTube and

498
00:28:41,400 --> 00:28:45,240
Speaker 4:  YouTubers in particular all the time where the YouTubers know what is

499
00:28:45,240 --> 00:28:49,200
Speaker 4:  happening on the platform well before YouTube executives can see it.

500
00:28:49,200 --> 00:28:50,880
Speaker 1:  Yeah. We've talked about it that a

501
00:28:50,880 --> 00:28:53,440
Speaker 4:  Bunch on the show we were talking about in the context of, of Susan stepping

502
00:28:53,440 --> 00:28:57,400
Speaker 4:  down a C of YouTube, right? Like their view of YouTube is like

503
00:28:57,430 --> 00:29:00,960
Speaker 4:  echoes through data that is weeks or months behind

504
00:29:01,260 --> 00:29:04,560
Speaker 4:  the frontline of actual YouTube. Yeah. So you assume they know, but

505
00:29:04,920 --> 00:29:08,280
Speaker 4:  that's not, might not actually be what's happening. And in particular the

506
00:29:08,310 --> 00:29:12,280
Speaker 4:  data that they need to see, if you train some automated system

507
00:29:12,280 --> 00:29:15,880
Speaker 4:  against it, the system will have an error rate and you need to calibrate

508
00:29:15,880 --> 00:29:19,640
Speaker 4:  what is your acceptable error rate. So if I stand up for a Twitter account

509
00:29:19,750 --> 00:29:23,320
Speaker 4:  that's like, do you remember when Verizon had a payment service called

510
00:29:23,320 --> 00:29:23,760
Speaker 4:  isis?

511
00:29:25,110 --> 00:29:27,560
Speaker 1:  ISIS was a very popular dim for a very

512
00:29:27,560 --> 00:29:31,080
Speaker 4:  Long time. For one minute it was like, we're gonna have a wallet called ISIS

513
00:29:31,080 --> 00:29:34,360
Speaker 4:  and everyone's gonna use ISIS on their phones. And they're like, wait,

514
00:29:35,030 --> 00:29:38,880
Speaker 4:  just ignore our whole plan. My bad we'll come back to you with a new payment

515
00:29:38,880 --> 00:29:42,640
Speaker 4:  system. And then they just didn't, I think they, all the

516
00:29:42,640 --> 00:29:46,560
Speaker 4:  ISIS people at Verizon were like, we don't, we don't even wanna explain what

517
00:29:46,760 --> 00:29:47,080
Speaker 4:  happened here.

518
00:29:47,080 --> 00:29:48,680
Speaker 1:  You're putting lipstick on that pitch.

519
00:29:49,080 --> 00:29:52,720
Speaker 4:  But like if you start up a Twitter account that's like Verizon ISIS lover

520
00:29:53,540 --> 00:29:57,360
Speaker 4:  and like the system just like rejects you because of straight keyword

521
00:29:57,360 --> 00:30:01,240
Speaker 4:  filtering. Right. It is actually not a correct error. Or if I tweet,

522
00:30:01,920 --> 00:30:04,760
Speaker 4:  remember when Verizon had a payment system called isis, didn't we all love

523
00:30:04,760 --> 00:30:08,720
Speaker 4:  isis? Yeah. And it's like strict keyword filtering. Like that's an

524
00:30:08,720 --> 00:30:09,440
Speaker 4:  unacceptable

525
00:30:09,440 --> 00:30:13,240
Speaker 1:  Error. Yeah. Cuz it was just like there was Archer, the main like

526
00:30:13,240 --> 00:30:14,680
Speaker 1:  spy organization Archer

527
00:30:14,680 --> 00:30:16,320
Speaker 6:  Not bringing up Archer and you went and you wouldn't

528
00:30:16,320 --> 00:30:20,000
Speaker 1:  Wow. I did it. I'm sorry but they, they called it isis. There's like a character

529
00:30:20,160 --> 00:30:24,000
Speaker 1:  in DC cos like ISIS is a very common thing that people would talk about

530
00:30:24,230 --> 00:30:25,960
Speaker 1:  when they're not talking about terrorism.

531
00:30:25,960 --> 00:30:29,720
Speaker 6:  Yeah. There's also the speech nu the nuance that there is terrorist

532
00:30:29,720 --> 00:30:33,600
Speaker 6:  speech, but there is actually a lot of speech about terrorism that

533
00:30:33,600 --> 00:30:37,200
Speaker 6:  is not terrorist speech. Right. That posting a video of

534
00:30:37,290 --> 00:30:41,080
Speaker 6:  someone, of a terrorist speaking can also just be a thing that

535
00:30:41,080 --> 00:30:45,040
Speaker 6:  CNN does. So you also can't automatically necessarily filter

536
00:30:45,210 --> 00:30:48,160
Speaker 6:  by names or by video clips even.

537
00:30:48,460 --> 00:30:52,040
Speaker 4:  And the justices brought this up, right? They like struggled with this very

538
00:30:52,040 --> 00:30:54,960
Speaker 4:  basic and I think the answer is, well the first amendment would cover that.

539
00:30:54,960 --> 00:30:58,040
Speaker 4:  Which is, that was the part when I was reading the live blog where I was

540
00:30:58,040 --> 00:31:01,560
Speaker 4:  like, oh they, they got there, they got it, they arrived the first and finally

541
00:31:01,560 --> 00:31:02,880
Speaker 4:  they have arrived at the first amendment.

542
00:31:02,880 --> 00:31:04,560
Speaker 1:  But they did talk about pagers first.

543
00:31:04,840 --> 00:31:08,440
Speaker 5:  There is, there is a really interesting nuance also to the knowing question

544
00:31:08,440 --> 00:31:11,600
Speaker 5:  that I wish they would've had 12 hours to explore. Which is like, what do

545
00:31:11,600 --> 00:31:15,240
Speaker 5:  the recommendation algorithms even know? A lot of these systems don't

546
00:31:15,520 --> 00:31:18,840
Speaker 5:  evaluate stuff in the same criteria. They might know what a certain type

547
00:31:18,840 --> 00:31:22,000
Speaker 5:  of person wants to see and what other people that they have liked things

548
00:31:22,070 --> 00:31:25,560
Speaker 5:  have seen, but they might not know the actual content that they're surfacing

549
00:31:25,560 --> 00:31:27,680
Speaker 1:  At some point. Right. Because they're not, they're not humans. They're not

550
00:31:27,680 --> 00:31:31,240
Speaker 1:  perceiving, they're perceiving data and that's it. And depending on what

551
00:31:31,240 --> 00:31:34,880
Speaker 1:  that data is, like the worst thing we can ever do is assume the AI is

552
00:31:34,890 --> 00:31:36,840
Speaker 1:  in any way things like human processes.

553
00:31:36,930 --> 00:31:40,280
Speaker 4:  No, it has one, it thinks like a human in one specific way, which is if you

554
00:31:40,350 --> 00:31:44,160
Speaker 4:  make it mad Yeah. It will be super mad at you in

555
00:31:46,580 --> 00:31:48,440
Speaker 1:  The one way that they think like humans

556
00:31:48,440 --> 00:31:52,120
Speaker 4:  It be weird and defensive if you ask it to do something it

557
00:31:52,120 --> 00:31:52,960
Speaker 4:  doesn't wanna do

558
00:31:53,240 --> 00:31:56,680
Speaker 1:  Doesn't think like a human except for like in the worst ways of things like

559
00:31:56,680 --> 00:31:57,280
Speaker 1:  a human. Exactly.

560
00:31:57,590 --> 00:32:00,160
Speaker 4:  It's like I've been a good bing, I don't know about you.

561
00:32:01,780 --> 00:32:02,200
Speaker 1:  I'm

562
00:32:02,440 --> 00:32:06,080
Speaker 6:  Actually honestly waiting for the moment where somebody asks an AI how it

563
00:32:06,080 --> 00:32:09,000
Speaker 6:  works and tries to bring that up in court as an explanation of how the AI

564
00:32:09,000 --> 00:32:10,920
Speaker 6:  works. Somebody's gonna try it.

565
00:32:10,920 --> 00:32:12,360
Speaker 1:  It's gonna be weird. I'm so excited. It's

566
00:32:12,360 --> 00:32:15,120
Speaker 4:  Gonna be weird. Okay. So these are the arguments. The justices seem like

567
00:32:15,120 --> 00:32:18,040
Speaker 4:  they get it, they understand they're playing with fire in terms of the stakes.

568
00:32:18,470 --> 00:32:21,160
Speaker 4:  What do you think happens now? Are they gonna horse trade between the two

569
00:32:21,160 --> 00:32:24,040
Speaker 4:  cases? Like you were suggesting tc, I, I'm

570
00:32:24,040 --> 00:32:27,720
Speaker 5:  All out. No one knows. But they did seem to be proposing

571
00:32:27,990 --> 00:32:31,960
Speaker 5:  subtly that they have an escape hatch. So they might just say, forget

572
00:32:31,960 --> 00:32:35,880
Speaker 5:  this, we're gonna wait for the really fun one, which is dealing

573
00:32:35,880 --> 00:32:39,760
Speaker 5:  with Texas and Florida and making websites host things that we want them

574
00:32:39,760 --> 00:32:39,880
Speaker 5:  to

575
00:32:39,880 --> 00:32:41,680
Speaker 4:  Host. Yeah. Addie, what do you think?

576
00:32:42,230 --> 00:32:45,920
Speaker 6:  I really don't wanna make predictions, but yes, it seems possible they're

577
00:32:45,920 --> 00:32:49,800
Speaker 6:  just gonna rule in the narrowest least two 30 precedent

578
00:32:49,800 --> 00:32:53,400
Speaker 6:  making possible way. And I don't know that they

579
00:32:53,400 --> 00:32:57,160
Speaker 6:  necessarily seem that likely to say that Twitter is in the clear, but it

580
00:32:57,160 --> 00:33:00,600
Speaker 6:  seems like they could pretty clearly just say this was

581
00:33:01,480 --> 00:33:05,080
Speaker 6:  actually the most unsatisfying result that I have heard

582
00:33:05,080 --> 00:33:08,600
Speaker 6:  described is that they just say, Hey, you changed your argument way too much.

583
00:33:08,600 --> 00:33:10,960
Speaker 6:  Gonzalez lawyers we're throwing this out for now.

584
00:33:12,310 --> 00:33:16,200
Speaker 4:  Well there's a, there's like a extremely funny sea plot to all

585
00:33:16,200 --> 00:33:19,800
Speaker 4:  of this. Yeah. Which is that the big tech companies have hired

586
00:33:19,970 --> 00:33:23,960
Speaker 4:  so many lobbyists and lawyers that the plaintiffs were

587
00:33:23,960 --> 00:33:27,760
Speaker 4:  not able to hire a good lawyer. Aw. And they ended up with

588
00:33:27,760 --> 00:33:31,640
Speaker 4:  this like gentleman Yeah. Who did a bad job. Aw. They've found

589
00:33:31,640 --> 00:33:34,200
Speaker 5:  Plenty of good lawyers for other things like copyright

590
00:33:34,200 --> 00:33:38,080
Speaker 4:  Infringement. Well, right. But it just, big tech has hired so many of the

591
00:33:38,360 --> 00:33:42,240
Speaker 4:  lawyers in DC Yeah. That they were all conflicted

592
00:33:42,240 --> 00:33:45,920
Speaker 4:  out of arguing this case for the, for the plaintiffs On both, in both cases.

593
00:33:45,920 --> 00:33:49,800
Speaker 4:  Yeah. So both Tam Nay and Gonzalez had the same lawyer

594
00:33:50,260 --> 00:33:54,040
Speaker 4:  and he'd it would not, he was what was left, I don't know how else to say

595
00:33:54,040 --> 00:33:54,520
Speaker 1:  This. He's doing

596
00:33:54,520 --> 00:33:57,040
Speaker 4:  His best. He was the guy, he he was there. So

597
00:33:57,040 --> 00:34:01,000
Speaker 1:  It was the same guy both days. Yeah. Oh that's like, that's a

598
00:34:01,000 --> 00:34:01,280
Speaker 1:  week.

599
00:34:01,310 --> 00:34:02,240
Speaker 4:  Yeah. He had a,

600
00:34:02,310 --> 00:34:03,560
Speaker 1:  I hope you have a drink sir.

601
00:34:03,710 --> 00:34:07,640
Speaker 4:  Yeah. So we'll see. I mean I I I do think that particularly with the Gonzalez

602
00:34:07,640 --> 00:34:11,120
Speaker 4:  case, I think we, the Supreme Court rule that Google is illegal does not

603
00:34:11,120 --> 00:34:15,040
Speaker 4:  seem like a likely outcome. Yeah. I think

604
00:34:15,040 --> 00:34:18,920
Speaker 4:  with a Twitter case the, we think you should do a better job of making

605
00:34:18,920 --> 00:34:22,880
Speaker 4:  sure the terrorists don't eat. Like that's a safe ruling. Yeah. Even though

606
00:34:22,880 --> 00:34:26,080
Speaker 4:  it might have some consequences about like verification or whatever. I also

607
00:34:26,080 --> 00:34:28,720
Speaker 4:  think it's the guy who owns Twitter right now

608
00:34:28,910 --> 00:34:30,080
Speaker 1:  Does not wanna hear that.

609
00:34:30,450 --> 00:34:33,040
Speaker 4:  He doesn't wanna hear it. But he is also, I mean like the politics has come

610
00:34:33,040 --> 00:34:37,000
Speaker 4:  into place somehow. Right? Like he's, he has a different ideological

611
00:34:37,000 --> 00:34:39,600
Speaker 4:  reputation than the other guy who,

612
00:34:39,600 --> 00:34:43,560
Speaker 1:  Which is when they took on the case. Yeah. Okay. They took it on when

613
00:34:43,560 --> 00:34:44,040
Speaker 1:  it was still,

614
00:34:44,110 --> 00:34:47,120
Speaker 4:  Yeah. When when they were still hanging out shirts. It said stay woke. Yeah,

615
00:34:47,870 --> 00:34:51,760
Speaker 4:  I have one. It's a little tight. You gotta work on that. But

616
00:34:51,760 --> 00:34:55,640
Speaker 4:  it's like, that's just a swirl around the Twitter case that I think is

617
00:34:55,640 --> 00:34:59,280
Speaker 4:  different than the swirl around the Google case, which is, boy this is a

618
00:34:59,280 --> 00:35:02,680
Speaker 4:  stretch and why are you here? Yeah. Which is not what I expected to hear,

619
00:35:03,200 --> 00:35:07,160
Speaker 4:  so we'll see. But it's, we're expecting decisions at the end of

620
00:35:07,160 --> 00:35:10,120
Speaker 4:  this term, which is like quite some time away. Right Addie?

621
00:35:10,470 --> 00:35:14,400
Speaker 6:  Yeah. I believe the next term starts in October. So yeah,

622
00:35:14,400 --> 00:35:16,160
Speaker 6:  we probably a month.

623
00:35:16,190 --> 00:35:16,920
Speaker 1:  Yeah. Okay.

624
00:35:17,140 --> 00:35:20,280
Speaker 4:  But I would say that the, the general valence from the

625
00:35:20,890 --> 00:35:23,920
Speaker 4:  lawyers in the world as whoa, that could have got a lot worse.

626
00:35:23,990 --> 00:35:25,360
Speaker 1:  Well that's good, right?

627
00:35:25,360 --> 00:35:26,880
Speaker 4:  Yeah. We'll see. But

628
00:35:26,950 --> 00:35:30,920
Speaker 6:  Yeah, the thing I've heard is clearly they could rule really bad ways down

629
00:35:30,920 --> 00:35:33,880
Speaker 6:  the line and there are specific subtle ways that we have not talked about

630
00:35:33,880 --> 00:35:36,680
Speaker 6:  here that they could do that even as part of Gonzalez.

631
00:35:37,300 --> 00:35:40,480
Speaker 1:  But we came out of this much more reassured than we thought we could be.

632
00:35:41,060 --> 00:35:44,760
Speaker 4:  For sure. Alright, thank you. Add and tc. We gotta take a break.

633
00:35:44,760 --> 00:35:45,800
Speaker 1:  You gotta go to Hot Pod.

634
00:35:45,800 --> 00:35:47,560
Speaker 4:  I gotta, yeah, I gotta go to Brooklyn. Yeah,

635
00:35:47,560 --> 00:35:49,400
Speaker 1:  I gotta, you gotta get in a car. Yeah.

636
00:35:49,600 --> 00:35:52,000
Speaker 4:  Tutu. I haven't been in Brooklyn in years. It's gonna be great. Oh wow. Used

637
00:35:52,000 --> 00:35:53,640
Speaker 4:  to live there. I go to wild, my old haunts

638
00:35:53,640 --> 00:35:54,400
Speaker 1:  Gonna go just,

639
00:35:54,860 --> 00:35:57,680
Speaker 4:  I'm gonna post one of those TikTok. It's like I'm back in the city. It's,

640
00:35:58,030 --> 00:36:00,560
Speaker 4:  Alex is gonna talk about some stuff after this. Who knows what it'll

641
00:36:00,560 --> 00:36:02,600
Speaker 1:  Be. It's gonna be super fun and stay tuned.

642
00:36:02,600 --> 00:36:03,360
Speaker 4:  We'll right back.

643
00:37:20,290 --> 00:37:23,980
Speaker 1:  Okay, we're back. Eli's gone. He's gonna go work on some

644
00:37:23,980 --> 00:37:27,900
Speaker 1:  gross podcast. You don't need to worry about. He's overdoing hot pod. He's

645
00:37:27,900 --> 00:37:31,580
Speaker 1:  having a great time. But I've got better people with me today. No, that's

646
00:37:31,580 --> 00:37:35,300
Speaker 1:  mean to Addie and TC who are lovely. Eli's fine. Anyway,

647
00:37:35,300 --> 00:37:39,100
Speaker 1:  Richard Lawler is here. Hey Richard. Hey. What's happening? And

648
00:37:39,100 --> 00:37:40,620
Speaker 1:  our good friend McKenna Kelly is here.

649
00:37:40,870 --> 00:37:41,620
Speaker 8:  Hi everyone.

650
00:37:41,920 --> 00:37:45,740
Speaker 1:  And we are gonna talk about some, some kind of breaking news.

651
00:37:45,740 --> 00:37:49,580
Speaker 1:  It happened today. Our good friend SPF F. How's he

652
00:37:49,580 --> 00:37:51,380
Speaker 1:  doing McKenna? Well,

653
00:37:51,850 --> 00:37:55,820
Speaker 8:  Four more charges against him. We had eight. Yeah. When he

654
00:37:55,820 --> 00:37:59,620
Speaker 8:  was first charged. 1,

655
00:38:00,700 --> 00:38:04,380
Speaker 8:  2, 3, 4, 12. Additional

656
00:38:04,380 --> 00:38:08,140
Speaker 8:  bank fraud and money laundering. And then of course they modified the

657
00:38:08,140 --> 00:38:11,700
Speaker 8:  campaign finance charges by adding, you know, a little bit more information

658
00:38:11,790 --> 00:38:15,580
Speaker 8:  that included a lot of the signal chat that we've kind of

659
00:38:15,580 --> 00:38:19,260
Speaker 8:  heard about their use of signal. But they, you know, cite some messages

660
00:38:19,410 --> 00:38:20,900
Speaker 8:  in there that were, I I was very surprised.

661
00:38:21,140 --> 00:38:23,380
Speaker 1:  Oh yeah, let's, let's talk about that.

662
00:38:23,460 --> 00:38:27,140
Speaker 9:  There were bank fraud charges left that he hadn't already been

663
00:38:27,140 --> 00:38:29,020
Speaker 9:  charged with. Like there are other kinds,

664
00:38:29,210 --> 00:38:33,060
Speaker 8:  I guess there's so many. How many bank charges

665
00:38:33,060 --> 00:38:34,260
Speaker 8:  can one man, he's

666
00:38:34,260 --> 00:38:38,200
Speaker 1:  Just an achiever. He's just an overachiever. He wants to get it all

667
00:38:38,200 --> 00:38:38,440
Speaker 1:  done.

668
00:38:38,810 --> 00:38:42,080
Speaker 8:  So yeah, they had a signal group and it was called very, I mean it's very

669
00:38:42,080 --> 00:38:44,920
Speaker 8:  easy for prosecutors here cause it's called donation processing.

670
00:38:45,800 --> 00:38:49,480
Speaker 8:  Great. Go in there for the campaign finance stuff. Yeah. And then you go

671
00:38:49,480 --> 00:38:52,800
Speaker 8:  in there and it is bankman freed, two

672
00:38:52,890 --> 00:38:56,240
Speaker 8:  unnamed co-conspirators who we can basically say

673
00:38:56,750 --> 00:39:00,200
Speaker 8:  were, you know, folks who have been reported before. Their names are escaping

674
00:39:00,200 --> 00:39:03,280
Speaker 8:  me right now, but the guy who made the Democrat donations and the guy who

675
00:39:03,280 --> 00:39:06,960
Speaker 8:  made the Republican donations. Okay. And they're in there and there's instances

676
00:39:07,130 --> 00:39:10,800
Speaker 8:  of them agreeing to make donations that Ummmon Fried wanted to make.

677
00:39:11,680 --> 00:39:15,360
Speaker 8:  But he had superseded right the limits of campaign finance. You can only

678
00:39:15,360 --> 00:39:18,760
Speaker 8:  donate to an individual campaign or a pack so much money every year. Right.

679
00:39:18,860 --> 00:39:21,720
Speaker 8:  And so he had already spent all of the money

680
00:39:22,720 --> 00:39:26,560
Speaker 8:  and needed other people to spend his money for him in order.

681
00:39:26,560 --> 00:39:30,240
Speaker 8:  So cuz you can look up everyone's donations on the fec fec.gov

682
00:39:30,540 --> 00:39:34,480
Speaker 8:  and yeah. So they did that and they

683
00:39:34,680 --> 00:39:38,560
Speaker 8:  found evidence of that being coordinated. And it's very funny. I love

684
00:39:39,080 --> 00:39:40,520
Speaker 1:  You were like really excited about

685
00:39:40,520 --> 00:39:43,760
Speaker 8:  No, I just love these indictments from the justice department because they're

686
00:39:43,760 --> 00:39:47,400
Speaker 8:  kind of silly to read. Yeah cuz it's like encrypted messaging

687
00:39:47,400 --> 00:39:51,200
Speaker 8:  app signal, right? Like it feels like so suspicious where it's like something

688
00:39:51,200 --> 00:39:54,960
Speaker 8:  we all use every day. Yeah. But yeah, so they saw that, you look

689
00:39:55,200 --> 00:39:59,120
Speaker 8:  at this one thing, there was one instance where bankman freed s

690
00:39:59,120 --> 00:40:02,800
Speaker 8:  sp f asked who's assumingly like the Democratic

691
00:40:02,800 --> 00:40:06,720
Speaker 8:  spender, you know, for his name to make at least like a $1 million

692
00:40:06,720 --> 00:40:10,560
Speaker 8:  donation to a pro L G B T pack. Okay. And he was so

693
00:40:10,560 --> 00:40:13,960
Speaker 8:  ridiculous cuz apparently there was like a political like, I dunno, consultant

694
00:40:13,960 --> 00:40:17,840
Speaker 8:  or something in there and told that guy, like in general, you

695
00:40:17,840 --> 00:40:20,800
Speaker 8:  being the center left face of our spending will mean you giving a lot of

696
00:40:21,800 --> 00:40:23,240
Speaker 8:  woke s curse

697
00:40:25,130 --> 00:40:27,960
Speaker 8:  word to you can say shit. Okay. I can say shit.

698
00:40:27,960 --> 00:40:29,200
Speaker 1:  Yeah. Oh, you can say shit on the

699
00:40:29,200 --> 00:40:31,560
Speaker 8:  Vergecast. I'm quoting someone else. I'm not cursing. I would never,

700
00:40:31,760 --> 00:40:33,120
Speaker 1:  Never, no, that's

701
00:40:33,120 --> 00:40:36,720
Speaker 8:  Wrong. So yeah, in general, you being the center left face of our spending

702
00:40:36,720 --> 00:40:39,520
Speaker 8:  will mean you giving a lot of woke shit for transactional purposes, which

703
00:40:39,520 --> 00:40:43,360
Speaker 8:  is like so blatantly like, I don't know, it just, it's not

704
00:40:43,360 --> 00:40:47,240
Speaker 8:  what SPF was ever painted as, you know. So he was always like, you know,

705
00:40:47,240 --> 00:40:49,600
Speaker 8:  the boy wonder's going change the world.

706
00:40:50,410 --> 00:40:50,760
Speaker 1:  He

707
00:40:50,760 --> 00:40:54,440
Speaker 8:  Wears shorts and he is like a little weird. But you know, he has good

708
00:40:54,440 --> 00:40:57,880
Speaker 8:  ideas and he wants the world to be great. Yeah. And it's like this woke shit.

709
00:40:57,880 --> 00:40:58,280
Speaker 8:  Does he

710
00:40:58,280 --> 00:40:58,600
Speaker 9:  Have good

711
00:40:58,600 --> 00:41:01,840
Speaker 8:  Ideas? Well that's what, do you know, were you following him before all this

712
00:41:01,960 --> 00:41:04,600
Speaker 8:  happened? Like everyone was like, this guy's gonna change the world. He's

713
00:41:04,600 --> 00:41:04,920
Speaker 8:  so great.

714
00:41:04,920 --> 00:41:05,880
Speaker 1:  Look at Richard's

715
00:41:05,880 --> 00:41:06,960
Speaker 8:  Face. I know

716
00:41:07,940 --> 00:41:11,600
Speaker 1:  You didn't believe it, Richard. When, when were you like, I don't know about

717
00:41:11,600 --> 00:41:12,440
Speaker 1:  this SBF guy.

718
00:41:12,780 --> 00:41:16,600
Speaker 9:  The very first moment I saw him, I mean like the instant I heard of him and

719
00:41:16,920 --> 00:41:20,240
Speaker 9:  looked him up and saw him, I said suss that guy is not

720
00:41:20,240 --> 00:41:23,080
Speaker 9:  trustworthy. I would not give him a billion dollars. I just wouldn't.

721
00:41:23,140 --> 00:41:26,760
Speaker 1:  Did you, did you know that he was gonna get 12 now

722
00:41:26,760 --> 00:41:28,960
Speaker 1:  indictments against him when you first saw him?

723
00:41:29,210 --> 00:41:32,040
Speaker 9:  No, I was not predicting that. I didn't, I didn't know that he was that kind

724
00:41:32,040 --> 00:41:34,520
Speaker 9:  of a achiever. I mean like that that, that is just going above and beyond

725
00:41:34,520 --> 00:41:37,560
Speaker 9:  really. Like, like what this guy is doing and, and what he has achieved.

726
00:41:37,560 --> 00:41:39,640
Speaker 9:  It's it's admirable in a way.

727
00:41:40,970 --> 00:41:42,640
Speaker 8:  In a way. What is that way Richard?

728
00:41:44,070 --> 00:41:47,680
Speaker 9:  I mean if you're gonna go for it, go go big. Yeah, that's true. Like, like

729
00:41:47,680 --> 00:41:49,880
Speaker 9:  why? Like why stop it? Just a little bit of fraud. Yeah.

730
00:41:50,200 --> 00:41:54,000
Speaker 8:  Right. Yeah. I mean when you, before you start like peeling the

731
00:41:54,000 --> 00:41:57,440
Speaker 8:  layers off of this, right? When you're just like all of these major

732
00:41:57,440 --> 00:42:01,080
Speaker 8:  companies and you know, whatever these people, investors putting money in,

733
00:42:01,080 --> 00:42:04,480
Speaker 8:  it's like, haha, that is funny. Get screwed. And then it's like, oh

734
00:42:04,760 --> 00:42:08,120
Speaker 8:  these people, these people were just, they thought it was a good spending

735
00:42:08,120 --> 00:42:11,960
Speaker 8:  opportunity and then I get sad. Yeah. Which is sucks. But speaking

736
00:42:11,960 --> 00:42:15,760
Speaker 8:  of the people who were affected of this, you look at this in the new indictment

737
00:42:15,760 --> 00:42:19,160
Speaker 8:  that we saw today, it mentions that the money

738
00:42:19,190 --> 00:42:22,920
Speaker 8:  making these political donations was coming from an Alameda bank

739
00:42:22,920 --> 00:42:26,840
Speaker 8:  account. Okay. So this was, it's very much like if this is proven in court,

740
00:42:26,990 --> 00:42:30,640
Speaker 8:  it's saying that customer funds were definitely used. Wow. And there was,

741
00:42:30,640 --> 00:42:34,560
Speaker 8:  I I think what was the number 300 political contributions made in the last

742
00:42:34,560 --> 00:42:37,520
Speaker 8:  midterms worth tens of millions of dollars.

743
00:42:38,410 --> 00:42:42,360
Speaker 1:  So he like probably affected the election LA than the midterms.

744
00:42:42,360 --> 00:42:46,080
Speaker 8:  That's the nuts money. That's the nuts thing about this because

745
00:42:46,460 --> 00:42:50,000
Speaker 8:  you can't, you know, you do a straw donor scheme cuz you don't want, and

746
00:42:50,000 --> 00:42:53,000
Speaker 8:  then of course you don't wanna mask, you wanna mask who you are, how much

747
00:42:53,000 --> 00:42:55,720
Speaker 8:  money you're spending. You have all these other people that you're doing

748
00:42:55,720 --> 00:42:59,520
Speaker 8:  that with. But then like the Republican donations, he called them and there

749
00:42:59,520 --> 00:43:03,160
Speaker 8:  was an interview with Tiffany Fong a couple months ago who I think

750
00:43:03,160 --> 00:43:06,040
Speaker 8:  she's just like a freelance journalist who reports on crypto and she had

751
00:43:06,040 --> 00:43:09,720
Speaker 8:  interviewed SPF and he was like, oh, all my Republican donations were dark.

752
00:43:10,200 --> 00:43:14,080
Speaker 8:  So you're giving, so dark is like a very weird word. It gets thrown around

753
00:43:14,080 --> 00:43:18,040
Speaker 8:  a lot of ways, but in some instances, like you can just say dark

754
00:43:18,040 --> 00:43:21,800
Speaker 8:  money goes to nonprofits, which then make that money, but then spend that

755
00:43:21,800 --> 00:43:25,680
Speaker 8:  money in politics. But, you know, it's very unclear how that happened

756
00:43:25,680 --> 00:43:28,600
Speaker 8:  if he's just talking about a dark money scheme of like getting his, you know,

757
00:43:28,800 --> 00:43:31,840
Speaker 8:  coworkers or friends involved, or if this really was like some weird nonprofit.

758
00:43:31,950 --> 00:43:35,880
Speaker 8:  Yeah. But then again, it's like really hard unless they

759
00:43:35,880 --> 00:43:39,720
Speaker 8:  have some kind of, you know, evidentiary, you know, response, like some kind

760
00:43:39,720 --> 00:43:42,080
Speaker 8:  of evidence produced throughout the hearing to like really figure out where

761
00:43:42,080 --> 00:43:44,600
Speaker 8:  all that money went. And seeing that they have like, some of the documents

762
00:43:44,600 --> 00:43:48,320
Speaker 8:  with a lot of transactions and things that have like such a large

763
00:43:48,390 --> 00:43:52,080
Speaker 8:  look at all of the, you know, everything with money and stuff at

764
00:43:52,080 --> 00:43:54,880
Speaker 8:  ftx, they have like a very, they have balance sheets and stuff. It would

765
00:43:54,880 --> 00:43:58,080
Speaker 8:  be interesting to see where it all is going. The, and that's why I was really

766
00:43:58,080 --> 00:44:01,600
Speaker 8:  interested in today's indictment because it shows us a little bit more direction

767
00:44:01,600 --> 00:44:02,080
Speaker 8:  there.

768
00:44:02,080 --> 00:44:05,800
Speaker 1:  Right. And so, so we saw today he donated money.

769
00:44:05,800 --> 00:44:09,080
Speaker 1:  They're, they're asking all of people to return the money too, right? They're

770
00:44:09,080 --> 00:44:11,520
Speaker 1:  asking all the politicians to like give it back.

771
00:44:11,590 --> 00:44:14,920
Speaker 8:  Yeah. So once this SPF f guy was

772
00:44:14,920 --> 00:44:18,880
Speaker 8:  indicted, I, I automatically was like, oh God, what happens to the money

773
00:44:19,880 --> 00:44:22,960
Speaker 8:  that, because for the most part, when you're a political campaign, you're

774
00:44:22,960 --> 00:44:26,000
Speaker 8:  spending that, yeah. You are getting that out the door. Yeah. You have a

775
00:44:26,200 --> 00:44:29,400
Speaker 8:  deadline and that's election day and you wanna win. Right. So like that money

776
00:44:29,400 --> 00:44:32,680
Speaker 8:  gone. Yeah. And so you look at, at, I think it was like the Democratic National

777
00:44:32,680 --> 00:44:36,360
Speaker 8:  Committee, their set at arm, you know, in a congressional house arm

778
00:44:36,390 --> 00:44:39,960
Speaker 8:  said that they were gonna be returning the money. The process is very, you

779
00:44:39,960 --> 00:44:43,600
Speaker 8:  know, weird where they have to like wait through certain processes for clawback

780
00:44:43,600 --> 00:44:46,080
Speaker 8:  or whatever, but they said, you know, together they're gonna return a million

781
00:44:46,080 --> 00:44:47,040
Speaker 8:  dollars. Could

782
00:44:47,310 --> 00:44:51,240
Speaker 1:  This bankrupt in Epacs who have to like return

783
00:44:51,240 --> 00:44:51,440
Speaker 1:  money?

784
00:44:51,440 --> 00:44:55,320
Speaker 8:  That would be nuts. Because I'm remembering, what

785
00:44:55,320 --> 00:44:58,880
Speaker 8:  was it, I had just recently watched that Madoff documentary,

786
00:44:59,160 --> 00:45:02,280
Speaker 8:  like going through all of that and like trying to claw back that money. Yeah.

787
00:45:02,280 --> 00:45:06,040
Speaker 8:  And a lot of times like that was, it was money for, for you know, just people

788
00:45:06,040 --> 00:45:09,040
Speaker 8:  who were saving for retirement or whatever. There was also like large investors

789
00:45:09,960 --> 00:45:13,680
Speaker 8:  and people like that. And even then, you know, in a case, so

790
00:45:13,680 --> 00:45:17,360
Speaker 8:  public facing like that, that really affected the entire, you know Right.

791
00:45:17,900 --> 00:45:20,960
Speaker 8:  The entire financial system calling back money from that was

792
00:45:21,180 --> 00:45:25,040
Speaker 8:  ridiculously hard. So I don't think it would bankrupt anyone.

793
00:45:25,420 --> 00:45:29,400
Speaker 8:  But I do think that in many ways the new FTX management

794
00:45:29,400 --> 00:45:33,200
Speaker 8:  wants to make customers whole as much as they can. But yeah, like when it

795
00:45:33,200 --> 00:45:36,320
Speaker 8:  comes to political spending, it that's already gone. You know, that's already

796
00:45:36,320 --> 00:45:40,160
Speaker 8:  gone through so many hands from like, right from, you know, SPF to his

797
00:45:40,260 --> 00:45:44,040
Speaker 8:  co-conspirator to the pack the campaign and the campaign to

798
00:45:44,170 --> 00:45:47,080
Speaker 8:  an advertising company, payroll, maybe.

799
00:45:47,780 --> 00:45:50,520
Speaker 8:  All kinds of things like that. So it's very hard to figure out, you know,

800
00:45:50,800 --> 00:45:51,800
Speaker 8:  what exactly have

801
00:45:51,800 --> 00:45:55,320
Speaker 1:  We seen this kind of stuff happen before where there was like a big fraud

802
00:45:55,320 --> 00:45:58,440
Speaker 1:  like that and then they have to like be like, Hey everybody give the money

803
00:45:58,440 --> 00:45:58,720
Speaker 1:  back

804
00:45:58,720 --> 00:46:02,560
Speaker 8:  In political finance. Yeah. I think there was one instance of it,

805
00:46:02,660 --> 00:46:06,600
Speaker 8:  but it's always, I forget who the candidate was and for what race it

806
00:46:06,600 --> 00:46:10,440
Speaker 8:  was. So this is very hard. But, but I,

807
00:46:10,440 --> 00:46:14,080
Speaker 8:  what happened? It's like an organizing between people on the campaign and

808
00:46:14,640 --> 00:46:18,400
Speaker 8:  campaign staffers. Okay. So like a candidate or someone on the campaign tells

809
00:46:18,400 --> 00:46:21,760
Speaker 8:  a certain staffer directs that to get the money there. It's never

810
00:46:22,050 --> 00:46:25,360
Speaker 8:  as, I've never seen something where it's like an outside third party.

811
00:46:25,360 --> 00:46:28,960
Speaker 1:  Yeah. So this is like, this is, this is big and, and a big deal. Not just

812
00:46:28,960 --> 00:46:32,680
Speaker 1:  because he went for broke, he got his 12 indictments, but also

813
00:46:32,730 --> 00:46:36,520
Speaker 1:  because that's a lot of money in the, in the campaign finance

814
00:46:36,520 --> 00:46:40,040
Speaker 1:  world that now is being asked for back. Yeah. And, and

815
00:46:40,680 --> 00:46:41,680
Speaker 1:  probably isn't gonna come back.

816
00:46:41,680 --> 00:46:45,560
Speaker 8:  Yeah. I, I mean I think, what was it, something I think the FTX

817
00:46:45,560 --> 00:46:46,360
Speaker 8:  folks wanted.

818
00:46:47,950 --> 00:46:50,320
Speaker 8:  What was, oh my gosh. That was the funny thing is that they were sending

819
00:46:50,320 --> 00:46:53,520
Speaker 8:  confidential messages to the campaigns being like, Hey, can we have this

820
00:46:53,520 --> 00:46:53,960
Speaker 8:  money back?

821
00:46:54,550 --> 00:46:56,000
Speaker 1:  Like, before they got

822
00:46:56,000 --> 00:46:59,760
Speaker 8:  Indicted. That was just earlier this year. So, so this was just like,

823
00:46:59,970 --> 00:47:03,600
Speaker 1:  So this was like recently like the government was like, yo, can you, can

824
00:47:03,600 --> 00:47:04,280
Speaker 1:  you Venmo me?

825
00:47:04,280 --> 00:47:05,040
Speaker 8:  Yeah. Well, yeah.

826
00:47:05,170 --> 00:47:07,360
Speaker 9:  Or was that the, the new CEO of

827
00:47:07,360 --> 00:47:09,640
Speaker 8:  Ftx? It's the new ceo. It's like the new FTX Jay,

828
00:47:10,390 --> 00:47:12,280
Speaker 9:  I, I can't Jay, Jonah Ray or something like

829
00:47:12,520 --> 00:47:15,200
Speaker 8:  That. John Ray. Yeah, John Ray. John.

830
00:47:15,900 --> 00:47:17,720
Speaker 9:  Yes. So he's like, that's Spidermans Bowl.

831
00:47:17,720 --> 00:47:21,040
Speaker 1:  He's out there like on his own signal chat to, to

832
00:47:21,520 --> 00:47:21,720
Speaker 1:  campaign

833
00:47:21,990 --> 00:47:23,000
Speaker 8:  I guess being

834
00:47:23,000 --> 00:47:24,080
Speaker 1:  Like, can I get that back?

835
00:47:24,520 --> 00:47:27,600
Speaker 8:  Hey, can you get that back to me? Because what it is, it's like, it says,

836
00:47:27,980 --> 00:47:31,840
Speaker 8:  I'm looking at the CNN n n article, it says, and other recipients, so the

837
00:47:31,840 --> 00:47:35,560
Speaker 8:  packs and everything, the clawback assets to repay the company's

838
00:47:35,560 --> 00:47:39,400
Speaker 8:  estimated 1 million creditors. So maybe that's Yeah. Just creditors. Oh

839
00:47:39,460 --> 00:47:42,400
Speaker 8:  wow. And that's, this is what happens too in these big cases. Cause if you

840
00:47:42,400 --> 00:47:46,280
Speaker 8:  look at the Madoff cases as well, a lot of that money ended up going to,

841
00:47:46,280 --> 00:47:50,240
Speaker 8:  you know, creditors in large investors instead of like this people who

842
00:47:50,240 --> 00:47:50,800
Speaker 8:  maybe just had like

843
00:47:50,800 --> 00:47:54,680
Speaker 1:  The people who were just really passionate Yeah. About the FTX mission

844
00:47:55,120 --> 00:47:58,640
Speaker 1:  and what everything's SPF was saying. If only they'd had Richard as their

845
00:47:58,640 --> 00:48:01,680
Speaker 1:  financial advisor. Well, no, you would've put all, what crypto would you

846
00:48:01,680 --> 00:48:02,800
Speaker 1:  have? Put it all into instead?

847
00:48:02,890 --> 00:48:06,800
Speaker 9:  My own obviously R JCC coin is mentoring soon. And

848
00:48:06,920 --> 00:48:10,280
Speaker 9:  you know, if you messaged me, I can get you in early for the premit. For

849
00:48:10,280 --> 00:48:12,680
Speaker 9:  the pre pre premit and then the PostIt. That's

850
00:48:12,680 --> 00:48:15,920
Speaker 8:  The only way you can win is to mi your own corn.

851
00:48:16,270 --> 00:48:18,680
Speaker 1:  Yeah. I love this for you Richard. You got this. Oh my

852
00:48:18,680 --> 00:48:18,840
Speaker 8:  Gosh.

853
00:48:18,950 --> 00:48:20,880
Speaker 9:  2023 is my big crypto year.

854
00:48:23,060 --> 00:48:23,480
Speaker 1:  Who

855
00:48:23,480 --> 00:48:25,040
Speaker 8:  Are you, who's gonna be like your brand ambassador?

856
00:48:25,900 --> 00:48:29,080
Speaker 9:  You know, I don't, I haven't decided. All the F1 teams are out on crypto

857
00:48:29,080 --> 00:48:32,280
Speaker 9:  now. All the, all the, the NBA teams are out on crypto.

858
00:48:32,470 --> 00:48:33,440
Speaker 8:  You're in late.

859
00:48:33,880 --> 00:48:37,720
Speaker 1:  You get Tom. Yeah. Brady. Brady needs, he needs some extra money

860
00:48:37,720 --> 00:48:37,960
Speaker 1:  now.

861
00:48:38,230 --> 00:48:39,560
Speaker 9:  Well, you know, he needs, he's had a

862
00:48:39,560 --> 00:48:39,760
Speaker 1:  Divorce

863
00:48:39,760 --> 00:48:42,400
Speaker 9:  To pay for I think. I think he, I think he's had enough. We'll, we'll we'll

864
00:48:42,400 --> 00:48:45,080
Speaker 9:  find someone else. We'll find some NFL quarterback who's out of a job. What's

865
00:48:45,080 --> 00:48:45,840
Speaker 9:  Johnny Menzel doing?

866
00:48:48,140 --> 00:48:48,560
Speaker 1:  You

867
00:48:48,560 --> 00:48:52,040
Speaker 8:  Know what I love? Did you hear about Shaquille O'Neal? Like he was not going

868
00:48:52,130 --> 00:48:55,440
Speaker 8:  to, he's in all the commercials. Like he's in the general, like that man

869
00:48:55,440 --> 00:48:58,800
Speaker 8:  is on everything. Yeah. And he was like, I'm not gonna do crypto. I don't

870
00:48:58,800 --> 00:49:01,840
Speaker 8:  understand how that works. It sounds like a scam. I love it. I love that

871
00:49:01,840 --> 00:49:03,160
Speaker 8:  for Shaq. Like the one added,

872
00:49:03,160 --> 00:49:06,360
Speaker 9:  I wish more people had done that. Syd, I don't know how this works. It means

873
00:49:06,360 --> 00:49:07,960
Speaker 9:  I probably shouldn't get involved. Right.

874
00:49:08,610 --> 00:49:09,240
Speaker 8:  Oh my gosh.

875
00:49:09,720 --> 00:49:10,520
Speaker 1:  Was onto something.

876
00:49:10,520 --> 00:49:12,080
Speaker 8:  Yeah. Everyone listened to Shaq.

877
00:49:12,700 --> 00:49:13,360
Speaker 1:  You should

878
00:49:13,600 --> 00:49:16,040
Speaker 9:  Actually. But I think, I think one of the things in in here that kind of

879
00:49:16,040 --> 00:49:19,080
Speaker 9:  jumped out to me, what exactly was he hoping to get back, get out of all

880
00:49:19,080 --> 00:49:22,200
Speaker 9:  these donations and making these donations secretly? Like, was he just like,

881
00:49:22,200 --> 00:49:25,040
Speaker 9:  I've, I really want these people to be elected. Did he think that he could

882
00:49:25,040 --> 00:49:28,320
Speaker 9:  get favors back once they were in? Or, or is, is there any kind of

883
00:49:28,800 --> 00:49:30,920
Speaker 9:  clear, clear word on that?

884
00:49:31,130 --> 00:49:35,080
Speaker 8:  So I, this would've been, oh my gosh, at least

885
00:49:35,080 --> 00:49:38,920
Speaker 8:  six months ago. But I think I interviewed someone at,

886
00:49:39,380 --> 00:49:42,880
Speaker 8:  you know, his spending arm or something. I forget what the, I forget what

887
00:49:42,880 --> 00:49:45,440
Speaker 8:  the organization was called. And because

888
00:49:45,440 --> 00:49:48,440
Speaker 1:  He was pretty open about the spending this money. Oh, before

889
00:49:48,440 --> 00:49:52,360
Speaker 8:  He was, yes. Well he was very open about the democratic spending. The Republican,

890
00:49:52,390 --> 00:49:56,280
Speaker 8:  yeah. Not the Republican spending. And he was, I guess

891
00:49:56,280 --> 00:49:58,440
Speaker 8:  we didn't really know, you know what I mean? We had like certain numbers.

892
00:49:58,440 --> 00:50:02,200
Speaker 8:  I think at the time it was like, I don't know it, it wasn't an illegal amount.

893
00:50:02,200 --> 00:50:05,840
Speaker 8:  Yeah. But now tens of million do tens of millions of dollars. I can't even

894
00:50:05,840 --> 00:50:09,560
Speaker 8:  say it spent. That is definitely illegal. Especially. So he

895
00:50:09,560 --> 00:50:12,960
Speaker 8:  did not wanna be seen on the Republican stuff. He wanted to be looking at

896
00:50:12,960 --> 00:50:15,200
Speaker 8:  like this progressive cool guy

897
00:50:16,540 --> 00:50:20,320
Speaker 8:  who, you know, just had the best Wait, what was it? He wanted He was

898
00:50:20,520 --> 00:50:23,640
Speaker 8:  pandemic preparedness. Okay. Is what he said it was. But if you look at the

899
00:50:23,640 --> 00:50:26,600
Speaker 8:  people who, the candidates who he supported, they're very like friendly to

900
00:50:26,600 --> 00:50:29,760
Speaker 8:  crypto. Yeah. Representative Maxwell Frost, who is the junior

901
00:50:30,440 --> 00:50:34,120
Speaker 8:  congressman from Florida now, like he's the first Gen Z congressman, he took

902
00:50:34,120 --> 00:50:37,960
Speaker 8:  some money, his campaign took money. I don't wanna say he did, cuz it

903
00:50:37,960 --> 00:50:41,440
Speaker 8:  wasn't dif, it wasn't like their hands exchanging it, which maybe would've

904
00:50:41,440 --> 00:50:44,160
Speaker 1:  Thank you for this money. I'm gonna go use it to boost Crypto

905
00:50:44,310 --> 00:50:48,120
Speaker 8:  Would've raised eyebrows probably earlier. Could you imagine

906
00:50:48,120 --> 00:50:51,480
Speaker 8:  like doing a, like an unlawful campaign finance

907
00:50:51,920 --> 00:50:53,200
Speaker 8:  exchange? Like on the street?

908
00:50:53,200 --> 00:50:56,720
Speaker 1:  Just on the street. Just what? In May, Miami? Is that where it

909
00:50:56,720 --> 00:50:59,840
Speaker 8:  Was? I know, I don't know. But I we're in Florida. I love the idea of like

910
00:50:59,840 --> 00:51:03,440
Speaker 8:  a person in a trench coat, like being like, here's some money for

911
00:51:03,450 --> 00:51:04,560
Speaker 8:  digital ads.

912
00:51:06,910 --> 00:51:10,160
Speaker 1:  Just opening it. Right? He's got all the digital ads like right here.

913
00:51:10,650 --> 00:51:13,080
Speaker 8:  No whole ad exchange in your coat.

914
00:51:14,260 --> 00:51:15,400
Speaker 8:  I'm taking it to Google.

915
00:51:16,440 --> 00:51:17,840
Speaker 1:  Google will buy it for $5.

916
00:51:18,240 --> 00:51:21,200
Speaker 8:  That's the competition Google is talking about. Yeah. When it comes to like

917
00:51:21,200 --> 00:51:24,920
Speaker 8:  the ads case that they're being sued by in the doj, it's like, it's the guy

918
00:51:24,920 --> 00:51:28,360
Speaker 8:  on the, it's that the guy on the street. Like your little mom and pop ad

919
00:51:28,360 --> 00:51:32,240
Speaker 8:  seller. Yeah. That definitely exists. Okay. So back to the

920
00:51:32,240 --> 00:51:35,320
Speaker 8:  point. SPF F. Right? So he wanted to be seen as a democratic spender. He,

921
00:51:35,320 --> 00:51:39,120
Speaker 8:  but if you look at the candidates, they're all very pro crypto. So

922
00:51:39,120 --> 00:51:42,720
Speaker 8:  there are folks who at least now are voicing a little bit of skepticism,

923
00:51:42,720 --> 00:51:45,680
Speaker 8:  but they're like, but crypto, we need to keep it around. It's the future.

924
00:51:45,680 --> 00:51:49,560
Speaker 8:  Yeah. So those, that's where a lot of the PAC spending. Okay. He sup. So

925
00:51:49,560 --> 00:51:52,320
Speaker 8:  he would send money to PACS that then supported Right. You know, certain

926
00:51:52,320 --> 00:51:52,760
Speaker 8:  candidates

927
00:51:52,760 --> 00:51:56,160
Speaker 1:  Because you, the, the limit to send directly to candidates is fairly small,

928
00:51:56,160 --> 00:52:00,040
Speaker 8:  Right? Yeah. Right. It's like $1,700 or something. Yeah. It's like,

929
00:52:00,040 --> 00:52:00,560
Speaker 8:  that's

930
00:52:00,670 --> 00:52:04,160
Speaker 1:  A lot of money for a lot of people, but fairly small for someone like S B

931
00:52:04,160 --> 00:52:07,360
Speaker 1:  F who is just blowing through tens of millions of dollars of other people's

932
00:52:07,360 --> 00:52:07,560
Speaker 1:  money.

933
00:52:07,560 --> 00:52:08,120
Speaker 8:  Right.

934
00:52:08,610 --> 00:52:08,960
Speaker 1:  So,

935
00:52:09,090 --> 00:52:09,720
Speaker 8:  Oh my goodness.

936
00:52:09,720 --> 00:52:12,840
Speaker 1:  So he would go, he'd take this money, he'd take you to the packs, and then

937
00:52:12,840 --> 00:52:16,520
Speaker 1:  he would talk with his two, his, his, his Republican guy and his his

938
00:52:16,760 --> 00:52:19,880
Speaker 1:  democrat guy. And they would like, didn't see the money go from the packs

939
00:52:19,880 --> 00:52:20,320
Speaker 1:  elsewhere.

940
00:52:20,400 --> 00:52:24,320
Speaker 8:  Right. So it you, so it's very easy when you can like people,

941
00:52:24,330 --> 00:52:28,320
Speaker 8:  it, this is like so hard for people who don't cover this or like paint

942
00:52:28,320 --> 00:52:28,600
Speaker 8:  very

943
00:52:28,600 --> 00:52:29,640
Speaker 1:  Explain it to me.

944
00:52:30,450 --> 00:52:34,440
Speaker 8:  So there are packs political action groups. Yeah. Political action committees.

945
00:52:34,440 --> 00:52:38,160
Speaker 8:  And what they do is they're very public about who they endorse. Right. And

946
00:52:38,160 --> 00:52:41,320
Speaker 8:  the candidates they type like to endorse. So if you wanna get money to a

947
00:52:41,320 --> 00:52:44,920
Speaker 8:  pack, which can receive much larger amount of money than a campaign, can

948
00:52:45,760 --> 00:52:49,480
Speaker 8:  you find the people who have endorsed the folks that you like? So this pack,

949
00:52:49,480 --> 00:52:53,000
Speaker 8:  they didn't name it in the indictment, had supported a candidate that

950
00:52:53,200 --> 00:52:57,000
Speaker 8:  sbf liked. So he gives a bunch of money through this, a name

951
00:52:57,000 --> 00:53:00,760
Speaker 8:  co-conspirator to the pack, which then will probably, what does APAC do?

952
00:53:00,760 --> 00:53:04,600
Speaker 8:  APAC runs ads. APAC does, you know, whatever they wanna do to help support

953
00:53:04,600 --> 00:53:07,520
Speaker 8:  legislative candidates that they want. So that's how that would get there.

954
00:53:07,520 --> 00:53:11,240
Speaker 8:  They would also donate directly to campaigns, but of course that is always

955
00:53:11,240 --> 00:53:14,600
Speaker 8:  going to be a smaller amount of money. Okay. At least when it's tied back

956
00:53:14,600 --> 00:53:18,480
Speaker 8:  to an individual person. What we, I mean if we're looking at, they, they

957
00:53:18,480 --> 00:53:22,000
Speaker 8:  named two co-conspirators. I don't know if there were more, it sounded like

958
00:53:22,000 --> 00:53:25,920
Speaker 8:  there was the indictment said SPF F was in this group chat to

959
00:53:25,940 --> 00:53:29,840
Speaker 8:  the unnamed co-conspirators. And others. And others. So I

960
00:53:29,840 --> 00:53:31,240
Speaker 8:  don't know who the others are.

961
00:53:31,340 --> 00:53:34,040
Speaker 1:  Are we gonna get more of the signal chat?

962
00:53:34,040 --> 00:53:37,440
Speaker 8:  Could you imagine a slow leak before trial? God, that would

963
00:53:37,440 --> 00:53:40,560
Speaker 1:  Be, that's what, that's what, look at Richard's face. Richard wants that

964
00:53:40,560 --> 00:53:41,960
Speaker 1:  more than anything in the world.

965
00:53:42,270 --> 00:53:43,760
Speaker 9:  I want to read those messages.

966
00:53:43,930 --> 00:53:47,080
Speaker 8:  If I was a prosecutor prosecuting, what is this? This case is supposed to

967
00:53:47,080 --> 00:53:48,240
Speaker 8:  come up later this year, right Richard?

968
00:53:49,160 --> 00:53:50,800
Speaker 9:  That is the idea, I think. Yes. Yeah,

969
00:53:50,800 --> 00:53:54,080
Speaker 8:  It's supposed to come up later this year. And could you imagine like the

970
00:53:54,080 --> 00:53:56,600
Speaker 8:  slowly, oh my god, I'm already, I'm already thinking about it.

971
00:53:57,480 --> 00:53:58,160
Speaker 8:  Richard's

972
00:53:58,160 --> 00:54:01,600
Speaker 1:  Like pre-writing blogs right now. Yeah. Already in anticipation.

973
00:54:01,790 --> 00:54:02,280
Speaker 1:  Some

974
00:54:02,480 --> 00:54:06,320
Speaker 8:  Reporter gets like a little, like a screenshot of the signal and everyone's

975
00:54:06,320 --> 00:54:07,600
Speaker 8:  like, how awful.

976
00:54:07,600 --> 00:54:08,120
Speaker 1:  It's gonna

977
00:54:08,120 --> 00:54:12,080
Speaker 8:  Be great. Yeah. And oh my gosh, I, that's gonna happen. Verge prediction,

978
00:54:12,760 --> 00:54:15,600
Speaker 8:  verge McKenna, Kelly prediction here. I'm very excited for the slow leak

979
00:54:15,600 --> 00:54:16,560
Speaker 8:  of signal messages.

980
00:54:16,700 --> 00:54:19,200
Speaker 9:  And I think that's something that, you know, maybe people should be aware

981
00:54:19,200 --> 00:54:22,960
Speaker 9:  of. Look, maybe you aren't running an international fraud worth

982
00:54:22,960 --> 00:54:26,440
Speaker 9:  billions of dollars, but if you are sending and receiving signal messages,

983
00:54:26,440 --> 00:54:29,920
Speaker 9:  especially if you're in your group chat, the messages are encrypted end to

984
00:54:29,920 --> 00:54:33,280
Speaker 9:  end. So they can't be intercepted and decrypted on the way. However,

985
00:54:34,090 --> 00:54:37,960
Speaker 9:  if someone like the FBI gets

986
00:54:37,960 --> 00:54:40,880
Speaker 9:  ahold of your friend's phone, they can just see what's in the chat.

987
00:54:41,180 --> 00:54:44,520
Speaker 1:  You set up those disappearing ones. Like even Instagram now is like, do you

988
00:54:44,520 --> 00:54:48,160
Speaker 1:  wanna disappear this message? And I'm like, I do. No one needs to know how

989
00:54:48,160 --> 00:54:50,440
Speaker 1:  stupid i I was at 2:00 AM. Right.

990
00:54:50,780 --> 00:54:53,120
Speaker 9:  You don't need to write everything down. You can just ha you can just talk

991
00:54:53,120 --> 00:54:56,720
Speaker 9:  with your voices perhaps and not into microphones

992
00:54:56,720 --> 00:54:57,960
Speaker 9:  like these that we're talking

993
00:54:57,960 --> 00:54:59,840
Speaker 1:  To right now. Just to avoid indictments with Richard

994
00:55:00,320 --> 00:55:03,960
Speaker 9:  R jcc. Coin is satire. Just for any

995
00:55:03,960 --> 00:55:04,640
Speaker 9:  agent's listening.

996
00:55:06,450 --> 00:55:09,880
Speaker 8:  Oh my gosh. Don't speak into a live

997
00:55:09,880 --> 00:55:11,680
Speaker 8:  microphone when committing crimes.

998
00:55:12,400 --> 00:55:15,640
Speaker 1:  Don't, don't go and talk to, what was it, the New York Times?

999
00:55:16,530 --> 00:55:19,360
Speaker 9:  He talked to everyone. He talked to everyone. Tim McNare was on the media

1000
00:55:19,360 --> 00:55:22,600
Speaker 9:  tour, the likes of which we've never seen before. I love it. Did

1001
00:55:22,600 --> 00:55:25,160
Speaker 1:  That, did that come up in the indictment? What, what were the other, was

1002
00:55:25,160 --> 00:55:27,200
Speaker 1:  there any other big news that came out of this indictment?

1003
00:55:27,600 --> 00:55:29,160
Speaker 8:  Let me think. I

1004
00:55:29,790 --> 00:55:32,680
Speaker 9:  Well, I think one of the, one of the things I was looking at that I thought

1005
00:55:32,680 --> 00:55:36,160
Speaker 9:  was really interesting was kind of how they broke down how the customer

1006
00:55:36,160 --> 00:55:39,880
Speaker 9:  deposits came in. And he was, he really, he was using like personal bank

1007
00:55:39,880 --> 00:55:43,680
Speaker 9:  accounts because his business couldn't get a bank account. Oh God. He's so

1008
00:55:43,680 --> 00:55:44,400
Speaker 9:  messy. And he

1009
00:55:44,400 --> 00:55:48,000
Speaker 8:  Was like getting bank accounts under different names of different

1010
00:55:48,000 --> 00:55:49,480
Speaker 8:  companies that he was like, making up.

1011
00:55:49,560 --> 00:55:50,520
Speaker 9:  Seems legit.

1012
00:55:50,550 --> 00:55:54,520
Speaker 8:  Yeah, that's exactly what you do. Oh my gosh, that's my favorite thing. And

1013
00:55:54,520 --> 00:55:56,880
Speaker 8:  it's my least favorite thing. I'd say my favorite thing, but it's actually

1014
00:55:56,880 --> 00:55:59,840
Speaker 8:  my least favorite thing. It's like when you make a company and then you like

1015
00:55:59,840 --> 00:56:03,520
Speaker 8:  get it, you start in, you register it in Wyoming and then you re-register

1016
00:56:03,520 --> 00:56:07,160
Speaker 8:  it in Delaware and it's like, where's, where's this company? Where's, what

1017
00:56:07,160 --> 00:56:08,880
Speaker 8:  does it do? What does it do?

1018
00:56:09,680 --> 00:56:13,560
Speaker 1:  It exi It does, it exists to launder finance money

1019
00:56:13,930 --> 00:56:17,920
Speaker 8:  Or just can't look at anything. Yeah. Or if you don't wanna know who is

1020
00:56:17,920 --> 00:56:20,960
Speaker 8:  running the company, it's just a name. This is, that's like one of the things

1021
00:56:20,960 --> 00:56:24,440
Speaker 8:  that I have learned to do recently is to like go through those

1022
00:56:24,840 --> 00:56:28,800
Speaker 8:  business registration things and it's so nuts how those work. But

1023
00:56:28,800 --> 00:56:32,040
Speaker 8:  yeah, there's so many ways to hide so many things, whether it's like a business

1024
00:56:32,040 --> 00:56:33,320
Speaker 8:  thing and money, whatever.

1025
00:56:34,100 --> 00:56:38,040
Speaker 1:  FDF didn't use any of them? No, none of them. All right, well we're

1026
00:56:38,040 --> 00:56:41,200
Speaker 1:  gonna, we're gonna take a quick break and when we get back, McKenna and Richard

1027
00:56:41,200 --> 00:56:44,040
Speaker 1:  are gonna stick around. We're gonna have a lightning round. It's gonna be

1028
00:56:44,160 --> 00:56:44,280
Speaker 1:  exciting.

1029
00:56:50,590 --> 00:56:54,560
Speaker 1:  Okay, we're back. It's a lightning round. There's actually a, a

1030
00:56:54,560 --> 00:56:57,040
Speaker 1:  fair amount of news. It was kind of a short week. President's Day was on

1031
00:56:57,040 --> 00:57:00,920
Speaker 1:  Monday, so it was, it was a little quiet except for Chris Welch breaking

1032
00:57:00,920 --> 00:57:04,800
Speaker 1:  big Sonos News. We're gonna have a really great episode of the Vercast that

1033
00:57:04,800 --> 00:57:07,600
Speaker 1:  goes super deep dive into that. So we're not gonna be talking about that

1034
00:57:07,600 --> 00:57:11,440
Speaker 1:  today in the lightning round. We are, however, gonna be talking about Spotify's

1035
00:57:11,440 --> 00:57:15,240
Speaker 1:  new AI powered dj. Richard, do you have thoughts?

1036
00:57:15,790 --> 00:57:19,560
Speaker 9:  I don't trust it. As we've said before, I'm anti

1037
00:57:19,690 --> 00:57:20,240
Speaker 9:  ai, I

1038
00:57:21,380 --> 00:57:23,280
Speaker 1:  Pro crypto anti ai. I get it.

1039
00:57:23,500 --> 00:57:27,120
Speaker 9:  You, you gotta, you gotta pick your battles. But so Spotify, they will create

1040
00:57:27,120 --> 00:57:29,480
Speaker 9:  a custom playlist and then they'll have someone talk over it because that's

1041
00:57:29,480 --> 00:57:32,880
Speaker 9:  what you wanted to hear was a computer generated voice talking over your

1042
00:57:32,880 --> 00:57:36,760
Speaker 9:  music. Like, unless it's Def punk, then no, that's not, that's not

1043
00:57:36,760 --> 00:57:37,160
Speaker 9:  what I wanted.

1044
00:57:39,060 --> 00:57:41,360
Speaker 9:  Did Was this something that people were asking for? Well,

1045
00:57:41,360 --> 00:57:44,800
Speaker 1:  McKenna, you were talking before the show about how like this is a thing

1046
00:57:44,800 --> 00:57:45,920
Speaker 1:  on TikTok already.

1047
00:57:45,920 --> 00:57:49,800
Speaker 8:  Well, okay, so no, now I'm actually kind of disturbed

1048
00:57:50,490 --> 00:57:53,440
Speaker 8:  by hearing that there was an a, because I was like, what is the difference

1049
00:57:53,440 --> 00:57:56,560
Speaker 8:  between an AI playlist and like an algorithm? You know, something that like

1050
00:57:56,560 --> 00:57:59,120
Speaker 8:  learns your interest. I was like, that sounds stupid. What are they doing?

1051
00:57:59,120 --> 00:58:02,320
Speaker 8:  Are they just like hopping on like the brand train? Yeah. Or like the branding

1052
00:58:02,320 --> 00:58:06,160
Speaker 8:  train to be like, we have ai, but no, I did not know that there was a

1053
00:58:06,160 --> 00:58:09,880
Speaker 8:  voice. Can you imagine if they sold voice packs or something?

1054
00:58:10,390 --> 00:58:13,960
Speaker 1:  A hundred percent That's coming. You're gonna get a Joe Rogan voice pack?

1055
00:58:14,030 --> 00:58:17,840
Speaker 1:  Well, well maybe not because I think, I think they're trying to, to

1056
00:58:18,080 --> 00:58:18,200
Speaker 1:  separate.

1057
00:58:18,470 --> 00:58:22,360
Speaker 8:  Yeah. Well I mean the thing is is the Spotify playlists are

1058
00:58:22,550 --> 00:58:26,080
Speaker 8:  very well beloved and well hated from musicians,

1059
00:58:26,080 --> 00:58:29,200
Speaker 8:  especially like smaller musicians, right? Because like if you get on a Spotify

1060
00:58:29,210 --> 00:58:32,760
Speaker 8:  or an Apple playlist, like that's crazy that like your song's gonna do numbers,

1061
00:58:32,760 --> 00:58:36,280
Speaker 8:  people just go to these things. But at the same time it's like you can't,

1062
00:58:36,280 --> 00:58:39,600
Speaker 8:  it's hard to find any other reach. Yeah. Besides these playlists and like

1063
00:58:39,600 --> 00:58:43,360
Speaker 8:  going viral online. And so something that was very interesting going on is

1064
00:58:43,360 --> 00:58:46,360
Speaker 8:  like on TikTok there's people who were, I, I forget her name, but there's

1065
00:58:46,360 --> 00:58:49,840
Speaker 8:  a person who makes curates like Spotify playlists

1066
00:58:49,940 --> 00:58:53,000
Speaker 8:  and went viral on like just putting all these playlists out there for different

1067
00:58:53,000 --> 00:58:56,840
Speaker 8:  moods for different things. And of course like it's something that people

1068
00:58:56,840 --> 00:59:00,520
Speaker 8:  wanted people seeking new music, new things, just somebody to

1069
00:59:00,520 --> 00:59:04,480
Speaker 8:  curate it for them. Yeah. And I mean it was, it's nice that you, some

1070
00:59:04,480 --> 00:59:08,280
Speaker 8:  they actually had a person editorializing that and then having I guess like

1071
00:59:08,280 --> 00:59:11,600
Speaker 8:  the off, you know, they would explain it in a TikTok video, but like the

1072
00:59:11,600 --> 00:59:13,800
Speaker 8:  idea of having a DJ in the middle

1073
00:59:13,800 --> 00:59:15,080
Speaker 1:  Of it, an AI dj.

1074
00:59:15,290 --> 00:59:18,520
Speaker 9:  So it's pretending to be a person that picks up music for you, but it is

1075
00:59:18,520 --> 00:59:19,720
Speaker 9:  not actually a person. Is

1076
00:59:19,720 --> 00:59:22,880
Speaker 8:  It just gonna be like Siri I, or like what is the voice doing

1077
00:59:22,880 --> 00:59:25,680
Speaker 9:  To sound like, it sounds like they've got like script writers. They, you

1078
00:59:25,680 --> 00:59:29,440
Speaker 9:  know, they're really trying to create a whole se probably more than one vibe

1079
00:59:29,440 --> 00:59:32,840
Speaker 9:  that it can take and then it can, the AI generated voice can say whatever

1080
00:59:32,840 --> 00:59:34,120
Speaker 9:  the writers decide you

1081
00:59:34,120 --> 00:59:37,560
Speaker 1:  Might wanna hear. Okay. So the voice will be based on Xavier X

1082
00:59:38,110 --> 00:59:41,440
Speaker 1:  J again, and I cannot knock the name X because that's what I was known as

1083
00:59:41,440 --> 00:59:45,160
Speaker 1:  in college. It was a dumb and I can't explain it. But anyway, so X

1084
00:59:45,160 --> 00:59:49,120
Speaker 1:  is gonna be their, their DJ voice and apparently they're

1085
00:59:49,120 --> 00:59:52,960
Speaker 1:  already working on ho they're hosting some podcasts and stuff already for

1086
00:59:52,960 --> 00:59:54,280
Speaker 1:  Spotify. So like

1087
00:59:54,280 --> 00:59:55,800
Speaker 8:  It's a familiar voice in a way.

1088
00:59:56,220 --> 00:59:59,840
Speaker 1:  Theoretically it'll be a familiar voice for Spotify listeners

1089
01:00:00,140 --> 01:00:03,200
Speaker 1:  who also listen to the Get Up, which is

1090
01:00:03,360 --> 01:00:04,520
Speaker 1:  Spotify's podcast.

1091
01:00:04,810 --> 01:00:08,200
Speaker 8:  In a way, this kind of reminds me there are drivers

1092
01:00:08,500 --> 01:00:12,480
Speaker 8:  and I think like a lot of drivers who are either Uber drivers,

1093
01:00:12,480 --> 01:00:15,640
Speaker 8:  Lyft drivers, every single time I'm in there. Cuz that's the only instance

1094
01:00:15,640 --> 01:00:18,440
Speaker 8:  that I'm really in a car. Yeah. And I don't really have friends who have

1095
01:00:18,440 --> 01:00:21,480
Speaker 8:  cars in New York. So when you're in there, a lot of the time they play those

1096
01:00:21,590 --> 01:00:25,360
Speaker 8:  radio, like those DJ stations, right. Where there's a DJ who is like blending

1097
01:00:25,360 --> 01:00:28,720
Speaker 8:  things in during callouts and stuff. That's the only thing I can think of

1098
01:00:28,720 --> 01:00:31,680
Speaker 8:  where people actually listen to music on the radio still.

1099
01:00:31,680 --> 01:00:32,120
Speaker 1:  Yeah.

1100
01:00:32,520 --> 01:00:35,960
Speaker 8:  And if that's the only, you know, if they're trying to take that on, I don't

1101
01:00:35,960 --> 01:00:37,280
Speaker 8:  know, that seems like a kind of silly thing,

1102
01:00:37,380 --> 01:00:41,200
Speaker 9:  But it it, once they have this voice they can have it, you know, obviously

1103
01:00:41,200 --> 01:00:44,640
Speaker 9:  they can start reading stuff off the artist Wiki page as the, as the song

1104
01:00:44,640 --> 01:00:48,160
Speaker 9:  starts. Or maybe where they're playing or maybe or maybe a, a new, a concert

1105
01:00:48,160 --> 01:00:51,720
Speaker 9:  nearby where you can buy a ticket or something or just a, that kind of basic

1106
01:00:51,720 --> 01:00:55,520
Speaker 9:  advertisement. But I, this is just interesting. You remember when Apple Music

1107
01:00:55,520 --> 01:00:59,320
Speaker 9:  launched and they suddenly paid a bunch of real DJs, a lot of

1108
01:00:59,320 --> 01:01:02,600
Speaker 9:  money to come on and like curate playlists and record shows and things.

1109
01:01:02,730 --> 01:01:06,560
Speaker 9:  Spotify decided, no, we gave Joe Rogan a trillion

1110
01:01:06,560 --> 01:01:09,680
Speaker 9:  dollars. So we have no money left for DJs, so we'll have a computer do

1111
01:01:09,680 --> 01:01:13,280
Speaker 1:  It. We're gonna give the ai the DJ's job. Aw,

1112
01:01:13,680 --> 01:01:14,240
Speaker 9:  Great

1113
01:01:14,240 --> 01:01:17,760
Speaker 8:  Job. Doesn't title do some of that too? Or am I just making that up?

1114
01:01:17,970 --> 01:01:19,200
Speaker 9:  No one knows. No,

1115
01:01:19,250 --> 01:01:20,640
Speaker 1:  No one listens to title.

1116
01:01:20,650 --> 01:01:22,960
Speaker 8:  My fiance uses title.

1117
01:01:23,380 --> 01:01:24,960
Speaker 1:  The one we found him. He's,

1118
01:01:24,960 --> 01:01:28,720
Speaker 9:  He's, we have a title user at, at at at the Verge. I I will

1119
01:01:28,720 --> 01:01:30,840
Speaker 9:  keep their identity confidential.

1120
01:01:30,910 --> 01:01:34,600
Speaker 1:  I think I I really respect you for that, but I wanna know who it is. It's

1121
01:01:34,600 --> 01:01:37,880
Speaker 1:  Neely, isn't it? Neely, just he's gotta have that, like, that flack quality

1122
01:01:38,230 --> 01:01:42,200
Speaker 1:  Lossless audio while he is driving up and down the, in his giant truck,

1123
01:01:42,200 --> 01:01:42,680
Speaker 1:  mid truck.

1124
01:01:44,780 --> 01:01:48,560
Speaker 1:  All right, so, so some other news that happened this week. Microsoft

1125
01:01:48,630 --> 01:01:52,520
Speaker 1:  went to Brussels. Tom Warren was also in Brussels. He's not on the show today

1126
01:01:52,520 --> 01:01:54,040
Speaker 1:  because he is recovering from going to

1127
01:01:54,040 --> 01:01:54,240
Speaker 8:  Brus,

1128
01:01:54,240 --> 01:01:57,040
Speaker 1:  Brussels. He just messaged me one day. I was like, I have to go to Brussels.

1129
01:01:57,040 --> 01:02:00,840
Speaker 1:  And I was like, why? And why am I learning about, why do I need to know this?

1130
01:02:00,840 --> 01:02:04,480
Speaker 1:  Oh, it's for work. Okay, so, so Microsoft went to

1131
01:02:04,480 --> 01:02:08,160
Speaker 1:  Brussels because they're trying to defend their, their right to acquire

1132
01:02:08,160 --> 01:02:12,040
Speaker 1:  Activision. And a lot of people are upset with that, notably Sony. Sony

1133
01:02:12,040 --> 01:02:15,120
Speaker 1:  does not want them to acquire it because then they'll be like, well we won't

1134
01:02:15,120 --> 01:02:18,360
Speaker 1:  get all the games. You're gonna, you're gonna cut us out. And so Microsoft

1135
01:02:18,360 --> 01:02:21,480
Speaker 1:  goes to, to brussel to defend and they brought some people with them. They

1136
01:02:21,480 --> 01:02:24,680
Speaker 1:  also had a bunch of big timely announcements at the same time, including

1137
01:02:24,710 --> 01:02:28,040
Speaker 1:  with Nvidia, where they said, okay, we signed a deal. All of our games are

1138
01:02:28,040 --> 01:02:31,440
Speaker 1:  gonna be coming to Nvidia, GForce Now's cloud streaming. It's gonna be really

1139
01:02:31,440 --> 01:02:35,040
Speaker 1:  great. It's gonna be super exciting. And also Call of Duty Day and date

1140
01:02:35,090 --> 01:02:38,560
Speaker 1:  on Nintendo products. So the Switch,

1141
01:02:39,110 --> 01:02:42,520
Speaker 9:  That sounds a little bit odd that you would be playing Call of Duty on the

1142
01:02:42,520 --> 01:02:46,320
Speaker 9:  Switch, but if they release a switch too, it makes a little bit more sense.

1143
01:02:46,540 --> 01:02:50,120
Speaker 8:  Yes. Yeah. Cuz I can't even play Scarlet Violet on my

1144
01:02:50,120 --> 01:02:52,440
Speaker 8:  Nintendo Switch without running into like,

1145
01:02:52,450 --> 01:02:52,800
Speaker 1:  It,

1146
01:02:52,800 --> 01:02:53,480
Speaker 8:  It frame issue,

1147
01:02:53,810 --> 01:02:54,480
Speaker 1:  It struggles,

1148
01:02:54,630 --> 01:02:56,120
Speaker 8:  It's off and it gets hot. Yeah.

1149
01:02:57,670 --> 01:03:00,360
Speaker 8:  I mean people play Call of Duty on their phones, right? You can play like

1150
01:03:00,360 --> 01:03:01,520
Speaker 8:  War Zone or whatever on

1151
01:03:01,520 --> 01:03:04,240
Speaker 1:  Your phone. Yeah. And, and but this is gonna be those, like the, the ones

1152
01:03:04,240 --> 01:03:07,120
Speaker 1:  that are meant for the pc, the one that's coming for the Xbox Series X and

1153
01:03:07,120 --> 01:03:10,760
Speaker 1:  stuff like that. I was really surprised that we didn't actually hear game

1154
01:03:10,760 --> 01:03:14,600
Speaker 1:  passes coming to the Nintendo Switch, which would've made sense, right? Like

1155
01:03:14,600 --> 01:03:18,560
Speaker 1:  have cloud gaming on the switch. Huge great for Microsoft, but I guess that

1156
01:03:18,560 --> 01:03:22,080
Speaker 1:  was probably would not have been great for Nintendo. Who wants to sell games?

1157
01:03:22,080 --> 01:03:22,520
Speaker 1:  Wants

1158
01:03:22,520 --> 01:03:26,360
Speaker 8:  To sell games, and then also and would game pass work on the

1159
01:03:26,360 --> 01:03:27,920
Speaker 8:  switch as it is right now. Like

1160
01:03:27,970 --> 01:03:31,400
Speaker 9:  If, if you stream the games, I guess. Okay. Because some games do stream

1161
01:03:31,400 --> 01:03:33,840
Speaker 9:  on the switch, right? Like there are some kind of like high performance games

1162
01:03:33,840 --> 01:03:37,240
Speaker 9:  to Stream plus like all the emulated games I think like the, the Nintendo

1163
01:03:37,240 --> 01:03:37,800
Speaker 9:  online stuff.

1164
01:03:38,120 --> 01:03:41,640
Speaker 1:  Yeah, yeah, yeah. And, and that, that all like streams

1165
01:03:41,820 --> 01:03:45,120
Speaker 1:  and, and you know, I use my steam deck for streaming all the time and it

1166
01:03:45,120 --> 01:03:48,800
Speaker 1:  works and it really helps to, to like, I get way more battery life when I

1167
01:03:48,800 --> 01:03:52,640
Speaker 1:  stream than when I try to actually play the game directly on the,

1168
01:03:52,640 --> 01:03:56,600
Speaker 1:  the steam deck. Then it just sounds like it's on fire. Just that in those

1169
01:03:56,600 --> 01:04:00,480
Speaker 1:  fans were very loudly. So, so yeah, this was kind of some

1170
01:04:00,480 --> 01:04:04,160
Speaker 1:  surprising news from Microsoft. It is unclear if,

1171
01:04:04,210 --> 01:04:07,720
Speaker 1:  if you know, kind of regulatory groups like UK's

1172
01:04:07,720 --> 01:04:11,480
Speaker 1:  competition and Markets authority are gonna allow this

1173
01:04:11,480 --> 01:04:15,200
Speaker 1:  to go through right now. Richard, how do you feel? Do you think it's gonna

1174
01:04:15,200 --> 01:04:15,400
Speaker 1:  happen?

1175
01:04:16,230 --> 01:04:20,080
Speaker 9:  I am a bit skeptical just because there's so many hurdles.

1176
01:04:20,230 --> 01:04:24,080
Speaker 9:  I think that there's an issue where the, the, the regulators who are trying

1177
01:04:24,080 --> 01:04:27,160
Speaker 9:  to stop this deal, maybe the case they have isn't so strong, but there's

1178
01:04:27,160 --> 01:04:27,840
Speaker 9:  so many of them.

1179
01:04:27,870 --> 01:04:29,160
Speaker 1:  Yeah. There's

1180
01:04:29,160 --> 01:04:30,000
Speaker 9:  A lot of so

1181
01:04:30,000 --> 01:04:33,280
Speaker 1:  Big and like, and all of Sony is there saying

1182
01:04:33,730 --> 01:04:37,240
Speaker 1:  no. And Sony's argument right now is that like, the arguments for this are

1183
01:04:37,240 --> 01:04:40,560
Speaker 1:  very, very funny because Sony's argument is you're gonna get all the games

1184
01:04:40,560 --> 01:04:43,880
Speaker 1:  and then you're not gonna give us the games or you'll give us the games for

1185
01:04:43,880 --> 01:04:47,080
Speaker 1:  10 years and then you're gonna charge us way more money in 10 years. Which

1186
01:04:47,140 --> 01:04:50,360
Speaker 1:  yes. That, that's what will happen eventually.

1187
01:04:50,600 --> 01:04:54,320
Speaker 1:  Meanwhile, Microsoft's explanation is, yeah, well we

1188
01:04:54,320 --> 01:04:58,280
Speaker 1:  suck and Sony doesn't suck, so we should be allowed to buy it

1189
01:04:58,280 --> 01:05:02,160
Speaker 1:  because we suck more than Sony. And that's just the wildest

1190
01:05:02,160 --> 01:05:05,080
Speaker 1:  like, argument on the planet for this.

1191
01:05:05,720 --> 01:05:09,440
Speaker 9:  Watching game companies insist that they are small and have no market share

1192
01:05:09,530 --> 01:05:13,080
Speaker 9:  is incredible. Yeah. These two absolutely massive giants

1193
01:05:13,320 --> 01:05:16,360
Speaker 9:  worldwide. And Microsoft's just like, yo, but I mean we're, we have no, we

1194
01:05:16,360 --> 01:05:19,680
Speaker 9:  have no share in Europe. Europe though. Yeah. And, and Sony is, well, I mean,

1195
01:05:19,680 --> 01:05:22,680
Speaker 9:  but we, you own so many studios. You have this, you're Microsoft, of course.

1196
01:05:22,680 --> 01:05:26,080
Speaker 9:  It it's just, it's like watching two people just compliment each other. Listen,

1197
01:05:26,760 --> 01:05:30,480
Speaker 9:  don't to to try and prove that, that the other one is actually the better

1198
01:05:30,480 --> 01:05:30,680
Speaker 9:  one.

1199
01:05:30,680 --> 01:05:34,440
Speaker 8:  Doesn't Microsoft bring up the IP stuff with Sony too? Like

1200
01:05:34,440 --> 01:05:38,400
Speaker 8:  Sony's just huge when it comes to, you know, just ip. They

1201
01:05:38,400 --> 01:05:40,520
Speaker 8:  have, I think they, they have Spider-Man, right? I'm

1202
01:05:40,520 --> 01:05:44,320
Speaker 1:  Like, yeah, they've got Spider-Man. They've notably have all of the

1203
01:05:44,320 --> 01:05:47,880
Speaker 1:  naughty dog games. So the last of us, which is, you know,

1204
01:05:48,060 --> 01:05:52,000
Speaker 8:  But even music like Sony music, the company there like, like Sony's

1205
01:05:52,000 --> 01:05:55,960
Speaker 1:  Big, but they're, but this is very narrowly focused on the gaming. And

1206
01:05:56,240 --> 01:06:00,000
Speaker 1:  for the gaming, it's Microsoft saying they suck even though they own

1207
01:06:00,000 --> 01:06:03,400
Speaker 1:  all of the game studios now. And soon, hopefully for them,

1208
01:06:03,400 --> 01:06:07,360
Speaker 1:  Activision and Sony saying we actually kind of suck

1209
01:06:07,360 --> 01:06:11,080
Speaker 1:  because yeah, we have all of the market share, but

1210
01:06:11,090 --> 01:06:15,040
Speaker 1:  we won't have Call of Duty and everyone plays Call of Duty and like

1211
01:06:15,040 --> 01:06:18,600
Speaker 1:  that's basically the argument. It's, it's a fight over Call of Duty. Hmm.

1212
01:06:18,940 --> 01:06:22,880
Speaker 1:  10 years ago me would be like very into this fight too. Like, call of

1213
01:06:22,880 --> 01:06:23,960
Speaker 1:  Duty was my life.

1214
01:06:23,960 --> 01:06:26,600
Speaker 9:  Well, and and I think that's one of the other things to consider is that,

1215
01:06:26,600 --> 01:06:29,360
Speaker 9:  especially when it comes to Call of Duty, because they've got the other aspects

1216
01:06:29,360 --> 01:06:32,200
Speaker 9:  of it, the mobile gaming, there are, there are all these other things that

1217
01:06:32,200 --> 01:06:35,880
Speaker 9:  activation owns. Activation is such a massive company. Yeah. But when you,

1218
01:06:35,880 --> 01:06:39,200
Speaker 9:  when you look at like Call of Duty kind of specifically, how long is that

1219
01:06:39,200 --> 01:06:42,440
Speaker 9:  going to be relevant in that way? Do do you expect it has been relevant for

1220
01:06:42,440 --> 01:06:45,000
Speaker 9:  a long time? We, we we can go back to what the mid aughts,

1221
01:06:45,000 --> 01:06:48,920
Speaker 1:  They're running out of like armed conflicts to base the games

1222
01:06:48,920 --> 01:06:49,880
Speaker 1:  on. They're

1223
01:06:49,880 --> 01:06:51,080
Speaker 9:  Now like, they're just repeating the ones they already did.

1224
01:06:51,080 --> 01:06:54,840
Speaker 1:  They're like, what if we do the cold word now? What if we do it in space?

1225
01:06:54,930 --> 01:06:55,280
Speaker 1:  Oh

1226
01:06:55,280 --> 01:06:58,680
Speaker 8:  My goodness. Well what is like Call of Duty's, like Twitch streaming numbers

1227
01:06:58,740 --> 01:07:02,680
Speaker 8:  now compared to maybe like even five years ago? Has it gone down?

1228
01:07:02,680 --> 01:07:05,200
Speaker 8:  Cuz like the only FPSs, I I don't play FPSs

1229
01:07:05,200 --> 01:07:08,720
Speaker 1:  Call of Duty is still massive. Like War Zone is still a really, really big

1230
01:07:08,720 --> 01:07:12,560
Speaker 1:  game. It's, you know, I know there's some Apex Legends fans here

1231
01:07:12,560 --> 01:07:13,840
Speaker 1:  right now, Richard, hello

1232
01:07:14,470 --> 01:07:15,040
Speaker 9:  With us.

1233
01:07:15,820 --> 01:07:19,720
Speaker 1:  But, but Call of Duty is still really, really, really big. And, and

1234
01:07:19,720 --> 01:07:22,880
Speaker 1:  it's not, I mean it isn't just Call of Duty, right? Like Activision owns

1235
01:07:22,880 --> 01:07:26,720
Speaker 1:  a lot of these other games and so it, it

1236
01:07:26,720 --> 01:07:30,600
Speaker 1:  makes sense that Sony's gonna be like, no. Right. That's like a

1237
01:07:30,600 --> 01:07:31,400
Speaker 1:  half of our catalog.

1238
01:07:32,500 --> 01:07:34,560
Speaker 8:  But you can play Call of Duty on your switch.

1239
01:07:34,590 --> 01:07:38,120
Speaker 1:  Yeah. Well I think the idea is like go, if you go with

1240
01:07:38,790 --> 01:07:42,360
Speaker 1:  Sony, if, if Activision gets successfully bought by

1241
01:07:42,600 --> 01:07:46,560
Speaker 1:  Microsoft, Sony is then becomes the noon Nintendo

1242
01:07:46,560 --> 01:07:49,680
Speaker 1:  where they're like, we can only ha like we're just doing naughty dog games

1243
01:07:50,280 --> 01:07:53,920
Speaker 1:  and Spider-Man forever. And then one day we'll try, what was it?

1244
01:07:54,160 --> 01:07:56,520
Speaker 1:  1893? 1883. 1886.

1245
01:07:56,640 --> 01:07:57,280
Speaker 9:  1890, yes.

1246
01:07:57,800 --> 01:08:01,640
Speaker 1:  1886. There we go. That 1886 game, they're going to keep

1247
01:08:01,640 --> 01:08:02,560
Speaker 1:  trying to bring that back and

1248
01:08:02,560 --> 01:08:03,720
Speaker 9:  Make that happen. And Stray

1249
01:08:03,900 --> 01:08:06,640
Speaker 1:  And Stray. I didn't like Stray. Did you

1250
01:08:06,640 --> 01:08:09,160
Speaker 8:  Play Stray? I did not play it. But

1251
01:08:09,430 --> 01:08:10,600
Speaker 1:  People really like

1252
01:08:10,600 --> 01:08:14,480
Speaker 8:  It. I just got a gaming pc. I only had a switch. Yeah. I just got a gaming

1253
01:08:14,480 --> 01:08:16,840
Speaker 8:  PC for the, the first time like a couple weeks ago.

1254
01:08:17,130 --> 01:08:18,280
Speaker 1:  Do you just play more? That

1255
01:08:18,280 --> 01:08:19,840
Speaker 9:  Was your response to Stadia going away?

1256
01:08:20,360 --> 01:08:24,240
Speaker 8:  Yes. Okay. So Stadia was literally the only thing keeping me

1257
01:08:24,240 --> 01:08:27,240
Speaker 8:  from getting a pc. Yeah. Really?

1258
01:08:28,000 --> 01:08:31,880
Speaker 8:  Because I was using it for Elder Scrolls online. I really, really like

1259
01:08:32,030 --> 01:08:35,680
Speaker 8:  MMOs. Something about it, I, I can't Good, good. I can't play. Like I have

1260
01:08:35,680 --> 01:08:39,480
Speaker 8:  to play like one game. Yeah. With a lot of things in it. Yeah. You know,

1261
01:08:39,480 --> 01:08:42,960
Speaker 8:  that I can do. And so Stadia was allowing me to do that.

1262
01:08:43,390 --> 01:08:47,360
Speaker 8:  Stadia goes away. There are, there's nothing on the

1263
01:08:47,360 --> 01:08:47,640
Speaker 8:  switch.

1264
01:08:47,810 --> 01:08:48,920
Speaker 1:  So you just built a pc.

1265
01:08:49,090 --> 01:08:50,560
Speaker 8:  It didn't build one. You

1266
01:08:50,560 --> 01:08:51,800
Speaker 1:  Bought a, you bought a gaming pc.

1267
01:08:51,870 --> 01:08:52,760
Speaker 8:  I bought a laptop.

1268
01:08:52,760 --> 01:08:54,120
Speaker 1:  There we go. That's

1269
01:08:54,360 --> 01:08:57,480
Speaker 8:  Great. But it works really good cuz I went back on my MacBook and I was somehow

1270
01:08:57,480 --> 01:09:01,160
Speaker 8:  playing World of Warcraft at like, it was running at eight FPSs

1271
01:09:02,770 --> 01:09:04,120
Speaker 8:  or like 11 tops.

1272
01:09:05,570 --> 01:09:06,800
Speaker 1:  So you just died a lot.

1273
01:09:06,950 --> 01:09:10,520
Speaker 8:  Yeah. Yeah. And I was wondering why I was so bad cuz I was reading and I

1274
01:09:10,520 --> 01:09:13,240
Speaker 8:  thought I was really good and I thought I knew everything front and back.

1275
01:09:13,240 --> 01:09:17,120
Speaker 8:  But no, my frame rate, like I, my fiance's friend came over and he like looked

1276
01:09:17,120 --> 01:09:20,360
Speaker 8:  at my computer. He is like, that's making me sick. How are you doing? This

1277
01:09:20,360 --> 01:09:24,240
Speaker 8:  is making me Ill. And I guess like, I was never used to

1278
01:09:24,350 --> 01:09:28,120
Speaker 8:  what it should look like. And now I'm like doing stuff on my, I I

1279
01:09:28,120 --> 01:09:31,960
Speaker 8:  I have like the ACEs Zephyrus G 14. Yeah. I love

1280
01:09:31,960 --> 01:09:32,840
Speaker 8:  it. Monica

1281
01:09:32,920 --> 01:09:34,000
Speaker 1:  Loves that laptop.

1282
01:09:34,240 --> 01:09:37,800
Speaker 8:  That's why I got it. Shout out Monica. I read her review on the site. That's

1283
01:09:37,800 --> 01:09:41,480
Speaker 8:  great. And I got it. And it, it works amazing. I'm getting like 60 fps.

1284
01:09:42,000 --> 01:09:45,240
Speaker 8:  I I don't even know what that looks like. And now I'm, now it's just so fluid

1285
01:09:45,240 --> 01:09:48,600
Speaker 8:  and I, I don't know. Anyways, this is McKenna Discovers computers, a tech

1286
01:09:48,760 --> 01:09:50,120
Speaker 8:  reporter McKenna discovers computers.

1287
01:09:51,150 --> 01:09:51,640
Speaker 1:  It's

1288
01:09:51,640 --> 01:09:54,000
Speaker 9:  Great. McKenna's Gaming Corner. Needs to be a SEC segment. Yeah.

1289
01:09:54,000 --> 01:09:56,120
Speaker 1:  New, new segment. McKenna's Gaming Quarter

1290
01:09:56,840 --> 01:09:59,320
Speaker 8:  Where I learn about the games where

1291
01:09:59,320 --> 01:10:02,920
Speaker 1:  You're like 60 fps. This is cool. I love,

1292
01:10:02,920 --> 01:10:03,360
Speaker 1:  have

1293
01:10:03,360 --> 01:10:04,680
Speaker 8:  You guys ever seen anything like this

1294
01:10:04,680 --> 01:10:07,320
Speaker 1:  Before? They're so smooth.

1295
01:10:08,490 --> 01:10:11,920
Speaker 1:  So other, other big news. Do you have anything you wanna talk about, Richard?

1296
01:10:11,920 --> 01:10:12,800
Speaker 1:  We got some other news.

1297
01:10:13,110 --> 01:10:17,080
Speaker 9:  Well, there was the, Tesla announced a new engineering headquarters in California

1298
01:10:17,080 --> 01:10:19,480
Speaker 9:  this week, which I found really interesting because they just moved their

1299
01:10:19,480 --> 01:10:20,680
Speaker 9:  headquarters out of California.

1300
01:10:20,680 --> 01:10:22,040
Speaker 1:  Right. They went to Texas, right.

1301
01:10:22,230 --> 01:10:26,200
Speaker 9:  Yeah. But now they have another headquarters in California still. So,

1302
01:10:26,420 --> 01:10:27,560
Speaker 1:  But just engineering.

1303
01:10:27,790 --> 01:10:31,160
Speaker 9:  It's just engineering. It's, I find it interesting that it seems like

1304
01:10:31,640 --> 01:10:35,360
Speaker 9:  Elon is moving his manufacturing to hi to the, the location in Texas,

1305
01:10:35,360 --> 01:10:39,200
Speaker 9:  which coincidentally might be a bit closer to the plants in

1306
01:10:39,200 --> 01:10:43,080
Speaker 9:  Mexico that make the parts for the cars. Yeah. Just, just seems like a,

1307
01:10:43,080 --> 01:10:46,400
Speaker 9:  a weird coincidence. That weird, you know, he says that he's moving it because

1308
01:10:46,400 --> 01:10:49,960
Speaker 9:  of the taxes or this or that, but I think there's a really practical reason

1309
01:10:49,960 --> 01:10:52,800
Speaker 9:  for why these things are suddenly being set up that way. And why he would

1310
01:10:52,870 --> 01:10:56,000
Speaker 9:  keep his engineering. They're in California where, where there's plenty of

1311
01:10:56,000 --> 01:10:56,560
Speaker 9:  people to hire.

1312
01:10:56,800 --> 01:10:59,960
Speaker 1:  Had he successfully moved everybody from

1313
01:10:59,990 --> 01:11:03,880
Speaker 1:  California before this? Like had everybody relocated or was

1314
01:11:03,880 --> 01:11:07,800
Speaker 1:  people, or there's still holdouts, engineering holdouts specifically in

1315
01:11:07,800 --> 01:11:08,240
Speaker 1:  California.

1316
01:11:08,590 --> 01:11:12,480
Speaker 9:  That is, that is a good question. I, I won. I wonder, although they, they've

1317
01:11:12,480 --> 01:11:15,720
Speaker 9:  also had layoffs. They, they've had so many things at Tesla. It's, it's really

1318
01:11:15,720 --> 01:11:19,200
Speaker 9:  hard to say how, and they had the, we had his weird demand to come into the

1319
01:11:19,200 --> 01:11:22,920
Speaker 9:  office that everyone must be in the office all the time because he's in the

1320
01:11:22,920 --> 01:11:25,400
Speaker 9:  office and he sleeps in the office and he doesn't live anywhere else except

1321
01:11:25,400 --> 01:11:27,360
Speaker 9:  for when he's at Twitter, where he also lives in the office there.

1322
01:11:28,150 --> 01:11:31,640
Speaker 1:  He's a house. Someone get that man house. In fact, Elon

1323
01:11:31,840 --> 01:11:32,200
Speaker 1:  get

1324
01:11:32,200 --> 01:11:34,840
Speaker 9:  A house. I'm, I'm sure they, they built a really nice couple of bedrooms

1325
01:11:34,840 --> 01:11:36,560
Speaker 9:  in the new engineering hq.

1326
01:11:36,560 --> 01:11:39,800
Speaker 1:  Yeah. Just for him. All right. McKenna, any any fun news for you you wanna

1327
01:11:39,800 --> 01:11:40,120
Speaker 1:  talk about

1328
01:11:40,120 --> 01:11:41,880
Speaker 8:  Today? Any fun news that I wanna talk about?

1329
01:11:43,910 --> 01:11:47,120
Speaker 8:  What's going on? There was scotus. We

1330
01:11:47,120 --> 01:11:47,760
Speaker 1:  Talked about scotus.

1331
01:11:47,760 --> 01:11:48,760
Speaker 8:  Okay, well rest in peace.

1332
01:11:50,770 --> 01:11:51,640
Speaker 8:  No, you

1333
01:11:51,640 --> 01:11:53,240
Speaker 1:  Don't wanna talk about the PCV boards?

1334
01:11:53,240 --> 01:11:53,960
Speaker 8:  What is that?

1335
01:12:00,980 --> 01:12:03,600
Speaker 8:  I'm learning, I'm learning. Dammit.

1336
01:12:03,910 --> 01:12:07,720
Speaker 1:  Tech corner. What is that? No. Gaming corner. Gaming corner. No,

1337
01:12:07,720 --> 01:12:10,400
Speaker 1:  there is one. There's one story I do wanna shout out. It's from Chris person

1338
01:12:10,500 --> 01:12:13,800
Speaker 1:  who is hanging out with us for a while. He just

1339
01:12:14,190 --> 01:12:17,800
Speaker 1:  revealed that if you wanna make your own keyboard and you want it to look

1340
01:12:17,800 --> 01:12:21,320
Speaker 1:  exactly how you want it to look, you can just like pay someone and they'll

1341
01:12:21,320 --> 01:12:24,960
Speaker 1:  print it and you can put little fun designs and stuff on it. Mm. And just

1342
01:12:24,960 --> 01:12:28,680
Speaker 1:  like print the board yourself. You do have to know soldering and a whole

1343
01:12:28,680 --> 01:12:31,880
Speaker 1:  lot of other words. And programming. I was learning a lot of this. I feel

1344
01:12:31,880 --> 01:12:35,560
Speaker 1:  like I could be an engineer working at Tesla, reading this curse person

1345
01:12:35,560 --> 01:12:35,880
Speaker 1:  story

1346
01:12:35,910 --> 01:12:37,640
Speaker 8:  Just in the corner building keyboards.

1347
01:12:38,590 --> 01:12:42,360
Speaker 1:  Like nobody, nobody asked me to do actual engineering work. My soldering

1348
01:12:42,360 --> 01:12:46,120
Speaker 1:  is dope. It is not dope. I do really bad soldering,

1349
01:12:46,420 --> 01:12:49,960
Speaker 1:  but maybe I'll do better with buying a keyboard and then putting like,

1350
01:12:50,060 --> 01:12:53,960
Speaker 1:  you can put anything on it. Like any image you just wanna put in, in like

1351
01:12:53,960 --> 01:12:57,880
Speaker 1:  black and white, you can do on a keyboards pcb, which I think is

1352
01:12:57,880 --> 01:12:58,320
Speaker 1:  really cool.

1353
01:12:58,800 --> 01:12:58,920
Speaker 8:  Hmm.

1354
01:12:59,430 --> 01:13:02,520
Speaker 9:  I love, I love the images here. I would never do any of this myself.

1355
01:13:02,520 --> 01:13:03,360
Speaker 1:  Yeah, you're like

1356
01:13:03,360 --> 01:13:04,680
Speaker 9:  That's, but I'm glad that someone else is doing.

1357
01:13:04,800 --> 01:13:08,640
Speaker 1:  There you go. There you go. Alright. Well

1358
01:13:08,640 --> 01:13:12,560
Speaker 1:  that is it for the show. We have had a very, very busy one. It has been

1359
01:13:12,560 --> 01:13:16,320
Speaker 1:  a very fun one. And we are gonna be back next

1360
01:13:16,320 --> 01:13:20,000
Speaker 1:  week. There's a new episode of our mini-series solo acts that's gonna be

1361
01:13:20,000 --> 01:13:23,840
Speaker 1:  on Monday. It is going to be absolutely delicious. Yes.

1362
01:13:23,840 --> 01:13:27,000
Speaker 1:  Because it's about Mick Ribs. I've never had one, but after listening to

1363
01:13:27,000 --> 01:13:29,640
Speaker 1:  this episode, you're gonna want one. It's really unfortunate because they,

1364
01:13:29,640 --> 01:13:32,920
Speaker 1:  you can't get 'em anywhere. And then on Wednesday we're gonna have another

1365
01:13:32,920 --> 01:13:36,720
Speaker 1:  great episode of the show and we'll be back on Friday. So stay

1366
01:13:36,720 --> 01:13:37,480
Speaker 1:  classy until then.

1367
01:13:40,100 --> 01:13:44,000
Speaker 7:  And that's a wrap for Vergecast this week. Thanks for listening. If you enjoy

1368
01:13:44,000 --> 01:13:47,680
Speaker 7:  the show, subscribe in the podcast app of your choice or tell a friend, you

1369
01:13:47,680 --> 01:13:51,640
Speaker 7:  can send us feedback at bcast@theverge.com. This show is produced by

1370
01:13:51,640 --> 01:13:55,520
Speaker 7:  me, Liam James, and our senior audio director, Andrew Marino. This

1371
01:13:55,520 --> 01:13:59,240
Speaker 7:  episode was edited and mixed by Amanda Rose Smith.

1372
01:13:59,600 --> 01:14:03,120
Speaker 7:  Our editorial director is Brooke Min and our executive producer is

1373
01:14:03,340 --> 01:14:07,280
Speaker 7:  Eleanor Donovan. The Verge is a production of the Verge and Box Media

1374
01:14:07,540 --> 01:14:09,400
Speaker 7:  Podcast network. And that's it. We'll see you next week.

