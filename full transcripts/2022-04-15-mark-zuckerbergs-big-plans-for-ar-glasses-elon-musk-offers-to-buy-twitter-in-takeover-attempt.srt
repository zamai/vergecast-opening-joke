1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 9eeda0b0-bd1c-11ec-ab72-3b8274bd4ba1
Status: Done
Stage: Done
Title: Mark Zuckerbergâ€™s big plans for AR glasses / Elon Musk offers to buy Twitter in takeover attempt
Audio URL: https://jfe93e.s3.amazonaws.com/1922991724938814339/8443761330437407172/s93290-US-5817s-1650063852.mp3
Description: The Verge's Nilay Patel, Liz Lopatto, Alex Cranz, and Alex Heath discuss Elon Musk's offer to buy 100 percent of Twitter and what it could mean for the company.
Senior reporter Adi Robertson joins the show to discuss Elon's limited thoughts on content moderation and Alex Heath's scoop on Meta's plans for their AR glasses.

2
00:00:00,060 --> 00:00:03,850
Speaker 1:  This week on the verge cast, Liz Lopatto, Addie Robertson and Alex

3
00:00:03,850 --> 00:00:07,500
Speaker 1:  Heath joined the show. We talk about Elon Musk's offer to buy

4
00:00:07,500 --> 00:00:10,980
Speaker 1:  Twitter. What that means, what would happen, what he is talking about. When

5
00:00:10,980 --> 00:00:14,190
Speaker 1:  he talks about free speech, then Aussies had big soup on medicine tire AR

6
00:00:14,190 --> 00:00:17,700
Speaker 1:  roadmap. We get into that. That's come up on the Vergecast. Now

7
00:01:35,010 --> 00:01:38,790
Speaker 1:  Hello, and welcome to the Vergecast flagship podcast. The

8
00:01:38,790 --> 00:01:42,390
Speaker 1:  first amendment or the only podcast that reads

9
00:01:42,390 --> 00:01:45,840
Speaker 1:  it, I've read it. I assure you that I've read the first

10
00:01:45,840 --> 00:01:49,650
Speaker 1:  amendment and the associated case law. Look, I'm all about free

11
00:01:49,650 --> 00:01:53,610
Speaker 1:  speech here. And if anyone says I'm not they're censoring me anyway,

12
00:01:53,610 --> 00:01:56,130
Speaker 1:  I'm Neil. I'm your friend Alex grants here.

13
00:01:56,130 --> 00:01:59,850
Speaker 5:  I'm actually buying another social media network. So I too can be a

14
00:01:59,850 --> 00:02:01,350
Speaker 5:  champion of free speech.

15
00:02:01,350 --> 00:02:05,130
Speaker 1:  So that's what you need. Market competition, Alex, Heath, this here.

16
00:02:05,130 --> 00:02:08,940
Speaker 6:  Hi, I am your fire, your one desire.

17
00:02:08,940 --> 00:02:10,950
Speaker 6:  And it's been a long week.

18
00:02:10,950 --> 00:02:13,500
Speaker 1:  The last night I was talking to Alex. He goes, I think I almost passed out

19
00:02:13,500 --> 00:02:16,530
Speaker 1:  today. And I was like, what happened? He's like, I forgot to breathe. Which

20
00:02:16,530 --> 00:02:20,340
Speaker 1:  is like a real thing you said to me, he was like, I was typing

21
00:02:20,340 --> 00:02:23,760
Speaker 1:  so fast. I forgot to breathe. Liz Lopatto was here.

22
00:02:23,760 --> 00:02:27,750
Speaker 7:  I bring with me tidings of Elan, as I think long time listeners. Now

23
00:02:27,750 --> 00:02:31,230
Speaker 1:  The winds of Ilan have brought Liz back to us. Once again, the

24
00:02:31,230 --> 00:02:33,360
Speaker 7:  Winds are changing.

25
00:02:33,360 --> 00:02:37,140
Speaker 1:  So we got to talk about it and buy it. I mean,

26
00:02:37,140 --> 00:02:40,230
Speaker 1:  Intel's next generation. I don't mean that at all. What I mean is that Elon

27
00:02:40,230 --> 00:02:43,170
Speaker 1:  Musk threatened to buy Twitter this week. He blew up our entire publishing

28
00:02:43,170 --> 00:02:47,130
Speaker 1:  schedule. We had so many great stories this week and suddenly our whole

29
00:02:47,130 --> 00:02:49,620
Speaker 1:  site yesterday, I looked at the site and the entire top of it was the alumni

30
00:02:49,620 --> 00:02:53,550
Speaker 1:  stories. So if you will recall, I think I was not on the show the

31
00:02:53,550 --> 00:02:55,020
Speaker 1:  week this happened, but you

32
00:02:55,020 --> 00:02:55,620
Speaker 5:  Are not

33
00:02:55,620 --> 00:02:59,410
Speaker 1:  Many of you were here that week. I was trapped on a plane, I believe.

34
00:02:59,410 --> 00:03:03,250
Speaker 5:  And I were just dropping curse words the whole time you were gone.

35
00:03:03,250 --> 00:03:05,260
Speaker 8:  Yeah. Dad's out everybody party.

36
00:03:05,260 --> 00:03:09,160
Speaker 1:  Well, trust me, the curse words are coming free. Speech. You can't, you

37
00:03:09,160 --> 00:03:12,190
Speaker 1:  can't shut me down. What's the FCC going to do? Half of my personality is

38
00:03:12,190 --> 00:03:15,940
Speaker 1:  the movie pump up the volume scene where he drives around in the Jeep

39
00:03:15,940 --> 00:03:19,810
Speaker 1:  Wrangler. And the FCC tries to chase him down. That's Easily

40
00:03:19,810 --> 00:03:23,410
Speaker 1:  half of my personality. So Elan

41
00:03:23,410 --> 00:03:27,250
Speaker 1:  bought 9% of Twitter. He was offered a board seat.

42
00:03:27,250 --> 00:03:31,000
Speaker 1:  They thought he had were tricking him from what we gather into like having

43
00:03:31,000 --> 00:03:34,930
Speaker 1:  a fiduciary duty to the best interest of the shareholders and

44
00:03:34,930 --> 00:03:38,500
Speaker 1:  a lockup provision that said he couldn't buy more than like almost 15% of

45
00:03:38,500 --> 00:03:41,590
Speaker 1:  the company. He declined this. He gets away from it ever and wonders. What's

46
00:03:41,590 --> 00:03:45,280
Speaker 1:  he going to do? He filed, he sends it out another letter saying I

47
00:03:45,280 --> 00:03:49,010
Speaker 1:  offered to buy the whole thing for Liz's favorite number 54,

48
00:03:49,380 --> 00:03:53,230
Speaker 1:  20 a share is his favorite number all for favorite

49
00:03:53,230 --> 00:03:56,980
Speaker 1:  number, chaos ensues. He did an interview at

50
00:03:56,980 --> 00:03:59,650
Speaker 1:  Ted, which was horrible. We'll talk about that later with Addy. Cause it

51
00:03:59,650 --> 00:04:03,490
Speaker 1:  was all about content moderation. What? Just start at the

52
00:04:03,490 --> 00:04:07,150
Speaker 1:  start. Alex, walk me through like what this

53
00:04:07,150 --> 00:04:10,630
Speaker 1:  offer is you reported right out of the Twitter, all hands

54
00:04:10,630 --> 00:04:14,410
Speaker 1:  yesterday, you reported a little bit on what the board was thinking. What's

55
00:04:14,410 --> 00:04:15,070
Speaker 1:  going on here.

56
00:04:15,070 --> 00:04:18,880
Speaker 6:  Yeah. So Elan is offering about

57
00:04:18,880 --> 00:04:22,630
Speaker 6:  of 43 ish billion dollar and I guess

58
00:04:22,630 --> 00:04:26,080
Speaker 6:  cash. We don't really know. And I guess that's the Elan of all. This is

59
00:04:26,080 --> 00:04:29,470
Speaker 6:  that he says funding is secured

60
00:04:29,470 --> 00:04:33,400
Speaker 6:  TBD. We'll see what that is. He says he has the assets to

61
00:04:33,400 --> 00:04:37,330
Speaker 6:  buy Twitter and take it private at a nice takeout premium,

62
00:04:37,330 --> 00:04:40,930
Speaker 6:  which is higher than when he started buying shares towards the beginning

63
00:04:40,930 --> 00:04:44,860
Speaker 6:  of the year. And he notified Twitter of this

64
00:04:44,860 --> 00:04:48,700
Speaker 6:  one evening this week, the next morning made it public and an sec

65
00:04:48,700 --> 00:04:51,820
Speaker 6:  filing and a tweet just saying like I made an

66
00:04:51,820 --> 00:04:55,780
Speaker 6:  offer Twitter quickly responded and was like, we're going

67
00:04:55,780 --> 00:04:59,440
Speaker 6:  to review this. Obviously they have to, Twitter is not a founder

68
00:04:59,440 --> 00:05:02,380
Speaker 6:  controlled company like Facebook or snap or

69
00:05:02,380 --> 00:05:05,980
Speaker 6:  Google. And they can't just have one person be like,

70
00:05:05,980 --> 00:05:09,310
Speaker 6:  not when stuff like this happens. So they had to

71
00:05:09,310 --> 00:05:12,970
Speaker 6:  respond. And then the chaos that is

72
00:05:12,970 --> 00:05:16,690
Speaker 6:  Elan continued to ensue throughout the day. And I

73
00:05:16,690 --> 00:05:18,880
Speaker 6:  forgot to breathe at some point after that,

74
00:05:18,880 --> 00:05:21,520
Speaker 5:  You're too busy. You couldn't breathe. You had to blog.

75
00:05:21,520 --> 00:05:24,940
Speaker 1:  And what are you doing with breathing? You don't breathe on my dime

76
00:05:24,940 --> 00:05:28,810
Speaker 1:  Alex, on your own time, when you

77
00:05:28,810 --> 00:05:32,650
Speaker 1:  taking breaths, there was actually a moment yesterday when Keith were boy,

78
00:05:32,650 --> 00:05:36,130
Speaker 1:  who's like a high-end VC was

79
00:05:36,130 --> 00:05:39,850
Speaker 1:  like the woke Twitter employees are going to see what's coming to them

80
00:05:39,850 --> 00:05:43,600
Speaker 1:  one time, Elan threatened to fire all of the interns at Tesla because they

81
00:05:43,600 --> 00:05:47,470
Speaker 1:  were waiting in line for coffee. And it was like, what?

82
00:05:47,470 --> 00:05:51,340
Speaker 1:  Why are you turning heel so deeply, right? That just like buy

83
00:05:51,340 --> 00:05:54,220
Speaker 1:  another coffee machine.

84
00:05:54,220 --> 00:05:57,830
Speaker 6:  Well, it was really like, you know, the phrase like reality

85
00:05:57,830 --> 00:06:01,310
Speaker 6:  is stranger than fiction or something like that. Like it was, it was focus

86
00:06:01,310 --> 00:06:05,180
Speaker 6:  week for Twitter. So did employees had Monday off as a day of

87
00:06:05,180 --> 00:06:08,870
Speaker 6:  rest. And that was when a lot of this started. And

88
00:06:08,870 --> 00:06:12,170
Speaker 6:  then for the week they were supposed to take minimal meetings and be heads

89
00:06:12,170 --> 00:06:16,130
Speaker 6:  down on projects. And the CEO then scheduled

90
00:06:16,130 --> 00:06:16,880
Speaker 6:  an emergency all

91
00:06:16,880 --> 00:06:20,450
Speaker 6:  hands.

92
00:06:20,450 --> 00:06:24,440
Speaker 6:  Pretty interesting. And you know, had some Backstreet

93
00:06:24,440 --> 00:06:28,310
Speaker 6:  boys playing. So yeah, it was a, it was the CFO's birthday, I think

94
00:06:28,310 --> 00:06:32,210
Speaker 6:  when Elan made the bid as well. So you just,

95
00:06:32,210 --> 00:06:33,620
Speaker 6:  you can't write this stuff.

96
00:06:33,620 --> 00:06:36,980
Speaker 1:  The Backstreet boys line by the way is real. They open the meeting by playing

97
00:06:36,980 --> 00:06:40,970
Speaker 1:  Backstreet boys, which is perfect. Like someone had to choose

98
00:06:40,970 --> 00:06:44,420
Speaker 1:  like that's you have to make you don't just like hit the button in zoom.

99
00:06:44,420 --> 00:06:47,990
Speaker 1:  That's like play some music you like now let's go with Backstreet. Yeah.

100
00:06:47,990 --> 00:06:51,860
Speaker 6:  The first song was I say a little prayer and then they

101
00:06:51,860 --> 00:06:55,220
Speaker 6:  jumped into, I want it that way.

102
00:06:55,220 --> 00:06:56,060
Speaker 1:  Amazing.

103
00:06:56,060 --> 00:06:59,870
Speaker 7:  I think we've all been saying little prayers ever since this, this offer

104
00:06:59,870 --> 00:07:01,040
Speaker 7:  became public. Yeah.

105
00:07:01,040 --> 00:07:04,850
Speaker 1:  Yeah. So Liz, you are a noted Elan watcher. I believe you described

106
00:07:04,850 --> 00:07:08,600
Speaker 1:  yourself as a sicko yesterday. That might've been the exact phrase you

107
00:07:08,600 --> 00:07:11,720
Speaker 1:  used to walk us through the Ilan of this all.

108
00:07:11,720 --> 00:07:15,530
Speaker 7:  Okay. Well I just want to say like, although he's got Morgan Stanley

109
00:07:15,530 --> 00:07:19,010
Speaker 7:  advising him, he says that funding is contingent on

110
00:07:19,010 --> 00:07:22,970
Speaker 7:  winning. And then if this seems familiar, it's because he doesn't have

111
00:07:22,970 --> 00:07:26,900
Speaker 7:  funding lined up. And like during his Ted talk, he was like,

112
00:07:26,900 --> 00:07:30,500
Speaker 7:  well, I don't, I don't, I could buy Twitter, but I don't want to which who

113
00:07:30,500 --> 00:07:33,770
Speaker 7:  could blame him because he'd have to sell a bunch of Tesla shares and that

114
00:07:33,770 --> 00:07:37,550
Speaker 7:  would make the Tesla people very unhappy. So the funding is

115
00:07:37,550 --> 00:07:41,390
Speaker 7:  TBD. And if this were anybody else, I would say, this is not a

116
00:07:41,390 --> 00:07:45,110
Speaker 7:  serious offer because serious offers have funding lined

117
00:07:45,110 --> 00:07:49,010
Speaker 7:  up. But in Elan's case, it's impossible to tell what serious and

118
00:07:49,010 --> 00:07:52,970
Speaker 7:  what some kind of joke, which is like, you know, Matt Levine has suggested

119
00:07:52,970 --> 00:07:56,960
Speaker 7:  Twitter, come back to him with another price, which is the

120
00:07:56,960 --> 00:07:59,330
Speaker 7:  other fund, internet number $69 a

121
00:07:59,330 --> 00:08:03,140
Speaker 7:  share, Which, you know what I

122
00:08:03,140 --> 00:08:06,920
Speaker 7:  like, I like, I like how he was thinking here, but you know, if you, if you

123
00:08:06,920 --> 00:08:10,760
Speaker 7:  consider what he's doing, I got to say, like, I kind of feel

124
00:08:10,760 --> 00:08:14,120
Speaker 7:  like he's just fucking with Twitter because the share price

125
00:08:14,120 --> 00:08:17,510
Speaker 7:  is it, you know, it's, it's relatively low compared to where Twitter has

126
00:08:17,510 --> 00:08:21,380
Speaker 7:  been trading since like last year. Like I think it was in the

127
00:08:21,380 --> 00:08:24,800
Speaker 7:  sixties in October and then the seventies, like a year

128
00:08:24,800 --> 00:08:28,580
Speaker 7:  ago. So like relatively recently, the, the share price was

129
00:08:28,580 --> 00:08:32,570
Speaker 7:  trading at more than a lot more than what he's offering. Well, the

130
00:08:32,570 --> 00:08:35,330
Speaker 7:  other thing that I noticed about shares because the secret of shares is like,

131
00:08:35,330 --> 00:08:38,780
Speaker 7:  they're just like feelings. Like this is just people putting money on like

132
00:08:38,780 --> 00:08:42,770
Speaker 7:  their feelings. That's that's all the market is now. You're a trader.

133
00:08:42,770 --> 00:08:46,580
Speaker 7:  I noticed that the shares were trading well

134
00:08:46,580 --> 00:08:50,450
Speaker 7:  below where the offer was

135
00:08:50,450 --> 00:08:53,300
Speaker 7:  all of yesterday, which means that the market doesn't believe it's real.

136
00:08:53,300 --> 00:08:56,190
Speaker 7:  Now, if the market did believe it's real, it would be trading at the same

137
00:08:56,190 --> 00:09:00,060
Speaker 7:  price or around the price of the offer. And if they thought there was going

138
00:09:00,060 --> 00:09:03,780
Speaker 7:  to be somebody else jumping in, like for instance, Disney, they would be

139
00:09:03,780 --> 00:09:07,560
Speaker 7:  trading above the offer. So you can look at that and you can say,

140
00:09:07,560 --> 00:09:11,520
Speaker 7:  oh, these people are familiar with Elon Musk. They're aware that sometimes

141
00:09:11,520 --> 00:09:14,820
Speaker 7:  when he says he was taking a company private, he's just

142
00:09:14,820 --> 00:09:17,860
Speaker 7:  kidding.

143
00:09:17,860 --> 00:09:21,630
Speaker 1:  Let's we gotta talk about that specific line of Elon in a minute.

144
00:09:21,630 --> 00:09:25,440
Speaker 1:  But real quickly is I think most people are vaguely

145
00:09:25,440 --> 00:09:29,130
Speaker 1:  aware that you can offer to buy a company, but there's like a lot of steps

146
00:09:29,130 --> 00:09:32,760
Speaker 1:  before the company is actually yours. So what's the process here that Elon

147
00:09:32,760 --> 00:09:33,600
Speaker 1:  would have to go through.

148
00:09:33,600 --> 00:09:37,560
Speaker 7:  So I, the first, this is a hostile bid. So that's that in and of itself

149
00:09:37,560 --> 00:09:41,550
Speaker 7:  is unusual. Ordinarily one works with companies management to

150
00:09:41,550 --> 00:09:45,510
Speaker 7:  like come together with a thing that will be acceptable to

151
00:09:45,510 --> 00:09:48,480
Speaker 7:  major shareholders. And this is not what's happening here. This is Ilan is

152
00:09:48,480 --> 00:09:52,080
Speaker 7:  like, I got a bid. So Twitter has to think about it. They have like a whole

153
00:09:52,080 --> 00:09:56,040
Speaker 7:  fiduciary duty. The board is going to be thinking about it, but I, if

154
00:09:56,040 --> 00:09:59,370
Speaker 7:  I'm Ilan right now, I'm out here pressuring shareholders to pressure the

155
00:09:59,370 --> 00:10:03,330
Speaker 7:  board. So I'm calling them. I'm saying things like in that script that we

156
00:10:03,330 --> 00:10:06,450
Speaker 7:  all saw that was filed with the sec where it's like, this is my best and

157
00:10:06,450 --> 00:10:09,900
Speaker 7:  final offer. And I, you know what, I'm going to sell all my

158
00:10:09,900 --> 00:10:13,740
Speaker 7:  shares if it doesn't work, which in Elan's case is like a little bit like,

159
00:10:13,740 --> 00:10:17,550
Speaker 7:  Hmm, okay, this is, this is hard ball. This is like some Carl Icahn shit

160
00:10:17,550 --> 00:10:20,940
Speaker 7:  we got going on here. And he loved because he created a premium by getting

161
00:10:20,940 --> 00:10:23,970
Speaker 7:  into the stock, by having his ownership announced, which is the sort of thing

162
00:10:23,970 --> 00:10:26,940
Speaker 7:  that you associate actually with corporate Raiders. Like that's also what

163
00:10:26,940 --> 00:10:30,840
Speaker 7:  happens with Carl Icahn gets into a stock. So Twitter's board has to think

164
00:10:30,840 --> 00:10:34,620
Speaker 7:  this through, like, just because I'm a, I'm a lady who is like, maybe

165
00:10:34,620 --> 00:10:38,310
Speaker 7:  not particularly privy to this stuff. I think it's going to be

166
00:10:38,310 --> 00:10:42,240
Speaker 7:  like, I don't think that the board is going to say yes to

167
00:10:42,240 --> 00:10:45,930
Speaker 7:  this. Just speaking as somebody who doesn't know anybody on the board or

168
00:10:45,930 --> 00:10:48,630
Speaker 7:  doesn't have any connections to the board. I feel like if I were on the board,

169
00:10:48,630 --> 00:10:52,530
Speaker 7:  I'd be like, no, but what's interesting. Here is now the company is

170
00:10:52,530 --> 00:10:55,230
Speaker 7:  in place. So there could potentially be another bidder. The market doesn't

171
00:10:55,230 --> 00:10:59,220
Speaker 7:  totally believe it. But if I'm a tech company and I ever wanted to acquire

172
00:10:59,220 --> 00:11:02,220
Speaker 7:  a Twitter, like now's the moment. Because one of the things that the board

173
00:11:02,220 --> 00:11:05,970
Speaker 7:  can do is come back with somebody else's better offer and say, sorry, Elan.

174
00:11:05,970 --> 00:11:09,840
Speaker 1:  Yeah. So Alex, you were tracking a little bit of this yesterday.

175
00:11:09,840 --> 00:11:13,230
Speaker 1:  Twitter's management and board is kind of an interesting spot, right? Jack

176
00:11:13,230 --> 00:11:16,500
Speaker 1:  Dorsey left. He basically said my hand selected

177
00:11:16,500 --> 00:11:20,190
Speaker 1:  successors progra wall as CEO and my hand selected

178
00:11:20,190 --> 00:11:22,530
Speaker 1:  chairman of the board is this guy, Brett Taylor. And he's like, these are

179
00:11:22,530 --> 00:11:26,490
Speaker 1:  my guys. I picked him. I love him. They're Jack skies. They were, I,

180
00:11:26,490 --> 00:11:30,390
Speaker 1:  you can't like if someone came to your job and was like,

181
00:11:30,390 --> 00:11:33,780
Speaker 1:  we think you've been doing a really bad job, so we're just gonna take it

182
00:11:33,780 --> 00:11:37,440
Speaker 1:  from here. Like it's, I don't think it's instinctual for you to be like,

183
00:11:37,440 --> 00:11:41,310
Speaker 1:  I agree with you. And I have trapping accepted this job. I

184
00:11:41,310 --> 00:11:44,400
Speaker 1:  will in fact recede into the distance and let you destroy whatever it is.

185
00:11:44,400 --> 00:11:47,940
Speaker 1:  I was supposed to be in charge of like, especially for a young

186
00:11:47,940 --> 00:11:51,780
Speaker 1:  CEO who has no name, otherwise, like, that's just

187
00:11:51,780 --> 00:11:55,000
Speaker 1:  like, you're done, right? Like you're, you might as well just ride off into

188
00:11:55,000 --> 00:11:58,870
Speaker 1:  the sunset, like your career kind of stops there. So

189
00:11:58,870 --> 00:12:00,580
Speaker 1:  what were you hearing yesterday, Alex?

190
00:12:00,580 --> 00:12:04,480
Speaker 6:  Well, I think every, I mean, obviously I think Liz is right. The

191
00:12:04,480 --> 00:12:08,170
Speaker 6:  board doesn't want to do this. I think that was the tone of the all hands

192
00:12:08,170 --> 00:12:12,130
Speaker 6:  with employees was Prague was, you know, I think employees

193
00:12:12,130 --> 00:12:14,740
Speaker 6:  were because Prague and the leadership had not

194
00:12:14,740 --> 00:12:18,670
Speaker 6:  directly addressed Ilan besides

195
00:12:18,670 --> 00:12:22,570
Speaker 6:  Pirog tweeting when he decided not to join the board and

196
00:12:22,570 --> 00:12:26,530
Speaker 6:  saying distractions are ahead. Foreboding kind of what was to come employees

197
00:12:26,530 --> 00:12:30,340
Speaker 6:  were expecting. I think at this emergency all hands to kind of get some more

198
00:12:30,340 --> 00:12:34,270
Speaker 6:  clarity about where the company was leaning, what the timeline was going

199
00:12:34,270 --> 00:12:37,570
Speaker 6:  to look like here, because this could be a protracted messy thing, especially

200
00:12:37,570 --> 00:12:41,440
Speaker 6:  if they have to run an official bidding process. And these large

201
00:12:41,440 --> 00:12:44,860
Speaker 6:  companies take time to reviews the review of these deals.

202
00:12:44,860 --> 00:12:48,070
Speaker 6:  So, you know, that's not really what happened. And Pirog was like, look for

203
00:12:48,070 --> 00:12:51,880
Speaker 6:  legal reasons. I can't really say, but I can just say that we're

204
00:12:51,880 --> 00:12:55,810
Speaker 6:  following a rigorous process. Yada, yada, everything was obviously very

205
00:12:55,810 --> 00:12:58,870
Speaker 6:  lawyered. They knew it was going to leak like almost in real

206
00:12:58,870 --> 00:13:02,830
Speaker 6:  time. It wasn't quite in real time, but we got ours up like

207
00:13:02,830 --> 00:13:06,760
Speaker 6:  right when it ended. So our, our story on it. So yeah,

208
00:13:06,760 --> 00:13:10,390
Speaker 6:  it was, I would say there was a sense of resistance,

209
00:13:10,390 --> 00:13:13,990
Speaker 6:  but from Pirog he said something about how, you know, we're not going to

210
00:13:13,990 --> 00:13:17,890
Speaker 6:  be held hostage. And, you know, basically saying that

211
00:13:17,890 --> 00:13:21,130
Speaker 6:  like it's too early to speculate about what it would look like if we were

212
00:13:21,130 --> 00:13:24,790
Speaker 6:  taken private, because there's big implications for employees in that, you

213
00:13:24,790 --> 00:13:27,880
Speaker 6:  know, if that happens, because what happens to their stock

214
00:13:27,880 --> 00:13:31,510
Speaker 6:  options, there were questions about layoffs, which is something I was actually

215
00:13:31,510 --> 00:13:35,170
Speaker 6:  hearing before this meeting was the moment Elon announced his interest. I

216
00:13:35,170 --> 00:13:38,650
Speaker 6:  had like three Twitter employees to be like, there's going to be layoffs.

217
00:13:38,650 --> 00:13:39,790
Speaker 1:  Like, absolutely.

218
00:13:39,790 --> 00:13:43,690
Speaker 6:  Like Twitter is fairly bloated relative to

219
00:13:43,690 --> 00:13:47,440
Speaker 6:  the money it makes and it's tech peers. So those are the

220
00:13:47,440 --> 00:13:50,920
Speaker 6:  questions on people's minds, which to me says, people think that this is,

221
00:13:50,920 --> 00:13:54,640
Speaker 6:  you know, the rank and file thinks this is probably happening no matter what,

222
00:13:54,640 --> 00:13:57,940
Speaker 6:  at least like there's going to be a messy, maybe bad outcome for

223
00:13:57,940 --> 00:14:01,930
Speaker 6:  them from this. And so in that way, you know, there's a,

224
00:14:01,930 --> 00:14:05,890
Speaker 6:  maybe a fear that management at Twitter has kind of lost faith. You

225
00:14:05,890 --> 00:14:09,340
Speaker 6:  know, that the employees have lost faith in them to a degree, a little bit.

226
00:14:09,340 --> 00:14:12,130
Speaker 6:  One was like, are we just going to be letting any billionaire who wants a

227
00:14:12,130 --> 00:14:15,280
Speaker 6:  board seat? Give them a board seat. Are we going to just like, do this for

228
00:14:15,280 --> 00:14:17,650
Speaker 6:  everyone now who asks

229
00:14:17,650 --> 00:14:19,210
Speaker 5:  They got enough money, right.

230
00:14:19,210 --> 00:14:23,050
Speaker 6:  Yeah. And it's an interesting time for Elan to be doing this because Jack

231
00:14:23,050 --> 00:14:26,950
Speaker 6:  is on the board for like another month or so before the next board votes

232
00:14:26,950 --> 00:14:30,880
Speaker 6:  on the new board seats. And so, I don't know, you know, my favorite conspiracy

233
00:14:30,880 --> 00:14:34,640
Speaker 6:  theory here is that Jack and Ilan are aligned on this. And this is Jack

234
00:14:34,640 --> 00:14:38,590
Speaker 6:  Sweet revenge on Elliot management, which was the last active

235
00:14:38,590 --> 00:14:42,550
Speaker 6:  investor that came into Twitter and really kind

236
00:14:42,550 --> 00:14:46,480
Speaker 6:  of led to Jack stepping aside and naming Pirog last November.

237
00:14:46,480 --> 00:14:49,330
Speaker 1:  Walk us through that. There's a lot of detail to explore there

238
00:14:49,330 --> 00:14:52,760
Speaker 7:  Before you walk us through it. We need to have the succession theme play

239
00:14:52,760 --> 00:14:56,150
Speaker 7:  because this is like very like, this is a very

240
00:14:56,150 --> 00:14:59,090
Speaker 7:  succession theme moment here.

241
00:14:59,090 --> 00:15:03,020
Speaker 6:  Oh my God. I had a very senior former Twitter person be like text me right

242
00:15:03,020 --> 00:15:06,890
Speaker 6:  before Elan walked out on stage at Ted and be like, I have money on Jack

243
00:15:06,890 --> 00:15:10,430
Speaker 6:  walking out with him on stage attack, which is like,

244
00:15:10,430 --> 00:15:13,310
Speaker 6:  yeah, that's, that's better than whatever the next

245
00:15:13,310 --> 00:15:16,520
Speaker 5:  Jack wanted to take the company private awhile ago. Didn't he?

246
00:15:16,520 --> 00:15:20,120
Speaker 6:  Yeah. So I had done some reporting a couple of years ago that, you know,

247
00:15:20,120 --> 00:15:24,050
Speaker 6:  the last time Twitter ran a formal sales process was

248
00:15:24,050 --> 00:15:27,620
Speaker 6:  around 2016. And that was when Salesforce and Disney were the two kind of

249
00:15:27,620 --> 00:15:31,610
Speaker 6:  lead suitors and Bob Iger and mark Benioff have talked publicly about

250
00:15:31,610 --> 00:15:34,880
Speaker 6:  that. There were two other players who didn't get as far apple and

251
00:15:34,880 --> 00:15:38,690
Speaker 6:  Google that were looking at IQ at apple and Sundar at Google.

252
00:15:38,690 --> 00:15:42,440
Speaker 6:  And there were also some offers informally to take the company

253
00:15:42,440 --> 00:15:45,560
Speaker 6:  private at the time. And this was when Twitter's valuation was like 10 billion.

254
00:15:45,710 --> 00:15:49,640
Speaker 6:  And it was just really, almost struggling more than it has been

255
00:15:49,640 --> 00:15:53,270
Speaker 6:  since, which is like hard to think about. So I had heard from

256
00:15:53,270 --> 00:15:56,870
Speaker 6:  someone kind of directly involved in those talks that Jack was in favor of

257
00:15:56,870 --> 00:15:59,960
Speaker 6:  taking the company private at that time. And it makes sense because Twitter

258
00:15:59,960 --> 00:16:03,740
Speaker 6:  has just, when you have these quarterly expectations in the

259
00:16:03,740 --> 00:16:07,700
Speaker 6:  market and your comp to Facebook, which has just been this unstoppable

260
00:16:07,700 --> 00:16:11,450
Speaker 6:  growth juggernaut for over a decade and even like

261
00:16:11,450 --> 00:16:15,410
Speaker 6:  snap snap is added like a hundred more, a hundred million more users than

262
00:16:15,410 --> 00:16:19,310
Speaker 6:  Twitter. And it's like four years younger as a company and been public

263
00:16:19,310 --> 00:16:23,120
Speaker 6:  way less. So Twitter is just really struggled relative to its

264
00:16:23,120 --> 00:16:27,020
Speaker 6:  peer set. And it could probably use the insulation that going private

265
00:16:27,020 --> 00:16:30,890
Speaker 6:  would provide it right where like they can actually go heads down and focus

266
00:16:30,890 --> 00:16:34,850
Speaker 6:  on just not these near term quarterly benchmarks they need to meet,

267
00:16:34,850 --> 00:16:38,810
Speaker 6:  but like really, really, you know, reinvent the product and Jack and

268
00:16:38,810 --> 00:16:42,560
Speaker 6:  Elan saw eye to eye on that. You know, they both talk about open

269
00:16:42,560 --> 00:16:45,560
Speaker 6:  sourcing the algorithm, which God we got to get into that because that makes

270
00:16:45,560 --> 00:16:49,400
Speaker 6:  absolutely no sense. None, you know, opening up the algorithms cause that'll

271
00:16:49,400 --> 00:16:51,950
Speaker 6:  solve everything and, you know, crypto

272
00:16:51,950 --> 00:16:55,730
Speaker 6:  decentralization putting, making it a protocol again, going back to the

273
00:16:55,730 --> 00:16:59,450
Speaker 6:  Tweety days when like all these clients were sat on top of

274
00:16:59,450 --> 00:17:03,440
Speaker 6:  Twitter and invented, you know, the best parts of Twitter. And I

275
00:17:03,440 --> 00:17:07,010
Speaker 6:  could see there's an argument to be made that Jack maybe wants this in the

276
00:17:07,010 --> 00:17:10,340
Speaker 6:  background. And the fact that Elon is doing this while Jack is still on the

277
00:17:10,340 --> 00:17:14,180
Speaker 6:  board and still has about 2%. It's like the timing of that. Very interesting.

278
00:17:14,180 --> 00:17:17,750
Speaker 1:  So Jack had a tweet a couple of weeks ago now where he was

279
00:17:17,750 --> 00:17:21,140
Speaker 1:  like, I now realize that centralizing all internet

280
00:17:21,140 --> 00:17:24,560
Speaker 1:  services was a mistake and I am to blame. And then someone was like, well

281
00:17:24,560 --> 00:17:27,110
Speaker 1:  fix it. He was like, I'm working on it. And I've just been thinking about

282
00:17:27,110 --> 00:17:30,680
Speaker 1:  that, that little interaction in this context a lot, because

283
00:17:30,680 --> 00:17:34,670
Speaker 1:  he, Twitter has this program called blue sky, which

284
00:17:34,670 --> 00:17:38,090
Speaker 1:  has been moving in an absolutely glacial pace, which is

285
00:17:38,090 --> 00:17:42,020
Speaker 1:  designed to decentralize Twitter and turn it into a protocol. When I

286
00:17:42,020 --> 00:17:45,860
Speaker 1:  say a glacial pace, I mean every six months, they're like, we've

287
00:17:45,860 --> 00:17:49,800
Speaker 1:  hired one more person. We're up to four.

288
00:17:49,800 --> 00:17:52,950
Speaker 6:  They recently announced. So it's actually a separate legal entity. It's like

289
00:17:52,950 --> 00:17:56,700
Speaker 6:  a nonprofit that Twitter funds, they're not the only funder and Jack is on

290
00:17:56,700 --> 00:17:59,610
Speaker 6:  the board of blue sky. And he's about to leave the board of Twitter and he

291
00:17:59,610 --> 00:18:02,280
Speaker 6:  just joined blue sky interesting note, but continuing the line.

292
00:18:02,280 --> 00:18:05,970
Speaker 1:  Yeah. It just put the it's the heaven, you know, I feel about vaporware.

293
00:18:05,970 --> 00:18:09,120
Speaker 1:  You can tell me you're going to do shit all day long. Right? You have to

294
00:18:09,120 --> 00:18:12,930
Speaker 1:  do it. And they've, they've not shipped a single line of code. They've

295
00:18:12,930 --> 00:18:15,750
Speaker 1:  shipped a white paper and a bunch of press releases about people they've

296
00:18:15,750 --> 00:18:16,410
Speaker 1:  hired.

297
00:18:16,410 --> 00:18:19,830
Speaker 7:  Well, you know, Jack is very into crypto, so that's like kind of standard

298
00:18:19,830 --> 00:18:23,700
Speaker 7:  in that world. Although in fairness, I should say he's a

299
00:18:23,700 --> 00:18:27,600
Speaker 7:  Bitcoin maxi and not into crypto per se so much as he's just

300
00:18:27,600 --> 00:18:28,620
Speaker 7:  into Bitcoin.

301
00:18:28,620 --> 00:18:32,610
Speaker 1:  Great. He's the one who's beefing with Chris Dixon about Andreessen Horowitz

302
00:18:32,610 --> 00:18:36,450
Speaker 1:  being just like another, like a VC front for web three. And

303
00:18:36,450 --> 00:18:40,050
Speaker 1:  he's like, Bitcoin is the alpha and the omega at web three is just another

304
00:18:40,050 --> 00:18:42,750
Speaker 1:  way for VCs Tony internet. Like that's Jack very

305
00:18:42,750 --> 00:18:46,740
Speaker 1:  publicly beefing with the people who are funding web three.

306
00:18:46,740 --> 00:18:50,730
Speaker 1:  However you wanted to find next to that is Ilan who, who just

307
00:18:50,730 --> 00:18:54,150
Speaker 1:  loves Twitter. Like the man just loves Twitter. Lizzie mentioned Matt Levine

308
00:18:54,150 --> 00:18:57,600
Speaker 1:  earlier in it. He's a columnist for Bloomberg

309
00:18:57,600 --> 00:19:01,200
Speaker 1:  in one of his columns, I think earlier last week, maybe he was

310
00:19:01,200 --> 00:19:03,660
Speaker 1:  like, if you're really rich and you play a video game all the time, you're

311
00:19:03,660 --> 00:19:07,650
Speaker 1:  going to have ideas about the video game and that is Ilan and Twitter,

312
00:19:07,650 --> 00:19:11,460
Speaker 1:  right? Like it's a video game that he constantly plays and stuff like

313
00:19:11,460 --> 00:19:14,700
Speaker 1:  open up the algorithm. Like that's the sort of idea you might have about

314
00:19:14,700 --> 00:19:18,630
Speaker 1:  it. Like I have a million ideas on how to fix Madden NFL. Like I, every

315
00:19:18,630 --> 00:19:22,530
Speaker 1:  day I can just generate a list and there's like, what I think should happen

316
00:19:22,530 --> 00:19:26,430
Speaker 1:  and reality of that game and how it is architected. And

317
00:19:26,430 --> 00:19:30,240
Speaker 1:  so if you think, you know, I can just buy it and open source Twitter or buy

318
00:19:30,240 --> 00:19:33,300
Speaker 1:  it and turn it into a protocol. The technical effort to do

319
00:19:33,300 --> 00:19:36,870
Speaker 1:  that is extraordinarily high. The company is

320
00:19:36,870 --> 00:19:40,770
Speaker 1:  already trying to do some of that stuff and hasn't done any of it. And all

321
00:19:40,770 --> 00:19:43,350
Speaker 1:  the employees are pissed off and thinking about leaving or we're worried

322
00:19:43,350 --> 00:19:47,220
Speaker 1:  about being laid off. Like, I don't know how you go from here to there. And

323
00:19:47,220 --> 00:19:48,930
Speaker 1:  Ilan hasn't really laid it out.

324
00:19:48,930 --> 00:19:52,110
Speaker 6:  Yeah. I mean, there was a pretty senior Twitter person who I saw a tweet

325
00:19:52,110 --> 00:19:56,100
Speaker 6:  in response to someone saying like, what will you do if Elan takes over?

326
00:19:56,100 --> 00:19:58,440
Speaker 6:  And is I, and she was like, well, you know, I hope he likes working at a

327
00:19:58,440 --> 00:20:02,250
Speaker 6:  company with no employees. Like she was like the head of like machine

328
00:20:02,250 --> 00:20:05,520
Speaker 6:  learning or something. So yeah, open sourcing the algorithm. Can we talk

329
00:20:05,520 --> 00:20:08,670
Speaker 6:  about this? Because you know, I actually, I just did a story a couple of

330
00:20:08,670 --> 00:20:12,570
Speaker 6:  weeks ago about Facebook. It took them six months to try to figure

331
00:20:12,570 --> 00:20:16,350
Speaker 6:  out what went wrong with the newsfeed algorithm. This is the people

332
00:20:16,350 --> 00:20:20,340
Speaker 6:  who directly work on the newsfeed literally did not

333
00:20:20,340 --> 00:20:24,150
Speaker 6:  know what broke for months. And they are still dissecting

334
00:20:24,150 --> 00:20:27,750
Speaker 6:  it. Like I've heard from people who work on these kinds of algorithms that

335
00:20:27,750 --> 00:20:31,200
Speaker 6:  these social media companies, they don't really know how they

336
00:20:31,200 --> 00:20:34,860
Speaker 6:  work fully. It's all, it's like a Pandora's box that once you

337
00:20:34,860 --> 00:20:38,790
Speaker 6:  open these things have millions of signals. And the idea that

338
00:20:38,790 --> 00:20:42,300
Speaker 6:  like, it's this one algorithm that everyone can just like look at and

339
00:20:42,300 --> 00:20:45,750
Speaker 6:  dissect and know all of its inputs and

340
00:20:45,750 --> 00:20:49,300
Speaker 6:  outputs. The whole point of this is that it's different for every single

341
00:20:49,300 --> 00:20:53,170
Speaker 6:  person. These algorithms are really just like a reflection of like

342
00:20:53,170 --> 00:20:57,160
Speaker 6:  your most carnal desires. Like they

343
00:20:57,160 --> 00:21:00,700
Speaker 6:  are a distorted mirror of you and your worst impulses.

344
00:21:00,700 --> 00:21:04,540
Speaker 5:  Well, a lot of them, but I mean, it's, Twitter's though like Twitter. I always

345
00:21:04,540 --> 00:21:06,160
Speaker 5:  envisioned as having the most

346
00:21:06,160 --> 00:21:08,930
Speaker 5:  simplistic

347
00:21:08,930 --> 00:21:11,050
Speaker 9:  Envision about Twitter

348
00:21:11,050 --> 00:21:11,620
Speaker 6:  Because

349
00:21:11,620 --> 00:21:15,400
Speaker 5:  Like, this is not Tik talk that like

350
00:21:15,400 --> 00:21:19,120
Speaker 5:  knows I like skiing before I did. This is like pretty

351
00:21:19,120 --> 00:21:19,930
Speaker 5:  simple

352
00:21:19,930 --> 00:21:23,710
Speaker 6:  As someone who woke up this morning and was seeing tweets from 17 hours ago

353
00:21:23,710 --> 00:21:26,740
Speaker 6:  at the top of my feet and had to switch to chronological. Cause I was like,

354
00:21:26,740 --> 00:21:30,700
Speaker 6:  I saw this yesterday and have that experience on a daily basis. You're

355
00:21:30,700 --> 00:21:34,150
Speaker 6:  right. That's what our algorithm is. Not really that good at mind reading,

356
00:21:34,150 --> 00:21:37,390
Speaker 6:  but my point is just that the algorithm is different for every single

357
00:21:37,390 --> 00:21:41,380
Speaker 6:  person. And I think we're fooling ourselves to say that like millions

358
00:21:41,380 --> 00:21:45,310
Speaker 6:  of people care or want to know the innards of like the

359
00:21:45,310 --> 00:21:48,190
Speaker 6:  feed that is giving them information.

360
00:21:48,190 --> 00:21:52,120
Speaker 5:  I don't think millions, but one of these algorithms being open source

361
00:21:52,120 --> 00:21:56,050
Speaker 5:  is a fascinating thing because we don't really have one now to my

362
00:21:56,050 --> 00:22:00,040
Speaker 5:  knowledge and, and, and having those researchers and those people

363
00:22:00,040 --> 00:22:03,310
Speaker 5:  in the open source community, having access and starting to tweak with these

364
00:22:03,310 --> 00:22:07,090
Speaker 5:  things and deal with them is really interesting. I

365
00:22:07,090 --> 00:22:10,900
Speaker 5:  think for Twitter itself, it makes zero sense and as

366
00:22:10,900 --> 00:22:14,680
Speaker 5:  super stupid, but like, I think 90% of Twitter's business

367
00:22:14,680 --> 00:22:18,400
Speaker 5:  is kind of stupid. Like it's not a company that's making a lot of

368
00:22:18,400 --> 00:22:22,090
Speaker 5:  money. It's not a company that's building subscribers, everything about this.

369
00:22:22,090 --> 00:22:25,690
Speaker 5:  It's just like a billionaire kind of shit posting his

370
00:22:25,690 --> 00:22:29,530
Speaker 5:  way into notoriety with a company that he can afford to

371
00:22:29,530 --> 00:22:30,400
Speaker 5:  do it with.

372
00:22:30,400 --> 00:22:33,970
Speaker 7:  I also don't know that like the algorithm is the problem with

373
00:22:33,970 --> 00:22:37,750
Speaker 7:  Twitter. Like I just, I just want to be super real

374
00:22:37,750 --> 00:22:41,710
Speaker 7:  about like what Twitter is. It's like an all

375
00:22:41,710 --> 00:22:45,520
Speaker 7:  versus all hostiles zone where

376
00:22:45,520 --> 00:22:48,940
Speaker 7:  like, you know, roving gangs of people

377
00:22:48,940 --> 00:22:52,810
Speaker 7:  form and like it.

378
00:22:52,810 --> 00:22:56,710
Speaker 7:  So I'm going to compare it to four Chan in a very limited way, which

379
00:22:56,710 --> 00:22:57,880
Speaker 7:  is that it is,

380
00:22:57,880 --> 00:22:58,250
Speaker 5:  This is

381
00:22:58,250 --> 00:23:01,930
Speaker 7:  Getting good. It is a nexus of internet culture from which a lot of internet

382
00:23:01,930 --> 00:23:05,650
Speaker 7:  culture comes, but it is also a very hostile

383
00:23:05,650 --> 00:23:08,830
Speaker 7:  environment. And now the mechanics of it work differently from four Chan,

384
00:23:08,830 --> 00:23:11,710
Speaker 7:  like in ways that are like maybe not worth discussing right at this very

385
00:23:11,710 --> 00:23:15,640
Speaker 7:  moment. But in terms of like users, it's a relatively

386
00:23:15,640 --> 00:23:19,360
Speaker 7:  small base compared to like most of the other social

387
00:23:19,360 --> 00:23:22,870
Speaker 7:  media platforms, but it has this like outsized

388
00:23:22,870 --> 00:23:26,740
Speaker 7:  hold on the public's imagination, a of all, because every

389
00:23:26,740 --> 00:23:30,610
Speaker 7:  journalist is like addled by Twitter. Cause we're all there. Cause

390
00:23:30,610 --> 00:23:34,210
Speaker 7:  that's like what we do instead of like RSS feeds

391
00:23:34,210 --> 00:23:38,020
Speaker 7:  because Google reader doesn't exist anymore. So Google, thank

392
00:23:38,020 --> 00:23:39,400
Speaker 7:  you. This is your fault.

393
00:23:39,400 --> 00:23:43,070
Speaker 1:  RSS feeds. I, it would not be the virtuous. If I didn't remind that RSS feed

394
00:23:43,070 --> 00:23:46,580
Speaker 1:  still exist, there are many independent RSS readers over there and I'm counting

395
00:23:46,580 --> 00:23:50,240
Speaker 1:  on Google to be the sole shepherd or the open internet was always a mistake.

396
00:23:50,240 --> 00:23:52,580
Speaker 1:  That's it? Just my little digression, the act.

397
00:23:52,580 --> 00:23:56,360
Speaker 7:  So there's that. But also like we have these 24 hour cable

398
00:23:56,360 --> 00:24:00,200
Speaker 7:  news networks that like need to like put stuff on

399
00:24:00,200 --> 00:24:03,470
Speaker 7:  television. And so what do they do? They open up Twitter, they read tweets

400
00:24:03,470 --> 00:24:05,120
Speaker 7:  out loud and like, that's like a segment.

401
00:24:05,120 --> 00:24:08,570
Speaker 1:  And as somebody who has a cable news contract, I feel comfortable

402
00:24:08,570 --> 00:24:12,470
Speaker 1:  saying the majority of mainstream cable news is a podcast to people

403
00:24:12,470 --> 00:24:16,250
Speaker 1:  reading tweets. And then like, I'm not saying that's not

404
00:24:16,250 --> 00:24:20,120
Speaker 1:  our podcast. Like it's not like the harshest criticism in the world.

405
00:24:20,120 --> 00:24:22,760
Speaker 1:  Like we're talking about one guy who tweets a lot. We've been doing it for

406
00:24:22,760 --> 00:24:26,390
Speaker 1:  26 minutes. So like maybe that's all news now.

407
00:24:26,390 --> 00:24:30,290
Speaker 1:  But the point about like what Twitter is, whether it has a

408
00:24:30,290 --> 00:24:33,830
Speaker 1:  ranking algorithm or not, the problem is actually expressed

409
00:24:33,830 --> 00:24:37,700
Speaker 1:  in whether the ranking algorithm for Twitter is it's product or

410
00:24:37,700 --> 00:24:41,630
Speaker 1:  not. Facebook's product is a ranking algorithm. Take talks.

411
00:24:41,630 --> 00:24:45,230
Speaker 1:  Product is a ranking algorithm, right? Google's product

412
00:24:45,230 --> 00:24:48,620
Speaker 1:  is the world's most successful ranking algorithm. You

413
00:24:48,620 --> 00:24:52,460
Speaker 1:  express some interest to one of these platforms and they

414
00:24:52,460 --> 00:24:55,910
Speaker 1:  have a huge data set of all the other people. YouTube has a huge data set

415
00:24:55,910 --> 00:24:58,760
Speaker 1:  of all the other people have contributed content and then they show it to

416
00:24:58,760 --> 00:25:02,540
Speaker 1:  you, Twitter, you log in as a new user and it's like, the

417
00:25:02,540 --> 00:25:05,810
Speaker 1:  worst shit in the world is going to immediately start happening to you. And

418
00:25:05,810 --> 00:25:09,740
Speaker 1:  maybe you will find NBA Twitter, or maybe you will distract

419
00:25:09,740 --> 00:25:13,700
Speaker 1:  yourself with media, Twitter. Like maybe you will find some pocket of

420
00:25:13,700 --> 00:25:17,330
Speaker 1:  film Twitter. That's great. But at any moment, as Liz

421
00:25:17,330 --> 00:25:21,110
Speaker 1:  said, that angriest hoards of people might find you or the

422
00:25:21,110 --> 00:25:25,070
Speaker 1:  worst possible thing that could happen to you, your tweet will go viral and

423
00:25:25,070 --> 00:25:29,060
Speaker 1:  like Fox news will write about you. Like the worst possible

424
00:25:29,060 --> 00:25:32,000
Speaker 1:  things could happen to you at any moment. On the Twitter platform,

425
00:25:32,000 --> 00:25:35,990
Speaker 6:  I had a tweet go viral about Ilan and the day of

426
00:25:35,990 --> 00:25:39,380
Speaker 6:  the week of focus and the day of arrest, then the timing of it all. And it

427
00:25:39,380 --> 00:25:43,340
Speaker 6:  was just, it was that persona. It was just like his bot

428
00:25:43,340 --> 00:25:47,330
Speaker 6:  farms and his like, you know, people calling me an idiot and all

429
00:25:47,330 --> 00:25:50,870
Speaker 6:  these things saying, I don't understand things. Like I got like

430
00:25:50,870 --> 00:25:54,530
Speaker 6:  unsolicited, like hate mail. So it's

431
00:25:54,530 --> 00:25:58,220
Speaker 6:  like just for a tweet. And that was because it was about Elan, the guy trying

432
00:25:58,220 --> 00:26:02,000
Speaker 6:  to take Twitter, we should know like Elon has a like massive

433
00:26:02,000 --> 00:26:05,690
Speaker 6:  bot army that follows him on Twitter and amplifies and it

434
00:26:05,690 --> 00:26:09,170
Speaker 6:  attacks his critics. And he said that he wants to get rid of bots. But

435
00:26:09,170 --> 00:26:13,130
Speaker 6:  like, it's just very ironic to me that that's the

436
00:26:13,130 --> 00:26:16,460
Speaker 6:  case that like this, this one individual has that

437
00:26:16,460 --> 00:26:19,970
Speaker 5:  Well, like I think a lot of people fail to understand that

438
00:26:19,970 --> 00:26:23,840
Speaker 5:  Ilan success at this point that his current success is

439
00:26:23,840 --> 00:26:27,770
Speaker 5:  kind of based on Twitter and this like this fandom he's

440
00:26:27,770 --> 00:26:31,760
Speaker 5:  created around himself that centralized on Twitter. Right.

441
00:26:31,760 --> 00:26:35,720
Speaker 1:  I think Elan might feel that way. And I think we might feel that way. I'm

442
00:26:35,720 --> 00:26:39,470
Speaker 1:  not entirely like, I don't know the conversion rate between

443
00:26:39,470 --> 00:26:43,230
Speaker 1:  having a bot army and lots of followers on Twitter to Tesla sales.

444
00:26:43,230 --> 00:26:47,160
Speaker 1:  Right. And Tessa has apparently like inelastic demand. Like you can price

445
00:26:47,160 --> 00:26:50,400
Speaker 1:  a Tesla at any number or produce any number of Teslas and all of them will

446
00:26:50,400 --> 00:26:54,090
Speaker 1:  sell. Like they can't, every Tesla they make at any price will sell out.

447
00:26:54,090 --> 00:26:56,520
Speaker 1:  Right. What does that have to do with the Twitter army?

448
00:26:56,520 --> 00:27:00,360
Speaker 7:  I'm like dying here because like I have been covering Elon

449
00:27:00,360 --> 00:27:04,080
Speaker 7:  Musk for a very long time and like, yeah, like

450
00:27:04,080 --> 00:27:07,680
Speaker 7:  welcome to tweeting about Elon Musk. Like those aren't all bots. Like some

451
00:27:07,680 --> 00:27:11,610
Speaker 7:  of those are just people who can't spell very well. You discover

452
00:27:11,610 --> 00:27:15,510
Speaker 7:  if you reply to them. So Tesla is the original meme stock.

453
00:27:15,510 --> 00:27:18,960
Speaker 7:  And like one of the things that was going on with Tesla

454
00:27:18,960 --> 00:27:22,757
Speaker 7:  shares, you know, in 20 16, 20 17, 20 18

455
00:27:22,920 --> 00:27:26,760
Speaker 7:  is like Elon Musk can make the shares go up by tweeting positive things about

456
00:27:26,760 --> 00:27:30,450
Speaker 7:  Tesla. And like he's been disclosed and their company filings as being like

457
00:27:30,450 --> 00:27:33,870
Speaker 7:  a source of legitimate information, like his Twitter account. Isn't there

458
00:27:33,870 --> 00:27:37,680
Speaker 7:  a legitimate source of information about the stock. I take your point. And

459
00:27:37,680 --> 00:27:41,490
Speaker 7:  you lie about like, being a little cautious about wanting to say that like

460
00:27:41,490 --> 00:27:45,180
Speaker 7:  he's built his army on Twitter, but like I kind of think he has. And like

461
00:27:45,180 --> 00:27:48,870
Speaker 7:  he is maybe the most important financial influencer currently

462
00:27:48,870 --> 00:27:52,650
Speaker 7:  going, like he can send cryptocurrency up and down just by tweeting about

463
00:27:52,650 --> 00:27:56,280
Speaker 7:  it, you know? Yeah. There was the brief period where he, Tesla was accepting

464
00:27:56,280 --> 00:27:59,490
Speaker 7:  Bitcoin and that made Bitcoin go up. And then Tesla like stopped accepting

465
00:27:59,490 --> 00:28:03,030
Speaker 7:  Bitcoin and Bitcoin went down and like he used tweeted about dose and dose

466
00:28:03,030 --> 00:28:06,450
Speaker 7:  has gone up. So I do think that there's something very real here and it's

467
00:28:06,450 --> 00:28:10,380
Speaker 7:  because so many finance people are on Twitter. Like I don't, you

468
00:28:10,380 --> 00:28:13,650
Speaker 7:  gotta, you gotta, you gotta want to be on finance Twitter. I'll be real with

469
00:28:13,650 --> 00:28:17,610
Speaker 7:  you. Slotted penny stock guys that hate you. But like, there is a very real

470
00:28:17,610 --> 00:28:21,510
Speaker 7:  community like tweets show up on the Bloomberg terminal. And so that's

471
00:28:21,510 --> 00:28:24,900
Speaker 7:  another reason why Twitter is so powerful. Like all of these communities,

472
00:28:24,900 --> 00:28:28,650
Speaker 7:  like it's just, it's like the most hardcore people in any

473
00:28:28,650 --> 00:28:32,490
Speaker 7:  given community are all on Twitter together. And so Elon Musk like

474
00:28:32,490 --> 00:28:35,970
Speaker 7:  Twitter is really a source of power for him. Because

475
00:28:35,970 --> 00:28:39,300
Speaker 7:  again, he can, he can really move real world

476
00:28:39,300 --> 00:28:43,260
Speaker 7:  stocks by, by tweeting about them. And like, if you remember the game

477
00:28:43,260 --> 00:28:47,160
Speaker 7:  stop thing in January, 2020, there are a bunch of people who

478
00:28:47,160 --> 00:28:51,090
Speaker 7:  made a lot of money, but realizing that an Elon Musk tweet was the top and

479
00:28:51,090 --> 00:28:53,340
Speaker 7:  immediately selling as soon as they saw it, you

480
00:28:53,340 --> 00:28:55,590
Speaker 7:  know,

481
00:28:55,590 --> 00:28:58,950
Speaker 1:  I'm not saying it's, I'm not saying it's zero. I'm saying it's not a hundred.

482
00:28:58,950 --> 00:29:02,910
Speaker 1:  And we don't, there's no way of knowing, right? Like what is

483
00:29:02,910 --> 00:29:06,870
Speaker 1:  the conversion rate between an Ilan tweet and people mind, like,

484
00:29:06,870 --> 00:29:10,410
Speaker 1:  I don't know, that would be an amazing PhD thesis. Someone should do it.

485
00:29:10,410 --> 00:29:13,830
Speaker 1:  And then we'll write about it. Like, I would love to know that information

486
00:29:13,830 --> 00:29:16,290
Speaker 7:  If you're an economist,

487
00:29:16,290 --> 00:29:19,950
Speaker 1:  By the way, by throwing the game software, this reminds me this, the next

488
00:29:19,950 --> 00:29:22,170
Speaker 1:  hour is just gonna be talking about tweets that went viral and how bad it

489
00:29:22,170 --> 00:29:26,100
Speaker 1:  was. Like I did like the most in the light

490
00:29:26,100 --> 00:29:29,040
Speaker 1:  tweet during the whole GameStop fiasco and Robin hood was

491
00:29:29,040 --> 00:29:32,970
Speaker 1:  stopping sales. And I was like, here's a screenshot of the

492
00:29:32,970 --> 00:29:35,910
Speaker 1:  terms of service. Like this is what happens when everyone just hits agree

493
00:29:35,910 --> 00:29:39,810
Speaker 1:  without reading it. And then like the hordes descended on me that I was

494
00:29:39,810 --> 00:29:42,910
Speaker 1:  not being sympathetic to the, to the retail investor. And I was like, no,

495
00:29:42,910 --> 00:29:46,420
Speaker 1:  I'm just in my bag. Like, this is what I talked about. Like every day is

496
00:29:46,420 --> 00:29:49,900
Speaker 1:  terms of service agreements. Like I had nothing to

497
00:29:49,900 --> 00:29:53,770
Speaker 1:  say, anyhow, my broader

498
00:29:53,770 --> 00:29:56,830
Speaker 1:  point is like, Elon loves the service. When Addy comes on, we're going to

499
00:29:56,830 --> 00:29:59,890
Speaker 1:  talk about whether it's a town square or any of that

500
00:29:59,890 --> 00:30:03,640
Speaker 1:  noise. But the core of the problem, like we keep kind of

501
00:30:03,640 --> 00:30:07,570
Speaker 1:  dancing around. It was just say it out loud. Twitter needs to grow in order

502
00:30:07,570 --> 00:30:11,500
Speaker 1:  to grow, it needs to be much more consumer friendly. It needs

503
00:30:11,500 --> 00:30:15,490
Speaker 1:  to be a much friendlier environment to just sign up for it. You want to

504
00:30:15,490 --> 00:30:19,270
Speaker 1:  get a hundred million users, which is what we're all supposed to do in his

505
00:30:19,270 --> 00:30:23,110
Speaker 1:  tenure. As CEO, you cannot, you cannot

506
00:30:23,110 --> 00:30:26,410
Speaker 1:  have this experience with Twitter for initiate a hundred million new people

507
00:30:26,410 --> 00:30:29,060
Speaker 1:  who want to use your service every day. That's not how tic Tucker that's

508
00:30:29,060 --> 00:30:32,620
Speaker 1:  how YouTube grow. That's how snap is growing. Those are friendly,

509
00:30:32,620 --> 00:30:36,490
Speaker 1:  meaningfully safer environments than Twitter. But then the thing that

510
00:30:36,490 --> 00:30:40,420
Speaker 1:  makes Twitter valuable is just like the state of nature

511
00:30:40,420 --> 00:30:44,320
Speaker 1:  for nerds, right? We're like life is bloody and short

512
00:30:44,320 --> 00:30:48,100
Speaker 1:  on Twitter, but if you're a nerd, it's where you want to be. Right. And like,

513
00:30:48,100 --> 00:30:51,730
Speaker 1:  I don't like fucking, can you

514
00:30:51,730 --> 00:30:55,630
Speaker 1:  sell nature red in tooth and claw to a hundred million more people? Like,

515
00:30:55,630 --> 00:30:57,790
Speaker 1:  I don't know the answer to that question.

516
00:30:57,790 --> 00:31:01,190
Speaker 5:  I don't think we can because people keep comparing Twitter

517
00:31:01,190 --> 00:31:05,080
Speaker 5:  to Facebook or to snap or whatever.

518
00:31:05,080 --> 00:31:08,500
Speaker 5:  But really its main comparison is tumbler.

519
00:31:08,500 --> 00:31:12,490
Speaker 5:  Like it's a place, it's a place it's like developing fans and

520
00:31:12,490 --> 00:31:16,180
Speaker 5:  fandoms. And whether that's an Elan fandom or a TV show

521
00:31:16,180 --> 00:31:20,080
Speaker 5:  fandom or movies, or you know, tech, a lot of us are in

522
00:31:20,080 --> 00:31:23,860
Speaker 5:  that fandom. It, it it's building like a fandom. It's building like a community

523
00:31:23,860 --> 00:31:27,730
Speaker 5:  of like weirdos who want to talk about this stuff. Facebook is

524
00:31:27,730 --> 00:31:31,630
Speaker 5:  not super great at that. Snap is not super great at

525
00:31:31,630 --> 00:31:35,440
Speaker 5:  that. Tumblr was I guess, okay. At it,

526
00:31:35,440 --> 00:31:38,710
Speaker 5:  Twitter is really good at it. And that's like, but that's also like a small

527
00:31:38,710 --> 00:31:42,280
Speaker 5:  market. Those weird obsessives. I say that as one, those weird

528
00:31:42,280 --> 00:31:46,270
Speaker 5:  obsessives, like that's a small group of people. I don't think Twitter,

529
00:31:46,270 --> 00:31:49,840
Speaker 5:  like, unless it fundamentally changes its approach and how it

530
00:31:49,840 --> 00:31:53,830
Speaker 5:  operates. It's never going to move out of that. And I don't think it

531
00:31:53,830 --> 00:31:54,850
Speaker 5:  necessarily

532
00:31:54,850 --> 00:31:57,890
Speaker 5:  should.

533
00:31:57,890 --> 00:32:01,780
Speaker 1:  I'm curious your view here. Right? You've got a company that had an activist

534
00:32:01,780 --> 00:32:04,510
Speaker 1:  investor. They kicked out the founder CEO. They brought in the new CEO. He

535
00:32:04,510 --> 00:32:07,480
Speaker 1:  said, we're going to grow it. There's one way to grow it, which is to make

536
00:32:07,480 --> 00:32:11,230
Speaker 1:  it friendlier and to explore, expose more of the good things to more

537
00:32:11,230 --> 00:32:15,190
Speaker 1:  people and get them to sign up and engage. Then you've got Ilan who wants

538
00:32:15,190 --> 00:32:19,180
Speaker 1:  to open-source the algorithm, which implies the algorithm is important. And

539
00:32:19,180 --> 00:32:22,420
Speaker 1:  we have all been talking to the algorithm is like not what Twitter is, power

540
00:32:22,420 --> 00:32:26,410
Speaker 1:  users care about and it is bad. So you can open source it all

541
00:32:26,410 --> 00:32:29,680
Speaker 1:  day, right? Like the ranking algorithm on Twitter shows you tweets from 17

542
00:32:29,680 --> 00:32:33,340
Speaker 1:  hours ago or whatever, like the product isn't good. But somewhere in

543
00:32:33,340 --> 00:32:36,850
Speaker 1:  there is, I mean, that's the story of Twitter whip sawing

544
00:32:36,850 --> 00:32:40,820
Speaker 1:  between not having a chronological or doing the home

545
00:32:40,820 --> 00:32:44,780
Speaker 1:  feed by default and making you hit 15 buttons

546
00:32:44,780 --> 00:32:47,480
Speaker 1:  to click back to chronological recently and then walking that back. Cause

547
00:32:47,480 --> 00:32:51,350
Speaker 1:  all our power users like that dynamic between the people who make Twitter,

548
00:32:51,350 --> 00:32:55,250
Speaker 1:  what it is and the people it needs to attract the

549
00:32:55,250 --> 00:32:58,430
Speaker 1:  service are radically different, have radically different needs. That's been

550
00:32:58,430 --> 00:33:01,850
Speaker 1:  the motivating dynamic of Twitter, I think maybe the whole time. And I don't

551
00:33:01,850 --> 00:33:04,940
Speaker 1:  see it. I don't actually know where the company is on that perspective

552
00:33:04,940 --> 00:33:08,300
Speaker 1:  now. And I don't know if Ellen's going to knock it off that path, whatever

553
00:33:08,300 --> 00:33:08,810
Speaker 1:  it is

554
00:33:08,810 --> 00:33:12,620
Speaker 6:  Funny. You asked me that I did an interview with Twitter, new product leadership

555
00:33:12,620 --> 00:33:16,520
Speaker 6:  team about three weeks ago, talking about this. And before

556
00:33:16,520 --> 00:33:20,510
Speaker 6:  Ilan came in, they were addressing, hoping to address a law

557
00:33:20,510 --> 00:33:24,460
Speaker 6:  under a full-time CEO for the first time in years. And we should know

558
00:33:24,460 --> 00:33:27,140
Speaker 6:  like the reason Twitter has been so stagnant as a product is because Jack

559
00:33:27,140 --> 00:33:31,130
Speaker 6:  let it run by committee. Cause he was part-time and only weighed in on things

560
00:33:31,130 --> 00:33:34,940
Speaker 6:  he cared about. And so it was just this like imagine humping like

561
00:33:34,940 --> 00:33:38,930
Speaker 6:  Twitter being run by like 12 executives who need to like figure out

562
00:33:38,930 --> 00:33:42,890
Speaker 6:  their little fiefdoms. It didn't get anywhere right now.

563
00:33:42,890 --> 00:33:46,670
Speaker 6:  They, they are trying to reignite it and also

564
00:33:46,670 --> 00:33:49,280
Speaker 6:  turn it back a little bit to the early days, which I think is the answer

565
00:33:49,280 --> 00:33:53,240
Speaker 6:  here. If we were like up until 2015, Twitter was there were, there

566
00:33:53,240 --> 00:33:56,420
Speaker 6:  was a VC fund that was literally just investing in Twitter

567
00:33:56,420 --> 00:34:00,200
Speaker 6:  clients. Like there was a whole economy. People

568
00:34:00,200 --> 00:34:03,860
Speaker 6:  thought that Twitter was going to be this platform. And the

569
00:34:03,860 --> 00:34:07,160
Speaker 6:  best features of Twitter were invented by developers who were building on

570
00:34:07,160 --> 00:34:10,760
Speaker 6:  top of Twitter. And then we remember when they pulled the API out from everyone,

571
00:34:10,760 --> 00:34:13,340
Speaker 6:  when they realized we've got to bring everyone into our client because we

572
00:34:13,340 --> 00:34:17,060
Speaker 6:  need to sell advertising, we're going to start gating our new features to

573
00:34:17,060 --> 00:34:20,900
Speaker 6:  our client. And they slowly choked the life out of the tweet, decks, the

574
00:34:20,900 --> 00:34:24,840
Speaker 6:  Tweenies et cetera. I was just talking to their head of the guy, the

575
00:34:24,840 --> 00:34:28,790
Speaker 6:  lead guy. That's trying to bring the API back to those early days,

576
00:34:28,790 --> 00:34:32,540
Speaker 6:  like a few weeks ago. So they were wanting to, and then in the background

577
00:34:32,540 --> 00:34:36,170
Speaker 6:  they eventually want to just put it on like a decentralized protocol where

578
00:34:36,170 --> 00:34:39,860
Speaker 6:  Twitter is one of the platforms that then has

579
00:34:39,860 --> 00:34:43,790
Speaker 6:  clients on top of it. Right? So they want to like really abstract this

580
00:34:43,790 --> 00:34:47,750
Speaker 6:  and turn it into like a, a protocol for others to build on similar

581
00:34:47,750 --> 00:34:51,050
Speaker 6:  to email, similar to RSS. And that's a

582
00:34:51,050 --> 00:34:54,590
Speaker 6:  unique point of view in the world. Facebook snap, all these

583
00:34:54,590 --> 00:34:58,250
Speaker 6:  other social media companies, they're becoming like their own silos, their

584
00:34:58,250 --> 00:35:01,760
Speaker 6:  own super apps, trying to pull all the interactions into their

585
00:35:01,760 --> 00:35:05,330
Speaker 6:  client. So that's the opportunity. I think Twitter has to do something unique

586
00:35:05,330 --> 00:35:08,570
Speaker 6:  in the world and also kind of it aligns with the public nature of

587
00:35:08,570 --> 00:35:11,870
Speaker 6:  Twitter. And also they're just finally starting to look into

588
00:35:11,870 --> 00:35:15,050
Speaker 6:  subscriptions, which has been like, everyone's been saying, and Elon was

589
00:35:15,050 --> 00:35:18,680
Speaker 6:  saying this, he would want the company to do subscriptions and to charge

590
00:35:18,680 --> 00:35:22,310
Speaker 6:  its highest users. I guess maybe not him. Maybe he gets a special

591
00:35:22,310 --> 00:35:26,270
Speaker 6:  discount there, but you know, charge these accounts

592
00:35:26,270 --> 00:35:29,900
Speaker 6:  that use these as their main marketing, you know, Tesla doesn't spend

593
00:35:29,900 --> 00:35:33,620
Speaker 6:  anything on marketing because Elan has a Twitter account with 80 million

594
00:35:33,620 --> 00:35:36,650
Speaker 6:  users, Twitter to Elan is worth billions of dollars a

595
00:35:36,650 --> 00:35:40,230
Speaker 6:  year and they could be charging for that kind of stuff.

596
00:35:40,230 --> 00:35:43,830
Speaker 6:  And you know, like I pay for Twitter blue because I want to change my app

597
00:35:43,830 --> 00:35:47,820
Speaker 6:  icon. And I'm like, it doesn't really do anything. They put nuzzle in

598
00:35:47,820 --> 00:35:51,780
Speaker 6:  it like, cool. Like they're going to bring, edit tweet to it. Cool. But like

599
00:35:51,780 --> 00:35:54,720
Speaker 6:  they just haven't really, they're starting to show signs of willingness to

600
00:35:54,720 --> 00:35:57,180
Speaker 6:  experiment again, but they haven't really leaned into it fully, but they

601
00:35:57,180 --> 00:36:00,840
Speaker 6:  were in the process before Elan came in. So it's like, they didn't get the

602
00:36:00,840 --> 00:36:04,830
Speaker 6:  chance to really try. So it's like, it's just kind of sad in

603
00:36:04,830 --> 00:36:08,430
Speaker 6:  that way that now everyone's like been derailed by this because they were

604
00:36:08,430 --> 00:36:11,970
Speaker 6:  really trying to turn the ship around and they had this gun to their head,

605
00:36:11,970 --> 00:36:14,880
Speaker 6:  which was this growth target that their activist investor gave them.

606
00:36:14,880 --> 00:36:18,330
Speaker 1:  I feel bad for the new CEO. Right. Like, yeah. Can you imagine getting that

607
00:36:18,330 --> 00:36:20,970
Speaker 1:  job and being all excited and then he had a baby he's

608
00:36:20,970 --> 00:36:22,680
Speaker 6:  Supposed to be on parental leave.

609
00:36:22,680 --> 00:36:26,280
Speaker 1:  Yeah. And he made this like, I think extremely cool public

610
00:36:26,280 --> 00:36:29,610
Speaker 1:  declaration that even after taking the new job as a CEO, he was going on

611
00:36:29,610 --> 00:36:32,670
Speaker 1:  leave to like set an example. That leaves should be like, I love that. I

612
00:36:32,670 --> 00:36:35,800
Speaker 1:  love this dude. Probably come on decoder. He's not going to do it. I don't,

613
00:36:35,800 --> 00:36:39,210
Speaker 1:  I don't think I'm getting any Twitter executives for a while, but Elan you

614
00:36:39,210 --> 00:36:43,200
Speaker 1:  are invited. Come on, decoder. Let's let's talk. But I

615
00:36:43,200 --> 00:36:46,020
Speaker 1:  just, I feel bad for that guy in particular. Like, I don't know if his vision

616
00:36:46,020 --> 00:36:49,170
Speaker 1:  was right out. If it was going to work, it seems very clear. He was never,

617
00:36:49,170 --> 00:36:52,590
Speaker 1:  he's never going to get the shot to do whatever he wanted for even her appearance.

618
00:36:52,590 --> 00:36:56,190
Speaker 6:  Yeah. I agree. And now what is the best outcome of

619
00:36:56,190 --> 00:37:00,030
Speaker 6:  this? I mean, it's that Elan potentially

620
00:37:00,030 --> 00:37:03,900
Speaker 6:  backs out. I mean, I don't want to, like, he backs out the stock

621
00:37:03,900 --> 00:37:07,500
Speaker 6:  tanks. Maybe they get some other passive investor in there and morale is

622
00:37:07,500 --> 00:37:11,280
Speaker 6:  destroyed and people's options are underwater or Elon

623
00:37:11,280 --> 00:37:15,120
Speaker 6:  does take it private and then people quit and mass, like, those are

624
00:37:15,120 --> 00:37:18,270
Speaker 6:  kind of the two options right now, which is like not a good sign

625
00:37:18,270 --> 00:37:21,480
Speaker 5:  Stock tank if he backs out, like, cause it hasn't

626
00:37:21,480 --> 00:37:25,260
Speaker 5:  changed. It didn't change when he, he announced he was going to buy

627
00:37:25,260 --> 00:37:25,560
Speaker 5:  it.

628
00:37:25,560 --> 00:37:28,890
Speaker 6:  Well, there's still his premium in there from when he first disclosed his

629
00:37:28,890 --> 00:37:31,020
Speaker 6:  steak, it would go, it would go 30 or below for

630
00:37:31,020 --> 00:37:31,440
Speaker 1:  Sure.

631
00:37:31,440 --> 00:37:34,950
Speaker 7:  Yeah. So Alex, I think that those are two possible

632
00:37:34,950 --> 00:37:38,760
Speaker 7:  outcomes. I do not think that they are the only possible outcomes. And

633
00:37:38,760 --> 00:37:42,390
Speaker 7:  again, the reason I don't think so is because now Twitter is in play and

634
00:37:42,390 --> 00:37:45,990
Speaker 7:  it is an incredibly influential service for all that. It

635
00:37:45,990 --> 00:37:49,830
Speaker 7:  is like all versus all fighting and like

636
00:37:49,830 --> 00:37:52,770
Speaker 7:  very unpleasant. If you have more than about 2000

637
00:37:52,770 --> 00:37:56,580
Speaker 7:  followers, because whoever controls, Twitter

638
00:37:56,580 --> 00:38:00,480
Speaker 7:  controls a lot of what happens on cable news.

639
00:38:00,480 --> 00:38:04,050
Speaker 7:  And like to some degree, you know,

640
00:38:04,050 --> 00:38:07,750
Speaker 7:  buying Twitter, like there was this comparison that I saw on, on

641
00:38:07,750 --> 00:38:11,430
Speaker 7:  Twitter, but about, you know, Bezos

642
00:38:11,430 --> 00:38:15,330
Speaker 7:  buying the Washington post and Musk buying Twitter. And

643
00:38:15,330 --> 00:38:18,600
Speaker 7:  like they are actually pretty comparable. Like the Washington post, I think

644
00:38:18,600 --> 00:38:22,020
Speaker 7:  Bezos has been a great steward of, but with Twitter, what you're

645
00:38:22,020 --> 00:38:25,890
Speaker 7:  controlling is essentially the comments section for the entire internet.

646
00:38:25,890 --> 00:38:29,220
Speaker 7:  And that's really powerful. So, you know, if

647
00:38:29,220 --> 00:38:33,090
Speaker 7:  I'm at another tech company, I'm like calling my

648
00:38:33,090 --> 00:38:36,300
Speaker 7:  lawyers to see what I can buy without triggering antitrust.

649
00:38:36,300 --> 00:38:37,780
Speaker 1:  This was the mark Cuban tweet.

650
00:38:37,780 --> 00:38:41,500
Speaker 7:  Yeah. Because like that, like that actually is a

651
00:38:41,500 --> 00:38:45,430
Speaker 7:  very, very powerful tool for marketing, for culture making for all of those

652
00:38:45,430 --> 00:38:48,460
Speaker 7:  things. And I think that there are probably people out there who are interested

653
00:38:48,460 --> 00:38:52,090
Speaker 7:  in the, in it. And like at this point, you know, if you're the board, maybe

654
00:38:52,090 --> 00:38:55,810
Speaker 7:  you're looking for a white Knight. So like we might see another

655
00:38:55,810 --> 00:38:57,070
Speaker 7:  player emerge.

656
00:38:57,070 --> 00:38:58,510
Speaker 6:  Oh no, you're totally right.

657
00:38:58,510 --> 00:39:01,540
Speaker 1:  I would not discount the power if you go for some of this, like if you're

658
00:39:01,540 --> 00:39:04,450
Speaker 1:  one of those players, you get to put one over on Ilan.

659
00:39:04,450 --> 00:39:07,840
Speaker 1:  Like there's more interest in that than I think you would

660
00:39:07,840 --> 00:39:08,830
Speaker 1:  imagine.

661
00:39:08,830 --> 00:39:12,760
Speaker 6:  No, I guarantee you if, if the year was 2015 and antitrust

662
00:39:12,760 --> 00:39:16,660
Speaker 6:  scrutiny had not reached what it was, mark Zuckerberg has a blank check right

663
00:39:16,660 --> 00:39:19,810
Speaker 6:  now. Like absolutely like what he did for WhatsApp. He put this

664
00:39:19,810 --> 00:39:23,230
Speaker 6:  like, like Zuck multiple on it. That made no

665
00:39:23,230 --> 00:39:26,320
Speaker 6:  sense. Like he did for Instagram. Like he's done for all the deals he cares

666
00:39:26,320 --> 00:39:28,900
Speaker 6:  about. He'd buy Twitter for a hundred billion dollar

667
00:39:28,900 --> 00:39:31,210
Speaker 1:  If he could get it. Right. But he's also just a rich guy. Like why isn't

668
00:39:31,210 --> 00:39:34,960
Speaker 1:  he just staking someone else to do it? There's a lot of that out

669
00:39:34,960 --> 00:39:37,900
Speaker 1:  there. Right? Like you actually didn't who doesn't. We never got to this,

670
00:39:37,900 --> 00:39:40,810
Speaker 1:  but you know who doesn't have all the cash to this is Elon Musk.

671
00:39:40,810 --> 00:39:44,350
Speaker 7:  Yeah. It does not know. He he's, he's not liquid. And he's already got a

672
00:39:44,350 --> 00:39:46,240
Speaker 7:  bunch of loans against his shares.

673
00:39:46,240 --> 00:39:49,780
Speaker 1:  Yeah. Tesla shares. Maybe he'll just like, I don't know. So advertising the

674
00:39:49,780 --> 00:39:53,410
Speaker 1:  side of space, X rockets, like raise funds. We've gone way over on the

675
00:39:53,410 --> 00:39:57,160
Speaker 1:  segment list. Thank you so much. I imagine you're going to be back to talk

676
00:39:57,160 --> 00:39:59,590
Speaker 1:  about this a few times over the next few.

677
00:39:59,590 --> 00:40:03,580
Speaker 7:  Listen, I, you know, I got relaxed, right? Like after 2018,

678
00:40:03,580 --> 00:40:07,450
Speaker 7:  like things calm down and I was like, okay, okay. Like I don't have to, like,

679
00:40:07,450 --> 00:40:10,750
Speaker 7:  I don't have to constantly pay attention to Ilan. I can take the alerts off

680
00:40:10,750 --> 00:40:14,650
Speaker 7:  his tweets and now I regret it because the Ilan cycle has

681
00:40:14,650 --> 00:40:18,460
Speaker 7:  started again. So I'm sure we're going to be talking about this

682
00:40:18,460 --> 00:40:20,230
Speaker 7:  again,

683
00:40:20,230 --> 00:40:23,020
Speaker 1:  Liz, every now and again, I would get text messages from, it was like in

684
00:40:23,020 --> 00:40:26,140
Speaker 1:  the <inaudible> of early on cycles, usually like he's e-learning

685
00:40:26,140 --> 00:40:29,770
Speaker 1:  again. And like the Elon Musk war room would spin up in slack.

686
00:40:29,770 --> 00:40:33,670
Speaker 1:  Like here we are. That's great. All right. We

687
00:40:33,670 --> 00:40:36,940
Speaker 1:  gotta take a break. Addie Robertson is going to come and join us to talk

688
00:40:36,940 --> 00:40:40,150
Speaker 1:  about content moderation because that's another whole side of this. Thank

689
00:40:40,150 --> 00:40:40,960
Speaker 1:  you, Liz. We'll be right

690
00:40:40,960 --> 00:40:45,040
Speaker 1:  back.

691
00:41:33,050 --> 00:41:36,800
Speaker 10:  You listened to this show weekend and week out because, well, we'd like

692
00:41:36,800 --> 00:41:40,130
Speaker 10:  to think we offer sharp unfiltered insights into the biggest stories in

693
00:41:40,130 --> 00:41:43,850
Speaker 10:  tech, business and politics. We also understand that your time is

694
00:41:43,850 --> 00:41:47,030
Speaker 10:  valuable and we want you to apply the lessons that you hear on this

695
00:41:47,030 --> 00:41:48,290
Speaker 10:  podcast.

696
00:42:54,000 --> 00:42:57,720
Speaker 1:  We're back. Eddie Robertson is here. Hey Eddie. Hey, so

697
00:42:57,720 --> 00:43:01,530
Speaker 1:  Alex's house. Heath is still here and Alex trans. So Addie we've been talking

698
00:43:01,530 --> 00:43:05,400
Speaker 1:  about Ilan and Twitter. Of course, we just have Liz on. We must be

699
00:43:05,400 --> 00:43:08,340
Speaker 1:  talking to the mechanics of the deal. What would happen? Twitter is a company

700
00:43:08,340 --> 00:43:11,820
Speaker 1:  I wanted you on you and I spent an awful lot of time in our live segment,

701
00:43:11,820 --> 00:43:15,540
Speaker 1:  content moderation and free speech and all the stuff Elan seems

702
00:43:15,540 --> 00:43:18,750
Speaker 1:  very motivated to somehow change how Twitter

703
00:43:18,750 --> 00:43:22,380
Speaker 1:  moderates and yesterday, I don't know if this was by

704
00:43:22,380 --> 00:43:25,740
Speaker 1:  coincidence or by design or just the pure,

705
00:43:25,740 --> 00:43:29,700
Speaker 1:  chaotic energy of the universe. He was onstage at Ted. You might

706
00:43:29,700 --> 00:43:33,510
Speaker 1:  remember Ted, Ted, Pete, once influential conference for big

707
00:43:33,510 --> 00:43:37,350
Speaker 1:  thoughts. We all, we all signed up for a free stream of

708
00:43:37,350 --> 00:43:41,280
Speaker 1:  Ted yesterday because Elan was going to be there. And we like

709
00:43:41,280 --> 00:43:45,030
Speaker 1:  sat through what appeared to be like a ninth grade social studies course

710
00:43:45,030 --> 00:43:48,120
Speaker 1:  on a manual con and John Stuart mill. Like I literally read John cert melanose

711
00:43:48,120 --> 00:43:51,030
Speaker 1:  in ninth grade. And Ted was like, have you heard, have you heard of John

712
00:43:51,100 --> 00:43:54,120
Speaker 1:  Stuart mill? Like, I don't know what's going on there. It was like, you should

713
00:43:54,120 --> 00:43:57,940
Speaker 1:  read it. It's foundations of Western liberalism were contained in John

714
00:43:58,000 --> 00:44:01,500
Speaker 1:  Stuart mill, but I don't know why the Ted audience needed to know that information.

715
00:44:01,500 --> 00:44:05,400
Speaker 1:  Anyway. So Chris Anderson, who is the head of Ted sat

716
00:44:05,400 --> 00:44:09,090
Speaker 1:  down for an interview with Elan, I would describe that interview is

717
00:44:09,090 --> 00:44:12,930
Speaker 1:  embarrassingly stupid, just on both ends. Like it was just real

718
00:44:12,930 --> 00:44:16,680
Speaker 1:  bad, but we did learn some bits and bobs about

719
00:44:16,680 --> 00:44:20,610
Speaker 1:  what Elon thinks and given just our history and covering it. I think

720
00:44:20,610 --> 00:44:23,670
Speaker 1:  there's some challenges that so Eddie, you want to walk through what we think

721
00:44:23,670 --> 00:44:27,420
Speaker 1:  we learned about Elan's approach to content moderation on Twitter?

722
00:44:27,420 --> 00:44:31,050
Speaker 12:  Yeah. So there is very

723
00:44:31,050 --> 00:44:34,920
Speaker 12:  long string of dumb things that were said that I am sure that maybe we'll

724
00:44:34,920 --> 00:44:38,340
Speaker 12:  get into in more detail, but basically Elon Musks

725
00:44:38,340 --> 00:44:41,970
Speaker 12:  says, okay, we understand that we want this to be a platform for global

726
00:44:41,970 --> 00:44:45,210
Speaker 12:  free speech, but we know that you're going to have to follow laws. And we

727
00:44:45,210 --> 00:44:48,840
Speaker 12:  think basically Twitter should just follow the laws of countries,

728
00:44:48,840 --> 00:44:52,770
Speaker 12:  which implies sort of that ears. They're supposed to be just like

729
00:44:52,770 --> 00:44:56,700
Speaker 12:  a first amendment Twitter. But at the same time, he also is really

730
00:44:56,700 --> 00:45:00,420
Speaker 12:  mad about bots and spam. So he wants to get rid of bots because

731
00:45:00,420 --> 00:45:04,230
Speaker 12:  bots are not part of free speech, mostly because he's mad about

732
00:45:04,230 --> 00:45:07,800
Speaker 12:  cryptocurrency Schilling. I, it seems like. And

733
00:45:07,800 --> 00:45:11,790
Speaker 12:  then at the same time, he's very worried about the idea that there's an

734
00:45:11,790 --> 00:45:15,600
Speaker 12:  opaque algorithm that is demoting or promoting tweets in

735
00:45:15,600 --> 00:45:19,470
Speaker 12:  ways that are not visible, which is one of those things that

736
00:45:19,470 --> 00:45:23,280
Speaker 12:  it was a big meme and sort of conservative circles several years ago that

737
00:45:23,280 --> 00:45:26,700
Speaker 12:  the idea that there was shadow banning quote unquote, that was making it

738
00:45:26,700 --> 00:45:30,630
Speaker 12:  harder to search for things. But I'm not entirely certain what

739
00:45:30,630 --> 00:45:34,560
Speaker 12:  he's referring to in this particular case, because Twitter tends

740
00:45:34,560 --> 00:45:38,550
Speaker 12:  to be kind of upfront about telling people when they're banned and about

741
00:45:38,550 --> 00:45:42,420
Speaker 12:  telling people like they to Russian

742
00:45:42,420 --> 00:45:46,230
Speaker 12:  news outlet, like state news outlet, information and tweets, but they were

743
00:45:46,230 --> 00:45:48,780
Speaker 12:  just like, yes, we're going to do this. We're telling you about this thing

744
00:45:48,780 --> 00:45:52,500
Speaker 12:  proactively. So I'm, it feels like he's responding to a bunch of

745
00:45:52,500 --> 00:45:55,710
Speaker 12:  things that are a little bit weird and

746
00:45:55,710 --> 00:45:59,250
Speaker 12:  confusing and not actually the speech challenges

747
00:45:59,250 --> 00:46:01,230
Speaker 12:  that Twitter faces.

748
00:46:01,230 --> 00:46:04,320
Speaker 1:  Yeah. Let's start with opensource algorithm. Cause we were just talking about

749
00:46:04,320 --> 00:46:08,040
Speaker 1:  it in the previous section a little bit, all these platforms have done

750
00:46:08,040 --> 00:46:11,730
Speaker 1:  things to try to be more transparent. The thing that occurs to me in

751
00:46:11,730 --> 00:46:14,580
Speaker 1:  just listening to people blather on about opensource source

752
00:46:14,580 --> 00:46:18,060
Speaker 1:  algorithms and Ilan was like, we'll just put it on GitHub and people can

753
00:46:18,060 --> 00:46:21,810
Speaker 1:  leave comments that has no connection

754
00:46:21,810 --> 00:46:25,770
Speaker 1:  to being transparent about what happens to an individual tweet,

755
00:46:25,770 --> 00:46:29,730
Speaker 1:  right? Like if you are, if you think you can derive what happened to your

756
00:46:29,730 --> 00:46:33,630
Speaker 1:  tweet from the code being on get hub, you are the

757
00:46:33,630 --> 00:46:36,690
Speaker 1:  single best computer scientists in the history of the world, right? Like

758
00:46:36,690 --> 00:46:39,840
Speaker 1:  there's no way, like, I don't know how you would do that. Well, you want

759
00:46:39,840 --> 00:46:43,740
Speaker 1:  as a button on every tweet that says this was down ranked or

760
00:46:43,740 --> 00:46:47,520
Speaker 1:  up ranked or your, I dunno, your army of

761
00:46:47,520 --> 00:46:51,420
Speaker 1:  Tesla, people was busy watching a baseball game today. So they didn't like,

762
00:46:51,420 --> 00:46:54,780
Speaker 1:  you want some of that information or like Jack was mad at you when he turned

763
00:46:54,780 --> 00:46:58,200
Speaker 1:  down the big knob in his office and said, Elan's reach, right. Like that's

764
00:46:58,200 --> 00:47:02,130
Speaker 1:  the thing you're actually, after the code might tell you that those buttons

765
00:47:02,130 --> 00:47:05,640
Speaker 1:  and knobs exist, but they will not actually tell you how they're used. So

766
00:47:05,640 --> 00:47:09,600
Speaker 1:  just like from the jump, there's this massive discrepancy in the

767
00:47:09,600 --> 00:47:12,210
Speaker 1:  outcome you want and the solution proposed.

768
00:47:12,210 --> 00:47:15,510
Speaker 12:  Yes. It also does not mention really how, I guess you leave a comment on

769
00:47:15,510 --> 00:47:18,570
Speaker 12:  GitHub and then there's just an army of moderators that will check and be

770
00:47:18,570 --> 00:47:22,500
Speaker 12:  like, okay. So all the tweets that we moderated based on this turns

771
00:47:22,500 --> 00:47:25,200
Speaker 12:  out, we should have done that differently. Like it doesn't, the appeals

772
00:47:25,200 --> 00:47:29,050
Speaker 12:  process is also a huge part of how Twitter works and why it's

773
00:47:29,050 --> 00:47:31,840
Speaker 12:  frustrating when it's frustrating. And he, we didn't really learn much about

774
00:47:31,840 --> 00:47:32,350
Speaker 12:  that.

775
00:47:32,350 --> 00:47:36,130
Speaker 1:  Yeah. So that's open source algorithm, which I feel like we can just

776
00:47:36,130 --> 00:47:40,030
Speaker 1:  dispense with. Although I will say that when pressed in this interview on

777
00:47:40,030 --> 00:47:44,020
Speaker 1:  any specific Elan returned to the idea of open

778
00:47:44,020 --> 00:47:47,830
Speaker 1:  source algorithm is a, was like his happy place. Like

779
00:47:47,830 --> 00:47:51,430
Speaker 1:  here's this very, I mean, just embarrassingly stupid

780
00:47:51,430 --> 00:47:54,670
Speaker 1:  exchange. This is my I'm just going to, I'm going to end up repeating this

781
00:47:54,670 --> 00:47:58,270
Speaker 1:  until I die. So Chris Anderson is like, here's the

782
00:47:58,270 --> 00:48:01,240
Speaker 1:  challenge. I'm just quoting Chris Anderson. It's a nuanced difference between

783
00:48:01,240 --> 00:48:04,870
Speaker 1:  different things. There's incitement to violence. That's a no, if it's

784
00:48:04,870 --> 00:48:08,740
Speaker 1:  illegal, which I would just say implies that there are some,

785
00:48:08,740 --> 00:48:12,610
Speaker 1:  some form of legal incitement to violence, which if you can

786
00:48:12,610 --> 00:48:16,150
Speaker 1:  discover, like let me know. Cause is that just like, let's do

787
00:48:16,150 --> 00:48:17,680
Speaker 1:  swords. And like, it's like, I

788
00:48:17,680 --> 00:48:21,010
Speaker 1:  dunno,

789
00:48:21,010 --> 00:48:24,490
Speaker 1:  Whatever. So there's incitements to violence. That's a no, if it's

790
00:48:24,490 --> 00:48:28,180
Speaker 1:  illegal, there's hate speech. Which some forms of hate

791
00:48:28,180 --> 00:48:31,150
Speaker 1:  speech are fine. I hate spinach,

792
00:48:31,150 --> 00:48:32,710
Speaker 13:  Which

793
00:48:32,710 --> 00:48:36,100
Speaker 1:  Like, I just like echoing through my head. Like you didn't

794
00:48:36,100 --> 00:48:39,730
Speaker 1:  prepare, you don't know what you're talking about. All right. So then here's

795
00:48:39,730 --> 00:48:43,450
Speaker 1:  the example. Ilan says, spinach is delicious. If it's sauteed in cream sauce,

796
00:48:43,450 --> 00:48:47,110
Speaker 1:  this is all hack actually happened. Then Chris Anderson says, let's say I

797
00:48:47,110 --> 00:48:50,590
Speaker 1:  tweet, I hate politician X. The next tweet is I wish politician X wasn't

798
00:48:50,590 --> 00:48:54,400
Speaker 1:  alive. And then he qualifies this. Some of them said, this is about Putin

799
00:48:54,400 --> 00:48:58,000
Speaker 1:  right now. So that's legitimate speech there. Just like

800
00:48:58,000 --> 00:49:01,660
Speaker 1:  UN like if you're like, you know what if it's about that guy or some of us

801
00:49:01,660 --> 00:49:04,780
Speaker 1:  have said it that's obviously, like I said it. So that's obviously cool.

802
00:49:04,780 --> 00:49:08,500
Speaker 1:  Who knows? And then another tweet is I wish politician X wasn't a

803
00:49:08,500 --> 00:49:11,530
Speaker 1:  lad alive with a picture of their head over with a gun Gunsight over at plus

804
00:49:11,530 --> 00:49:14,110
Speaker 1:  her address. At some point, someone has to make a decision as to which of

805
00:49:14,110 --> 00:49:17,500
Speaker 1:  those is not okay. Ken, an algorithm do that. The answer to that question

806
00:49:17,500 --> 00:49:19,990
Speaker 1:  is fucking of course not.

807
00:49:19,990 --> 00:49:21,880
Speaker 12:  I also like to point out none of that is hate

808
00:49:21,880 --> 00:49:25,570
Speaker 12:  speech. Well, not even

809
00:49:25,570 --> 00:49:28,540
Speaker 12:  forget whether it's legal or not. None of that actually is what people who

810
00:49:28,540 --> 00:49:30,820
Speaker 12:  hate, who are like upset about hate speech say is

811
00:49:30,820 --> 00:49:33,220
Speaker 12:  hate.

812
00:49:33,220 --> 00:49:36,850
Speaker 1:  I hate spinach. Some sun hate speeches. Okay.

813
00:49:36,850 --> 00:49:38,890
Speaker 1:  I,

814
00:49:38,890 --> 00:49:42,790
Speaker 12:  I hate, spinaches actually closer to hates. I don't want

815
00:49:42,790 --> 00:49:43,570
Speaker 12:  this politician

816
00:49:43,570 --> 00:49:48,280
Speaker 12:  dead.

817
00:49:48,280 --> 00:49:52,060
Speaker 1:  So I was just wondering if you asked

818
00:49:52,060 --> 00:49:55,540
Speaker 1:  any four people, you know, like the four of us or like four of your

819
00:49:55,540 --> 00:49:59,410
Speaker 1:  friends to make determinations on how to rank those things as

820
00:49:59,410 --> 00:50:03,400
Speaker 1:  acceptable Arctic social speech, we would all disagree. That is the nature

821
00:50:03,400 --> 00:50:06,880
Speaker 1:  of these debates. So can an algorithm do that? I was like, I dunno,

822
00:50:06,880 --> 00:50:09,100
Speaker 1:  man, probably not

823
00:50:09,100 --> 00:50:10,000
Speaker 13:  Just based

824
00:50:10,000 --> 00:50:13,840
Speaker 1:  On what I know about human nature. So he says, surely you need some human

825
00:50:13,840 --> 00:50:17,800
Speaker 1:  judgment. That point fine. He is minorly rescued himself.

826
00:50:17,800 --> 00:50:21,700
Speaker 1:  Elan's response to this is just in my view. Twitter

827
00:50:21,700 --> 00:50:24,880
Speaker 1:  should match the laws of the country. That's it? That's his

828
00:50:24,880 --> 00:50:28,490
Speaker 1:  answer. And he's like, there's an obligation to that. But going beyond

829
00:50:28,490 --> 00:50:31,880
Speaker 1:  that and having it be unclear, who's making what changes to where having

830
00:50:31,880 --> 00:50:35,570
Speaker 1:  tweets, mysteriously promoted, demoted having a black box algorithm, promotes

831
00:50:35,570 --> 00:50:38,510
Speaker 1:  some things, not other things. I think this can be quite dangerous. None

832
00:50:38,510 --> 00:50:42,200
Speaker 1:  of that is an answer to I've given you three tweets, which one is unacceptable,

833
00:50:42,200 --> 00:50:46,190
Speaker 1:  by the way, what's the deal with spinach and like the whole conversation

834
00:50:46,190 --> 00:50:50,000
Speaker 1:  is here's something that appears to be a hard question. Anyone

835
00:50:50,000 --> 00:50:53,960
Speaker 1:  saying open the algorithm up in some way. And I just don't see the

836
00:50:53,960 --> 00:50:56,960
Speaker 1:  connection between the two Addie. I'm wondering if you do.

837
00:50:56,960 --> 00:51:00,830
Speaker 12:  I don't either. It feels, this is a thing that it feels like a bunch of

838
00:51:00,830 --> 00:51:04,640
Speaker 12:  people doing policy. I hear it mostly with anti-trust and speech where it's

839
00:51:04,640 --> 00:51:07,460
Speaker 12:  like, oh, well, okay. So what's the problem with moderation it's that these

840
00:51:07,460 --> 00:51:11,000
Speaker 12:  companies are too big, which is like, okay, fair. These are problems, but

841
00:51:11,000 --> 00:51:14,330
Speaker 12:  there's you can't just like fix one thing in tech

842
00:51:14,330 --> 00:51:18,290
Speaker 12:  and platforms and everything else gets fixed with it. This all

843
00:51:18,290 --> 00:51:22,070
Speaker 12:  just seems there's so many nonsequitors in that interview.

844
00:51:22,070 --> 00:51:24,590
Speaker 12:  It really, it was extremely frustrating.

845
00:51:24,590 --> 00:51:27,980
Speaker 1:  Give me my favorite one. I'm sorry. This whole thing was so

846
00:51:27,980 --> 00:51:31,940
Speaker 1:  stupid. So Ilan says obviously Twitter or any form is bound by the laws of

847
00:51:31,940 --> 00:51:34,850
Speaker 1:  the country that operates in. So obviously there are some limitations in

848
00:51:34,850 --> 00:51:37,940
Speaker 1:  three speech in the United States. He says, obviously there's some limitations

849
00:51:37,940 --> 00:51:40,370
Speaker 1:  in Twitter. I have to abide by those rules. And then Chris Anderson follows

850
00:51:40,370 --> 00:51:43,850
Speaker 1:  up. So you can incite people to violence, like direct incitement of violence.

851
00:51:43,850 --> 00:51:47,420
Speaker 1:  You can't do the equivalent of crying fire in a movie theater, for example,

852
00:51:47,420 --> 00:51:51,410
Speaker 1:  and Ilan responds with it. That is a crime. It should be a

853
00:51:51,410 --> 00:51:55,370
Speaker 1:  crime. Now I will tell you that you can walk into

854
00:51:55,370 --> 00:51:58,340
Speaker 1:  any theater in America and yell fire. And it is not a

855
00:51:58,340 --> 00:52:02,300
Speaker 1:  crime definitely should not be a crime.

856
00:52:02,300 --> 00:52:06,290
Speaker 1:  There's a long history of that. People who say that kind of immediately

857
00:52:06,290 --> 00:52:08,990
Speaker 1:  out themselves as having no idea what's going on at a, you want to kind of

858
00:52:08,990 --> 00:52:12,860
Speaker 1:  explain, I could try, but I'll just start crying.

859
00:52:12,860 --> 00:52:16,670
Speaker 12:  I'm the, they're actually very, very good explanations

860
00:52:16,670 --> 00:52:20,030
Speaker 12:  online. The Atlantic's that are really fantastic. One, but the long story

861
00:52:20,030 --> 00:52:23,210
Speaker 12:  short is there has never been a case about whether there was a crowded theater

862
00:52:23,210 --> 00:52:26,930
Speaker 12:  in some guy cried fire. It was a metaphor that was used in a

863
00:52:26,930 --> 00:52:30,920
Speaker 12:  case, a Supreme court case. That was what was,

864
00:52:30,920 --> 00:52:34,370
Speaker 12:  what was it about? It was about draft speech, right?

865
00:52:34,370 --> 00:52:38,360
Speaker 1:  Yes. It was world war, one husband or two people in our,

866
00:52:38,360 --> 00:52:41,360
Speaker 1:  her as a husband or wife. But two people were distributing leaflets in the

867
00:52:41,360 --> 00:52:44,780
Speaker 1:  run up to world war. One saying, you should disobey the draft because war

868
00:52:44,780 --> 00:52:48,530
Speaker 1:  is bad. They were socialists. And the government arrested them. And the

869
00:52:48,530 --> 00:52:51,500
Speaker 1:  Supreme court said, that's cool that you can limit their speech in that way,

870
00:52:51,500 --> 00:52:54,356
Speaker 1:  because they are creating a clear and present dangerous United States of

871
00:52:54,380 --> 00:52:54,638
Speaker 1:  America,

872
00:52:54,680 --> 00:52:57,680
Speaker 12:  Which was like shouting fire in a crowded theater

873
00:52:57,680 --> 00:53:01,250
Speaker 1:  With the metaphor is like, certainly there are some limits and that was the

874
00:53:01,250 --> 00:53:02,360
Speaker 1:  metaphor.

875
00:53:02,360 --> 00:53:05,990
Speaker 12:  And then this was later overturned if I remember correctly. And so not only

876
00:53:05,990 --> 00:53:09,710
Speaker 12:  was it not a thing that happened, it was a metaphor for a thing that

877
00:53:09,710 --> 00:53:13,040
Speaker 12:  wasn't even that is at this point no longer even current.

878
00:53:13,040 --> 00:53:16,850
Speaker 1:  Yes. So that shank was overturned in part by what's it

879
00:53:16,850 --> 00:53:20,810
Speaker 1:  Brandenburg? Yeah. It's Brandenburg, Ohio. I don't remember that

880
00:53:20,810 --> 00:53:23,840
Speaker 1:  case. It's about Brandenburg was a guy he talked to Ohio was mad at him.

881
00:53:23,840 --> 00:53:27,780
Speaker 1:  I don't know. But they out clear and present danger, which is where that

882
00:53:27,780 --> 00:53:31,170
Speaker 1:  phrase comes from. They threw out the whole case and they replaced it and

883
00:53:31,170 --> 00:53:34,920
Speaker 1:  said, the government cannot punish inflammatory speech unless it directly

884
00:53:34,920 --> 00:53:38,340
Speaker 1:  incites or produces imminent lawless action.

885
00:53:38,340 --> 00:53:42,300
Speaker 1:  And that's it like, that's the thing, the government. So if you are

886
00:53:42,300 --> 00:53:45,150
Speaker 1:  like, I want everyone to write, let's do

887
00:53:45,150 --> 00:53:49,140
Speaker 1:  swords right now against the guy

888
00:53:49,140 --> 00:53:53,010
Speaker 1:  is like, you're directing people towards imminent lawless action. The government

889
00:53:53,010 --> 00:53:54,750
Speaker 1:  can stop that speech,

890
00:53:54,750 --> 00:53:55,590
Speaker 5:  Right?

891
00:53:55,590 --> 00:53:58,950
Speaker 1:  This has that's the limit of the first amendment. So all this other stuff

892
00:53:58,950 --> 00:54:02,040
Speaker 1:  is not like Twitter would just have to let it go. If they're going to be

893
00:54:02,040 --> 00:54:03,480
Speaker 1:  restricted by the first amendment

894
00:54:03,480 --> 00:54:06,690
Speaker 6:  Also clear and present danger is a great Harrison Ford film,

895
00:54:06,690 --> 00:54:07,590
Speaker 5:  Wonderful movie,

896
00:54:07,590 --> 00:54:11,250
Speaker 6:  But also the U S of this is fascinating

897
00:54:11,250 --> 00:54:15,060
Speaker 6:  because like most actual American based social media companies, the

898
00:54:15,060 --> 00:54:18,670
Speaker 6:  vast majority of Twitter's users are not in the United States.

899
00:54:18,670 --> 00:54:22,650
Speaker 6:  And the promise of Twitter is that someone in one country can

900
00:54:22,650 --> 00:54:26,340
Speaker 6:  talk and tweet to someone in another. So either you're going to

901
00:54:26,340 --> 00:54:29,550
Speaker 6:  geo lock Twitter and it's ad tech stack, and it's

902
00:54:29,550 --> 00:54:33,510
Speaker 6:  everything to every country you operate in, or

903
00:54:33,510 --> 00:54:36,960
Speaker 6:  you have to pick some balance of the U S

904
00:54:36,960 --> 00:54:40,800
Speaker 6:  constitution, what the EU wants, what India wants, which is

905
00:54:40,800 --> 00:54:44,430
Speaker 6:  increasingly less democratic. And China's just like,

906
00:54:44,430 --> 00:54:48,420
Speaker 6:  no, you can't be here. Right? So like, how do you navigate all that? And

907
00:54:48,420 --> 00:54:51,690
Speaker 6:  if you're going to abide by the laws of the country, you operate

908
00:54:51,690 --> 00:54:55,560
Speaker 6:  in this, every platform is tied up in this right now because you

909
00:54:55,560 --> 00:54:59,070
Speaker 6:  can't really do it. You can't like, like Facebook

910
00:54:59,070 --> 00:55:02,880
Speaker 6:  has this problem all the time. And I just don't see

911
00:55:02,880 --> 00:55:05,880
Speaker 6:  Ian really wrestling with that in the interview. It wasn't brought up at

912
00:55:05,880 --> 00:55:06,240
Speaker 6:  all.

913
00:55:06,240 --> 00:55:09,090
Speaker 12:  So the funny thing is, Twitter has done that in the Twitter, had to deal

914
00:55:09,090 --> 00:55:12,480
Speaker 12:  with this with hate speech in Germany and France around 2012. And its solution

915
00:55:12,480 --> 00:55:16,470
Speaker 12:  was withholding tweets, which is that you can be a Nazi

916
00:55:16,470 --> 00:55:19,890
Speaker 12:  on Twitter and post Nazi stuff. But in Germany, you won't get to see the

917
00:55:19,890 --> 00:55:23,010
Speaker 12:  Nazi tweets. They will be basically as geo locked.

918
00:55:23,010 --> 00:55:26,490
Speaker 12:  And Elon seems to have given this much less thought than everyone at

919
00:55:26,490 --> 00:55:28,620
Speaker 12:  Twitter,

920
00:55:28,620 --> 00:55:29,610
Speaker 1:  Shocker.

921
00:55:29,610 --> 00:55:33,540
Speaker 5:  I'm just kind of shocked that he announced, like when he said he

922
00:55:33,540 --> 00:55:37,170
Speaker 5:  wanted to buy this company, it was to protect free speech. And then he

923
00:55:37,170 --> 00:55:40,230
Speaker 5:  immediately goes to this Ted talk and says, oh yeah, yeah, yeah. I'll just

924
00:55:40,230 --> 00:55:43,830
Speaker 5:  listen to the, the, the laws of the whatever country and like, okay.

925
00:55:43,830 --> 00:55:45,870
Speaker 5:  But those are not the same thing.

926
00:55:45,870 --> 00:55:49,560
Speaker 1:  Okay. So I sort of understand that argument. I will say, I have a sympathy

927
00:55:49,560 --> 00:55:53,400
Speaker 1:  towards that argument in this specific way, these

928
00:55:53,400 --> 00:55:56,100
Speaker 1:  companies are all pretty big. This is I think, where the fuzziness or the

929
00:55:56,100 --> 00:55:59,970
Speaker 1:  antitrust comes along. So if you're like, there's only four companies

930
00:55:59,970 --> 00:56:03,690
Speaker 1:  that really have it locked in social media, their moderation. I don't know

931
00:56:03,690 --> 00:56:07,650
Speaker 1:  that I agree with all this, but this is my sympathy. Facebook's moderation

932
00:56:07,650 --> 00:56:11,640
Speaker 1:  decisions are opaque to me. There's no process. It doesn't

933
00:56:11,640 --> 00:56:15,210
Speaker 1:  feel fair. Twitter's moderate, whatever it is, it would be

934
00:56:15,210 --> 00:56:18,990
Speaker 1:  better if our elected representatives were voting on speech

935
00:56:18,990 --> 00:56:22,830
Speaker 1:  regulations, right? Like that's, that's where you're going. You're saying,

936
00:56:22,830 --> 00:56:26,410
Speaker 1:  I believe in free speech, I'd rather have the government issue, speech

937
00:56:26,410 --> 00:56:30,280
Speaker 1:  regulations than these companies that I hate and are really big. And I can't

938
00:56:30,280 --> 00:56:34,240
Speaker 1:  switch away from, I would put, I understand that

939
00:56:34,240 --> 00:56:37,300
Speaker 1:  argument. It is also

940
00:56:37,300 --> 00:56:41,090
Speaker 1:  crazy to be like, what I would like is for the United States

941
00:56:41,230 --> 00:56:45,070
Speaker 1:  to throw away the first amendment and start issuing government

942
00:56:45,070 --> 00:56:46,720
Speaker 1:  speech regulations.

943
00:56:46,720 --> 00:56:49,960
Speaker 12:  It's also the very, the U S ness of it comes up again, because there are

944
00:56:49,960 --> 00:56:53,020
Speaker 12:  just, there are a bunch of countries where if you value any kind of free

945
00:56:53,020 --> 00:56:56,470
Speaker 12:  expression, they are not following those rules. Like you just, you cannot

946
00:56:56,470 --> 00:57:00,190
Speaker 12:  possibly say I value free expression. And I believe that we need to operate

947
00:57:00,190 --> 00:57:03,580
Speaker 12:  according to these two, like China's regulations,

948
00:57:03,580 --> 00:57:07,180
Speaker 1:  Right? So this is, I've been trying to describe the mental

949
00:57:07,180 --> 00:57:11,050
Speaker 1:  flip that you need to make if you're based in the United States. So

950
00:57:11,050 --> 00:57:14,830
Speaker 1:  in the United States, your baseline perspective is that you can

951
00:57:14,830 --> 00:57:18,310
Speaker 1:  say anything you want in the first amendment,

952
00:57:18,310 --> 00:57:22,270
Speaker 1:  prohibits the government from doing all that X, Y, and Z, right?

953
00:57:22,270 --> 00:57:26,050
Speaker 1:  So you're like in a permissive structure and all like

954
00:57:26,050 --> 00:57:29,860
Speaker 1:  let's do, swords is illegal and that's child pornography is

955
00:57:29,860 --> 00:57:33,670
Speaker 1:  illegal. Even make a tortured argument that the expansion

956
00:57:33,670 --> 00:57:37,330
Speaker 1:  of copyright law makes some kind of expression illegal. And the government

957
00:57:37,330 --> 00:57:39,970
Speaker 1:  has created a private whatever. That's just me. I'm going to leave that alone.

958
00:57:39,970 --> 00:57:42,850
Speaker 1:  I'm defamation law, right? It's like, there's an explosion in defamation

959
00:57:42,850 --> 00:57:46,270
Speaker 1:  law cases. We're getting more letters than ever for our coverage. Okay. But

960
00:57:46,270 --> 00:57:49,570
Speaker 1:  that's still a permissive structure. There's a small list of things that

961
00:57:49,570 --> 00:57:52,930
Speaker 1:  might reasonably chill your speech. Otherwise, it's a free for all. You go

962
00:57:52,930 --> 00:57:55,690
Speaker 1:  to a company like outset, or you go to a country like outside, like

963
00:57:55,690 --> 00:57:58,840
Speaker 1:  India. It is actually the absolute

964
00:57:58,840 --> 00:58:02,620
Speaker 1:  flip where it's a restricted culture. And there's a smallest of things you

965
00:58:02,620 --> 00:58:06,550
Speaker 1:  are able to say. And that those countries have demanded

966
00:58:06,550 --> 00:58:10,360
Speaker 1:  that social media companies like Twitter and Facebook have offices. There

967
00:58:10,360 --> 00:58:14,290
Speaker 1:  have employees there. The Indian government has sent the police to

968
00:58:14,290 --> 00:58:17,950
Speaker 1:  Twitter's offices because Twitter was labeling tweets

969
00:58:17,950 --> 00:58:21,850
Speaker 1:  from the BJP party as manipulated information. They're

970
00:58:21,850 --> 00:58:25,420
Speaker 1:  calling out lies from the government and the police showed up at their office.

971
00:58:25,420 --> 00:58:28,330
Speaker 1:  That is a restrictive culture. So if you're based in the U S and you're like,

972
00:58:28,330 --> 00:58:31,990
Speaker 1:  it should be the laws of the country. Your entire framework is the

973
00:58:31,990 --> 00:58:35,980
Speaker 1:  permissive framework of speech United States, and almost everywhere else.

974
00:58:35,980 --> 00:58:39,970
Speaker 1:  It is a restrictive culture where there's mostly, mostly, you cannot

975
00:58:39,970 --> 00:58:43,900
Speaker 1:  do things. And I don't know that you want us thought about it. Ilan is

976
00:58:43,900 --> 00:58:47,170
Speaker 1:  not from this country. He grew up in another country with a different attitude

977
00:58:47,170 --> 00:58:50,590
Speaker 1:  towards speech, South Africa. He obviously has huge factories in

978
00:58:50,590 --> 00:58:54,580
Speaker 1:  China, in Germany. It's just unclear why he hasn't thought

979
00:58:54,580 --> 00:58:55,420
Speaker 1:  any of that through.

980
00:58:55,420 --> 00:58:58,930
Speaker 5:  He did think it through. And I think he was saying one thing in his

981
00:58:58,930 --> 00:59:02,650
Speaker 5:  sec, filing to like drum up support

982
00:59:02,650 --> 00:59:06,610
Speaker 5:  from, from his, his base fandom. And then another thing

983
00:59:06,610 --> 00:59:10,240
Speaker 5:  in this Ted talk, like I think he various cognizant of the fact

984
00:59:10,240 --> 00:59:13,810
Speaker 5:  that he can't have it both ways, but he can say I'm a

985
00:59:13,810 --> 00:59:16,780
Speaker 5:  champion of free speech. Cause what's the, like the sec can just be like,

986
00:59:16,780 --> 00:59:20,650
Speaker 5:  no, you're not like they're not going to do that. Like, like we're all

987
00:59:20,650 --> 00:59:23,780
Speaker 5:  sitting there. Like nobody, you D like, that's, doesn't make sense. And he's

988
00:59:23,780 --> 00:59:26,450
Speaker 5:  like, oh yeah, yeah, I know. But I'm a champion of free speech. Like, that's

989
00:59:26,450 --> 00:59:29,900
Speaker 5:  just always is kind of like modus operandi, right? Like he's always kind

990
00:59:29,900 --> 00:59:30,500
Speaker 5:  of like,

991
00:59:30,500 --> 00:59:33,800
Speaker 12:  Was always, also did this, like Dick Costolo was in

992
00:59:33,800 --> 00:59:37,310
Speaker 12:  2012 or 2011 was late. Twitter's were the free speech wing of the free speech

993
00:59:37,310 --> 00:59:40,340
Speaker 12:  party. And at the same time, yes. We also know that we have to follow the

994
00:59:40,340 --> 00:59:43,820
Speaker 12:  laws of these countries and operate because we're a company. So it's totally

995
00:59:43,820 --> 00:59:47,510
Speaker 12:  not a unique thing. But the interview was just, and I'm to be clear,

996
00:59:47,510 --> 00:59:51,080
Speaker 12:  blaming Chris Anderson for this, it was an atrocious interview.

997
00:59:51,080 --> 00:59:55,070
Speaker 1:  Yeah. I mean, it was extra, just embarrassing. So let's end here, which I

998
00:59:55,070 --> 00:59:58,040
Speaker 1:  think is the hardest problem in the first amendment debate. And this was

999
00:59:58,040 --> 01:00:01,490
Speaker 1:  handled very poorly. So we were talking about hate speech earlier. I will

1000
01:00:01,490 --> 01:00:05,456
Speaker 1:  just tell everyone hate speech is not illegal in the United States of

1001
01:00:05,456 --> 01:00:07,910
Speaker 1:  America. Various states have tried to pass these laws. The Supreme court

1002
01:00:07,910 --> 01:00:11,810
Speaker 1:  has mostly struck them down. You can just do some hate speech. People think

1003
01:00:11,810 --> 01:00:15,740
Speaker 1:  it's illegal because it feels right for it to be illegal because the social

1004
01:00:15,740 --> 01:00:19,490
Speaker 1:  platforms themselves have decided they don't want hate speech on their

1005
01:00:19,490 --> 01:00:23,480
Speaker 1:  platforms. You know, it's really hard to do. If you hate speech on your platform,

1006
01:00:23,480 --> 01:00:27,140
Speaker 1:  sell advertising. So they have like a business imperative, they have a

1007
01:00:27,140 --> 01:00:30,560
Speaker 1:  consumer imperative. So the social platforms are decided, this is unacceptable.

1008
01:00:30,560 --> 01:00:33,920
Speaker 1:  You know, norms in society have decided that being outright

1009
01:00:33,920 --> 01:00:37,800
Speaker 1:  racist is unacceptable, but there's no log in hate speech. United States.

1010
01:00:37,850 --> 01:00:41,600
Speaker 1:  There's just a bunch of social conditioning, then Ilan going on and on

1011
01:00:41,600 --> 01:00:45,320
Speaker 1:  about spam and spam bots, commercial speeches,

1012
01:00:45,320 --> 01:00:48,680
Speaker 1:  protected speech. So if you think about your physical mailbox, where the

1013
01:00:48,810 --> 01:00:52,340
Speaker 1:  United States postal service delivers your mail, the government cannot

1014
01:00:52,340 --> 01:00:54,860
Speaker 1:  proactively filter out junk mail.

1015
01:00:54,860 --> 01:00:56,390
Speaker 5:  It should,

1016
01:00:56,390 --> 01:01:00,320
Speaker 1:  You might want it to, there are laws in the books that enable you to opt

1017
01:01:00,320 --> 01:01:04,190
Speaker 1:  out of it or to be on a, do not mail list or do not call list. But you,

1018
01:01:04,190 --> 01:01:07,610
Speaker 1:  the individual citizen have to like proactively say

1019
01:01:07,610 --> 01:01:11,060
Speaker 1:  it. The government cannot make that decision for you because it's a, it's

1020
01:01:11,060 --> 01:01:14,300
Speaker 1:  a prior strand speech, right? This is crazy. This also applies to

1021
01:01:14,300 --> 01:01:18,170
Speaker 1:  email. There's a law called the canned spam act. You can see if it was

1022
01:01:18,170 --> 01:01:21,200
Speaker 1:  effective, but the government basically is like commercial emailers have

1023
01:01:21,200 --> 01:01:24,350
Speaker 1:  to provide these tags. They have to give you an opt out. They have to give

1024
01:01:24,350 --> 01:01:28,340
Speaker 1:  you a button that says unsubscribe me, but they can't block it. And in

1025
01:01:28,340 --> 01:01:31,160
Speaker 1:  fact, the thing that has walked it most effectively, email monopoly. This

1026
01:01:31,160 --> 01:01:33,380
Speaker 1:  is what Sarah, John will tell you that like Gmail is and email monopoly.

1027
01:01:33,380 --> 01:01:36,410
Speaker 1:  And that has been more effective at spam than any government

1028
01:01:36,410 --> 01:01:39,860
Speaker 1:  regulation. Take that for what you will, but that's because the first amendment

1029
01:01:39,860 --> 01:01:43,400
Speaker 1:  prohibits the government from doing it. So then Elon is like, the top priority

1030
01:01:43,400 --> 01:01:46,460
Speaker 1:  I have is limiting spammers and scam bots and bot armies on Twitter. They

1031
01:01:46,460 --> 01:01:49,400
Speaker 1:  make the product worse. I agree with him. But if you're limiting yourself

1032
01:01:49,400 --> 01:01:53,360
Speaker 1:  to I'm following the law of the country, you're kind of immediately in a

1033
01:01:53,360 --> 01:01:55,280
Speaker 1:  pretzel of logic, right? Addy.

1034
01:01:55,280 --> 01:01:58,310
Speaker 12:  Yes. And it's, again, just one of those things where it feels like there

1035
01:01:58,310 --> 01:02:02,030
Speaker 12:  is a specific thing that he just doesn't want the site to

1036
01:02:02,030 --> 01:02:05,960
Speaker 12:  do. And then he's trying to build this consistent framework around

1037
01:02:05,960 --> 01:02:09,860
Speaker 12:  it. And it's, that's very difficult to do when it seems like what he

1038
01:02:09,860 --> 01:02:13,040
Speaker 12:  has is just sort of a hobby horse that he doesn't want Twitter to do a thing

1039
01:02:13,040 --> 01:02:16,730
Speaker 12:  that feels like it's political censorship, but

1040
01:02:16,730 --> 01:02:20,700
Speaker 12:  hasn't, can't just like come out and say things that he's actually upset

1041
01:02:20,700 --> 01:02:24,570
Speaker 12:  that they're doing. And so it makes up this very broad, like it has to follow

1042
01:02:24,570 --> 01:02:25,980
Speaker 12:  the law kind of framework

1043
01:02:25,980 --> 01:02:29,820
Speaker 5:  Wouldn't getting rid of all the bots also affect him

1044
01:02:29,820 --> 01:02:33,690
Speaker 5:  because of his army of bots. Yeah. Like that seems bad for him.

1045
01:02:33,690 --> 01:02:37,560
Speaker 12:  I mean, bod is also a weird term. Like I assume that like, there were a

1046
01:02:37,560 --> 01:02:40,560
Speaker 12:  bunch of art bots on Twitter. There are a bunch of really fantastic things.

1047
01:02:40,560 --> 01:02:44,040
Speaker 12:  I think he's just mad at the ones that pretend to be him and sell crypto.

1048
01:02:44,040 --> 01:02:47,880
Speaker 1:  Right. So w like, how do you make that distinct distinction? Like, here's

1049
01:02:47,880 --> 01:02:51,630
Speaker 1:  like another problem no algorithm can solve. Like, how do you decide

1050
01:02:51,630 --> 01:02:55,380
Speaker 1:  between one of the bots that sneakerheads use to know when there's a

1051
01:02:55,380 --> 01:02:58,590
Speaker 1:  sneaker drop? That's like, obviously sending a lots and lots of tweets to

1052
01:02:58,590 --> 01:03:02,490
Speaker 1:  lots and lots of people at any given moment. And the crypto spam bots are

1053
01:03:02,490 --> 01:03:06,090
Speaker 1:  applied at Ilan everyday. Like maybe a person could do it, but like the

1054
01:03:06,090 --> 01:03:09,990
Speaker 1:  signals you actually get are very low. The verge automatically tweets every

1055
01:03:09,990 --> 01:03:13,830
Speaker 1:  story that's a bot, is that allowed, like these are very fine

1056
01:03:13,830 --> 01:03:16,230
Speaker 1:  distinctions and saying, I'm just going to get rid of spam bots, I think

1057
01:03:16,230 --> 01:03:19,200
Speaker 1:  is like one of the more challenging aspects of Twitter, because some of the

1058
01:03:19,200 --> 01:03:22,770
Speaker 1:  best parts of Twitter are automated or there aren't projects. There's a bot

1059
01:03:22,770 --> 01:03:25,830
Speaker 1:  that every time we change the tagline and image in our masthead, there's

1060
01:03:25,830 --> 01:03:29,310
Speaker 1:  actually two bots. Every time we change those things, it's just tweets what

1061
01:03:29,310 --> 01:03:32,610
Speaker 1:  they are. I love that bot someone just made it for us to be, well, two people

1062
01:03:32,610 --> 01:03:36,120
Speaker 1:  just made them for us. Are they illegal under Elon law? Like, I don't know.

1063
01:03:36,120 --> 01:03:39,090
Speaker 1:  The, you should know the answers to these questions before you propose these

1064
01:03:39,090 --> 01:03:39,660
Speaker 1:  kinds of things.

1065
01:03:39,660 --> 01:03:42,120
Speaker 12:  We're just, I wish just all talk about our favorite Twitter bots. Now I

1066
01:03:42,120 --> 01:03:45,840
Speaker 12:  have a bot that tweets random words on dark souls. You died

1067
01:03:45,840 --> 01:03:47,040
Speaker 12:  screens.

1068
01:03:47,040 --> 01:03:50,940
Speaker 1:  It's pretty good. My favorite bot is called schemer. It just, it just

1069
01:03:50,940 --> 01:03:53,760
Speaker 1:  comes up with color schemes and gives them insane names

1070
01:03:53,760 --> 01:03:57,630
Speaker 6:  Students. Next week, we need you to read chapter three of the free speech

1071
01:03:57,630 --> 01:04:00,990
Speaker 6:  century. Come back with your questions. I feel like

1072
01:04:00,990 --> 01:04:04,920
Speaker 6:  we've, I feel like if we say free speech again, and this podcast that

1073
01:04:04,920 --> 01:04:05,790
Speaker 6:  will combust.

1074
01:04:05,790 --> 01:04:09,090
Speaker 1:  Yeah. We shouldn't take a break. This interview

1075
01:04:09,090 --> 01:04:10,440
Speaker 1:  sucked and everyone should feel

1076
01:04:10,440 --> 01:04:16,470
Speaker 1:  bad.

1077
01:04:16,470 --> 01:04:18,960
Speaker 1:  I mean, it is like directly as I, as I'm saying it

1078
01:04:18,960 --> 01:04:22,920
Speaker 1:  like as a person who interviews people about

1079
01:04:22,920 --> 01:04:26,160
Speaker 1:  their ideas every week, like if your

1080
01:04:26,160 --> 01:04:29,970
Speaker 1:  approach is, I think Twitter's moderation is bad, such that it is a threat

1081
01:04:29,970 --> 01:04:33,300
Speaker 1:  to civilization, which is more or less what he wants that yesterday, right?

1082
01:04:33,300 --> 01:04:36,810
Speaker 1:  This is a civilizational catastrophe editor. You got to really interrogate

1083
01:04:36,810 --> 01:04:40,170
Speaker 1:  that. And you got to really be prepared to answer those questions. And if

1084
01:04:40,170 --> 01:04:43,500
Speaker 1:  you think that the first amendment is the correct set of rules,

1085
01:04:43,500 --> 01:04:47,160
Speaker 1:  like you're going to run into every other country in the world that thinks

1086
01:04:47,160 --> 01:04:50,790
Speaker 1:  differently that you are trying to sell Teslas in. And that's just like a

1087
01:04:50,790 --> 01:04:53,790
Speaker 1:  bad dynamic for you. All right, we're gonna take a break. We're gonna come

1088
01:04:53,790 --> 01:04:57,720
Speaker 1:  back. We're gonna talk about gadgets, Gadgets for your face. We'll be

1089
01:04:57,720 --> 01:04:57,840
Speaker 1:  right

1090
01:04:57,840 --> 01:05:02,490
Speaker 1:  back.

1091
01:05:54,370 --> 01:05:57,370
Speaker 15:  Hi, I'm Zach Beecham and I cover global politics and democracy for

1092
01:05:57,370 --> 01:06:00,820
Speaker 15:  Vox. I'm currently hosting a special four-part series in the Vox

1093
01:06:00,820 --> 01:06:04,600
Speaker 15:  conversations feed called the war in Ukraine explained each

1094
01:06:04,600 --> 01:06:07,540
Speaker 15:  episode. I sit down with an expert to unpack a different dimension of this

1095
01:06:07,540 --> 01:06:11,440
Speaker 15:  generation defining conflict. The series is designed to give you a top

1096
01:06:11,440 --> 01:06:15,430
Speaker 15:  to bottom understanding of the Ukraine war, starting from the very beginning

1097
01:06:15,430 --> 01:06:18,550
Speaker 15:  to what it could mean for the future, not only of Europe, but the entire

1098
01:06:18,550 --> 01:06:22,270
Speaker 15:  world. In part one, I spoke with political scientists and scholar of Russian

1099
01:06:22,270 --> 01:06:26,050
Speaker 15:  politics, Yoshiko Herrera about why Putin invaded Ukraine in the first

1100
01:06:26,050 --> 01:06:29,740
Speaker 15:  place. And part two, I spoke with Tufts university professor and sanctions

1101
01:06:29,740 --> 01:06:33,670
Speaker 15:  expert, Dan Drezner on the unprecedented economic warfare being waged

1102
01:06:33,670 --> 01:06:37,480
Speaker 15:  by Western countries on Russia, in retaliation for the attack. And this

1103
01:06:37,480 --> 01:06:41,140
Speaker 15:  week for part three of speaking with nuclear policy expert, Jeff Lewis about

1104
01:06:41,140 --> 01:06:45,100
Speaker 15:  the risks that this war escalates to a nuclear exchange and what

1105
01:06:45,100 --> 01:06:48,700
Speaker 15:  we can do to prevent them listen and subscribe to Fox conversations

1106
01:06:48,700 --> 01:06:49,810
Speaker 15:  today, to hear the whole

1107
01:06:49,810 --> 01:06:55,450
Speaker 15:  series.

1108
01:06:55,450 --> 01:06:59,320
Speaker 1:  Alright, we are back at a and Mr. Heath are still with us.

1109
01:06:59,320 --> 01:07:03,070
Speaker 1:  Let's talk about gadgets, Alex, in the midst of all this, you published

1110
01:07:03,070 --> 01:07:06,820
Speaker 1:  a huge scoop this week. You have the entire Metta AR

1111
01:07:06,820 --> 01:07:10,810
Speaker 1:  roadmap. What the company formerly known as Facebook wants to do for computers

1112
01:07:10,810 --> 01:07:14,650
Speaker 1:  on your face. It seems challenging. It does seem like bet the company

1113
01:07:14,650 --> 01:07:15,490
Speaker 1:  walk us through it.

1114
01:07:15,490 --> 01:07:19,360
Speaker 6:  Yeah. So everyone remembers that pretty wild video that

1115
01:07:19,360 --> 01:07:22,510
Speaker 6:  mark Zuckerberg shot last

1116
01:07:22,510 --> 01:07:26,230
Speaker 6:  fall. October-ish when he announced the rebrand to Metta,

1117
01:07:26,230 --> 01:07:30,010
Speaker 6:  here's a AR you know, chess game on a table, and you're

1118
01:07:30,010 --> 01:07:33,250
Speaker 6:  playing with a hologram, you know, the classic stuff that we've seen in these,

1119
01:07:33,250 --> 01:07:36,730
Speaker 6:  these demos on stage. Yeah. Mark fenced with a

1120
01:07:36,730 --> 01:07:37,900
Speaker 6:  hologram let's do

1121
01:07:37,900 --> 01:07:41,290
Speaker 1:  Shorts. See it all comes back to doing stories.

1122
01:07:41,290 --> 01:07:44,860
Speaker 6:  It's all connected. What is the, what

1123
01:07:44,860 --> 01:07:48,130
Speaker 6:  is free speech in the metaverse? Oh boy.

1124
01:07:48,130 --> 01:07:52,000
Speaker 6:  And the holy grail device. He's called around all this

1125
01:07:52,000 --> 01:07:55,510
Speaker 6:  as AR glasses, which we've talked about a lot on this show. I

1126
01:07:55,510 --> 01:07:59,500
Speaker 6:  imagine most of our listeners know the distinctions between VR and AR.

1127
01:07:59,500 --> 01:08:03,400
Speaker 6:  So I won't explain that right now, but yeah, I set out to answer the question.

1128
01:08:03,400 --> 01:08:07,000
Speaker 6:  How soon are these things actually coming? What are they going to look like?

1129
01:08:07,000 --> 01:08:10,510
Speaker 6:  And what is the long-term commitment to this as a product category, it turns

1130
01:08:10,510 --> 01:08:14,110
Speaker 6:  out it's a pretty, long-term big commitment. So they have three

1131
01:08:14,110 --> 01:08:17,990
Speaker 6:  generations of full AR glasses. They're called Nazare

1132
01:08:17,990 --> 01:08:21,830
Speaker 6:  it's sucker Berg's pet project. And he's got thousands of people working

1133
01:08:21,830 --> 01:08:25,190
Speaker 6:  on them, spending billions of dollars. And the first version is coming in

1134
01:08:25,190 --> 01:08:28,980
Speaker 6:  2024, it's going to be an early adopter developer gear

1135
01:08:28,980 --> 01:08:32,750
Speaker 6:  device. They're going to sell like low tens of thousands of them. The bill

1136
01:08:32,750 --> 01:08:36,680
Speaker 6:  of materials on these classes is in the thousands of dollars. So

1137
01:08:36,680 --> 01:08:40,310
Speaker 6:  they have to figure out how to subsidize that, which is going to be challenging.

1138
01:08:40,310 --> 01:08:44,210
Speaker 6:  The battery life is like four hours. So they're mostly going to be used

1139
01:08:44,210 --> 01:08:48,170
Speaker 6:  indoors, but a pretty impressive field of you. Some

1140
01:08:48,170 --> 01:08:51,230
Speaker 6:  pretty impressive specs, honestly, custom silicone custom wave

1141
01:08:51,230 --> 01:08:55,010
Speaker 6:  guides, the best of the best and you know, no

1142
01:08:55,010 --> 01:08:58,430
Speaker 6:  expense spared. And that's what we're getting from the company. Literally

1143
01:08:58,430 --> 01:09:02,180
Speaker 6:  investing the most with like a founder led just

1144
01:09:02,180 --> 01:09:06,050
Speaker 6:  insane fervor behind it. That's what's coming

1145
01:09:06,050 --> 01:09:09,050
Speaker 6:  with that. And then they have another version two years later, and another

1146
01:09:09,050 --> 01:09:12,800
Speaker 6:  version two years later, and there's another device coming in 2024,

1147
01:09:12,800 --> 01:09:16,790
Speaker 6:  another pair of glasses that has like a smaller heads up display.

1148
01:09:16,790 --> 01:09:20,570
Speaker 6:  That's more Google glass. Like that's more kind of notifications.

1149
01:09:20,570 --> 01:09:24,050
Speaker 6:  And the distinction that one's called hyper Nova, they all have these, like

1150
01:09:24,050 --> 01:09:28,010
Speaker 6:  the main air glasses are a Ryan, the smart glasses with the little

1151
01:09:28,010 --> 01:09:31,970
Speaker 6:  display or hyper Nova. And the distinction is

1152
01:09:31,970 --> 01:09:35,840
Speaker 6:  that hyper Nova will be, you know, still needing a phone

1153
01:09:35,840 --> 01:09:39,740
Speaker 6:  to work. So, but it's the idea is to lessen the dependence

1154
01:09:39,740 --> 01:09:43,520
Speaker 6:  on the phone and the main air glasses do not need a phone to

1155
01:09:43,520 --> 01:09:47,240
Speaker 6:  work. They'll have a little phone like device. It's not a phone

1156
01:09:47,240 --> 01:09:51,110
Speaker 6:  though that will offload wireless li some of the compute.

1157
01:09:51,110 --> 01:09:54,920
Speaker 6:  So it doesn't literally burn your face, which we've

1158
01:09:54,920 --> 01:09:58,850
Speaker 6:  also talked about on the ship that's what's coming and what Mehta has

1159
01:09:58,850 --> 01:10:01,910
Speaker 6:  up its sleeve that I think a lot of people will be surprised by when this

1160
01:10:01,910 --> 01:10:05,810
Speaker 6:  ships is this a neural interface. I guess we

1161
01:10:05,810 --> 01:10:09,500
Speaker 6:  could call it mind reading technology. They don't want to call it that because

1162
01:10:09,500 --> 01:10:10,620
Speaker 6:  it's not reading your mind, but

1163
01:10:10,620 --> 01:10:12,440
Speaker 12:  It's kind of

1164
01:10:12,440 --> 01:10:15,710
Speaker 6:  Reading. I know, I know, I know Addie will explain this cause she's actually

1165
01:10:15,710 --> 01:10:19,490
Speaker 6:  tried it, but you know, everyone, they bought this company called

1166
01:10:19,490 --> 01:10:23,420
Speaker 6:  control labs in 2019 for like a billion dollars. It was like

1167
01:10:23,420 --> 01:10:27,380
Speaker 6:  12 people pre product launch. One of those classic, like funny

1168
01:10:27,380 --> 01:10:31,080
Speaker 6:  Zuck, multiple acquisitions. And that is kind of the, the

1169
01:10:31,080 --> 01:10:34,460
Speaker 6:  key that unlocks all this from an input perspective, because how do you

1170
01:10:34,460 --> 01:10:38,390
Speaker 6:  control glasses on your face? They obviously don't have a touch

1171
01:10:38,390 --> 01:10:40,460
Speaker 6:  screen. You're not going to have a mouse with you. You're not going to have

1172
01:10:40,460 --> 01:10:44,090
Speaker 6:  a keyboard. You know, what, how do you do that? And so it's this

1173
01:10:44,090 --> 01:10:48,020
Speaker 6:  armband wristband that lets you kind of have a Phantom limb

1174
01:10:48,020 --> 01:10:52,010
Speaker 6:  to type and control them. So that's at a high level. What is coming on

1175
01:10:52,010 --> 01:10:53,540
Speaker 6:  the AR side for Metta

1176
01:10:53,540 --> 01:10:57,440
Speaker 5:  Is the idea here though, that like you can put on this wristband and

1177
01:10:57,440 --> 01:11:00,650
Speaker 5:  not lift your hands up and look like a dill weed while you're manipulating

1178
01:11:00,650 --> 01:11:02,360
Speaker 5:  things that nobody else can see.

1179
01:11:02,360 --> 01:11:03,560
Speaker 6:  How do you want us to explain?

1180
01:11:03,560 --> 01:11:07,250
Speaker 12:  Yes. So they're magic leap and

1181
01:11:07,250 --> 01:11:11,150
Speaker 12:  Hola lens to different extents, but kind of similar. You use optical tracking

1182
01:11:11,150 --> 01:11:15,120
Speaker 12:  there's cameras in front of you. It can kind of pick up your hands

1183
01:11:15,120 --> 01:11:18,930
Speaker 12:  and then you gesture and it's like minority report. And the minority

1184
01:11:18,930 --> 01:11:22,380
Speaker 12:  report interface is both. You look weird cause you're gesturing to stuff.

1185
01:11:22,380 --> 01:11:25,320
Speaker 12:  Nobody else can see. But also it's really tiring because you're lifting

1186
01:11:25,320 --> 01:11:28,470
Speaker 12:  your hands constantly. And you have to be in a situation where you know

1187
01:11:28,470 --> 01:11:32,280
Speaker 12:  that you like it's inside your field of view. You also just it's

1188
01:11:32,280 --> 01:11:35,820
Speaker 12:  tiring because you're moving your actual hands around a bunch and you have

1189
01:11:35,820 --> 01:11:39,330
Speaker 12:  to use these graphical interfaces that rely on you grabbing and

1190
01:11:39,330 --> 01:11:43,290
Speaker 12:  pulling if you want. But the way that this works

1191
01:11:43,290 --> 01:11:46,710
Speaker 12:  is that you put on this wrist band and it reads the

1192
01:11:46,710 --> 01:11:50,700
Speaker 12:  signals that are going down your arm down like

1193
01:11:50,700 --> 01:11:53,790
Speaker 12:  the neurons there that are telling your hands how to move

1194
01:11:53,790 --> 01:11:57,480
Speaker 12:  basically. So it can act like a

1195
01:11:57,480 --> 01:12:00,660
Speaker 12:  hand tracker that just doesn't need a camera because it can kind of detect,

1196
01:12:00,660 --> 01:12:04,020
Speaker 12:  okay, you want to move your index finger, but it can also

1197
01:12:04,020 --> 01:12:07,500
Speaker 12:  detect intention. Like the way that

1198
01:12:07,500 --> 01:12:11,280
Speaker 12:  they tell people to start learning how to do this is that you put the

1199
01:12:11,280 --> 01:12:15,090
Speaker 12:  armband on and you move your finger. Like you make a fist and

1200
01:12:15,090 --> 01:12:18,570
Speaker 12:  your virtual hand reflects this because you're telling your

1201
01:12:18,570 --> 01:12:22,260
Speaker 12:  arm, I want to make a fist. Then you put your hand on something hard and

1202
01:12:22,260 --> 01:12:25,590
Speaker 12:  flat where you can't move it, but you tell your arm to make a

1203
01:12:25,590 --> 01:12:29,520
Speaker 12:  fist. And it does the same thing because you

1204
01:12:29,520 --> 01:12:33,240
Speaker 12:  are sending the instructions to, through your neural

1205
01:12:33,240 --> 01:12:37,200
Speaker 12:  system, through your arm to say, I want to make a fist. And it doesn't

1206
01:12:37,200 --> 01:12:39,360
Speaker 12:  matter that your actual hand isn't doing it.

1207
01:12:39,360 --> 01:12:43,200
Speaker 5:  So this is like the same tech that you're seeing with like a

1208
01:12:43,200 --> 01:12:46,800
Speaker 5:  lot of arm prosthetics, right?

1209
01:12:46,800 --> 01:12:47,220
Speaker 5:  Yeah.

1210
01:12:47,220 --> 01:12:51,210
Speaker 12:  It's really, it's a thing that does a lot of the same work as

1211
01:12:51,210 --> 01:12:54,870
Speaker 12:  brain interfaces like Neuralink. Okay. I shouldn't say neural link. Cause

1212
01:12:54,870 --> 01:12:58,380
Speaker 12:  neuro link is like, I don't even know what's going on with it completely,

1213
01:12:58,380 --> 01:13:00,960
Speaker 12:  but there are a bunch of brain interfaces. And the idea is that they're

1214
01:13:00,960 --> 01:13:04,650
Speaker 12:  supposed to mimic the way that your brain sends signals to your

1215
01:13:04,650 --> 01:13:08,460
Speaker 12:  limbs. These are really good. If you don't have a working

1216
01:13:08,460 --> 01:13:12,390
Speaker 12:  limb there at all, there is not really a super

1217
01:13:12,390 --> 01:13:16,380
Speaker 12:  great reason. Why, if you have a functional limb, you

1218
01:13:16,380 --> 01:13:20,310
Speaker 12:  should try to go straight to like Jack into your brain

1219
01:13:20,310 --> 01:13:24,090
Speaker 12:  to do a thing that you could just tell your arm to do. It's like,

1220
01:13:24,090 --> 01:13:27,210
Speaker 12:  I can't remember if I'm stealing this analogy from the

1221
01:13:27,210 --> 01:13:31,020
Speaker 12:  actual, from like control labs, but it's

1222
01:13:31,020 --> 01:13:34,530
Speaker 12:  like saying I want to write a command. And instead of using

1223
01:13:34,530 --> 01:13:38,100
Speaker 12:  JavaScript or something, you learn binary code and go and like write in

1224
01:13:38,100 --> 01:13:39,180
Speaker 12:  binary code

1225
01:13:39,180 --> 01:13:42,720
Speaker 1:  Once I kind of understand why you'd want to do that for an AR interface

1226
01:13:42,720 --> 01:13:46,350
Speaker 1:  though. Right? You're seeing something no one else can see. And you're just

1227
01:13:46,350 --> 01:13:50,160
Speaker 1:  like hit that button and the button gets hit. Like that would rule.

1228
01:13:50,160 --> 01:13:54,150
Speaker 6:  Yeah. So hyper Nova the heads up display glasses that are more minimal and

1229
01:13:54,150 --> 01:13:57,990
Speaker 6:  use a phone. That's the ideas like really fast texting, really

1230
01:13:57,990 --> 01:14:01,920
Speaker 6:  like responding to notifications without needing to talk without needing

1231
01:14:01,920 --> 01:14:05,880
Speaker 6:  to gesture. That's potentially going to be pretty cool. And Addy, like

1232
01:14:05,880 --> 01:14:09,210
Speaker 6:  everyone I've talked to, who's tried the prototype. That's kind of more recent

1233
01:14:09,210 --> 01:14:13,090
Speaker 6:  that Matta has. It's based on this as it's like one of the coolest tech demos

1234
01:14:13,090 --> 01:14:16,840
Speaker 6:  they've ever tried and that it could be like pay for everything. If it

1235
01:14:16,840 --> 01:14:20,710
Speaker 6:  works at scale, what do you think? Cause you, you tried it. How do you,

1236
01:14:20,710 --> 01:14:22,480
Speaker 6:  what do you agree? There are not,

1237
01:14:22,480 --> 01:14:26,380
Speaker 12:  I think literally the only thing I've ever tried tech wise

1238
01:14:26,380 --> 01:14:29,320
Speaker 12:  that felt more cool and magical is the Oculus

1239
01:14:29,320 --> 01:14:33,250
Speaker 12:  rift, which ironically is also a Facebook now.

1240
01:14:33,250 --> 01:14:37,210
Speaker 12:  I, everything I like, no, I think it's incredible because the, yeah, the

1241
01:14:37,210 --> 01:14:40,960
Speaker 12:  thing I'm getting at is that it feels like mind control

1242
01:14:40,960 --> 01:14:44,860
Speaker 12:  to an extent, like it's not putting a thing literally in your brain,

1243
01:14:44,860 --> 01:14:48,640
Speaker 12:  but it's letting you just feel like you're gesturing using your hand, but

1244
01:14:48,640 --> 01:14:51,940
Speaker 12:  your hand is not necessarily moving. And your hand isn't necessarily

1245
01:14:51,940 --> 01:14:55,720
Speaker 12:  using a thing where you have to have a one-to-one I'm pressing this

1246
01:14:55,720 --> 01:14:59,680
Speaker 12:  button. It's like to some extent it can kind of learn

1247
01:14:59,680 --> 01:15:03,250
Speaker 12:  from the way that you are thinking that it

1248
01:15:03,250 --> 01:15:07,210
Speaker 12:  can match up. When I make this brain pattern, I want this thing

1249
01:15:07,210 --> 01:15:10,450
Speaker 12:  to happen. And if you blend that with a bunch of really

1250
01:15:10,450 --> 01:15:14,410
Speaker 12:  advanced sort of predictive artificial intelligence systems, you can end

1251
01:15:14,410 --> 01:15:18,040
Speaker 12:  up getting these really interesting, smart, like control

1252
01:15:18,040 --> 01:15:21,790
Speaker 12:  systems that are super

1253
01:15:21,790 --> 01:15:25,330
Speaker 12:  different from anything that you've tried ever. In theory. Again, the thing

1254
01:15:25,330 --> 01:15:29,200
Speaker 12:  I tried is a very, it was a 2019 demo. It was very early, it was basically

1255
01:15:29,200 --> 01:15:32,830
Speaker 12:  playing pong, but it was super fun.

1256
01:15:32,830 --> 01:15:36,100
Speaker 6:  Yeah. And like, so what's going to happen is med is going to Trojan horse

1257
01:15:36,100 --> 01:15:39,730
Speaker 6:  this into smartwatch that is coming first this year,

1258
01:15:39,730 --> 01:15:43,660
Speaker 6:  without this technology that I reported on last year, it'll have a detachable

1259
01:15:43,660 --> 01:15:47,650
Speaker 6:  display with two cameras, like a little detachable GoPro on your

1260
01:15:47,650 --> 01:15:48,850
Speaker 6:  wrist, which is

1261
01:15:48,850 --> 01:15:50,710
Speaker 5:  Dick Tracy. Yeah.

1262
01:15:50,710 --> 01:15:50,920
Speaker 6:  That's

1263
01:15:50,920 --> 01:15:52,540
Speaker 1:  A classic Metta idea. That's not going

1264
01:15:52,540 --> 01:15:55,630
Speaker 6:  To succeed. So the idea is like, get people use to a smartwatch from Metta,

1265
01:15:55,630 --> 01:15:59,430
Speaker 6:  by the way, like the name change very much like motivated by all this hardware

1266
01:15:59,430 --> 01:16:02,590
Speaker 6:  coming because they know that like the Facebook glasses, people in the U

1267
01:16:02,590 --> 01:16:06,580
Speaker 6:  S are not going to buy. Right? So like the metal watch. And then by

1268
01:16:06,580 --> 01:16:10,420
Speaker 6:  2024, when the glasses come out, they ship one with

1269
01:16:10,420 --> 01:16:14,170
Speaker 6:  control, abs tech and it syncs with the glasses and

1270
01:16:14,170 --> 01:16:18,160
Speaker 6:  apple. I don't like apples scared of this. I've heard because of,

1271
01:16:18,160 --> 01:16:21,130
Speaker 6:  they think it's going to have privacy concerns obviously, and people are

1272
01:16:21,130 --> 01:16:25,060
Speaker 6:  going to think it's weird and too intimate. And so apple is very scared

1273
01:16:25,060 --> 01:16:28,900
Speaker 6:  of this space. Snap. I just scooped bought a similar, it

1274
01:16:28,900 --> 01:16:31,870
Speaker 6:  wasn't doing EMG. It was doing a different version of this that you wear

1275
01:16:31,870 --> 01:16:35,740
Speaker 6:  on your head. But another BCI company called next mind

1276
01:16:35,740 --> 01:16:39,010
Speaker 6:  and they want to hook that up to their spectacles down the road. So everyone's

1277
01:16:39,010 --> 01:16:42,730
Speaker 6:  trying to like with maybe the exception of apple come up with this

1278
01:16:42,730 --> 01:16:46,630
Speaker 6:  brain BCI interface for these classes,

1279
01:16:46,630 --> 01:16:50,020
Speaker 5:  Apple scared of it because these other companies are relying on

1280
01:16:50,020 --> 01:16:53,140
Speaker 5:  like algorithms and stuff. Because I mean, this technology itself is not

1281
01:16:53,140 --> 01:16:57,080
Speaker 5:  new. We're seeing it a lot in the accessibility space. Like

1282
01:16:57,080 --> 01:17:00,610
Speaker 5:  it's used for people who are paraplegic and need to control computers and

1283
01:17:00,610 --> 01:17:04,480
Speaker 5:  stuff. It's used for people with residual limbs. Like it's

1284
01:17:04,480 --> 01:17:07,690
Speaker 5:  not new technology. And I don't think there's privacy concerns about people

1285
01:17:07,690 --> 01:17:10,040
Speaker 5:  being able to pick up a can of soda in real life.

1286
01:17:10,040 --> 01:17:13,820
Speaker 6:  Yeah. Apple may come around to it. They've been hesitant, the apple headset

1287
01:17:13,820 --> 01:17:16,430
Speaker 6:  that's coming that I think they'll probably still announce by the end of

1288
01:17:16,430 --> 01:17:19,940
Speaker 6:  the year, has this ring that they've worked on that has like touch on it.

1289
01:17:19,940 --> 01:17:23,030
Speaker 6:  That's like their input. So people are finding all these ways that they can

1290
01:17:23,030 --> 01:17:26,390
Speaker 6:  try to control these glasses. But it sounds like has crack something with

1291
01:17:26,390 --> 01:17:29,450
Speaker 6:  control labs that maybe will put them ahead. They're on input.

1292
01:17:29,450 --> 01:17:32,870
Speaker 1:  You know, it's funny because you know, apple is sort of like reverse engineered

1293
01:17:32,870 --> 01:17:36,740
Speaker 1:  the Steve jobs magic. If you like, whenever they do a new category, the

1294
01:17:36,740 --> 01:17:40,700
Speaker 1:  watch was like this, the Sterling example of this they're like, what we

1295
01:17:40,700 --> 01:17:44,270
Speaker 1:  invented was a new input device. First. It was the mess. Then it was the

1296
01:17:44,270 --> 01:17:47,770
Speaker 1:  touch screen. And then it was the digital crown. And you're like,

1297
01:17:47,770 --> 01:17:51,320
Speaker 1:  what are you talking about? And so like

1298
01:17:51,320 --> 01:17:55,160
Speaker 1:  Crown for such like apple, like

1299
01:17:55,160 --> 01:17:58,610
Speaker 1:  for a long time, it was all in I'm like what you need to enter a new category

1300
01:17:58,610 --> 01:18:02,540
Speaker 1:  is like a big idea about input devices. So if they

1301
01:18:02,540 --> 01:18:06,320
Speaker 1:  do glasses and they haven't like, they

1302
01:18:06,320 --> 01:18:09,620
Speaker 1:  don't drop some big idea about input devices, they will actually be really

1303
01:18:09,620 --> 01:18:13,010
Speaker 1:  out of form for them. Maybe it maybe it's this ring, but it just strange

1304
01:18:13,010 --> 01:18:16,460
Speaker 1:  that they wouldn't be chasing that as hard as anything.

1305
01:18:16,460 --> 01:18:20,240
Speaker 6:  I mean, the continuum of these devices is that the glasses are coming

1306
01:18:20,240 --> 01:18:23,630
Speaker 6:  later. Metal will probably be first with like something that

1307
01:18:23,630 --> 01:18:27,380
Speaker 6:  is like at least consumer like usable, like

1308
01:18:27,380 --> 01:18:30,920
Speaker 6:  snaps, not even selling their AR spectacles because they're so expensive.

1309
01:18:30,920 --> 01:18:33,110
Speaker 5:  Apples is like a developer kit, right?

1310
01:18:33,110 --> 01:18:35,570
Speaker 6:  No, the apple headset's going to be consumer. I mean, it's going to cost

1311
01:18:35,570 --> 01:18:39,230
Speaker 6:  a lot of money, but the glasses Apple's not close on

1312
01:18:39,230 --> 01:18:42,710
Speaker 6:  really. So like the continuum will be these high-end mixed reality headsets,

1313
01:18:42,710 --> 01:18:46,490
Speaker 6:  that sheet into AR by mixing high resolution

1314
01:18:46,490 --> 01:18:50,090
Speaker 6:  video pass through with AR effects. So that's what Matt is going to do with

1315
01:18:50,090 --> 01:18:53,870
Speaker 6:  Cambridge headset coming out later this year, Apple's going to do.

1316
01:18:53,870 --> 01:18:57,770
Speaker 6:  And then later on that continuum, sooner for Matta is glasses

1317
01:18:57,770 --> 01:19:01,670
Speaker 6:  that are designed to not fully immerse you and

1318
01:19:01,670 --> 01:19:05,210
Speaker 6:  Zach's thesis for this is like presence. It's like, you

1319
01:19:05,210 --> 01:19:09,200
Speaker 6:  know, holodeck like holograms of people

1320
01:19:09,200 --> 01:19:13,160
Speaker 6:  around you, which like it's supposed to be meant in D w mostly

1321
01:19:13,160 --> 01:19:16,880
Speaker 6:  worn indoors, the battery life's like four hours Nila. Does this sound

1322
01:19:16,880 --> 01:19:20,300
Speaker 6:  appealing to you? Would you spend, would you spend like

1323
01:19:20,300 --> 01:19:23,990
Speaker 6:  $1,500, let's say conservatively on something

1324
01:19:23,990 --> 01:19:27,740
Speaker 6:  that, you know, needs a wireless thing in your pocket, an

1325
01:19:27,740 --> 01:19:31,430
Speaker 6:  arm band that reads your mind essentially control with your mind that

1326
01:19:31,430 --> 01:19:32,000
Speaker 6:  you,

1327
01:19:32,000 --> 01:19:35,240
Speaker 1:  It gets me about all that, right? If it's primarily designed for indoors,

1328
01:19:35,240 --> 01:19:38,630
Speaker 1:  like, I don't know why it need a strap, a bunch of extra shit to my body

1329
01:19:38,630 --> 01:19:39,860
Speaker 1:  to make that go

1330
01:19:39,860 --> 01:19:42,320
Speaker 6:  Cause of input. Cause well, cause of computing and I

1331
01:19:42,320 --> 01:19:45,080
Speaker 1:  Don't need the processing on my body. I don't, you know, like maybe the arm

1332
01:19:45,080 --> 01:19:48,500
Speaker 1:  band, but like I don't need a battery pack. Like if you're telling me this

1333
01:19:48,500 --> 01:19:49,880
Speaker 1:  is mostly, so I sit,

1334
01:19:49,880 --> 01:19:51,020
Speaker 6:  It's not battery, it's

1335
01:19:51,020 --> 01:19:54,560
Speaker 1:  Not battery. Okay. Mostly I'm sitting in my chair in my office. And like

1336
01:19:54,560 --> 01:19:57,480
Speaker 1:  you pop up in a hologram. Yes.

1337
01:19:57,480 --> 01:19:57,830
Speaker 6:  Yes.

1338
01:19:57,830 --> 01:20:01,760
Speaker 1:  That's it. That's the whole game. Yeah. I, I dunno, man.

1339
01:20:01,760 --> 01:20:05,570
Speaker 6:  I mean, for V1 V2, maybe like you can take it outdoors with you and has more

1340
01:20:05,570 --> 01:20:09,150
Speaker 6:  battery life. It's going have better optics, lighter design. The first version

1341
01:20:09,150 --> 01:20:12,240
Speaker 6:  is going to weigh a hundred grams, which is about four X, the weight of regular

1342
01:20:12,240 --> 01:20:15,270
Speaker 6:  regular glasses. You're not going to, wouldn't want to wear these for long.

1343
01:20:15,270 --> 01:20:17,370
Speaker 5:  People are going to be fee ears.

1344
01:20:17,370 --> 01:20:21,210
Speaker 6:  Yeah. And what I wanted to suggest with the story is like, this is all very

1345
01:20:21,210 --> 01:20:24,630
Speaker 6:  expensive. He's spending over 10 million a year on this. He's literally bet

1346
01:20:24,630 --> 01:20:28,080
Speaker 6:  the company on this and it's going to take like they're, they're they're

1347
01:20:28,080 --> 01:20:31,590
Speaker 6:  modeling that they may be selling tens of millions of smart glasses

1348
01:20:31,590 --> 01:20:35,250
Speaker 6:  collectively between the Ray-Bans between the cheaper hyper Nova

1349
01:20:35,250 --> 01:20:39,030
Speaker 6:  glasses between the full AR glasses. That's all of them by like 20,

1350
01:20:39,420 --> 01:20:43,230
Speaker 6:  28, like maybe tens of millions. And I'm just like, I don't know how

1351
01:20:43,230 --> 01:20:45,390
Speaker 6:  much leash they have.

1352
01:20:45,390 --> 01:20:49,380
Speaker 5:  I mean, we've, we've talked a lot about AR and in the success of AR and there's

1353
01:20:49,380 --> 01:20:53,340
Speaker 5:  like two big things. One, have they resolved the deal we'd faster.

1354
01:20:53,340 --> 01:20:57,150
Speaker 5:  Will you look like a deal we'd wearing 'em into? Have they resolved the fact

1355
01:20:57,150 --> 01:21:01,110
Speaker 5:  that there was zero killer app? Because the best killer app is a privacy

1356
01:21:01,110 --> 01:21:01,890
Speaker 5:  nightmare?

1357
01:21:01,890 --> 01:21:05,430
Speaker 6:  Well, they haven't been invented yet. Like there's not the pro like the opening

1358
01:21:05,430 --> 01:21:08,940
Speaker 6:  of this story. It's like these glasses that they showed in this video,

1359
01:21:08,940 --> 01:21:12,900
Speaker 6:  like it's not based on any running code. It was all unity prototypes.

1360
01:21:12,900 --> 01:21:16,290
Speaker 6:  There's not a wearable prototype internally of the one yet. It's like a

1361
01:21:16,290 --> 01:21:20,070
Speaker 6:  stationary desk board. And like, they're hoping to have their wearable

1362
01:21:20,070 --> 01:21:23,640
Speaker 6:  prototype internally by the end of the year. Meanwhile, Zuck has literally

1363
01:21:23,640 --> 01:21:27,390
Speaker 6:  bet the company on this concept and they do not have a working wearable

1364
01:21:27,390 --> 01:21:31,380
Speaker 6:  prototype. So like the guts are incredible. I mean, I got like,

1365
01:21:31,380 --> 01:21:35,310
Speaker 6:  there's, there's no company that that's like that. And

1366
01:21:35,310 --> 01:21:38,010
Speaker 6:  we can talk about the motivations. I mean, I put in the story, it's like

1367
01:21:38,010 --> 01:21:41,820
Speaker 6:  Facebook and it's always like apple and Google that are always over them.

1368
01:21:41,820 --> 01:21:45,090
Speaker 6:  And this is their chance, Zack, thanks to gal from under them. And also just

1369
01:21:45,090 --> 01:21:48,750
Speaker 6:  like be seen as innovative and cool again, you know, like, cause Turkish,

1370
01:21:48,750 --> 01:21:52,170
Speaker 6:  the traditional social media, stuff's not anymore. Then they can't buy.

1371
01:21:52,170 --> 01:21:55,830
Speaker 1:  Look, I am very sympathetic to the idea that Facebook will become or

1372
01:21:55,830 --> 01:21:59,310
Speaker 1:  Metta will become the next great hardware platform that will break

1373
01:21:59,310 --> 01:22:02,970
Speaker 1:  the dominance of the two phone plans. Like that sounds

1374
01:22:02,970 --> 01:22:06,210
Speaker 1:  great. But you're saying the first one is going to connect to your phone

1375
01:22:06,210 --> 01:22:09,720
Speaker 1:  and like apple is going to have a competitive product right next to it. And

1376
01:22:09,720 --> 01:22:12,960
Speaker 1:  like the idea that you're gonna spend $10 million a year to turn into pebble

1377
01:22:12,960 --> 01:22:16,890
Speaker 1:  smartwatches, because apple will not let you send a text message

1378
01:22:16,890 --> 01:22:20,550
Speaker 1:  in response to a notification on their phone is ridiculous.

1379
01:22:20,550 --> 01:22:23,580
Speaker 1:  Like I hear that story and it's like, the first thing you're gonna do is

1380
01:22:23,580 --> 01:22:26,850
Speaker 1:  quickly reply to notifications and it's connected to your phone. Like Apple's

1381
01:22:26,850 --> 01:22:30,390
Speaker 1:  just going to turn off the notification API for everybody and be like, yeah,

1382
01:22:30,390 --> 01:22:33,870
Speaker 1:  the watch can do it in our glasses. Can do it. Everything else is a security

1383
01:22:33,870 --> 01:22:35,160
Speaker 1:  problem. We're very sorry.

1384
01:22:35,160 --> 01:22:39,000
Speaker 6:  I am so fascinated by antitrust. Like potentially making apple have to

1385
01:22:39,000 --> 01:22:42,180
Speaker 6:  open up here, but they're not going to want Metta to be anywhere near their

1386
01:22:42,180 --> 01:22:45,930
Speaker 6:  glasses. And like they're unmet as glasses to be anywhere near their phone.

1387
01:22:45,930 --> 01:22:49,500
Speaker 6:  Right. And so how do you you're right. How do you navigate that?

1388
01:22:49,500 --> 01:22:53,190
Speaker 5:  I mean, this just sounds like a perfect setup for the usual kind of like

1389
01:22:53,190 --> 01:22:56,550
Speaker 5:  apple playbook, Facebook, Medis, excuse me. Will come

1390
01:22:56,550 --> 01:23:00,120
Speaker 5:  in. They'll do this will be like, this is really cool, but not super

1391
01:23:00,120 --> 01:23:04,110
Speaker 5:  viable. They'll do it. What, till 20, 26 or whatever. And then apple will

1392
01:23:04,110 --> 01:23:08,020
Speaker 5:  be like, we figured out AR look at, look at what. Now you

1393
01:23:08,020 --> 01:23:11,860
Speaker 5:  can use your mind to control things from your apple, watch on your

1394
01:23:11,860 --> 01:23:15,670
Speaker 5:  new glasses that we sold you. Isn't it great. Like they're just going to

1395
01:23:15,670 --> 01:23:19,030
Speaker 5:  let Facebook, excuse me, Mehta, figure out all of the

1396
01:23:19,030 --> 01:23:22,000
Speaker 5:  like hard parts. And then

1397
01:23:22,000 --> 01:23:25,990
Speaker 6:  It's classic apple. It's classic apple. They love to skip the messy middle

1398
01:23:25,990 --> 01:23:29,920
Speaker 6:  of new computing. Right. And like quest has been that up until

1399
01:23:29,920 --> 01:23:33,760
Speaker 6:  quest two and they finally, there were more quest, two sold last year, the

1400
01:23:33,760 --> 01:23:37,750
Speaker 6:  next boxes. So like that thing is hitting meaningful scale and the next version

1401
01:23:37,750 --> 01:23:41,170
Speaker 6:  will have eye tracking, which will make the avatars like super cool and immersive.

1402
01:23:41,170 --> 01:23:45,070
Speaker 6:  So it's only going to get like dramatically better. Apple's watched all that.

1403
01:23:45,070 --> 01:23:48,310
Speaker 6:  They'd been secretly buying things. They'd been building experimenting

1404
01:23:48,310 --> 01:23:51,670
Speaker 6:  prototypes. They're going to come out with this super high-end luxury. Like

1405
01:23:51,670 --> 01:23:55,420
Speaker 6:  state-of-the-art with all their M two chips, sensors, LIDAR and

1406
01:23:55,420 --> 01:23:59,110
Speaker 6:  skip a year basically of where Mehta has been tech, not like from the

1407
01:23:59,110 --> 01:24:02,920
Speaker 6:  technology standpoint and have like not dealt with all that messy

1408
01:24:02,920 --> 01:24:05,590
Speaker 6:  stuff that got up to now. And I think they're going to do the same thing

1409
01:24:05,590 --> 01:24:09,490
Speaker 6:  with glasses. I think mark wants to be out there with them to show that

1410
01:24:09,490 --> 01:24:12,430
Speaker 6:  they're the leader, but Apple's going to wait and probably come out. And

1411
01:24:12,430 --> 01:24:16,330
Speaker 6:  like, I guess my thesis here is that if Meta's is coming out with their first

1412
01:24:16,330 --> 01:24:20,320
Speaker 6:  classes by 2024, I wouldn't expect Apple's until at least 2026.

1413
01:24:20,320 --> 01:24:23,290
Speaker 12:  It's bizarre that we have not mentioned Google in this because Google is

1414
01:24:23,290 --> 01:24:26,230
Speaker 12:  the only one of these companies that is actually currently selling a

1415
01:24:26,230 --> 01:24:30,190
Speaker 12:  commercialized AR headset. They also own the

1416
01:24:30,190 --> 01:24:34,180
Speaker 12:  company that made the predecessor to control labs project, which did not

1417
01:24:34,180 --> 01:24:38,170
Speaker 12:  work very well, but was the same idea was a commercial product like

1418
01:24:38,170 --> 01:24:42,070
Speaker 12:  five years before control labs existed. And we are

1419
01:24:42,070 --> 01:24:45,010
Speaker 12:  totally not talking about them because it seems like it's not even necessarily

1420
01:24:45,010 --> 01:24:46,240
Speaker 12:  clear they're doing anything.

1421
01:24:46,240 --> 01:24:49,150
Speaker 1:  They probably forgot like sooner if I just forgot that he even has any of

1422
01:24:49,150 --> 01:24:49,850
Speaker 1:  this right.

1423
01:24:49,850 --> 01:24:53,110
Speaker 5:  Of these three companies, Google has the worst track record for

1424
01:24:53,110 --> 01:24:55,990
Speaker 5:  shipping hardware that people actually want to buy.

1425
01:24:55,990 --> 01:24:59,920
Speaker 6:  Well, the problem with Google is focus. It's focus and they have

1426
01:24:59,920 --> 01:25:03,010
Speaker 6:  the resources. They have the money, they have the assets, they have the best

1427
01:25:03,010 --> 01:25:06,490
Speaker 6:  machine learning. They have maps with like, which is such a key asset for

1428
01:25:06,490 --> 01:25:09,640
Speaker 6:  glasses. They have like, they literally have the world mapped, which is like

1429
01:25:09,640 --> 01:25:13,600
Speaker 6:  something that it has to figure out. And that assistant, they have the most

1430
01:25:13,600 --> 01:25:17,260
Speaker 6:  impressive visual assistant that can like just see something and

1431
01:25:17,260 --> 01:25:20,620
Speaker 6:  identify it. It's incredible with classes, right? Like search on your

1432
01:25:20,620 --> 01:25:24,250
Speaker 6:  face. They have that. The problem is, is that they don't have like a

1433
01:25:24,250 --> 01:25:28,000
Speaker 6:  founder like mark, who is like pushing them to like

1434
01:25:28,000 --> 01:25:31,570
Speaker 6:  focus everything on this. And they have a million things going on. They have

1435
01:25:31,570 --> 01:25:34,630
Speaker 6:  all this like regulatory scrutiny. They have all these big business lines,

1436
01:25:34,630 --> 01:25:38,100
Speaker 6:  all this optionality. And I, you know, I did a story for us about they're

1437
01:25:38,100 --> 01:25:41,560
Speaker 6:  doing this. High-end they're also working on a high-end mixed reality headset.

1438
01:25:41,560 --> 01:25:44,320
Speaker 6:  They've Al they have all of the big tech guys have all kind of decided the

1439
01:25:44,320 --> 01:25:47,500
Speaker 6:  high-end mixed reality is like, that's the near term. They have a glasses

1440
01:25:47,500 --> 01:25:51,280
Speaker 6:  project. They bought north, which was like making smart glasses. They're

1441
01:25:51,280 --> 01:25:54,760
Speaker 6:  working on something up in Canada. It'll work with a phone I'm

1442
01:25:54,760 --> 01:25:58,540
Speaker 6:  told. And, but like, I just don't see the passion there from Google

1443
01:25:58,540 --> 01:26:02,230
Speaker 6:  yet. I don't see the focus. And I see where talent goes and all the

1444
01:26:02,230 --> 01:26:05,570
Speaker 6:  talent, either going to Metta or apple right now and back and

1445
01:26:05,570 --> 01:26:06,560
Speaker 6:  forth.

1446
01:26:06,560 --> 01:26:09,830
Speaker 1:  The thing that you mentioned, the founder thing get mark is a founder of

1447
01:26:09,830 --> 01:26:13,310
Speaker 1:  Facebook. I think Tim cook is very focused on his legacy at apple. And he

1448
01:26:13,310 --> 01:26:17,150
Speaker 1:  has been saying for years that health and AR will be his twin

1449
01:26:17,150 --> 01:26:19,640
Speaker 1:  legacies. And you can see how he's going to combine all of that.

1450
01:26:19,640 --> 01:26:23,600
Speaker 6:  Yep. I repeat this all the time, but AR is literally the only

1451
01:26:23,600 --> 01:26:27,590
Speaker 6:  thing that apple and Mehta agree on. Like the only thing that they agree

1452
01:26:27,590 --> 01:26:30,980
Speaker 6:  that is going to be big. So, I mean, we'll

1453
01:26:30,980 --> 01:26:32,090
Speaker 6:  see.

1454
01:26:32,090 --> 01:26:34,850
Speaker 1:  Well, let's talk about the actual Metta glasses for a minute. Cause Addie

1455
01:26:34,850 --> 01:26:38,240
Speaker 1:  you've tried on virtually every one of these products.

1456
01:26:38,240 --> 01:26:41,730
Speaker 12:  I'm not tried on there. I've not tried on snaps thing, but I try

1457
01:26:41,730 --> 01:26:42,680
Speaker 12:  to.

1458
01:26:42,680 --> 01:26:46,130
Speaker 1:  Yeah, but so like between the two of you, there isn't a face computer in

1459
01:26:46,130 --> 01:26:49,670
Speaker 1:  the world that we haven't, we haven't strapped on here. So these

1460
01:26:49,670 --> 01:26:53,630
Speaker 1:  glasses here's Ms. RA the full AR experience. And does doc wants 3d

1461
01:26:53,630 --> 01:26:56,990
Speaker 1:  graphics, large field of view, socially acceptable design, which by the way,

1462
01:26:56,990 --> 01:26:59,570
Speaker 1:  mark soccer, where he's like, I need a socially acceptable design. That's

1463
01:26:59,570 --> 01:27:02,660
Speaker 1:  a pretty broad remit coming out of duck, like

1464
01:27:02,660 --> 01:27:03,380
Speaker 12:  Full

1465
01:27:03,380 --> 01:27:07,280
Speaker 1:  Sunscreen face, like whatever you want socially acceptable, but sign.

1466
01:27:07,280 --> 01:27:11,270
Speaker 1:  But whatever the team is hoping for a 70 degree field of view,

1467
01:27:11,270 --> 01:27:14,840
Speaker 1:  maybe that'll happen. And won't, they look like the Superman glasses when

1468
01:27:14,840 --> 01:27:18,230
Speaker 1:  he was Clark. Kent is what Alex right here. And they are really heavy. Now

1469
01:27:18,230 --> 01:27:22,070
Speaker 1:  we have talked on the show a lot about just like the stack of problems for

1470
01:27:22,070 --> 01:27:25,640
Speaker 1:  AR glasses, right? You've got a display problem. You've got a

1471
01:27:25,640 --> 01:27:29,300
Speaker 1:  camera problem. You've got a processing problem. You've got a battery

1472
01:27:29,300 --> 01:27:32,840
Speaker 1:  problem. Where is this sitting in the current state of the art? Do you think

1473
01:27:32,840 --> 01:27:33,500
Speaker 1:  Alex,

1474
01:27:33,500 --> 01:27:36,470
Speaker 6:  It's pretty advanced. You know, a lot of the people I've talked to for this

1475
01:27:36,470 --> 01:27:40,100
Speaker 6:  story. It was like, this is actually a pretty impressive like tax spec that

1476
01:27:40,100 --> 01:27:44,000
Speaker 6:  is in search of a compelling use case because the software side of this is

1477
01:27:44,000 --> 01:27:47,600
Speaker 6:  still being built. They actually just decided the OSTP route. They want to

1478
01:27:47,600 --> 01:27:50,000
Speaker 6:  take like in December, basically just

1479
01:27:50,000 --> 01:27:50,780
Speaker 1:  Fuchsia, which is

1480
01:27:50,780 --> 01:27:54,200
Speaker 6:  Hilarious. No, no, no. They, so they killed, they killed the future approach.

1481
01:27:54,200 --> 01:27:57,860
Speaker 6:  So they were going to do fuchsia. They were doing a microkernel like this

1482
01:27:57,860 --> 01:27:57,980
Speaker 6:  show,

1483
01:27:57,980 --> 01:28:00,290
Speaker 1:  They were still using Google glasses. They just went to Android.

1484
01:28:00,290 --> 01:28:03,770
Speaker 6:  They're using the open fork of Android. And that's the same thing that powers

1485
01:28:03,770 --> 01:28:06,680
Speaker 6:  the quest. So there'll be able to share like apple libraries and stuff. So

1486
01:28:06,680 --> 01:28:09,590
Speaker 6:  they may be able to bring some of the developer ecosystem over, but they

1487
01:28:09,590 --> 01:28:13,040
Speaker 6:  were wanting to do a microkernel like fully custom

1488
01:28:13,040 --> 01:28:16,880
Speaker 6:  OS. And they decided that won't ship in time for 20, 24, basically.

1489
01:28:16,880 --> 01:28:20,720
Speaker 6:  So like this is like do or die. And like, I just, the software

1490
01:28:20,720 --> 01:28:24,290
Speaker 6:  side of it is the biggest question, mark. They do not have the platform,

1491
01:28:24,290 --> 01:28:28,010
Speaker 6:  DNA developer DNA that apple does. And developers are

1492
01:28:28,010 --> 01:28:31,970
Speaker 6:  very wary of Facebook dating back to Farmville. And

1493
01:28:31,970 --> 01:28:35,330
Speaker 6:  when FameBit pulled the plug on all these developers, Oculus

1494
01:28:35,330 --> 01:28:36,740
Speaker 1:  Is sort of salsa in this problem.

1495
01:28:36,740 --> 01:28:40,310
Speaker 6:  Right. They, they have, they have they've, they've got very meaningful, you

1496
01:28:40,310 --> 01:28:43,250
Speaker 6:  know, they're buying everything that's doing, going really well. But yeah,

1497
01:28:43,250 --> 01:28:47,180
Speaker 6:  they do have developers that are doing well there. But

1498
01:28:47,180 --> 01:28:50,840
Speaker 6:  the software side is the biggest question mark by far. And like, how do you

1499
01:28:50,840 --> 01:28:54,590
Speaker 6:  build a holographic Kodak to use with messenger where it works, where like

1500
01:28:54,590 --> 01:28:58,340
Speaker 6:  on your phone, you can see the hologram, you know, or someone didn't

1501
01:28:58,340 --> 01:29:01,620
Speaker 6:  like, cause the idea is like they've got these social apps that like they

1502
01:29:01,620 --> 01:29:04,920
Speaker 6:  could use with the glasses where like, I don't need the glasses to like call

1503
01:29:04,920 --> 01:29:08,490
Speaker 6:  you on your glasses. Right. And like that's the real unlock. And that's,

1504
01:29:08,490 --> 01:29:11,160
Speaker 6:  that's a hard problem that they're still very early on

1505
01:29:11,160 --> 01:29:15,090
Speaker 1:  Adding the displays here, Alex is reporting their costly custom wave guides

1506
01:29:15,090 --> 01:29:18,990
Speaker 1:  and micro led projectors. I'm still just so

1507
01:29:18,990 --> 01:29:21,930
Speaker 1:  skeptical of all of these display ideas.

1508
01:29:21,930 --> 01:29:25,890
Speaker 12:  That just sounds like what? Every, like everybody has custom wave guides

1509
01:29:25,890 --> 01:29:29,700
Speaker 12:  and everybody okay. Projection tech is like complicated, but it just

1510
01:29:29,700 --> 01:29:31,440
Speaker 12:  depends on, it depends on how good the white cards.

1511
01:29:31,440 --> 01:29:34,530
Speaker 6:  Yeah. I mean, snaps were for the spectacles. They were accustomed, but they

1512
01:29:34,530 --> 01:29:37,260
Speaker 6:  were working with wave optics on them. And then they just bought wave optics

1513
01:29:37,260 --> 01:29:41,100
Speaker 6:  for half a billion dollars when they decided they wanted to own that.

1514
01:29:41,100 --> 01:29:45,030
Speaker 6:  But Metta is like, they're, I mean, they're building everything fully

1515
01:29:45,030 --> 01:29:48,030
Speaker 6:  like custom because they've decided they have to go that route, which is

1516
01:29:48,030 --> 01:29:51,690
Speaker 6:  what apple has shown is kind of the best way forward with like wearable.

1517
01:29:51,690 --> 01:29:54,930
Speaker 1:  Right. But I'm saying you're like everyone's in on high-end mixed reality

1518
01:29:54,930 --> 01:29:58,770
Speaker 1:  and right. And I think the reason for that is like

1519
01:29:58,770 --> 01:30:02,730
Speaker 1:  to really do the glasses. You have to invent a radically new kind of display

1520
01:30:02,730 --> 01:30:06,270
Speaker 1:  tech and costly custom way of guides in micro led projectors. Like maybe

1521
01:30:06,270 --> 01:30:09,930
Speaker 1:  they're going to do the best job of that, but that's still as near as I can

1522
01:30:09,930 --> 01:30:13,170
Speaker 1:  say, that's still pretty much in the pocket of where everybody is right now.

1523
01:30:13,170 --> 01:30:16,950
Speaker 12:  So just a really broad term. You could, it's such a weird, messy, new

1524
01:30:16,950 --> 01:30:20,010
Speaker 12:  technology that there could be a really good version of that. That's very

1525
01:30:20,010 --> 01:30:23,910
Speaker 12:  thin and very interesting and has really good optics,

1526
01:30:23,910 --> 01:30:27,750
Speaker 12:  but it doesn't necessarily tell me a bunch about how

1527
01:30:27,750 --> 01:30:31,650
Speaker 12:  good the system currently is. Like magically. I think magic

1528
01:30:31,650 --> 01:30:35,640
Speaker 12:  leap too. At this point, it has some of the best stuff that I've seen and

1529
01:30:35,640 --> 01:30:39,630
Speaker 12:  tried with some limitations. And I don't know how this would stack

1530
01:30:39,630 --> 01:30:42,870
Speaker 12:  up to that. They have at this point, a pretty solid field of view

1531
01:30:42,870 --> 01:30:45,210
Speaker 6:  Magically two 70 field of view. Isn't it?

1532
01:30:45,210 --> 01:30:48,270
Speaker 1:  Magic leap two is 70 diagonal degrees field of view.

1533
01:30:48,270 --> 01:30:51,390
Speaker 12:  Yeah. It's, they've started shipping them to like small partners

1534
01:30:51,390 --> 01:30:55,320
Speaker 5:  Or the meta headset's going to be, cause you've mentioned in your piece that

1535
01:30:55,320 --> 01:30:58,440
Speaker 5:  they're going to be 70 field of you. Is that also diagonal? Are we talking

1536
01:30:58,440 --> 01:30:58,590
Speaker 5:  about

1537
01:30:58,590 --> 01:31:01,380
Speaker 6:  No, that's what the hope that they had. That's the hope that they had, and

1538
01:31:01,380 --> 01:31:04,050
Speaker 6:  I don't think they're going to get there because then they're probably going

1539
01:31:04,050 --> 01:31:07,260
Speaker 6:  for like a color richness and a resolution that's

1540
01:31:07,260 --> 01:31:10,560
Speaker 6:  very, mark wants very crisp, like vivid

1541
01:31:10,560 --> 01:31:14,430
Speaker 6:  holograms, like you to feel like you can you're someone's almost there.

1542
01:31:14,430 --> 01:31:17,790
Speaker 5:  You're looking at through a porthole on a boat

1543
01:31:17,790 --> 01:31:19,470
Speaker 6:  Like these glasses have eye tracking

1544
01:31:19,470 --> 01:31:23,160
Speaker 12:  Chicken leaps holograms at this point are pretty dear pretty

1545
01:31:23,160 --> 01:31:27,150
Speaker 12:  solid, but also in the package that very much does not

1546
01:31:27,150 --> 01:31:31,020
Speaker 12:  look normal. They basically just acknowledged. He had look, there's

1547
01:31:31,020 --> 01:31:34,980
Speaker 12:  going to be weird, big sunglasses. That's what we're doing. This

1548
01:31:34,980 --> 01:31:38,760
Speaker 12:  is what serves our user base. So they have kind of

1549
01:31:38,760 --> 01:31:41,310
Speaker 12:  different prerogatives and different requirements.

1550
01:31:41,310 --> 01:31:44,820
Speaker 6:  And the metal glasses will have eye tracking, which we consumers have not

1551
01:31:44,820 --> 01:31:48,300
Speaker 6:  experienced yet. They will starting later this year with the new quest and

1552
01:31:48,300 --> 01:31:52,020
Speaker 6:  apple. But like that, that changes the game. I did this demo of a

1553
01:31:52,020 --> 01:31:55,740
Speaker 6:  contact lens where it used my eye movement to like do the

1554
01:31:55,740 --> 01:31:59,440
Speaker 6:  input and like go around the dial and select the directions. And like

1555
01:31:59,440 --> 01:32:03,430
Speaker 6:  that stuff is wild. Like eye tracking com is going

1556
01:32:03,430 --> 01:32:06,760
Speaker 6:  to be very cool and it makes your avatar follow your face and your where

1557
01:32:06,760 --> 01:32:09,970
Speaker 6:  you're looking and all that. So they will have that magic leap to does not

1558
01:32:09,970 --> 01:32:13,240
Speaker 6:  have that. So I do think from a spec perspective, these will be the best

1559
01:32:13,240 --> 01:32:15,970
Speaker 6:  when they come out, but they're going to be expensive and they're going to

1560
01:32:15,970 --> 01:32:18,190
Speaker 6:  be for early adopters and they're not going to sell that much.

1561
01:32:18,190 --> 01:32:21,880
Speaker 12:  I'm curious what the early adopter thing is going to is going to do because

1562
01:32:21,880 --> 01:32:25,030
Speaker 12:  like glass tried that. And I think that's kind of one of the reasons why

1563
01:32:25,030 --> 01:32:29,020
Speaker 12:  people hated glass so much because it was seen as this thing that you had

1564
01:32:29,020 --> 01:32:33,010
Speaker 12:  these very small groups of super rich, early

1565
01:32:33,010 --> 01:32:36,250
Speaker 12:  adopter guys trying out, and they looked like jerks and you hated

1566
01:32:36,250 --> 01:32:37,150
Speaker 12:  them

1567
01:32:37,150 --> 01:32:40,390
Speaker 5:  Glass holes, like what are we going to call these? Are these guys also going

1568
01:32:40,390 --> 01:32:42,850
Speaker 5:  to be called glass holes?

1569
01:32:42,850 --> 01:32:46,390
Speaker 1:  Well, the idea is that you won't know like glass was, I mean, this is my

1570
01:32:46,390 --> 01:32:50,230
Speaker 1:  favorite. So to start my class so many times glass looks so intense and it

1571
01:32:50,230 --> 01:32:53,890
Speaker 1:  came with so much hype in that moment of hype that like

1572
01:32:53,890 --> 01:32:57,640
Speaker 1:  I went to the Indy 500 wearing Google glass. And like

1573
01:32:57,640 --> 01:33:00,940
Speaker 1:  people were like falling out of trailers in the infield and be like, can

1574
01:33:00,940 --> 01:33:04,270
Speaker 1:  you see through clothes? And like, that was the thing that would have sold

1575
01:33:04,270 --> 01:33:07,900
Speaker 1:  what Google glass looked like to people like they were heartbroken when I

1576
01:33:07,900 --> 01:33:11,440
Speaker 1:  was like, no, I can not see through clothes at best. It like

1577
01:33:11,440 --> 01:33:15,430
Speaker 1:  lightly buzzes and takes a two megapixel photo,

1578
01:33:15,430 --> 01:33:18,970
Speaker 1:  like reach up to it. Like that's what it does. But I think you have to make

1579
01:33:18,970 --> 01:33:22,000
Speaker 1:  them look really normal, right? That's like, that's where everyone is headed

1580
01:33:22,000 --> 01:33:23,080
Speaker 1:  with these. They have to look

1581
01:33:23,080 --> 01:33:26,290
Speaker 6:  And to be clear, the first version of these there, there's going to be no

1582
01:33:26,290 --> 01:33:29,770
Speaker 6:  mistaking that you're doing have normal classes.

1583
01:33:29,770 --> 01:33:31,960
Speaker 6:  So like,

1584
01:33:31,960 --> 01:33:34,810
Speaker 1:  Well, this is a great story. We're obviously tracking this very closely.

1585
01:33:34,810 --> 01:33:38,770
Speaker 1:  I'm I mean, this is the next generation of gadgets, right? Like

1586
01:33:38,770 --> 01:33:42,490
Speaker 1:  It's like folding phones and this and my, you know, one of my big

1587
01:33:42,490 --> 01:33:46,480
Speaker 1:  theses is that display technology drives the generations of

1588
01:33:46,480 --> 01:33:49,450
Speaker 1:  gadgets. Like if you can predict this way, technology can predict the shape

1589
01:33:49,450 --> 01:33:52,810
Speaker 1:  of things. And that will like folding phones. Is that thing. I imagine Apple's

1590
01:33:52,810 --> 01:33:55,300
Speaker 1:  working in one of those too, right? Like it's just about that time for that

1591
01:33:55,300 --> 01:33:59,170
Speaker 1:  to hit the mainstream and right next to it is can we make the display that

1592
01:33:59,170 --> 01:34:02,920
Speaker 1:  goes into your face? And I'm, I'm very curious to see how this plays out,

1593
01:34:02,920 --> 01:34:06,460
Speaker 6:  Which will happen first Elan, making Twitter a

1594
01:34:06,460 --> 01:34:09,430
Speaker 6:  bastion of free speech or really good displays for our

1595
01:34:09,430 --> 01:34:12,390
Speaker 6:  face

1596
01:34:12,390 --> 01:34:14,080
Speaker 1:  To face displays. I gotta be honest with

1597
01:34:14,080 --> 01:34:15,430
Speaker 5:  You.

1598
01:34:15,430 --> 01:34:19,120
Speaker 1:  All right. We have gone spectacularly over time. I appreciate all of you.

1599
01:34:19,120 --> 01:34:23,080
Speaker 1:  I appreciate all of you for listening to us. You can tweet at

1600
01:34:23,080 --> 01:34:26,650
Speaker 1:  us. I'm very confident that you'll and bots. We tweeting us this

1601
01:34:26,650 --> 01:34:30,300
Speaker 1:  week. Alex is Alex H Kranz. The other Alex is Alex

1602
01:34:30,550 --> 01:34:34,360
Speaker 1:  Heath. It's good. It worked out. Addie is at the Dexter hierarchy. Liz is

1603
01:34:34,360 --> 01:34:38,320
Speaker 1:  Ms. Lopatto and reckless on Twitter is a fun week on

1604
01:34:38,320 --> 01:34:42,130
Speaker 1:  the virtual icon actually Corrine our new senior security reporter had a

1605
01:34:42,130 --> 01:34:46,030
Speaker 1:  big story on why crypto bridges are where all the hacks are. So if

1606
01:34:46,030 --> 01:34:48,460
Speaker 1:  you've been watching all these crypto hacks, like he has a deep dive into

1607
01:34:48,460 --> 01:34:52,060
Speaker 1:  like why it's happening and the structure that creates it. And then on decoder,

1608
01:34:52,060 --> 01:34:55,720
Speaker 1:  we had more crypto Chris Dixon, who is a Forbes, just called them the number

1609
01:34:55,720 --> 01:34:59,540
Speaker 1:  one VC in the world. He's the big web three VC. He and I got into it. And

1610
01:34:59,540 --> 01:35:02,300
Speaker 1:  whether any of that stuff is real, it was a really fun conversation. I'm

1611
01:35:02,300 --> 01:35:05,870
Speaker 1:  just going to tell the virtuous audiences now next week on decoder on

1612
01:35:05,870 --> 01:35:09,440
Speaker 1:  Tuesday, Alan Young, the guy who was in charge of the Fox on

1613
01:35:09,440 --> 01:35:13,250
Speaker 1:  project in Wisconsin asked us to be under coder.

1614
01:35:13,250 --> 01:35:17,120
Speaker 1:  Oh wow. And it was nuts. At one point I was like, what's in the

1615
01:35:17,120 --> 01:35:19,610
Speaker 1:  dome. And he's like, I gotta be honest with you. I think the dome should

1616
01:35:19,610 --> 01:35:22,610
Speaker 1:  have been bigger. And it was like, yes.

1617
01:35:22,610 --> 01:35:23,990
Speaker 17:  When he's right. He's right.

1618
01:35:23,990 --> 01:35:27,590
Speaker 1:  I was like, yeah, I agree with you. I can't, I find no fault in any, if anyone

1619
01:35:27,590 --> 01:35:31,310
Speaker 1:  tells me don't shoot bigger, I'm in it. I agree. It was just a

1620
01:35:31,310 --> 01:35:34,970
Speaker 1:  Bunker's conversation. He has a book. I don't know why he has to be in the

1621
01:35:34,970 --> 01:35:37,640
Speaker 1:  show, but he has a book that's coming out. That's on Tuesday and declutter.

1622
01:35:37,640 --> 01:35:38,600
Speaker 1:  All right, that's

1623
01:35:38,600 --> 01:35:59,210
Speaker 1:  it.

