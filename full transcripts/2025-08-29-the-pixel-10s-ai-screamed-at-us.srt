1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 01897e1b-f19c-472e-884d-a3b40f269640
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-5160715714893428604/5127460524943486535/s93290-US-5897s-1756690257.mp3
Description: The Pixel 10 is in the house, and we’ve been testing them for over a week now. Allison and Vee sit down with Jake to discuss their tests — the good, the bad, and the poorly translated. They demo the Pixel 10's live phone call translations and dive into Pro Res Zoom, which uses AI to enhance photos zoomed in up to 100x. Then, it’s time to talk Dish, Intel, and Elon. Dish is giving up on being a major mobile carrier, Intel is now partially owned by the US government, and Elon has filed a questionable lawsuit against Apple. Finally, we wrap up with a Thunder Round to discuss K-Pop Demon Hunters, YouTube Shorts’ secret “AI,” Android’s registration requirement for developers, Taco Bell’s drive through AI attempt, and a delivery locker on wheels.


2
00:01:56,495 --> 00:02:00,085
Speaker 5:  Hello and Welcome to The Vergecast, the flagship podcast of suing Your Way

3
00:02:00,085 --> 00:02:03,485
Speaker 5:  to the top. I'm Jay Kanakas, executive editor of The Verge.

4
00:02:03,735 --> 00:02:07,645
Speaker 5:  David and Neli will be back leader this year. We've got a lot to

5
00:02:07,645 --> 00:02:11,525
Speaker 5:  talk about today and a really good crew, senior reviewer, the

6
00:02:11,525 --> 00:02:15,165
Speaker 5:  songs with me. Hello and senior reviewer, Alison Johnson.

7
00:02:15,495 --> 00:02:19,325
Speaker 5:  Hello. Lots of talk about Dish continues to be a complete

8
00:02:19,725 --> 00:02:23,405
Speaker 5:  disaster. The US now owns part of Intel, which is a questionable

9
00:02:23,405 --> 00:02:26,765
Speaker 5:  investment. Elon actually sued Apple and

10
00:02:27,025 --> 00:02:30,925
Speaker 5:  OpenAI. I thought that was just a random tweet that was

11
00:02:30,925 --> 00:02:34,685
Speaker 5:  gonna go nowhere. Senior news editor, Richard Lawler is gonna join us

12
00:02:34,695 --> 00:02:38,645
Speaker 5:  later to talk about all that. But first, let's talk phones. A lot of phone

13
00:02:38,645 --> 00:02:41,845
Speaker 5:  news this week, And I think most importantly, the two of you have been testing

14
00:02:42,065 --> 00:02:45,485
Speaker 5:  the Google Pixel. 10 lots of new stuff there. It's got

15
00:02:45,595 --> 00:02:49,365
Speaker 5:  magnets in it, tons of ai. There's Magic Q, which runs

16
00:02:49,425 --> 00:02:53,125
Speaker 5:  on device, makes suggestions about what you're doing. ProRes Zoom,

17
00:02:53,215 --> 00:02:56,845
Speaker 5:  which uses AI to reach a hundred x. Both of you have been testing these.

18
00:02:56,845 --> 00:03:00,685
Speaker 5:  Alison, your review just published yesterday. How is it, how's the,

19
00:03:00,735 --> 00:03:02,085
Speaker 5:  how's The Pixel 10 Pro?

20
00:03:02,815 --> 00:03:06,025
Speaker 6:  Basically good, like I would, I would say

21
00:03:06,845 --> 00:03:10,785
Speaker 6:  you look at all that together, it's pretty good. Like Google's been

22
00:03:10,785 --> 00:03:13,785
Speaker 6:  on a good trajectory with The Pixel phones. I think like

23
00:03:14,455 --> 00:03:18,025
Speaker 6:  they're feeling more and more flagship. You know, they got more expensive,

24
00:03:18,045 --> 00:03:21,905
Speaker 6:  but it feels like the price is justified. So the

25
00:03:22,085 --> 00:03:25,985
Speaker 6:  10 Pro, just to review, you know, we

26
00:03:25,985 --> 00:03:29,665
Speaker 6:  get Tensor G five, Q2 is built in with

27
00:03:29,665 --> 00:03:33,345
Speaker 6:  Magnus. Thank you. V She's doing a little QVC segment over there.

28
00:03:33,685 --> 00:03:36,825
Speaker 5:  Excuse me. You're taking Jimmy's money? I am.

29
00:03:37,685 --> 00:03:41,225
Speaker 7:  I'm showing Jimmy how it's done. See, I'm holding the phone right side up.

30
00:03:41,375 --> 00:03:45,065
Speaker 7:  It's not upside down. Yeah, this is for the YouTube

31
00:03:45,065 --> 00:03:45,825
Speaker 7:  viewers, but good

32
00:03:45,825 --> 00:03:48,985
Speaker 6:  Call. Yeah, yeah, my, my TLDR is

33
00:03:49,765 --> 00:03:53,425
Speaker 6:  the AI is starting to actually make sense,

34
00:03:55,345 --> 00:03:59,265
Speaker 6:  starting to be a little more helpful than just a random smattering of like,

35
00:03:59,655 --> 00:04:03,345
Speaker 6:  it's over in this app, it's in the screenshots. Gemini's gonna bother you

36
00:04:03,345 --> 00:04:06,905
Speaker 6:  every time you open a Google Doc and

37
00:04:07,875 --> 00:04:11,785
Speaker 6:  don't run out and buy it for any of that. But the it, it's a very good

38
00:04:11,785 --> 00:04:12,025
Speaker 6:  phone.

39
00:04:13,055 --> 00:04:16,185
Speaker 5:  Yeah. You said some of that in your review. You were like, it's actually

40
00:04:16,185 --> 00:04:19,505
Speaker 5:  getting helpful this year. And then I was like, well, what It didn't, they

41
00:04:19,505 --> 00:04:22,505
Speaker 5:  have some helpful stuff like in the past And I was like, oh, right. Like

42
00:04:22,505 --> 00:04:26,425
Speaker 5:  it's, it really was like a lot of like weird like gimmicks grab baggy stuff

43
00:04:26,445 --> 00:04:30,025
Speaker 5:  and it's like some of it makes sense and a lot of it, like there's the, the

44
00:04:30,205 --> 00:04:34,185
Speaker 5:  Add Me feature for photos where you can like hand your phone to

45
00:04:34,385 --> 00:04:38,065
Speaker 5:  somebody else and then it'll put you into, And I am like, this is a really

46
00:04:38,065 --> 00:04:41,905
Speaker 5:  lovely demo. And I would love to know if they have

47
00:04:41,935 --> 00:04:44,225
Speaker 5:  data on how many people actually use that.

48
00:04:45,095 --> 00:04:48,945
Speaker 7:  It's like not a situation that happens that often. It's, it's like it happens

49
00:04:49,295 --> 00:04:51,465
Speaker 7:  when you're on vacation, but like,

50
00:04:51,965 --> 00:04:55,825
Speaker 5:  Are you gonna really like take time and be like, Hey, good friend of

51
00:04:55,825 --> 00:04:59,425
Speaker 5:  mine, I know we could just hold our arm out,

52
00:04:59,775 --> 00:05:03,665
Speaker 5:  take a selfie, but there is this specific feature on my phone that you

53
00:05:03,665 --> 00:05:05,865
Speaker 5:  don't know how to use it. I need to learn how to use, but in this moment

54
00:05:05,865 --> 00:05:09,785
Speaker 5:  we can. And like there were, there were demos there. It

55
00:05:09,785 --> 00:05:13,025
Speaker 5:  felt like there were demos And I think like you're kind of right. It seems

56
00:05:13,025 --> 00:05:15,945
Speaker 5:  like this year, maybe Magic Q in particular,

57
00:05:16,695 --> 00:05:20,225
Speaker 5:  there's actually the start of something that you actually would

58
00:05:20,225 --> 00:05:21,065
Speaker 6:  Use. Yeah,

59
00:05:21,795 --> 00:05:25,705
Speaker 7:  Kinda. I think Magic Q worked a little bit better for Allison than it

60
00:05:25,705 --> 00:05:29,225
Speaker 7:  did for me because we would be talking on Slack and they're like, can you

61
00:05:29,225 --> 00:05:32,985
Speaker 7:  send me this message? So it was like faking it a little bit because you just

62
00:05:32,985 --> 00:05:36,945
Speaker 7:  wanna see if it's gonna, because it, it like, I think you have to

63
00:05:37,045 --> 00:05:40,905
Speaker 7:  prompt it in such a way that it will be like Aha information

64
00:05:40,925 --> 00:05:43,825
Speaker 7:  is being asked for, but that's not necessarily how people

65
00:05:44,765 --> 00:05:48,665
Speaker 7:  convey it to you. So like Alison, in your

66
00:05:48,665 --> 00:05:52,105
Speaker 7:  review, and this was real, she slacked me and she's like, can you ask me

67
00:05:52,105 --> 00:05:56,065
Speaker 7:  when my Japanese breakfast concert is? And I did and it worked

68
00:05:56,065 --> 00:05:59,565
Speaker 7:  for her. And then I was like, oh, can you ask me when I'm seeing Heather's?

69
00:06:00,105 --> 00:06:01,925
Speaker 7:  And she did. And it didn't work for me.

70
00:06:02,065 --> 00:06:02,285
Speaker 5:  Aw.

71
00:06:02,745 --> 00:06:04,365
Speaker 7:  So I was like, but it's in my calendar.

72
00:06:05,105 --> 00:06:08,925
Speaker 5:  The thing that I, I think feels limiting right now. And

73
00:06:09,005 --> 00:06:12,645
Speaker 5:  I, And I am curious if they'll ever be, be able to expand. This

74
00:06:12,825 --> 00:06:16,485
Speaker 5:  is, I think Allison, you said it only works in Google's own apps.

75
00:06:16,915 --> 00:06:18,845
Speaker 5:  Like it only has access to Google data.

76
00:06:19,515 --> 00:06:23,285
Speaker 6:  Yeah. And kind of small list of them, like calendar Gmail

77
00:06:23,765 --> 00:06:27,685
Speaker 6:  messages, like messages is kind of the most frequent place I saw

78
00:06:27,685 --> 00:06:31,565
Speaker 6:  it. It can suggest things in Gboard. So you can see

79
00:06:32,585 --> 00:06:35,805
Speaker 6:  one I had just now as I opened Spotify and

80
00:06:36,745 --> 00:06:40,205
Speaker 6:  it came up with a suggestion from The Pixel screenshots

81
00:06:40,625 --> 00:06:44,565
Speaker 6:  app of like some band names I might have been searching for. I was like,

82
00:06:44,565 --> 00:06:48,405
Speaker 6:  that's actually helpful 'cause I'll screenshot something as playing on

83
00:06:48,645 --> 00:06:51,885
Speaker 6:  KXP. And I'm like, I should listen to that later. And I never do anything

84
00:06:52,205 --> 00:06:52,525
Speaker 6:  with it.

85
00:06:54,105 --> 00:06:57,685
Speaker 6:  So that kind of thing. There are, there is a way for them to kind of like

86
00:06:58,915 --> 00:07:02,905
Speaker 6:  creep into other apps, but it's mainly in the main Google apps right

87
00:07:02,905 --> 00:07:03,025
Speaker 6:  now.

88
00:07:03,395 --> 00:07:07,185
Speaker 5:  Which like in fairness, you know, they do contain a lot of your life,

89
00:07:07,785 --> 00:07:11,585
Speaker 5:  But that is, it's still like pretty limiting if it's like,

90
00:07:11,615 --> 00:07:13,025
Speaker 5:  okay, it's calendar events.

91
00:07:13,415 --> 00:07:14,105
Speaker 6:  Yeah. I,

92
00:07:14,415 --> 00:07:18,185
Speaker 7:  It's like I, I think the one that they, they message they mentioned was

93
00:07:18,185 --> 00:07:21,985
Speaker 7:  like, oh, and you can get reminders for your Google Keep app. And I was like,

94
00:07:22,325 --> 00:07:22,545
Speaker 7:  the

95
00:07:22,705 --> 00:07:24,025
Speaker 6:  I don't use Oh, I know. Yeah,

96
00:07:24,125 --> 00:07:28,105
Speaker 7:  Google keep for reminders, but okay, I'll try it.

97
00:07:28,245 --> 00:07:31,985
Speaker 7:  So I was on Tuesday, I was at Shakespeare in the park, which if you're not

98
00:07:32,025 --> 00:07:35,905
Speaker 7:  a native New Yorker, you wait in a really fricking long line at Central

99
00:07:35,935 --> 00:07:39,625
Speaker 7:  Park at, and this year it's a stacked cast. So I had to get to Central Park

100
00:07:39,625 --> 00:07:41,665
Speaker 7:  at 4 45 in the morning. This

101
00:07:41,665 --> 00:07:45,625
Speaker 5:  Is ludicrous. It's ludicrous. You messaged me at like some ungodly hour

102
00:07:45,625 --> 00:07:49,225
Speaker 5:  of the morning And I'm like, what? Sorry, why are you awake this

103
00:07:49,225 --> 00:07:53,105
Speaker 7:  Early? Oh yeah. And I was doing an edit because I was sitting in

104
00:07:53,105 --> 00:07:56,665
Speaker 7:  line at like 4 45 in the morning And I was like, I'll just do a little work

105
00:07:56,685 --> 00:08:00,585
Speaker 7:  now because I'm gonna be here for the next seven hours trying to get a ticket

106
00:08:00,645 --> 00:08:03,825
Speaker 7:  to this, this free ticket to this show. But the whole point was I was there

107
00:08:03,825 --> 00:08:06,705
Speaker 7:  with a couple of friends and one of my friends turns to me and she's like,

108
00:08:06,705 --> 00:08:10,685
Speaker 7:  remind me that I need a sweatshirt tonight because it's gonna be

109
00:08:10,685 --> 00:08:13,885
Speaker 7:  the same temperature and when we're in this outdoor theater, I need a sweatshirt.

110
00:08:13,885 --> 00:08:17,805
Speaker 7:  And I was like, how about I remind you by you telling

111
00:08:17,945 --> 00:08:18,165
Speaker 7:  me

112
00:08:19,825 --> 00:08:23,605
Speaker 7:  in a text message to remind you to bring

113
00:08:23,665 --> 00:08:27,605
Speaker 7:  the sweatshirt and, 'cause I need to test this Magic Q thing. And

114
00:08:27,605 --> 00:08:31,485
Speaker 7:  she gave me this look like, are you fucking for real? And

115
00:08:31,485 --> 00:08:35,285
Speaker 7:  I said, yes, please, please. I, I I just wanna see if this works. And it

116
00:08:35,285 --> 00:08:39,125
Speaker 7:  did. And I got a little Google keep reminder that

117
00:08:39,125 --> 00:08:42,885
Speaker 7:  said, at eight o'clock remind friend to bring a

118
00:08:42,885 --> 00:08:46,845
Speaker 7:  sweatshirt. And I was like, oh, there go. And then at eight o'clock it did

119
00:08:46,845 --> 00:08:48,205
Speaker 7:  not pop up to say no,

120
00:08:48,585 --> 00:08:49,525
Speaker 5:  No. So close.

121
00:08:50,065 --> 00:08:52,845
Speaker 7:  It was so close, but, or at least I didn't notice it. So yeah,

122
00:08:53,745 --> 00:08:56,565
Speaker 6:  The way it feels to me, me is a little bit like,

123
00:08:57,665 --> 00:09:01,285
Speaker 6:  you know, when auto, like one time passwords

124
00:09:01,515 --> 00:09:05,085
Speaker 6:  started auto filling from text messages. It was huge.

125
00:09:05,235 --> 00:09:08,005
Speaker 6:  Yeah, it was great. I was like, this is awesome. I don't have to do anything.

126
00:09:08,025 --> 00:09:11,845
Speaker 6:  And then you, you just kind of forget about it. And that's just how phones

127
00:09:11,915 --> 00:09:15,725
Speaker 6:  work now. It sort of feels like it's gonna be that.

128
00:09:16,595 --> 00:09:19,525
Speaker 6:  Like I, I did have more success with it having

129
00:09:20,735 --> 00:09:21,685
Speaker 6:  stuff popping up,

130
00:09:23,385 --> 00:09:26,765
Speaker 6:  but then I don't, I don't think that's something I was like, well thank God

131
00:09:26,785 --> 00:09:29,845
Speaker 6:  for this. You know, I was just kinda like, oh, this is super nice

132
00:09:30,745 --> 00:09:34,205
Speaker 6:  And I will just get used to this and take it for granted. Probably.

133
00:09:34,925 --> 00:09:38,805
Speaker 5:  I mean, I it whenever like a password doesn't autofill for me. I'm like,

134
00:09:38,805 --> 00:09:42,085
Speaker 5:  this is the end of the world. Yeah. Like, I can't go on like this.

135
00:09:42,085 --> 00:09:43,645
Speaker 6:  You have to throw your phone into the C

136
00:09:43,995 --> 00:09:47,885
Speaker 5:  Yeah, yeah. Something has gone horribly awry. I, I like that as

137
00:09:48,195 --> 00:09:52,005
Speaker 5:  sort of like a, a framework for where useful AI

138
00:09:52,025 --> 00:09:55,325
Speaker 5:  is. Like that's not doing something

139
00:09:55,885 --> 00:09:59,725
Speaker 5:  ostentatious, it's not doing, it's not not changing the world.

140
00:10:00,025 --> 00:10:03,645
Speaker 5:  It is just being like legit. It's helpful in small, very like, precise

141
00:10:03,755 --> 00:10:07,725
Speaker 5:  ways that like require a little bit more contextual awareness. And

142
00:10:07,725 --> 00:10:11,205
Speaker 5:  I think like perhaps traditional programming could have gotten you.

143
00:10:11,675 --> 00:10:15,565
Speaker 6:  Yeah. Like we've had ways to put things on calendars before that

144
00:10:15,565 --> 00:10:19,485
Speaker 6:  didn't involve generative ai. You know, we've been trying

145
00:10:19,485 --> 00:10:22,925
Speaker 6:  to do that in emails and everything like that. This just feels the extra

146
00:10:22,965 --> 00:10:26,725
Speaker 6:  a little bit smarter where it can connect dots a little bit

147
00:10:26,725 --> 00:10:27,045
Speaker 6:  better.

148
00:10:28,425 --> 00:10:32,385
Speaker 7:  I did remember one time Magic Q came up with a conversation with my

149
00:10:32,385 --> 00:10:35,985
Speaker 7:  friend And I was like, oh, and she was mentioning things that we could do

150
00:10:35,985 --> 00:10:39,905
Speaker 7:  for day trips and she mentioned some festival and Rhinebeck And

151
00:10:39,945 --> 00:10:43,065
Speaker 7:  I was like, I don't know where Rhinebeck is. And then it popped up like a

152
00:10:43,335 --> 00:10:47,145
Speaker 7:  Maps magic Q thingy and it's like, Rhinebeck. And I was like, oh,

153
00:10:47,345 --> 00:10:50,945
Speaker 7:  I can see how far Reinbeck is. And I was like, oh, I'm not driving two and

154
00:10:50,945 --> 00:10:52,465
Speaker 7:  a half hours. So,

155
00:10:53,245 --> 00:10:54,065
Speaker 6:  And now that's

156
00:10:54,065 --> 00:10:57,385
Speaker 7:  Out, you know? Yeah. So now I know, but I was like, that was one time where

157
00:10:57,385 --> 00:10:59,865
Speaker 7:  the magic cue, I was like, oh, okay,

158
00:11:00,745 --> 00:11:03,625
Speaker 5:  That's not so bad. YouTube tested a bunch of these sort of

159
00:11:04,965 --> 00:11:08,905
Speaker 5:  AI applied features. Right? What, what were the other interesting ones?

160
00:11:09,335 --> 00:11:13,145
Speaker 6:  Okay, so ve helped me out on the day that she was

161
00:11:13,295 --> 00:11:17,025
Speaker 6:  technically not working, but it sounds like doing kind of a lot of work.

162
00:11:17,745 --> 00:11:21,585
Speaker 7:  Listen, it's a long seven hour wait. Like you gotta figure out stuff

163
00:11:21,585 --> 00:11:22,665
Speaker 7:  to make and make the time pass.

164
00:11:23,345 --> 00:11:26,625
Speaker 6:  I, I was like, can I call you and

165
00:11:27,245 --> 00:11:31,105
Speaker 6:  you can speak in a different language to, to this phone. And

166
00:11:31,105 --> 00:11:33,585
Speaker 6:  she had the, The Pixel 10 as well.

167
00:11:34,885 --> 00:11:37,905
Speaker 6:  So we tried out the voice translate and that one is

168
00:11:39,335 --> 00:11:42,665
Speaker 6:  like, we've seen this from Samsung previously.

169
00:11:43,095 --> 00:11:46,985
Speaker 6:  It's, it just kind of chimes in and speaks

170
00:11:47,005 --> 00:11:50,065
Speaker 6:  the translation live on the phone while you're talking back and forth.

171
00:11:51,405 --> 00:11:55,245
Speaker 6:  So you can make a dinner reservation or, or whatever. This one mimics the

172
00:11:55,245 --> 00:11:59,165
Speaker 6:  speaker's voice both ways, which

173
00:11:59,225 --> 00:12:00,445
Speaker 6:  is a little wild.

174
00:12:02,185 --> 00:12:05,645
Speaker 6:  And we had like a weird time with it.

175
00:12:06,305 --> 00:12:10,285
Speaker 7:  My friend was also there off the side and my friend was just like, her eyes

176
00:12:10,285 --> 00:12:11,125
Speaker 7:  were just like,

177
00:12:11,425 --> 00:12:12,605
Speaker 6:  Yo, what?

178
00:12:13,675 --> 00:12:17,645
Speaker 7:  Because at one point Robot Allison, who

179
00:12:17,645 --> 00:12:21,405
Speaker 7:  was speaking in Japanese to me was just went like, she

180
00:12:21,645 --> 00:12:23,165
Speaker 7:  glitched, she went, eh,

181
00:12:24,865 --> 00:12:25,525
Speaker 6:  It was wild.

182
00:12:26,025 --> 00:12:28,965
Speaker 7:  And we were screaming like it was just like going, what happened? And then,

183
00:12:29,265 --> 00:12:32,765
Speaker 7:  you know, we kept like talking through it and so then we were

184
00:12:32,765 --> 00:12:36,725
Speaker 7:  confusing the robot at one point. And then on her end, robot V

185
00:12:36,725 --> 00:12:37,645
Speaker 7:  went, ah,

186
00:12:38,275 --> 00:12:41,125
Speaker 6:  Yeah, it started like screaming back at me.

187
00:12:42,755 --> 00:12:46,645
Speaker 6:  This doesn't sound very successful. It seems like worse

188
00:12:46,955 --> 00:12:47,445
Speaker 6:  than it was

189
00:12:47,445 --> 00:12:51,205
Speaker 7:  Before. It was kind of half successful. I think like Allison coming through

190
00:12:51,205 --> 00:12:54,885
Speaker 7:  in Japanese was actually pretty good. I was, it was one of the

191
00:12:54,975 --> 00:12:58,565
Speaker 7:  times until the screaming until the screaming, the screaming until we broke

192
00:12:58,565 --> 00:13:01,845
Speaker 7:  it. But I, I actually think I was the one that like broke things because

193
00:13:01,845 --> 00:13:05,525
Speaker 7:  Allison was talking like a normal person and fluent English because that's

194
00:13:05,525 --> 00:13:08,885
Speaker 7:  her native language. And unless, unless you're hiding something from me,

195
00:13:08,885 --> 00:13:12,805
Speaker 7:  Allison, but she's talking in her natural language, she's

196
00:13:12,805 --> 00:13:16,645
Speaker 7:  talking in full sentences that make sense. Me in my Japanese, which is

197
00:13:16,965 --> 00:13:20,805
Speaker 7:  a little rusty, my sentences were a little bit broken. They

198
00:13:20,805 --> 00:13:24,285
Speaker 7:  weren't fully textbook appropriate like, you know, like textbook grammar.

199
00:13:24,725 --> 00:13:28,605
Speaker 7:  'cause when you talk in Japanese it can be very, you can just

200
00:13:28,605 --> 00:13:32,365
Speaker 7:  start leaving out pronouns and people will understand it 'cause it's

201
00:13:32,365 --> 00:13:34,005
Speaker 7:  contextual. And

202
00:13:35,715 --> 00:13:39,095
Speaker 7:  the only way I could explain it was that I was glitching in real time because

203
00:13:39,155 --> 00:13:41,855
Speaker 7:  I'm code switching between languages. So part of me would forget a word And

204
00:13:41,855 --> 00:13:45,215
Speaker 7:  I'd be like, ah, what would I call that? And it's, and it's

205
00:13:45,215 --> 00:13:48,935
Speaker 7:  translating all those little like, oh, what, how do I say that again? And

206
00:13:48,955 --> 00:13:52,095
Speaker 7:  me talking to myself a little bit while I'm talking to Allison and that's

207
00:13:52,095 --> 00:13:55,855
Speaker 7:  what I think caused it to go. That's fascinating. Absolutely not. I broke

208
00:13:55,915 --> 00:13:59,055
Speaker 7:  the machine it and you know, she's asking me about Shakespeare. I'm like,

209
00:13:59,055 --> 00:14:02,615
Speaker 7:  oh, I don't know the Japanese title for 12th night, so

210
00:14:03,035 --> 00:14:07,015
Speaker 7:  I'm just gonna ize it and go 12 night. And it was just

211
00:14:07,015 --> 00:14:10,975
Speaker 7:  like, nope. Not translating that you hoe this is what I imagine

212
00:14:11,115 --> 00:14:14,955
Speaker 7:  Gemini is thinking in its head. But it was very much

213
00:14:14,985 --> 00:14:18,555
Speaker 7:  like, oh that kind of informs when you would use this, right?

214
00:14:18,555 --> 00:14:22,195
Speaker 7:  Because if you're making the reservation, like Allison saying,

215
00:14:22,335 --> 00:14:24,915
Speaker 7:  you're talking in a measured tone,

216
00:14:26,175 --> 00:14:30,115
Speaker 7:  you're talking in full sentences probably. And the person is also doing

217
00:14:30,115 --> 00:14:33,795
Speaker 7:  that and you're talking in your own native languages. But if, you know, I

218
00:14:33,795 --> 00:14:36,355
Speaker 7:  think one of the demos they were saying you could talk to family members

219
00:14:36,935 --> 00:14:40,715
Speaker 7:  who you maybe don't necessarily speak the same language as, and

220
00:14:40,715 --> 00:14:43,395
Speaker 7:  that's a situation in my actual family. And I was like, listen,

221
00:14:44,575 --> 00:14:48,475
Speaker 7:  we speak konglish in my family, in which people are non

222
00:14:48,555 --> 00:14:52,485
Speaker 7:  sequitur, just mashing grammar of two different

223
00:14:52,725 --> 00:14:56,445
Speaker 7:  languages together in a pigeon. And I think that would just confuse

224
00:14:56,445 --> 00:15:00,245
Speaker 7:  just trip up the, the robot. And so the solution

225
00:15:00,245 --> 00:15:03,725
Speaker 7:  would to just speak, be speaking in your native language. But

226
00:15:03,895 --> 00:15:07,685
Speaker 7:  after years and years of just talking to each other in this pigeon, I

227
00:15:07,685 --> 00:15:11,565
Speaker 7:  don't like, that would be really hard to, to just do. 'cause

228
00:15:11,565 --> 00:15:14,925
Speaker 7:  my aunt will be like, ah, did you get

229
00:15:16,195 --> 00:15:19,725
Speaker 7:  call? And it is just like those going back and forth in between the English

230
00:15:19,725 --> 00:15:22,685
Speaker 7:  and plugging vocabulary in different ways. And I was like, oh, that would

231
00:15:22,685 --> 00:15:22,925
Speaker 7:  not work.

232
00:15:23,265 --> 00:15:26,885
Speaker 5:  That's, that's fascinating though. Like, it, it hasn't adapted to the way

233
00:15:26,885 --> 00:15:28,445
Speaker 5:  that a lot of people actually speak

234
00:15:29,085 --> 00:15:32,285
Speaker 7:  Yet. And that's my like, commentary on most translation tech. It's getting

235
00:15:32,365 --> 00:15:36,285
Speaker 7:  a lot better with ai. That's very exciting to me. But just the way that

236
00:15:36,285 --> 00:15:39,725
Speaker 7:  we actually speak is so varied and changes

237
00:15:39,905 --> 00:15:43,645
Speaker 7:  minute to minute. And I mean, you see the creative linguistics the kids are

238
00:15:43,645 --> 00:15:47,565
Speaker 7:  doing with Gibby toilet and all of that. So, you know, we should

239
00:15:47,565 --> 00:15:49,805
Speaker 7:  test that. If it can un if it can translate gibby toilet,

240
00:15:50,265 --> 00:15:52,125
Speaker 6:  Oh, I'm calling you right now. Yeah,

241
00:15:52,405 --> 00:15:54,405
Speaker 5:  That's, it's gonna scream again. It's

242
00:15:54,405 --> 00:15:55,445
Speaker 7:  Gonna go like scream

243
00:15:55,755 --> 00:15:59,405
Speaker 6:  What? And the, I mean it it, it did sound like

244
00:15:59,885 --> 00:16:03,645
Speaker 6:  V at times it was just so weird to hear like an,

245
00:16:03,745 --> 00:16:07,445
Speaker 6:  an imitation of your voice coming through the phone. There were, there were

246
00:16:07,445 --> 00:16:10,525
Speaker 6:  times it sounded like, kind of like you and there were times it would just

247
00:16:10,865 --> 00:16:14,605
Speaker 6:  go in a different direction. I'm like, that that is a different person. That's

248
00:16:14,605 --> 00:16:17,645
Speaker 6:  a different voice. But it it, it tried. The little robot tried.

249
00:16:18,185 --> 00:16:20,765
Speaker 7:  So Allison speaking in Japanese is very deep voiced.

250
00:16:21,175 --> 00:16:21,525
Speaker 6:  She's

251
00:16:21,595 --> 00:16:25,285
Speaker 7:  Just very deep voiced, which is like an interesting choice

252
00:16:25,285 --> 00:16:29,125
Speaker 7:  because like Japanese and Korean and especially those are languages

253
00:16:29,125 --> 00:16:32,685
Speaker 7:  where you tend to speak higher. So like when I speak in those languages,

254
00:16:32,745 --> 00:16:35,725
Speaker 7:  my voice pitch goes up. 'cause that's just like naturally a feature of the

255
00:16:35,885 --> 00:16:36,965
Speaker 7:  language. Everyone's pitch goes up.

256
00:16:37,025 --> 00:16:38,445
Speaker 5:  It still sounded like Allison to you

257
00:16:38,795 --> 00:16:42,605
Speaker 7:  Sometimes. It's like in the beginning. Yes. And

258
00:16:42,685 --> 00:16:44,965
Speaker 7:  then it would just turn into a robot, Japanese lady.

259
00:16:45,145 --> 00:16:48,245
Speaker 5:  The other question for me on these is like, what is the latency? Because

260
00:16:48,245 --> 00:16:51,925
Speaker 5:  it's like, it has to, number one, do the translation. It then has to make

261
00:16:51,925 --> 00:16:55,485
Speaker 5:  it sound like the other person's voice. And if there's any latency, like

262
00:16:55,485 --> 00:16:57,685
Speaker 5:  this just breaks, nobody's gonna sit around for that. It's

263
00:16:57,685 --> 00:17:01,645
Speaker 6:  Sort of intentionally like, let's you talk for a minute and then there's

264
00:17:01,645 --> 00:17:05,365
Speaker 6:  a little like, woo. Like it's, it makes a little noise, like something is

265
00:17:05,365 --> 00:17:09,085
Speaker 6:  happening and then the translator starts talking. Okay. So

266
00:17:09,345 --> 00:17:12,805
Speaker 6:  it, it's kind of nice 'cause you, it gives you a minute to be like, okay,

267
00:17:12,805 --> 00:17:16,125
Speaker 6:  they're talking, you hear them talk for a second. It doesn't, it doesn't

268
00:17:16,125 --> 00:17:20,005
Speaker 6:  feel like a delay. I am annoyed about it's, it just sort of,

269
00:17:20,585 --> 00:17:24,085
Speaker 6:  and and from there it flows pretty well. I didn't think it was

270
00:17:24,315 --> 00:17:26,005
Speaker 6:  like, like super slow or anything.

271
00:17:26,195 --> 00:17:29,485
Speaker 7:  Just like that conversational rhythm that Allison was going into where she's

272
00:17:29,485 --> 00:17:33,155
Speaker 7:  pausing, stopping to think, switching gears mid-sentence

273
00:17:33,305 --> 00:17:36,915
Speaker 7:  that trips it up. It absolutely trips it up because like in the demos when

274
00:17:36,915 --> 00:17:40,235
Speaker 7:  you get that, you're just speaking in full formed thoughts and that's just

275
00:17:40,235 --> 00:17:43,995
Speaker 7:  not how conversation is. Yeah. So I like, this is not necessarily

276
00:17:44,195 --> 00:17:47,795
Speaker 7:  a Google AI problem. This is a, all AI translators in my experience problem

277
00:17:47,795 --> 00:17:51,685
Speaker 7:  where you just be like, oh, you know, I'm talking and then, oh wait

278
00:17:51,685 --> 00:17:55,485
Speaker 7:  a second, I have a different thought. And actually mm, yeah. You know, like,

279
00:17:55,505 --> 00:17:59,445
Speaker 7:  mm. So anyway, and you do that very natural. You understand

280
00:17:59,445 --> 00:18:02,765
Speaker 7:  in, in person the robot just goes, ah,

281
00:18:04,145 --> 00:18:08,085
Speaker 7:  starts, that's actually what the robot did starts screaming on both

282
00:18:08,085 --> 00:18:11,405
Speaker 7:  of our ends. And I was like, oh, okay. Yeah.

283
00:18:11,785 --> 00:18:15,565
Speaker 5:  You both have the, the phones in front of you, right? Yeah. Let's, I, I need

284
00:18:15,565 --> 00:18:17,685
Speaker 5:  to hear this. I need to, let's like, let's give this a try.

285
00:18:18,705 --> 00:18:19,965
Speaker 6:  All right. I'm calling you V

286
00:18:20,355 --> 00:18:21,445
Speaker 7:  Okay, yeah, yeah. The

287
00:18:21,445 --> 00:18:24,045
Speaker 5:  Rest of The Vergecast will be conducted this way. Oh

288
00:18:24,045 --> 00:18:24,845
Speaker 7:  God. This is gonna be

289
00:18:28,035 --> 00:18:31,085
Speaker 5:  Okay. Do you need to like activate a certain mode or just like starts?

290
00:18:31,645 --> 00:18:35,245
Speaker 7:  I, you do need to activate like a certain thing. Okay, here we go.

291
00:18:35,625 --> 00:18:38,725
Speaker 7:  Answer call assist voice translate.

292
00:18:39,565 --> 00:18:42,445
Speaker 6:  V what did you have for breakfast today? What did you have today? Breakfast

293
00:18:42,445 --> 00:18:42,605
Speaker 6:  today?

294
00:18:43,425 --> 00:18:44,405
Speaker 8:  Any do

295
00:19:03,645 --> 00:19:06,175
Speaker 9:  Some mandarin oranges too? I ate it.

296
00:19:18,945 --> 00:19:19,465
Speaker 7:  Japanese people are saying

297
00:19:19,605 --> 00:19:22,185
Speaker 9:  My lines. I'll definitely know. Maybe

298
00:19:26,025 --> 00:19:28,305
Speaker 5:  V whose fault do we think this translation is?

299
00:19:28,775 --> 00:19:29,865
Speaker 7:  Okay. Okay, I'm stopping this

300
00:19:31,405 --> 00:19:31,825
Speaker 5:  No more.

301
00:19:32,015 --> 00:19:35,665
Speaker 7:  Like, so, you know, my Japanese is not perfect, but

302
00:19:35,885 --> 00:19:39,545
Speaker 7:  an actual, what I, what I said basically was I think a Japanese person

303
00:19:39,925 --> 00:19:43,145
Speaker 7:  who was listening to me, even if I'm not using the right exact

304
00:19:43,935 --> 00:19:46,665
Speaker 7:  word, that a native speaker would, they would understand the meaning of what

305
00:19:46,665 --> 00:19:49,825
Speaker 7:  I was saying. They would probably be like, oh, you know, she's, it is like

306
00:19:49,825 --> 00:19:52,585
Speaker 7:  when you talk to a non-native English speaker and they use a funny word,

307
00:19:52,585 --> 00:19:55,225
Speaker 7:  but you get the gist across and you don't really think about it because it's

308
00:19:55,605 --> 00:19:58,785
Speaker 7:  normal. You, you understand it. The AI

309
00:19:58,955 --> 00:20:02,825
Speaker 7:  translated it in a funny little way. It's not wrong.

310
00:20:02,925 --> 00:20:06,705
Speaker 7:  The translation's not wrong. But because Allison is speaking in, in a native

311
00:20:06,765 --> 00:20:10,625
Speaker 7:  way, the Japanese that's coming across is more correct in a certain thing.

312
00:20:10,725 --> 00:20:14,425
Speaker 7:  And so that was something I noticed and was like, oh, it reminded me of this

313
00:20:14,425 --> 00:20:18,265
Speaker 7:  time when I was in college And I went to college in Tokyo and

314
00:20:18,285 --> 00:20:22,065
Speaker 7:  one of the classes that they made us take, if you didn't like test

315
00:20:22,125 --> 00:20:25,825
Speaker 7:  out of it was conversational Japanese. And so the assignment was

316
00:20:26,255 --> 00:20:29,985
Speaker 7:  that you had to actually call Pizza Hut,

317
00:20:30,365 --> 00:20:33,105
Speaker 7:  the Japanese Pizza Hut version and order a pizza.

318
00:20:34,225 --> 00:20:34,445
Speaker 7:  And

319
00:20:36,085 --> 00:20:39,885
Speaker 7:  'cause they were like, you're gonna live here for so many months, so you

320
00:20:39,885 --> 00:20:43,285
Speaker 7:  need to know how to be a functional human. And these are not necessarily

321
00:20:43,285 --> 00:20:46,965
Speaker 7:  things that you learn in a high school Japanese class. So

322
00:20:48,025 --> 00:20:50,765
Speaker 7:  we were doing this and my friend had a meltdown

323
00:20:51,925 --> 00:20:55,765
Speaker 7:  because she didn't realize that how to read a Japanese

324
00:20:56,155 --> 00:20:59,885
Speaker 7:  address and how to give a Japanese address. Oh. So the person on the phone

325
00:20:59,985 --> 00:21:03,685
Speaker 7:  was just like, she was just reading out numbers as you would in

326
00:21:03,925 --> 00:21:07,885
Speaker 7:  a like, you know, like something, something

327
00:21:07,885 --> 00:21:11,405
Speaker 7:  something in Tokyo. And it's like you have to actually say no in between

328
00:21:11,405 --> 00:21:15,085
Speaker 7:  instead of dash. That's how you would say it. And so she lost her mind. They

329
00:21:15,085 --> 00:21:17,405
Speaker 7:  lost their mind. And I, you know, I was thinking back to that And I was like,

330
00:21:17,755 --> 00:21:21,285
Speaker 7:  this would super have like coming

331
00:21:21,885 --> 00:21:25,245
Speaker 7:  crutch at that time she could have just talked to them, ordered a pizza,

332
00:21:25,245 --> 00:21:29,165
Speaker 7:  given them the address, and that would've been written, that would've been

333
00:21:29,165 --> 00:21:32,925
Speaker 7:  like translated in a way that's super helpful but not helpful in the sense

334
00:21:32,925 --> 00:21:36,005
Speaker 7:  that she wouldn't have learned how to actually say it properly in Japanese.

335
00:21:36,355 --> 00:21:39,925
Speaker 7:  Yeah. So in that sense I was like, oh, it's actually also not great

336
00:21:40,425 --> 00:21:44,005
Speaker 7:  if you're trying to get better at the language, but might come in

337
00:21:44,155 --> 00:21:47,685
Speaker 7:  like handy in a pinch when you're just like, I I don't have the brain cells

338
00:21:47,745 --> 00:21:50,965
Speaker 7:  to, to attempt this today. Yeah. And I just want a pizza like

339
00:21:51,475 --> 00:21:52,405
Speaker 7:  fair. So it's interesting.

340
00:21:53,245 --> 00:21:57,045
Speaker 6:  I don't like ordering a pizza on the phone in English. It makes me

341
00:21:57,045 --> 00:21:57,285
Speaker 6:  nervous.

342
00:21:57,945 --> 00:22:00,285
Speaker 7:  It was, it was a, I watch several,

343
00:22:00,435 --> 00:22:03,765
Speaker 5:  They also have AI for that Allison, they, they like,

344
00:22:04,275 --> 00:22:08,165
Speaker 5:  this is all use pointless. Oh no, they're just going to AI everything

345
00:22:08,165 --> 00:22:08,765
Speaker 5:  eventually.

346
00:22:08,795 --> 00:22:09,645
Speaker 6:  Yeah, it's true.

347
00:22:11,305 --> 00:22:15,125
Speaker 5:  The, okay. There is another big AI feature that I really wanna make sure

348
00:22:15,125 --> 00:22:17,805
Speaker 5:  we talk about, which is the ProRes zoom, which

349
00:22:18,905 --> 00:22:22,845
Speaker 5:  on the camera there's a five x telephoto optically and you

350
00:22:22,845 --> 00:22:26,765
Speaker 5:  can zoom in and after 30 x it starts using AI built

351
00:22:26,765 --> 00:22:30,445
Speaker 5:  into the camera app to enhance the photos. And

352
00:22:31,315 --> 00:22:34,245
Speaker 5:  last week I was like, I was like, this looks

353
00:22:34,875 --> 00:22:38,865
Speaker 5:  incredible. This looks, this is like blowing my mind because I'd

354
00:22:38,865 --> 00:22:42,665
Speaker 5:  just seen a few sample images out of the early hands ons

355
00:22:42,965 --> 00:22:45,545
Speaker 5:  and some more stuff has come out since then. You, you've had a chance to

356
00:22:45,545 --> 00:22:47,225
Speaker 5:  test it. And I'm,

357
00:22:47,855 --> 00:22:49,185
Speaker 7:  There's, I would say some,

358
00:22:49,215 --> 00:22:52,625
Speaker 5:  Some flaws are showing in places. I'm curious, Alison, like you actually,

359
00:22:52,645 --> 00:22:56,385
Speaker 5:  you towed it around another giant camera. So like test this

360
00:22:56,575 --> 00:23:00,385
Speaker 5:  against like an actual a hundred x zoom. How, how is it,

361
00:23:00,565 --> 00:23:04,465
Speaker 5:  how do you feel now that you've tried it about having AI in

362
00:23:04,465 --> 00:23:05,905
Speaker 5:  your camera, changing your photos?

363
00:23:06,985 --> 00:23:07,985
Speaker 6:  I feel weird about it.

364
00:23:09,925 --> 00:23:13,585
Speaker 6:  My big picture take is like, it is pretty good if you

365
00:23:13,815 --> 00:23:17,225
Speaker 6:  stay in the 30 to maybe like 50

366
00:23:17,705 --> 00:23:21,465
Speaker 6:  x range. It can, you know, I'll, I'll take a picture of

367
00:23:21,625 --> 00:23:25,425
Speaker 6:  a container ship from a bridge and it knows what to do

368
00:23:25,615 --> 00:23:29,305
Speaker 6:  with the, you know, you get kind of those jagged lines and digital

369
00:23:29,495 --> 00:23:33,465
Speaker 6:  zoom and things just look weird and crunchy. It will clean

370
00:23:33,665 --> 00:23:37,265
Speaker 6:  that up and it looks totally passable.

371
00:23:37,685 --> 00:23:41,385
Speaker 6:  I'm like, all right, this, it doesn't look amazing. It's not like a

372
00:23:41,825 --> 00:23:45,285
Speaker 6:  National Geographic photo of a container ship. Like, it, it looks like a,

373
00:23:45,445 --> 00:23:49,325
Speaker 6:  a decent phone picture of a container ship. But I

374
00:23:49,385 --> 00:23:53,125
Speaker 6:  did carry around at Jake's urging,

375
00:23:53,845 --> 00:23:55,085
Speaker 6:  so this was his fault.

376
00:23:56,405 --> 00:24:00,365
Speaker 5:  I, I'm sorry, I didn't know how much of a burden this would be. I

377
00:24:00,365 --> 00:24:04,285
Speaker 5:  was like, we should get a real hundred X zoom. And turns out that's

378
00:24:04,375 --> 00:24:05,765
Speaker 5:  heavy and large.

379
00:24:06,245 --> 00:24:10,045
Speaker 6:  I know I, I should have known too, but I did the

380
00:24:10,045 --> 00:24:14,005
Speaker 6:  math And I was like, there is exactly one camera that can do this. It

381
00:24:14,005 --> 00:24:17,885
Speaker 6:  is a Nikon cool picks P 1100. It is a

382
00:24:18,365 --> 00:24:19,045
Speaker 6:  gigantic beast.

383
00:24:19,515 --> 00:24:23,365
Speaker 5:  This thing is hilarious there. I like the photos of it that they

384
00:24:23,365 --> 00:24:26,045
Speaker 5:  advertise like, it's like the size of somebody's face.

385
00:24:26,435 --> 00:24:30,205
Speaker 6:  Yeah, it is. And there there's just no like camera

386
00:24:30,425 --> 00:24:33,965
Speaker 6:  bag. I have to put it in. That makes sense. I just had to like

387
00:24:34,385 --> 00:24:38,125
Speaker 6:  lu it around. The first time I, I went out, I was like, I'm gonna take side

388
00:24:38,125 --> 00:24:42,045
Speaker 6:  by side photos. I've got this phone, this camera was trying,

389
00:24:42,205 --> 00:24:46,165
Speaker 6:  I was trying to handhold the camera at 2,400 millimeters.

390
00:24:46,875 --> 00:24:50,685
Speaker 6:  That doesn't work. You don't, like I failed deep review

391
00:24:50,685 --> 00:24:54,645
Speaker 6:  college or whatever. So I went out again with the

392
00:24:54,705 --> 00:24:58,285
Speaker 6:  tripod. My tripod is flimsy and crappy

393
00:24:58,505 --> 00:25:02,405
Speaker 6:  and it, the, the camera, once you get zoomed out far enough,

394
00:25:02,905 --> 00:25:06,245
Speaker 6:  it starts sagging a little bit even though I've got it locked in.

395
00:25:06,745 --> 00:25:09,445
Speaker 6:  So I did not end up with amazing

396
00:25:10,045 --> 00:25:14,005
Speaker 6:  side-by-side photos. I have a side-by-side

397
00:25:14,005 --> 00:25:17,725
Speaker 6:  photo in the review, which is basically a boulder with some

398
00:25:17,725 --> 00:25:18,965
Speaker 6:  trash next to it

399
00:25:19,475 --> 00:25:23,205
Speaker 5:  Because that wasn't a most scenic photo. No,

400
00:25:23,205 --> 00:25:27,125
Speaker 5:  you've like everything else in your review. Lovely. It's like a boulder and

401
00:25:27,125 --> 00:25:27,805
Speaker 5:  like a stick.

402
00:25:28,115 --> 00:25:31,525
Speaker 6:  Yeah, yeah. There's like some garbage just sort of laying around And I was

403
00:25:31,525 --> 00:25:34,845
Speaker 6:  like, well this is what we've got. 'cause it was the only thing I could get

404
00:25:34,945 --> 00:25:36,485
Speaker 6:  framed up reliably

405
00:25:38,305 --> 00:25:42,085
Speaker 6:  and very humbling experience the, the

406
00:25:42,725 --> 00:25:46,605
Speaker 6:  Nikon. And I put the, The Pixel on the tripod too to give it

407
00:25:46,605 --> 00:25:50,485
Speaker 6:  a fair shot. Yeah. The, as you would expect,

408
00:25:50,585 --> 00:25:54,565
Speaker 6:  the optical zoom does better. I was sort of surprised. I thought

409
00:25:54,885 --> 00:25:58,685
Speaker 6:  the The Pixel did okay, like considering everything

410
00:25:58,685 --> 00:25:59,245
Speaker 6:  it was doing.

411
00:25:59,675 --> 00:26:03,365
Speaker 5:  Yeah. The a hundred x like that one clearly looks optical and it's not, it's

412
00:26:03,365 --> 00:26:07,125
Speaker 5:  not a great photo, but it's significantly clearer. The Pixel photo

413
00:26:07,505 --> 00:26:11,285
Speaker 5:  is not bad though, particularly when you compare it to the like pre AI

414
00:26:11,285 --> 00:26:15,005
Speaker 5:  version. And this is the thing that's been fascinating is like,

415
00:26:15,445 --> 00:26:19,125
Speaker 5:  I think there are cer there seems to be certain subjects where this

416
00:26:19,705 --> 00:26:23,285
Speaker 5:  AI enhancement is great And I think if you didn't tell me it was ai,

417
00:26:23,725 --> 00:26:26,365
Speaker 5:  I would be like cool. They're just doing some fancy upscale and looks great.

418
00:26:26,545 --> 00:26:30,405
Speaker 5:  And it's like, you know, particularly for photographs of

419
00:26:30,765 --> 00:26:33,645
Speaker 5:  buildings, architecture in general,

420
00:26:34,675 --> 00:26:38,415
Speaker 5:  it seems to look great. It just like, you can't tell that

421
00:26:39,045 --> 00:26:42,375
Speaker 5:  this has been upscaled in any way. I've done, like I've looked really closely

422
00:26:42,485 --> 00:26:46,285
Speaker 5:  side by side and I'm not seeing any weird AI wackiness. Yeah.

423
00:26:47,305 --> 00:26:50,925
Speaker 5:  The thing that trips it up, as soon as there is a

424
00:26:50,925 --> 00:26:54,645
Speaker 5:  single letter of text, it goes uhoh,

425
00:26:54,825 --> 00:26:58,565
Speaker 5:  I'm ai I have no freaking idea what I'm doing. Yeah. And so

426
00:26:58,565 --> 00:27:02,325
Speaker 5:  there'll be this image that like, so you you, the thing that's

427
00:27:02,325 --> 00:27:05,325
Speaker 5:  wonderful is like The Pixel gives you the before image and the after image

428
00:27:05,325 --> 00:27:09,205
Speaker 5:  so you can see what's happening. And so you'll have this, this image

429
00:27:09,205 --> 00:27:12,965
Speaker 5:  that by, by all account everything else, it's,

430
00:27:13,115 --> 00:27:16,365
Speaker 5:  it's just like perfectly cleaned up, crisper, sharper

431
00:27:17,055 --> 00:27:20,765
Speaker 5:  looks clearer. And then there'll just be these letters that are just like,

432
00:27:20,805 --> 00:27:24,365
Speaker 5:  I don't know what language is. It is just like, it's just total

433
00:27:24,395 --> 00:27:27,885
Speaker 5:  garbage. And I'm like, what happened here? Like

434
00:27:27,955 --> 00:27:31,885
Speaker 7:  It's just weird 'cause it's like, when are you gonna do this? Right? Like,

435
00:27:31,885 --> 00:27:35,685
Speaker 7:  when are you gonna zoom in a hundred times? For what purpose are you zooming

436
00:27:35,685 --> 00:27:39,645
Speaker 7:  in a hundred times on a thing? If it's, I mean I know I did it on the Shakespeare

437
00:27:39,645 --> 00:27:43,445
Speaker 7:  line to beat for chits and giggles 'cause I was just bored. And so I

438
00:27:43,445 --> 00:27:47,165
Speaker 7:  went a hundred, a hundred times zoom on my friend's eye and that was a

439
00:27:47,165 --> 00:27:47,525
Speaker 7:  nightmare.

440
00:27:47,885 --> 00:27:51,445
Speaker 5:  I hope was a scary hope that didn't as that's, I saw her eye, that is a nightmare.

441
00:27:51,635 --> 00:27:55,515
Speaker 7:  That is her eye before the AI got involved. Okay. That's the

442
00:27:55,775 --> 00:27:56,435
Speaker 7:  AI after.

443
00:27:57,015 --> 00:27:57,235
Speaker 5:  Oh

444
00:27:57,855 --> 00:28:01,195
Speaker 7:  That's, And I think that's just actually a really fun art project

445
00:28:01,385 --> 00:28:03,915
Speaker 7:  because what is this, this is a nightmare. It's

446
00:28:03,915 --> 00:28:07,795
Speaker 5:  Beautiful. This is like a black pit of despair with some like streaks

447
00:28:07,795 --> 00:28:08,275
Speaker 5:  of, of

448
00:28:08,375 --> 00:28:08,995
Speaker 7:  That's I'm

449
00:28:08,995 --> 00:28:10,795
Speaker 5:  Like color around that side. That's

450
00:28:10,795 --> 00:28:14,155
Speaker 7:  Like an eyelash maybe that's, that's something and you know,

451
00:28:15,305 --> 00:28:18,955
Speaker 7:  sure. Like what, what are you trying to do? Are you trying to find some

452
00:28:19,105 --> 00:28:22,715
Speaker 7:  interlopers license plate number? Is that why you're doing a hundred Zoom?

453
00:28:22,915 --> 00:28:26,555
Speaker 6:  I mean the idea is landscape kind of stuff.

454
00:28:27,115 --> 00:28:30,315
Speaker 6:  Landmarks, you're on vacation, you're taking a picture of Statue of Liberty.

455
00:28:30,945 --> 00:28:34,795
Speaker 6:  It's kind of far away. So I, I get that

456
00:28:35,215 --> 00:28:39,035
Speaker 6:  it, there are all kinds of implications of what is a

457
00:28:39,035 --> 00:28:42,475
Speaker 6:  photo. Did you really take a photo of the statue? Liberty if AI just kind

458
00:28:42,475 --> 00:28:45,915
Speaker 6:  of filled in the gaps for you? The the writing thing is

459
00:28:46,125 --> 00:28:49,955
Speaker 6:  funny because it does get really nervous and doesn't know what to do. I

460
00:28:49,955 --> 00:28:53,915
Speaker 6:  found that like really big lettering it could do okay.

461
00:28:54,055 --> 00:28:57,595
Speaker 6:  But as soon as it was a little smaller in the image,

462
00:28:57,935 --> 00:29:00,635
Speaker 6:  it just kinda like, I don't know what that is. I,

463
00:29:00,705 --> 00:29:03,435
Speaker 5:  That, that's what kind of breaks my mind about it. Because like, again, if

464
00:29:03,435 --> 00:29:06,835
Speaker 5:  I, I think if I took a, a Statue of Liberty photo with this, like,

465
00:29:07,455 --> 00:29:10,515
Speaker 5:  it, it, it would, it would appear to be my photo by all accounts.

466
00:29:11,895 --> 00:29:15,635
Speaker 5:  But as soon as there is text in it, I'm like, no,

467
00:29:15,635 --> 00:29:18,875
Speaker 5:  there's literally an AI rewriting this image.

468
00:29:19,555 --> 00:29:23,395
Speaker 5:  And that's very trippy. And I think last week I was like, man,

469
00:29:23,565 --> 00:29:26,915
Speaker 5:  maybe this is some new era of photography. Everything is getting upscaled,

470
00:29:26,975 --> 00:29:30,355
Speaker 5:  but it seems like they've actually been pretty precise here

471
00:29:30,855 --> 00:29:34,595
Speaker 5:  in and pretty limiting in what it's applied to. It feels like it probably

472
00:29:34,595 --> 00:29:38,035
Speaker 5:  shouldn't be applied to text at this point in time, even though it'd be cool

473
00:29:38,035 --> 00:29:41,405
Speaker 5:  to be able to zoom in and read some signs from far away, but it doesn't know

474
00:29:41,405 --> 00:29:43,125
Speaker 5:  what language is, so it's not gonna happen.

475
00:29:43,715 --> 00:29:47,565
Speaker 6:  Yeah. And I didn't see anything too wacky

476
00:29:47,585 --> 00:29:51,285
Speaker 6:  or unexpected outside of this. The letters and

477
00:29:51,335 --> 00:29:55,325
Speaker 6:  signs, it gets a little confused by, I think specular

478
00:29:55,325 --> 00:29:59,165
Speaker 6:  highlights if there is, you know, like bright sunlight on the

479
00:29:59,165 --> 00:30:03,045
Speaker 6:  surface of a pond and there were, there's a bunch of garbage and

480
00:30:03,045 --> 00:30:06,845
Speaker 6:  like duck feathers floating around in the pond. It didn't know what to do

481
00:30:06,845 --> 00:30:10,725
Speaker 6:  with those because they just look like points of white stuff and

482
00:30:10,745 --> 00:30:14,725
Speaker 6:  you magnify that a hundred times, it just looks weirder and

483
00:30:14,915 --> 00:30:18,685
Speaker 6:  weirder. So it could kind of, it sort of took those things

484
00:30:18,705 --> 00:30:21,725
Speaker 6:  and went in a different direction. You're like, no, this, this isn't it.

485
00:30:22,825 --> 00:30:26,565
Speaker 6:  But for the most part it was, it was pretty impressive.

486
00:30:26,785 --> 00:30:30,445
Speaker 6:  Closer to 30 times. And as long as you're not asking it to,

487
00:30:30,665 --> 00:30:31,445
Speaker 6:  to read English,

488
00:30:31,955 --> 00:30:35,645
Speaker 5:  This is like gonna give us, i I think material to debate

489
00:30:36,125 --> 00:30:40,085
Speaker 5:  the, what is a photo question for, for years to come. I suspect this is

490
00:30:40,085 --> 00:30:43,085
Speaker 5:  also not going to be the end of AI and camera apps.

491
00:30:44,115 --> 00:30:47,845
Speaker 5:  Very interesting year for The Pixel. Not a lot of like huge hardware

492
00:30:47,845 --> 00:30:51,645
Speaker 5:  changes, but the AI stuff is, is I think particularly interesting this year.

493
00:30:52,255 --> 00:30:56,165
Speaker 5:  Other big thing in phones happening this week. Apple announced the

494
00:30:56,185 --> 00:31:00,085
Speaker 5:  iPhone 17 event. It's happening September 9th. I think we're expecting

495
00:31:00,085 --> 00:31:03,365
Speaker 5:  that to be a pretty interesting one. But now that we've seen what Samsung

496
00:31:03,585 --> 00:31:07,405
Speaker 5:  has to offer this year, now the pixels are out. What do we need to see from

497
00:31:07,405 --> 00:31:10,085
Speaker 5:  Apple to kind of keep up the momentum?

498
00:31:11,165 --> 00:31:15,005
Speaker 6:  I think it's gonna be another situation where they kind of

499
00:31:15,025 --> 00:31:18,125
Speaker 6:  tap dance around the ai, you know, yeah.

500
00:31:18,815 --> 00:31:22,765
Speaker 6:  Thing that's not happening or happening. It seems

501
00:31:22,765 --> 00:31:26,525
Speaker 6:  like we're gonna get some, some different hardware, camera

502
00:31:26,595 --> 00:31:30,565
Speaker 6:  bump goes sideways or something like that. But I think that

503
00:31:30,785 --> 00:31:34,085
Speaker 6:  the elephant in the room is gonna be AI again.

504
00:31:34,755 --> 00:31:38,165
Speaker 7:  Yeah. I don't know. It's in, in a weird way, it feels like these phones are

505
00:31:38,165 --> 00:31:41,725
Speaker 7:  merging into each other in a, 'cause if the camera bump goes sideways, like

506
00:31:41,725 --> 00:31:44,885
Speaker 7:  some of the leaked renders I've seen, I'm like, it just kind of looks like

507
00:31:44,885 --> 00:31:45,085
Speaker 7:  this.

508
00:31:45,835 --> 00:31:48,245
Speaker 5:  It's not, it's not a bad look. I've like, it's not, I've looked at these

509
00:31:48,245 --> 00:31:50,245
Speaker 5:  mockups And I'm like, it's a little funky, but

510
00:31:50,475 --> 00:31:54,405
Speaker 7:  It's not a bad look. But then I'm just like, they're, they're, they're merging

511
00:31:54,475 --> 00:31:57,525
Speaker 7:  into each other in, in a way that's like, you know, Pixel comes out with

512
00:31:57,565 --> 00:32:01,285
Speaker 7:  a journal app, this comes out with a sideways sidebar.

513
00:32:01,515 --> 00:32:05,125
Speaker 7:  MagSafe is on both of them, which, you know, honestly, that was great having

514
00:32:05,125 --> 00:32:08,245
Speaker 7:  MagSafe fond. This was like really great. But

515
00:32:09,445 --> 00:32:13,365
Speaker 7:  I don't know. It's, it's weird, it's bizarre. And Google

516
00:32:13,365 --> 00:32:17,285
Speaker 7:  coming out with such a strong opinion on AI and where

517
00:32:17,305 --> 00:32:21,085
Speaker 7:  it belongs in our devices and, and our lives.

518
00:32:21,465 --> 00:32:25,115
Speaker 7:  I'm just sort of like, ooh, what are they? They threw down the gauntlet

519
00:32:25,555 --> 00:32:26,475
Speaker 7:  a big, what's Apple gonna do?

520
00:32:26,495 --> 00:32:30,195
Speaker 5:  Big a big design refresh is always a good way to, I would say

521
00:32:31,115 --> 00:32:34,035
Speaker 5:  distract from, from any other things. Like you can, you can do nothing else

522
00:32:34,055 --> 00:32:36,955
Speaker 5:  and you're like completely brand new design. Like I, I say, you know what,

523
00:32:36,955 --> 00:32:40,605
Speaker 5:  I'll, I'll take it. I take it. I feel like there's such liquid ass right

524
00:32:40,605 --> 00:32:43,445
Speaker 5:  there. Oh yeah. This is the thing. You've gotta, you've gotta find something

525
00:32:43,445 --> 00:32:47,325
Speaker 5:  to pair with the lovely liquid glass designed, you

526
00:32:47,325 --> 00:32:50,205
Speaker 5:  know, the, the, I think different years are for different things, but also

527
00:32:50,205 --> 00:32:53,085
Speaker 5:  like, it's been a minute since the iPhone has changed in a pretty significant

528
00:32:53,185 --> 00:32:56,765
Speaker 5:  way. The current design is getting a little stale. So like, I I don't hate

529
00:32:56,765 --> 00:33:00,005
Speaker 5:  it. I do, the AI thing is really interesting because it's true.

530
00:33:01,015 --> 00:33:04,925
Speaker 5:  Apple sat down like a year and a half ago. They're like, we're

531
00:33:04,925 --> 00:33:07,925
Speaker 5:  gonna be a major player in this year and a half later. And we kind of know

532
00:33:07,925 --> 00:33:11,405
Speaker 5:  that they're skirting around it, skirting around it still.

533
00:33:12,725 --> 00:33:15,285
Speaker 5:  I don't know. They could, they could have some surprises in store. I don't

534
00:33:15,285 --> 00:33:18,965
Speaker 5:  know that it's gonna happen. I think there's been like ongoing rumors that

535
00:33:18,965 --> 00:33:22,685
Speaker 5:  they're gonna get a Gemini partnership to go alongside the chat GBT

536
00:33:22,685 --> 00:33:25,605
Speaker 5:  thing. It's not clear if that's actually gonna happen in time, but

537
00:33:27,055 --> 00:33:31,005
Speaker 7:  We'll see. Yeah, we'll see what the one more thing is if there is one. And

538
00:33:31,635 --> 00:33:32,445
Speaker 5:  It's not gonna be Google.

539
00:33:33,565 --> 00:33:37,125
Speaker 7:  I mean honestly, honestly, I was just like orange,

540
00:33:37,425 --> 00:33:40,685
Speaker 7:  orange phone. Orange, orange maybe. Yeah. Orange. Yeah,

541
00:33:41,105 --> 00:33:45,005
Speaker 7:  orange. Because orange is not my color, but I have, I have

542
00:33:45,005 --> 00:33:48,885
Speaker 7:  yelled for years that I would like an iPhone pro model

543
00:33:48,945 --> 00:33:52,645
Speaker 7:  to actually have a color and not like a hint of a color.

544
00:33:52,865 --> 00:33:56,525
Speaker 7:  So I saw the, the renders And I was like, listen, that color's not for me.

545
00:33:56,665 --> 00:34:00,405
Speaker 7:  I'm not an orange person, but I appreciate it. Cool.

546
00:34:00,405 --> 00:34:03,765
Speaker 5:  This is what Apple needs to do this year. Make colors happen. Make colors,

547
00:34:03,765 --> 00:34:04,645
Speaker 5:  real colors. Sat

548
00:37:18,925 --> 00:37:22,255
Speaker 5:  like sad dark place that Neli isn't here to

549
00:37:22,645 --> 00:37:24,135
Speaker 5:  commiserate with us about this one.

550
00:37:26,485 --> 00:37:27,485
Speaker 5:  I think as you all know,

551
00:37:29,075 --> 00:37:32,805
Speaker 5:  NELI for a period of time this year has run a, a very

552
00:37:32,945 --> 00:37:36,085
Speaker 5:  famous, well-respected, well-received segment,

553
00:37:36,465 --> 00:37:40,405
Speaker 5:  internationally known as Brendan Carr as a dummy. This

554
00:37:40,405 --> 00:37:44,055
Speaker 5:  segment highlighted the, you know, ups and

555
00:37:44,065 --> 00:37:47,935
Speaker 5:  downs and mostly the downs of FCC chairman Brendan Carr.

556
00:37:48,075 --> 00:37:51,975
Speaker 5:  Now this segment will not be returning today because it

557
00:37:51,975 --> 00:37:55,935
Speaker 5:  is copyright trademark All rights Reserve Neli Patel Enterprises. And, And

558
00:37:56,095 --> 00:37:59,935
Speaker 5:  I, I just couldn't get a license to it. But it is here in spirit

559
00:38:00,245 --> 00:38:03,855
Speaker 5:  because I cannot emphasize

560
00:38:04,315 --> 00:38:08,295
Speaker 5:  the disaster, the absolute disaster of what

561
00:38:08,295 --> 00:38:11,815
Speaker 5:  has happened with Dish, like Dish was already a disaster.

562
00:38:12,655 --> 00:38:16,555
Speaker 5:  And, And I I do not know how it could get worse. And now it

563
00:38:16,555 --> 00:38:20,115
Speaker 5:  it it did, it got worse. It's just, it's just a complete fa like there's

564
00:38:20,115 --> 00:38:23,195
Speaker 5:  been nothing redeemable about Dish's

565
00:38:23,865 --> 00:38:25,715
Speaker 5:  5G Network. Is that correct?

566
00:38:26,535 --> 00:38:29,435
Speaker 11:  I'm sorry. I was, I'm thinking, I'm trying to think of something And I'm

567
00:38:29,435 --> 00:38:32,395
Speaker 11:  just, I'm coming up with nothing. I I got nothing. I think Mitchell, this,

568
00:38:32,625 --> 00:38:36,485
Speaker 11:  this piece that Mitchell Clark wrote three, nearly three years

569
00:38:36,485 --> 00:38:40,325
Speaker 11:  ago now, I went to the Genesis point of Dish 5G Network and all I got

570
00:38:40,325 --> 00:38:44,085
Speaker 11:  was disappointment just captures the entire experience, I think.

571
00:38:44,185 --> 00:38:44,405
Speaker 11:  Oh

572
00:38:44,405 --> 00:38:47,885
Speaker 5:  My God. And not only that, that was the high point of Dish Network.

573
00:38:48,815 --> 00:38:52,595
Speaker 5:  So, okay, what happened this week is that they no longer

574
00:38:52,705 --> 00:38:56,475
Speaker 5:  have a network. They just sold it. So here is

575
00:38:56,475 --> 00:38:59,275
Speaker 5:  what happened in April,

576
00:39:00,175 --> 00:39:04,015
Speaker 5:  SpaceX, Elon Musk guns complains to the

577
00:39:04,095 --> 00:39:07,775
Speaker 5:  FCC. That dish isn't making use of some of their spectrum.

578
00:39:08,355 --> 00:39:12,325
Speaker 5:  Now, in fairness to SpaceX, they're

579
00:39:12,405 --> 00:39:16,285
Speaker 5:  right. Dish isn't doing anything. They should have, they should

580
00:39:16,285 --> 00:39:20,045
Speaker 5:  be doing something. They're on the hook to launch a

581
00:39:20,055 --> 00:39:23,925
Speaker 5:  nationwide wireless carrier. They didn't do it. So a few

582
00:39:23,925 --> 00:39:27,565
Speaker 5:  weeks After that, FC chairman Brendan Carr opened the investigation

583
00:39:27,675 --> 00:39:29,165
Speaker 5:  into Dish's 5G Network.

584
00:39:30,815 --> 00:39:34,665
Speaker 5:  This causes huge problems for Dish because Dish is a disaster dish,

585
00:39:34,845 --> 00:39:38,625
Speaker 5:  by the way, I don't even know how to get into this dish. Sold

586
00:39:38,625 --> 00:39:42,585
Speaker 5:  itself back to EchoStar, which was originally what Dish was called and then

587
00:39:42,615 --> 00:39:45,825
Speaker 5:  spun out Dish. It, it doesn't matter. The point is they have,

588
00:39:46,425 --> 00:39:50,275
Speaker 5:  they're not in a great financial place. So the Wall Street Journal reports

589
00:39:50,275 --> 00:39:53,515
Speaker 5:  in June that EchoStar considering filing for

590
00:39:53,725 --> 00:39:57,395
Speaker 5:  bankruptcy because Brendan Carr's crusade

591
00:39:57,395 --> 00:40:01,355
Speaker 5:  against them is making it impossible for them to do anything. They

592
00:40:01,355 --> 00:40:05,075
Speaker 5:  said that they, that Carr's threats have quote, effectively frozen our

593
00:40:05,075 --> 00:40:08,995
Speaker 5:  ability to make decisions, which in fairness, it was frozen

594
00:40:09,095 --> 00:40:12,195
Speaker 5:  anyway. They weren't doing anything, but it would've been nice if they had

595
00:40:12,195 --> 00:40:15,515
Speaker 5:  done it. Let's go back to 2019. Donald Trump was president.

596
00:40:16,065 --> 00:40:19,675
Speaker 5:  This is important because Donald Trump was president again and in

597
00:40:19,675 --> 00:40:22,795
Speaker 5:  2019, T-Mobile wanted to buy Sprint.

598
00:40:23,575 --> 00:40:27,475
Speaker 5:  And at the time the Department of Justice, again, under Trump,

599
00:40:27,485 --> 00:40:31,355
Speaker 5:  Trump was like, Hey, it's a bad idea

600
00:40:32,015 --> 00:40:35,555
Speaker 5:  to reduce the number of major wireless carriers in this country.

601
00:40:36,135 --> 00:40:40,035
Speaker 5:  We had four that would make three. And so they cut this deal, they

602
00:40:40,035 --> 00:40:43,335
Speaker 5:  cut a deal where T-Mobile says, fine,

603
00:40:44,885 --> 00:40:48,585
Speaker 5:  we will sell off Boost Mobile, Virgin

604
00:40:48,585 --> 00:40:51,665
Speaker 5:  Mobile, sprint Prepaid and some spectrum

605
00:40:52,365 --> 00:40:55,285
Speaker 5:  to Dish in order to buy Sprint

606
00:40:56,625 --> 00:41:00,605
Speaker 5:  and Dish will agree to actually launch a

607
00:41:00,605 --> 00:41:04,445
Speaker 5:  mobile network. They'll agree to become a 5G carrier. And Dish

608
00:41:04,445 --> 00:41:08,165
Speaker 5:  even agrees to a timeline. They work with the FCC. And

609
00:41:08,385 --> 00:41:12,365
Speaker 5:  by June, 2022, they had to cover 20% of the US population with this 5G network.

610
00:41:12,505 --> 00:41:16,325
Speaker 5:  By June, 2023, they had to cover 70% of the population. They do it,

611
00:41:16,325 --> 00:41:20,285
Speaker 5:  they meet these, these requirements in some,

612
00:41:20,385 --> 00:41:24,125
Speaker 5:  you know, alleged capacity and then they never

613
00:41:24,525 --> 00:41:28,405
Speaker 5:  actually launch a service. This is Neli has been on about project Gen five

614
00:41:28,505 --> 00:41:32,005
Speaker 5:  sis, AKA project, Genesis. I don't know what this thing is.

615
00:41:32,275 --> 00:41:35,365
Speaker 5:  This website hasn't been touched in years. They haven't made a social media

616
00:41:35,395 --> 00:41:37,085
Speaker 5:  post since 2022.

617
00:41:39,055 --> 00:41:42,945
Speaker 5:  It's now the second Trump term. Okay?

618
00:41:43,895 --> 00:41:47,755
Speaker 5:  The first Trump term. They were like, dish, you have to be a wireless carrier.

619
00:41:47,755 --> 00:41:51,315
Speaker 5:  It's important that there are four wireless carriers. Okay, that's a big

620
00:41:51,375 --> 00:41:54,915
Speaker 5:  gamble because who's gonna trust Dish to do this?

621
00:41:55,895 --> 00:41:58,675
Speaker 5:  But, but they have a point. That would be good. It would be good if Dish

622
00:41:58,915 --> 00:42:02,525
Speaker 5:  launched a wireless network. Now it is 2025

623
00:42:02,575 --> 00:42:06,525
Speaker 5:  Trump is president again. I know I keep reiterating that, but it's important

624
00:42:06,525 --> 00:42:10,405
Speaker 5:  because this is the same administration. And

625
00:42:10,505 --> 00:42:14,365
Speaker 5:  now Brendan Carr, the FCC chairman is

626
00:42:14,385 --> 00:42:18,245
Speaker 5:  out here saying stuff like, there isn't a quote magic number

627
00:42:18,265 --> 00:42:21,965
Speaker 5:  of carriers that the US needs. He doesn't care. He's like four carriers,

628
00:42:22,445 --> 00:42:26,365
Speaker 5:  whatever. This all leads to this week. When Dish just comes out and

629
00:42:26,365 --> 00:42:29,925
Speaker 5:  they're like, okay, okay, we just sold all our spectrum to at t

630
00:42:30,195 --> 00:42:34,005
Speaker 5:  like basically all of it. They, they sold all the good 5G Spectrum to at

631
00:42:34,285 --> 00:42:37,805
Speaker 5:  t and Dish is like, yeah, we're, we're like kind of a carrier, but not really.

632
00:42:38,115 --> 00:42:40,685
Speaker 5:  It's three carriers. Again, we're just back to three carriers. Not only are

633
00:42:40,685 --> 00:42:44,365
Speaker 5:  we back to three carriers, but at t got this huge chunk of really good spectrum

634
00:42:45,025 --> 00:42:47,725
Speaker 5:  that's gonna make them an even better carrier. We're just gonna make it even

635
00:42:47,725 --> 00:42:51,225
Speaker 5:  harder for other people to compete with them. Dish, everybody,

636
00:42:51,615 --> 00:42:55,185
Speaker 5:  Brendan Carr, everyone the Trump administer This is, this is how that has

637
00:42:55,185 --> 00:42:57,025
Speaker 5:  gone. Did did I miss anything?

638
00:42:58,135 --> 00:42:58,875
Speaker 7:  My brain hurts.

639
00:43:00,235 --> 00:43:02,715
Speaker 11:  I I think you covered everything in, in the middle of that, I went back

640
00:43:02,715 --> 00:43:06,635
Speaker 11:  and pulled up our visual history of the whole timeline from that

641
00:43:06,635 --> 00:43:08,715
Speaker 11:  got to DirecTV and Dish coming together

642
00:43:10,265 --> 00:43:13,535
Speaker 11:  there there's even more mess when you consider the media side

643
00:43:13,965 --> 00:43:17,935
Speaker 11:  another neli favorite. But yeah, that, that, that is about

644
00:43:17,935 --> 00:43:18,415
Speaker 11:  it. I think.

645
00:43:18,875 --> 00:43:22,015
Speaker 5:  Has Dish done literally anything other than like

646
00:43:22,935 --> 00:43:26,485
Speaker 5:  split apart and reassemble its assets over and over again

647
00:43:26,745 --> 00:43:27,805
Speaker 5:  for like 20 years?

648
00:43:28,935 --> 00:43:30,195
Speaker 11:  No, that's

649
00:43:30,195 --> 00:43:33,835
Speaker 5:  What that timeline shows, right? Like that's what it is. I don't, I don't,

650
00:43:34,155 --> 00:43:37,435
Speaker 5:  I don't understand what Dish is now. I don't know, understand what it ever

651
00:43:37,455 --> 00:43:37,675
Speaker 5:  was.

652
00:43:38,665 --> 00:43:41,275
Speaker 11:  They did some innovative DVR stuff for a while there in

653
00:43:41,275 --> 00:43:42,715
Speaker 5:  The middle. Yeah, honestly, I,

654
00:43:43,135 --> 00:43:47,075
Speaker 7:  You know, I, I'm realizing that I've heard of Dish, but I don't think

655
00:43:47,075 --> 00:43:48,915
Speaker 7:  I've ever known truly what Dish is

656
00:43:49,935 --> 00:43:53,795
Speaker 5:  One. If you're still rocking a dish, DVR Vergecast to The Verge dot com,

657
00:43:53,935 --> 00:43:57,695
Speaker 5:  we wanna hear from you. How's that going for you? If you ever use

658
00:43:57,715 --> 00:44:01,675
Speaker 5:  Pro Gen five sis please it. I

659
00:44:01,675 --> 00:44:05,555
Speaker 5:  don't, I do not know what's happening. Brendan Carr's argument

660
00:44:05,555 --> 00:44:09,315
Speaker 5:  here is he's like, oh, the MVNOs are really big

661
00:44:09,375 --> 00:44:13,155
Speaker 5:  now. And so we don't need more major carriers that have their own

662
00:44:13,155 --> 00:44:17,035
Speaker 5:  networks because there's competition. This does not seem like a

663
00:44:17,035 --> 00:44:20,755
Speaker 5:  good argument to me, right? The MVNOs, they

664
00:44:20,755 --> 00:44:23,795
Speaker 5:  operate on the carriers. The carriers can do whatever they want with them.

665
00:44:24,385 --> 00:44:28,275
Speaker 11:  Competition that is the same, that is someone in, in the

666
00:44:28,275 --> 00:44:31,685
Speaker 11:  same outfit wearing a different shirt is not really competition.

667
00:44:31,875 --> 00:44:35,565
Speaker 5:  That is like precisely it. This is such a disaster and

668
00:44:35,665 --> 00:44:39,365
Speaker 5:  no one cares. And I think for the FCC

669
00:44:39,625 --> 00:44:43,405
Speaker 5:  in particular, it's just a complete abdication of their

670
00:44:43,405 --> 00:44:46,765
Speaker 5:  role. And I, I think particularly like this is

671
00:44:47,195 --> 00:44:51,085
Speaker 5:  cars doing. He was just like, Hey, we gave him

672
00:44:51,165 --> 00:44:54,915
Speaker 5:  a deadline. They didn't really exactly

673
00:44:54,935 --> 00:44:58,845
Speaker 5:  follow through on it, so just, just quit, right? Instead of

674
00:44:58,845 --> 00:45:02,245
Speaker 5:  being like, Hey, you're on the hook for being a major wireless carrier, a

675
00:45:02,245 --> 00:45:05,405
Speaker 5:  thing that we regulate. Why don't you go do that? He was like, Hey, get outta

676
00:45:05,405 --> 00:45:05,565
Speaker 5:  here.

677
00:45:06,825 --> 00:45:10,765
Speaker 7:  He just wanted to wash himself of the dishes, wash

678
00:45:10,825 --> 00:45:14,205
Speaker 7:  the dishes. Listen, I've been, I've been, I've been trying to think of that

679
00:45:14,205 --> 00:45:17,165
Speaker 7:  one for a solid two minutes.

680
00:45:18,545 --> 00:45:22,405
Speaker 5:  We listen, nobody wants to spend time with Dish. That's,

681
00:45:22,405 --> 00:45:23,845
Speaker 5:  that's the lesson we've all learned.

682
00:45:26,185 --> 00:45:27,455
Speaker 7:  There are EchoStar now

683
00:45:27,505 --> 00:45:29,535
Speaker 5:  There they are Echo star. Now

684
00:45:31,095 --> 00:45:33,935
Speaker 5:  a fact that confuses me every time I see it.

685
00:45:36,695 --> 00:45:40,675
Speaker 5:  And you know what a fact, that won't matter 'cause it will never show up

686
00:45:40,675 --> 00:45:41,355
Speaker 5:  on our cell phones.

687
00:45:42,305 --> 00:45:45,605
Speaker 11:  But I mean, as as you mentioned, that really is the problem. Because if,

688
00:45:45,625 --> 00:45:49,605
Speaker 11:  if, if you live in an area that isn't well covered by the cell towers, if

689
00:45:49,985 --> 00:45:53,605
Speaker 11:  the prices are too high for the kind of service that you want, if you have

690
00:45:53,605 --> 00:45:57,445
Speaker 11:  fewer real options, there's no one who has a reason to try and get your

691
00:45:57,605 --> 00:46:01,365
Speaker 11:  business. And oh, and, and NVO we've seen the NV Os, we, we have

692
00:46:01,785 --> 00:46:04,765
Speaker 11:  any number of them. Maybe some of you have signed up for Trump Mobile or

693
00:46:04,765 --> 00:46:08,685
Speaker 11:  tried to let us know if you get those phones. But I

694
00:46:08,685 --> 00:46:11,125
Speaker 11:  don't look at any of those. I I've tried some of them. I don't look at any

695
00:46:11,125 --> 00:46:13,605
Speaker 11:  of them as competitors for the major carriers. They just aren't. They aren't

696
00:46:13,605 --> 00:46:14,125
Speaker 11:  on the same level.

697
00:46:14,385 --> 00:46:18,125
Speaker 5:  The thing that gets me is like what say one of them became like

698
00:46:18,285 --> 00:46:22,005
Speaker 5:  a, you know, a, a major player. What leverage do they have? Right?

699
00:46:22,195 --> 00:46:26,165
Speaker 5:  Like what's going to stop the major carriers from being like, Hey, pay

700
00:46:26,165 --> 00:46:29,685
Speaker 5:  us more money. Okay, now they have to raise their prices. Like now this all

701
00:46:29,685 --> 00:46:33,445
Speaker 5:  goes away. They're only working because it wasn't necessarily

702
00:46:33,565 --> 00:46:36,845
Speaker 5:  a huge thing. The carriers are giving them discounts. The carriers can make

703
00:46:36,845 --> 00:46:39,685
Speaker 5:  their service worse anytime they want. The carriers can charge them more

704
00:46:39,685 --> 00:46:40,925
Speaker 5:  anytime they want. And

705
00:46:40,925 --> 00:46:44,605
Speaker 11:  When you're on one of these more budget services, oftentimes you're dealing

706
00:46:44,605 --> 00:46:48,285
Speaker 11:  with things like the kind of prioritization or these caps or

707
00:46:48,425 --> 00:46:51,565
Speaker 11:  you know, kind of fine details. And I think there should be more options

708
00:46:51,565 --> 00:46:54,405
Speaker 11:  and plans, but there needs to be more competition and you just don't have

709
00:46:54,405 --> 00:46:56,805
Speaker 11:  that if you don't have carriers with their own towers.

710
00:46:57,125 --> 00:47:00,965
Speaker 5:  I think you can look back like the decade leading up to T-Mobile buying

711
00:47:00,965 --> 00:47:04,765
Speaker 5:  Sprint. I think because of T-Mobile in particular, there was actually

712
00:47:04,925 --> 00:47:08,805
Speaker 5:  a lot of competition in the mobile carrier space. They

713
00:47:08,805 --> 00:47:12,645
Speaker 5:  were, they were making noise. They, they had to get scrappy, they had to

714
00:47:12,645 --> 00:47:16,565
Speaker 5:  fight pretty hard to be on par with at and t and

715
00:47:16,565 --> 00:47:17,325
Speaker 5:  Verizon. I mean

716
00:47:17,325 --> 00:47:21,245
Speaker 7:  That's why I got on T-Mobile because you know, like they had that guy, I

717
00:47:21,245 --> 00:47:22,405
Speaker 7:  forget his name, but John

718
00:47:22,525 --> 00:47:23,125
Speaker 5:  Ledger J Yes.

719
00:47:23,125 --> 00:47:26,965
Speaker 7:  Him. Yeah, yeah. He was in the pitch CEO and he was like acting

720
00:47:27,065 --> 00:47:31,005
Speaker 7:  up, but at the same time he was acting up all the time. But at the same

721
00:47:31,005 --> 00:47:34,485
Speaker 7:  time I was like, oh, you know, that's more affordable. I'm poor. I can do

722
00:47:34,685 --> 00:47:38,645
Speaker 7:  t T-Mobile and the service is actually pretty good. But I went

723
00:47:38,645 --> 00:47:42,325
Speaker 7:  down to my spouse's ancestral Homeland of T and

724
00:47:42,645 --> 00:47:43,205
Speaker 7:  Maryland recently,

725
00:47:45,025 --> 00:47:48,925
Speaker 7:  you know, and everyone there has Verizon 'cause it's the only network

726
00:47:49,035 --> 00:47:52,925
Speaker 7:  that works. And we're gonna put works in Square Co in

727
00:47:52,925 --> 00:47:56,885
Speaker 7:  scare coat quotes because on his pop hop's farm,

728
00:47:57,035 --> 00:48:00,805
Speaker 7:  there's only one tree where you can get one bar of service. Oh gosh. So to

729
00:48:00,805 --> 00:48:04,765
Speaker 7:  make a phone call before wifi, you always

730
00:48:04,765 --> 00:48:08,685
Speaker 7:  had to go to the one tree on the nut farm to, to make a phone call.

731
00:48:08,905 --> 00:48:12,765
Speaker 7:  So to your point, maybe Dish could have helped him out there.

732
00:48:12,835 --> 00:48:16,685
Speaker 7:  Yeah, yeah. If he had lived up to the promises. 'cause that tree

733
00:48:16,705 --> 00:48:20,645
Speaker 7:  is not close to the farmhouse. It's quite far to have to get the one bar

734
00:48:20,645 --> 00:48:21,925
Speaker 7:  of Verizon. But

735
00:48:21,925 --> 00:48:25,325
Speaker 11:  Maybe now the government has more reason to give SpaceX a lot of money for

736
00:48:25,325 --> 00:48:26,165
Speaker 11:  starlink. This

737
00:48:26,165 --> 00:48:30,085
Speaker 5:  Is the weirdest part. They didn't, the, the thing that instigated all

738
00:48:30,085 --> 00:48:34,005
Speaker 5:  of this SpaceX complaining Echo Star slash

739
00:48:34,075 --> 00:48:37,685
Speaker 5:  Dish slash whatever it's called now, still owns the spectrum that

740
00:48:37,745 --> 00:48:41,285
Speaker 5:  SpaceX wants. They're not, they're not using it, they're not doing anything

741
00:48:41,285 --> 00:48:44,645
Speaker 5:  with it as far as I know or as far as Face X claims.

742
00:48:45,985 --> 00:48:48,965
Speaker 5:  So in the end everybody loses,

743
00:48:50,435 --> 00:48:53,775
Speaker 7:  Yay. That's just the story of America in

744
00:48:53,775 --> 00:48:54,575
Speaker 7:  2025.

745
00:48:55,005 --> 00:48:58,895
Speaker 5:  Okay, it's now time to talk about a slightly more successful

746
00:48:58,895 --> 00:49:01,415
Speaker 5:  company at times. And that is Intel,

747
00:49:03,185 --> 00:49:03,535
Speaker 5:  which

748
00:49:03,875 --> 00:49:05,655
Speaker 11:  May, if you average it out over the years,

749
00:49:06,235 --> 00:49:10,055
Speaker 7:  If you, you know, it think slightly is a little harsh. You know, people know

750
00:49:10,055 --> 00:49:11,935
Speaker 7:  what Intel is and what Intel does.

751
00:49:12,145 --> 00:49:14,215
Speaker 5:  Those stickers went a really long way. They

752
00:49:14,215 --> 00:49:16,375
Speaker 7:  Really d you know, bu bubu bu like I

753
00:49:16,875 --> 00:49:17,935
Speaker 5:  That's true. What is

754
00:49:17,935 --> 00:49:21,495
Speaker 11:  The current name of Intel's processor series Core what?

755
00:49:22,065 --> 00:49:22,415
Speaker 5:  Pente

756
00:49:23,355 --> 00:49:27,265
Speaker 7:  10, lake Moon something.

757
00:49:28,225 --> 00:49:29,305
Speaker 7:  I don't do laptops no more.

758
00:49:29,705 --> 00:49:33,425
Speaker 5:  Yeah, yeah, yeah. No. If you average it out is right. Intel hu has been a

759
00:49:33,425 --> 00:49:36,945
Speaker 5:  bit of a rough streak, which has led up to,

760
00:49:37,535 --> 00:49:41,345
Speaker 5:  this happened on last Friday, so it was after we recorded the last

761
00:49:41,545 --> 00:49:41,905
Speaker 5:  vergecast,

762
00:49:43,955 --> 00:49:47,705
Speaker 5:  Intel gave a 10% stake of the company to the

763
00:49:47,845 --> 00:49:51,465
Speaker 5:  US government in exchange for

764
00:49:52,115 --> 00:49:55,785
Speaker 5:  $8.9 billion. This is weird for a lot of reasons,

765
00:49:55,855 --> 00:49:59,585
Speaker 5:  including that the US government does not typically take ownership stakes

766
00:49:59,585 --> 00:50:00,505
Speaker 5:  in private companies.

767
00:50:02,685 --> 00:50:06,305
Speaker 5:  But I, I wanna talk about why this is a just bizarre and question we'll deal

768
00:50:06,305 --> 00:50:09,825
Speaker 5:  for Intel for a minute. The first being that

769
00:50:10,855 --> 00:50:14,545
Speaker 5:  they were already getting this money. This, this money was

770
00:50:14,545 --> 00:50:17,905
Speaker 5:  already awarded to them, primarily from the CHIPS Act.

771
00:50:19,375 --> 00:50:23,145
Speaker 5:  What happened is that Trump was like, eh, what if we just don't give it to

772
00:50:23,145 --> 00:50:26,145
Speaker 5:  them? Now, I don't know if that's legal, might not be

773
00:50:27,025 --> 00:50:30,935
Speaker 5:  the CHIPS act. I will, you know, remind everyone passed the

774
00:50:30,935 --> 00:50:34,735
Speaker 5:  Senate and the House on a bipartisan basis. Everybody was like, Hey, it would

775
00:50:34,735 --> 00:50:38,415
Speaker 5:  be a good idea to prop up the US semiconductor industry.

776
00:50:39,195 --> 00:50:42,815
Speaker 5:  That's an important industry. We realized that during COVID, it has a lot

777
00:50:42,815 --> 00:50:46,455
Speaker 5:  of national security implications. Let's make sure that's doing well. Intel,

778
00:50:46,455 --> 00:50:48,335
Speaker 5:  they're not doing great. Let's give them some cash.

779
00:50:50,385 --> 00:50:53,695
Speaker 5:  Trump didn't like it because it was associated with Biden. And so he is like,

780
00:50:54,915 --> 00:50:58,815
Speaker 5:  I'm just gonna, I'm not gonna give him the cash. And so he, so thi

781
00:50:58,815 --> 00:51:02,175
Speaker 5:  this is the timeline of what happens early August. Trump

782
00:51:02,925 --> 00:51:06,855
Speaker 5:  demands that Intel's new CEO the Bhutan resign. He's just

783
00:51:06,855 --> 00:51:10,095
Speaker 5:  like, I don't like this guy. He has, he has some ties to China, get him outta

784
00:51:10,095 --> 00:51:13,495
Speaker 5:  here. Tan goes and visits the White House

785
00:51:13,995 --> 00:51:17,935
Speaker 5:  as CEOs do these days and suddenly they're good

786
00:51:18,085 --> 00:51:22,055
Speaker 5:  pals. And I assume that has to do something with

787
00:51:22,055 --> 00:51:22,655
Speaker 5:  the fact that,

788
00:51:24,475 --> 00:51:28,415
Speaker 5:  you know, a week or so later, he gives 10% of the company to the

789
00:51:28,415 --> 00:51:32,175
Speaker 5:  United States in exchange for this money that the United States was already

790
00:51:32,175 --> 00:51:32,695
Speaker 5:  paying them.

791
00:51:34,235 --> 00:51:35,925
Speaker 5:  This is what a deal. It

792
00:51:36,395 --> 00:51:37,525
Speaker 7:  What a deal. Art of the deal.

793
00:51:37,595 --> 00:51:41,565
Speaker 5:  This is, I mean I I this is very, this is bizarre on,

794
00:51:41,625 --> 00:51:45,045
Speaker 5:  on all counts. Number one, tan is in some ways

795
00:51:45,355 --> 00:51:49,005
Speaker 5:  gave away 10% of Intel to protect his job, which I think is

796
00:51:49,565 --> 00:51:50,205
Speaker 5:  questionable precision.

797
00:51:52,195 --> 00:51:55,455
Speaker 5:  Number two, the what is really happening here

798
00:51:55,995 --> 00:51:59,975
Speaker 5:  is that Intel is his foundry business. Foundry business is not doing great,

799
00:51:59,985 --> 00:52:03,285
Speaker 5:  right? Is not making the best ships.

800
00:52:04,165 --> 00:52:06,605
Speaker 5:  Companies don't wanna make a deal with Intel's foundry business 'cause they

801
00:52:06,605 --> 00:52:09,565
Speaker 5:  don't even know if the Intel's Foundry foundry business is gonna be there

802
00:52:09,565 --> 00:52:13,265
Speaker 5:  long enough to keep making the chips. And so the US

803
00:52:13,265 --> 00:52:16,945
Speaker 5:  government is now getting involved And I think the reason that Intel wants

804
00:52:16,945 --> 00:52:19,825
Speaker 5:  to do this is that they're like, Hey Trump,

805
00:52:20,915 --> 00:52:24,715
Speaker 5:  maybe he will just pressure Apple to make some chips with us. Everyone's

806
00:52:24,715 --> 00:52:28,705
Speaker 5:  leaving Intel and, and they're, they're, they're looking to Trump

807
00:52:28,805 --> 00:52:32,785
Speaker 5:  to put some pressure on them to get

808
00:52:32,945 --> 00:52:33,585
Speaker 5:  customers for Intel.

809
00:52:34,345 --> 00:52:38,035
Speaker 7:  There's some real galaxy brain math happening here. Like I feel like my

810
00:52:38,035 --> 00:52:41,915
Speaker 7:  brain is leaving my skull ying itself to Jupiter and

811
00:52:42,325 --> 00:52:45,955
Speaker 7:  doing some mental gymnastics to try and see the logic behind this. And I,

812
00:52:45,955 --> 00:52:47,755
Speaker 7:  it's finding none. It's finding none.

813
00:52:48,175 --> 00:52:51,915
Speaker 5:  The, the, the logic as far as I can tell is, is

814
00:52:51,915 --> 00:52:54,955
Speaker 5:  like crony capitalism, right? It is. The logic here is

815
00:52:56,055 --> 00:52:59,925
Speaker 5:  the US government will get involved on

816
00:52:59,955 --> 00:53:03,805
Speaker 5:  Intel's behalf and Trump is gonna put

817
00:53:04,045 --> 00:53:05,805
Speaker 5:  pressure on people and

818
00:53:07,465 --> 00:53:09,965
Speaker 5:  the folks he likes are gonna win. The folks he doesn't like are gonna lose.

819
00:53:10,465 --> 00:53:14,355
Speaker 5:  And right now, at this moment, he likes Intel and he's gonna

820
00:53:14,415 --> 00:53:18,135
Speaker 5:  try to get some people to be Intel's customers and hopefully

821
00:53:18,145 --> 00:53:21,985
Speaker 5:  Intel can make some good chips for them. But we don't know if they can, which

822
00:53:22,005 --> 00:53:25,765
Speaker 11:  Has been basically Intel's whole problem for years now that they've been

823
00:53:26,005 --> 00:53:29,885
Speaker 11:  supposed to be making better chips and sort of haven't. And I,

824
00:53:29,965 --> 00:53:32,245
Speaker 11:  I guess we'll see what happens with these next generation of chips that

825
00:53:32,245 --> 00:53:35,805
Speaker 11:  they're building. But they've missed out on really the AI boom. They

826
00:53:36,425 --> 00:53:36,645
Speaker 11:  had.

827
00:53:38,245 --> 00:53:41,925
Speaker 11:  TSMC has become massive on, on their watch. They've missed out on this

828
00:53:41,925 --> 00:53:45,715
Speaker 11:  technology that they developed. Is that just

829
00:53:45,785 --> 00:53:48,635
Speaker 11:  Pure, from a tech perspective? I don't see how this solves the problem of

830
00:53:49,045 --> 00:53:52,835
Speaker 11:  Intel messing up, which has been the story of their last decade or so

831
00:53:53,505 --> 00:53:56,395
Speaker 11:  from a business perspective, I don't know what's going on. And from a political

832
00:53:56,395 --> 00:53:57,835
Speaker 11:  perspective, it's scary.

833
00:53:59,975 --> 00:54:01,955
Speaker 11:  Is there, is there another part that I forgot?

834
00:54:02,145 --> 00:54:05,875
Speaker 7:  They also missed out on the mobile boom, which is like, if you talk to

835
00:54:06,035 --> 00:54:09,315
Speaker 7:  Qualcomm for any like second 'cause Qualcomm has always had a

836
00:54:09,785 --> 00:54:13,755
Speaker 7:  chip on their shoulder about how everyone knows about Intel

837
00:54:13,855 --> 00:54:17,035
Speaker 7:  and not them. Even though they're in every like Android phone and they're

838
00:54:17,035 --> 00:54:20,075
Speaker 7:  all over the place. 'cause you know, you literally talk to Qualcomm for five

839
00:54:20,075 --> 00:54:23,235
Speaker 7:  seconds and they're like, where we're in mobile, we're in everything where

840
00:54:23,435 --> 00:54:26,675
Speaker 7:  Qualcomm Snapdragon, yay. And nobody knows,

841
00:54:27,425 --> 00:54:31,155
Speaker 7:  like outside of tech nerd Damm, no one knows what Qualcomm is, but everyone

842
00:54:31,155 --> 00:54:35,075
Speaker 7:  knows Intel and, but yeah, they miss the

843
00:54:35,075 --> 00:54:38,805
Speaker 7:  mobile train, they miss the AI boom and now they're like

844
00:54:40,045 --> 00:54:42,025
Speaker 7:  the, I don't know, Intel's just sad, man.

845
00:54:42,455 --> 00:54:45,665
Speaker 5:  Well, and and to Richard's point, like this doesn't solve Intel's

846
00:54:46,325 --> 00:54:50,145
Speaker 5:  the, you know, recent inability to make the correct business

847
00:54:50,385 --> 00:54:53,305
Speaker 5:  decisions. Yeah. Like the tech isn't there. The business digits have been

848
00:54:53,305 --> 00:54:53,505
Speaker 5:  off,

849
00:54:55,095 --> 00:54:58,965
Speaker 5:  right? Like forcing people to be Intel's customers, it keeps them

850
00:54:58,985 --> 00:55:02,765
Speaker 5:  afloat, which, you know, I, I would like Intel to survive.

851
00:55:02,885 --> 00:55:06,365
Speaker 5:  I would like its Foundry business to succeed. I think that that is a, a good

852
00:55:06,365 --> 00:55:10,045
Speaker 5:  thing for the industry, but they need to make good products,

853
00:55:10,215 --> 00:55:10,565
Speaker 5:  right?

854
00:55:10,875 --> 00:55:14,125
Speaker 7:  Fundamental rule of business is make a good product. So

855
00:55:14,955 --> 00:55:18,345
Speaker 7:  yeah, they, they kind of need to do that. But

856
00:55:19,225 --> 00:55:23,105
Speaker 7:  I don't know, man, their marketing team is still great, But that might be

857
00:55:23,105 --> 00:55:25,985
Speaker 7:  the only team at Intel that's still good. I

858
00:55:25,985 --> 00:55:29,105
Speaker 11:  Dunno. But they've been losing ground to A and B and now Windows on Arm

859
00:55:29,365 --> 00:55:33,345
Speaker 11:  is really good. Suddenly, apparently Qualcomm and ev, everyone is coming

860
00:55:33,345 --> 00:55:37,265
Speaker 11:  for them. Their Nvidia is reportedly making a chip. There's

861
00:55:37,265 --> 00:55:38,425
Speaker 11:  a lot of crossover and a lot of,

862
00:55:40,025 --> 00:55:42,705
Speaker 11:  a lot of overlap, I think in, in these companies and what they're doing.

863
00:55:42,705 --> 00:55:46,145
Speaker 11:  And I just, I still don't see the, the position for Intel to get themselves

864
00:55:46,145 --> 00:55:47,065
Speaker 11:  into, to make things better.

865
00:55:47,175 --> 00:55:50,385
Speaker 5:  There's also a bizarre national security component to this. Part of this

866
00:55:50,405 --> 00:55:53,945
Speaker 5:  is great. We wanna make sure, you know, the United States wants to make sure

867
00:55:54,375 --> 00:55:57,585
Speaker 5:  that there are foundries in the US that can make good chips

868
00:55:58,175 --> 00:56:01,985
Speaker 5:  that the supply chain is here. This makes sense. On the other hand,

869
00:56:02,985 --> 00:56:06,345
Speaker 5:  TSMC was a really valuable national, national security

870
00:56:07,745 --> 00:56:11,345
Speaker 5:  contribution to Taiwan, right? And if

871
00:56:11,685 --> 00:56:15,385
Speaker 5:  the US is removing, trying to remove reliance on

872
00:56:15,545 --> 00:56:19,025
Speaker 5:  TSMC, then that's bad news for tsmc. It's bad news for Taiwan,

873
00:56:20,105 --> 00:56:22,415
Speaker 5:  which is bad news for the tech industry in general because

874
00:56:24,005 --> 00:56:27,815
Speaker 5:  most of the chips are coming from TSMC, tt, SMC has

875
00:56:27,815 --> 00:56:31,695
Speaker 5:  the best stuff. And so this just

876
00:56:31,695 --> 00:56:34,055
Speaker 5:  gets messy on basically every level.

877
00:56:35,615 --> 00:56:39,415
Speaker 5:  I, I Trump sees what he's doing as just getting

878
00:56:39,515 --> 00:56:43,455
Speaker 5:  10% of this company. But if you continue to follow out the

879
00:56:43,455 --> 00:56:47,215
Speaker 5:  next steps of this, what has to happen in order for this to make any

880
00:56:47,215 --> 00:56:51,055
Speaker 5:  sense? It it does, it is not good for anybody in the

881
00:56:51,295 --> 00:56:53,375
Speaker 5:  industry except perhaps Intel

882
00:56:53,915 --> 00:56:56,735
Speaker 11:  And maybe not even Intel. I, I guess right,

883
00:56:57,755 --> 00:57:01,655
Speaker 11:  the, I don't specifically know what the counterargument is, but I guess

884
00:57:01,915 --> 00:57:04,695
Speaker 11:  if you, if you take a look, how different is this from something like the

885
00:57:04,695 --> 00:57:08,335
Speaker 11:  auto bailout, like in the treasury, like

886
00:57:08,405 --> 00:57:11,895
Speaker 11:  kind of taking over GM in these bankruptcies? I feel like that was an

887
00:57:11,895 --> 00:57:15,255
Speaker 11:  emergency from outside. This is an emergency that Trump created and so it's

888
00:57:15,255 --> 00:57:15,655
Speaker 11:  not the same.

889
00:57:15,995 --> 00:57:18,815
Speaker 5:  No, I think that makes sense. And then I, correct me if I'm wrong, I don't,

890
00:57:18,955 --> 00:57:22,815
Speaker 5:  the, the US didn't take a stake in those companies, right? Like that that

891
00:57:22,815 --> 00:57:25,095
Speaker 5:  was it. It they were giving them money, right.

892
00:57:25,755 --> 00:57:28,815
Speaker 11:  For a time, I believe the treasury owned gm.

893
00:57:29,395 --> 00:57:31,415
Speaker 5:  Oh, interesting. Okay. Yeah. But

894
00:57:31,415 --> 00:57:34,975
Speaker 7:  Like, part of that was just that, you know, the, the US bailed out the auto

895
00:57:35,335 --> 00:57:38,895
Speaker 7:  industry just because of, you know, the, the jobs and like the, the

896
00:57:39,255 --> 00:57:43,175
Speaker 7:  industry, the local, how much it would devastate the local economy that this

897
00:57:43,175 --> 00:57:46,775
Speaker 7:  kind of just devastates Intel. And yes, there are like jobs that are

898
00:57:46,775 --> 00:57:50,735
Speaker 7:  associated with Intel and whatnot. It's just, I

899
00:57:50,735 --> 00:57:54,015
Speaker 7:  feel like there were bigger implications with, I don't know, the entire US

900
00:57:54,085 --> 00:57:57,775
Speaker 7:  auto industry going versus one

901
00:57:58,005 --> 00:58:00,695
Speaker 7:  semiconductor company that happens to be American.

902
00:58:01,035 --> 00:58:04,295
Speaker 11:  If their problem had started with the president of the United States claiming

903
00:58:04,295 --> 00:58:08,135
Speaker 11:  that their CEO might be an agent for a foreign government, we would be

904
00:58:08,135 --> 00:58:09,015
Speaker 11:  discussing it very differently.

905
00:58:09,325 --> 00:58:09,815
Speaker 7:  That too.

906
00:58:10,365 --> 00:58:13,735
Speaker 5:  Yeah, And I think the, the auto bailout also goes to show the, the government

907
00:58:13,755 --> 00:58:17,335
Speaker 5:  has a lot of levers it can pull from the outside

908
00:58:17,845 --> 00:58:21,375
Speaker 5:  without going so far as taking an ownership stake in a company

909
00:58:21,495 --> 00:58:25,175
Speaker 5:  indefinitely. And I think that is where it gets concerning,

910
00:58:25,625 --> 00:58:29,215
Speaker 5:  right? Intel has the backing of the US government that just

911
00:58:29,215 --> 00:58:32,935
Speaker 5:  necessarily gives Intel a very fascinating position

912
00:58:33,195 --> 00:58:37,095
Speaker 5:  and advantage over everyone else in the industry. And

913
00:58:37,155 --> 00:58:39,855
Speaker 5:  it puts pressures on everybody else in the industry, whether they like it

914
00:58:39,855 --> 00:58:43,815
Speaker 5:  or not. Okay. One more thing we have to talk about today. A few weeks ago

915
00:58:44,355 --> 00:58:48,335
Speaker 5:  we maybe may made fun of Elon Musk

916
00:58:48,415 --> 00:58:49,615
Speaker 5:  a little bit. Did, did

917
00:58:49,615 --> 00:58:50,695
Speaker 7:  We call it a she again?

918
00:58:50,855 --> 00:58:54,735
Speaker 5:  I we did, I think we, we said, is this was a Elon threatened

919
00:58:55,075 --> 00:58:59,055
Speaker 5:  to sue Apple over not featuring Grok in the

920
00:58:59,055 --> 00:59:02,815
Speaker 5:  app store. And I, I think we all thought it was a joke, right? Because

921
00:59:02,925 --> 00:59:06,255
Speaker 5:  Elon, Elon threatens the lawsuits a lot, does not always file them. He did

922
00:59:06,275 --> 00:59:07,895
Speaker 5:  In fact file a lawsuit. He,

923
00:59:08,835 --> 00:59:12,765
Speaker 7:  I don't know whether to say yay, you know, like, oh, you, you finally

924
00:59:13,445 --> 00:59:17,325
Speaker 7:  followed through on a, on a thing, yay for you. Or to be like, oh my

925
00:59:17,325 --> 00:59:20,965
Speaker 7:  God, you followed through on this thing. Not yay for me.

926
00:59:21,875 --> 00:59:25,165
Speaker 5:  It's not a great lawsuit, but he did do it. So

927
00:59:25,565 --> 00:59:29,205
Speaker 5:  XAI suing OpenAI and Apple and they're

928
00:59:29,205 --> 00:59:33,125
Speaker 5:  alleging anti-competitive conduct over a bunch of different issues. They,

929
00:59:33,125 --> 00:59:36,805
Speaker 5:  they have, they, they have beef with OpenAI getting

930
00:59:37,025 --> 00:59:40,845
Speaker 5:  an exclusive deal to put Jet g BT into Siri.

931
00:59:41,755 --> 00:59:45,645
Speaker 5:  They claim that Apple is quote manipulating app store rankings.

932
00:59:45,945 --> 00:59:49,685
Speaker 5:  And they say that Apple is delaying app store reviews for

933
00:59:49,795 --> 00:59:52,885
Speaker 5:  Grok. Now, I think we can just say off the bat,

934
00:59:53,365 --> 00:59:57,325
Speaker 5:  famously all the apps store reviews take forever. So that's, that's

935
00:59:57,325 --> 01:00:00,405
Speaker 5:  not unique. But the, the premise here is basically

936
01:00:01,035 --> 01:00:04,405
Speaker 5:  this deal is unfair because it's not open to everybody and we're getting

937
01:00:04,405 --> 01:00:07,885
Speaker 5:  screwed in the app store. And I, they, they

938
01:00:08,285 --> 01:00:12,245
Speaker 5:  undercut that argument right away because in this

939
01:00:12,245 --> 01:00:16,045
Speaker 5:  lawsuit, they highlight the fact that X

940
01:00:16,105 --> 01:00:19,965
Speaker 5:  is ranked number one as the free app for news. And

941
01:00:20,105 --> 01:00:23,725
Speaker 5:  GR has ranked number two on the, as the free apps for productivity.

942
01:00:24,025 --> 01:00:27,765
Speaker 5:  So they're in this lawsuit, they're like, Hey, we have these great apps.

943
01:00:27,765 --> 01:00:31,085
Speaker 5:  They're at the top of the app store, but they're also getting screwed by

944
01:00:31,085 --> 01:00:34,525
Speaker 5:  the app store algorithm. And it's like, what is this case?

945
01:00:34,955 --> 01:00:38,525
Speaker 5:  What case are you making here? They're just mad that it's not in the

946
01:00:38,715 --> 01:00:42,165
Speaker 5:  must have apps section. Don't, don't

947
01:00:42,655 --> 01:00:46,085
Speaker 5:  argue against yourself. You're doing great. You just said you're doing great.

948
01:00:46,675 --> 01:00:49,325
Speaker 5:  Your argument is we're doing great. And so we should also get featured in

949
01:00:49,325 --> 01:00:49,805
Speaker 5:  this other area.

950
01:00:51,015 --> 01:00:54,785
Speaker 7:  Having tested Annie Gru's not a must

951
01:00:54,785 --> 01:00:56,745
Speaker 7:  have app, I must say.

952
01:00:57,165 --> 01:01:00,585
Speaker 11:  And Elon has been obsessed with app store rankings as someone who has

953
01:01:00,585 --> 01:01:04,305
Speaker 11:  notifications on for his tweets. He was always tweeting about

954
01:01:04,905 --> 01:01:08,705
Speaker 11:  X or X AI or grok or whatever, being number one in the app store in

955
01:01:08,705 --> 01:01:10,105
Speaker 11:  some country that you forgot existed.

956
01:01:11,935 --> 01:01:15,555
Speaker 11:  He gets his feet of data and must share the news with everyone all the time.

957
01:01:15,745 --> 01:01:19,435
Speaker 5:  Richard, that is extremely unhealthy. I have to tell you,

958
01:01:19,585 --> 01:01:20,515
Speaker 11:  It's the healthiest.

959
01:01:23,075 --> 01:01:26,795
Speaker 11:  I, I know that I have not gone into active psychosis because I still can't

960
01:01:26,795 --> 01:01:29,795
Speaker 11:  understand what Elon is saying and, And I get a, And I get a reminder of

961
01:01:29,795 --> 01:01:32,035
Speaker 11:  this every 10 minutes or so. It's great. That's,

962
01:01:32,035 --> 01:01:35,555
Speaker 5:  It really is. It is really that often it's horrible. You are right though.

963
01:01:35,555 --> 01:01:39,355
Speaker 5:  Like just the other day he like retweeted somebody who's like, which again

964
01:01:39,385 --> 01:01:42,475
Speaker 5:  cuts against their argument. He's like, we're featured in the must-haves

965
01:01:42,475 --> 01:01:45,515
Speaker 5:  with one of those editorial sections. They said they were featured in the

966
01:01:45,515 --> 01:01:49,355
Speaker 5:  United Arab Emirates in one of those editorial sections. And it's like, okay,

967
01:01:49,355 --> 01:01:51,435
Speaker 5:  so they are getting featured.

968
01:01:53,315 --> 01:01:57,115
Speaker 5:  I I do not know that they have much of a case here. This is

969
01:01:57,115 --> 01:02:01,075
Speaker 5:  just like not a good thing to argue when at the same time

970
01:02:01,075 --> 01:02:04,835
Speaker 5:  you're saying that actually we're doing quite well. The, the other part of

971
01:02:04,835 --> 01:02:08,635
Speaker 5:  this case I think is actually very interesting. And

972
01:02:08,875 --> 01:02:12,675
Speaker 5:  I, they're saying, Hey, we, we actually can't

973
01:02:12,675 --> 01:02:16,355
Speaker 5:  compete with chat GBT because they're getting so much

974
01:02:16,425 --> 01:02:20,405
Speaker 5:  data and so many queries via Siri, which

975
01:02:20,405 --> 01:02:23,405
Speaker 5:  is this really interesting thing where they're kind of acknowledging, oh,

976
01:02:23,465 --> 01:02:27,045
Speaker 5:  we have to farm how everyone uses this thing or

977
01:02:27,265 --> 01:02:30,325
Speaker 5:  we can't improve it. Which is one of those things I think we all know, but

978
01:02:30,325 --> 01:02:32,165
Speaker 5:  it's like another thing to see it written down.

979
01:02:34,025 --> 01:02:37,905
Speaker 7:  I mean, if you want grok baked into Siri, maybe

980
01:02:37,905 --> 01:02:41,705
Speaker 7:  don't make sex bots. Like, I, I don't, I don't know how

981
01:02:41,705 --> 01:02:45,585
Speaker 7:  else to say that part, but yeah, I don't want to talk

982
01:02:45,585 --> 01:02:48,985
Speaker 7:  to Siri and have bad Rudy, the little angry panda

983
01:02:49,925 --> 01:02:53,745
Speaker 7:  say that I'm lame because he's, if you insult

984
01:02:53,745 --> 01:02:57,345
Speaker 7:  that panda, all he says is like, I'm not lame. You are lame.

985
01:02:57,525 --> 01:03:01,265
Speaker 7:  And that's just, I, I don't know what he expects

986
01:03:01,655 --> 01:03:04,985
Speaker 7:  when he acts out the way that he does. And Apple,

987
01:03:05,675 --> 01:03:09,585
Speaker 7:  apple has not been like hiding how they are this entire

988
01:03:09,655 --> 01:03:13,425
Speaker 7:  time and what they want out of the apps in their app store.

989
01:03:13,495 --> 01:03:14,865
Speaker 7:  They, they haven't hidden that

990
01:03:15,815 --> 01:03:19,545
Speaker 5:  Well. And, and it's like, right, you have the chat bot that number one

991
01:03:21,145 --> 01:03:24,265
Speaker 5:  recently went on a tear and called itself Mega Hitler

992
01:03:25,185 --> 01:03:29,005
Speaker 5:  for, for no clear reason that you have now baked several,

993
01:03:29,075 --> 01:03:32,485
Speaker 5:  like pre-made boss into it. They've updated Annie, the, the,

994
01:03:32,865 --> 01:03:36,365
Speaker 5:  you know, physical digital person

995
01:03:36,755 --> 01:03:40,165
Speaker 5:  incarnation of grok with some new outfits

996
01:03:40,545 --> 01:03:44,405
Speaker 5:  so she can so much like strip down into like a bathing suit or something

997
01:03:44,465 --> 01:03:48,285
Speaker 5:  now, which is just very embarrassing. But again, it, it's like

998
01:03:48,775 --> 01:03:52,165
Speaker 5:  Apple is like pretty, you know, for better or worse, they're fairly like

999
01:03:52,165 --> 01:03:55,205
Speaker 5:  prudish with the app store, right? They could do that thing. They're,

1000
01:03:55,205 --> 01:03:58,765
Speaker 7:  They always say that they're family friendly that Yeah, Annie is not family

1001
01:03:59,005 --> 01:04:00,965
Speaker 7:  friendly. Like, sh it's just not

1002
01:04:01,455 --> 01:04:04,485
Speaker 5:  Right. It, it's, I think there's, like, again, this is another world where

1003
01:04:04,725 --> 01:04:08,605
Speaker 5:  I, I think if this was a random small company, like they're,

1004
01:04:08,635 --> 01:04:12,485
Speaker 5:  they're getting kicked outta the app store. Like I, I think Apple

1005
01:04:12,585 --> 01:04:15,725
Speaker 5:  has plenty of reasons to not feature this app

1006
01:04:17,105 --> 01:04:21,005
Speaker 5:  and Elon is still asking for more And I, I get it. Like, I understand

1007
01:04:22,345 --> 01:04:26,085
Speaker 5:  why you would be pissed about not getting a, a special deal. But actually

1008
01:04:26,085 --> 01:04:29,925
Speaker 5:  one thing that's really interesting, Tom Warren on our team, he writes

1009
01:04:29,925 --> 01:04:33,805
Speaker 5:  the Notepad newsletter reports on Microsoft. He reported, I I think it was

1010
01:04:33,805 --> 01:04:37,565
Speaker 5:  this month that Microsoft has been testing,

1011
01:04:38,665 --> 01:04:42,525
Speaker 5:  you know, the latest version of Grok and they don't wanna deploy it

1012
01:04:42,785 --> 01:04:46,605
Speaker 5:  to, to Azure because they're worried about the safety of it,

1013
01:04:46,975 --> 01:04:50,845
Speaker 5:  right? Like this is not just an Apple thing. Like

1014
01:04:50,845 --> 01:04:54,245
Speaker 5:  this is, this is Elon's product. It is not ready for this,

1015
01:04:55,015 --> 01:04:57,515
Speaker 5:  it is not a thing other companies want to be associated with.

1016
01:04:58,615 --> 01:05:02,315
Speaker 7:  And you know, I don't think any of us are surprised when Elon has a tantrum,

1017
01:05:02,695 --> 01:05:05,715
Speaker 7:  but this is the equivalent of him having a tantrum orally. Like, I'm not

1018
01:05:05,975 --> 01:05:09,835
Speaker 7:  the best in every single thing, so I'm gonna sue you for not

1019
01:05:09,835 --> 01:05:13,715
Speaker 7:  letting me be the best in every single thing. And it's not my fault. I

1020
01:05:13,715 --> 01:05:17,035
Speaker 7:  have not done anything in this product that makes it not the best.

1021
01:05:17,665 --> 01:05:21,245
Speaker 7:  You clearly, you know, it's just like very juvenile behavior

1022
01:05:23,375 --> 01:05:27,345
Speaker 7:  and external blaming of, of, I

1023
01:05:27,345 --> 01:05:31,145
Speaker 7:  think if he were to listen, he would hear some very fair feedback and

1024
01:05:31,145 --> 01:05:34,865
Speaker 7:  criticism for all of his products. And despite that, it's still,

1025
01:05:35,495 --> 01:05:39,465
Speaker 7:  like you said, it's still ranked very highly in various sections, just not

1026
01:05:39,465 --> 01:05:42,425
Speaker 7:  the section he wants to be the highest in. And that's just

1027
01:05:43,855 --> 01:05:46,865
Speaker 5:  Richard, have you spent any quality time with chatting with Grock?

1028
01:05:47,305 --> 01:05:47,705
Speaker 11:  I have not.

1029
01:05:49,415 --> 01:05:52,905
Speaker 5:  This, that's where you draw the line. You'll get a constant feed of

1030
01:05:53,215 --> 01:05:55,105
Speaker 5:  Elon. Yeah, but you won't talk.

1031
01:05:55,475 --> 01:05:57,105
Speaker 11:  We've got rock at home. Okay.

1032
01:05:57,855 --> 01:05:58,145
Speaker 5:  Like

1033
01:05:58,375 --> 01:06:02,225
Speaker 7:  He's already getting the Elon feed, you know, he don't need the rock feed

1034
01:06:02,325 --> 01:06:03,065
Speaker 7:  on top of that.

1035
01:06:03,365 --> 01:06:03,945
Speaker 5:  That's real.

1036
01:06:04,525 --> 01:06:08,385
Speaker 11:  But I, I think the thing about this lawsuit that destructs me is that

1037
01:06:08,755 --> 01:06:12,425
Speaker 11:  we're about as receptive as of an audience as there is, I think to criticisms

1038
01:06:12,425 --> 01:06:15,105
Speaker 11:  of the App store and the way Apple maintains it. We've been talking about

1039
01:06:15,105 --> 01:06:18,705
Speaker 11:  this for years, anti-competitive monopolistic practices,

1040
01:06:19,245 --> 01:06:22,585
Speaker 11:  the decisions that they make, the way that they do that. Other companies

1041
01:06:22,585 --> 01:06:24,305
Speaker 11:  have made these arguments before,

1042
01:06:26,195 --> 01:06:30,095
Speaker 11:  but as, as v put it, there are reasons why gr might

1043
01:06:30,095 --> 01:06:30,415
Speaker 11:  not be.

1044
01:06:32,965 --> 01:06:35,575
Speaker 11:  They're, they're really good reasons and, And I think that they can make

1045
01:06:35,575 --> 01:06:38,495
Speaker 11:  those arguments, but it's pretty consistent with Elon's overall

1046
01:06:40,055 --> 01:06:44,015
Speaker 11:  approach. That if he's not, if he's not very visibly winning and

1047
01:06:44,015 --> 01:06:47,815
Speaker 11:  celebrated for winning, then it's someone else's fault that he's

1048
01:06:47,815 --> 01:06:50,375
Speaker 11:  losing and he's going to make it everyone's problem.

1049
01:06:51,155 --> 01:06:53,895
Speaker 5:  That's really true. And I do, I do think there is some degree to which like,

1050
01:06:53,965 --> 01:06:57,455
Speaker 5:  okay, apple didn't pander to him and so,

1051
01:06:58,005 --> 01:07:01,535
Speaker 5:  okay, he's just gonna try to fight them a little bit. I naturally, the lawsuit

1052
01:07:01,635 --> 01:07:05,575
Speaker 5:  is filed in Texas where he can potentially find a friendlier

1053
01:07:05,575 --> 01:07:09,135
Speaker 5:  judge. So we'll see what happens. Maybe he can, you know,

1054
01:07:09,265 --> 01:07:12,875
Speaker 5:  cause some problems for a little bit. I'm

1055
01:07:13,135 --> 01:07:17,115
Speaker 5:  not convinced this is a good case. Oh, one one last very quick

1056
01:07:17,115 --> 01:07:20,155
Speaker 5:  thing. You, Elon is also suing

1057
01:07:20,945 --> 01:07:24,635
Speaker 5:  open AI over the fact that they're, you know, trying to become a public company

1058
01:07:24,645 --> 01:07:28,355
Speaker 5:  where they say, were in the meantime, X AI

1059
01:07:29,245 --> 01:07:33,155
Speaker 5:  drops its status as a public benefit corporation. Thi

1060
01:07:33,225 --> 01:07:35,995
Speaker 5:  this is, I don't know, I don't,

1061
01:07:37,195 --> 01:07:40,835
Speaker 5:  I don't know what's happening over there. Other than that Elon is

1062
01:07:40,835 --> 01:07:41,395
Speaker 5:  obviously

1063
01:10:03,025 --> 01:10:06,525
Speaker 5:  All right, we're back time for the Thunder on Richard. I, you don't know

1064
01:10:06,525 --> 01:10:10,365
Speaker 5:  this yet. Neli obviously is not around and so I

1065
01:10:10,365 --> 01:10:14,125
Speaker 5:  have gone on a power trip. He's, you know how Neli feels with the

1066
01:10:14,125 --> 01:10:17,725
Speaker 5:  lightning round. So he, he thinks this is a, a fantastic part of the show.

1067
01:10:17,845 --> 01:10:20,965
Speaker 5:  I have beef with it because there's a bunch of stories we wanna get to and

1068
01:10:20,965 --> 01:10:24,885
Speaker 5:  we never get to all the stories. And so I have destroyed the lightning round.

1069
01:10:25,035 --> 01:10:28,925
Speaker 5:  It's gone. It's done. It's never coming back until maybe Neela gets

1070
01:10:28,925 --> 01:10:32,625
Speaker 5:  back. We'll find out in place. We have declared

1071
01:10:32,695 --> 01:10:36,425
Speaker 5:  this the thunder round. This is the thunder round

1072
01:10:37,095 --> 01:10:40,225
Speaker 5:  instead of the lightning round, we're gonna do five stories, five minutes

1073
01:10:40,225 --> 01:10:43,825
Speaker 5:  each. I've given Eric Gomez our producer, the Power of Thunder. If we

1074
01:10:44,185 --> 01:10:47,745
Speaker 5:  go too long, we're gonna hear a little rolling thunder playing us off. And

1075
01:10:47,745 --> 01:10:51,225
Speaker 5:  if we need to move on, thunder will crash. Strike

1076
01:10:51,395 --> 01:10:54,745
Speaker 5:  smack will, there will be thunder, there will be a loud noise

1077
01:10:55,095 --> 01:10:58,945
Speaker 5:  telling us it is time to move on. Alright, let's get into it.

1078
01:10:59,225 --> 01:11:00,585
Speaker 5:  V what have you got for us?

1079
01:11:01,085 --> 01:11:04,865
Speaker 7:  So my new title at The Verge dot com is Senior

1080
01:11:05,035 --> 01:11:06,425
Speaker 7:  K-Pop Demon Hunters Reporter.

1081
01:11:07,285 --> 01:11:08,545
Speaker 5:  Congratulations in

1082
01:11:08,645 --> 01:11:10,185
Speaker 7:  The promotion. You, this is, this is, you know

1083
01:11:10,295 --> 01:11:11,585
Speaker 5:  This a long time coming. I

1084
01:11:11,585 --> 01:11:14,805
Speaker 7:  Was a senior reviewer then I was a senior curse tech reviewer, and now I

1085
01:11:14,805 --> 01:11:18,605
Speaker 7:  am senior K-Pop Demon Hunters reporter. And that's because

1086
01:11:18,935 --> 01:11:22,485
Speaker 7:  K-Pop Demon Hunters is officially the most ever

1087
01:11:22,755 --> 01:11:26,325
Speaker 7:  watched movie on Netflix most popular movie

1088
01:11:26,985 --> 01:11:27,645
Speaker 7:  of all time.

1089
01:11:28,065 --> 01:11:30,165
Speaker 5:  And people actually like it. It's a

1090
01:11:30,165 --> 01:11:31,485
Speaker 7:  Great movie. This is, this

1091
01:11:31,565 --> 01:11:35,165
Speaker 5:  Is the crazy, I cannot think of that for it Sold out real theaters.

1092
01:11:35,755 --> 01:11:36,605
Speaker 7:  Real theaters.

1093
01:11:36,955 --> 01:11:37,445
Speaker 5:  This is,

1094
01:11:37,675 --> 01:11:41,645
Speaker 7:  This is going, this is going to the Oscars. Like mark my words.

1095
01:11:41,965 --> 01:11:45,725
Speaker 7:  I think I I read that they are actually going to be nominating

1096
01:11:45,785 --> 01:11:48,605
Speaker 7:  the soundtrack for the Oscars. And

1097
01:11:49,985 --> 01:11:53,845
Speaker 7:  as they should, it is a banger of a soundtrack. And I'm not

1098
01:11:53,845 --> 01:11:57,445
Speaker 7:  at all biased as a K-pop stand, but it is a banger of a

1099
01:11:57,445 --> 01:12:01,365
Speaker 7:  soundtrack. And like the movie is actually, it's a reezy

1100
01:12:01,365 --> 01:12:03,765
Speaker 7:  90 minutes. It's, it's very good.

1101
01:12:04,395 --> 01:12:08,165
Speaker 5:  This is legitimately the first Netflix movie

1102
01:12:08,635 --> 01:12:11,965
Speaker 5:  that I've heard people talk about after the movie came out

1103
01:12:12,385 --> 01:12:14,405
Speaker 7:  And continue to talk about it. Yes,

1104
01:12:14,635 --> 01:12:18,605
Speaker 5:  It's always right, the they'll create buzz in the lead up to a movie. The

1105
01:12:18,605 --> 01:12:22,045
Speaker 5:  movie comes out, you don't hear a thing, you don't hear a, this is the only

1106
01:12:22,045 --> 01:12:24,805
Speaker 5:  one that, I didn't hear anything about this before it came out and

1107
01:12:25,555 --> 01:12:27,245
Speaker 5:  nonstop for the past like two months,

1108
01:12:27,375 --> 01:12:30,645
Speaker 7:  There was like almost no promotion there. I think there was one trailer

1109
01:12:31,185 --> 01:12:35,165
Speaker 7:  for it. They did not promote it at all. It came out all of a

1110
01:12:35,165 --> 01:12:38,885
Speaker 7:  sudden all my TikTok things were, And I actually didn't wanna watch it at

1111
01:12:38,885 --> 01:12:42,445
Speaker 7:  first because I like K-pop and because I'm Korean, I was like, oh, K-pop

1112
01:12:42,815 --> 01:12:44,885
Speaker 7:  demon hunters. Are you kidding?

1113
01:12:44,885 --> 01:12:45,885
Speaker 5:  Pandering. Kidding me.

1114
01:12:46,155 --> 01:12:49,965
Speaker 7:  Pandering. Yeah. And then just the, the, the feed was way too

1115
01:12:49,965 --> 01:12:52,885
Speaker 7:  much. And then all of the people were like, oh my God, it's based on this

1116
01:12:52,885 --> 01:12:56,805
Speaker 7:  panel. And I was like, all right, all right, let me go watch this movie.

1117
01:12:57,225 --> 01:12:58,605
Speaker 7:  And then I watched it three times.

1118
01:12:59,545 --> 01:13:02,205
Speaker 11:  That's one of my least favorite things when everybody's right. Like

1119
01:13:02,595 --> 01:13:05,165
Speaker 7:  It's just, it's just a good time.

1120
01:13:05,665 --> 01:13:09,405
Speaker 5:  The the singers in the who voice the, the characters,

1121
01:13:09,405 --> 01:13:10,485
Speaker 5:  they're actual K-pop

1122
01:13:10,775 --> 01:13:14,685
Speaker 7:  Performers. So ej, they're, they're not one of 'em.

1123
01:13:14,705 --> 01:13:18,645
Speaker 7:  One of them is Audrey Nuna and she's kind of a, a rapper. And

1124
01:13:18,645 --> 01:13:22,125
Speaker 7:  then the other one is Ray Ami, but the main one, ej,

1125
01:13:23,145 --> 01:13:26,165
Speaker 7:  she is a bonafide, she's what

1126
01:13:27,065 --> 01:13:30,965
Speaker 7:  you could call her the Benny Blanco of Korea because you know Benny

1127
01:13:30,965 --> 01:13:31,885
Speaker 7:  Blanco has written all

1128
01:13:31,905 --> 01:13:32,805
Speaker 5:  Of these. How is she on a podcast?

1129
01:13:33,425 --> 01:13:36,605
Speaker 7:  She speaks English, so she's probably very good. That's okay. But

1130
01:13:37,595 --> 01:13:40,965
Speaker 7:  basically she's written all of these major bangers

1131
01:13:41,425 --> 01:13:41,845
Speaker 7:  for these

1132
01:13:41,895 --> 01:13:43,645
Speaker 5:  These Oh, wild, okay. Huge bang. Yeah, yeah,

1133
01:13:43,645 --> 01:13:47,085
Speaker 7:  Yeah. These huge, that's hence the Benny Blanco thing. Like I found out that

1134
01:13:47,085 --> 01:13:50,485
Speaker 7:  she wrote this song Psycho by Red Velvet And I went, what? That's on all

1135
01:13:50,485 --> 01:13:54,245
Speaker 7:  my playlists. Holy crap girl. And she was a trainee

1136
01:13:54,265 --> 01:13:58,085
Speaker 7:  at one of the major K-pop studios. Sm

1137
01:13:58,215 --> 01:14:01,965
Speaker 7:  known for putting out a lot of

1138
01:14:02,405 --> 01:14:06,245
Speaker 7:  insanely talented vocalists, which she obviously is, but she

1139
01:14:06,245 --> 01:14:10,125
Speaker 7:  wrote all of the songs for the movie and performed in them. And so it

1140
01:14:10,125 --> 01:14:13,885
Speaker 7:  has that very authentic, genuine thing. But one of the most

1141
01:14:13,945 --> 01:14:17,885
Speaker 7:  insane parts of k-pop demon hunter's insane success to me

1142
01:14:18,505 --> 01:14:22,325
Speaker 7:  is that Sony, which made the, the, the, the thing, they basically

1143
01:14:22,325 --> 01:14:25,085
Speaker 7:  gave it to Netflix. 'cause they didn't think it was gonna do

1144
01:14:25,085 --> 01:14:27,925
Speaker 5:  Numbers. That's, they just had no idea what they had on their hands. They

1145
01:14:27,925 --> 01:14:28,045
Speaker 5:  had

1146
01:14:28,045 --> 01:14:31,045
Speaker 7:  No, this movie took nine years to make is the other thing. Oh my, it's like

1147
01:14:31,365 --> 01:14:31,725
Speaker 7:  a passion

1148
01:14:31,725 --> 01:14:32,605
Speaker 5:  Project. Gosh.

1149
01:14:32,875 --> 01:14:36,765
Speaker 7:  They put their whole heart and soul into like, representing Korea,

1150
01:14:37,765 --> 01:14:41,405
Speaker 7:  representing like this whole culture. Which is why I think it's

1151
01:14:41,405 --> 01:14:45,165
Speaker 7:  resonated a lot with K-Pop fans because there's just so much love put

1152
01:14:45,165 --> 01:14:49,085
Speaker 7:  into every single part of it and it's authentic and, but

1153
01:14:49,085 --> 01:14:52,845
Speaker 7:  it's still just like a good story. And like the visuals are

1154
01:14:52,845 --> 01:14:56,365
Speaker 7:  very like unique and fun and kind of not,

1155
01:14:56,875 --> 01:15:00,445
Speaker 7:  it's not Pixar, right? It, it kind of has the same vibe as the

1156
01:15:00,725 --> 01:15:02,965
Speaker 7:  Spider-Man animated movies. So there's

1157
01:15:03,105 --> 01:15:04,605
Speaker 5:  The animation looks beautiful, it's

1158
01:15:04,605 --> 01:15:08,245
Speaker 7:  Really fun and it's like got a di di distinctive style. And

1159
01:15:08,385 --> 01:15:12,205
Speaker 7:  one of the things I see people saying online that I agree with is that for

1160
01:15:12,205 --> 01:15:15,965
Speaker 7:  once it's original, yes. It's like an original story. It's

1161
01:15:16,165 --> 01:15:19,285
Speaker 7:  original characters. It's not a remake of a remake of a remake, it's just

1162
01:15:19,285 --> 01:15:22,405
Speaker 7:  something new that's out there and it's gonna be done to death in the same

1163
01:15:22,405 --> 01:15:26,205
Speaker 7:  view as squid games. But yeah, just the

1164
01:15:26,205 --> 01:15:29,885
Speaker 7:  fact that Sony needs a hit and they're like, I don't know what to do with

1165
01:15:29,885 --> 01:15:32,205
Speaker 7:  this here. Netflix, you take it and then Netflix is like,

1166
01:15:32,475 --> 01:15:35,645
Speaker 5:  This explains a lot. I wanna ask something really quick of both of you.

1167
01:15:36,705 --> 01:15:40,525
Speaker 5:  Can you both try to name one other movie in Netflix's

1168
01:15:40,525 --> 01:15:44,485
Speaker 5:  top 10 of their original films? One other movie

1169
01:15:44,515 --> 01:15:47,365
Speaker 5:  that Netflix has made that was was Successful

1170
01:15:47,955 --> 01:15:48,645
Speaker 7:  Bird Box.

1171
01:15:49,275 --> 01:15:53,015
Speaker 5:  Okay. Okay. That's one of them. That's one of them. That's number six. Richard,

1172
01:15:53,085 --> 01:15:55,815
Speaker 5:  have you, do you have ever any Netflix original movies?

1173
01:15:56,155 --> 01:16:00,055
Speaker 11:  Oh, only because I saw the list. Do I know read notice. The, the movie

1174
01:16:00,055 --> 01:16:01,975
Speaker 11:  that made no one admits to of having watched I

1175
01:16:02,725 --> 01:16:03,015
Speaker 5:  I've

1176
01:16:03,135 --> 01:16:03,655
Speaker 7:  Never heard of it.

1177
01:16:04,415 --> 01:16:08,055
Speaker 5:  I constantly am like, I'm like, it's like gray

1178
01:16:08,115 --> 01:16:11,815
Speaker 5:  notice 'cause they have, they have Gray Man and red notice And I can't remember.

1179
01:16:11,915 --> 01:16:15,375
Speaker 5:  I'm like, that's all I can re remember remember as far as Netflix originals

1180
01:16:15,375 --> 01:16:17,465
Speaker 5:  go, but this is it. This is like,

1181
01:16:19,605 --> 01:16:23,225
Speaker 5:  nobody talks about 'em. The only thing that matters is K-pop demon

1182
01:16:23,225 --> 01:16:26,265
Speaker 5:  unders. But at least they've got a hit. Alright, Richard, what have you got

1183
01:16:26,265 --> 01:16:26,465
Speaker 5:  for us?

1184
01:16:27,805 --> 01:16:31,265
Speaker 11:  Pretty big controversy going on in the YouTube world. Surprisingly never

1185
01:16:31,265 --> 01:16:35,145
Speaker 11:  happens. Several creators are upset because they, they noticed

1186
01:16:35,525 --> 01:16:38,305
Speaker 11:  and they've started talking about how some of their YouTube Shorts videos

1187
01:16:39,455 --> 01:16:43,265
Speaker 11:  look different. Brett Shaw I think had made a video about this.

1188
01:16:43,565 --> 01:16:46,865
Speaker 11:  It was reported on the BBC and, and in other places. And YouTube

1189
01:16:47,055 --> 01:16:50,905
Speaker 11:  responded. Basically what what they noticed, particularly comparing

1190
01:16:50,905 --> 01:16:54,825
Speaker 11:  the YouTube Shorts version, I think to Instagram reels, is that the

1191
01:16:54,825 --> 01:16:58,545
Speaker 11:  versions of their videos that were on YouTube had

1192
01:16:58,625 --> 01:17:02,305
Speaker 11:  these extra details and looked really absurdly sharp and

1193
01:17:02,415 --> 01:17:05,665
Speaker 11:  they, they felt like, it looked like, it looked like an AI deep fake is,

1194
01:17:05,685 --> 01:17:09,405
Speaker 11:  is how they kind of felt like their videos looked. And YouTube's Renee

1195
01:17:09,485 --> 01:17:12,805
Speaker 11:  Ritchie, he responded, he said that it was an experiment that YouTube was

1196
01:17:12,805 --> 01:17:16,005
Speaker 11:  running as far as I can tell, they hadn't told anyone about this beforehand.

1197
01:17:17,515 --> 01:17:20,775
Speaker 11:  And that they are using the kind of machine learning you experience with

1198
01:17:20,775 --> 01:17:24,455
Speaker 11:  computational photography on smartphones, not generative ai. This is

1199
01:17:24,605 --> 01:17:28,415
Speaker 11:  kind of the push and pull that is going back and forth between people saying

1200
01:17:28,415 --> 01:17:32,295
Speaker 11:  it's AI or not ai. So if you've, if you've taken a look at the evidence,

1201
01:17:32,445 --> 01:17:34,935
Speaker 11:  what do you guys think? AI or just upscaling?

1202
01:17:35,695 --> 01:17:35,775
Speaker 7:  Hmm.

1203
01:17:37,235 --> 01:17:40,855
Speaker 5:  It, it looks like nasty upscaling to me. Like

1204
01:17:41,045 --> 01:17:44,735
Speaker 5:  this is just a failure on YouTube's part in general. It looks they,

1205
01:17:44,955 --> 01:17:48,175
Speaker 5:  you know what they look like. They looked like ripped videos

1206
01:17:48,875 --> 01:17:52,615
Speaker 5:  on TikTok that somebody has like put an ad on top of

1207
01:17:53,005 --> 01:17:56,815
Speaker 5:  like, I've seen this look. And it is when somebody is boosting your

1208
01:17:56,815 --> 01:18:00,135
Speaker 5:  video and trying to hide that they have like downloaded off something else.

1209
01:18:00,205 --> 01:18:00,495
Speaker 5:  Know

1210
01:18:00,495 --> 01:18:04,215
Speaker 7:  What that's, I had to go onto a Zoom call earlier today on the

1211
01:18:04,395 --> 01:18:08,215
Speaker 7:  AI phone And I don't know if it was Zoom or something else, but I

1212
01:18:08,215 --> 01:18:11,055
Speaker 7:  looked at what I looked like in the camera. I'm like, oh no,

1213
01:18:12,075 --> 01:18:16,045
Speaker 7:  that's too much like me. Like it just, you know, it was

1214
01:18:16,045 --> 01:18:19,285
Speaker 7:  just sharpened to the point where I was like, it's uncanny, it doesn't look

1215
01:18:19,285 --> 01:18:21,245
Speaker 7:  right. And so I turned my camera off.

1216
01:18:21,885 --> 01:18:25,365
Speaker 5:  I Do you think there's this like very funny, weird thing happening where

1217
01:18:25,365 --> 01:18:29,005
Speaker 5:  people are like, I don't like it. It must be ai and and

1218
01:18:29,465 --> 01:18:33,365
Speaker 5:  the thing is this is actually just bad. It doesn't matter what it is, it

1219
01:18:33,365 --> 01:18:36,285
Speaker 5:  is just bad. Like they just shouldn't do this.

1220
01:18:36,475 --> 01:18:39,485
Speaker 11:  Yeah. It, it didn't look good. They didn't tell people that this was happening

1221
01:18:39,485 --> 01:18:43,245
Speaker 11:  to their videos. The select number of videos they use this on.

1222
01:18:43,465 --> 01:18:47,445
Speaker 11:  But yeah, I've been sitting in demos at trade shows with

1223
01:18:48,245 --> 01:18:51,525
Speaker 11:  companies showing off their HD upscaling that would make your four a DP

1224
01:18:51,675 --> 01:18:55,405
Speaker 11:  into 10 80 p for years. And this is what it looks like. Yes. If someone

1225
01:18:55,825 --> 01:18:59,485
Speaker 11:  did that and turned the dial up and they all, they all claim every TV claim

1226
01:18:59,485 --> 01:19:03,445
Speaker 11:  that it had an AI processor in it for the last 10 years. So is

1227
01:19:03,445 --> 01:19:07,365
Speaker 11:  it ai, is it machine learning? What is the difference? I do not know,

1228
01:19:08,105 --> 01:19:11,165
Speaker 11:  but it's, it just, it looks way over sharpened. They, they turned it up

1229
01:19:11,165 --> 01:19:15,085
Speaker 11:  too much. And I think that the other part is how all of the

1230
01:19:15,085 --> 01:19:18,765
Speaker 11:  tests and all of the features that YouTube is adding, we, we saw some backlash.

1231
01:19:19,025 --> 01:19:22,525
Speaker 11:  We saw the thing pop up. There's a woman who was kind of upset because she

1232
01:19:22,525 --> 01:19:26,365
Speaker 11:  noticed on Instagram reels that her voice translation, it translates

1233
01:19:26,365 --> 01:19:30,345
Speaker 11:  it into another language, but also simulates your voice, which I

1234
01:19:30,345 --> 01:19:34,165
Speaker 11:  think people were not ready for. So whenever you see

1235
01:19:34,165 --> 01:19:37,205
Speaker 11:  something and it's altered, of course for many people, the first question's

1236
01:19:37,205 --> 01:19:40,165
Speaker 11:  gonna be, oh, it's ai it's just where we are now.

1237
01:19:40,385 --> 01:19:43,725
Speaker 5:  No, that's a good point though. And like that, that did happen and Instagram

1238
01:19:43,725 --> 01:19:47,125
Speaker 5:  really was doing that and like, it was kind of impressive.

1239
01:19:47,545 --> 01:19:51,365
Speaker 5:  But like you, it was probably good. Good to know that that's gonna happen

1240
01:19:51,365 --> 01:19:55,045
Speaker 5:  before you see yourself talking in another language. I was like

1241
01:19:55,285 --> 01:19:58,685
Speaker 5:  watching those and like, is it even sinking the lips? Like I Yep.

1242
01:19:59,115 --> 01:20:02,965
Speaker 5:  It's trippy. It's really trippy and, and super

1243
01:20:02,965 --> 01:20:05,925
Speaker 5:  impressive. I'm sure if you're a creator, like seems good if you want more

1244
01:20:05,925 --> 01:20:09,885
Speaker 5:  reach, but like, you wanna know what's happening with your videos.

1245
01:20:10,505 --> 01:20:13,685
Speaker 5:  And there's this other element here where like YouTube is

1246
01:20:13,835 --> 01:20:17,325
Speaker 5:  acknowledging that it's machine learning and it's like, oh God, where is

1247
01:20:17,325 --> 01:20:20,205
Speaker 5:  the difference between machine learning and ai and which is the one that

1248
01:20:20,205 --> 01:20:23,445
Speaker 5:  we're mad at and which is the one that we, and like my whole thing is like,

1249
01:20:23,445 --> 01:20:27,245
Speaker 5:  what is the outcome here? And the outcome here is they made

1250
01:20:27,295 --> 01:20:30,925
Speaker 5:  weird crunchy videos and didn't tell anybody that they were gonna make 'em

1251
01:20:30,925 --> 01:20:34,165
Speaker 5:  weird and crunchy and everybody was just mad that they looked ugly.

1252
01:20:35,675 --> 01:20:39,355
Speaker 7:  I mean that's the, the real crux of it is just like, if you're gonna do this,

1253
01:20:39,725 --> 01:20:43,435
Speaker 7:  don't make me look ugly. Right? That's vain creatures.

1254
01:20:43,815 --> 01:20:47,635
Speaker 7:  If I looked better, I'd be like, oh, thank you ai. Right? I look

1255
01:20:47,635 --> 01:20:48,075
Speaker 7:  crunchy.

1256
01:20:48,415 --> 01:20:50,995
Speaker 11:  You have these creators who worked really hard to make their videos look

1257
01:20:50,995 --> 01:20:54,035
Speaker 11:  the way that they look. They, they have expectations. They know how YouTube

1258
01:20:54,035 --> 01:20:56,915
Speaker 11:  works, they know how they'll be delivered on different platforms and they've

1259
01:20:56,915 --> 01:21:00,715
Speaker 11:  gone to a great degree to get it to look this way. And then suddenly

1260
01:21:00,715 --> 01:21:01,595
Speaker 11:  you change it. Yeah.

1261
01:21:01,595 --> 01:21:05,315
Speaker 5:  At first when I heard they were secretly AIing videos, I'm like, oh my God,

1262
01:21:05,335 --> 01:21:09,155
Speaker 5:  did they face tune people? 'cause that's creepy. That's messed up. No,

1263
01:21:09,185 --> 01:21:13,115
Speaker 5:  they, they took the clarity slider and dragged it all the way to the top.

1264
01:21:14,165 --> 01:21:17,515
Speaker 5:  Don't do that. Okay, next story. Google

1265
01:21:17,625 --> 01:21:20,995
Speaker 5:  announced this week that they're gonna start requiring

1266
01:21:22,195 --> 01:21:26,075
Speaker 5:  verification for developers who want to publish

1267
01:21:26,985 --> 01:21:30,355
Speaker 5:  apps on Android that live outside the play store.

1268
01:21:31,265 --> 01:21:35,235
Speaker 5:  This I think doesn't sound monumental, but it

1269
01:21:35,235 --> 01:21:38,915
Speaker 5:  kind of is, kind of is, right? This is essentially how Apple

1270
01:21:38,915 --> 01:21:42,635
Speaker 5:  operates the Mac, right? Anybody can publish a Mac app,

1271
01:21:43,135 --> 01:21:47,035
Speaker 5:  but it's a pretty, it it's secretly a lockdown platform, which is, it makes

1272
01:21:47,035 --> 01:21:50,995
Speaker 5:  sense, it's sort, that's how Apple has always kind of kept its

1273
01:21:51,525 --> 01:21:55,515
Speaker 5:  gates tight. But if you wanna publish outside of the app store in the

1274
01:21:55,515 --> 01:21:58,115
Speaker 5:  Mac, as people traditionally have, you still have to have a Dev Apple developer

1275
01:21:58,355 --> 01:22:01,315
Speaker 5:  count. Apple's always require these developer counts. And So

1276
01:22:02,485 --> 01:22:06,025
Speaker 5:  if you, you know, go awry and do something

1277
01:22:06,225 --> 01:22:08,945
Speaker 5:  illegitimate, they can just shut down your app.

1278
01:22:10,325 --> 01:22:12,855
Speaker 5:  This makes a little bit of sense to me on the Mac where Apple has always

1279
01:22:12,855 --> 01:22:16,815
Speaker 5:  had this level of control. Android on the other hand, is ostensibly

1280
01:22:16,845 --> 01:22:20,735
Speaker 5:  this open platform is ostensibly a platform that is open source

1281
01:22:20,845 --> 01:22:23,775
Speaker 5:  that nobody controls. Now Google is saying

1282
01:22:25,385 --> 01:22:29,285
Speaker 5:  it is an open source platform, however, we would like every developer to

1283
01:22:29,485 --> 01:22:33,365
Speaker 5:  register with us and we will therefore be able to shut off your app if

1284
01:22:33,835 --> 01:22:37,805
Speaker 5:  it's a problem. Now, on one hand that, okay, great, that's a good security

1285
01:22:37,805 --> 01:22:41,645
Speaker 5:  feature, right? I see the reason for it. I see how

1286
01:22:41,645 --> 01:22:45,325
Speaker 5:  that makes sense. But to me, the other thing I see here

1287
01:22:45,745 --> 01:22:49,685
Speaker 5:  is Google continuing to tighten its grip on Android and make it feel

1288
01:22:49,755 --> 01:22:53,405
Speaker 5:  even more and more like a product that belongs solely to Google.

1289
01:22:53,905 --> 01:22:57,685
Speaker 5:  And that's sort of been the entire trajectory of Android. I feel like

1290
01:22:57,835 --> 01:22:59,445
Speaker 5:  over the past, you know, decade plus.

1291
01:23:00,845 --> 01:23:04,805
Speaker 7:  I think that's only gonna get more true the more that they try and

1292
01:23:04,825 --> 01:23:08,765
Speaker 7:  tie Android to Gemini. You know, like I think as that

1293
01:23:08,765 --> 01:23:12,685
Speaker 7:  happens, as Gemini becomes another like vector in

1294
01:23:12,685 --> 01:23:16,605
Speaker 7:  which Google tries to assert itself into products, I think

1295
01:23:17,745 --> 01:23:20,665
Speaker 7:  I think you're gonna see more of this going forward. Yeah.

1296
01:23:20,665 --> 01:23:24,025
Speaker 11:  And, And I don't think this is the reason why everyone who uses Android

1297
01:23:24,025 --> 01:23:27,985
Speaker 11:  uses Android, but there was in the past a reason to use Android because

1298
01:23:28,405 --> 01:23:31,705
Speaker 11:  you could have more control of your device. You could install apps from

1299
01:23:31,905 --> 01:23:35,625
Speaker 11:  anywhere you could, you know, do do what you wanted. And that is just,

1300
01:23:35,695 --> 01:23:39,185
Speaker 11:  it's a bit, it's becoming a bit less true. And we saw kind of an example

1301
01:23:39,185 --> 01:23:42,985
Speaker 11:  of what this could become. Just, I think in the last

1302
01:23:43,085 --> 01:23:46,905
Speaker 11:  day Apple pulled a a, a Torrent app for the iPhone that was

1303
01:23:46,905 --> 01:23:49,925
Speaker 11:  published in the alt store in Europe. But they can do that

1304
01:23:50,065 --> 01:23:53,605
Speaker 5:  And, and it's like, it's not clear why. And you know, like we, we can ask

1305
01:23:53,605 --> 01:23:57,125
Speaker 5:  them, but they don't have to answer. They're like, you know, they can still

1306
01:23:57,125 --> 01:24:00,365
Speaker 5:  just enforce their arbitrary rules and look like,

1307
01:24:01,265 --> 01:24:05,085
Speaker 5:  is that app being used for piracy? A hundred percent. A hundred percent.

1308
01:24:06,545 --> 01:24:10,325
Speaker 5:  Should Apple be in charge of deciding whether you can

1309
01:24:10,385 --> 01:24:14,365
Speaker 5:  pirate stuff or not? Like I, I don't think that's the right

1310
01:24:14,395 --> 01:24:18,165
Speaker 5:  move here. And like, this is sort of the direction this takes where

1311
01:24:18,185 --> 01:24:20,885
Speaker 5:  Google now is going to have that authority.

1312
01:24:21,745 --> 01:24:25,525
Speaker 7:  And it's also different if you, from the get go, like Apple from the get

1313
01:24:25,525 --> 01:24:29,245
Speaker 7:  go is like, no, we, we have the control. But Google has historically

1314
01:24:29,315 --> 01:24:33,045
Speaker 7:  been like, we're, we love open source. We like being that

1315
01:24:33,555 --> 01:24:37,285
Speaker 7:  kind of more interoperable company. And to

1316
01:24:37,285 --> 01:24:41,245
Speaker 7:  Richard's point, like who among us doesn't know the, the Android stand?

1317
01:24:41,245 --> 01:24:44,245
Speaker 7:  And I say this lovingly who's just like, oh, I just sideloaded onto this.

1318
01:24:44,315 --> 01:24:47,885
Speaker 7:  Like I can just do whatever I want with this versus you

1319
01:24:47,905 --> 01:24:51,885
Speaker 7:  iPhone people enjoy your prison in the walled garden or whatnot.

1320
01:24:51,985 --> 01:24:55,845
Speaker 7:  And so I think that when you position yourself

1321
01:24:55,945 --> 01:24:59,775
Speaker 7:  one way and then you pivot and people

1322
01:24:59,775 --> 01:25:03,535
Speaker 7:  start noticing that pivot, that's when people get mad. It's like when

1323
01:25:03,535 --> 01:25:07,295
Speaker 7:  you're a free app and then all of a sudden you're like, hey guys,

1324
01:25:08,415 --> 01:25:12,375
Speaker 7:  subscription. Even if it's a baby step, even if it's like a small thing

1325
01:25:12,435 --> 01:25:16,295
Speaker 7:  in a direction, you just get a lot of outlash be a

1326
01:25:16,495 --> 01:25:18,775
Speaker 7:  backlash rather because people are like, that's not what you said you were,

1327
01:25:19,035 --> 01:25:22,295
Speaker 7:  that's not what you said. That's not what we signed up for and now you're

1328
01:25:22,545 --> 01:25:25,775
Speaker 7:  going the other way. So it's slippery slope. Very

1329
01:25:25,895 --> 01:25:28,455
Speaker 5:  Slippery. I I think you're completely right and I've talked about this before,

1330
01:25:28,875 --> 01:25:32,375
Speaker 5:  but it's fascinating. If you look at Google's contracts with other,

1331
01:25:32,675 --> 01:25:36,415
Speaker 5:  you know, smartphone makers, they do everything to lock down

1332
01:25:36,525 --> 01:25:39,575
Speaker 5:  Android to make sure it's a Google product. Even though it's open source.

1333
01:25:39,685 --> 01:25:43,495
Speaker 5:  They say, if you want the play store, if you want our best apps, you

1334
01:25:43,495 --> 01:25:47,295
Speaker 5:  gotta put them in this folder on this screen in this spot. They're super

1335
01:25:47,535 --> 01:25:50,775
Speaker 5:  specific about all of this. And so, you know, I think we don't quite see

1336
01:25:50,775 --> 01:25:54,415
Speaker 5:  it all the time, but there are just a lot of different ways that app

1337
01:25:54,655 --> 01:25:58,455
Speaker 5:  that Google very clearly keeps its hands around Android

1338
01:25:58,455 --> 01:26:01,855
Speaker 5:  to make sure that it is a Google product, even though it remains

1339
01:26:01,855 --> 01:26:05,645
Speaker 5:  technically open source. So you'll still be able to side load this is still

1340
01:26:05,645 --> 01:26:09,285
Speaker 5:  gonna be an option. I do think like, yes, this is a good security feature,

1341
01:26:09,745 --> 01:26:12,845
Speaker 5:  but it it, it has some very, I think, strange

1342
01:26:13,005 --> 01:26:16,525
Speaker 5:  implications for the future of Android as an open platform.

1343
01:26:17,515 --> 01:26:19,445
Speaker 5:  Alright, the what's next?

1344
01:26:20,945 --> 01:26:24,645
Speaker 7:  So I, I really love this one, but Taco Bell, an

1345
01:26:24,645 --> 01:26:28,525
Speaker 7:  executive there is just like, hmm. So, you know, our plan to put AI in the

1346
01:26:28,525 --> 01:26:29,205
Speaker 7:  drive-throughs.

1347
01:26:31,255 --> 01:26:33,965
Speaker 7:  Maybe I have second thoughts about doing that because

1348
01:26:34,875 --> 01:26:38,645
Speaker 7:  there's a lot of people complaining on social media and a lot of people

1349
01:26:38,665 --> 01:26:42,605
Speaker 7:  trying to troll the AI on social media by asking for,

1350
01:26:43,245 --> 01:26:45,365
Speaker 7:  I believe 18,000 cups of water.

1351
01:26:48,065 --> 01:26:51,565
Speaker 7:  And so he, he tells the wa the Wall Street Journal, we're learning a lot.

1352
01:26:52,025 --> 01:26:55,445
Speaker 7:  I'm gonna be honest with you. I think like everybody, sometimes

1353
01:26:55,905 --> 01:26:59,845
Speaker 7:  it lets me down, but sometimes it really surprises me, which

1354
01:26:59,845 --> 01:27:03,485
Speaker 7:  is, you know, I don't wanna go to a drive-through and talk to an AI and get

1355
01:27:03,525 --> 01:27:06,605
Speaker 7:  a surprise. So I just,

1356
01:27:07,395 --> 01:27:11,045
Speaker 7:  this tickles me because it's just, the other thing he says is that

1357
01:27:11,795 --> 01:27:15,685
Speaker 7:  he's discovered that using AI exclusively in certain situations, like a drive-through

1358
01:27:15,705 --> 01:27:19,685
Speaker 7:  for quote super busy restaurants with long lines might

1359
01:27:19,685 --> 01:27:22,765
Speaker 7:  not be such a good idea after all. Which

1360
01:27:24,275 --> 01:27:25,075
Speaker 7:  I don't know, you think?

1361
01:27:25,865 --> 01:27:28,475
Speaker 5:  Yeah, Richard Drive through ai thumbs up, thumbs down

1362
01:27:30,875 --> 01:27:32,055
Speaker 11:  Co. Could we just have buttons?

1363
01:27:33,725 --> 01:27:34,015
Speaker 11:  Like

1364
01:27:35,595 --> 01:27:38,695
Speaker 11:  you, you have, you offer like 10 things. Just let me press the button and

1365
01:27:38,695 --> 01:27:41,655
Speaker 11:  then we we can do it. Yeah. But yeah,

1366
01:27:42,575 --> 01:27:46,215
Speaker 11:  I think it's not that hard to imagine that a bored teenager

1367
01:27:46,355 --> 01:27:50,015
Speaker 11:  in many cases or basically anyone just ha

1368
01:27:50,015 --> 01:27:53,455
Speaker 11:  happening to do this job is better equipped to deal with someone

1369
01:27:53,765 --> 01:27:56,975
Speaker 11:  walking up to the, you know, driving up to the window and saying, yeah,

1370
01:27:56,975 --> 01:28:00,175
Speaker 11:  I want 18,000 cups of water than an AI is.

1371
01:28:01,125 --> 01:28:05,095
Speaker 11:  It's just not, it's just not going to work. Like also they might fight you

1372
01:28:05,685 --> 01:28:08,285
Speaker 11:  underwriting waffle, waffle of having waffle human employed Waffle House.

1373
01:28:08,285 --> 01:28:08,485
Speaker 11:  Could you,

1374
01:28:08,935 --> 01:28:10,165
Speaker 7:  Would you imagine you're

1375
01:28:10,165 --> 01:28:11,525
Speaker 11:  Not, you're not going to do that at Waffle House.

1376
01:28:11,525 --> 01:28:14,525
Speaker 7:  You're not doing that at Waffle House, waffle House. They, have you ever

1377
01:28:14,525 --> 01:28:17,325
Speaker 7:  seen that video of the lady at the Waffle House and they throw the chair

1378
01:28:17,325 --> 01:28:21,285
Speaker 7:  at her and she's like, not on my watch and she just pulls the chair out

1379
01:28:21,285 --> 01:28:24,005
Speaker 7:  of the air. Yes. Yeah, you're not gonna do that at Waffle House.

1380
01:28:24,525 --> 01:28:27,565
Speaker 5:  I, this is such a fascinating use case. 'cause like I, on one hand,

1381
01:28:28,305 --> 01:28:32,285
Speaker 5:  you know, could, could an AI handle a drive-through order? Like probably

1382
01:28:32,675 --> 01:28:36,205
Speaker 5:  with, with some frequency it could, it could do that correctly.

1383
01:28:37,265 --> 01:28:40,845
Speaker 5:  But this is like a business that serves customers and needs to do things

1384
01:28:40,915 --> 01:28:43,365
Speaker 5:  correctly, like all the time quickly

1385
01:28:43,585 --> 01:28:43,805
Speaker 7:  Too.

1386
01:28:44,065 --> 01:28:47,965
Speaker 5:  Yes. And it's one of those things where like, okay, great, if AI

1387
01:28:48,105 --> 01:28:51,965
Speaker 5:  screws up a response to me one in 10

1388
01:28:51,965 --> 01:28:55,805
Speaker 5:  times, like it's not, it's not, it's annoying, it's not great, but it's not

1389
01:28:55,805 --> 01:28:59,605
Speaker 5:  a big deal. It's just talking to me. I see that it's messed up. I can deal

1390
01:28:59,605 --> 01:29:03,525
Speaker 5:  with it if it's in a drive through d dealing

1391
01:29:03,555 --> 01:29:07,365
Speaker 5:  with th thousands of customers a day, like you can't

1392
01:29:07,365 --> 01:29:11,325
Speaker 5:  have that error rate. Like AI's error rate is not ready for this.

1393
01:29:11,355 --> 01:29:15,005
Speaker 5:  This is like precisely one of those situations where a company is like

1394
01:29:15,845 --> 01:29:18,645
Speaker 5:  automate, we can make a ton of money. CEO I'm gonna sound cool, my earnings

1395
01:29:18,645 --> 01:29:22,565
Speaker 5:  call saying ai, ai, ai. And then you put it into use and it's

1396
01:29:22,565 --> 01:29:26,425
Speaker 5:  like, oh, like even if this works most of the time, like it needs

1397
01:29:26,425 --> 01:29:27,385
Speaker 5:  to work all of it.

1398
01:29:27,665 --> 01:29:30,905
Speaker 7:  Well, you like, I sometimes when, when Big Tech talks about things that way,

1399
01:29:31,085 --> 01:29:34,865
Speaker 7:  I'm just like, did they seem so pure? Yeah.

1400
01:29:34,885 --> 01:29:38,785
Speaker 7:  Inside. Because I was like, did you give it to a troll?

1401
01:29:39,035 --> 01:29:42,865
Speaker 7:  Right. Did you troll test this for 'cause

1402
01:29:42,865 --> 01:29:46,025
Speaker 7:  there it only takes one jerk to break the whole thing. And they're like,

1403
01:29:46,025 --> 01:29:49,785
Speaker 7:  it'll be great. It'll do this. The ideal situation will happen and we're

1404
01:29:49,785 --> 01:29:52,705
Speaker 7:  helping everyone. And it's like, did you give it to a 13-year-old

1405
01:29:53,965 --> 01:29:57,545
Speaker 7:  who, who really lives for the shits and giggles? Because

1406
01:29:58,165 --> 01:30:02,065
Speaker 7:  if it can't survive that it's, it's just not gonna survive the

1407
01:30:02,065 --> 01:30:05,985
Speaker 7:  real world. And if you're taco be the only people I know

1408
01:30:05,985 --> 01:30:09,665
Speaker 7:  who go to Taco Bell are probably having the munchies and or really

1409
01:30:09,665 --> 01:30:13,385
Speaker 7:  hungry. And so if your AI is not working, they have the

1410
01:30:13,385 --> 01:30:17,145
Speaker 7:  munchies, they're in a drive through line, they have road rage and they're

1411
01:30:17,145 --> 01:30:20,945
Speaker 7:  hangry. This is not the ideal Yeah. Situation for,

1412
01:30:21,085 --> 01:30:24,785
Speaker 7:  for an ai. And I, I kind of feel bad for the, not that AI has

1413
01:30:24,785 --> 01:30:28,585
Speaker 7:  feelings, but I kind of feel bad for it in, in, in one sense. And then my

1414
01:30:28,585 --> 01:30:32,385
Speaker 7:  other spiel is that, you know, whenever I call an insurance

1415
01:30:32,385 --> 01:30:36,265
Speaker 7:  company and they put me in the AI like robot tree And I have to listen

1416
01:30:36,285 --> 01:30:39,705
Speaker 7:  to the dumb thing And I'm just going, let me talk to a human,

1417
01:30:40,525 --> 01:30:44,465
Speaker 7:  you know, how is that not the same thing? It be the

1418
01:30:44,465 --> 01:30:46,545
Speaker 5:  Drive through, you just yell zero at the ai.

1419
01:30:46,545 --> 01:30:50,385
Speaker 7:  Yeah, And I would just be like, human, human just, I want a

1420
01:30:50,455 --> 01:30:54,345
Speaker 7:  taco mild sauce please. Chalupa.

1421
01:30:54,695 --> 01:30:58,585
Speaker 7:  Like, it's, it's, it's, it's not, it's not. I just want

1422
01:30:58,585 --> 01:30:59,185
Speaker 7:  a chalupa.

1423
01:30:59,985 --> 01:31:03,945
Speaker 5:  I i it makes a lot of sense that Taco Bell's customer

1424
01:31:03,945 --> 01:31:07,745
Speaker 5:  base immediately made them regret this, right? Yeah. And I appreciate they

1425
01:31:07,745 --> 01:31:08,985
Speaker 5:  did good work here. Live

1426
01:31:09,015 --> 01:31:10,385
Speaker 7:  Moss, you know, live,

1427
01:31:13,725 --> 01:31:15,945
Speaker 5:  Unless you're the AI at Taco Bell, in which case

1428
01:31:17,685 --> 01:31:21,385
Speaker 5:  Liv Mayos I think, and that is

1429
01:31:21,485 --> 01:31:25,065
Speaker 5:  the limit of my Spanish. Richard, wrap us up. What have you got?

1430
01:31:25,895 --> 01:31:29,465
Speaker 11:  This one came from Andrew Hawkins. We have Robo Mart.

1431
01:31:29,615 --> 01:31:33,585
Speaker 11:  They have introduced a new delivery robot that aims to shake up

1432
01:31:33,585 --> 01:31:37,025
Speaker 11:  autonomous delivery. It, it is essentially, remember when

1433
01:31:37,365 --> 01:31:39,825
Speaker 11:  Amazon introduced those lockers where you could get something delivered?

1434
01:31:39,825 --> 01:31:42,265
Speaker 11:  Like if you didn't want to it to be delivered to your house and sit outside

1435
01:31:42,265 --> 01:31:45,265
Speaker 11:  all day, you can maybe have it delivered to the locker, get a code or something

1436
01:31:45,885 --> 01:31:48,985
Speaker 11:  and you just go pick it up. So what if the locker drove to your house?

1437
01:31:49,765 --> 01:31:49,985
Speaker 7:  No,

1438
01:31:51,185 --> 01:31:55,095
Speaker 5:  I, I can't tell if this is brilliant or the dumbest thing I've ever heard

1439
01:31:55,095 --> 01:31:55,215
Speaker 5:  of.

1440
01:31:55,245 --> 01:31:58,255
Speaker 11:  It's both. That is exactly what I said. I I, it's, it's one of those two

1441
01:31:58,255 --> 01:32:00,685
Speaker 11:  things And I dunno know, it might be both of them all at once.

1442
01:32:00,875 --> 01:32:04,045
Speaker 5:  It's both. I think the future is just lockers

1443
01:32:04,495 --> 01:32:08,325
Speaker 5:  everywhere for everything. People love the lockers. I love the lockers.

1444
01:32:08,705 --> 01:32:11,205
Speaker 5:  I'm like, Ooh, a locker, I could pick something. I don't have anything to

1445
01:32:11,205 --> 01:32:13,285
Speaker 5:  pick up. It could have something

1446
01:32:14,825 --> 01:32:18,125
Speaker 11:  The size of a shuttle bus and can carry up to 500 pounds. What? You've gotta

1447
01:32:18,125 --> 01:32:21,805
Speaker 5:  Look at this nature. What? No, no. Oh my God. It's, I thought, sorry,

1448
01:32:22,085 --> 01:32:25,825
Speaker 5:  I thought this was a small robot, not a

1449
01:32:25,855 --> 01:32:28,965
Speaker 5:  full bus sized vehicle. Oh my god. Yeah.

1450
01:32:28,965 --> 01:32:32,165
Speaker 11:  It's like a sprinter van, you know, just rolling around full of it's

1451
01:32:32,885 --> 01:32:32,965
Speaker 11:  whatever.

1452
01:32:33,635 --> 01:32:37,485
Speaker 5:  Does this make any sense? No, I don't. But the problem is,

1453
01:32:37,485 --> 01:32:41,405
Speaker 5:  this requires what happens if the person is the, the thing with the

1454
01:32:41,405 --> 01:32:44,725
Speaker 5:  locker. The locker sits there, it waits for you.

1455
01:32:45,905 --> 01:32:49,885
Speaker 5:  If I'm not at home and the locker shows up, is this,

1456
01:32:49,885 --> 01:32:50,325
Speaker 5:  well, the locker

1457
01:32:50,335 --> 01:32:51,165
Speaker 11:  Could come to your office,

1458
01:32:52,075 --> 01:32:52,975
Speaker 5:  But is it gonna, like,

1459
01:32:53,165 --> 01:32:55,135
Speaker 7:  It's not useful for, for my groceries

1460
01:32:55,535 --> 01:32:59,355
Speaker 5:  Parallel park and try to like, wait for me, like

1461
01:32:59,585 --> 01:33:02,635
Speaker 5:  what happens to the person who is like 10 lockers in? What,

1462
01:33:02,635 --> 01:33:06,515
Speaker 11:  What I loved was one of the quotes on their website from the mar from Mars

1463
01:33:06,515 --> 01:33:09,755
Speaker 11:  unattended retail. This partnership helps solve the industry's biggest issue.

1464
01:33:10,175 --> 01:33:13,515
Speaker 11:  Ice cream melting before reaching the home when purchased in store or through

1465
01:33:13,515 --> 01:33:14,355
Speaker 11:  other delivery methods.

1466
01:33:15,025 --> 01:33:18,975
Speaker 5:  What? Just get a cooler Wait because it's refrigerated. How

1467
01:33:18,975 --> 01:33:22,845
Speaker 5:  is it? Oh, it's refrigerated. I mean, just get a cooler.

1468
01:33:22,845 --> 01:33:26,685
Speaker 5:  This is a really expensive locker. This is, this

1469
01:33:26,685 --> 01:33:30,045
Speaker 5:  is the least efficient way to ship ice cream.

1470
01:33:31,045 --> 01:33:34,725
Speaker 5:  I what, how is this an issue? I don't understand this.

1471
01:33:35,365 --> 01:33:38,485
Speaker 5:  I like, I've tried, I've tried grocery delivery before.

1472
01:33:39,245 --> 01:33:42,765
Speaker 5:  I would say it has mostly been a bad experience. I've,

1473
01:33:43,035 --> 01:33:46,565
Speaker 7:  I've not had that bad of an experience with grocery delivery, even with ice

1474
01:33:46,565 --> 01:33:46,765
Speaker 7:  cream.

1475
01:33:46,915 --> 01:33:50,085
Speaker 5:  Have you, have you tried Amazon Whole Foods, like grocery delivery.

1476
01:33:50,195 --> 01:33:52,405
Speaker 7:  Okay. I don't use, I, my they they were not

1477
01:33:52,405 --> 01:33:56,205
Speaker 5:  Great. They, this is so I tr I tried this because I, I got

1478
01:33:56,205 --> 01:33:59,685
Speaker 5:  mad at my local grocery stores. You know what they should, they should carry

1479
01:33:59,745 --> 01:34:03,685
Speaker 5:  better produce. All right. It is. You cut open too many

1480
01:34:03,685 --> 01:34:06,405
Speaker 5:  peppers that are bad inside. You get pissed.

1481
01:34:07,625 --> 01:34:11,005
Speaker 5:  The problem I kept running into with, with Amazon is I'd be like, all right,

1482
01:34:11,005 --> 01:34:14,045
Speaker 5:  I'm gonna order some food and then they would just be out of stock of like

1483
01:34:14,045 --> 01:34:17,925
Speaker 5:  everything. Like there is there some weird online thing, but

1484
01:34:17,925 --> 01:34:21,525
Speaker 5:  just it's not, I was never upset. Like, I'm not gonna order ice cream.

1485
01:34:21,715 --> 01:34:25,525
Speaker 5:  It's fine. I don't need to order ice cream for delivery. Like I can manage

1486
01:34:25,525 --> 01:34:28,205
Speaker 11:  You dream. You're sitting at home wanting an ice cream sandwich and having

1487
01:34:28,325 --> 01:34:31,885
Speaker 11:  a bus, okay, rolled up to deliver you one ice cream sandwich. This is that.

1488
01:34:31,885 --> 01:34:32,765
Speaker 11:  That's not, can I

1489
01:34:32,765 --> 01:34:33,445
Speaker 7:  Tell that's

1490
01:34:33,445 --> 01:34:37,245
Speaker 5:  An abuse power. Richard, you have invented a business known as an

1491
01:34:37,265 --> 01:34:41,205
Speaker 5:  ice cream truck. This exists, this is famously

1492
01:34:41,605 --> 01:34:43,325
Speaker 5:  a very successful industry.

1493
01:34:44,915 --> 01:34:45,205
Speaker 5:  They

1494
01:34:46,105 --> 01:34:49,885
Speaker 15:  In New York, those things are like rolling around at all

1495
01:34:50,015 --> 01:34:53,445
Speaker 15:  hours of the night. I can walk outside at any time and get

1496
01:34:53,445 --> 01:34:56,925
Speaker 5:  An ice cream sandwich. I don't need a giant truck

1497
01:34:57,075 --> 01:34:58,085
Speaker 15:  With a personalized

1498
01:34:58,145 --> 01:34:58,565
Speaker 5:  Locker

1499
01:34:59,185 --> 01:35:00,565
Speaker 15:  To gimme eight of them at once.

1500
01:35:02,635 --> 01:35:06,525
Speaker 15:  They're solving an already solved problem. We have

1501
01:35:06,845 --> 01:35:10,765
Speaker 15:  delivery, we have refrigerated trucks. Oh,

1502
01:35:11,165 --> 01:35:13,805
Speaker 15:  I don't know what this is for. I

1503
01:35:15,105 --> 01:35:15,805
Speaker 15:  I'm saying

1504
01:35:19,185 --> 01:35:19,475
Speaker 15:  they,

1505
01:35:19,665 --> 01:35:20,475
Speaker 5:  They saw that they,

1506
01:35:20,475 --> 01:35:20,755
Speaker 15:  You know, when

1507
01:35:20,755 --> 01:35:21,355
Speaker 11:  You put it that way?

1508
01:35:24,015 --> 01:35:25,115
Speaker 15:  Oh yeah.

1509
01:35:25,615 --> 01:35:29,435
Speaker 5:  You know, IIII understand why a business

1510
01:35:29,435 --> 01:35:33,395
Speaker 5:  person, I understand why like an Amazon would look at us and go,

1511
01:35:33,395 --> 01:35:37,275
Speaker 5:  this is genius. This is going, this is on the level of ai. It will

1512
01:35:37,515 --> 01:35:41,155
Speaker 5:  revolutionize our business. And then as soon as you figure out, think about

1513
01:35:41,155 --> 01:35:44,995
Speaker 5:  like, how am I actually gonna put this to use? I just

1514
01:35:44,995 --> 01:35:46,235
Speaker 5:  have literally no idea.

1515
01:35:46,535 --> 01:35:50,075
Speaker 7:  I'm also afraid of this on the road. I already have seen a TikTok of someone

1516
01:35:50,265 --> 01:35:54,155
Speaker 7:  just like filming one of those say like the, like the small ones that

1517
01:35:54,185 --> 01:35:58,075
Speaker 7:  deliver drinks around and they're just like, don't hit him.

1518
01:35:58,215 --> 01:35:58,795
Speaker 7:  Oh my God.

1519
01:35:58,865 --> 01:36:00,795
Speaker 15:  It's like Frogger, is it gonna get across

1520
01:36:00,855 --> 01:36:03,795
Speaker 7:  The street and we gonna do this? I'm afraid of this.

1521
01:36:03,945 --> 01:36:06,955
Speaker 5:  It's got a bit of a tank vibe. It's got a, it's like, yeah, this thing is

1522
01:36:07,005 --> 01:36:10,235
Speaker 5:  hefty. It's hefty. Depends how much ice cream is carrying.

1523
01:36:10,335 --> 01:36:13,595
Speaker 7:  But it can you just imagine it getting into a collision and then just the

1524
01:36:13,615 --> 01:36:17,075
Speaker 7:  ice cream exploding out of the lockers and then it melts that way.

1525
01:36:17,135 --> 01:36:18,075
Speaker 5:  You know what, that's the dream.

1526
01:36:18,215 --> 01:36:21,315
Speaker 11:  Yes, I can that, that is going immediately to the top of The Verge dot com.

1527
01:36:21,465 --> 01:36:21,755
Speaker 11:  Yeah.

1528
01:36:23,945 --> 01:36:27,875
Speaker 5:  Alright, that's it for The Vergecast. Stay tuned. Allison will be hosting

1529
01:36:28,055 --> 01:36:31,795
Speaker 5:  on Tuesday. If you like. What we do here, the best way to support us is to

1530
01:36:31,795 --> 01:36:35,315
Speaker 5:  get a subscription to The Verge at The Verge dot com. We publish a lot of

1531
01:36:35,315 --> 01:36:38,955
Speaker 5:  great stuff every single day. We'd love to hear your questions and

1532
01:36:39,235 --> 01:36:42,275
Speaker 5:  feedback. Let us know what you want us to talk about in the show. Email us

1533
01:36:42,475 --> 01:36:46,435
Speaker 5:  vergecast at The Verge dot com or give us a call. 8 6 6 VERGE 11.

1534
01:36:46,935 --> 01:36:50,515
Speaker 5:  The Vergecast is a production of The Verge and Vox Media Podcast network.

1535
01:36:50,875 --> 01:36:54,395
Speaker 5:  A show is produced by Eric Gomez, Brandon Keefer, Travis Uck, and Andrew

1536
01:36:54,455 --> 01:36:55,635
Speaker 5:  Marino. See you next week.

