1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: cce8cc60-d747-4906-80aa-bccd1a847bb3
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-5163312489323103879/2416802536545814872/s93290-US-5832s-1712921796.mp3
Description: The Verge's Nilay Patel, David Piece, and Alex Cranz discuss David's review of the Humane AI Pin, Taylor Swift's music back on TikTok, a new party speaker, and much more.

2
00:01:08,405 --> 00:01:12,115
Speaker 2:  Hello. And Welcome To our Chest, the flagship podcast of appreciating The

3
00:01:12,115 --> 00:01:13,275
Speaker 2:  good vibes on the hardware

4
00:01:16,585 --> 00:01:19,315
Speaker 5:  That that might be the truest one we've ever done. Yeah.

5
00:01:19,945 --> 00:01:23,195
Speaker 2:  Yeah. That's, that's what we do. But we totally get it. There's room to make

6
00:01:23,195 --> 00:01:27,155
Speaker 2:  things even better. By which I mean functional. Hi, I'm your friend. NELI

7
00:01:27,155 --> 00:01:30,515
Speaker 2:  David. Pierce is here. Hi Alex Cranz is here. Hi.

8
00:01:31,625 --> 00:01:34,915
Speaker 2:  It's a big week on The Vergecast. All right. It's like maybe the biggest

9
00:01:34,915 --> 00:01:38,755
Speaker 2:  week we've ever had because David has reviewed the Humane ai pin. Yeah.

10
00:01:39,135 --> 00:01:42,875
Speaker 5:  It it it has come full circle. This really does feel like it's been a very

11
00:01:42,875 --> 00:01:46,715
Speaker 5:  funny experience in that like almost exactly a year ago

12
00:01:47,095 --> 00:01:50,955
Speaker 5:  was that first Ted Talk that Imran Chowdry, the CEO and

13
00:01:50,955 --> 00:01:54,555
Speaker 5:  Co-founder of Humane gave that We were just like, this is nothing.

14
00:01:55,155 --> 00:01:58,275
Speaker 5:  I don't believe it. We basically sat here and we're like, I don't believe

15
00:01:58,375 --> 00:02:01,635
Speaker 5:  any of the things that happened in this demo. And

16
00:02:04,095 --> 00:02:05,195
Speaker 5:  it turned out that's sort of true.

17
00:02:05,285 --> 00:02:09,155
Speaker 2:  We'll get, we'll get to it. We'll get to it. And If you were a fan of arguing

18
00:02:09,165 --> 00:02:13,115
Speaker 2:  about the Vision Pro Score. Boy, get ready for us to Oh hell yeah. Tell

19
00:02:13,115 --> 00:02:16,915
Speaker 2:  you about the Humane score from totally opposite perspectives. I'm ready.

20
00:02:17,225 --> 00:02:21,035
Speaker 2:  It's very good. That happened. Taylor Swift is back on TikTok.

21
00:02:21,035 --> 00:02:24,355
Speaker 2:  Oh my God. I have some completely irresponsible gossip to share about the

22
00:02:24,355 --> 00:02:27,475
Speaker 2:  status of TikTok in Universal Music Group. I love this. And so, If, you're

23
00:02:27,475 --> 00:02:31,435
Speaker 2:  listening and you feel like you've always wanted to substantiate some gossip.

24
00:02:31,775 --> 00:02:33,675
Speaker 2:  Get ready. 'cause I'm looking for you.

25
00:02:35,185 --> 00:02:39,075
Speaker 2:  There's other news OpenAI and copyright trouble as always. So is Google.

26
00:02:39,235 --> 00:02:42,395
Speaker 6:  I mean, there's really important news, which is that Kobo announced

27
00:02:42,965 --> 00:02:46,115
Speaker 6:  color E-readers. Yes. All three of us are just

28
00:02:46,575 --> 00:02:48,715
Speaker 6:  stoked. The three of us that care.

29
00:02:48,995 --> 00:02:50,315
Speaker 5:  It's just Alex, Alex and

30
00:02:50,315 --> 00:02:53,875
Speaker 2:  Alex again, the flagship podcast of appreciating The good vibes on the hardware.

31
00:02:54,225 --> 00:02:56,995
Speaker 2:  Yeah, exactly. That's, that's a hundred percent that story. That's who you're,

32
00:02:56,995 --> 00:03:00,285
Speaker 2:  yeah. And then the most important news of all, which is that Sony has a new

33
00:03:00,285 --> 00:03:02,165
Speaker 2:  party speaker. Exactly. We're appreciating

34
00:03:02,235 --> 00:03:03,565
Speaker 5:  Lies. This really is a big Vergecast.

35
00:03:03,835 --> 00:03:07,325
Speaker 2:  Yeah. If. you like The Vergecast. You're gonna love this Vergecast

36
00:03:08,475 --> 00:03:12,125
Speaker 2:  that I can promise you. Let's start, let's get right into it. David,

37
00:03:12,345 --> 00:03:15,725
Speaker 2:  you magnetically clipped a robot to your body

38
00:03:16,785 --> 00:03:18,365
Speaker 2:  for several weeks. It was

39
00:03:18,365 --> 00:03:19,605
Speaker 5:  Warm as I want to do.

40
00:03:19,905 --> 00:03:23,525
Speaker 2:  It got warm. It only ever spoke to me and I believe Korean or Arabic.

41
00:03:23,905 --> 00:03:27,325
Speaker 2:  Oh. Because every time I tried to use it, it was locked in a translation

42
00:03:27,355 --> 00:03:31,125
Speaker 2:  loop. So I I have no experience with it. What, you gave it a four,

43
00:03:31,185 --> 00:03:33,365
Speaker 2:  you said it didn't work. Tell us about it. Sure.

44
00:03:33,465 --> 00:03:37,205
Speaker 5:  So the whole idea of the pin and this like new

45
00:03:37,205 --> 00:03:40,925
Speaker 5:  generation of AI gadgets is that your smartphone is actually

46
00:03:41,345 --> 00:03:45,245
Speaker 5:  slower and worse than you think. Basically the

47
00:03:45,245 --> 00:03:49,165
Speaker 5:  idea is like instead of taking out my phone and unlocking it and doing

48
00:03:49,165 --> 00:03:51,925
Speaker 5:  stuff and tapping on screens and typing things, I should just be able to

49
00:03:51,925 --> 00:03:55,485
Speaker 5:  like touch the thing on my chest and

50
00:03:56,105 --> 00:03:59,805
Speaker 5:  it should do things on my behalf. And I think that is like a super

51
00:04:00,125 --> 00:04:03,285
Speaker 5:  interesting idea. Anyone who has ever watched a sci-fi movie ever

52
00:04:04,395 --> 00:04:05,725
Speaker 5:  sort of knows what that looks like.

53
00:04:07,385 --> 00:04:11,085
Speaker 5:  And so I spent a couple of weeks like running around

54
00:04:11,225 --> 00:04:15,205
Speaker 5:  asking it questions about where I was and asking it questions about the

55
00:04:15,205 --> 00:04:19,125
Speaker 5:  world and using it to try to play music and try to, you know, make

56
00:04:19,125 --> 00:04:23,005
Speaker 5:  phone calls, which you can do on the AI pin and you can send text messages

57
00:04:23,025 --> 00:04:26,405
Speaker 5:  and it takes notes for you and all this different stuff. And

58
00:04:27,245 --> 00:04:30,965
Speaker 5:  I am simultaneously more bullish on AI gadgets than

59
00:04:31,205 --> 00:04:34,885
Speaker 5:  ever and so deeply done with

60
00:04:35,265 --> 00:04:36,365
Speaker 5:  the Humane AI pin.

61
00:04:36,645 --> 00:04:39,245
Speaker 2:  All right. I wanna unpack that and I will say

62
00:04:40,635 --> 00:04:44,565
Speaker 2:  bizarrely a huge split in the comment base on this review. Yeah. The

63
00:04:44,565 --> 00:04:48,085
Speaker 2:  commenters on YouTube love the review as far as I can tell. I think you did

64
00:04:48,085 --> 00:04:51,845
Speaker 2:  a great job. Your buddy Ben Strass, we gotta talk to him. He's calling, he's

65
00:04:51,845 --> 00:04:55,445
Speaker 2:  calling in the show later, later today. They're like, this is great. We love

66
00:04:55,445 --> 00:04:58,885
Speaker 2:  this review. All the things we, and then our commenters picked up on the

67
00:04:59,045 --> 00:05:02,365
Speaker 2:  inherent tension of what David just said, which is, this product is horrible,

68
00:05:02,365 --> 00:05:05,605
Speaker 2:  but I'm excited about this category. And said, why nothing about this product

69
00:05:05,745 --> 00:05:09,525
Speaker 2:  has proven that this category is viable. Which I would say is a fair

70
00:05:09,525 --> 00:05:13,285
Speaker 2:  criticism. Why do you think this thing which does not work,

71
00:05:13,845 --> 00:05:16,925
Speaker 2:  I believe is a sentence that is in your review, made you excited about this

72
00:05:17,165 --> 00:05:17,365
Speaker 2:  category?

73
00:05:17,945 --> 00:05:21,485
Speaker 5:  It so this, I mean, this is where we're gonna get into arguing about the

74
00:05:21,485 --> 00:05:24,365
Speaker 5:  score, right? Because it, it, it's not that it doesn't work ever,

75
00:05:25,665 --> 00:05:29,605
Speaker 5:  it works seriously. No, this is like, there were, there were a handful

76
00:05:29,745 --> 00:05:33,485
Speaker 5:  of moments in two weeks of testing this thing that were like

77
00:05:33,565 --> 00:05:37,285
Speaker 5:  legitimately eye-opening, right? Like standing in Penn Station,

78
00:05:38,065 --> 00:05:41,445
Speaker 5:  tap the thing. I'm like, tell me what restaurant this is and if, if it has

79
00:05:41,445 --> 00:05:45,285
Speaker 5:  good reviews and somewhere between one and a thousand

80
00:05:45,285 --> 00:05:48,765
Speaker 5:  seconds later it comes back and, and gives me like real

81
00:05:48,875 --> 00:05:52,845
Speaker 5:  information. Like people like this on the menu. It has 4.3 stars

82
00:05:52,865 --> 00:05:56,725
Speaker 5:  on Google. People say it's a little expensive, but the servers are

83
00:05:56,805 --> 00:06:00,565
Speaker 5:  friendly like that. That is like a astonishingly useful thing

84
00:06:00,905 --> 00:06:04,685
Speaker 5:  to have done for me. Just truthfully. So like one of the things I've

85
00:06:04,765 --> 00:06:08,685
Speaker 5:  discovered is like I have a dog, so I'm forever walking the dog and I

86
00:06:08,685 --> 00:06:12,645
Speaker 5:  have a kid who is often only happy in a stroller. So I'm just forever out

87
00:06:12,645 --> 00:06:16,565
Speaker 5:  walking in my neighborhood. And one thing that

88
00:06:16,605 --> 00:06:20,045
Speaker 5:  I do when I am forever out walking in my neighborhood is I end up like

89
00:06:20,805 --> 00:06:23,765
Speaker 5:  remembering things I need to do. So I'm, I'm constantly like pulling out

90
00:06:23,765 --> 00:06:27,205
Speaker 5:  my phone and using Siri to set reminders or add something to my calendar

91
00:06:27,345 --> 00:06:30,125
Speaker 5:  or just write a note down in my notes app.

92
00:06:31,135 --> 00:06:34,925
Speaker 5:  Every single one of those things better to do on a

93
00:06:34,925 --> 00:06:38,725
Speaker 5:  device that is touching my body, right? Like it was better on my

94
00:06:38,735 --> 00:06:40,245
Speaker 5:  chest than it was in my pocket.

95
00:06:42,305 --> 00:06:44,925
Speaker 5:  All true. Right? So there were like a handful of these moments where I was

96
00:06:44,925 --> 00:06:48,645
Speaker 5:  like, oh, having this thing that actually does abstract away all of the

97
00:06:48,645 --> 00:06:52,605
Speaker 5:  things I have to do to get into my phone and just lets me say the

98
00:06:52,725 --> 00:06:56,245
Speaker 5:  thing that's in my brain and it's then out of my brain. Awesome.

99
00:06:56,975 --> 00:07:00,325
Speaker 5:  Loved it. Great. The problem with the pin

100
00:07:00,945 --> 00:07:04,605
Speaker 5:  is that it doesn't do it enough. And the problem with this whole category

101
00:07:04,785 --> 00:07:08,245
Speaker 5:  is that like I know how to write down a note in my phone and I know when

102
00:07:08,245 --> 00:07:11,925
Speaker 5:  it has worked it, it's annoying, but it works

103
00:07:12,345 --> 00:07:16,245
Speaker 5:  and I can do it reliably every time. The

104
00:07:16,265 --> 00:07:19,205
Speaker 5:  pin I just stopped trusting was the problem, right? So it's like when it

105
00:07:19,205 --> 00:07:22,965
Speaker 5:  works, it's cool. And it worked just enough that I was like,

106
00:07:22,965 --> 00:07:26,885
Speaker 5:  there is, there is something here. This, it's just

107
00:07:27,025 --> 00:07:30,605
Speaker 5:  so far away from that something at this moment that it's like I can't in

108
00:07:30,605 --> 00:07:34,525
Speaker 5:  good conscience tell you to even try it, but like, you should want The good

109
00:07:34,525 --> 00:07:36,245
Speaker 5:  version of this to exist.

110
00:07:36,785 --> 00:07:40,045
Speaker 2:  So I know you talked to Humane through the course of this review. They can't

111
00:07:40,045 --> 00:07:43,765
Speaker 2:  have been surprised by their own pro One assumes they're not hopelessly surprised

112
00:07:44,265 --> 00:07:48,125
Speaker 2:  by their own product. Why did they ship this now? Did you get a

113
00:07:48,125 --> 00:07:48,285
Speaker 2:  sense?

114
00:07:49,165 --> 00:07:52,445
Speaker 5:  I I don't know the answer to that question and I have asked that question

115
00:07:52,705 --> 00:07:56,085
Speaker 5:  to myself and to them many times. I think the, the honest truth is like

116
00:07:56,535 --> 00:08:00,125
Speaker 5:  there comes a time in the process of making hardware where you just have

117
00:08:00,125 --> 00:08:04,005
Speaker 5:  to ship the damn thing. Like it's just really expensive to not

118
00:08:04,285 --> 00:08:07,565
Speaker 5:  ship a product. Especially a product that a lot of people have given you

119
00:08:07,565 --> 00:08:11,485
Speaker 5:  money for. And you have made many of, I

120
00:08:11,485 --> 00:08:15,325
Speaker 5:  think If you were to rewind a year and tell

121
00:08:15,345 --> 00:08:19,165
Speaker 5:  Humane this is the point where they would be Now I would bet they would not

122
00:08:19,165 --> 00:08:22,765
Speaker 5:  be shipping this product right now because they also have, they claim this

123
00:08:23,205 --> 00:08:26,125
Speaker 5:  gigantic software update coming this summer that adds really basic stuff

124
00:08:26,125 --> 00:08:30,085
Speaker 5:  like setting timers. You can't set a timer that's

125
00:08:30,085 --> 00:08:33,685
Speaker 5:  like the joke about assistance is that the only thing they can do is set

126
00:08:33,685 --> 00:08:37,445
Speaker 5:  timers. The joke about Sir is they can only set one Humane can't set any,

127
00:08:38,065 --> 00:08:39,405
Speaker 5:  it can't set any. And

128
00:08:39,585 --> 00:08:42,925
Speaker 2:  So like, look, AI systems are bad at math, I think historically is a thing.

129
00:08:43,145 --> 00:08:45,805
Speaker 2:  And so just counting must be very challenging for you. Yeah,

130
00:08:45,955 --> 00:08:46,845
Speaker 5:  It's totally down

131
00:08:47,275 --> 00:08:48,005
Speaker 2:  Even harder.

132
00:08:49,585 --> 00:08:52,925
Speaker 5:  But, but yeah. So there's this big software update supposedly coming this

133
00:08:52,925 --> 00:08:55,645
Speaker 5:  summer that's gonna add some of that functionality and, and fix some of the

134
00:08:55,645 --> 00:08:59,085
Speaker 5:  things that don't work now. So my guess would be they

135
00:08:59,515 --> 00:09:03,485
Speaker 5:  want to ship the like August version of this thing today.

136
00:09:03,625 --> 00:09:07,605
Speaker 5:  And that is just not where they are. But like there comes a time

137
00:09:07,605 --> 00:09:10,085
Speaker 5:  when, for a variety of reasons you're just locked in to when something ships

138
00:09:10,085 --> 00:09:13,925
Speaker 5:  and it's really expensive and really hard, especially for a first generation

139
00:09:14,125 --> 00:09:17,285
Speaker 5:  hardware company to delay by months and months. Yeah. It's just

140
00:09:17,285 --> 00:09:20,085
Speaker 2:  Hard. Alright, let's take it in pieces. 'cause there's a line in your review

141
00:09:20,335 --> 00:09:22,845
Speaker 2:  where you say, none of this is ready. Not the hardware, not the software,

142
00:09:22,865 --> 00:09:25,325
Speaker 2:  not the ai. That's a, that's all of it.

143
00:09:26,145 --> 00:09:27,285
Speaker 5:  That's all the things that's

144
00:09:27,285 --> 00:09:27,685
Speaker 2:  That's all of it.

145
00:09:28,305 --> 00:09:29,485
Speaker 5:  I'm ready. That's one thing

146
00:09:29,665 --> 00:09:32,725
Speaker 2:  You very clearly are ready for this thing.

147
00:09:32,915 --> 00:09:34,285
Speaker 6:  David's a 10 out of 10. Yeah,

148
00:09:34,285 --> 00:09:36,685
Speaker 2:  He's he's ready. He's ready to go. Yeah, of course. There's a picture in

149
00:09:36,685 --> 00:09:39,245
Speaker 2:  the review of David using the thing where he just looks like the world's

150
00:09:39,365 --> 00:09:43,205
Speaker 2:  happiest secret service agent. you know, he's just like, he's just beyond

151
00:09:43,525 --> 00:09:46,165
Speaker 2:  thrilled to be like tapping on his chest and whispering a secret, you know?

152
00:09:46,865 --> 00:09:49,045
Speaker 2:  So you're obviously ready, but the thing isn't ready. Let's start with the

153
00:09:49,205 --> 00:09:53,165
Speaker 2:  hardware. Three pieces, right? Hardware, software, ai. Yeah. What about

154
00:09:53,165 --> 00:09:53,965
Speaker 2:  the hardware isn't ready?

155
00:09:54,625 --> 00:09:58,485
Speaker 5:  The hardware is kind of in the thing

156
00:09:58,505 --> 00:10:02,085
Speaker 5:  we see a lot with first generation hardware, which is just, it's full of

157
00:10:02,865 --> 00:10:06,605
Speaker 5:  little wonky bugs. The biggest one

158
00:10:06,745 --> 00:10:10,565
Speaker 5:  by a mile is thermals. Like this thing is small. I have it sitting

159
00:10:10,565 --> 00:10:11,765
Speaker 5:  right here. Like it's, it's, it's,

160
00:10:11,765 --> 00:10:15,165
Speaker 2:  It is shockingly small. I saw it big in the office and I was amazed at how

161
00:10:15,165 --> 00:10:15,365
Speaker 2:  small

162
00:10:15,365 --> 00:10:18,805
Speaker 5:  It's, yeah, it is like not a large thing and it's, it's a nicely made thing.

163
00:10:18,805 --> 00:10:21,525
Speaker 5:  It's made of aluminum. It's pretty durable. I've dropped it, I threw it in

164
00:10:21,525 --> 00:10:23,205
Speaker 5:  the wash the other day just to see it's,

165
00:10:23,205 --> 00:10:26,565
Speaker 6:  It's pretty big to put on your chest. Like,

166
00:10:26,645 --> 00:10:30,605
Speaker 5:  I mean yes, it, that's compared to all of the other chest wearables

167
00:10:30,605 --> 00:10:31,485
Speaker 5:  that I've had in my life.

168
00:10:31,845 --> 00:10:35,645
Speaker 6:  I mean, it, it's like, it's like broach size and how many people do you know

169
00:10:35,645 --> 00:10:38,965
Speaker 6:  that wear broaches that aren't 80 and at church?

170
00:10:39,595 --> 00:10:39,885
Speaker 2:  Yeah,

171
00:10:40,185 --> 00:10:41,125
Speaker 5:  But they kill it. Those

172
00:10:41,125 --> 00:10:43,965
Speaker 2:  Bro. Huge market though. Those people are sitting on an enormous amount of

173
00:10:43,965 --> 00:10:46,005
Speaker 2:  wealth that they have to transfer to their children. Some, I mean,

174
00:10:46,445 --> 00:10:48,085
Speaker 6:  Yeah, and they can just transfer it into Humane

175
00:10:48,085 --> 00:10:48,245
Speaker 5:  Kids.

176
00:10:48,275 --> 00:10:49,485
Speaker 2:  It's directly to Humane,

177
00:10:51,785 --> 00:10:53,845
Speaker 2:  but it's, it's smaller than you think, but bigger than it should be I think

178
00:10:53,865 --> 00:10:54,285
Speaker 2:  is a fair.

179
00:10:54,315 --> 00:10:58,165
Speaker 5:  Yeah. Yeah. I think that's exactly right. It is, it is both too big and impressively

180
00:10:58,165 --> 00:11:02,085
Speaker 5:  small for sure. But the biggest pro, I mean I'm holding it now. I've been

181
00:11:02,085 --> 00:11:05,445
Speaker 5:  holding it in my hand for 15 seconds and it's warm. Like

182
00:11:06,185 --> 00:11:08,925
Speaker 5:  I'm not kidding. That's like a real thing that's happening right now. And

183
00:11:09,395 --> 00:11:12,605
Speaker 5:  that is the overwhelming issue, right? It gets warm when you use it.

184
00:11:13,545 --> 00:11:17,165
Speaker 5:  It gets warm so quickly when you use it that it pretty frequently

185
00:11:17,195 --> 00:11:20,805
Speaker 5:  overheats and shuts down. It gets warm when

186
00:11:20,805 --> 00:11:24,645
Speaker 5:  service is bad. Which service is often bad because you're using like, they're

187
00:11:24,645 --> 00:11:28,605
Speaker 5:  using some weird MVNO of T-Mobile that doesn't God really work.

188
00:11:28,865 --> 00:11:32,125
Speaker 5:  So it's just like, again, all these are things that like most of the time

189
00:11:32,125 --> 00:11:35,965
Speaker 5:  when we review a first gadget from a company, it's like this, like

190
00:11:35,965 --> 00:11:39,525
Speaker 5:  the first pixel watch had a lot of little tiny hardware bugs.

191
00:11:39,745 --> 00:11:43,405
Speaker 5:  And that's the kind of stuff where you're like, okay, this is the sort of

192
00:11:43,405 --> 00:11:47,205
Speaker 5:  stuff that with an extra couple of revs of the tooling

193
00:11:47,205 --> 00:11:49,685
Speaker 5:  and the engineering and working on this, you actually start to solve these.

194
00:11:49,685 --> 00:11:52,805
Speaker 5:  So there's like, in the hardware, there's not a lot that I would call like

195
00:11:53,035 --> 00:11:56,965
Speaker 5:  show stopping bad. It's just that

196
00:11:57,035 --> 00:12:00,805
Speaker 5:  none of it is quite ready. But the main thing that really like

197
00:12:01,505 --> 00:12:04,765
Speaker 5:  causes problems is like it, it'd be nice to be able to use this thing and

198
00:12:04,765 --> 00:12:08,645
Speaker 5:  you use it and it's like you can't anymore because it's too hot and it

199
00:12:08,645 --> 00:12:10,205
Speaker 5:  also, right, let's send the video sits on your skin,

200
00:12:10,575 --> 00:12:14,485
Speaker 2:  Right? So it gets hot and shuts itself down. Which If you

201
00:12:14,485 --> 00:12:17,965
Speaker 2:  will recall was happening a lot in the demos at efa.

202
00:12:18,345 --> 00:12:22,165
Speaker 2:  The trade show there. I think we were, everyone rightfully was like, well

203
00:12:22,165 --> 00:12:25,925
Speaker 2:  it's a trade show, bad wifi, bad cell signal using generous a trade show.

204
00:12:26,045 --> 00:12:29,965
Speaker 2:  Generous. Generous we given of doubt. That's exactly right. It

205
00:12:30,165 --> 00:12:33,965
Speaker 2:  feels like you were probably not using it on the floor of a

206
00:12:33,965 --> 00:12:34,645
Speaker 2:  trade show in Berlin.

207
00:12:35,045 --> 00:12:38,965
Speaker 5:  I mean, I do love a trade show. Yeah, but no, I was using it, I was

208
00:12:38,965 --> 00:12:42,325
Speaker 5:  using it at my house, I was using it in our office. I was using it on the

209
00:12:42,325 --> 00:12:45,965
Speaker 5:  streets of New York and Washington DC Like I I, one of the weird things

210
00:12:45,975 --> 00:12:49,885
Speaker 5:  about reviewing this is that it's not

211
00:12:49,925 --> 00:12:53,845
Speaker 5:  a gadget you're supposed to use very much. Like the whole point of it is

212
00:12:53,845 --> 00:12:56,925
Speaker 5:  that you don't use it all the time. You can just like quickly accomplish

213
00:12:56,925 --> 00:12:59,605
Speaker 5:  the thing you need to accomplish and then put it away. And to that I would

214
00:12:59,605 --> 00:13:03,045
Speaker 5:  say it's $700. That's insane. But,

215
00:13:03,625 --> 00:13:06,365
Speaker 5:  so it's like, it was a weird thing to test in that sense because it's like,

216
00:13:06,365 --> 00:13:09,285
Speaker 5:  yes, if I use this thing constantly for several hours,

217
00:13:10,075 --> 00:13:13,765
Speaker 5:  it's gonna, it it's gonna die. Like the battery life in that sense

218
00:13:13,905 --> 00:13:17,845
Speaker 5:  is bad. But you're only supposed to use it for

219
00:13:17,845 --> 00:13:21,285
Speaker 5:  like a few seconds at a time, a dozen times a day. And

220
00:13:21,715 --> 00:13:25,365
Speaker 5:  that is very different from what it takes to actually like test the edges

221
00:13:25,365 --> 00:13:29,245
Speaker 5:  of a product like this. So I, I struggled with that a lot in the course of

222
00:13:29,245 --> 00:13:33,125
Speaker 5:  this. It's like it has this little green laser projector that it's like if

223
00:13:33,125 --> 00:13:37,005
Speaker 5:  I use that for a three minutes at a go, it overheats and the

224
00:13:37,005 --> 00:13:37,525
Speaker 5:  battery dies.

225
00:13:37,525 --> 00:13:41,365
Speaker 2:  That's, so I, that's the one that's just a flaw in

226
00:13:41,365 --> 00:13:45,285
Speaker 2:  the product. Yes. Straightforwardly, If you use its display

227
00:13:45,385 --> 00:13:47,165
Speaker 2:  for more than a few minutes. It shuts down.

228
00:13:48,905 --> 00:13:51,125
Speaker 2:  You there's being generous, there's the benefit, the

229
00:13:51,125 --> 00:13:53,445
Speaker 6:  Doubt. Yeah. No, that's just bad. How are you supposed to watch Dune too?

230
00:13:54,355 --> 00:13:57,645
Speaker 2:  Like come on as, as villain view as he is the author

231
00:13:58,485 --> 00:14:02,005
Speaker 2:  intended. Yeah. Great. So there's that,

232
00:14:02,215 --> 00:14:05,805
Speaker 2:  right? Like some parts of it don't work and it just overheats. Then there's

233
00:14:05,805 --> 00:14:09,725
Speaker 2:  the battery life, which I think in all product reviewing is

234
00:14:09,925 --> 00:14:12,365
Speaker 2:  challenging 'cause we overuse this stuff. Yeah.

235
00:14:12,385 --> 00:14:13,365
Speaker 6:  That's like our job.

236
00:14:13,805 --> 00:14:17,765
Speaker 5:  I tried to like alternate days between like use the hell out of

237
00:14:17,765 --> 00:14:21,445
Speaker 5:  this and use it like a regular person and on the, on the use

238
00:14:21,605 --> 00:14:25,365
Speaker 5:  the hell out of it days. I mean I killed it comes with the, the thing itself,

239
00:14:26,105 --> 00:14:29,845
Speaker 5:  two extra batteries and a charging case and I would kill

240
00:14:29,985 --> 00:14:33,925
Speaker 5:  all of those in a, in a day of like heavily testing the thing.

241
00:14:34,145 --> 00:14:38,005
Speaker 5:  On a normal day I would kill the two battery boosters and

242
00:14:38,005 --> 00:14:41,925
Speaker 5:  the thing, but like I would get through the day, I would charge everything

243
00:14:41,925 --> 00:14:45,005
Speaker 5:  overnight and it would be fine. So it's like, it was kind of like being a

244
00:14:45,005 --> 00:14:48,845
Speaker 5:  power user of a phone on a normal day. So like that I track is like not great,

245
00:14:48,865 --> 00:14:52,725
Speaker 5:  but not showstopping. But it's like, again, if I look at the

246
00:14:52,725 --> 00:14:54,325
Speaker 5:  screen, it dies. Yeah.

247
00:14:54,675 --> 00:14:57,245
Speaker 2:  Okay. So that's the, just like the thermals and the battery, right? Yeah,

248
00:14:57,245 --> 00:15:00,445
Speaker 2:  yeah. Like the seam inherently flawed. If the thing is overheating and the

249
00:15:00,605 --> 00:15:03,885
Speaker 2:  batteries are dying too fast, then there's this screen, right? Just keeping

250
00:15:03,885 --> 00:15:07,725
Speaker 2:  on with the hardware or the projector, which there's

251
00:15:07,935 --> 00:15:11,045
Speaker 2:  again, the line in your review is like they try to do everything possible

252
00:15:11,045 --> 00:15:13,485
Speaker 2:  to not have a screen and then they have this,

253
00:15:14,045 --> 00:15:18,005
Speaker 5:  This thing should just have a tiny touch screen. Like I'm, I, I am so a hundred

254
00:15:18,005 --> 00:15:21,725
Speaker 5:  percent convinced that for the stuff they want to do with the screen, which

255
00:15:21,725 --> 00:15:25,325
Speaker 5:  is basically like simple music playback settings,

256
00:15:25,455 --> 00:15:29,165
Speaker 5:  menus, and to be able to look at a text instead of

257
00:15:29,305 --> 00:15:32,805
Speaker 5:  having it spoken aloud to you tiny touchscreen, the end.

258
00:15:32,945 --> 00:15:36,365
Speaker 2:  But then you'd have to take it off and like look at the screen. This I I

259
00:15:36,685 --> 00:15:39,805
Speaker 2:  I was reading that and I'd like, but then you would have to manipulate it

260
00:15:40,235 --> 00:15:43,205
Speaker 2:  instead of having it clipped to your chest. No, you do this, you go, you

261
00:15:43,205 --> 00:15:43,485
Speaker 2:  one of these

262
00:15:44,595 --> 00:15:47,685
Speaker 5:  Look upside down. Yeah. Yeah. I would remind you that either way I'm, I'm

263
00:15:47,685 --> 00:15:51,645
Speaker 5:  putting my hand right here. Like my hand is out. I have

264
00:15:51,645 --> 00:15:52,045
Speaker 5:  to use my

265
00:15:52,075 --> 00:15:53,885
Speaker 2:  Hand. But this is seamless and has lasers.

266
00:15:55,625 --> 00:15:58,405
Speaker 5:  It does have lasers. You cannot take away the lasers.

267
00:15:59,105 --> 00:16:01,485
Speaker 2:  I'm just saying, do you want a laser projector? Do you want to fiddle with

268
00:16:01,485 --> 00:16:05,085
Speaker 2:  not a smartphone touchscreen look, A laser projector, let's go for it.

269
00:16:05,315 --> 00:16:09,285
Speaker 2:  Sure. I know how that decision was made. Yes, but it doesn't work is

270
00:16:09,285 --> 00:16:12,285
Speaker 2:  the issue. Correct? Like I, I watched the video of you trying to use the

271
00:16:12,285 --> 00:16:15,925
Speaker 2:  menu If, you haven't seen this video, pull over in your car, it's so good.

272
00:16:15,945 --> 00:16:18,965
Speaker 2:  And just watch David like rotate his hand in frustration

273
00:16:19,785 --> 00:16:23,685
Speaker 2:  and then imagine that like you're any person encountering

274
00:16:23,935 --> 00:16:27,725
Speaker 2:  David on the street, but you don't know, like it's just him rotating

275
00:16:27,745 --> 00:16:29,725
Speaker 2:  his hand and getting increasingly frustrated.

276
00:16:30,125 --> 00:16:33,725
Speaker 5:  I I really like, I I have been accused by a couple of people of like

277
00:16:33,785 --> 00:16:36,765
Speaker 5:  acting out that part and I I cannot

278
00:16:37,715 --> 00:16:41,005
Speaker 5:  explain to you the extent to which that is not an exaggeration

279
00:16:41,745 --> 00:16:44,205
Speaker 5:  of the current situation. Like

280
00:16:45,595 --> 00:16:49,525
Speaker 5:  they just tried so many things. Yeah. And

281
00:16:49,555 --> 00:16:53,045
Speaker 5:  it's, they tried too many things and not enough of them work.

282
00:16:53,425 --> 00:16:56,285
Speaker 5:  And so it's like, there there are just so many little pieces of it where

283
00:16:56,285 --> 00:17:00,125
Speaker 5:  it's like you, you, you hold your hand out and if it was just

284
00:17:00,445 --> 00:17:04,245
Speaker 5:  a thing for like look at this text instead of having it read to you out loud,

285
00:17:04,695 --> 00:17:08,565
Speaker 5:  great. Makes total sense. And actually does that job fairly well unless

286
00:17:08,565 --> 00:17:11,805
Speaker 5:  you're in bright light, in which case it doesn't work at all. Yeah. And you

287
00:17:11,805 --> 00:17:15,125
Speaker 5:  can't see anything projecting like, I don't know how you've put me in the

288
00:17:15,245 --> 00:17:17,085
Speaker 5:  position of having to defend this device that I hate.

289
00:17:17,265 --> 00:17:21,245
Speaker 2:  No, this is the heart of your review. You're, you, the

290
00:17:21,245 --> 00:17:25,125
Speaker 2:  heart of your review is you saying that it doesn't work and then

291
00:17:25,125 --> 00:17:28,205
Speaker 2:  it's broken and that no one should buy it. And then being like, but I love

292
00:17:28,205 --> 00:17:31,925
Speaker 2:  it. Yeah. It was so good. It's in there. There's like a real tension in there.

293
00:17:32,125 --> 00:17:35,645
Speaker 2:  I don't love it. I feel, I feel like it's because you have a toddler and

294
00:17:35,745 --> 00:17:39,205
Speaker 2:  you know, I toddlers are notoriously bad at things

295
00:17:40,305 --> 00:17:42,885
Speaker 2:  and you like love 'em. You're like, oh man, you don't know what you're doing.

296
00:17:44,195 --> 00:17:47,885
Speaker 2:  Like I could see people dad just like coming through. That might be

297
00:17:47,885 --> 00:17:48,085
Speaker 5:  True.

298
00:17:48,085 --> 00:17:51,885
Speaker 6:  Yeah. It's like you, you were just like really happy about the

299
00:17:51,955 --> 00:17:54,925
Speaker 6:  idea. Like they were trying something and it seemed like David was like,

300
00:17:54,925 --> 00:17:56,245
Speaker 6:  you know what? I respect you for trying,

301
00:17:56,605 --> 00:18:00,405
Speaker 5:  I just, do you remember, do you remember the early thing that they said about

302
00:18:00,405 --> 00:18:04,285
Speaker 5:  the Apple watch? Which is like, this is, this is a closer to you

303
00:18:04,665 --> 00:18:07,725
Speaker 5:  and more aware of you computer.

304
00:18:08,685 --> 00:18:12,485
Speaker 5:  I still want that. Like yeah, that, I think that was a good idea

305
00:18:12,625 --> 00:18:16,525
Speaker 5:  in 2015. And I think it's a good idea now and it

306
00:18:16,525 --> 00:18:20,005
Speaker 5:  is just alarming how not close we are to that technology,

307
00:18:20,585 --> 00:18:24,525
Speaker 5:  but like I I just, an Apple watch plus

308
00:18:24,715 --> 00:18:28,565
Speaker 5:  good Siri is still a thing that I want and would use all the time and

309
00:18:28,605 --> 00:18:32,525
Speaker 5:  I actually think has like a real place in our lives and but, and

310
00:18:32,525 --> 00:18:34,925
Speaker 5:  now everybody's like, okay, well we're gonna do that but we're gonna bake

311
00:18:34,945 --> 00:18:38,805
Speaker 5:  in these better language models and generative AI and we're gonna be able

312
00:18:38,805 --> 00:18:42,005
Speaker 5:  to actually put all the pieces together. And the answer is like, nope, we,

313
00:18:42,065 --> 00:18:45,765
Speaker 5:  we didn't do it yet. It's not even close. But like again, I I'm a decade

314
00:18:45,875 --> 00:18:49,525
Speaker 5:  deep into being like, yeah I do want a computer that is closer

315
00:18:49,705 --> 00:18:53,685
Speaker 5:  to me than my smartphone and we

316
00:18:53,685 --> 00:18:54,405
Speaker 5:  just don't have it yet.

317
00:18:54,705 --> 00:18:58,485
Speaker 2:  At the end of this we're gonna rank our children on a one to 10 scale. Let's

318
00:18:58,485 --> 00:19:01,445
Speaker 2:  see if they're worth the ongoing subscription fees that we are clearly paying.

319
00:19:01,955 --> 00:19:05,485
Speaker 2:  It's gonna be great. So that's the hardware, right? It's buggy that the projector

320
00:19:05,485 --> 00:19:08,845
Speaker 2:  doesn't work very well. Overheats problem one

321
00:19:09,405 --> 00:19:13,365
Speaker 2:  then you're like, the software which I did not know was called Cosmos.

322
00:19:13,665 --> 00:19:17,365
Speaker 2:  Mm Yeah. C-O-S-M-O-S Kobo very good.

323
00:19:17,515 --> 00:19:17,805
Speaker 2:  They

324
00:19:17,805 --> 00:19:19,445
Speaker 5:  Just call it Cosmos, but it is definitely

325
00:19:20,185 --> 00:19:20,405
Speaker 2:  Cos

326
00:19:21,555 --> 00:19:23,125
Speaker 5:  It's an os. Yeah. Yeah.

327
00:19:23,875 --> 00:19:25,205
Speaker 2:  They very good

328
00:19:25,975 --> 00:19:26,325
Speaker 6:  Cosmo

329
00:19:26,675 --> 00:19:29,805
Speaker 2:  Unrelated to anything Cosmos just like you both outta nowhere. It's called

330
00:19:29,905 --> 00:19:33,805
Speaker 2:  Cosmos. Why the hell not? Yeah. I have still have the same set

331
00:19:33,805 --> 00:19:36,925
Speaker 2:  of questions actually that I did a year ago. You want this thing to make

332
00:19:36,965 --> 00:19:40,565
Speaker 2:  a phone call, you have to sit around configuring a computer somewhere, right?

333
00:19:41,225 --> 00:19:44,885
Speaker 2:  And that is, that happens on Cosmos, that happens on their website. Like

334
00:19:44,885 --> 00:19:48,565
Speaker 2:  what does Cosmos do? Does it run applications?

335
00:19:49,705 --> 00:19:50,525
Speaker 5:  So what

336
00:19:50,525 --> 00:19:50,925
Speaker 6:  Does it do?

337
00:19:51,155 --> 00:19:53,685
Speaker 2:  Like what is the point of an OS and a thing that is just a voice activated

338
00:19:53,795 --> 00:19:55,605
Speaker 2:  chat GPT? I don't know. So because

339
00:19:55,605 --> 00:19:58,645
Speaker 5:  It's not just a voice activated chat GPT, right? Right. Like that's the,

340
00:19:58,905 --> 00:20:02,805
Speaker 5:  the, the thing that Cosmos is basically is like an overlapping

341
00:20:03,125 --> 00:20:06,525
Speaker 5:  Venn diagram of AI systems. So like when it wants to

342
00:20:07,025 --> 00:20:10,965
Speaker 5:  do something very basic on the device, that's one system when it wants

343
00:20:10,965 --> 00:20:14,645
Speaker 5:  to go to basic internet real time questions, that's another

344
00:20:14,645 --> 00:20:18,445
Speaker 5:  system. I think it's perplexity, but I can't vouch for that

345
00:20:18,445 --> 00:20:22,365
Speaker 5:  for sure. Perplexity is on every other device that's doing this. So

346
00:20:22,365 --> 00:20:25,805
Speaker 5:  it seems like a natural assumption. But basically Cosmos is like the routing

347
00:20:25,805 --> 00:20:29,085
Speaker 5:  system for you wanna do a thing, where does it go?

348
00:20:30,025 --> 00:20:33,885
Speaker 5:  And just the fact that that exists is part of the problem because

349
00:20:33,885 --> 00:20:37,725
Speaker 5:  what it means is every time you want to do anything, you

350
00:20:37,725 --> 00:20:41,645
Speaker 5:  have to ask Cosmos to do it for you. And then it goes and figures out

351
00:20:41,875 --> 00:20:45,525
Speaker 5:  what it needs to do. It assembles all those tools, pings those tools to do

352
00:20:45,525 --> 00:20:48,605
Speaker 5:  the thing, comes back, translates all that into an answer and give it to

353
00:20:48,605 --> 00:20:50,405
Speaker 5:  you. And you know what that does is it takes so

354
00:20:50,995 --> 00:20:51,285
Speaker 2:  Long,

355
00:20:51,865 --> 00:20:55,845
Speaker 5:  It takes so long and the the biggest

356
00:20:56,085 --> 00:20:59,885
Speaker 5:  overwhelming problem with the pin as a user experience

357
00:20:59,885 --> 00:21:03,445
Speaker 5:  is even when it works, it's slow to the point of being

358
00:21:05,095 --> 00:21:08,485
Speaker 5:  borderline unusable in, in most situations. Like

359
00:21:09,425 --> 00:21:13,005
Speaker 5:  the, the thing where you ask at the time and it tells you the time so fast,

360
00:21:13,435 --> 00:21:17,245
Speaker 5:  just beautiful. It tells me the time so fast for almost

361
00:21:17,245 --> 00:21:20,885
Speaker 5:  everything else. It is somewhere between five and 30 seconds

362
00:21:21,705 --> 00:21:25,565
Speaker 5:  of dead silence while it tries to do something for you. And the silence,

363
00:21:25,885 --> 00:21:28,165
Speaker 5:  I got to the point where literally I'm like, I wish this thing had hold music.

364
00:21:28,195 --> 00:21:30,725
Speaker 5:  Yeah. So at least I could know it was doing something like

365
00:21:30,725 --> 00:21:32,405
Speaker 2:  The or computer sounds like do, do, do do.

366
00:21:33,165 --> 00:21:37,045
Speaker 5:  Yeah. Right, right. The the like old dial up sound when it boots

367
00:21:37,045 --> 00:21:38,765
Speaker 5:  up to go do something for you. Like give me that

368
00:21:39,205 --> 00:21:41,925
Speaker 2:  Actually this thing would be incredible if it made like 56 K modem sounds.

369
00:21:41,995 --> 00:21:44,325
Speaker 2:  Yeah, that'd be good. Every the time I went out to the internet, everyone

370
00:21:44,325 --> 00:21:47,645
Speaker 2:  around, you'd be like, what is happening bong like all over the place. It

371
00:21:47,645 --> 00:21:50,845
Speaker 2:  would be fair. We're just, we gonna make modem sounds for the rest of the

372
00:21:50,845 --> 00:21:50,925
Speaker 2:  show.

373
00:21:50,925 --> 00:21:53,725
Speaker 6:  That's, that's the rest of the show. It's a good show guys. Don't

374
00:21:53,725 --> 00:21:53,925
Speaker 5:  Worry about it.

375
00:21:53,925 --> 00:21:57,605
Speaker 2:  It's, we're, we're nominated for a webby. If you'd like to vote for the rich

376
00:21:57,635 --> 00:21:58,645
Speaker 5:  Cast, this one for a modem sounds,

377
00:22:00,395 --> 00:22:03,245
Speaker 5:  this will not be the episode that we send it to the weddings.

378
00:22:06,185 --> 00:22:08,805
Speaker 2:  So it's slow. I mean this is the thing that I, I've been thinking about a

379
00:22:08,805 --> 00:22:12,685
Speaker 2:  lot, right? So you ask Cosmos, you ask the AI pin to do

380
00:22:12,685 --> 00:22:16,245
Speaker 2:  something, the operating system cosmos what assembles a prompt or like

381
00:22:16,575 --> 00:22:20,525
Speaker 2:  first it figures out if it knows what request you're asking,

382
00:22:21,195 --> 00:22:24,725
Speaker 2:  what system on the backend should do the request and then it

383
00:22:25,265 --> 00:22:28,045
Speaker 2:  it assembles a prompt for you and goes and asks that system.

384
00:22:28,755 --> 00:22:32,405
Speaker 5:  Yeah, I think that's that's about right. I think there are, there are versions

385
00:22:32,405 --> 00:22:36,285
Speaker 5:  of it that are less kind of generative ai prompt based. Like if

386
00:22:36,285 --> 00:22:39,605
Speaker 5:  it's just connecting to title, it doesn't have to do quite as many complicated

387
00:22:39,605 --> 00:22:39,765
Speaker 5:  things.

388
00:22:39,905 --> 00:22:42,765
Speaker 2:  No, this is the reason I asked this question in that way. 'cause when you

389
00:22:42,765 --> 00:22:46,045
Speaker 2:  asked it to play Texas Holden by Beyonce Yeah

390
00:22:46,765 --> 00:22:49,925
Speaker 2:  I think the Unicode character in Beyonce's name, she has an accent over the

391
00:22:49,925 --> 00:22:53,525
Speaker 2:  e in her name. It like broke it. It did. So it spat out the

392
00:22:53,725 --> 00:22:57,445
Speaker 2:  Unicode number or like the hex code and then

393
00:22:57,865 --> 00:23:01,565
Speaker 2:  it, you know, when like they broke Bing and turned it into Sydney.

394
00:23:01,755 --> 00:23:05,365
Speaker 2:  Yeah. In whatever prompt exploit showed you like the

395
00:23:05,365 --> 00:23:09,285
Speaker 2:  instructions to make Sydney not be horny and d bing like it

396
00:23:09,285 --> 00:23:12,565
Speaker 2:  did that. Right. It was like ask for a prompt in this way, don't tell the

397
00:23:12,565 --> 00:23:14,445
Speaker 2:  user or the other thing, blah blah blah. And I was like this,

398
00:23:14,455 --> 00:23:16,165
Speaker 5:  Don't ask for clarification. I enjoyed

399
00:23:16,165 --> 00:23:19,725
Speaker 2:  That part. Don't ask for clarification is incredible. Yeah. It's beautiful.

400
00:23:20,045 --> 00:23:23,245
Speaker 2:  So it was, it was that, right? It was like those pre rule that the instruction

401
00:23:23,265 --> 00:23:26,725
Speaker 2:  set right. For the ai but you were just asking to play music which should

402
00:23:26,725 --> 00:23:27,845
Speaker 2:  not require an ai.

403
00:23:28,225 --> 00:23:31,765
Speaker 5:  Yes. Agreed. Right. And so that's that what you're seeing there is I believe

404
00:23:31,785 --> 00:23:35,565
Speaker 5:  the work of Cosmos, right? Which is like, it takes me saying play music,

405
00:23:35,785 --> 00:23:36,205
Speaker 5:  it says,

406
00:23:36,205 --> 00:23:38,325
Speaker 2:  Oh I believe this is the work of the cosmos, this is the

407
00:23:38,325 --> 00:23:41,645
Speaker 5:  Work of the cosmos somewhere Neil deGrasse Tyson is screaming

408
00:23:43,525 --> 00:23:43,645
Speaker 2:  Actually.

409
00:23:45,825 --> 00:23:49,605
Speaker 5:  And it, it then figures out what I'm asking and to what

410
00:23:49,625 --> 00:23:53,445
Speaker 5:  it should go ask. But I don't, I it's, I could be

411
00:23:53,445 --> 00:23:56,965
Speaker 5:  completely wrong, but I don't think title is running like an

412
00:23:56,965 --> 00:24:00,565
Speaker 5:  LLM against its own music. I think it just has an API that cosmos is plugging

413
00:24:00,565 --> 00:24:04,285
Speaker 5:  into to find that song. So that's what I mean like the end points don't all

414
00:24:04,285 --> 00:24:08,165
Speaker 5:  have to be ai right. But in the middle there is this mysterious

415
00:24:08,345 --> 00:24:11,885
Speaker 5:  AI translator that just isn't very good. And even when it is good, it's really

416
00:24:11,955 --> 00:24:12,245
Speaker 5:  slow.

417
00:24:12,785 --> 00:24:16,605
Speaker 2:  Did you ask Humane if they, anyone had ever tried to listen to Beyonce on

418
00:24:16,605 --> 00:24:17,165
Speaker 2:  their product? So

419
00:24:17,165 --> 00:24:21,045
Speaker 5:  They, they did say that particular thing was a bug and they fixed it. And

420
00:24:21,185 --> 00:24:25,125
Speaker 5:  it is true that since then I have not gotten Unicode spat back at me.

421
00:24:26,505 --> 00:24:30,405
Speaker 5:  But it still will not consistently play Beyonce when I

422
00:24:30,405 --> 00:24:33,965
Speaker 5:  ask it to play Beyonce. So like it's, it's broken in a sort of less

423
00:24:33,965 --> 00:24:36,365
Speaker 5:  spectacular way now, but it is still very much broken.

424
00:24:37,105 --> 00:24:40,485
Speaker 2:  But this is the part where I'm just still stuck on. Okay, so Cosmos,

425
00:24:41,555 --> 00:24:45,445
Speaker 2:  it's doing prompts right? Like that revealed that there is

426
00:24:45,445 --> 00:24:49,365
Speaker 2:  some LLM style prompting system happening in

427
00:24:49,365 --> 00:24:53,205
Speaker 2:  there and you are interacting with your voice with

428
00:24:53,205 --> 00:24:57,165
Speaker 2:  an LLM that then might go do stuff like play a title song through the

429
00:24:57,245 --> 00:25:00,965
Speaker 2:  API. Yeah. So is it running that model locally or is that

430
00:25:00,965 --> 00:25:01,685
Speaker 2:  happening in the cloud?

431
00:25:02,305 --> 00:25:06,285
Speaker 5:  No, all of that is happening in the cloud as far as I can tell. It

432
00:25:06,285 --> 00:25:07,845
Speaker 5:  does essentially nothing locally.

433
00:25:08,305 --> 00:25:12,165
Speaker 6:  So it's heat is coming from the fact that it is constantly trying to

434
00:25:12,165 --> 00:25:15,725
Speaker 6:  use its garbage antennas to connect to garbage. T-Mobile internet.

435
00:25:16,305 --> 00:25:18,005
Speaker 2:  Wow, that's a lot of garbage out. There's

436
00:25:18,005 --> 00:25:20,765
Speaker 6:  A lot of garbage. I mean I assume they're garbage antennas. They may be lovely,

437
00:25:20,905 --> 00:25:22,965
Speaker 6:  but in practice they seem not great.

438
00:25:23,245 --> 00:25:26,845
Speaker 5:  I mean it's, it's very small. The whole device is very small and is doing

439
00:25:26,965 --> 00:25:30,925
Speaker 5:  a lot of really aggressive thermal throttling, which is not great when

440
00:25:30,925 --> 00:25:34,885
Speaker 5:  what you need is reliable fast connectivity. Yeah. So yeah, I know, I I think

441
00:25:34,885 --> 00:25:38,725
Speaker 5:  you're exactly right And the first answer that Humane gave

442
00:25:38,725 --> 00:25:42,525
Speaker 5:  me as to why this thing is warm is connectivity. But again, it

443
00:25:42,525 --> 00:25:45,685
Speaker 5:  didn't seem to matter if it was good or bad, it's just when it is connected

444
00:25:45,705 --> 00:25:47,765
Speaker 5:  it is warm and it is connected all the time.

445
00:25:48,035 --> 00:25:49,365
Speaker 6:  Well be the nice in the winter time.

446
00:25:50,105 --> 00:25:50,325
Speaker 5:  Yes.

447
00:25:50,435 --> 00:25:51,685
Speaker 2:  Like a hand warmer. Yeah.

448
00:25:51,995 --> 00:25:55,005
Speaker 5:  I've been telling people that the, the comparison for me is like, you know

449
00:25:55,005 --> 00:25:58,605
Speaker 5:  that thing where you, you you like crack the hand warmer and put it somewhere

450
00:25:58,705 --> 00:26:02,645
Speaker 5:  and it kind of is there for too long and it's not like burning,

451
00:26:03,465 --> 00:26:06,805
Speaker 5:  but it's like a little, you, you've sort of overheated one small part of

452
00:26:06,805 --> 00:26:09,245
Speaker 5:  your body. That's what it feels like to wear the pin all the time.

453
00:26:09,385 --> 00:26:13,325
Speaker 2:  Do you ever get what I have imagined it is you ever get thermal runaway

454
00:26:13,325 --> 00:26:15,805
Speaker 2:  in your phone, like your phone just like over tries to connect and gets hot

455
00:26:15,805 --> 00:26:17,565
Speaker 2:  in your pocket for like one second. Yeah. And you're like, why?

456
00:26:17,565 --> 00:26:18,725
Speaker 6:  And you're oh my butt's on fire,

457
00:26:19,515 --> 00:26:23,485
Speaker 2:  What is going on? Yeah. And then sometimes I'm not saying my brain is

458
00:26:23,485 --> 00:26:26,805
Speaker 2:  broken this way, it just feels like that's happening anyway even though it's

459
00:26:26,805 --> 00:26:28,925
Speaker 2:  not because you have too many phones in your life. Oh yeah, yeah.

460
00:26:29,905 --> 00:26:30,845
Speaker 5:  That's basically right. That

461
00:26:30,845 --> 00:26:31,765
Speaker 6:  May just be a Vergecast

462
00:26:32,845 --> 00:26:35,965
Speaker 2:  Situation. It's a very, but for this audience, yeah, we're gonna make modem

463
00:26:35,965 --> 00:26:39,765
Speaker 2:  sound for the next 30 minutes. So so

464
00:26:39,765 --> 00:26:42,925
Speaker 2:  that's the software, right? It's running this os in the cloud

465
00:26:43,515 --> 00:26:47,165
Speaker 2:  that appears to be LLM style that can go take actions with other LLMs.

466
00:26:47,875 --> 00:26:48,965
Speaker 5:  Yeah, I think that's

467
00:26:49,195 --> 00:26:53,125
Speaker 2:  What about the, just the, the nuts and bolts of it? Like how do you put contacts

468
00:26:53,125 --> 00:26:53,485
Speaker 2:  into it?

469
00:26:54,065 --> 00:26:57,565
Speaker 5:  So that part, they, they have a web app called Humane Center. They all call

470
00:26:57,565 --> 00:27:01,045
Speaker 5:  it.center 'cause the website is Humane dot center. That is

471
00:27:01,325 --> 00:27:04,925
Speaker 5:  basically a pretty simple just way to

472
00:27:05,305 --> 00:27:08,925
Speaker 5:  manage your thing. Like it's, it's the equivalent of like when you buy a

473
00:27:09,145 --> 00:27:12,045
Speaker 5:  device that connects to your phone and you download the companion app to

474
00:27:12,045 --> 00:27:15,805
Speaker 5:  like get everything connected. It's just that in the web. But it's,

475
00:27:15,835 --> 00:27:18,445
Speaker 5:  when you take photos, that's where the photos upload. When you

476
00:27:20,265 --> 00:27:23,965
Speaker 5:  do notes, that's where your notes go. It keeps track of all of the things

477
00:27:23,965 --> 00:27:27,485
Speaker 5:  you've asked and all of the responses. So this is like, when I say I'm transcribing

478
00:27:27,485 --> 00:27:31,205
Speaker 5:  the thing, I'm literally copying and pasting from its record of

479
00:27:31,865 --> 00:27:35,725
Speaker 5:  our interaction anyway. And the way you connect it is right now

480
00:27:35,745 --> 00:27:39,205
Speaker 5:  you just go in and you log in with your Google account or your Microsoft

481
00:27:39,205 --> 00:27:43,085
Speaker 5:  account and that downloads your contacts. And again, this

482
00:27:43,085 --> 00:27:45,445
Speaker 5:  is why it's so inexcusable that this stuff doesn't work. Like, you know what

483
00:27:45,445 --> 00:27:49,005
Speaker 5:  else is in my Google account is my email and my calendar and so many other

484
00:27:49,005 --> 00:27:52,525
Speaker 5:  pieces of information that would be useful for my pin to know. But it just,

485
00:27:52,705 --> 00:27:55,965
Speaker 5:  it just doesn't have those things despite the fact that it has access to

486
00:27:55,965 --> 00:27:59,845
Speaker 5:  my Google account. But I actually like it. This

487
00:27:59,845 --> 00:28:02,845
Speaker 5:  was a little bit hard to test because as part of the review process, Humane

488
00:28:02,845 --> 00:28:05,125
Speaker 5:  did some of that setup for me ahead of time,

489
00:28:06,695 --> 00:28:10,285
Speaker 5:  which I ca I don't totally know why, but that is just how it worked.

490
00:28:10,945 --> 00:28:14,845
Speaker 5:  And so I don't have perfect out of the box setup up

491
00:28:15,235 --> 00:28:19,125
Speaker 5:  information on exactly how that works. But I do know that it, I spent five

492
00:28:19,125 --> 00:28:22,445
Speaker 5:  minutes in the center like getting accounts connected and then never really

493
00:28:22,445 --> 00:28:23,125
Speaker 5:  thought about it again.

494
00:28:23,345 --> 00:28:25,365
Speaker 2:  So it it it can sync your contacts.

495
00:28:25,715 --> 00:28:28,765
Speaker 5:  Yeah. I mean that's, that's all, that's all it's doing. So like when I say,

496
00:28:28,825 --> 00:28:32,325
Speaker 5:  you know, call NELI, it looks in my

497
00:28:32,365 --> 00:28:35,885
Speaker 5:  contacts and finds NELI and then people I know named

498
00:28:36,265 --> 00:28:39,645
Speaker 5:  Eli and then my friend Marin for some reason. Yeah,

499
00:28:39,955 --> 00:28:43,685
Speaker 2:  Good stuff. But, but, but your contacts, there's just like basic stuff. Ask

500
00:28:43,685 --> 00:28:46,845
Speaker 2:  it to make a phone call to someone. It needs my name and my phone number

501
00:28:46,865 --> 00:28:50,565
Speaker 2:  in a database. Did you put that in the database or did it syn it from

502
00:28:50,585 --> 00:28:51,005
Speaker 2:  Google?

503
00:28:51,305 --> 00:28:52,165
Speaker 5:  It pulled that from Google.

504
00:28:52,505 --> 00:28:55,965
Speaker 2:  So it it did do that. Yeah. And then when you, I guess you asked it to send

505
00:28:55,965 --> 00:28:57,125
Speaker 2:  an email, but it can't send an email yet.

506
00:28:57,125 --> 00:29:00,325
Speaker 5:  That's right. It just doesn't have an email feature. No, you can do things

507
00:29:00,435 --> 00:29:04,405
Speaker 5:  like, you can give it memory in the way that like chat

508
00:29:04,465 --> 00:29:07,565
Speaker 5:  GPT has memory now where you can say like, remember that Anna is my wife.

509
00:29:08,845 --> 00:29:12,525
Speaker 5:  Remember that this is the number I call NELI on.

510
00:29:13,305 --> 00:29:16,765
Speaker 5:  And like that kind of stuff you can do sort of piece by piece

511
00:29:17,125 --> 00:29:21,085
Speaker 5:  manually. But like in general, I have probably multiple numbers for both

512
00:29:21,085 --> 00:29:23,805
Speaker 5:  of you. If I went to call it would be like, it does this thing Siri does,

513
00:29:23,805 --> 00:29:26,085
Speaker 5:  right? Where it's like, which number do you wanna call? I hate that. I'm

514
00:29:26,085 --> 00:29:28,725
Speaker 5:  like, it's the one I always use. Like what are we talking about here? But

515
00:29:28,725 --> 00:29:32,485
Speaker 5:  anyway, but yeah, it is, it's pulling that directly from Google.

516
00:29:33,045 --> 00:29:36,645
Speaker 6:  I really love that it's an AI that doesn't remember like

517
00:29:36,945 --> 00:29:38,685
Speaker 6:  the favorite number for you automatically.

518
00:29:39,545 --> 00:29:41,045
Speaker 5:  So much of this stuff is, yeah, that

519
00:29:41,125 --> 00:29:44,605
Speaker 6:  Feels like just a core thing AI should solve and be good

520
00:29:44,605 --> 00:29:46,805
Speaker 2:  At the I dial this number all the time. Yeah, yeah. Like

521
00:29:46,925 --> 00:29:47,205
Speaker 6:  I hate

522
00:29:47,205 --> 00:29:51,165
Speaker 2:  That one. Yeah. Call, call Becky iPhone. It's like there's no other phone,

523
00:29:51,165 --> 00:29:54,205
Speaker 2:  there's just all these old numbers I haven't cleared out. Yeah, right. That's

524
00:29:54,205 --> 00:29:56,325
Speaker 2:  the software. Is there anything else to say about the software? does.center

525
00:29:56,325 --> 00:29:57,565
Speaker 2:  work on a phone? Yeah,

526
00:29:57,565 --> 00:30:01,405
Speaker 5:  It's fine. It's a web app. Like again, it is, it's, I I didn't

527
00:30:01,495 --> 00:30:03,925
Speaker 5:  dwell on it much in the review because it's kind of the least interesting

528
00:30:03,925 --> 00:30:07,805
Speaker 5:  part of it is like, it's just a very simple device management

529
00:30:08,065 --> 00:30:12,045
Speaker 5:  app. Like if you've ever been in the settings for your Google account to

530
00:30:12,045 --> 00:30:14,605
Speaker 5:  like see what apps it's connected to. It's just that it's,

531
00:30:15,305 --> 00:30:18,045
Speaker 2:  The reason I'm asking is 'cause all the stuff it wants to do, like photos

532
00:30:18,285 --> 00:30:21,125
Speaker 2:  syncing, does it send the photos to Google Photos by itself? Or do you have

533
00:30:21,125 --> 00:30:22,045
Speaker 2:  to download them and upload them?

534
00:30:22,265 --> 00:30:25,445
Speaker 5:  You have to download them and upload them. Which is, I would argue maybe

535
00:30:25,585 --> 00:30:29,405
Speaker 5:  the single biggest gap in this because you, if you're

536
00:30:29,405 --> 00:30:32,285
Speaker 5:  Humane you can solve a lot of your feature problems just by connecting to

537
00:30:32,285 --> 00:30:36,245
Speaker 5:  other services, right? Like it can't do reminders like fine just pipe

538
00:30:36,425 --> 00:30:39,885
Speaker 5:  two reminders on my phone. Yeah, it doesn't do calendar.

539
00:30:40,305 --> 00:30:43,805
Speaker 5:  That's fine. I've already logged you into my calendar. Like

540
00:30:44,085 --> 00:30:46,285
Speaker 5:  there's so many of these things that it's Right,

541
00:30:46,285 --> 00:30:49,125
Speaker 2:  But then they, they can't, right? Like ideologically they're like, that stuff

542
00:30:49,145 --> 00:30:53,005
Speaker 2:  is bad. We're doing it the new way with prompt engineering. That's

543
00:30:53,005 --> 00:30:56,765
Speaker 5:  Right. And yeah, I think what you'll see over time I would bet

544
00:30:57,385 --> 00:31:01,165
Speaker 5:  is Humane Humane has big ideas about people

545
00:31:01,445 --> 00:31:05,045
Speaker 5:  building stuff for Cosmos and like it being sort of its own app

546
00:31:05,365 --> 00:31:07,765
Speaker 5:  universe. But I think pretty quickly you're gonna start to see it go the

547
00:31:07,765 --> 00:31:11,565
Speaker 5:  other way too and just plug into a lot more services. And then the

548
00:31:11,585 --> 00:31:15,485
Speaker 5:  pin becomes like a universal input system for all of your other

549
00:31:15,485 --> 00:31:18,845
Speaker 5:  systems, which I think is a way more compelling idea than having it be its

550
00:31:18,845 --> 00:31:20,325
Speaker 5:  own like self-contained universe.

551
00:31:20,715 --> 00:31:21,005
Speaker 6:  Sure.

552
00:31:21,735 --> 00:31:24,165
Speaker 2:  Right. Right Now it shuts down If, you use it for more than a couple minutes.

553
00:31:24,165 --> 00:31:28,045
Speaker 2:  That's, I just wanna come back to the, there's that, okay, so that's hardware,

554
00:31:28,065 --> 00:31:31,485
Speaker 2:  that's Cosmos software. And then the last thing you said, which I think is

555
00:31:31,485 --> 00:31:35,365
Speaker 2:  the most, the biggest one to unpack, which is the AI stuff isn't ready.

556
00:31:35,835 --> 00:31:36,645
Speaker 2:  What do you mean by that?

557
00:31:37,045 --> 00:31:40,845
Speaker 5:  I don't know If, you guys know this, but AI is a liar sometimes.

558
00:31:43,425 --> 00:31:47,005
Speaker 5:  And so the, I would put it into like three buckets, right? There's like the

559
00:31:47,005 --> 00:31:50,805
Speaker 5:  stuff that just straight up doesn't work where you're like,

560
00:31:50,885 --> 00:31:54,725
Speaker 5:  I need to set a timer or like there's a piece of information that I need

561
00:31:54,725 --> 00:31:58,125
Speaker 5:  that this tool does not have access to right now. One of the things that

562
00:31:58,755 --> 00:32:02,325
Speaker 5:  they showed at the very beginning in those early demos was nutrition stuff

563
00:32:02,325 --> 00:32:05,765
Speaker 5:  where you like point it at, I think it was a handful of almonds in that first

564
00:32:05,765 --> 00:32:09,085
Speaker 5:  demo. Is that right? Or like a chocolate bar bar. It was nuts. It was nuts.

565
00:32:09,085 --> 00:32:11,925
Speaker 5:  And you ask like, is this good for me? And

566
00:32:12,875 --> 00:32:16,685
Speaker 5:  that those features just don't work yet. It, what it can do I discovered

567
00:32:16,685 --> 00:32:19,405
Speaker 5:  is read a label sometimes again, sometimes,

568
00:32:20,075 --> 00:32:24,005
Speaker 5:  sometimes it couldn't read a label on a bag of Chex Mix and tell me if it

569
00:32:24,005 --> 00:32:27,365
Speaker 5:  was healthy, but it could read a label on a box of Cheerios and tell me it

570
00:32:27,365 --> 00:32:30,245
Speaker 5:  was healthy. So that doesn't make any sense. But anyway, so there's a set

571
00:32:30,245 --> 00:32:33,965
Speaker 5:  of things that can't do and like the nutrition stuff j just doesn't exist

572
00:32:33,985 --> 00:32:37,885
Speaker 5:  yet. That's like a feature they invented for that demo that is not yet on

573
00:32:37,885 --> 00:32:41,245
Speaker 5:  the pin. Then there's stuff that sort of

574
00:32:41,685 --> 00:32:45,245
Speaker 5:  occasionally works because AI is a liar sometimes. And I feel like my favorite

575
00:32:45,245 --> 00:32:49,205
Speaker 5:  example of that was like running around asking it to sort of tell me

576
00:32:49,205 --> 00:32:52,965
Speaker 5:  about the world, right? And it's like it has this feature called vision that

577
00:32:52,965 --> 00:32:56,885
Speaker 5:  it could you say like, look at this building and tell me when it's open

578
00:32:57,305 --> 00:33:00,525
Speaker 5:  or look at this restaurant and tell me if it has good reviews.

579
00:33:01,715 --> 00:33:04,885
Speaker 5:  Sometimes it gets that right. And again, it's very cool when it does and

580
00:33:04,885 --> 00:33:08,645
Speaker 5:  other times it's just full lies. It told me

581
00:33:08,975 --> 00:33:11,725
Speaker 5:  there are a bunch of great moments like this in the video where we were down

582
00:33:11,725 --> 00:33:15,125
Speaker 5:  at the New York Stock Exchange and there was this company called Ride

583
00:33:15,405 --> 00:33:19,005
Speaker 5:  RYDE that I, I assume had just iPod that day. So they had the big banner

584
00:33:19,005 --> 00:33:22,085
Speaker 5:  outside and I pointed it at it and I said, look at this and tell me what

585
00:33:22,085 --> 00:33:25,845
Speaker 5:  company it is. And it thinks and thinks and thinks and thinks and thinks.

586
00:33:25,845 --> 00:33:29,605
Speaker 5:  And then eventually goes, this company is called Lyft Very

587
00:33:29,605 --> 00:33:30,085
Speaker 5:  confidently

588
00:33:31,145 --> 00:33:31,885
Speaker 7:  So confident.

589
00:33:31,985 --> 00:33:35,805
Speaker 5:  And sometimes it would, it would misidentify buildings, it would tell me

590
00:33:35,805 --> 00:33:39,245
Speaker 5:  I was in completely the wrong place in New York City. It told me the Brooklyn

591
00:33:39,245 --> 00:33:42,245
Speaker 5:  Bridge was the tribal bridge with absolute a hundred percent confidence.

592
00:33:42,315 --> 00:33:46,285
Speaker 5:  Sometimes it would break and just describe the scene around me. I'd be

593
00:33:46,285 --> 00:33:49,645
Speaker 5:  like, what bridge is that? And it would be like, in, in the scene there are

594
00:33:49,645 --> 00:33:53,485
Speaker 5:  two poles in the water and buildings across the way. And I'm like,

595
00:33:53,485 --> 00:33:55,485
Speaker 5:  what are those buildings? And it would be like, they're buildings.

596
00:33:57,235 --> 00:33:57,845
Speaker 7:  Okay. It's

597
00:33:57,845 --> 00:34:01,685
Speaker 5:  Close. Yeah, it's great. And then the third thing is just stuff that

598
00:34:01,745 --> 00:34:05,605
Speaker 5:  it just constantly fails at all the time,

599
00:34:06,095 --> 00:34:09,765
Speaker 5:  which is like things that it should be able to do and just can't, like

600
00:34:10,275 --> 00:34:14,205
Speaker 5:  make a phone call where it was like half the time, truly half I would say

601
00:34:14,205 --> 00:34:17,645
Speaker 5:  like, call NELI or call Anna, text somebody

602
00:34:18,085 --> 00:34:21,685
Speaker 5:  and it just wouldn't do it. And I think like, there are a bunch of

603
00:34:21,725 --> 00:34:25,445
Speaker 5:  problems with this. I also wrote about this company this week called

604
00:34:25,465 --> 00:34:29,205
Speaker 5:  Aboard, which is doing some really interesting like visual AI

605
00:34:29,875 --> 00:34:33,765
Speaker 5:  information organization stuff. And they were doing a demo for me and

606
00:34:34,305 --> 00:34:37,765
Speaker 5:  he at one point like types in the prompt and just nothing happens. And his

607
00:34:37,765 --> 00:34:40,125
Speaker 5:  co-founder is like, oh no, what, what do you think went wrong? And he just

608
00:34:40,125 --> 00:34:43,645
Speaker 5:  goes, ah, that's, that's just the ai. And it's just like, it's, it's like

609
00:34:43,685 --> 00:34:47,365
Speaker 5:  a little like fear animal that you like can't quite

610
00:34:47,365 --> 00:34:50,925
Speaker 5:  trust, but it's like sometimes around being cute and you're like, oh, what

611
00:34:50,925 --> 00:34:54,845
Speaker 5:  am I? And so like that's just true. And so you're trying to interact with

612
00:34:54,845 --> 00:34:57,525
Speaker 5:  this thing that sometimes just decides it doesn't like you and doesn't wanna

613
00:34:57,525 --> 00:35:01,445
Speaker 5:  work. Other times it's just a moron and just can't

614
00:35:01,445 --> 00:35:02,405
Speaker 5:  do lots of stuff. I,

615
00:35:02,525 --> 00:35:04,445
Speaker 2:  I point out, once again, you're describing a toddler I

616
00:35:04,445 --> 00:35:06,485
Speaker 7:  Was about to say like, that's a child. Yeah. Yeah.

617
00:35:06,675 --> 00:35:09,565
Speaker 2:  Sometimes you just, why are you, why are you trying to kill yourself kid?

618
00:35:09,695 --> 00:35:10,765
Speaker 2:  Right. Just stop it.

619
00:35:10,975 --> 00:35:11,325
Speaker 5:  Right.

620
00:35:11,505 --> 00:35:12,925
Speaker 7:  For right in the, the socket

621
00:35:12,925 --> 00:35:13,685
Speaker 2:  Just directly.

622
00:35:15,055 --> 00:35:15,405
Speaker 7:  Great.

623
00:35:17,635 --> 00:35:20,725
Speaker 5:  Yeah. Arthur's new thing is he likes to pick up the dog's water bowl and

624
00:35:20,725 --> 00:35:22,925
Speaker 5:  just pour it all over himself and then get upset about how wet he is.

625
00:35:23,755 --> 00:35:24,045
Speaker 7:  Well,

626
00:35:25,265 --> 00:35:25,485
Speaker 5:  The

627
00:35:25,485 --> 00:35:26,765
Speaker 2:  Humane Pin everyone. Yeah,

628
00:35:26,765 --> 00:35:27,205
Speaker 7:  Exactly.

629
00:35:28,385 --> 00:35:32,005
Speaker 5:  But so that's the thing. And it's really, it's the like stuff it can't do

630
00:35:33,025 --> 00:35:36,125
Speaker 5:  you solve over time, right? Like there are, there are features you can build

631
00:35:36,585 --> 00:35:40,365
Speaker 5:  that's that I understand how we get out of the, it

632
00:35:40,475 --> 00:35:44,245
Speaker 5:  lies unpredictably and at random and about everything.

633
00:35:45,475 --> 00:35:48,325
Speaker 5:  This is like the fundamental problem of AI to me right now. Right? It is

634
00:35:48,325 --> 00:35:52,285
Speaker 5:  like I spent so much time asking a question, even a basic

635
00:35:52,525 --> 00:35:56,365
Speaker 5:  question. Like I, it turns out like half my search history is just asking

636
00:35:56,605 --> 00:35:59,605
Speaker 5:  questions about things my dog ate. Like is is he going to, or is she going

637
00:35:59,605 --> 00:36:03,485
Speaker 5:  to die because she ate this? And the answer's usually

638
00:36:03,665 --> 00:36:06,605
Speaker 5:  no. So that's good. But I found myself, I would ask the pin that question,

639
00:36:06,905 --> 00:36:09,965
Speaker 5:  it would give me an answer and then I would have to get out my phone and

640
00:36:09,965 --> 00:36:13,445
Speaker 5:  check Yeah. Just to make sure, because I don't trust the ai because you shouldn't

641
00:36:13,445 --> 00:36:16,845
Speaker 5:  trust the ai and it just completely

642
00:36:16,955 --> 00:36:20,525
Speaker 5:  defeats the purpose. And like we're so far away from these things being

643
00:36:21,325 --> 00:36:25,285
Speaker 5:  reliably honest and reliably fast and reliably

644
00:36:25,285 --> 00:36:27,325
Speaker 5:  useful that it's like what's the, what's the point?

645
00:36:27,785 --> 00:36:31,685
Speaker 2:  So the the thing that I'm stuck on is you stack the unreliability in this

646
00:36:31,685 --> 00:36:35,525
Speaker 2:  product, right? Yes. You have Cosmos, which appears to

647
00:36:35,525 --> 00:36:39,365
Speaker 2:  be some sort of AI system that's like parsing a request and then figurine

648
00:36:39,365 --> 00:36:43,005
Speaker 2:  out what to do. And then often it's going to another AI system, which is

649
00:36:43,315 --> 00:36:46,725
Speaker 2:  chat GPT in a lot of cases it feels like, which is unreliable in its specific

650
00:36:46,725 --> 00:36:49,805
Speaker 2:  ways or perplexity, which is in unreliable. It's specific ways.

651
00:36:50,925 --> 00:36:54,325
Speaker 2:  I asked per perplexity to compare how long

652
00:36:54,805 --> 00:36:58,005
Speaker 2:  a car I'm thinking about buying is with my current car. And it just couldn't

653
00:36:58,005 --> 00:37:01,365
Speaker 2:  figure it out. It was just like the, the length of a Jeep Grand Cherokee

654
00:37:01,385 --> 00:37:04,365
Speaker 2:  is not available. And I was like, that's, I'm pretty sure it's available.

655
00:37:04,515 --> 00:37:07,045
Speaker 2:  Yeah, that's a lot. And I asked it again and said, please provide me the

656
00:37:07,045 --> 00:37:08,365
Speaker 2:  length of the Jeep Grand Cherokee.

657
00:37:08,835 --> 00:37:10,485
Speaker 5:  Like, no, that's literally your job.

658
00:37:10,835 --> 00:37:14,405
Speaker 2:  It's like, just go get, you're a search engine, just do it. Just

659
00:37:14,705 --> 00:37:18,445
Speaker 2:  figure it out. But that's like you're stacking it up, right? You're

660
00:37:18,445 --> 00:37:22,005
Speaker 2:  stacking up one unreliable LLM with another one with potentially another

661
00:37:22,005 --> 00:37:25,885
Speaker 2:  one. And you, it just feels like you get to a place where it's,

662
00:37:26,195 --> 00:37:30,005
Speaker 2:  it's fun. But all of that is almost guaranteed to have a mistake

663
00:37:30,165 --> 00:37:30,805
Speaker 2:  embedded in somewhere.

664
00:37:30,805 --> 00:37:31,365
Speaker 5:  Yeah. Or so

665
00:37:31,485 --> 00:37:32,925
Speaker 2:  A hallucination embedded in somewhere.

666
00:37:32,925 --> 00:37:36,885
Speaker 5:  Yeah. I I, I had a call with Bethany Bonno, the co-founder of

667
00:37:36,885 --> 00:37:40,325
Speaker 5:  Humane the day before the review went live, just to basically be like, Hey,

668
00:37:40,325 --> 00:37:43,845
Speaker 5:  this thing is not kind. Just like no surprises, wanna let you know what's

669
00:37:43,845 --> 00:37:46,565
Speaker 5:  coming. This is the thing we do with a lot of stories. Just like you, you

670
00:37:46,565 --> 00:37:48,725
Speaker 5:  should not be surprised by what's coming. Yeah. This is,

671
00:37:48,725 --> 00:37:50,245
Speaker 2:  This is like, we had questions core journalism

672
00:37:50,345 --> 00:37:52,925
Speaker 5:  Too. Yeah, yeah. And I, I, I had questions and I was like, If, you wanna

673
00:37:52,925 --> 00:37:54,925
Speaker 5:  respond to some of these things, let me know. And that was when, you know,

674
00:37:54,925 --> 00:37:57,685
Speaker 5:  she started talking about software updates. But I started by saying, here's

675
00:37:57,685 --> 00:38:01,445
Speaker 5:  what I wanna understand is like a lot of these things just straight up

676
00:38:01,445 --> 00:38:04,485
Speaker 5:  don't work and I can't figure out whose fault it is.

677
00:38:05,155 --> 00:38:09,045
Speaker 5:  Like is it yours as, as hardware builders? Is it yours

678
00:38:09,045 --> 00:38:12,045
Speaker 5:  as software builders? Is it the AI models

679
00:38:12,375 --> 00:38:16,285
Speaker 5:  underlying a lot of this? Is it the end points that don't know how to interact

680
00:38:16,285 --> 00:38:19,605
Speaker 5:  with those LLMs? Like who is it? And she literally just laughed and, and

681
00:38:19,605 --> 00:38:22,965
Speaker 5:  the answer is kind of everybody. But the problem is, you're exactly right.

682
00:38:23,295 --> 00:38:27,005
Speaker 5:  Until all of that stuff is good, not just one part of it, but

683
00:38:27,005 --> 00:38:30,965
Speaker 5:  until all of it is extremely reliable and fast and

684
00:38:30,965 --> 00:38:32,165
Speaker 5:  good, none of it works.

685
00:38:32,555 --> 00:38:36,085
Speaker 2:  Okay. So now we come to the heart of the matter here on The Vergecast,

686
00:38:36,775 --> 00:38:40,645
Speaker 2:  which is, I read the review in drafts. I did not

687
00:38:40,645 --> 00:38:44,525
Speaker 2:  receive a no surprises phone call. I just read the review in Google

688
00:38:44,715 --> 00:38:48,685
Speaker 2:  Docs. And at the bottom, David gave it a four out of 10. And then he

689
00:38:48,685 --> 00:38:52,045
Speaker 2:  left himself a note that said maybe this should be a three. I was told fine

690
00:38:52,045 --> 00:38:56,005
Speaker 2:  with a four. And I add the comment and I said it should be a three. And you

691
00:38:56,005 --> 00:38:56,365
Speaker 2:  did, you

692
00:38:56,365 --> 00:38:59,325
Speaker 5:  Said you should give it a three LOL. Yeah. Was the entirety of your comment?

693
00:39:00,085 --> 00:39:01,205
Speaker 2:  I think I made myself clear.

694
00:39:03,095 --> 00:39:06,285
Speaker 2:  We've just described a thing that overheats that does not work for more than

695
00:39:06,285 --> 00:39:10,165
Speaker 2:  minutes at a time. Who software is often confused by the existence of

696
00:39:10,165 --> 00:39:13,845
Speaker 2:  Beyonce unforgivable sin and it shows reliant on a number

697
00:39:13,945 --> 00:39:17,845
Speaker 2:  of systems that lie to you consistently and may murder your

698
00:39:17,845 --> 00:39:21,365
Speaker 2:  dog. That $700, I think is what I would say with a

699
00:39:21,365 --> 00:39:22,005
Speaker 2:  $24 month.

700
00:39:22,005 --> 00:39:23,805
Speaker 5:  Thank you. I was gonna say, you have to yell the price at me several times.

701
00:39:23,905 --> 00:39:27,765
Speaker 2:  I'm gonna be, it's coming so many times is, is that not a three? What,

702
00:39:27,835 --> 00:39:29,525
Speaker 2:  where'd the extra point come from Pierce

703
00:39:29,905 --> 00:39:32,685
Speaker 5:  It honestly, there there's a decent chance it should have been a three. I

704
00:39:32,845 --> 00:39:35,005
Speaker 5:  I was, I it's like, I'll just, I'll just be honest about

705
00:39:35,005 --> 00:39:37,645
Speaker 2:  That. Every review podcast is just us being like, I should have taken one.

706
00:39:37,925 --> 00:39:38,365
Speaker 5:  No, it's

707
00:39:38,365 --> 00:39:38,445
Speaker 2:  Like,

708
00:39:38,755 --> 00:39:42,245
Speaker 5:  I'll I'll tell you off my logic and you can tell me whether it's fair or

709
00:39:42,245 --> 00:39:45,765
Speaker 5:  not. A three in our score, I believe the exact

710
00:39:45,825 --> 00:39:49,805
Speaker 5:  phrase is bad. That is, that is the first word next to

711
00:39:49,805 --> 00:39:53,725
Speaker 5:  three in our review rubric. Yeah. And I think four

712
00:39:53,905 --> 00:39:57,885
Speaker 5:  is like multiple outstanding issues, I think is what it said. And so

713
00:39:57,885 --> 00:40:00,965
Speaker 5:  it's, it's somewhere between those two things. Right? And for me,

714
00:40:01,885 --> 00:40:05,805
Speaker 5:  I, I tilted the scale based on there were enough

715
00:40:05,805 --> 00:40:09,525
Speaker 5:  things that it did that were cool and valuable

716
00:40:09,585 --> 00:40:13,485
Speaker 5:  and eye-opening that I wouldn't just call it bad, like I, I call

717
00:40:13,485 --> 00:40:17,165
Speaker 5:  it broken on purpose, right? Because it's like, it is not,

718
00:40:17,515 --> 00:40:21,485
Speaker 5:  this is not a stupid product, it's just not a

719
00:40:21,485 --> 00:40:24,045
Speaker 5:  good product. And I think

720
00:40:25,215 --> 00:40:28,245
Speaker 5:  again, it's very possible it's a three. Like I would not tell anyone to buy

721
00:40:28,245 --> 00:40:28,525
Speaker 5:  this product

722
00:40:28,815 --> 00:40:31,965
Speaker 2:  $700 20 10 24

723
00:40:32,805 --> 00:40:36,605
Speaker 2:  a month. It's just funny coming for the guy. Seven. It's very good. What's

724
00:40:36,605 --> 00:40:39,045
Speaker 2:  very good. Would If you give a Vision Pro estimate? No, I, I, I saw the,

725
00:40:39,385 --> 00:40:41,925
Speaker 2:  the score published this morning and I thought to myself, I know exactly

726
00:40:41,925 --> 00:40:43,285
Speaker 2:  how the bridge has goes. Would you

727
00:40:43,285 --> 00:40:47,245
Speaker 5:  Rather have eight eight Humane AI pins

728
00:40:47,305 --> 00:40:48,285
Speaker 5:  or one Vision Pro?

729
00:40:50,865 --> 00:40:54,605
Speaker 2:  I'm taking the cash man, just gimme the cache neither.

730
00:40:54,665 --> 00:40:57,925
Speaker 5:  To be fair, it's also only five Humane pins. But anyway,

731
00:40:58,845 --> 00:41:02,485
Speaker 5:  I, yeah, I I'm, I'm still torn between those two things. And the, the

732
00:41:02,485 --> 00:41:05,565
Speaker 5:  tiebreaker for me was like, was honestly, I think the same as the tiebreaker

733
00:41:05,585 --> 00:41:09,565
Speaker 5:  for you was like, this occasionally does things that are awesome and I

734
00:41:09,565 --> 00:41:12,005
Speaker 5:  don't know how to, I don't know how to factor that in. It doesn't do it often

735
00:41:12,005 --> 00:41:15,925
Speaker 5:  enough. It doesn't do enough of them, but it occasionally does things

736
00:41:15,925 --> 00:41:19,725
Speaker 5:  that you're like, oh, I see it now. And I had, I had just enough of

737
00:41:19,725 --> 00:41:21,845
Speaker 5:  those moments that I was like, okay, this thing is not

738
00:41:22,285 --> 00:41:26,245
Speaker 2:  Reviewer. Like you've spent a long time with a thing. Yeah. And

739
00:41:26,245 --> 00:41:28,565
Speaker 2:  then you write about what it's like and then someone else reads it and they're

740
00:41:28,565 --> 00:41:32,445
Speaker 2:  like, Nope. And, but I look bit weird. See, I, I, I

741
00:41:32,445 --> 00:41:32,765
Speaker 2:  read this

742
00:41:32,765 --> 00:41:36,325
Speaker 6:  Review and I was like, you know, the four makes sense because I could see

743
00:41:36,345 --> 00:41:40,325
Speaker 6:  his affection for like what they tried, what they attempted.

744
00:41:40,405 --> 00:41:43,325
Speaker 6:  I could see like, he was like, you know what, there's some cool shit happening

745
00:41:43,325 --> 00:41:43,485
Speaker 6:  here.

746
00:41:43,895 --> 00:41:45,005
Speaker 2:  $700

747
00:41:45,555 --> 00:41:45,845
Speaker 6:  Once

748
00:41:46,515 --> 00:41:47,765
Speaker 2:  Dollars a month, 20

749
00:41:47,895 --> 00:41:49,045
Speaker 6:  Times. You try it. I

750
00:41:49,045 --> 00:41:52,765
Speaker 5:  Think the fairest criticism you can give me of this score is I think if a

751
00:41:52,765 --> 00:41:55,165
Speaker 5:  bigger company had made it, I probably would've given it a three.

752
00:41:55,515 --> 00:41:56,965
Speaker 6:  Yeah. But it's a small company.

753
00:41:57,165 --> 00:42:00,845
Speaker 5:  I think I, I think I might have given them one point just for a startup

754
00:42:00,895 --> 00:42:02,605
Speaker 5:  tried to do a hard thing. Yeah.

755
00:42:02,925 --> 00:42:03,645
Speaker 2:  I, I buy that

756
00:42:04,525 --> 00:42:07,405
Speaker 5:  Ultimately, which I don't think I realized until it had published, but I

757
00:42:07,405 --> 00:42:08,885
Speaker 5:  think that might be what I did.

758
00:42:09,305 --> 00:42:12,365
Speaker 6:  But I think that's okay. Right? I mean

759
00:42:12,365 --> 00:42:15,445
Speaker 5:  It is and it isn't, right? Like it doesn't make the product any better, but

760
00:42:15,445 --> 00:42:16,325
Speaker 5:  it at least, like,

761
00:42:18,045 --> 00:42:21,925
Speaker 5:  I think I, I think I might have given it, like I think if Google had

762
00:42:21,925 --> 00:42:24,805
Speaker 5:  made this thing and it was this exact thing, I think I probably would've

763
00:42:24,805 --> 00:42:26,005
Speaker 5:  been slightly harder on it. Yeah.

764
00:42:26,005 --> 00:42:29,325
Speaker 6:  Because Google also wouldn't have the same excuses.

765
00:42:30,135 --> 00:42:33,925
Speaker 6:  Right? Like, like I think Humane has some real fundamental

766
00:42:34,895 --> 00:42:38,885
Speaker 6:  acceptable excuses. I mean, not when it comes to money, it has a ton of

767
00:42:38,885 --> 00:42:39,005
Speaker 6:  money.

768
00:42:39,445 --> 00:42:39,765
Speaker 5:  It does.

769
00:42:40,675 --> 00:42:44,045
Speaker 6:  It's, it is not lacking for money, but it is lacking for

770
00:42:44,435 --> 00:42:48,245
Speaker 6:  that experience putting this kind of product out into the world on a

771
00:42:48,245 --> 00:42:51,685
Speaker 6:  consistent basis and taking a big swing. And like Google

772
00:42:51,755 --> 00:42:53,245
Speaker 6:  doesn't do that. Well,

773
00:42:53,245 --> 00:42:56,925
Speaker 2:  If Google had put this out one, they would've leaked it in full like a long

774
00:42:56,925 --> 00:42:57,685
Speaker 2:  time ago. Right.

775
00:42:57,845 --> 00:42:58,685
Speaker 6:  It would've been plastic.

776
00:42:58,955 --> 00:43:02,245
Speaker 2:  Yeah. It still wouldn't have properly worked with Genome. Correct.

777
00:43:02,705 --> 00:43:06,405
Speaker 2:  That's a classic. And then we would've taken a point off,

778
00:43:07,195 --> 00:43:10,845
Speaker 2:  like a pre point deduction because they're, they would've killed it in a

779
00:43:10,845 --> 00:43:10,925
Speaker 2:  year.

780
00:43:11,235 --> 00:43:12,205
Speaker 6:  Yeah, yeah. We would've known

781
00:43:12,205 --> 00:43:14,565
Speaker 2:  It's gonna, no further updates would be available for this product.

782
00:43:14,885 --> 00:43:17,885
Speaker 6:  Yeah. In fact, that's what happened with their last attempt at like a wearable

783
00:43:17,885 --> 00:43:18,765
Speaker 6:  came off, right?

784
00:43:18,835 --> 00:43:21,765
Speaker 2:  It's like we're deducting a point. The Google pre-death point,

785
00:43:22,805 --> 00:43:25,965
Speaker 2:  it's just coming off. I look, I buy the, you know, it's a startup. They did

786
00:43:25,965 --> 00:43:29,925
Speaker 2:  a hard thing, but the way they talk about it actually, I'll, I'll,

787
00:43:30,085 --> 00:43:32,405
Speaker 2:  I will connect it to the Vision Pro. The way Apple talked about the Vision

788
00:43:32,405 --> 00:43:36,325
Speaker 2:  Pro definitely affected how I reviewed the product in the end. Is that

789
00:43:36,325 --> 00:43:37,885
Speaker 2:  fair? Is, it's as fair as

790
00:43:39,425 --> 00:43:42,525
Speaker 2:  Humane being a startup, right? It it affects your perception of the product.

791
00:43:44,305 --> 00:43:48,285
Speaker 2:  And it's like they can't, it couldn't do it couldn't, it couldn't withstand

792
00:43:48,305 --> 00:43:51,885
Speaker 2:  the weight that was being placed on the marketing, which is also

793
00:43:52,425 --> 00:43:55,365
Speaker 2:  1000% true of Humane. Oh,

794
00:43:55,565 --> 00:43:56,285
Speaker 5:  Absolutely. Humane thing,

795
00:43:56,735 --> 00:44:00,205
Speaker 2:  Right? And like today, you know, the, the joke at the top of the show,

796
00:44:00,625 --> 00:44:03,845
Speaker 2:  Humane put out their statement in response to all the reviews. I don't think

797
00:44:03,845 --> 00:44:06,965
Speaker 2:  they were surprised by these reviews. No. And they said big thanks for all

798
00:44:06,965 --> 00:44:10,325
Speaker 2:  the reviewer feedback on the AI pin. It's been a wild ride from launch till

799
00:44:10,325 --> 00:44:13,245
Speaker 2:  now. Hearing from all of you is super valuable. The team appreciates The

800
00:44:13,245 --> 00:44:15,645
Speaker 2:  good vibes and the hardware and the potential it's unlocking, but we totally

801
00:44:15,645 --> 00:44:19,565
Speaker 2:  get it. There's rooms makes things even better. We'll, all we we're all ears

802
00:44:19,565 --> 00:44:23,085
Speaker 2:  and how we can up our game, especially with Cosmos os your insights are gold

803
00:44:23,285 --> 00:44:26,525
Speaker 2:  guidance. We can improve making AI Pin better for daily use. That is

804
00:44:26,685 --> 00:44:30,045
Speaker 2:  180 degrees different from how they talked about this thing

805
00:44:30,655 --> 00:44:34,645
Speaker 2:  until yesterday. Yeah. Right. And I, there's something

806
00:44:34,655 --> 00:44:36,405
Speaker 2:  there that I think is just really interesting.

807
00:44:36,685 --> 00:44:40,605
Speaker 5:  I would give them 8% more credit than that. I think If, you look at

808
00:44:40,605 --> 00:44:44,325
Speaker 5:  Humane in the last several weeks, they've been putting out this stream of

809
00:44:44,325 --> 00:44:48,005
Speaker 5:  like videos and stuff showing people how to use it and they've been making

810
00:44:48,005 --> 00:44:51,045
Speaker 5:  more jokes at their own expense. I mean, If, you go back to that original

811
00:44:51,045 --> 00:44:54,925
Speaker 5:  launch video, it is the most ludicrously selfer.

812
00:44:55,555 --> 00:44:58,365
Speaker 5:  Like, it's the video where Imran and Bethany are standing there like looking

813
00:44:58,505 --> 00:45:00,725
Speaker 5:  sad as they use their pin. Do you know what I'm talking about? Yeah, yeah.

814
00:45:00,725 --> 00:45:04,165
Speaker 5:  It's beautiful. The video is absurd and it's, it's extra absurd now having

815
00:45:04,235 --> 00:45:07,805
Speaker 5:  used this thing because they, they treat it as if they had invented fire

816
00:45:08,505 --> 00:45:12,325
Speaker 5:  and like it's, it's preposterous. But I think recently

817
00:45:13,665 --> 00:45:17,645
Speaker 5:  for whatever reason they have, they have pulled back a bit and been

818
00:45:17,765 --> 00:45:21,485
Speaker 5:  a little more honest and open about what this thing is and how it works in

819
00:45:21,485 --> 00:45:25,005
Speaker 5:  some of the limitations and all that stuff. And I think that's good. And

820
00:45:25,005 --> 00:45:28,685
Speaker 5:  I, I do, I do wonder, and I have wondered many times if they had

821
00:45:29,565 --> 00:45:32,685
Speaker 5:  launched this thing more the way Rabbit has, which is like, oh, look at this

822
00:45:32,705 --> 00:45:36,565
Speaker 5:  fun silly little toy we made. Isn't it cool looking if I would feel differently?

823
00:45:37,285 --> 00:45:40,525
Speaker 5:  I don't know. I'm getting a rabbit in two weeks and I'm very curious to see,

824
00:45:40,705 --> 00:45:41,765
Speaker 6:  I'm so excited how it

825
00:45:41,765 --> 00:45:44,245
Speaker 5:  Feels as a cheaper, much less

826
00:45:45,235 --> 00:45:47,805
Speaker 5:  ambitious in a lot of ways. Gadget.

827
00:45:48,195 --> 00:45:48,485
Speaker 2:  Yeah.

828
00:45:48,635 --> 00:45:51,805
Speaker 6:  Yeah. I think, I think the, the score for the Humane pin is

829
00:45:52,775 --> 00:45:56,165
Speaker 6:  reflecting that ambition in kind of a positive way. And we're reflecting,

830
00:45:56,165 --> 00:45:59,165
Speaker 6:  we're saying, you know what, it's okay to be ambitious. That's why the vision,

831
00:45:59,235 --> 00:46:02,925
Speaker 6:  like part of the Vision Pro having a seven is it's ambitious. It, it, it

832
00:46:03,095 --> 00:46:05,205
Speaker 6:  swung for the fences. It missed

833
00:46:07,035 --> 00:46:07,645
Speaker 6:  real bad.

834
00:46:08,185 --> 00:46:09,725
Speaker 2:  Can I just say one thing about Vision Pro? And then we, we,

835
00:46:09,925 --> 00:46:12,685
Speaker 6:  I just like to, I like to poke this at the seven. I can't wait till I read

836
00:46:12,685 --> 00:46:15,165
Speaker 2:  Something. I'm aware that everyone thinks the seven is pure cowardice. Yeah.

837
00:46:15,525 --> 00:46:17,405
Speaker 2:  I would just, I would just tell you

838
00:46:17,425 --> 00:46:21,245
Speaker 6:  My Kobo review, we're gonna have like a two hour VERGE cast on the

839
00:46:21,245 --> 00:46:22,605
Speaker 6:  score for, I thought about

840
00:46:22,605 --> 00:46:25,245
Speaker 5:  It's gonna be thought about filing a draft of this review with a seven out

841
00:46:25,245 --> 00:46:26,845
Speaker 5:  of 10 as the score just to see what would've

842
00:46:26,965 --> 00:46:27,045
Speaker 6:  Happened.

843
00:46:30,225 --> 00:46:32,605
Speaker 2:  By the way, one thing about the Vision Pro then we, we then we gotta move

844
00:46:32,605 --> 00:46:32,765
Speaker 2:  on.

845
00:46:34,385 --> 00:46:38,325
Speaker 2:  Hot new app for the Vision Pros and Netflix and Amazon Prime app. It's

846
00:46:38,325 --> 00:46:42,165
Speaker 2:  a browser. It's just a browser with, with the skin on it. Web apps are saving

847
00:46:42,185 --> 00:46:43,525
Speaker 2:  the Vision Pro. Yep.

848
00:46:43,905 --> 00:46:44,325
Speaker 5:  Saving

849
00:46:44,465 --> 00:46:45,125
Speaker 6:  Is generous.

850
00:46:45,305 --> 00:46:48,925
Speaker 2:  I'm saying your YouTube app, your Netflix app, the one that actually supports

851
00:46:48,925 --> 00:46:49,685
Speaker 2:  facial audio.

852
00:46:51,865 --> 00:46:53,485
Speaker 2:  I'm just saying Turnabout's fair play. Yep.

853
00:46:53,485 --> 00:46:57,325
Speaker 5:  Apple apple's like Official accounts. We're recommending that app. Yeah,

854
00:46:57,325 --> 00:46:57,565
Speaker 2:  Like

855
00:46:58,315 --> 00:47:01,245
Speaker 5:  It's it's gonna be web apps, it's gonna be anything. It's gonna be web apps.

856
00:47:01,395 --> 00:47:04,925
Speaker 2:  It's very good. It's so funny. It's very good. All right, David, I know that

857
00:47:04,925 --> 00:47:08,125
Speaker 2:  you're excited about the prospect of AI gadgets, but I'm sorry man. Maybe

858
00:47:08,125 --> 00:47:09,565
Speaker 2:  the rabbit's gonna do it for you.

859
00:47:10,125 --> 00:47:10,445
Speaker 5:  Probably not.

860
00:47:11,025 --> 00:47:12,325
Speaker 2:  We have to take a break. We'll be right back.

861
00:49:59,825 --> 00:50:00,045
Speaker 8:  dog.

862
00:50:04,515 --> 00:50:08,375
Speaker 2:  All right, we're back. There's quite a lot in this section. We thought

863
00:50:08,375 --> 00:50:11,495
Speaker 2:  it was gonna be a, like a lightning round, but we should just start with

864
00:50:11,495 --> 00:50:12,895
Speaker 2:  Taylor Swift, I think. Yeah.

865
00:50:13,485 --> 00:50:16,775
Speaker 6:  Well that's how we start Every VERGE cast now it feels like.

866
00:50:17,615 --> 00:50:18,095
Speaker 6:  I love Taylor.

867
00:50:18,315 --> 00:50:20,855
Speaker 2:  Who doesn't? I was on the Ezra Klein show and I mentioned Taylor Swift, and

868
00:50:20,855 --> 00:50:23,175
Speaker 2:  he goes, well, she's singular. She can't, you can't win the argument with

869
00:50:23,175 --> 00:50:27,015
Speaker 2:  that. He's very good. He was, he was not wrong. No, but I, I

870
00:50:27,125 --> 00:50:29,205
Speaker 2:  thought I'd like play the Ace card, you know? Yeah. And he was like, Nope.

871
00:50:29,395 --> 00:50:33,365
Speaker 2:  Nope. That's, that's not everyone can be Taylor Swift Hearts broke

872
00:50:33,365 --> 00:50:37,285
Speaker 2:  across America that day. Alright. Taylor Swift famously

873
00:50:37,285 --> 00:50:41,005
Speaker 2:  owns her own masters, but has distribution through Universal Music Group.

874
00:50:41,035 --> 00:50:44,965
Speaker 2:  Also her publishing the songwriting money flows through Universal, universal,

875
00:50:45,065 --> 00:50:48,605
Speaker 2:  you may know. So a bit of a spat with the tiktoks.

876
00:50:48,605 --> 00:50:50,125
Speaker 6:  They, they're not happy with each other. They

877
00:50:50,125 --> 00:50:53,645
Speaker 2:  Don't like each other. Universal is pulled all of its music off TikTok leading

878
00:50:53,665 --> 00:50:56,325
Speaker 2:  to oceans of TikTok grids that are just silent.

879
00:50:57,475 --> 00:51:01,165
Speaker 2:  Very sad people. It's been a long time. Right. This happened

880
00:51:01,265 --> 00:51:05,085
Speaker 2:  and I, I think on this show we talked about it and the, the sort of consensus

881
00:51:05,085 --> 00:51:07,605
Speaker 2:  prediction was either to get fixed right away or never get fixed. Yeah.

882
00:51:08,235 --> 00:51:10,685
Speaker 2:  Unfortunately, Taylor has a new album coming out.

883
00:51:11,035 --> 00:51:11,885
Speaker 6:  It's gonna get fixed.

884
00:51:12,235 --> 00:51:15,765
Speaker 2:  Well, I don't know if it's fixed, but Taylor's music is back on TikTok.

885
00:51:16,105 --> 00:51:19,285
Speaker 2:  Mm. And the prevailing theory is because she owns her own masters,

886
00:51:19,985 --> 00:51:21,525
Speaker 2:  she can just cut her own deal

887
00:51:21,835 --> 00:51:24,085
Speaker 6:  That that tracks, that tracks

888
00:51:24,265 --> 00:51:25,925
Speaker 2:  It tracks. But she's got the album command.

889
00:51:26,695 --> 00:51:27,725
Speaker 6:  She's gotta promote it.

890
00:51:27,775 --> 00:51:31,245
Speaker 2:  She's gotta promote it. Lots of artists have been like kind of doing this

891
00:51:31,245 --> 00:51:34,045
Speaker 2:  on the side. Like Olivia Rodrigo also universal artist. Her music isn't there,

892
00:51:34,045 --> 00:51:37,925
Speaker 2:  but she made a TikTok promoting her shows using a fan edit of her songs.

893
00:51:38,275 --> 00:51:42,205
Speaker 5:  It's been so fun, by the way, to watch artists try to figure this out. Like

894
00:51:42,325 --> 00:51:45,925
Speaker 5:  I think the situation is terrible for artists generally speaking, but seeing

895
00:51:46,685 --> 00:51:50,285
Speaker 5:  a bunch of, like, there, I saw a bunch of singer songwriters after this was

896
00:51:50,285 --> 00:51:54,085
Speaker 5:  happening, like start to play live versions of

897
00:51:54,085 --> 00:51:56,885
Speaker 5:  their songs on TikTok so that other people could use it as sounds

898
00:51:58,225 --> 00:52:01,925
Speaker 5:  and everybody's playing like sped up versions of their songs and like the,

899
00:52:02,025 --> 00:52:05,685
Speaker 5:  the remixes are blowing up everywhere. It's the, all the ways people are

900
00:52:05,685 --> 00:52:09,085
Speaker 5:  finding around this band is just totally fascinating.

901
00:52:09,285 --> 00:52:12,845
Speaker 6:  I just love that you guys have much nicer tiktoks than mine. I was like,

902
00:52:12,885 --> 00:52:14,045
Speaker 6:  I wish I saw all of that. Yeah.

903
00:52:14,045 --> 00:52:15,445
Speaker 2:  It's just like deep fried memes. It's

904
00:52:15,445 --> 00:52:19,405
Speaker 6:  Just Jojo siwa climbing out of the sea over and over again.

905
00:52:19,405 --> 00:52:19,445
Speaker 6:  I

906
00:52:19,445 --> 00:52:20,685
Speaker 2:  Don't refuse to know who this is.

907
00:52:20,845 --> 00:52:21,885
Speaker 6:  I won't keep it that way.

908
00:52:22,165 --> 00:52:22,845
Speaker 2:  I will not know.

909
00:52:23,055 --> 00:52:24,645
Speaker 6:  Watch it. Oh my God. It's upset.

910
00:52:24,665 --> 00:52:27,885
Speaker 2:  And the internet knows that. I don't wanna know. Even I'm a person saying

911
00:52:27,945 --> 00:52:31,165
Speaker 2:  out loud like, yeah, I won't say it. you know what I mean? Yeah.

912
00:52:31,165 --> 00:52:32,525
Speaker 6:  Tiktoks like pass and

913
00:52:32,525 --> 00:52:34,725
Speaker 2:  Yeah. It's just like, don't show it to 'em. Like the tar, the ad targeting

914
00:52:34,725 --> 00:52:35,725
Speaker 2:  is like, this ain't gonna work. Yeah.

915
00:52:36,825 --> 00:52:38,325
Speaker 6:  Not the right audience. Keep going for

916
00:52:38,325 --> 00:52:40,165
Speaker 2:  You. Keep going. If you, I will not know. No,

917
00:52:40,165 --> 00:52:43,365
Speaker 6:  I'm happy for you. I like, I wanna be over there. How do I get, how do I

918
00:52:43,365 --> 00:52:46,445
Speaker 6:  get to that TikTok? I wanna watch people like do cool covers of their songs.

919
00:52:46,555 --> 00:52:50,245
Speaker 2:  Yeah. It's truck jumps and, and acoustic covers. Yeah. Okay. So I'm very

920
00:52:50,245 --> 00:52:54,205
Speaker 2:  curious about the status of universal and TikTok. Yeah. Every social

921
00:52:54,685 --> 00:52:57,925
Speaker 2:  platform has an existential dependency on the music industry.

922
00:52:58,625 --> 00:53:01,605
Speaker 2:  If, you lose those rights. All kinds of bad things stop happening to you.

923
00:53:02,025 --> 00:53:05,405
Speaker 2:  So Universal is the biggest label around it has CEO,

924
00:53:05,945 --> 00:53:09,005
Speaker 2:  sir Lucian Grange is sir very powerful,

925
00:53:09,745 --> 00:53:12,525
Speaker 2:  not shy kind of bra. I have a little mustache. I don't know if he has a little

926
00:53:12,685 --> 00:53:12,845
Speaker 2:  mustache.

927
00:53:13,045 --> 00:53:14,605
Speaker 6:  I just, I think that when I hear Sir,

928
00:53:14,955 --> 00:53:17,565
Speaker 2:  He's like a guy behind the guy type. Okay. But also very famous and very

929
00:53:17,765 --> 00:53:21,045
Speaker 2:  powerful. He basically told YouTube to cut it out with fake Drake.

930
00:53:21,835 --> 00:53:25,605
Speaker 2:  Like that stuff went up on YouTube and he was like, cut it out. And, and

931
00:53:25,605 --> 00:53:28,445
Speaker 2:  YouTube caved. Right. They put out If, you recall they put out their like

932
00:53:28,945 --> 00:53:32,285
Speaker 2:  AI principles. They have this like new licensing system that they're going

933
00:53:32,285 --> 00:53:36,205
Speaker 2:  to deploy that is like outside of regular copyright law. It's like special

934
00:53:36,795 --> 00:53:40,365
Speaker 2:  YouTube, ai, copyright law, all this stuff they're gonna do, they're doing

935
00:53:40,365 --> 00:53:44,085
Speaker 2:  it at the besst of Universal music. 'cause YouTube isn't done. And Neil

936
00:53:44,135 --> 00:53:47,645
Speaker 2:  Mohan who runs YouTube is like, this is a licensing business. And like our

937
00:53:47,645 --> 00:53:51,285
Speaker 2:  partners need to be happy and great TikTok just done.

938
00:53:51,355 --> 00:53:54,845
Speaker 2:  Like, they just, whatever the other labels are actually kinda happy about

939
00:53:54,845 --> 00:53:58,325
Speaker 2:  this. Yeah. If, you look at the charts right now, top two artists on the

940
00:53:58,325 --> 00:54:02,285
Speaker 2:  charts are Warner Music artists. Mm. This is like a real thing that

941
00:54:02,285 --> 00:54:02,685
Speaker 2:  is going on.

942
00:54:02,795 --> 00:54:03,885
Speaker 6:  It's one of them. Beyonce.

943
00:54:04,595 --> 00:54:08,565
Speaker 5:  Well, and there's a sense among some people fighting this

944
00:54:08,565 --> 00:54:12,125
Speaker 5:  fight that universal overplayed its hand in that sense, right? That like

945
00:54:12,125 --> 00:54:15,965
Speaker 5:  Right. TikTok was the only company with enough clout of its own

946
00:54:16,625 --> 00:54:20,485
Speaker 5:  to fight back against Lucid Grange and all of these sort of

947
00:54:20,865 --> 00:54:24,005
Speaker 5:  big swinging music labels. Because TikTok

948
00:54:24,465 --> 00:54:28,325
Speaker 5:  needed UMG less than UMG needed TikTok. And that seems to be this

949
00:54:28,625 --> 00:54:31,285
Speaker 5:  battle that we are still very much in the middle of that. Again, Taylor,

950
00:54:31,285 --> 00:54:34,245
Speaker 5:  Swift being singular is able to just cleave her way out of,

951
00:54:34,635 --> 00:54:37,765
Speaker 2:  Well, so TikTok thinks it has this power to break artists and create

952
00:54:38,605 --> 00:54:41,645
Speaker 2:  Cher. And really it's just kind of kinda like moving money around because

953
00:54:41,825 --> 00:54:45,765
Speaker 2:  the labels make no money from TikTok plays as it is. They make very

954
00:54:45,765 --> 00:54:49,245
Speaker 2:  little money from those streams. All their money is on Spotify, apple music

955
00:54:49,245 --> 00:54:53,085
Speaker 2:  or whatever. But they create share, like people listen

956
00:54:53,085 --> 00:54:56,685
Speaker 2:  to the songs in TikTok, then they go to their streaming services. So it can

957
00:54:56,685 --> 00:54:59,805
Speaker 2:  shift the money and especially can break new artists. It

958
00:54:59,825 --> 00:55:03,565
Speaker 6:  Can break new artists and like, isn't still the primary way that these artists

959
00:55:03,565 --> 00:55:07,485
Speaker 6:  make money is performing. It's not, it's not anything digital. It's

960
00:55:07,505 --> 00:55:09,885
Speaker 6:  all going and doing a concert and taking a

961
00:55:09,885 --> 00:55:13,565
Speaker 2:  Lot of money. Yeah. It's like 70% of artist revenue is that side.

962
00:55:13,715 --> 00:55:14,005
Speaker 2:  Yeah.

963
00:55:14,185 --> 00:55:18,045
Speaker 6:  And, and this is really good. Great for that. Right? Like a

964
00:55:18,045 --> 00:55:19,485
Speaker 6:  perfect way to promote yourself.

965
00:55:19,675 --> 00:55:21,645
Speaker 2:  Yeah. You're on tour, you're doing a thing. I would argue

966
00:55:21,995 --> 00:55:25,805
Speaker 6:  Part of Taylor's recent success is because Yeah.

967
00:55:26,145 --> 00:55:27,205
Speaker 6:  Of this. Oh

968
00:55:27,205 --> 00:55:27,405
Speaker 5:  Yeah.

969
00:55:28,045 --> 00:55:31,645
Speaker 2:  I mean, Taylor's singular as we, she is Singular as we, as I have been told

970
00:55:32,985 --> 00:55:36,245
Speaker 2:  anyway, so I've just been poking at this and poking this and trying to figure

971
00:55:36,245 --> 00:55:39,565
Speaker 2:  out what's going on. So the other labels are happy. And what you would expect

972
00:55:40,265 --> 00:55:44,125
Speaker 2:  is Universal walks. The other labels are like, we also want a better deal.

973
00:55:44,175 --> 00:55:47,205
Speaker 2:  We're gonna walk and then all the labels together collectively pressure TikTok

974
00:55:47,205 --> 00:55:50,765
Speaker 2:  and cutting them better rates. But because of this dynamic, which is where

975
00:55:50,865 --> 00:55:54,525
Speaker 2:  the big bad label Universal with all of the artists

976
00:55:54,525 --> 00:55:58,485
Speaker 2:  walked, including Taylor, the other labels got a benefit.

977
00:55:58,755 --> 00:56:02,645
Speaker 2:  Yeah. So you see this dynamic like playing out in the industry now Taylor's

978
00:56:02,645 --> 00:56:06,365
Speaker 2:  back. We'll see how that goes. But here's what I've heard, and it's sketchy

979
00:56:06,365 --> 00:56:09,685
Speaker 2:  and un If, you can confirm this, let me know. Calling

980
00:56:09,785 --> 00:56:10,005
Speaker 6:  The

981
00:56:10,005 --> 00:56:13,445
Speaker 2:  Hotline was basically Universal. Wanted a bunch of, well, wanted a bunch

982
00:56:13,445 --> 00:56:15,325
Speaker 2:  of provisions that looked like they're YouTube provisions,

983
00:56:16,995 --> 00:56:19,885
Speaker 2:  like don't do bad AI stuff with their content. And they wanted an increase

984
00:56:19,885 --> 00:56:22,245
Speaker 2:  in royalties, obviously in TikTok said no.

985
00:56:23,275 --> 00:56:23,565
Speaker 6:  Yeah,

986
00:56:23,565 --> 00:56:24,405
Speaker 2:  That's what I got for you.

987
00:56:24,755 --> 00:56:28,165
Speaker 6:  That that sounds, that sounds like why, why this would break down.

988
00:56:28,395 --> 00:56:32,365
Speaker 2:  Yeah. So like if you know, you know, lemme know. Yeah. Just what

989
00:56:32,445 --> 00:56:36,125
Speaker 2:  I, I would love the actual deal terms, but the, the sense I get is that

990
00:56:36,125 --> 00:56:39,045
Speaker 2:  there's money, which should be solvable,

991
00:56:40,185 --> 00:56:44,045
Speaker 2:  but money is usually solvable. But next to it is a bunch

992
00:56:44,045 --> 00:56:47,085
Speaker 2:  of stuff that looks like what Universal wanted out of YouTube,

993
00:56:47,695 --> 00:56:51,645
Speaker 2:  which is we want some special AI stuff, AI terms,

994
00:56:52,505 --> 00:56:53,765
Speaker 2:  and like, it's just not happening.

995
00:56:54,035 --> 00:56:57,845
Speaker 6:  Yeah. And TikTok is just very different company

996
00:56:58,355 --> 00:57:02,245
Speaker 6:  than YouTube and Google. Right. Like, like their motivations for all of

997
00:57:02,245 --> 00:57:05,565
Speaker 6:  this is very different. I was seeing today, there was a story in I think

998
00:57:05,565 --> 00:57:09,325
Speaker 6:  the information about how TikTok was looking into working with

999
00:57:09,525 --> 00:57:13,445
Speaker 6:  companies to develop like AI avatars to better sell. Yeah. Like they

1000
00:57:13,445 --> 00:57:17,245
Speaker 6:  don't care who's on the platform, they can monetize it. So I think they're

1001
00:57:17,245 --> 00:57:20,685
Speaker 6:  much less incentivized to make these deals than, than

1002
00:57:20,875 --> 00:57:21,685
Speaker 6:  YouTube was.

1003
00:57:22,365 --> 00:57:26,285
Speaker 2:  I think you, I think TikTok is a danger to itself. Mm.

1004
00:57:26,965 --> 00:57:30,765
Speaker 2:  I agree with that. I I think they are so rapidly turning that thing into

1005
00:57:30,765 --> 00:57:34,245
Speaker 2:  the home shopping network and just making it so commercialized.

1006
00:57:34,685 --> 00:57:37,885
Speaker 6:  I, and I think in a, in a way that's really putting people off. We had a,

1007
00:57:37,925 --> 00:57:41,445
Speaker 6:  a story on the site this week about from, from V 'cause she called, she's

1008
00:57:41,445 --> 00:57:44,365
Speaker 6:  gonna be calling in a lot of these kind of gadgets we see on TikTok. And,

1009
00:57:44,385 --> 00:57:48,165
Speaker 6:  and the first one she did, she spent her own money like $350

1010
00:57:49,115 --> 00:57:52,885
Speaker 6:  because there was a, a wand that would make the skin beautiful. And like

1011
00:57:53,315 --> 00:57:56,965
Speaker 6:  some of us, just like our skin to be dewy and and glowing. And

1012
00:57:57,305 --> 00:57:57,925
Speaker 2:  I'm one of those people.

1013
00:57:57,925 --> 00:58:00,685
Speaker 5:  Yeah. Wait, let me guess. Can I guess, I bet it didn't work.

1014
00:58:02,115 --> 00:58:02,605
Speaker 6:  Unclear

1015
00:58:04,595 --> 00:58:08,365
Speaker 6:  what did work was the fomo, the, like, the, the massive, massive

1016
00:58:08,465 --> 00:58:11,965
Speaker 6:  regret she had. And, and, and that's like that we're seeing that

1017
00:58:12,135 --> 00:58:16,125
Speaker 6:  constantly with TikTok. They just constantly selling crap. There's the not

1018
00:58:16,125 --> 00:58:19,125
Speaker 6:  crap. Some of it seems to be workable. They, they love

1019
00:58:19,125 --> 00:58:21,965
Speaker 5:  To sell. No, the crap is fine. Yeah. You're good with crap. Okay. I think

1020
00:58:21,965 --> 00:58:25,805
Speaker 5:  you can, you can, you can stage a convincing legal defense on crap there.

1021
00:58:25,845 --> 00:58:26,365
Speaker 5:  I forget about

1022
00:58:26,365 --> 00:58:26,405
Speaker 2:  That.

1023
00:58:27,665 --> 00:58:31,485
Speaker 5:  But No, I, I, I totally agree. And I think in general, TikTok is

1024
00:58:31,835 --> 00:58:35,525
Speaker 5:  this platform that is not interested in being tied

1025
00:58:35,595 --> 00:58:39,405
Speaker 5:  down to anything. Like, it moves so fast culturally, it moves

1026
00:58:39,405 --> 00:58:42,685
Speaker 5:  so fast technologically that you don't get the sense that

1027
00:58:43,155 --> 00:58:46,525
Speaker 5:  it's interested in having any rules for anything for any reason.

1028
00:58:47,065 --> 00:58:49,845
Speaker 5:  And YouTube is just so much more

1029
00:58:50,915 --> 00:58:54,845
Speaker 5:  sort of lowercase and uppercase mature than that. Right. That it is like

1030
00:58:55,525 --> 00:58:58,965
Speaker 5:  learning how to play by these rules in order to be around for a long time.

1031
00:58:58,965 --> 00:59:02,205
Speaker 5:  And TikTok just wants to move a million miles an hour in every direction

1032
00:59:02,205 --> 00:59:06,165
Speaker 5:  all the time. Yeah. And I certainly enjoy TikTok less than I

1033
00:59:06,165 --> 00:59:09,085
Speaker 5:  used to because of that. Like the, the platform is sort of

1034
00:59:09,365 --> 00:59:13,085
Speaker 5:  unrecognizable from what it was even 12 months ago

1035
00:59:13,145 --> 00:59:13,365
Speaker 5:  now.

1036
00:59:13,635 --> 00:59:17,565
Speaker 2:  Yeah. The the sort of like 20 20 21 TikTok

1037
00:59:17,585 --> 00:59:21,525
Speaker 2:  heyday is, is gone. Yeah. you know, I, I maintain a list of

1038
00:59:21,525 --> 00:59:25,405
Speaker 2:  tiktoks that should be PhD thesis and media studies, and

1039
00:59:25,405 --> 00:59:28,405
Speaker 2:  many of them are deleted now. They're just gone. Like, the creators have

1040
00:59:28,405 --> 00:59:31,285
Speaker 2:  quit. They've pulled 'em down. Like they just don't wanna be a part of it

1041
00:59:31,285 --> 00:59:33,645
Speaker 2:  anymore. And I, there's something about that where it's like, this thing

1042
00:59:33,645 --> 00:59:34,885
Speaker 2:  is getting so commercial so fast,

1043
00:59:35,435 --> 00:59:39,365
Speaker 6:  It's so like, I can't tell you how many ads I see now. Yeah.

1044
00:59:39,365 --> 00:59:42,285
Speaker 6:  And it's just got, it's, it's mainly those Lenovo headphones and the guy

1045
00:59:42,285 --> 00:59:45,885
Speaker 6:  being like, man, I found you. I love these Lenovo headphones. Apple

1046
00:59:45,885 --> 00:59:49,045
Speaker 6:  doesn't want you to know how good they are. And I'm like, sir, I think that's

1047
00:59:49,045 --> 00:59:52,965
Speaker 6:  just, you're lying to me. I I don't think you have that insight

1048
00:59:52,965 --> 00:59:56,925
Speaker 6:  into Apple or Lenovo's business. Why are, why

1049
00:59:56,925 --> 01:00:00,645
Speaker 6:  is this on my feed? Why does it have thousands of likes and views? Is

1050
01:00:00,645 --> 01:00:01,845
Speaker 2:  That the ones with the screen on the case?

1051
01:00:02,985 --> 01:00:06,685
Speaker 6:  No, it's like they go in your ear. Oh, weird. Like twisty way those.

1052
01:00:06,755 --> 01:00:10,045
Speaker 6:  Yeah. Don't worry. We'll, we'll be checking 'em out. We wanna see if they

1053
01:00:10,045 --> 01:00:11,325
Speaker 6:  really are as good is Apple.

1054
01:00:11,685 --> 01:00:12,605
Speaker 2:  I love that you're just like V

1055
01:00:13,235 --> 01:00:13,525
Speaker 6:  Yeah.

1056
01:00:14,105 --> 01:00:15,045
Speaker 2:  Buy crap that

1057
01:00:15,275 --> 01:00:18,285
Speaker 6:  View. Like, do I have to? And I'm like, enjoy.

1058
01:00:18,585 --> 01:00:20,845
Speaker 2:  I'm excited for this. Yeah. It's gonna be fun. This is gonna be the greatest

1059
01:00:20,845 --> 01:00:24,725
Speaker 2:  series of TikTok videos we ever make because all the, everyone else on TikTok

1060
01:00:24,725 --> 01:00:25,325
Speaker 2:  is getting paid.

1061
01:00:25,595 --> 01:00:28,645
Speaker 6:  Yeah. Everybody else is getting paid. We're just like, what does it actually

1062
01:00:28,645 --> 01:00:32,285
Speaker 6:  work? Let's find out. Let's, and it doesn't, let's stop, stop buying it.

1063
01:00:32,925 --> 01:00:36,125
Speaker 2:  I look, I think between the Universal stuff, not having that catalog and

1064
01:00:36,345 --> 01:00:39,485
Speaker 2:  not from what I can tell, making any steps towards a resolution.

1065
01:00:39,925 --> 01:00:43,165
Speaker 2:  Eventually all the other label deals are going to come up.

1066
01:00:43,785 --> 01:00:47,725
Speaker 2:  And la the the labels might be short term thinking and like

1067
01:00:47,725 --> 01:00:51,605
Speaker 2:  they're getting benefit now all of them are actively

1068
01:00:51,605 --> 01:00:55,325
Speaker 2:  talking to all of the platforms about AI stuff. Yep. All of them

1069
01:00:55,705 --> 01:00:58,885
Speaker 2:  did. They do not want their stuff trained upon. They do not want the videos

1070
01:00:58,955 --> 01:01:02,525
Speaker 2:  that their stuff gets used in trained upon. They want AI

1071
01:01:02,565 --> 01:01:05,845
Speaker 2:  controls. And like, some of the platforms have like weird,

1072
01:01:06,795 --> 01:01:10,485
Speaker 2:  like you have to make a distinction between the kinds of AI you use. So If

1073
01:01:10,485 --> 01:01:14,365
Speaker 2:  you roll up to, I don't know, Spotify and you're like, don't use ai. Spotify

1074
01:01:14,505 --> 01:01:18,485
Speaker 2:  is like, dude, we've been using ML to do our playlist for like a

1075
01:01:18,485 --> 01:01:22,445
Speaker 2:  hundred years. Yeah. Like, we're gonna keep use. Like, and so

1076
01:01:22,445 --> 01:01:24,885
Speaker 2:  there's this whole education process happening in the industry, which is

1077
01:01:25,085 --> 01:01:28,925
Speaker 2:  fascinating. Other interesting companies or like tools that

1078
01:01:28,985 --> 01:01:32,685
Speaker 2:  use AI to like, help you filter sounds or like pull out stem. It's like

1079
01:01:32,685 --> 01:01:36,365
Speaker 2:  they're in the same conversation where like, this isn't that this isn't the

1080
01:01:36,365 --> 01:01:40,325
Speaker 2:  bad thing. Yeah. But it's all got, everyone thinks AI is generative ai.

1081
01:01:41,035 --> 01:01:44,925
Speaker 6:  Yeah. I I noticed that. It's, it's really consistent and

1082
01:01:44,925 --> 01:01:48,365
Speaker 6:  really annoying. Yeah. I'm like, somewhere James is just screaming.

1083
01:01:49,705 --> 01:01:53,085
Speaker 2:  But it's, I I just think like the next term for TikTok is like this rapid,

1084
01:01:53,805 --> 01:01:56,805
Speaker 2:  I mean the, the term is Kobo Al's term, it's ification.

1085
01:01:57,455 --> 01:02:01,205
Speaker 2:  Right. Where it gave a lot of value to users. It built up a huge user

1086
01:02:01,205 --> 01:02:05,125
Speaker 2:  base. In and Out is like aggressively trying to re extract that value

1087
01:02:05,125 --> 01:02:08,805
Speaker 2:  from its user base. I feel like people are being like, you know what, Instagram

1088
01:02:08,925 --> 01:02:12,805
Speaker 2:  reels exist, YouTube Shorts exists. Like I don't, I don't have to do

1089
01:02:12,805 --> 01:02:13,285
Speaker 6:  This here. Or like

1090
01:02:13,285 --> 01:02:14,325
Speaker 2:  Other ways also might get banned.

1091
01:02:14,725 --> 01:02:18,645
Speaker 6:  Y Yeah. Other, there's just other ways to spend

1092
01:02:18,755 --> 01:02:22,565
Speaker 6:  your time. And I, I, I don't know, I think we're, we're about to hear

1093
01:02:22,715 --> 01:02:26,485
Speaker 6:  have a, a turn from this real love of these short

1094
01:02:27,015 --> 01:02:29,685
Speaker 6:  super ephemeral videos to something more

1095
01:02:30,665 --> 01:02:31,085
Speaker 6:  stable

1096
01:02:31,505 --> 01:02:31,925
Speaker 2:  Novels.

1097
01:02:31,955 --> 01:02:35,725
Speaker 6:  Yeah. Scrolls. Yeah. We're we're everybody's gonna be doing only scrolls.

1098
01:02:36,165 --> 01:02:37,245
Speaker 6:  That's, that's The future

1099
01:02:37,315 --> 01:02:40,845
Speaker 2:  Only Scrolls is a hell of a name for a property. Yeah.

1100
01:02:42,385 --> 01:02:45,205
Speaker 2:  It just, it it just like Shakespeare nudes.

1101
01:02:47,715 --> 01:02:51,085
Speaker 6:  Yeah. The famous, famous writer of scrolls. William Shakespeare.

1102
01:02:51,815 --> 01:02:52,165
Speaker 2:  Sorry.

1103
01:02:55,465 --> 01:02:59,165
Speaker 6:  But I do think we're gonna see like a change in how, how people are

1104
01:02:59,235 --> 01:03:02,925
Speaker 6:  consuming. Like, I, I think there's, there's a rapid moment of change Yeah.

1105
01:03:03,025 --> 01:03:06,765
Speaker 6:  In how we consume media and, and, and what we're consuming and what we prioritize.

1106
01:03:07,345 --> 01:03:11,245
Speaker 6:  And I'm, and there's so much happening in the space. We're gonna talk about

1107
01:03:11,245 --> 01:03:14,405
Speaker 6:  it in a little bit, but like, what's happening with movies and, and streaming

1108
01:03:14,765 --> 01:03:18,525
Speaker 6:  a ton is happening there and it's happening really quickly. And now we're

1109
01:03:18,525 --> 01:03:20,565
Speaker 6:  having the same thing with social media. And these are all ways we consume

1110
01:03:20,565 --> 01:03:24,285
Speaker 6:  our media and they're all in this moment of enormous transition. And

1111
01:03:24,555 --> 01:03:25,885
Speaker 6:  what does that look like on the other side?

1112
01:03:26,115 --> 01:03:29,405
Speaker 5:  Alex, you are so close to pitching Quibi that I just,

1113
01:03:30,125 --> 01:03:31,805
Speaker 5:  I just want it You're, you're so

1114
01:03:32,395 --> 01:03:36,245
Speaker 6:  What what you want is, I think what The future is is actually like an app

1115
01:03:36,535 --> 01:03:40,485
Speaker 6:  where the vi whole video will change format If. you turn it when you

1116
01:03:40,485 --> 01:03:41,405
Speaker 6:  flip, when you turn the phone.

1117
01:03:41,835 --> 01:03:42,845
Speaker 2:  Have you thought about Below

1118
01:03:42,845 --> 01:03:45,205
Speaker 5:  Your Mind has a show that's only at night. Yeah. Yeah.

1119
01:03:45,335 --> 01:03:46,605
Speaker 6:  It'll only launch with

1120
01:03:46,765 --> 01:03:47,965
Speaker 2:  Pandemics thought about what if shows came out at night

1121
01:03:48,755 --> 01:03:49,045
Speaker 5:  That,

1122
01:03:50,335 --> 01:03:54,245
Speaker 2:  David, have you ever asked the AI pin to generate you a nude William Shakespeare?

1123
01:03:55,945 --> 01:03:59,685
Speaker 5:  No. I I could, but we'd get the explicit tag on, on the podcast. We can't

1124
01:03:59,685 --> 01:03:59,885
Speaker 5:  have that.

1125
01:04:00,325 --> 01:04:04,285
Speaker 2:  Actually, this brings us to the other big story of the week. I think a big

1126
01:04:04,305 --> 01:04:08,005
Speaker 2:  New York Times piece on OpenAI and Google and all the rest finding

1127
01:04:08,075 --> 01:04:12,045
Speaker 2:  ways to generate more training data. I think OpenAI is getting

1128
01:04:12,045 --> 01:04:15,285
Speaker 2:  itself into a lot of trouble here. Like they're a startup. They played really

1129
01:04:15,285 --> 01:04:18,565
Speaker 2:  fast and loose. Right. Ask for forgiveness, not permission. Oops.

1130
01:04:18,975 --> 01:04:22,085
Speaker 2:  We're now the hottest company in the world. We have billions of dollars in

1131
01:04:22,245 --> 01:04:25,165
Speaker 2:  Microsoft Partnership and we're getting sued by the Times. Yeah.

1132
01:04:25,165 --> 01:04:29,085
Speaker 6:  Like the, the the ask forgiveness not permission works when you're not

1133
01:04:29,245 --> 01:04:33,125
Speaker 6:  a super powerful company and you, you don't have a ton of money behind

1134
01:04:33,125 --> 01:04:37,085
Speaker 6:  you. Works a lot less when you've got Microsoft Money Bags,

1135
01:04:37,405 --> 01:04:38,485
Speaker 6:  Microsoft behind you. Yeah.

1136
01:04:39,025 --> 01:04:42,925
Speaker 2:  So OpenAI needed more training data to train GPT four. It

1137
01:04:43,035 --> 01:04:46,365
Speaker 2:  developed a system called a Whisper, which transcribed YouTube videos and

1138
01:04:46,365 --> 01:04:50,125
Speaker 2:  then trained on over a million hours of YouTube videos. That's not great.

1139
01:04:50,125 --> 01:04:54,005
Speaker 2:  There's a little back and forth in the media going on about this. So Joanna

1140
01:04:54,005 --> 01:04:56,885
Speaker 2:  Stern at the Wall Street Journal, we may have mentioned this particular thing

1141
01:04:56,885 --> 01:05:00,765
Speaker 2:  before, but Joanna was talking to Mira Mirati, the CTO of OpenAI. She

1142
01:05:00,765 --> 01:05:04,685
Speaker 2:  said, did you train on YouTube videos and Mira in every way

1143
01:05:04,885 --> 01:05:06,925
Speaker 2:  possible said, I don't know, which is insane.

1144
01:05:07,055 --> 01:05:09,325
Speaker 5:  While also making a face that said Yes

1145
01:05:12,985 --> 01:05:16,005
Speaker 2:  Not great. And then Neil Mon was on, I believe Bloomberg.

1146
01:05:17,545 --> 01:05:21,485
Speaker 2:  And they asked the same question and he was like, well, if they

1147
01:05:21,545 --> 01:05:23,445
Speaker 2:  did, that would violate our terms of service.

1148
01:05:23,745 --> 01:05:27,485
Speaker 6:  But it also, and that same New York Times story, wasn't there bits about

1149
01:05:27,545 --> 01:05:31,405
Speaker 6:  how you Google had done, had also trained on some YouTube

1150
01:05:31,465 --> 01:05:31,885
Speaker 6:  videos.

1151
01:05:32,035 --> 01:05:35,725
Speaker 2:  Yeah. So Google owns YouTube. Yeah. So this is very challenging for Google.

1152
01:05:36,185 --> 01:05:39,805
Speaker 2:  So Google is built, and IIII say this just as a factual

1153
01:05:39,865 --> 01:05:43,605
Speaker 2:  matter, Google is built on a very expansive view

1154
01:05:43,705 --> 01:05:47,405
Speaker 2:  of copyright law, and it has aggressively expanded the boundaries of

1155
01:05:47,405 --> 01:05:51,125
Speaker 2:  copyright law throughout its existence. So the, the very idea of a Google

1156
01:05:51,135 --> 01:05:54,565
Speaker 2:  index like requires you to go read a bunch of data.

1157
01:05:55,165 --> 01:05:58,685
Speaker 2:  Google image search requires them to host copies of lots of images. That

1158
01:05:58,685 --> 01:06:02,525
Speaker 2:  was a lawsuit. YouTube Viacom famously sued Google for a bunch of

1159
01:06:02,525 --> 01:06:05,605
Speaker 2:  stuff on YouTube. It was found out later Viacom employees uploading videos

1160
01:06:05,625 --> 01:06:09,205
Speaker 2:  to YouTube to promote them in the echoes of the TikTok situation. Yeah.

1161
01:06:09,565 --> 01:06:13,165
Speaker 2:  But they, they beat Viacom like YouTube YouTube exists. So Google

1162
01:06:13,275 --> 01:06:16,885
Speaker 2:  just like constantly expands the boundary of copy law, like

1163
01:06:17,065 --> 01:06:20,445
Speaker 2:  as a function of its existence. That's a thing that Google Books we're gonna

1164
01:06:20,515 --> 01:06:24,205
Speaker 2:  scan all the books in the world without permission to make an index of them

1165
01:06:24,425 --> 01:06:27,645
Speaker 2:  and then convince a judge that this will sell more books. Sure. It worked.

1166
01:06:27,645 --> 01:06:31,485
Speaker 2:  That, that worked. Google won those cases when it was like a, a cuddle

1167
01:06:31,485 --> 01:06:35,405
Speaker 2:  bug, you know, just a bunch of goofballs. We got slides in the office.

1168
01:06:35,435 --> 01:06:39,005
Speaker 2:  Have you ever, you know, like the judge had like a Dell pc Yeah. And they're

1169
01:06:39,005 --> 01:06:42,605
Speaker 2:  like, this internet's amazing. Right. Those days are over. It's a gateway.

1170
01:06:43,005 --> 01:06:46,205
Speaker 5:  Well, and again, when the theory behind all of those products was to help

1171
01:06:46,205 --> 01:06:49,805
Speaker 5:  people find them. Yeah, yeah. Right. Like it wasn't always true, but that

1172
01:06:49,805 --> 01:06:53,565
Speaker 5:  was, at least the story was that Google is saying, we are going to, we are

1173
01:06:53,565 --> 01:06:57,525
Speaker 5:  going to ingest this stuff in service of helping people find them and

1174
01:06:57,545 --> 01:06:59,365
Speaker 5:  go back to you the creator of them.

1175
01:06:59,435 --> 01:07:01,605
Speaker 2:  Yeah. We are gonna, we're gonna index all these links and then we're gonna

1176
01:07:01,605 --> 01:07:04,605
Speaker 2:  send you to the webpages that Right. We're, we're linking to, we're gonna

1177
01:07:04,685 --> 01:07:08,005
Speaker 2:  ingest all these images and then you can go look at the images for real fine.

1178
01:07:08,345 --> 01:07:12,205
Speaker 2:  But it is also true that the judges were evaluating a service that

1179
01:07:12,205 --> 01:07:16,165
Speaker 2:  existed on like a CRT monitor in the den in the computer room.

1180
01:07:16,315 --> 01:07:20,285
Speaker 2:  Yeah. And Google was a bunch of like 20-year-old kids, and they were just

1181
01:07:20,325 --> 01:07:23,685
Speaker 2:  a, just like a different company, a different time, a different company,

1182
01:07:23,685 --> 01:07:27,645
Speaker 2:  different cast of characters, different relationships to power, blah, blah,

1183
01:07:27,645 --> 01:07:31,205
Speaker 2:  blah, blah, blah. You come to now, and you have

1184
01:07:31,465 --> 01:07:34,805
Speaker 2:  OpenAI headlong into a dispute with Google

1185
01:07:35,695 --> 01:07:39,245
Speaker 2:  about training on YouTube. You have Google in a headlong dispute with its

1186
01:07:39,245 --> 01:07:42,765
Speaker 2:  own creators about whatever the YouTube terms of service, say Google expanded

1187
01:07:42,865 --> 01:07:46,605
Speaker 2:  its terms of service recently. All these are gonna be, you have the

1188
01:07:46,605 --> 01:07:49,805
Speaker 2:  New York Times suing OpenAI, you have all these lawsuits happening simultaneously,

1189
01:07:49,985 --> 01:07:53,565
Speaker 2:  or all these conflicts happening simultaneously. And none of these, I just,

1190
01:07:53,595 --> 01:07:56,685
Speaker 2:  none of these companies are as sympathetic as Google was.

1191
01:07:57,245 --> 01:08:01,165
Speaker 6:  I think it's because it's, they're so nakedly doing it for money in

1192
01:08:01,165 --> 01:08:04,925
Speaker 6:  a way they weren't before. Right. Like, like the the the cost for

1193
01:08:04,985 --> 01:08:08,725
Speaker 6:  or the benefit for, for regular people is, is much lower

1194
01:08:08,905 --> 01:08:12,605
Speaker 6:  now than it was before. And, and I think that's what we're seeing with a

1195
01:08:12,605 --> 01:08:16,325
Speaker 6:  lot of the, just the general tension around generative ai is it feels like,

1196
01:08:16,355 --> 01:08:20,045
Speaker 6:  okay, we are, we are devaluing things that we

1197
01:08:20,155 --> 01:08:24,125
Speaker 6:  tend to feel very value, like very strongly about all in the effort

1198
01:08:24,125 --> 01:08:26,805
Speaker 6:  to make Eric Schmidt richer. Yeah. And

1199
01:08:27,725 --> 01:08:30,605
Speaker 6:  shockingly people don't wanna do that. I'm surprised.

1200
01:08:31,565 --> 01:08:34,285
Speaker 2:  So a very funny part of the story is Meta

1201
01:08:35,885 --> 01:08:39,725
Speaker 2:  a less sympathetic company on the scale of things. Although Zuck, now that

1202
01:08:39,725 --> 01:08:41,765
Speaker 2:  he's ripped with the shaggy hair and The good jackets. Oh my God.

1203
01:08:41,765 --> 01:08:42,085
Speaker 6:  He's got

1204
01:08:42,085 --> 01:08:43,205
Speaker 2:  The v he's come back around.

1205
01:08:45,755 --> 01:08:49,725
Speaker 2:  They want to train today so bad that they considered buying Simon and Schuster.

1206
01:08:50,555 --> 01:08:53,445
Speaker 2:  It's just a straight up book publisher. So they could just ingest all the

1207
01:08:53,445 --> 01:08:57,365
Speaker 2:  books without copyright Wars. That's crazy. That's just a crazy place for

1208
01:08:57,365 --> 01:08:57,885
Speaker 2:  us all to be,

1209
01:08:59,125 --> 01:09:02,485
Speaker 6:  I i i this New York Times story, and I really encourage our audience to,

1210
01:09:02,485 --> 01:09:06,045
Speaker 6:  to go read it. It's called How Tech Giants Cut Corners to Harvest

1211
01:09:06,275 --> 01:09:10,165
Speaker 6:  Data for ai. It came out on Sunday, this Sunday, if you're listening to

1212
01:09:10,405 --> 01:09:14,005
Speaker 6:  this on Friday. Yeah. It came out this last Sunday is it's really, really

1213
01:09:14,005 --> 01:09:17,165
Speaker 6:  good. And I think what really struck me, the big takeaway I came outta this

1214
01:09:17,165 --> 01:09:19,925
Speaker 6:  story with was that these companies have all

1215
01:09:20,885 --> 01:09:24,765
Speaker 6:  recognized there is an intrinsic value in creating cool stuff,

1216
01:09:25,265 --> 01:09:28,525
Speaker 6:  in putting it out into the world. They recognize that and they're now saying,

1217
01:09:28,545 --> 01:09:32,525
Speaker 6:  how do we make machines copy all of that and

1218
01:09:32,545 --> 01:09:36,005
Speaker 6:  so we can do it poor, like we can make, we can do it worse

1219
01:09:36,545 --> 01:09:40,005
Speaker 6:  for more money. And I think like that was just a really

1220
01:09:40,435 --> 01:09:43,805
Speaker 6:  like demoralizing thing I think to read as a cr as somebody who does create

1221
01:09:43,805 --> 01:09:47,005
Speaker 6:  things. And that's like my job. It was like, oh, that,

1222
01:09:47,675 --> 01:09:51,085
Speaker 6:  that sucks. But the story was just fascinating 'cause it was just like they

1223
01:09:51,085 --> 01:09:54,925
Speaker 6:  are in such a race to get data to buy Simon and Schuster to,

1224
01:09:55,265 --> 01:09:59,165
Speaker 6:  to potentially steal their own products from Google or

1225
01:09:59,165 --> 01:10:01,445
Speaker 6:  from YouTube and, and from, yeah.

1226
01:10:01,615 --> 01:10:04,565
Speaker 2:  Right. So a clear problem here, and this is sort of always a problem when

1227
01:10:04,565 --> 01:10:07,885
Speaker 2:  it comes to regulating companies this rich, is that all of this might just

1228
01:10:07,885 --> 01:10:11,845
Speaker 2:  seem like an acceptable tax to them. Yeah. So yep.

1229
01:10:11,845 --> 01:10:15,445
Speaker 2:  We ripped off a bunch of book publishers, Sarah Silverman's mad, we're just

1230
01:10:15,445 --> 01:10:16,085
Speaker 2:  gonna pay her the money.

1231
01:10:17,905 --> 01:10:21,365
Speaker 2:  But some of these, some of them are not going to settle. Like I don't think

1232
01:10:21,365 --> 01:10:23,765
Speaker 2:  Sarah Silverman's gonna settle. I don't think the Times is gonna settle that

1233
01:10:23,765 --> 01:10:27,725
Speaker 2:  case for money. And so if they, they lose those cases, the precedent

1234
01:10:27,725 --> 01:10:31,085
Speaker 2:  is really bad. And the case that I'm just thinking about a lot

1235
01:10:31,985 --> 01:10:35,405
Speaker 2:  is what if YouTube creators pressure Google

1236
01:10:35,955 --> 01:10:39,885
Speaker 2:  into suing OpenAI or being in some open financial conflict with,

1237
01:10:39,885 --> 01:10:43,765
Speaker 2:  with OpenAI? What if YouTube creators sue OpenAI directly and say you

1238
01:10:43,765 --> 01:10:47,525
Speaker 2:  scraped YouTube and implicate Google along the way

1239
01:10:48,035 --> 01:10:51,765
Speaker 2:  because neither one of those companies wants to set the precedent

1240
01:10:52,075 --> 01:10:54,765
Speaker 2:  that training an AI model is copyright infringement.

1241
01:10:54,835 --> 01:10:57,605
Speaker 6:  Yeah. They both want it to not be. And and that was one of the things in

1242
01:10:57,605 --> 01:11:01,445
Speaker 6:  the story was like Google is even like, are we suppo, are we allowed to,

1243
01:11:01,945 --> 01:11:04,445
Speaker 6:  to scrape our own stuff from YouTube? And everybody's like, don't worry about

1244
01:11:04,445 --> 01:11:04,525
Speaker 6:  it.

1245
01:11:04,575 --> 01:11:07,005
Speaker 2:  Right. And then they expanded their terms of service keeps. Yeah. OpenAI

1246
01:11:07,005 --> 01:11:10,285
Speaker 2:  told us when we were writing about the story that OpenAI uses numerous sources

1247
01:11:10,595 --> 01:11:14,525
Speaker 2:  including publicly available data, big circle around what that

1248
01:11:14,525 --> 01:11:18,365
Speaker 2:  means and partnerships for non-public data and that it

1249
01:11:18,365 --> 01:11:21,445
Speaker 2:  is looking into generating its own synthetic data,

1250
01:11:21,695 --> 01:11:22,885
Speaker 6:  Which is insane.

1251
01:11:23,025 --> 01:11:26,765
Speaker 2:  So multiple terms, I would just remind everyone

1252
01:11:26,765 --> 01:11:30,085
Speaker 2:  listening to this, having something on the internet does not mean it is free

1253
01:11:30,085 --> 01:11:33,925
Speaker 2:  to use. People get very confused about this concept. Like you can

1254
01:11:33,925 --> 01:11:37,005
Speaker 2:  put something on the internet that does not mean it is publicly available.

1255
01:11:38,195 --> 01:11:42,045
Speaker 2:  Like it me it means that it's there. Like

1256
01:11:42,045 --> 01:11:42,365
Speaker 2:  technically

1257
01:11:42,365 --> 01:11:45,285
Speaker 6:  It technically means that it look at it publicly available and that people

1258
01:11:45,305 --> 01:11:48,965
Speaker 6:  can access it. Access it. Yeah. But it doesn't mean that it's actually

1259
01:11:48,965 --> 01:11:51,965
Speaker 6:  legally publicly available. Can't just take it. Yeah. Because because there's

1260
01:11:52,085 --> 01:11:55,725
Speaker 6:  incidents of people being like getting sued for for accessing publicly

1261
01:11:55,725 --> 01:11:58,965
Speaker 6:  available publicly. Yeah. I'm using scare quotes here, publicly available

1262
01:11:58,995 --> 01:12:01,445
Speaker 6:  data. And then it was like, no, you weren't actually supposed to go in there

1263
01:12:01,445 --> 01:12:02,525
Speaker 6:  and you knew you weren't supposed to

1264
01:12:04,105 --> 01:12:04,765
Speaker 2:  Bad. Yeah. That's

1265
01:12:04,765 --> 01:12:08,245
Speaker 5:  Bad. I would say two things to that though. One is that the

1266
01:12:08,355 --> 01:12:12,285
Speaker 5:  precedent we have on a lot of that stuff leans toward

1267
01:12:12,345 --> 01:12:14,805
Speaker 5:  it is okay to scrape websites and

1268
01:12:16,265 --> 01:12:17,765
Speaker 2:  Eh eh,

1269
01:12:18,505 --> 01:12:22,045
Speaker 5:  And you are also the one who comes on this show every time we talk about

1270
01:12:22,045 --> 01:12:25,685
Speaker 5:  copyright law and reminds us all that copyright fights are a coin

1271
01:12:25,795 --> 01:12:26,085
Speaker 5:  toss.

1272
01:12:26,275 --> 01:12:30,085
Speaker 2:  Yeah. And like I'm just saying, like If you, if open AI's position is we

1273
01:12:30,085 --> 01:12:33,965
Speaker 2:  use publicly available information, the def what OpenAI believes publicly

1274
01:12:33,965 --> 01:12:37,405
Speaker 2:  available means actually turns out to be massively important, right? Yes.

1275
01:12:37,905 --> 01:12:41,445
Speaker 2:  And it, I think what they mean is we clicked on it on a website

1276
01:12:43,145 --> 01:12:44,685
Speaker 2:  and so it's ours now. That

1277
01:12:44,685 --> 01:12:46,005
Speaker 5:  Does seem to be the case. Yeah. Yes.

1278
01:12:46,065 --> 01:12:49,885
Speaker 2:  And and I'm just saying like, there's not an interpretation anywhere where

1279
01:12:49,885 --> 01:12:53,845
Speaker 2:  that is the thing. Right. And so you just get to a point where like any YouTube

1280
01:12:53,845 --> 01:12:57,805
Speaker 2:  creator, Mr. Beast, do it for the views. you know what I'm

1281
01:12:57,805 --> 01:13:01,605
Speaker 2:  saying? Just tell OpenAI you're gonna sue them because they copied your YouTube

1282
01:13:01,605 --> 01:13:01,845
Speaker 2:  videos.

1283
01:13:02,365 --> 01:13:04,805
Speaker 5:  I sued Google for a hundred million dollars. Right.

1284
01:13:06,035 --> 01:13:06,445
Speaker 6:  He'd give it

1285
01:13:06,445 --> 01:13:10,405
Speaker 2:  Away. No OpenAI, I'm a YouTube creator and I sued OpenAI. And OpenAI is

1286
01:13:10,405 --> 01:13:14,245
Speaker 2:  gonna say, your video is publicly available. We believe that this is

1287
01:13:14,245 --> 01:13:17,405
Speaker 2:  not copyright infringement. And Google is gonna sit in the middle of that

1288
01:13:17,415 --> 01:13:21,325
Speaker 2:  being like, we need to protect our YouTube creators. This

1289
01:13:21,325 --> 01:13:24,805
Speaker 2:  is not in our terms of service. That's what Google said to us. Google takes

1290
01:13:24,805 --> 01:13:28,245
Speaker 2:  technical and legal measures to prevent unauthorized use when we have a clear

1291
01:13:28,245 --> 01:13:32,045
Speaker 2:  legal or technical basis to do so that's, that's Google's approach,

1292
01:13:32,045 --> 01:13:34,885
Speaker 2:  right? This is ours. We have contracts with creators. You can't just take

1293
01:13:34,885 --> 01:13:38,765
Speaker 2:  it there. There's just a war here coming. Yeah. 'cause all these

1294
01:13:39,005 --> 01:13:42,685
Speaker 2:  companies are like, we're gonna go read the entire web. And Google's like,

1295
01:13:42,685 --> 01:13:46,525
Speaker 2:  but that's what we do. Like what? Like something bad. And

1296
01:13:46,525 --> 01:13:50,405
Speaker 2:  they run one closed platform in YouTube. And I I just could

1297
01:13:50,405 --> 01:13:53,365
Speaker 2:  not tell you what happens next. So that's one that's just publicly available.

1298
01:13:53,515 --> 01:13:57,405
Speaker 2:  Then there's synthetic data. Oh my God. Which is bananas. So all these

1299
01:13:57,685 --> 01:14:00,445
Speaker 2:  companies now are like, we're running outta training data. The humans are

1300
01:14:00,445 --> 01:14:04,285
Speaker 2:  not making art fast enough for us to steal. What if we train

1301
01:14:04,385 --> 01:14:08,365
Speaker 2:  on AI generated data? And I just, I'm like, you guys are gonna

1302
01:14:08,365 --> 01:14:08,965
Speaker 2:  kill yourselves.

1303
01:14:09,435 --> 01:14:12,685
Speaker 6:  Well that that was the, the consensus in the story too. They spoke to like

1304
01:14:13,025 --> 01:14:16,765
Speaker 6:  actual experts who were like, no, that's bad. That that just

1305
01:14:16,765 --> 01:14:20,685
Speaker 6:  makes all the problems worse. All the hallucinations just

1306
01:14:20,685 --> 01:14:24,005
Speaker 6:  get worse. You just start going down these weird rabbit holes. Yeah. And

1307
01:14:24,005 --> 01:14:27,925
Speaker 6:  then you have Kevin Rus like, am I in love with my ai? But that's what

1308
01:14:27,925 --> 01:14:28,125
Speaker 6:  happens.

1309
01:14:28,355 --> 01:14:31,245
Speaker 2:  Yeah. Sydnee is like, I love myself, but also I hate myself

1310
01:14:32,485 --> 01:14:35,485
Speaker 2:  becoming finally the full goth sydnee that we've been looking for.

1311
01:14:36,565 --> 01:14:38,845
Speaker 5:  I think the thing that really jumps out to me about this story just from

1312
01:14:38,845 --> 01:14:42,365
Speaker 5:  like a purely unrelated but insane thing is that Whisper

1313
01:14:43,545 --> 01:14:46,805
Speaker 5:  was created at OpenAI explicitly to do this.

1314
01:14:47,395 --> 01:14:51,205
Speaker 5:  Like Whisper is amazing technology and, and

1315
01:14:51,265 --> 01:14:54,685
Speaker 5:  the like speech to text stuff that is happening again. Like

1316
01:14:55,015 --> 01:14:58,365
Speaker 5:  we're all journalists, we like transcribe things all the time. More and more

1317
01:14:58,365 --> 01:15:01,405
Speaker 5:  of that is being powered by Whisper. Whisper is now like a publicly available

1318
01:15:02,055 --> 01:15:05,405
Speaker 5:  technology that OpenAI just put out there and open source than anybody can

1319
01:15:05,405 --> 01:15:09,245
Speaker 5:  have it. It is remarkable tech. And the idea that it was built

1320
01:15:09,315 --> 01:15:12,285
Speaker 5:  just to steal a bunch of YouTube videos is

1321
01:15:13,115 --> 01:15:16,645
Speaker 5:  wild. And like it's hysterical. What a deeply bizarre like,

1322
01:15:17,595 --> 01:15:21,325
Speaker 5:  secondary effect of all of this. Yeah. But it also, I think

1323
01:15:21,425 --> 01:15:24,965
Speaker 5:  the, this story of training data becoming

1324
01:15:25,225 --> 01:15:28,925
Speaker 5:  really valuable and really expensive is starting to be

1325
01:15:28,925 --> 01:15:32,365
Speaker 5:  everywhere. Like there, there's stories about all these companies, whether

1326
01:15:32,365 --> 01:15:36,165
Speaker 5:  you want to train a large language model or you're like a creative

1327
01:15:36,175 --> 01:15:40,125
Speaker 5:  tools company looking for images to train your healing brush

1328
01:15:40,265 --> 01:15:44,125
Speaker 5:  on this stuff is getting expensive. And I think like one thing we

1329
01:15:44,125 --> 01:15:47,565
Speaker 5:  hear a lot as journalists is that The future of the media is making

1330
01:15:47,995 --> 01:15:51,925
Speaker 5:  stories that are training data for these models that like our job is

1331
01:15:51,925 --> 01:15:55,725
Speaker 5:  gonna be to report real time information to a large language model. And that's

1332
01:15:55,725 --> 01:15:59,525
Speaker 5:  our business. And I find that like bleak is all hell. But there is that,

1333
01:15:59,525 --> 01:16:03,325
Speaker 5:  that that future is coming where like there are

1334
01:16:03,325 --> 01:16:06,245
Speaker 5:  going to be businesses that are made that make a lot of money

1335
01:16:07,115 --> 01:16:10,565
Speaker 5:  just being training data. And that is nuts. Yeah.

1336
01:16:10,565 --> 01:16:14,525
Speaker 6:  There are gonna be people who are like, don't worry, our artificial writing

1337
01:16:14,825 --> 01:16:16,205
Speaker 6:  is way better than anybody else. Oh,

1338
01:16:16,205 --> 01:16:18,205
Speaker 2:  That's already true. It's already true. I

1339
01:16:18,205 --> 01:16:22,005
Speaker 6:  Know. But it's like, it's gonna be like more common.

1340
01:16:22,195 --> 01:16:22,485
Speaker 6:  It's

1341
01:16:22,485 --> 01:16:23,765
Speaker 5:  Like the next phase of ClickFORMs.

1342
01:16:24,035 --> 01:16:27,085
Speaker 2:  Yeah. Yeah. Alex, you came to us from Geo Media. Yeah.

1343
01:16:28,545 --> 01:16:32,365
Speaker 6:  Wow. It wasn't always a click. It started

1344
01:16:32,505 --> 01:16:33,405
Speaker 6:  as one, then it

1345
01:16:33,405 --> 01:16:36,245
Speaker 2:  Stopped. I'm sorry. I'm, I then it went, I came up with Eng Gadget. I always

1346
01:16:36,245 --> 01:16:37,925
Speaker 2:  had Eye on Gizmoto and I

1347
01:16:38,555 --> 01:16:39,685
Speaker 6:  Yeah. Now I'm just talking

1348
01:16:39,685 --> 01:16:40,085
Speaker 2:  To No,

1349
01:16:40,085 --> 01:16:44,045
Speaker 6:  No, no, no. I like, like I, I think about that a lot and you, you hear people

1350
01:16:44,045 --> 01:16:47,725
Speaker 6:  talk about how upset I, I talk to a lot of writers and stuff who are like,

1351
01:16:47,725 --> 01:16:50,405
Speaker 6:  oh no, our jobs are gonna be taken away. And it's like, well, the click farm

1352
01:16:50,405 --> 01:16:53,045
Speaker 6:  jobs are gonna be taken away. Yeah. That, that, that first. But those are

1353
01:16:53,045 --> 01:16:57,005
Speaker 6:  also the first jobs You get media a lot of times now. Especially nowadays.

1354
01:16:57,025 --> 01:17:00,805
Speaker 6:  And it's like, okay. So that, that on Ramp is

1355
01:17:00,915 --> 01:17:01,205
Speaker 6:  gone.

1356
01:17:01,555 --> 01:17:04,925
Speaker 2:  Yeah. I look, I think the idea that they're gonna train these systems on

1357
01:17:05,125 --> 01:17:07,485
Speaker 2:  synthetic data, 'cause they've run out of real data. It's hysteric and they

1358
01:17:07,485 --> 01:17:11,325
Speaker 2:  copyright Wars are in full effect. I just keep saying, I know what we are

1359
01:17:11,325 --> 01:17:15,165
Speaker 2:  gonna write about for the next 10 years. Yeah. Yeah. Like, no AI can figure

1360
01:17:15,165 --> 01:17:16,125
Speaker 2:  that out. I

1361
01:17:16,125 --> 01:17:16,645
Speaker 12:  Just always thought it

1362
01:17:16,645 --> 01:17:18,125
Speaker 2:  Was gonna be, you can barely tell David the time.

1363
01:17:19,265 --> 01:17:22,925
Speaker 5:  That's also like that, that cycle you just described is the end of the internet.

1364
01:17:23,555 --> 01:17:24,165
Speaker 5:  Seriously.

1365
01:17:25,675 --> 01:17:29,445
Speaker 5:  Like Google training on Google

1366
01:17:29,675 --> 01:17:33,525
Speaker 5:  created data from Google training is the end of

1367
01:17:33,525 --> 01:17:37,445
Speaker 5:  the internet. Yep. Like that, that becomes an unusable disaster in

1368
01:17:37,625 --> 01:17:38,445
Speaker 5:  record time.

1369
01:17:39,045 --> 01:17:42,845
Speaker 2:  I keep, like Alex just said, she accidentally invented Quibi. Once you

1370
01:17:43,145 --> 01:17:46,485
Speaker 2:  get enough people talking about that, everyone accidentally invents Yahoo.

1371
01:17:47,125 --> 01:17:50,805
Speaker 2:  Like, what if we made a list of good websites? It's like, yeah, that's

1372
01:17:50,925 --> 01:17:52,525
Speaker 2:  Yahoo. That's, that's what that was.

1373
01:17:52,785 --> 01:17:53,805
Speaker 12:  It was unsustainable.

1374
01:17:55,295 --> 01:17:58,125
Speaker 2:  We're gonna make a list of good websites here at The Verge dot com, which

1375
01:17:58,125 --> 01:18:00,645
Speaker 2:  you can visit to which, which you can visit directly in your web browser

1376
01:18:00,705 --> 01:18:03,405
Speaker 2:  and which is written by people. And it'll be sustainable for now.

1377
01:18:04,625 --> 01:18:06,445
Speaker 2:  All right. We gotta take a break. We're gonna come back with lightning round.

1378
01:18:06,545 --> 01:18:08,925
Speaker 2:  I'm very excited about my lightning round item. We'll be right back.

1379
01:20:10,755 --> 01:20:13,175
Speaker 2:  All right, we're back. I'm gonna go last.

1380
01:20:13,565 --> 01:20:17,255
Speaker 6:  Yeah, you're, there's big caps next to yours. Like it's in all

1381
01:20:17,285 --> 01:20:20,175
Speaker 2:  Caps. It's in all caps. It's beautiful. You wanna go first? I'll

1382
01:20:20,175 --> 01:20:23,615
Speaker 6:  Go first. I'll go first. I'm gonna do two actually you do two. I love it.

1383
01:20:23,635 --> 01:20:26,135
Speaker 6:  I'm gonna do two. Yeah. The first one is gonna be, because I just have to

1384
01:20:26,135 --> 01:20:29,615
Speaker 6:  talk about it. Kobo has new color e-readers.

1385
01:20:29,725 --> 01:20:32,855
Speaker 6:  They're both sitting in my house right now. They're using Kaleido three.

1386
01:20:32,965 --> 01:20:36,575
Speaker 6:  It's exact same stuff that Onyx Books has been using since last year.

1387
01:20:36,795 --> 01:20:40,375
Speaker 5:  Oh. you know, Kaleido three, that technology that everyone knows and is familiar

1388
01:20:40,375 --> 01:20:41,735
Speaker 5:  with and has definitely heard us. The

1389
01:20:41,735 --> 01:20:45,055
Speaker 6:  Three people listening to this podcast right now are like, yes. She gets

1390
01:20:45,055 --> 01:20:45,375
Speaker 6:  sick. Classic

1391
01:20:45,645 --> 01:20:47,495
Speaker 5:  Kaleido three talk. Am I right Eli?

1392
01:20:48,055 --> 01:20:48,655
Speaker 6:  I got you.

1393
01:20:49,015 --> 01:20:51,095
Speaker 2:  I got you. I'm like, Alex is talking Kaleido three again.

1394
01:20:52,225 --> 01:20:56,135
Speaker 6:  Don't worry, but it, it, it, they look really cool. I only got

1395
01:20:56,215 --> 01:20:58,535
Speaker 6:  'em yesterday. I haven't gotten to play with 'em yet. But I think what's

1396
01:20:58,535 --> 01:21:01,575
Speaker 6:  really exciting about it is Kobo is probably one of the largest e-reader

1397
01:21:01,575 --> 01:21:05,095
Speaker 6:  manufacturers for buying stuff in the United States. Yeah. They're, they're

1398
01:21:05,095 --> 01:21:08,375
Speaker 6:  Amazon's primary competitor. So that was what was exciting to me. It was

1399
01:21:08,375 --> 01:21:11,575
Speaker 6:  like, oh wow. Someone who actually competes with Amazon is doing this and

1400
01:21:11,575 --> 01:21:14,575
Speaker 6:  Amazon has a new guy in charge of devices. Ooh. Just saying

1401
01:21:15,445 --> 01:21:17,975
Speaker 6:  that color e-reader from Amazon, that color Kindle. It's coming.

1402
01:21:18,275 --> 01:21:20,535
Speaker 2:  Hey, pano is shaking things up over there from, from

1403
01:21:20,535 --> 01:21:21,975
Speaker 6:  What I'm told. I'm feeling it. I'm feeling

1404
01:21:21,975 --> 01:21:22,775
Speaker 2:  It. I mean, how could he not?

1405
01:21:23,645 --> 01:21:23,935
Speaker 6:  Yeah.

1406
01:21:24,355 --> 01:21:26,735
Speaker 2:  Do you think he walked in there and was like, I'm pumped. He was like, I'm

1407
01:21:26,735 --> 01:21:30,695
Speaker 2:  not pumped. Let's work on it. Pumped. This is not

1408
01:21:30,695 --> 01:21:32,495
Speaker 2:  36 hours to pump me up. Get

1409
01:21:32,495 --> 01:21:33,175
Speaker 5:  My pump honey.

1410
01:21:33,715 --> 01:21:35,935
Speaker 6:  And they were like, Kaleido three. And he's like, I don't know what that

1411
01:21:35,935 --> 01:21:37,455
Speaker 6:  means. I, I, now I'm pumped.

1412
01:21:39,755 --> 01:21:41,655
Speaker 2:  So you getting one? You gonna review this thing? Yeah,

1413
01:21:41,685 --> 01:21:44,335
Speaker 6:  I've got 'em. They're You've got 'em. They're sitting in my house. I've only

1414
01:21:44,395 --> 01:21:47,375
Speaker 6:  turned on one so far. Oh my God. 'cause they got here last night and I was

1415
01:21:47,375 --> 01:21:51,255
Speaker 6:  like, woo. Very excited. Like in Slack at 6:00 PM

1416
01:21:51,255 --> 01:21:54,975
Speaker 6:  being like, guys, guys, guys, I got packages. People are like, it's 6:00 PM

1417
01:21:54,975 --> 01:21:56,535
Speaker 6:  and I'm leaving. I'm going home to sleep.

1418
01:21:56,855 --> 01:22:00,695
Speaker 2:  Very good. I'm excited for these reviews. Full one hour Vergecast,

1419
01:22:00,695 --> 01:22:03,535
Speaker 2:  whether or not we distribute that one. Yeah. We'll decide at a later time.

1420
01:22:03,535 --> 01:22:06,775
Speaker 6:  Yeah. And it'll just be for the three of us. And Liam, I'm sorry, Liam.

1421
01:22:07,195 --> 01:22:11,175
Speaker 2:  Hey If. you wanna hear more about the Kaleido three Write Alex a note. Yeah.

1422
01:22:11,175 --> 01:22:14,015
Speaker 2:  Alex dorans The Verge dot com. She'll read 'em. I will

1423
01:22:14,015 --> 01:22:17,735
Speaker 6:  Read it. Ondo three. Yeah. On That's, that's the technology. Please.

1424
01:22:18,045 --> 01:22:21,175
Speaker 6:  This is the Kobo Libra color.

1425
01:22:22,345 --> 01:22:22,695
Speaker 6:  Don't

1426
01:22:22,695 --> 01:22:24,855
Speaker 2:  Quote me on that. I I love that. you know the display tech but not the name

1427
01:22:24,855 --> 01:22:25,735
Speaker 2:  of the product. Yeah.

1428
01:22:26,195 --> 01:22:26,615
Speaker 6:  I'm just very

1429
01:22:26,885 --> 01:22:30,135
Speaker 5:  Excited. This is and it's color with a u It's very important 'cause it's

1430
01:22:30,255 --> 01:22:30,815
Speaker 5:  Canadian. All right.

1431
01:22:30,815 --> 01:22:31,735
Speaker 2:  So that's one. That's

1432
01:22:31,735 --> 01:22:35,575
Speaker 6:  One. The other one is the MPA. Do you remember they used to be called

1433
01:22:35,675 --> 01:22:36,655
Speaker 6:  the MPA. Yeah.

1434
01:22:36,885 --> 01:22:38,135
Speaker 2:  They got rid of America. Yeah,

1435
01:22:38,135 --> 01:22:39,895
Speaker 6:  They got rid of America because they did.

1436
01:22:40,315 --> 01:22:40,895
Speaker 2:  So what happens?

1437
01:22:41,045 --> 01:22:41,855
Speaker 6:  They did. They they were

1438
01:22:41,855 --> 01:22:43,175
Speaker 2:  Like, they're like the Chinese work. It's huge.

1439
01:22:43,245 --> 01:22:46,295
Speaker 6:  Yeah. We need to be more universe. We need to be more global. So we get rid

1440
01:22:46,295 --> 01:22:46,735
Speaker 6:  of the America.

1441
01:22:46,845 --> 01:22:49,375
Speaker 5:  They should have gone the other way. Just be the motion picture Americans

1442
01:22:49,715 --> 01:22:51,335
Speaker 2:  Or the w just add the world.

1443
01:22:52,795 --> 01:22:54,175
Speaker 6:  No, that and Paul.

1444
01:22:54,485 --> 01:22:55,375
Speaker 2:  Yeah. Au

1445
01:22:55,815 --> 01:22:58,895
Speaker 6:  Al that's what they call Jack Palitz.

1446
01:23:00,155 --> 01:23:03,695
Speaker 6:  But they, they are back and they, there's big

1447
01:23:04,835 --> 01:23:08,685
Speaker 6:  distributor conference happening right now in Vegas where they see a lot

1448
01:23:08,685 --> 01:23:12,165
Speaker 6:  of trailers and a lot of stuff. And the, the head of the MPA, the MPA

1449
01:23:12,835 --> 01:23:16,805
Speaker 6:  said We want to work with Congress and we are working with Congress

1450
01:23:16,865 --> 01:23:19,805
Speaker 6:  to bring back site blocking, which is Oh

1451
01:23:19,835 --> 01:23:20,285
Speaker 2:  Yeah, yeah.

1452
01:23:20,755 --> 01:23:24,565
Speaker 6:  It's just annoying. Yeah. Really. Like you hear from their side

1453
01:23:24,565 --> 01:23:28,445
Speaker 6:  where they say there's a lot of piracy going on. We're losing a billion

1454
01:23:28,445 --> 01:23:32,365
Speaker 6:  dollars a year to piracy. We wanna stop that. So we want to

1455
01:23:32,495 --> 01:23:36,245
Speaker 6:  block sites that Americans use to pirate stuff. And that's true.

1456
01:23:36,545 --> 01:23:39,965
Speaker 6:  And you know how they, they, they got piracy to not be a big deal the last

1457
01:23:39,965 --> 01:23:43,925
Speaker 6:  time they introduced easy to use accessible streaming

1458
01:23:44,285 --> 01:23:48,005
Speaker 6:  platforms. And that worked really, really well until they decided to make

1459
01:23:48,005 --> 01:23:51,925
Speaker 6:  them really expensive. Yeah. And it's like, there's a direct

1460
01:23:51,925 --> 01:23:55,845
Speaker 6:  correlation there, guys. You're not serving your audience. Yeah.

1461
01:23:55,845 --> 01:23:59,805
Speaker 6:  And, and so the MPA is back. It's like 2007 all over

1462
01:23:59,815 --> 01:24:00,565
Speaker 6:  again. I'm just,

1463
01:24:00,665 --> 01:24:03,605
Speaker 2:  I'm gonna have to write a sopa Pippa explainer one more time. Where's Alexis

1464
01:24:03,845 --> 01:24:04,605
Speaker 2:  Hanian when you need him.

1465
01:24:04,635 --> 01:24:05,565
Speaker 6:  Yeah. Get ready. Get ready.

1466
01:24:06,685 --> 01:24:10,405
Speaker 2:  Probably half of our audience doesn't remember this. There was an anti-piracy

1467
01:24:10,405 --> 01:24:14,045
Speaker 2:  bill called sopa that would basically have forced various

1468
01:24:14,315 --> 01:24:18,245
Speaker 2:  ISPs to take websites off the internet if they did piracy stuff. And Alexis

1469
01:24:18,645 --> 01:24:22,605
Speaker 2:  Hanian, founder of Reddit will and on the war path to stop this, we

1470
01:24:22,605 --> 01:24:25,205
Speaker 2:  wrote about it. I was all angry about it. The whole thing. Sopa and Pippa,

1471
01:24:25,205 --> 01:24:28,125
Speaker 2:  you can go look it up. It was a real thing that happened. It, it didn't happen.

1472
01:24:28,185 --> 01:24:31,525
Speaker 2:  The internet waged war on Congress and they didn't do it. And I don't think

1473
01:24:31,525 --> 01:24:33,045
Speaker 2:  they've ever forgiven us. Nope.

1474
01:24:33,265 --> 01:24:34,925
Speaker 6:  And they're back. They're back. They're

1475
01:24:34,925 --> 01:24:36,405
Speaker 2:  Ready. Some ideas never die. It

1476
01:24:36,405 --> 01:24:39,965
Speaker 5:  Is such funny deja vu that like Alex, you're exactly right that it's like,

1477
01:24:40,545 --> 01:24:44,045
Speaker 5:  how is no one at this meeting being like, what if we just didn't

1478
01:24:44,675 --> 01:24:46,845
Speaker 5:  make everything so horrible for everyone

1479
01:24:47,025 --> 01:24:47,485
Speaker 2:  All the time?

1480
01:24:48,035 --> 01:24:51,005
Speaker 5:  Like, isn't it weird that all these people are desperate to give us their

1481
01:24:51,005 --> 01:24:54,685
Speaker 5:  money to watch shows and we won't let them so they have to do it illegally.

1482
01:24:55,395 --> 01:24:58,125
Speaker 5:  Like strange wonder how we could fix that.

1483
01:24:58,275 --> 01:25:01,885
Speaker 6:  Yeah. They just need to figure out a digital distribution model where they

1484
01:25:02,075 --> 01:25:05,685
Speaker 6:  make enough money to be happy in all of their very expensive cars while also

1485
01:25:06,065 --> 01:25:09,485
Speaker 6:  not forcing apparently a billion dollars worth of money to go into piracy.

1486
01:25:09,485 --> 01:25:11,965
Speaker 6:  Instead. There might be something there.

1487
01:25:12,105 --> 01:25:14,925
Speaker 2:  If you got idea again? Alex Do Cranz The Verge dot com. Yeah.

1488
01:25:15,105 --> 01:25:15,325
Speaker 6:  Hit

1489
01:25:15,325 --> 01:25:17,405
Speaker 2:  Me up. She's open to ideas. All right. David, what's yours?

1490
01:25:18,595 --> 01:25:22,525
Speaker 5:  Mine is just a thing that I find very funny. So I think it was last week

1491
01:25:22,835 --> 01:25:26,605
Speaker 5:  that Marissa Mayer's company came out with this new app called Shine,

1492
01:25:26,815 --> 01:25:30,605
Speaker 5:  which was a way to basically share photo albums with friends.

1493
01:25:30,825 --> 01:25:34,365
Speaker 5:  And it was the ugliest app that anyone had ever created in the history. It's

1494
01:25:34,365 --> 01:25:38,325
Speaker 5:  very bad of the universe. It's like, imagine you

1495
01:25:38,905 --> 01:25:42,725
Speaker 5:  had never seen an app and you said, I'm going to design an

1496
01:25:42,725 --> 01:25:46,005
Speaker 5:  app in three minutes. That's what this app looked like. And there was some

1497
01:25:46,005 --> 01:25:48,925
Speaker 5:  great stories, including from our friends at Platformer about like the weird

1498
01:25:49,535 --> 01:25:52,725
Speaker 5:  chaos that it seems to be to work for Marissa Meyer and all this stuff that

1499
01:25:52,725 --> 01:25:56,645
Speaker 5:  went wrong. But it has brought up this very funny thing that I have come

1500
01:25:56,645 --> 01:25:59,805
Speaker 5:  to very much enjoy, which is that it turns out sharing photos with your friends

1501
01:26:00,115 --> 01:26:04,045
Speaker 5:  remains like a disastrous technical problem. It's part

1502
01:26:04,045 --> 01:26:07,605
Speaker 5:  of the whole like blue bubble, green bubble apple antitrust stuff.

1503
01:26:07,955 --> 01:26:10,885
Speaker 5:  It's a really hard thing to do across platforms. It's one of the things that

1504
01:26:10,885 --> 01:26:14,165
Speaker 5:  has made like Google Photos very successful, is it's a thing you can use

1505
01:26:14,165 --> 01:26:18,005
Speaker 5:  to share photos. And so Meta rolled

1506
01:26:18,005 --> 01:26:21,765
Speaker 5:  out an update to the Messenger app, which I still

1507
01:26:21,875 --> 01:26:24,325
Speaker 5:  want to call Facebook Messenger every time. Yeah. Even though it's just called

1508
01:26:24,325 --> 01:26:28,245
Speaker 5:  Messenger, that the whole update is that now you

1509
01:26:28,265 --> 01:26:32,245
Speaker 5:  can send better photos in Messenger and people are like amped

1510
01:26:32,245 --> 01:26:35,245
Speaker 5:  about it, where you can send like original resolution photos

1511
01:26:36,665 --> 01:26:39,325
Speaker 5:  in Messenger and they will get to their recipient.

1512
01:26:40,795 --> 01:26:42,845
Speaker 5:  Good. And I think that's great.

1513
01:26:43,635 --> 01:26:44,045
Speaker 2:  Well done.

1514
01:26:44,225 --> 01:26:47,965
Speaker 5:  You know what, what a cool world we live in that this is like an

1515
01:26:48,165 --> 01:26:48,605
Speaker 2:  Unbelievable

1516
01:26:48,835 --> 01:26:49,845
Speaker 5:  Feature upgrade.

1517
01:26:50,155 --> 01:26:53,725
Speaker 2:  Yeah. All that 5G. And we're just now getting to full res photo sharing.

1518
01:26:53,995 --> 01:26:54,285
Speaker 2:  Yeah.

1519
01:26:54,595 --> 01:26:57,685
Speaker 5:  It's just so funny to me. Like we, we've hit this point where phones are

1520
01:26:57,685 --> 01:27:00,325
Speaker 5:  just cameras for all intents and purposes. Like that is the most important

1521
01:27:00,325 --> 01:27:04,245
Speaker 5:  thing your phone does at this moment is take pictures and video. And we have

1522
01:27:04,265 --> 01:27:07,925
Speaker 5:  not yet solved how to send those to your friends.

1523
01:27:08,895 --> 01:27:10,965
Speaker 5:  Isn't that weird? It's very weird to me.

1524
01:27:11,025 --> 01:27:13,965
Speaker 2:  We, I we've solved it in a number of billion dollar company ways

1525
01:27:14,835 --> 01:27:15,325
Speaker 5:  Have we?

1526
01:27:15,945 --> 01:27:17,605
Speaker 6:  The US government's trained to solve it.

1527
01:27:18,305 --> 01:27:18,805
Speaker 2:  That's what you

1528
01:27:18,805 --> 01:27:22,325
Speaker 5:  Are. Eli If, you took a photo and you wanted to send an an

1529
01:27:22,645 --> 01:27:25,685
Speaker 5:  original resolution version of that photo to

1530
01:27:26,905 --> 01:27:30,165
Speaker 5:  my Android phone and Alex's computer right now. How would you do it?

1531
01:27:32,075 --> 01:27:32,565
Speaker 2:  Dropbox?

1532
01:27:33,825 --> 01:27:34,045
Speaker 5:  No,

1533
01:27:34,815 --> 01:27:37,725
Speaker 2:  Sorry. We just had the Kobo Dropbox on decoder, so that's what I came up

1534
01:27:37,725 --> 01:27:38,925
Speaker 2:  with. It's most upsetting possible answer,

1535
01:27:39,465 --> 01:27:40,605
Speaker 6:  But Yeah. But it's

1536
01:27:40,605 --> 01:27:42,205
Speaker 2:  True. Like he was great. I enjoyed

1537
01:27:42,625 --> 01:27:45,085
Speaker 6:  The interview. Maybe not Dropbox, but it would be cloud, right? Yeah. You

1538
01:27:45,085 --> 01:27:46,405
Speaker 6:  just, you'd upload it. That's what I

1539
01:27:46,405 --> 01:27:49,445
Speaker 2:  Told. Yeah. I'd walk over to you and I would plug one USBC cable into my

1540
01:27:49,445 --> 01:27:52,565
Speaker 2:  phone and one us BC cable into your phone and we would just see what end.

1541
01:27:52,835 --> 01:27:56,445
Speaker 2:  Yeah. That works, right? Isn't that a thing? That totally works with no confusion

1542
01:27:56,445 --> 01:27:57,285
Speaker 2:  all the time. Yeah.

1543
01:27:57,385 --> 01:28:00,405
Speaker 5:  So what could possibly go wrong? But anyway, yeah. Kudos to Messenger

1544
01:28:01,225 --> 01:28:01,445
Speaker 5:  for

1545
01:28:01,565 --> 01:28:03,245
Speaker 2:  Boosting Marissa Myers idea

1546
01:28:03,425 --> 01:28:05,565
Speaker 5:  For, for making Good Shine, as I

1547
01:28:05,565 --> 01:28:08,325
Speaker 2:  Call it. Yeah. By the way, everyone should go read. We'll link to the, the

1548
01:28:08,525 --> 01:28:12,045
Speaker 2:  platform or story about the internal, that company is like 30 people and

1549
01:28:12,045 --> 01:28:14,565
Speaker 2:  that story is shockingly well sourced.

1550
01:28:15,025 --> 01:28:15,365
Speaker 5:  Oh yeah.

1551
01:28:15,955 --> 01:28:17,685
Speaker 2:  It's like, wow. It's good. No one likes working.

1552
01:28:18,555 --> 01:28:18,845
Speaker 5:  Yeah,

1553
01:28:19,305 --> 01:28:19,845
Speaker 6:  I'm shocked.

1554
01:28:20,675 --> 01:28:21,725
Speaker 2:  It's very good. Where's the pick

1555
01:28:21,725 --> 01:28:22,965
Speaker 6:  Of her on the Zamboni?

1556
01:28:23,665 --> 01:28:27,445
Speaker 2:  Anyhow, it's beautiful by the way, here. Here's some like Verge lore.

1557
01:28:27,685 --> 01:28:30,845
Speaker 2:  A long time ago in our first office on fifth Avenue,

1558
01:28:32,175 --> 01:28:36,045
Speaker 2:  there was a company next to us and it was like a horrible office. Yeah. But

1559
01:28:36,045 --> 01:28:38,365
Speaker 2:  there's a company next to us called Stamped David. You remember Stamped?

1560
01:28:38,425 --> 01:28:39,765
Speaker 2:  Oh yeah. And this is when

1561
01:28:39,765 --> 01:28:40,645
Speaker 5:  With no E right? It was just

1562
01:28:40,835 --> 01:28:44,085
Speaker 2:  Stamped with no e Yeah. And I don't even, I don't even know what Stamped

1563
01:28:44,345 --> 01:28:48,285
Speaker 2:  did or does, but this was when Marissa Meyer was, she was

1564
01:28:48,305 --> 01:28:51,965
Speaker 2:  at Yahoo and she was buying every company to try to make Yahoo relevant again.

1565
01:28:52,265 --> 01:28:55,285
Speaker 2:  And one day she snuck into our offices and bought, stamped,

1566
01:28:56,585 --> 01:28:59,765
Speaker 2:  ran away again. And we were like, why didn't you tell us? We're right here.

1567
01:29:00,115 --> 01:29:03,445
Speaker 6:  Just we wouldn't wanted to say hi. Yeah. Very funny. It looks like some sort

1568
01:29:03,445 --> 01:29:04,485
Speaker 6:  of clothing company now.

1569
01:29:04,785 --> 01:29:08,325
Speaker 2:  Oh sure. Yeah. We like woke up to like a headline on TechCrunch that was

1570
01:29:08,325 --> 01:29:11,765
Speaker 2:  like Yahoo. Buy stamped. And we're like that right there. Like those guys

1571
01:29:11,875 --> 01:29:12,285
Speaker 2:  next to

1572
01:29:12,285 --> 01:29:13,005
Speaker 6:  Us. Oh no,

1573
01:29:13,105 --> 01:29:16,205
Speaker 2:  It was very good. All right. Here's mine. The most important story of the

1574
01:29:16,205 --> 01:29:19,925
Speaker 2:  week. As you may know, the

1575
01:29:20,035 --> 01:29:23,525
Speaker 2:  Sony Corporation in the late 1980s invented the concept of

1576
01:29:23,525 --> 01:29:24,045
Speaker 2:  Megabase.

1577
01:29:26,085 --> 01:29:29,805
Speaker 6:  I love where this is going also because I know, 'cause I have the rundown

1578
01:29:29,805 --> 01:29:29,965
Speaker 6:  in

1579
01:29:29,965 --> 01:29:33,805
Speaker 2:  Front of me. Megabase, I would say altered the fabric of our

1580
01:29:33,805 --> 01:29:34,205
Speaker 2:  reality

1581
01:29:35,125 --> 01:29:35,605
Speaker 6:  A hundred percent

1582
01:29:36,425 --> 01:29:39,285
Speaker 2:  Across America. And really the world naww

1583
01:29:40,225 --> 01:29:44,085
Speaker 2:  people would push the mega base button and things would change. Yeah.

1584
01:29:44,635 --> 01:29:46,885
Speaker 2:  Just you know, gravitational waves, the whole thing

1585
01:29:47,395 --> 01:29:48,445
Speaker 6:  Just bumping

1586
01:29:48,445 --> 01:29:50,605
Speaker 2:  Parties started happening. Yeah. The Butterfly

1587
01:29:50,605 --> 01:29:51,845
Speaker 5:  Effect is actually about mega bass

1588
01:29:53,025 --> 01:29:55,605
Speaker 2:  If. you add a yellow sports Walkman with a mega base button.

1589
01:29:56,675 --> 01:29:56,965
Speaker 6:  Whoa.

1590
01:29:57,655 --> 01:30:01,565
Speaker 2:  Right. World was your oyster Sony moved away from the

1591
01:30:01,565 --> 01:30:05,405
Speaker 2:  Megabase branding in the late 2010s, I would

1592
01:30:05,405 --> 01:30:05,565
Speaker 2:  say.

1593
01:30:05,665 --> 01:30:06,925
Speaker 6:  Was it because of Meghan Trainor?

1594
01:30:08,875 --> 01:30:10,965
Speaker 2:  They moved to something called Extra Base. Oh,

1595
01:30:11,625 --> 01:30:13,525
Speaker 6:  Ooh. I had a pair of those headphones. They're good. Which

1596
01:30:13,525 --> 01:30:16,365
Speaker 2:  Was deeply confusing and upsetting to, I think everyone is

1597
01:30:16,455 --> 01:30:18,445
Speaker 5:  Extra base, more or less than Megabase.

1598
01:30:18,555 --> 01:30:22,445
Speaker 2:  They never answered my questions. Okay. I've demanded the various CEOs

1599
01:30:22,445 --> 01:30:26,405
Speaker 2:  of Sony come on our shows and explain what happened. And they have all

1600
01:30:26,435 --> 01:30:27,685
Speaker 2:  uniformly refused. They,

1601
01:30:27,685 --> 01:30:28,565
Speaker 6:  They just suddenly emails. I don't even

1602
01:30:28,565 --> 01:30:30,765
Speaker 2:  Think deleted. Don't even, yeah. I don't think they got the, I think the

1603
01:30:30,765 --> 01:30:34,685
Speaker 2:  message were sent away. They were put on a memory stick and then

1604
01:30:34,685 --> 01:30:38,285
Speaker 2:  no one could read the memory stick. And that was the end of that extra bass

1605
01:30:38,285 --> 01:30:39,605
Speaker 2:  has been with us until recently.

1606
01:30:41,505 --> 01:30:43,445
Speaker 2:  Did it? Sony is replacing extra bass,

1607
01:30:43,905 --> 01:30:44,965
Speaker 6:  Double extra bass

1608
01:30:47,075 --> 01:30:48,605
Speaker 2:  With a ULT power sound.

1609
01:30:49,225 --> 01:30:52,925
Speaker 6:  No, no. Sony, who does the naming

1610
01:30:53,065 --> 01:30:53,285
Speaker 6:  at

1611
01:30:53,555 --> 01:30:57,285
Speaker 2:  Sony? ULT Power Sound is a new brand for Sony's new

1612
01:30:57,995 --> 01:31:01,845
Speaker 2:  like party speaker products. They have a new, they have

1613
01:31:01,925 --> 01:31:05,165
Speaker 2:  a new, they have a new party speaker which will come to, okay.

1614
01:31:05,745 --> 01:31:09,605
Speaker 2:  All of the new products have a ULT button on them. This

1615
01:31:09,605 --> 01:31:13,525
Speaker 2:  replaces the megabase or extra base buttons. The ULT,

1616
01:31:13,865 --> 01:31:17,805
Speaker 2:  the ULT buttons offers several modes. ULT one gets

1617
01:31:17,805 --> 01:31:21,645
Speaker 2:  you deeper lower frequency base while ULT two delivers.

1618
01:31:21,965 --> 01:31:23,245
Speaker 2:  Powerful punchy base. Mm.

1619
01:31:25,755 --> 01:31:26,365
Speaker 6:  What does that mean,

1620
01:31:26,395 --> 01:31:29,805
Speaker 2:  Mega An extra. Oh, they should have just labeled the mega. I don't understand.

1621
01:31:30,155 --> 01:31:33,085
Speaker 6:  Well that's too many X's. On on on the, the,

1622
01:31:33,505 --> 01:31:37,445
Speaker 2:  The flagship case of the ULT Power Sound Line is

1623
01:31:37,525 --> 01:31:41,245
Speaker 2:  a 64 pound party speaker. Yeah. Called the ULT Tower 10,

1624
01:31:41,335 --> 01:31:43,245
Speaker 2:  which costs $1,200. Does

1625
01:31:43,245 --> 01:31:43,965
Speaker 6:  It have a screen on it?

1626
01:31:44,625 --> 01:31:48,525
Speaker 2:  It has 34 LED light zones but no screen. It has like a

1627
01:31:48,525 --> 01:31:52,485
Speaker 2:  touch screen at the top. Oh. It just looks, it looks bananas. It really

1628
01:31:52,485 --> 01:31:55,725
Speaker 2:  does have 34 LE lights ends and I'm gonna have one

1629
01:31:58,145 --> 01:32:01,085
Speaker 2:  and there's not like another, I don't know what you, what you thought was

1630
01:32:01,085 --> 01:32:01,605
Speaker 2:  gonna happen. Does your

1631
01:32:01,605 --> 01:32:02,525
Speaker 6:  Wife story know you're gonna have

1632
01:32:02,525 --> 01:32:06,285
Speaker 2:  One? It has an ex balance speaker unit. I dunno what that means, but

1633
01:32:06,285 --> 01:32:06,805
Speaker 2:  Sounds sick.

1634
01:32:07,305 --> 01:32:08,365
Speaker 6:  He did just open up

1635
01:32:08,585 --> 01:32:10,445
Speaker 2:  Amazon and it comes, it comes with the wireless microphone.

1636
01:32:11,225 --> 01:32:12,125
Speaker 6:  Oh, for karaoke.

1637
01:32:12,335 --> 01:32:15,445
Speaker 5:  While you were saying all of those words. Oh my god. This thing's enormous.

1638
01:32:15,635 --> 01:32:16,325
Speaker 2:  It's so, so big.

1639
01:32:17,675 --> 01:32:21,645
Speaker 5:  It's literally, there is a picture of a man rolling it and it's, it's

1640
01:32:21,645 --> 01:32:24,925
Speaker 5:  like, like imagine do you, you know, do you ever see the thing where people

1641
01:32:24,955 --> 01:32:27,765
Speaker 5:  like check a bag with golf clubs in it? Yeah,

1642
01:32:28,155 --> 01:32:29,165
Speaker 2:  It's a si. It's the size of a golf

1643
01:32:29,165 --> 01:32:29,965
Speaker 5:  Bag. It's that size. Yeah.

1644
01:32:29,965 --> 01:32:33,085
Speaker 2:  Yeah. Every single photo that they made for this product is bananas.

1645
01:32:33,875 --> 01:32:37,525
Speaker 2:  Just like every single photo is, it's in the center of dance

1646
01:32:37,545 --> 01:32:40,325
Speaker 2:  floors just fully lit being

1647
01:32:40,325 --> 01:32:40,965
Speaker 5:  Four feet tall

1648
01:32:41,105 --> 01:32:43,165
Speaker 2:  And then everyone's standing in a circle around it.

1649
01:32:43,425 --> 01:32:44,445
Speaker 6:  That's how people party.

1650
01:32:45,405 --> 01:32:48,805
Speaker 2:  I love it so much. I can't, I can't get enough of it

1651
01:32:49,265 --> 01:32:53,205
Speaker 2:  and I all, I wish Is that a, anyone would

1652
01:32:53,205 --> 01:32:56,925
Speaker 2:  tell us what ULT stands for? I think it is

1653
01:32:56,935 --> 01:32:58,045
Speaker 2:  short for Ultimate.

1654
01:32:59,305 --> 01:33:03,165
Speaker 5:  So, okay. I'm so glad you brought this up because while you were saying

1655
01:33:03,605 --> 01:33:07,005
Speaker 5:  whatever those nonsensical words you were saying to explain this thing, I

1656
01:33:07,005 --> 01:33:10,645
Speaker 5:  was trying to figure out the answer to this by scrolling up and down Sony's

1657
01:33:10,645 --> 01:33:14,405
Speaker 5:  website. There are only two things that could be right.

1658
01:33:14,405 --> 01:33:17,525
Speaker 5:  It's either ultra or ultimate. Are there other possibilities?

1659
01:33:19,565 --> 01:33:19,685
Speaker 2:  I

1660
01:33:21,125 --> 01:33:21,685
Speaker 6:  Ulterior

1661
01:33:22,065 --> 01:33:25,365
Speaker 2:  It has like a ulterior, ulterior mode. Ulterior. I'm trying to think of like,

1662
01:33:25,945 --> 01:33:29,765
Speaker 2:  is it a acronym? Like it stands for Ultimate Live

1663
01:33:30,835 --> 01:33:32,205
Speaker 5:  Upsettingly Loud Tower?

1664
01:33:35,405 --> 01:33:35,565
Speaker 6:  Probably,

1665
01:33:36,065 --> 01:33:39,685
Speaker 5:  But I think based on again the, the deep journalism I've been doing scrolling

1666
01:33:39,685 --> 01:33:42,845
Speaker 5:  up and down this website, I believe it is ultimate because the word ultimate

1667
01:33:42,845 --> 01:33:46,405
Speaker 5:  appears, I would say a, a surprising number of times on this website.

1668
01:33:46,625 --> 01:33:48,045
Speaker 6:  It weighs more than a child.

1669
01:33:49,645 --> 01:33:52,565
Speaker 2:  I mean some children. Some children, yeah. Children comes in a variety of

1670
01:33:52,565 --> 01:33:52,845
Speaker 2:  sizes.

1671
01:33:54,255 --> 01:33:56,805
Speaker 6:  63 pounds. More than a toddler probably, right?

1672
01:33:57,075 --> 01:33:58,725
Speaker 2:  Yeah. That's more than my, well she's not

1673
01:33:58,725 --> 01:34:00,525
Speaker 6:  Toddler. I was like, I don't know what toddler weights are.

1674
01:34:00,525 --> 01:34:03,125
Speaker 2:  Yeah, definitely. It's like two Arthurs. It's like a max and a half.

1675
01:34:03,195 --> 01:34:04,205
Speaker 5:  Okay. Yeah, that's about right.

1676
01:34:05,095 --> 01:34:08,045
Speaker 2:  Right. I just got Arthur's a little younger than Max. I'm just like doing

1677
01:34:08,045 --> 01:34:09,125
Speaker 2:  some rough perfect

1678
01:34:09,125 --> 01:34:11,045
Speaker 6:  Units, Verge units of measurement.

1679
01:34:12,945 --> 01:34:16,645
Speaker 2:  That's good. I can't wait to get one of these. Also

1680
01:34:16,745 --> 01:34:20,685
Speaker 2:  the marketing material says massive base ultimate vibe. Yeah. Which is

1681
01:34:20,925 --> 01:34:21,845
Speaker 2:  just what I say now.

1682
01:34:22,555 --> 01:34:24,565
Speaker 6:  They're having so much fun in these, these picture

1683
01:34:24,565 --> 01:34:26,165
Speaker 2:  Massive base ultimate vibe. So

1684
01:34:26,165 --> 01:34:28,965
Speaker 5:  From now on If, you wanna sponsor the lightning round. You are officially

1685
01:34:29,365 --> 01:34:31,045
Speaker 5:  contributing to the NELI Patel party speaker fund.

1686
01:34:31,205 --> 01:34:33,965
Speaker 2:  Yeah, we're trying to get $1,200. We're gonna put one in the back here. Yeah,

1687
01:34:33,975 --> 01:34:37,365
Speaker 2:  we've had other Sony speakers, like other giant Sony speakers in our office

1688
01:34:37,745 --> 01:34:40,725
Speaker 2:  and we can't get rid of them. Like Sony will send them to us. They don't

1689
01:34:40,725 --> 01:34:43,605
Speaker 2:  want those because they know that we care and then we're like, do you want

1690
01:34:43,605 --> 01:34:46,765
Speaker 2:  this back? And they're like, no, it would cost much to mu many dollars to

1691
01:34:46,765 --> 01:34:47,525
Speaker 2:  send us back because

1692
01:34:47,555 --> 01:34:48,445
Speaker 6:  It's four feet

1693
01:34:48,515 --> 01:34:51,245
Speaker 2:  Tall. When we did the Mr. Robot be so tall when we did the Mr. Robot After

1694
01:34:51,245 --> 01:34:54,285
Speaker 2:  show, we would like wheel it into the after show, like the writer of Mr.

1695
01:34:54,295 --> 01:34:55,845
Speaker 2:  Robot Cord. I was like, what the fuck is that?

1696
01:34:57,915 --> 01:35:01,645
Speaker 2:  It's very good. Anyway, I just want to mark If, you are a certain

1697
01:35:01,645 --> 01:35:05,205
Speaker 2:  kind of technology fan. It is important to know that we've gone from

1698
01:35:05,205 --> 01:35:08,845
Speaker 2:  Megabase to extra base to ULT Power Sound.

1699
01:35:09,465 --> 01:35:12,045
Speaker 2:  And I, I think it's important to just take a moment and say, look, a new

1700
01:35:12,045 --> 01:35:15,045
Speaker 2:  generation is here. I don't know if that generation will be defined by ai.

1701
01:35:15,725 --> 01:35:19,645
Speaker 2:  I don't know if it'll be defined by face computers, I

1702
01:35:19,645 --> 01:35:22,925
Speaker 2:  dunno. But I know that this is the generation that is defined by ULT Power

1703
01:35:22,925 --> 01:35:23,205
Speaker 2:  Sound,

1704
01:35:24,195 --> 01:35:25,845
Speaker 5:  Massive base, ultimate vibe.

1705
01:35:26,985 --> 01:35:30,045
Speaker 2:  Oh, David, I forgot to ask you where the AI Pin fits on the, the scale of

1706
01:35:30,045 --> 01:35:33,845
Speaker 2:  wearable bullshit. Oh, I mean it's, it's,

1707
01:35:34,075 --> 01:35:37,445
Speaker 2:  it's nowhere. I think I know the answer. So it's, it's kind of, but it's

1708
01:35:37,445 --> 01:35:41,365
Speaker 2:  value over fids and I think I know the answer, which is zero value. So maximum

1709
01:35:41,675 --> 01:35:41,965
Speaker 2:  fids,

1710
01:35:42,145 --> 01:35:45,645
Speaker 5:  What's interesting of that's true, but what's interesting about it is I would

1711
01:35:45,645 --> 01:35:49,085
Speaker 5:  say it gets like a 0.75 of the face

1712
01:35:49,445 --> 01:35:49,765
Speaker 5:  multiplier

1713
01:35:50,285 --> 01:35:51,885
Speaker 2:  Because it's not your face. I

1714
01:35:52,085 --> 01:35:55,805
Speaker 5:  Was surprised at the extent to which people noticed it, like

1715
01:35:55,985 --> 01:35:59,805
Speaker 5:  out in public. There was, there was a very funny moment where we were

1716
01:35:59,915 --> 01:36:03,645
Speaker 5:  running around shooting the video and it's like a, a weird thing in general

1717
01:36:03,645 --> 01:36:06,285
Speaker 5:  because there's three people pointing cameras at me as I'm doing this. But

1718
01:36:06,285 --> 01:36:09,205
Speaker 5:  I stood in front of the, the Fearless Girl on Wall Street, that little statue

1719
01:36:10,385 --> 01:36:14,205
Speaker 5:  and people were taking pictures of me as I took pictures of the Fearless

1720
01:36:14,235 --> 01:36:18,115
Speaker 5:  Girl statue. It was fantastic. But like, it's

1721
01:36:18,115 --> 01:36:21,675
Speaker 5:  very noticeable, especially when you're standing there sort of talking down

1722
01:36:21,775 --> 01:36:22,435
Speaker 5:  at your chest.

1723
01:36:24,055 --> 01:36:28,035
Speaker 5:  You like, I get more looks wearing the pin than I do wearing like the Ray

1724
01:36:28,035 --> 01:36:29,035
Speaker 5:  Band Smart glasses.

1725
01:36:29,425 --> 01:36:33,075
Speaker 2:  That makes sense. you know, you know what gets a lot of looks, the ult talent

1726
01:36:33,175 --> 01:36:36,515
Speaker 2:  to which weighs 64 pounds and it's 34 L units

1727
01:36:36,535 --> 01:36:39,515
Speaker 5:  As you carry it above your head, say anything style,

1728
01:36:40,055 --> 01:36:43,155
Speaker 2:  Here's what you wanted to do. You want down shift from Ult one to Ult two.

1729
01:36:43,465 --> 01:36:47,395
Speaker 2:  They'll feel it every time. That's it. That's the Red Chest. Everybody

1730
01:36:47,785 --> 01:36:48,275
Speaker 2:  rock and roll.

1731
01:36:52,735 --> 01:36:55,955
Speaker 3:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1732
01:36:55,955 --> 01:36:59,635
Speaker 3:  Give us a call at eight six six Verge one. One. The Vergecast is the

1733
01:36:59,635 --> 01:37:03,195
Speaker 3:  production of The Verge and Vox Media Podcast Network. Our show is produced

1734
01:37:03,195 --> 01:37:06,915
Speaker 3:  by Andrew Marino and Liam James. That's it. We'll see you next week.

