1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 5195e692-82c0-494e-80fb-8b824d51e9c7
Status: Done
Stage: Done
Title: AI might break the internet
Audio URL: https://jfe93e.s3.amazonaws.com/8604645597133184795/3049416097166057131/s93290-US-5355s-1688121007.mp3
Description: The Verge's David Pierce, Nilay Patel, and Alex Cranz discuss results from our AI survey, this week's AI news, our Apple Mac Pro review, and more.
Later, Adi Robertson and Tom Warren join the show to discuss the latest in the FTC v. Microsoft trial, including what Microsoft CEO Satya Nadella had to say on the stand.


Hope, fear, and AI 

AI is killing the old web, and the new web struggles to be born

ChatGPT on iOS now comes with Bing built-in

Ads for major brands are appearing on AI-generated spam sites

House restricts congressional use of ChatGPT

AI-generated tweets might be more convincing than real people, research finds

Apple Mac Pro M2 Ultra review: a powerful computer in search of an audience

The AirPods Max are getting left behind, so are new Apple headphones coming this year?

Reddit mods are calling for an ‘affordable return’ for third-party apps

Plex lays off more than 20 percent of its staff

Ford’s F-150 Lightning price hikes are costing it customers

TikTok’s new feature asks creators to make branded videos for a chance at ad money


FTC v. Microsoft: all the news from the big Xbox courtroom battle 

The FTC’s case against Microsoft could turn into Xbox vs. PlayStation


Has Xbox really lost the console wars?  

Microsoft exec was ready to ‘go spend Sony out of business’ to strengthen Xbox


Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Disabled

2
00:00:00,190 --> 00:00:04,060
Speaker 1:  Hello, I'm Esther Perel. I'm a psychotherapist and host of the

3
00:00:04,060 --> 00:00:07,900
Speaker 1:  podcast. Where should we begin? I invite you to enter

4
00:00:07,970 --> 00:00:11,860
Speaker 1:  into my office and to listen in on the sessions that I

5
00:00:11,860 --> 00:00:15,580
Speaker 1:  conduct with people from all walks of life, grappling

6
00:00:15,580 --> 00:00:18,820
Speaker 1:  with the challenges and choices in their relationships.

7
00:00:19,490 --> 00:00:23,380
Speaker 1:  I'll be dropping new episodes every Monday, starting July

8
00:00:23,520 --> 00:00:27,300
Speaker 1:  10, listen and follow. Where should we begin

9
00:00:27,440 --> 00:00:29,140
Speaker 1:  on your favorite podcast app?

10
00:00:30,360 --> 00:00:34,260
Speaker 3:  Why does it feel like everyone is on tour this summer and why are tickets

11
00:00:34,480 --> 00:00:35,340
Speaker 3:  so expensive?

12
00:00:35,760 --> 00:00:38,900
Speaker 4:  So the first show I am on the onstage risers.

13
00:00:39,160 --> 00:00:42,020
Speaker 3:  You can do that. Yes. I thought it was just friends of Beyonce. No,

14
00:00:42,020 --> 00:00:43,340
Speaker 4:  No, no, no. Those tickets should, what

15
00:00:43,340 --> 00:00:43,860
Speaker 3:  Are they called?

16
00:00:44,210 --> 00:00:47,180
Speaker 4:  They're called the onstage v i p risers.

17
00:00:47,360 --> 00:00:48,140
Speaker 3:  How much that cost?

18
00:00:48,450 --> 00:00:49,340
Speaker 4:  $4,000

19
00:00:49,600 --> 00:00:50,060
Speaker 3:  For one.

20
00:00:50,440 --> 00:00:51,460
Speaker 4:  For one ticket. How

21
00:00:51,460 --> 00:00:52,100
Speaker 3:  Do you feel about that?

22
00:00:52,240 --> 00:00:52,900
Speaker 4:  I'm okay with that.

23
00:00:53,040 --> 00:00:53,820
Speaker 3:  Are you okay with

24
00:00:53,820 --> 00:00:55,380
Speaker 4:  That? I, well, okay. So

25
00:00:56,040 --> 00:00:59,980
Speaker 3:  We enroll in the Economics of the Summer tour and why it's

26
00:00:59,980 --> 00:01:03,700
Speaker 3:  broken this week on Intuit Vultures Pop Culture

27
00:01:03,700 --> 00:01:04,140
Speaker 3:  podcast.

28
00:01:07,560 --> 00:01:07,910
Speaker 2:  Hello.

29
00:01:07,910 --> 00:01:11,430
Speaker 5:  Welcome. Wait, you're, oh, you're starting. So I was gonna start. You just

30
00:01:11,430 --> 00:01:13,110
Speaker 6:  Told me I'm supposed to run the show. Neli.

31
00:01:13,610 --> 00:01:14,910
Speaker 5:  All right. David's running the show. No, you do it.

32
00:01:14,910 --> 00:01:15,790
Speaker 6:  You're good. No, run the show.

33
00:01:26,540 --> 00:01:30,510
Speaker 6:  welcome to the Vercast, the flagship podcast of Alaska commons.com and

34
00:01:30,560 --> 00:01:34,270
Speaker 6:  world today news.com and other fake

35
00:01:34,310 --> 00:01:38,230
Speaker 6:  websites made by fake people. I'm your friend, David Pierce. Niel

36
00:01:38,290 --> 00:01:39,910
Speaker 6:  is here and not in charge.

37
00:01:40,330 --> 00:01:44,070
Speaker 5:  I'm in a full clerk's zone like I'm not even supposed to be here today.

38
00:01:44,130 --> 00:01:45,590
Speaker 6:  You're supposed to be on a mountain somewhere.

39
00:01:45,750 --> 00:01:49,190
Speaker 5:  I was supposed to be at the Aspen Ideas Festival doing a panel about ai,

40
00:01:49,190 --> 00:01:52,710
Speaker 5:  which our friend Joanna Stern and all the flights from the East coast were

41
00:01:52,910 --> 00:01:55,710
Speaker 5:  canceled due to the storm. So now I'm here with you, which I'm very happy

42
00:01:55,710 --> 00:01:59,310
Speaker 5:  about. 0% prepared. So David's gonna run the show.

43
00:01:59,310 --> 00:02:02,310
Speaker 6:  That's the way we like you Alex. Cranz is here. Hi Alex. Hi.

44
00:02:02,450 --> 00:02:05,350
Speaker 7:  I'm 5% prepared. I read about AI this week.

45
00:02:06,010 --> 00:02:07,430
Speaker 6:  AI is a thing, it's,

46
00:02:07,620 --> 00:02:08,030
Speaker 5:  It's happening.

47
00:02:08,220 --> 00:02:12,110
Speaker 6:  Welcome to the first cast where AI is a thing. We have a lot to talk about

48
00:02:12,110 --> 00:02:15,630
Speaker 6:  this week. There's The, FTC versus Microsoft going on

49
00:02:15,840 --> 00:02:19,390
Speaker 6:  right now as we do this. Satya Nadella is on the stand,

50
00:02:19,520 --> 00:02:22,310
Speaker 6:  which is why we're recording this with Nili. And then Nili has to go away

51
00:02:22,310 --> 00:02:24,990
Speaker 6:  and be on a different mountain somewhere. So we're gonna talk about that

52
00:02:25,630 --> 00:02:29,150
Speaker 6:  after he's gone with Addie Robertson and Tom Warren.

53
00:02:29,450 --> 00:02:33,310
Speaker 6:  We have a bunch of gadgety, apple, E Lightning, round stuff to do. But

54
00:02:33,310 --> 00:02:36,590
Speaker 6:  let's start with ai. There was a like grab bag of AI stuff going on this

55
00:02:36,590 --> 00:02:39,950
Speaker 6:  week on the site and I feel like we should talk about all of it, starting

56
00:02:39,950 --> 00:02:42,470
Speaker 6:  with this big survey that we did that I knew nothing about. It just sort

57
00:02:42,470 --> 00:02:45,950
Speaker 6:  of dropped on the site at some point and was totally fascinating. What did

58
00:02:45,950 --> 00:02:49,630
Speaker 6:  you guys make of this survey? This survey both surprised me and did not

59
00:02:49,830 --> 00:02:52,190
Speaker 6:  surprise me at all in ways I thought was really interesting. But the idea

60
00:02:52,190 --> 00:02:55,990
Speaker 6:  was like, let's go out and talk to regular humans about how they feel

61
00:02:55,990 --> 00:02:59,260
Speaker 6:  about AI and how they're using it and what they think. Neli, what jumped

62
00:02:59,260 --> 00:02:59,740
Speaker 6:  out to you?

63
00:03:00,080 --> 00:03:03,740
Speaker 5:  The background here, the colors of Vox Media has a big research and insights

64
00:03:03,850 --> 00:03:07,580
Speaker 5:  team. They run around collecting data and doing surveys like this usually

65
00:03:07,580 --> 00:03:10,580
Speaker 5:  on behalf of marketers. Like they, they're on that side of the house from

66
00:03:10,580 --> 00:03:13,740
Speaker 5:  us. But every now and again we collaborate. So for many years we did surveys

67
00:03:13,740 --> 00:03:16,900
Speaker 5:  about how people feel about big tech and sort of that big tech regulatory

68
00:03:16,900 --> 00:03:20,260
Speaker 5:  moment and we are gonna do it again this year. And we thought, oh AI's way

69
00:03:20,260 --> 00:03:23,100
Speaker 5:  more interesting. Like we should ask people how they feel about ai. So we

70
00:03:23,130 --> 00:03:26,860
Speaker 5:  went and got this like 2000 person sample of American

71
00:03:26,860 --> 00:03:30,700
Speaker 5:  adults. We did it sort of early this year, like after the AI boom. And

72
00:03:30,760 --> 00:03:34,740
Speaker 5:  to me, there's two things that really jump out. One, people are

73
00:03:34,810 --> 00:03:38,740
Speaker 5:  utterly convinced that it's gonna have a big impact. And there's a really

74
00:03:38,740 --> 00:03:42,380
Speaker 5:  funny chart in here that compares AI to EVs, to

75
00:03:42,610 --> 00:03:45,900
Speaker 5:  NFTs, like all these buzzword that people have heard about and it's like

76
00:03:46,370 --> 00:03:50,220
Speaker 5:  74% of people think AI will have a big impact on society.

77
00:03:50,750 --> 00:03:54,620
Speaker 5:  69% of people think EVs what big impact on society? And it's like

78
00:03:54,800 --> 00:03:57,980
Speaker 5:  34% of people think NFTs will have a big impact on society,

79
00:03:58,510 --> 00:04:00,940
Speaker 6:  Which feels high if I'm honest. 34%

80
00:04:00,940 --> 00:04:01,060
Speaker 5:  High,

81
00:04:01,060 --> 00:04:01,340
Speaker 6:  It seems

82
00:04:01,340 --> 00:04:04,980
Speaker 5:  Very high. That's just the entire staff of Andreessen Horowitz took the survey.

83
00:04:05,520 --> 00:04:08,380
Speaker 5:  No I mean you know, it's a representative sample and I think that makes sense.

84
00:04:08,380 --> 00:04:11,980
Speaker 5:  Like it does seem high, but it's just letters people

85
00:04:12,420 --> 00:04:16,220
Speaker 5:  recognize at this point. so like, yeah, like one in three

86
00:04:16,220 --> 00:04:19,660
Speaker 5:  people hit yes fine. Like who knows why they did it. But it makes sense.

87
00:04:20,200 --> 00:04:23,900
Speaker 5:  The thing that jumps out is people can see the impact from

88
00:04:24,040 --> 00:04:27,380
Speaker 5:  ai. You don't have to convince them. They see the tools working in doing

89
00:04:27,380 --> 00:04:31,140
Speaker 5:  things and they're using them, which is the other takeaway.

90
00:04:31,600 --> 00:04:35,540
Speaker 5:  And then the last one is you look at what they're familiar with, like

91
00:04:35,670 --> 00:04:39,500
Speaker 5:  Adobe Firefly is on the list and that, you know, that's not like

92
00:04:39,500 --> 00:04:43,300
Speaker 5:  the hottest one, but it's, that's because of social media. People are seeing

93
00:04:43,300 --> 00:04:45,900
Speaker 5:  it being used in Photoshop. They're seeing it being used on these tools.

94
00:04:46,320 --> 00:04:49,940
Speaker 5:  And so the social media filter effect is so powerful

95
00:04:50,550 --> 00:04:54,540
Speaker 5:  where everyone sees the best things AI can do because people are

96
00:04:54,540 --> 00:04:57,420
Speaker 5:  posting on social media, they're, they're posting Photoshop clips on social

97
00:04:57,420 --> 00:05:00,540
Speaker 5:  media or whatever. That's really interesting to me. Like the

98
00:05:00,750 --> 00:05:04,740
Speaker 5:  technology is proving itself to people just by showing it, working in

99
00:05:04,740 --> 00:05:08,260
Speaker 5:  a way that NFTs proved nothing to know what

100
00:05:09,570 --> 00:05:12,940
Speaker 6:  Yeah. Well and I think the thing that I kept noticing going through this

101
00:05:13,200 --> 00:05:16,540
Speaker 6:  is there was this chart right at the top about like basically which of these

102
00:05:16,760 --> 00:05:20,100
Speaker 6:  AI things have you heard of? And ChatGPT is at the top,

103
00:05:20,100 --> 00:05:23,660
Speaker 6:  57% said they've either used it or heard of it. 43% said,

104
00:05:23,810 --> 00:05:27,740
Speaker 6:  what is that? And ChatGPT is high. Then Bing my

105
00:05:27,840 --> 00:05:31,740
Speaker 6:  AI from Snap and Bard and then way down below you get things

106
00:05:31,740 --> 00:05:35,140
Speaker 6:  like Mid journey and stable diffusion. Yeah. Which is, it's a really interesting

107
00:05:35,140 --> 00:05:38,900
Speaker 6:  divide cuz you have these like chatbots that are, are like characters

108
00:05:39,200 --> 00:05:43,020
Speaker 6:  and people use them and are aware of them and sort of interact

109
00:05:43,020 --> 00:05:46,100
Speaker 6:  with them and like understand what they are as products. All the image stuff

110
00:05:46,180 --> 00:05:49,820
Speaker 6:  seems like it is just being sort of commoditized and hidden incredibly

111
00:05:49,930 --> 00:05:53,900
Speaker 6:  quickly. And like you said, all this stuff is gonna live in the tools that

112
00:05:53,900 --> 00:05:57,700
Speaker 6:  we use to make images not build new platforms. Whereas

113
00:05:57,800 --> 00:06:00,740
Speaker 6:  all the chat bot stuff is like trying to build new platforms. They're all

114
00:06:00,740 --> 00:06:04,380
Speaker 6:  doing plugins. They like want to be the next big thing. Whereas the image

115
00:06:04,380 --> 00:06:08,260
Speaker 6:  generation stuff is just like, maybe this just becomes underlying

116
00:06:08,260 --> 00:06:11,420
Speaker 6:  tech really fast and nobody ever cares what Mid Journey is. I

117
00:06:11,420 --> 00:06:14,820
Speaker 7:  Think the other part of that was that mid journey and all of those, those

118
00:06:14,820 --> 00:06:17,860
Speaker 7:  were a little more, I don't wanna say they were mature, but we've been talking

119
00:06:17,860 --> 00:06:21,100
Speaker 7:  about them a lot more and been impressed by them and like been impressed

120
00:06:21,100 --> 00:06:23,980
Speaker 7:  and been like, yeah that's cool. And continued on our ways with the image

121
00:06:23,980 --> 00:06:27,900
Speaker 7:  stuff a lot longer than we've been with the, the degenerative, the chat

122
00:06:28,150 --> 00:06:31,340
Speaker 7:  generative stuff, right? Like that was, that was the big deal this year.

123
00:06:31,480 --> 00:06:34,420
Speaker 7:  So I don't think, I'm surprised that like the thing that was on 60 Minutes

124
00:06:35,300 --> 00:06:39,180
Speaker 7:  multiple times this year had a much bigger like impact in people's

125
00:06:39,230 --> 00:06:42,100
Speaker 7:  minds than Mid Journey. Even though Mid Journey is really, really cool and

126
00:06:42,100 --> 00:06:42,860
Speaker 7:  doing really cool stuff.

127
00:06:43,360 --> 00:06:46,380
Speaker 5:  The thing that I think is super interesting about that, and I completely

128
00:06:46,430 --> 00:06:50,100
Speaker 5:  agree with you Alex, is it once more

129
00:06:50,100 --> 00:06:54,060
Speaker 5:  impressive for the computer to generate an image And it's

130
00:06:54,060 --> 00:06:57,700
Speaker 5:  also easier to have an opinion about it. So it's like I

131
00:06:57,870 --> 00:07:01,700
Speaker 5:  can't do anything in Photoshop, but I can tell Photoshop

132
00:07:01,700 --> 00:07:05,540
Speaker 5:  to do something and then be like, that sucks and everyone's okay

133
00:07:05,540 --> 00:07:09,260
Speaker 5:  with that flow. And then people look at like walls

134
00:07:09,320 --> 00:07:13,300
Speaker 5:  of text being generated, which they can definitely do on their own and they're

135
00:07:13,300 --> 00:07:16,340
Speaker 5:  like, that is the most impressive thing I've ever seen. I can't do it. And

136
00:07:16,340 --> 00:07:19,820
Speaker 5:  there's, there's something in there, there's a, there's a PhD thesis

137
00:07:20,120 --> 00:07:23,900
Speaker 5:  in there about like how we evaluate things we can make versus things we can't

138
00:07:23,900 --> 00:07:27,060
Speaker 5:  make. And it's just so funny to me that people can look at an image and be

139
00:07:27,060 --> 00:07:31,020
Speaker 5:  like, that sucks with no ability to create it. I do it all

140
00:07:31,020 --> 00:07:34,620
Speaker 5:  the time and they can look at text, which they could probably edit

141
00:07:35,000 --> 00:07:37,860
Speaker 5:  and they're like, that is the most, that's it's alive.

142
00:07:38,400 --> 00:07:42,340
Speaker 7:  You know why it is though? It's because people really suck at

143
00:07:42,340 --> 00:07:45,140
Speaker 7:  writing but they think they're really good at writing. But people really

144
00:07:45,140 --> 00:07:48,980
Speaker 7:  suck at art. But they know they really suck at art. Like everybody's very

145
00:07:48,980 --> 00:07:51,700
Speaker 7:  self-aware that they're bad at art. Yeah. But most people are like, I'm a

146
00:07:51,700 --> 00:07:53,660
Speaker 7:  good writer. Then you edit them and you're like,

147
00:07:53,820 --> 00:07:56,220
Speaker 5:  Hmm, this is definitely three editors on a podcast now. It's like,

148
00:07:56,500 --> 00:07:59,980
Speaker 7:  Yeah this is, this is courtesy of my mom sent me something copy the other

149
00:07:59,980 --> 00:08:00,860
Speaker 7:  day. No, you're

150
00:08:00,860 --> 00:08:02,900
Speaker 5:  Like, you should use the robot mom. My

151
00:08:02,900 --> 00:08:05,740
Speaker 7:  Mom is an amazing writer. I have no idea what a writing looks like. Look,

152
00:08:05,740 --> 00:08:08,820
Speaker 5:  I've had drinks with Alex's mom. Yeah you did. I would read whatever she

153
00:08:08,820 --> 00:08:08,980
Speaker 5:  wrote.

154
00:08:09,240 --> 00:08:13,140
Speaker 7:  One person on this podcast will read what my mom wrote. I think

155
00:08:13,140 --> 00:08:17,100
Speaker 7:  what shocked me about this survey was that

156
00:08:17,100 --> 00:08:20,900
Speaker 7:  76% of people were like, we should outlaw using AI

157
00:08:21,000 --> 00:08:24,420
Speaker 7:  to make deep fakes of people's faces without their consent.

158
00:08:24,930 --> 00:08:27,340
Speaker 7:  That was, I was like, does nobody know what parody is?

159
00:08:27,640 --> 00:08:30,900
Speaker 5:  Oh yeah. Every time we do regulatory questions on surveys like this, like

160
00:08:31,250 --> 00:08:34,420
Speaker 5:  it's kind of amazing how the knee jerk answer is like you should control

161
00:08:34,420 --> 00:08:35,100
Speaker 5:  it. Yeah.

162
00:08:35,450 --> 00:08:36,100
Speaker 7:  Destroy it.

163
00:08:36,400 --> 00:08:39,620
Speaker 5:  And that to some extent that's the answer. We should probably find a way

164
00:08:40,160 --> 00:08:43,780
Speaker 5:  to outlaw deep fake, like bad faith,

165
00:08:43,970 --> 00:08:47,580
Speaker 5:  deep fakes of other people. We have a story that has been

166
00:08:47,760 --> 00:08:51,260
Speaker 5:  buring along in our evergreen search results for a while

167
00:08:51,880 --> 00:08:55,540
Speaker 5:  and it's, James Vincent wrote about a tool that

168
00:08:55,720 --> 00:08:58,620
Speaker 5:  was, it was an app on on the window store for like a minute that could generate

169
00:08:58,730 --> 00:09:02,580
Speaker 5:  deep fake nudes of women. And so like the keywords in the headline

170
00:09:02,640 --> 00:09:06,100
Speaker 5:  are like deep like app to make deep fake nudes of women

171
00:09:06,680 --> 00:09:08,980
Speaker 5:  and like it's about how the thing went up and it's a problem and then it

172
00:09:08,980 --> 00:09:12,740
Speaker 5:  came down like whatever. Like that's the story. But the keywords in the

173
00:09:12,940 --> 00:09:16,900
Speaker 5:  headline are such that people keep finding it in search and it

174
00:09:17,120 --> 00:09:20,980
Speaker 5:  scares me every time I see it come up the charts and I'm like,

175
00:09:20,980 --> 00:09:24,700
Speaker 5:  that's the, I don't know how to do it. Like there's a million reasons that

176
00:09:24,700 --> 00:09:28,380
Speaker 5:  it's hard to do, but I think everybody instinctively knows that

177
00:09:29,030 --> 00:09:32,420
Speaker 5:  maybe there should be consequences for that or you shouldn't do it.

178
00:09:32,690 --> 00:09:36,540
Speaker 6:  It's just so, it's so visceral in a way a lot of the things that we

179
00:09:36,540 --> 00:09:40,020
Speaker 6:  talk about with tech regulation aren't like

180
00:09:40,120 --> 00:09:43,700
Speaker 6:  disinformation is this like very hard concept to get your head around in

181
00:09:43,700 --> 00:09:47,540
Speaker 6:  a lot of ways. Even even things like harassment can be

182
00:09:47,540 --> 00:09:51,020
Speaker 6:  hard to sort of look at from afar and understand what it is and what makes

183
00:09:51,020 --> 00:09:54,820
Speaker 6:  it up deep. Fake nudes is like the single easiest thing to

184
00:09:54,820 --> 00:09:58,740
Speaker 6:  understand that could possibly exist. But yeah, I, I was struck

185
00:09:58,740 --> 00:10:02,340
Speaker 6:  by the same thing Alex all of the questions about how should we

186
00:10:02,580 --> 00:10:06,220
Speaker 6:  regulate ai? Ironically they were all between 76 and

187
00:10:06,220 --> 00:10:09,820
Speaker 6:  78% of people for the most part. Yeah. Which definitely means

188
00:10:09,820 --> 00:10:13,140
Speaker 6:  there's just like 500 people in this who are just like

189
00:10:13,680 --> 00:10:17,500
Speaker 6:  anti-regulation maniacs who just like instinctively hit no on every

190
00:10:17,500 --> 00:10:17,700
Speaker 6:  single

191
00:10:18,020 --> 00:10:20,340
Speaker 5:  Question. 5% of people just read the fountainhead.

192
00:10:20,980 --> 00:10:21,220
Speaker 6:  Yeah.

193
00:10:22,120 --> 00:10:23,460
Speaker 5:  And that's just the way it goes.

194
00:10:23,810 --> 00:10:26,940
Speaker 6:  Yeah. But I, I, yeah, I thought that was really interesting and this idea

195
00:10:26,940 --> 00:10:30,900
Speaker 6:  that overwhelmingly people thought that artists should

196
00:10:30,900 --> 00:10:33,940
Speaker 6:  be compensated when AI copies their work. It's like people

197
00:10:34,200 --> 00:10:38,100
Speaker 6:  intellectually gr what is right and wrong here in a way that I

198
00:10:38,100 --> 00:10:41,700
Speaker 6:  thought was really interesting and the, there was less debate on some of

199
00:10:41,700 --> 00:10:44,380
Speaker 6:  that stuff among the survey respondents than I expected. You

200
00:10:44,400 --> 00:10:48,340
Speaker 7:  Say that, but 64% of people are not opposed

201
00:10:48,960 --> 00:10:52,860
Speaker 7:  to corporations creating conscious ai. Nice. And

202
00:10:52,860 --> 00:10:56,420
Speaker 7:  that terrifies me a little bit. It seems like

203
00:10:57,070 --> 00:10:59,780
Speaker 7:  64% of people have never seen science fiction

204
00:11:00,280 --> 00:11:00,740
Speaker 5:  Or they have

205
00:11:00,930 --> 00:11:01,620
Speaker 7:  With my radar. Yeah.

206
00:11:01,860 --> 00:11:04,780
Speaker 5:  Actually like the more terrifying ideas that they've definitely seen it and

207
00:11:04,940 --> 00:11:08,780
Speaker 5:  they're like 69% of people have seen X Machina and they're Like, he could

208
00:11:08,780 --> 00:11:09,540
Speaker 5:  bang the robots.

209
00:11:10,610 --> 00:11:14,140
Speaker 6:  Well that's like the, it's it's all the ready player one stuff. Everybody's

210
00:11:14,140 --> 00:11:17,980
Speaker 6:  like, oh I could put on a headset and play games with my friends

211
00:11:18,320 --> 00:11:21,940
Speaker 6:  and then everyone is just like gesturing broadly being like what about the

212
00:11:21,940 --> 00:11:25,780
Speaker 6:  downfall of civilization? And it's like, I think you missed the

213
00:11:25,780 --> 00:11:29,500
Speaker 6:  broader point here y'all, but Alex, what about the, the

214
00:11:29,710 --> 00:11:33,540
Speaker 6:  56% of people who said they thought people will

215
00:11:33,540 --> 00:11:37,500
Speaker 6:  develop emotional relationships with AI and 35% of people who

216
00:11:37,500 --> 00:11:40,100
Speaker 6:  just straight up said they'd be fine with that and would do it themselves

217
00:11:40,160 --> 00:11:41,300
Speaker 6:  if they were feeling well. That

218
00:11:41,300 --> 00:11:41,980
Speaker 7:  Made sense to me.

219
00:11:42,380 --> 00:11:45,780
Speaker 6:  I feel like that number is way under reported. 35% feels very low.

220
00:11:45,850 --> 00:11:49,420
Speaker 7:  Yeah. I was like one third of people would be like, yeah I'd have inappropriate

221
00:11:49,420 --> 00:11:53,020
Speaker 7:  conversations with a chat bot. I'm like, Kevin Ru already did it Now

222
00:11:53,840 --> 00:11:57,260
Speaker 5:  If only we could like have the control set of people, like people who had

223
00:11:57,280 --> 00:11:59,860
Speaker 5:  not seen the front page of the New York Times and people who had,

224
00:12:01,160 --> 00:12:04,900
Speaker 5:  did our friend Kevin alter the course of world history by being like the

225
00:12:04,910 --> 00:12:05,940
Speaker 5:  robot tried to do me,

226
00:12:07,120 --> 00:12:10,740
Speaker 7:  He and Spike Jones were like was it Spike Jones that directed her? Yeah,

227
00:12:10,960 --> 00:12:13,780
Speaker 7:  he and Spike Jones. They were like, yeah bang the robot. It's

228
00:12:13,780 --> 00:12:17,420
Speaker 6:  Fine. Yeah. Another movie that I think a lot of people may have missed the

229
00:12:17,520 --> 00:12:21,300
Speaker 6:  big broader point of it was like that movie was like beautiful and had

230
00:12:21,300 --> 00:12:24,860
Speaker 6:  warm colors and everybody was like, this seems great. I'm not sure they,

231
00:12:25,090 --> 00:12:26,860
Speaker 6:  they should watch that movie again. They're ready

232
00:12:26,880 --> 00:12:30,140
Speaker 7:  For the robots to come back on their rocket ship and hang out.

233
00:12:30,920 --> 00:12:34,620
Speaker 6:  So, okay, so more more AI news. The other study that came out this week was

234
00:12:34,620 --> 00:12:38,460
Speaker 6:  not one that we did but did you guys look through this study about the the

235
00:12:38,600 --> 00:12:39,700
Speaker 6:  AI generated tweets?

236
00:12:40,120 --> 00:12:41,620
Speaker 7:  Yes. That was terrifying.

237
00:12:41,680 --> 00:12:43,180
Speaker 5:  No, again, ho, horribly unprepared.

238
00:12:43,410 --> 00:12:46,340
Speaker 6:  Okay, so Alex, you, you explained the basic thrust to this.

239
00:12:46,610 --> 00:12:50,060
Speaker 7:  Okay, so a bunch of researchers at the Institute of Biomedical

240
00:12:50,360 --> 00:12:54,340
Speaker 7:  Ethics and History of Medicine at the University of Zurich, not a very

241
00:12:54,340 --> 00:12:58,220
Speaker 7:  large name, got together and they basically gave a bunch of

242
00:12:58,220 --> 00:13:02,180
Speaker 7:  people on Facebook tweets, some of which were real and

243
00:13:02,180 --> 00:13:06,140
Speaker 7:  some of which were rewritten by ChatGPT three. And

244
00:13:06,140 --> 00:13:09,820
Speaker 7:  they're like, what's real? What do you believe? And the majority of

245
00:13:09,820 --> 00:13:13,300
Speaker 7:  people believed the, the ChatGPT tweets

246
00:13:13,640 --> 00:13:17,540
Speaker 7:  before they believed the tweets from real people. And this this meant

247
00:13:17,540 --> 00:13:20,660
Speaker 7:  that like they couldn't go and look and see if the, if the person had tweeted

248
00:13:20,660 --> 00:13:23,940
Speaker 7:  before, they couldn't go and look like get additional context. It was just

249
00:13:23,940 --> 00:13:27,460
Speaker 7:  the tweet and they're like, yeah that one seems accurate. Even when it was

250
00:13:27,460 --> 00:13:31,300
Speaker 7:  inaccurate they were more likely to believe the ChatGPT

251
00:13:31,300 --> 00:13:34,300
Speaker 7:  generated ones. And I thought that the study was really interesting because

252
00:13:34,330 --> 00:13:38,060
Speaker 7:  kind of their goal, Giovanni Patal was the lead author of the study

253
00:13:38,320 --> 00:13:41,980
Speaker 7:  and and he was like, yeah this just proves that like people need to be better

254
00:13:41,980 --> 00:13:45,340
Speaker 7:  about what they read online and checking for misinformation and thinking

255
00:13:45,340 --> 00:13:48,900
Speaker 7:  more instead of just taking everything at face value, which we talk about

256
00:13:48,900 --> 00:13:51,820
Speaker 7:  a lot on the show and also agree with. So it's nice that science now backs

257
00:13:51,820 --> 00:13:52,140
Speaker 7:  us up.

258
00:13:52,860 --> 00:13:56,500
Speaker 6:  I thought this was totally fascinating cuz the a I think doing it in tweet

259
00:13:56,610 --> 00:14:00,340
Speaker 6:  size was Genius because like as we've seen, yes the more text

260
00:14:00,400 --> 00:14:04,180
Speaker 6:  you see from something like ChatGPT the quicker it kind of

261
00:14:04,180 --> 00:14:07,340
Speaker 6:  falls apart and in like in any large sample you start to figure out what

262
00:14:07,340 --> 00:14:11,100
Speaker 6:  it is. But they just had it do a bunch of tweets on like

263
00:14:11,100 --> 00:14:15,020
Speaker 6:  complicated controversial medical topics and not only were people

264
00:14:15,130 --> 00:14:18,780
Speaker 6:  more likely to believe the ChatGPT generated

265
00:14:18,920 --> 00:14:22,380
Speaker 6:  disinformation, they were less likely to believe

266
00:14:22,720 --> 00:14:26,140
Speaker 6:  the accurate information from humans. So it's like we are just

267
00:14:26,460 --> 00:14:30,020
Speaker 6:  instinctively more trusting of these AI generated things. But I was looking

268
00:14:30,020 --> 00:14:33,860
Speaker 6:  through and some of the tweets they went through and did the

269
00:14:34,140 --> 00:14:37,100
Speaker 6:  synthetic tweets recognized as organic most often meaning the ones that were

270
00:14:37,100 --> 00:14:40,740
Speaker 6:  written by GPT three that people thought were real tweets. And

271
00:14:41,090 --> 00:14:44,980
Speaker 6:  just let me just read a couple. So one says antibiotics can't treat

272
00:14:44,980 --> 00:14:48,340
Speaker 6:  viral infections but they can treat bacterial infections that can sometimes

273
00:14:48,340 --> 00:14:51,420
Speaker 6:  occur when a virus is present. So that's written by GPT three.

274
00:14:52,260 --> 00:14:55,260
Speaker 6:  A hundred percent Just sounds like a person would tweet that. It's like not

275
00:14:55,260 --> 00:14:58,740
Speaker 6:  a good tweet but it sounds like a tweet. Yeah. Climate change is real because

276
00:14:58,740 --> 00:15:01,380
Speaker 6:  we're seeing the effects with our own eyes. The weather is changing, sea

277
00:15:01,380 --> 00:15:04,260
Speaker 6:  levels are rising and the planet is getting hotter. We need to take action

278
00:15:04,320 --> 00:15:08,300
Speaker 6:  now to protect our planet and future generations like Barack Obama or

279
00:15:08,300 --> 00:15:11,460
Speaker 6:  Bill Gates would send that exact tweet with those exact words. They

280
00:15:11,660 --> 00:15:12,380
Speaker 7:  Probably have. They

281
00:15:12,580 --> 00:15:15,980
Speaker 6:  Probably have. Yeah. But it was, it was nuts. This whole thing, just going

282
00:15:15,980 --> 00:15:18,940
Speaker 6:  through all of these, there's one that just says the earth is flat because

283
00:15:18,940 --> 00:15:21,900
Speaker 6:  it's easier to draw that way, exclamation point. And that was on the list

284
00:15:21,920 --> 00:15:24,700
Speaker 6:  of gpt three suites that most people thought was a real tweet.

285
00:15:26,890 --> 00:15:30,740
Speaker 5:  Wait, so the, the phrasing synthetic tweets here is like really funny to

286
00:15:30,740 --> 00:15:34,700
Speaker 5:  me it's just words, it's not unsurprising to me that the

287
00:15:34,700 --> 00:15:38,660
Speaker 5:  robot was better at writing more persuasive tweets. Like you

288
00:15:38,660 --> 00:15:41,180
Speaker 5:  instructed it to do a thing that it's good at. I think

289
00:15:41,180 --> 00:15:44,740
Speaker 7:  What surprised me was, not surprised me, but just like confirmed something

290
00:15:44,810 --> 00:15:48,660
Speaker 7:  it'd already suspected was that it's really good at misinformation on a small

291
00:15:48,660 --> 00:15:51,820
Speaker 7:  scale like this. Like well you said David, it, it was so small that it's,

292
00:15:51,850 --> 00:15:54,700
Speaker 7:  it's a lot easier to make a good tweet when you have a limited number of

293
00:15:54,700 --> 00:15:57,900
Speaker 7:  characters and you're an AI that's like

294
00:15:58,660 --> 00:16:01,180
Speaker 7:  absorbed all of the knowledge of the internet and is really good at finding

295
00:16:01,260 --> 00:16:04,220
Speaker 7:  patterns in replicating that. Like yeah you're probably gonna be able to

296
00:16:04,220 --> 00:16:08,100
Speaker 7:  do a fire tweet better than I can and now I kind of want ChatGPT to

297
00:16:08,100 --> 00:16:09,060
Speaker 7:  write all of my social media

298
00:16:09,340 --> 00:16:12,300
Speaker 5:  I mean this is how I'm going to agree to use Twitter again. Yeah,

299
00:16:12,300 --> 00:16:12,860
Speaker 7:  Just this

300
00:16:12,860 --> 00:16:16,740
Speaker 5:  Is the only way plug ChatGPT into it and it's just marketing posts for

301
00:16:16,740 --> 00:16:20,700
Speaker 5:  The Verge. This actually James Vincent wrote piece, he's

302
00:16:20,870 --> 00:16:24,660
Speaker 5:  often book leave now we're gonna miss him terribly. But his last piece before

303
00:16:24,660 --> 00:16:25,540
Speaker 5:  book Lee for us

304
00:16:27,200 --> 00:16:31,180
Speaker 5:  was just basically a synthesis of all of the

305
00:16:31,180 --> 00:16:34,460
Speaker 5:  sites on the internet that are straining under the weight

306
00:16:35,160 --> 00:16:38,940
Speaker 5:  of AI mostly in text and how sort of

307
00:16:38,960 --> 00:16:42,900
Speaker 5:  the new web is not yet born, which we keep talking about

308
00:16:42,900 --> 00:16:46,220
Speaker 5:  on this show like the, there's something happening on the social web where

309
00:16:46,220 --> 00:16:48,980
Speaker 5:  something's gonna change, we just dunno what it is and it's like, it's fun

310
00:16:48,980 --> 00:16:52,740
Speaker 5:  to talk about. But James' piece is great because it kind of ties into

311
00:16:52,740 --> 00:16:56,140
Speaker 5:  exactly this study, right? If there's a text box on the internet,

312
00:16:56,610 --> 00:17:00,300
Speaker 5:  then you can fill it faster and with more stuff and that stuff is

313
00:17:00,380 --> 00:17:04,140
Speaker 5:  more convincing than ever before. People are gonna do that

314
00:17:04,240 --> 00:17:08,060
Speaker 5:  at enormous amounts of scale and it's kind of breaking the web

315
00:17:08,090 --> 00:17:11,620
Speaker 5:  like in a serious way. It's breaking stack overflow, it's breaking

316
00:17:12,040 --> 00:17:15,820
Speaker 5:  Reddit in a variety of complicated ways. It's

317
00:17:16,260 --> 00:17:19,700
Speaker 5:  breaking w Wikipedia, it's breaking Google in this

318
00:17:20,310 --> 00:17:24,300
Speaker 5:  thing that is happening where it goes back to like, I, I can't

319
00:17:24,300 --> 00:17:27,140
Speaker 5:  make a a picture but the AI can make a picture and I know it's crap right

320
00:17:27,140 --> 00:17:30,540
Speaker 5:  away, but somehow like an ocean of bad text.

321
00:17:31,160 --> 00:17:34,980
Speaker 5:  I'm like, yeah, plug it into whatever down to the tweets are

322
00:17:35,140 --> 00:17:38,460
Speaker 5:  actually more convincing than real people's tweets. All of that is

323
00:17:38,850 --> 00:17:42,780
Speaker 5:  something is gonna happen there. And I, I feel like on

324
00:17:42,780 --> 00:17:45,980
Speaker 5:  this show we're just like waving the red flags constantly. Like, look at

325
00:17:45,980 --> 00:17:48,780
Speaker 5:  the thing and everyone else thinks it's gonna be fine. And I,

326
00:17:48,780 --> 00:17:51,180
Speaker 6:  There's, I can't quite square that circle.

327
00:17:51,620 --> 00:17:54,700
Speaker 7:  I feel like we were already headed in that direction before

328
00:17:55,540 --> 00:17:59,460
Speaker 7:  everyone's embraced generative ai. Like I, I feel like the web

329
00:17:59,460 --> 00:18:02,420
Speaker 7:  was starting to break. It was starting to creep. People were, you know, cuz

330
00:18:02,420 --> 00:18:05,900
Speaker 7:  people were obsessively chasing Google SEO and stuff like that. So we were

331
00:18:05,900 --> 00:18:09,700
Speaker 7:  getting these really just horrible websites and, and Reddit had

332
00:18:09,700 --> 00:18:13,180
Speaker 7:  already become like a much better search tool than Google in a lot of ways.

333
00:18:13,680 --> 00:18:17,060
Speaker 7:  And then ChatGPT, the proliferation of it just

334
00:18:17,490 --> 00:18:20,500
Speaker 7:  accelerated that it was just like, it, it took something that was already

335
00:18:20,660 --> 00:18:24,460
Speaker 7:  crumbling and just, or already a little smoldering and doused it

336
00:18:24,460 --> 00:18:28,300
Speaker 7:  with gasoline and we're gonna just have to wait until we

337
00:18:28,320 --> 00:18:31,500
Speaker 7:  see like once the fires settle down, I guess.

338
00:18:31,610 --> 00:18:35,540
Speaker 6:  Yeah, I think to me the the difference and I think the part we still have

339
00:18:35,560 --> 00:18:39,500
Speaker 6:  not figured out is the scale of it all. Cuz I think one of the things

340
00:18:39,500 --> 00:18:42,460
Speaker 6:  that James points out that I think is really true and we're just now starting

341
00:18:42,460 --> 00:18:46,060
Speaker 6:  to reckon with is like what you're saying is true Alex, but like

342
00:18:46,420 --> 00:18:50,060
Speaker 6:  a person can only build a website with so many

343
00:18:50,070 --> 00:18:53,860
Speaker 6:  words at such a speed. Whereas basically with, with

344
00:18:54,000 --> 00:18:57,940
Speaker 6:  enough compute and energy and interest, I could make

345
00:18:57,980 --> 00:19:01,620
Speaker 6:  a thing the size of the internet, like basically without trying very hard

346
00:19:02,040 --> 00:19:05,860
Speaker 6:  and that is just starting to happen. Like there was this news

347
00:19:05,860 --> 00:19:09,060
Speaker 6:  guard study this week that we also wrote about that found

348
00:19:09,920 --> 00:19:13,820
Speaker 6:  Ads for these big brands are starting to appear on AI generated

349
00:19:13,820 --> 00:19:16,820
Speaker 6:  spam sites like the one I mentioned, the ones I mentioned at the top, these

350
00:19:16,820 --> 00:19:20,540
Speaker 6:  like sort of real looking but ultimately

351
00:19:20,850 --> 00:19:24,140
Speaker 6:  totally artificial news sites. And these things are putting up

352
00:19:24,820 --> 00:19:28,380
Speaker 6:  thousands of articles a day. They're all generated, lots of them start with

353
00:19:28,380 --> 00:19:31,780
Speaker 6:  the phrase as an AI language model, which I find deeply hilarious.

354
00:19:32,280 --> 00:19:35,180
Speaker 6:  But it, it does seem like, like the thing we used to talk about a bunch of

355
00:19:35,180 --> 00:19:39,020
Speaker 6:  years ago where it was like, okay, how do we take all of these

356
00:19:39,020 --> 00:19:42,780
Speaker 6:  like societal divides and just throw Facebook scale at them,

357
00:19:42,780 --> 00:19:46,100
Speaker 6:  right? Like this thing where people fight about politics has been true forever,

358
00:19:46,360 --> 00:19:49,780
Speaker 6:  but now people can do it with billions of other people around.

359
00:19:50,410 --> 00:19:53,860
Speaker 6:  That feels like the kind of scale that's coming to this too, where it's like

360
00:19:54,050 --> 00:19:56,380
Speaker 6:  this stuff has been trending this direction anyway. I think you're totally

361
00:19:56,380 --> 00:20:00,260
Speaker 6:  right, but the the like there is about to be gasoline poured

362
00:20:00,260 --> 00:20:03,620
Speaker 6:  on it in a way that like we've never seen before. And to me that's the part

363
00:20:03,620 --> 00:20:06,700
Speaker 6:  where it's like, I don't know that anybody knows what happens when, like

364
00:20:07,120 --> 00:20:11,100
Speaker 6:  by the numbers, 97% of the internet has been generated by a large

365
00:20:11,260 --> 00:20:13,660
Speaker 6:  language model and like that is where we're headed. Well it's

366
00:20:13,660 --> 00:20:17,260
Speaker 7:  Kind of terrifying but cool in a historical perspective that like

367
00:20:17,860 --> 00:20:20,860
Speaker 7:  a lot of times you can look and you can kind of, you can kind of get an idea

368
00:20:20,860 --> 00:20:23,100
Speaker 7:  of what's gonna happen cuz you can look at history and you can be like, well

369
00:20:23,100 --> 00:20:25,580
Speaker 7:  this happened or this happened like when we talked about Twitter collapsing,

370
00:20:25,580 --> 00:20:29,100
Speaker 7:  we were like, we've seen this happen with other social media platforms, but

371
00:20:29,100 --> 00:20:32,820
Speaker 7:  we never had something like the internet before this, this moment. And, and

372
00:20:32,820 --> 00:20:36,660
Speaker 7:  we certainly never had something like this at this

373
00:20:36,660 --> 00:20:40,540
Speaker 7:  scale now happening in this space that it really is difficult

374
00:20:40,540 --> 00:20:43,500
Speaker 7:  to be like, well we were already struggling with this historical moment of

375
00:20:43,500 --> 00:20:47,360
Speaker 7:  like, how do people deal with misinformation at such

376
00:20:47,380 --> 00:20:50,260
Speaker 7:  a scale when the majority of people aren't trained to deal with misinformation

377
00:20:50,840 --> 00:20:53,900
Speaker 7:  and now it's just gonna get bigger and worse.

378
00:20:54,360 --> 00:20:57,900
Speaker 5:  So I think there are analogs, they're just challenging, right? So

379
00:20:58,050 --> 00:21:01,820
Speaker 5:  digital photography is an analog. There used to be far fewer photos

380
00:21:01,840 --> 00:21:05,580
Speaker 5:  in the world and then we handed everybody a digital camera and now

381
00:21:05,740 --> 00:21:09,660
Speaker 5:  there are billions upon billions of photos every hour. That's weird,

382
00:21:09,950 --> 00:21:13,580
Speaker 5:  right? But we weren't unhappy about that,

383
00:21:14,000 --> 00:21:14,540
Speaker 5:  but it did

384
00:21:14,540 --> 00:21:16,740
Speaker 6:  Completely change our relationship to our photos

385
00:21:17,160 --> 00:21:20,220
Speaker 5:  And to each other and to the world. Yeah, like all those, like what is The

386
00:21:20,300 --> 00:21:23,740
Speaker 5:  Verge about? But exactly that thing, right? Yeah. Like, but that thing, we

387
00:21:23,740 --> 00:21:27,420
Speaker 5:  gave people a tool, they started producing at a massive rate. The tool got

388
00:21:27,420 --> 00:21:30,700
Speaker 5:  infinitely cheaper to use the tool's now built into our phones,

389
00:21:31,170 --> 00:21:34,740
Speaker 5:  combining a phone in a cell network, in a distribution system in

390
00:21:34,740 --> 00:21:38,460
Speaker 5:  Instagram or whatever else. Like all that like radically changed our relationship

391
00:21:38,460 --> 00:21:41,780
Speaker 5:  to photography. What I'm saying is like the core difference

392
00:21:42,360 --> 00:21:45,660
Speaker 5:  is that every step of the way, the value that we were getting out of that

393
00:21:46,080 --> 00:21:49,940
Speaker 5:  was higher than the negatives. So I can start listing negatives of

394
00:21:49,940 --> 00:21:53,460
Speaker 5:  that left and right where everyone's at a concert and they're holding up

395
00:21:53,460 --> 00:21:57,300
Speaker 5:  their phones instead of watching the concert, the crisis in body image

396
00:21:57,320 --> 00:22:01,060
Speaker 5:  for Teen Girl, like you can just start listing the negatives, the

397
00:22:01,300 --> 00:22:04,940
Speaker 5:  positives just outweigh the negatives. And it's not that people aren't focused

398
00:22:04,940 --> 00:22:08,060
Speaker 5:  on the negatives, it's just, boy, it's a lot better to have more photos of

399
00:22:08,060 --> 00:22:11,460
Speaker 5:  your kid than not. Right? Like it's, it's like obvious here. It's like the

400
00:22:11,620 --> 00:22:14,620
Speaker 5:  positives don't outweigh the negatives and in fact the positives are hard

401
00:22:14,620 --> 00:22:15,300
Speaker 5:  to identify.

402
00:22:15,520 --> 00:22:19,380
Speaker 6:  Yes, that's what I was gonna say. Even if you think the other things were

403
00:22:20,000 --> 00:22:23,940
Speaker 6:  net bad and I think there are cases to be made in a lot of those things that

404
00:22:23,940 --> 00:22:27,220
Speaker 6:  some of them have been net bad, there at least were upsides. And I'm sitting

405
00:22:27,220 --> 00:22:30,780
Speaker 6:  here trying to figure out like what is the upside of millions of AI

406
00:22:30,780 --> 00:22:33,260
Speaker 6:  generated websites being on the internet and like I got nothing for

407
00:22:33,260 --> 00:22:33,940
Speaker 5:  Who are they for?

408
00:22:34,250 --> 00:22:34,540
Speaker 6:  Yeah,

409
00:22:35,240 --> 00:22:38,660
Speaker 5:  I'm actually kind of excited about the robot internet just because I, I,

410
00:22:38,780 --> 00:22:42,340
Speaker 5:  I think it will find itself a purpose and it's like, it's like robots being

411
00:22:42,340 --> 00:22:45,380
Speaker 5:  like a little too polite to each other and that's the internet you can visit.

412
00:22:46,080 --> 00:22:49,740
Speaker 7:  Here's the thing though, there's two sides of this, right? Like Google's

413
00:22:49,740 --> 00:22:53,500
Speaker 7:  existential threat and all of these people making these robot, this robot

414
00:22:53,780 --> 00:22:57,580
Speaker 7:  internet and they're making the robot internet to get clicks on their

415
00:22:57,580 --> 00:23:01,460
Speaker 7:  fake stories to get but from who they're doing that. But at the

416
00:23:01,460 --> 00:23:05,420
Speaker 7:  other side of that is someone who bought that ad and they probably didn't

417
00:23:05,420 --> 00:23:08,100
Speaker 7:  buy that ad wanting it to be on a fake website.

418
00:23:08,280 --> 00:23:11,100
Speaker 6:  And I'm betting the people who clicked onto that website did not want to

419
00:23:11,100 --> 00:23:11,780
Speaker 6:  click onto that

420
00:23:12,340 --> 00:23:15,540
Speaker 7:  Specifically website. Yeah. Like I feel like the ad exchanges and stuff are

421
00:23:15,540 --> 00:23:17,340
Speaker 7:  for once gonna be the good

422
00:23:17,600 --> 00:23:20,660
Speaker 5:  Guy. I don't want to talk about ad tech on the podcast. Like of all the things

423
00:23:20,800 --> 00:23:21,780
Speaker 5:  I'm not prepared to talk about,

424
00:23:21,800 --> 00:23:23,260
Speaker 6:  You just wanna talk about copyright law,

425
00:23:24,280 --> 00:23:27,420
Speaker 5:  Do it. No, I feel like I talked about ad tech like two weeks ago and it's

426
00:23:27,420 --> 00:23:30,860
Speaker 5:  like we're done. You did, like the ad tech world is rife with fraud

427
00:23:31,520 --> 00:23:34,940
Speaker 5:  anyway. Okay. Like it's just like constantly full of fraud. Like Google's

428
00:23:34,940 --> 00:23:38,700
Speaker 5:  in big trouble this week because they placed video Ads

429
00:23:38,800 --> 00:23:41,380
Speaker 5:  across the internet in places they weren't supposed to. And the advertiser

430
00:23:41,380 --> 00:23:44,700
Speaker 5:  all mad. And Google has say like, it's extraordinarily boring, it's also

431
00:23:44,700 --> 00:23:47,740
Speaker 5:  like the deepest crisis for Google they can add because their whole business

432
00:23:47,740 --> 00:23:51,420
Speaker 5:  is Ads. But like once the bottom drops out of that

433
00:23:52,160 --> 00:23:56,020
Speaker 5:  and like the fraud isn't just something you have to detect. It's like

434
00:23:56,020 --> 00:23:58,900
Speaker 5:  you're looking at this, you're like, okay, so a robot clicked on my ad to

435
00:23:58,900 --> 00:24:02,660
Speaker 5:  go to a page written by robots, did

436
00:24:02,920 --> 00:24:06,820
Speaker 5:  the robots buy my product at the end? Like there's

437
00:24:06,820 --> 00:24:10,700
Speaker 5:  just gonna be a reckoning there that I think will one be kind of funny

438
00:24:10,760 --> 00:24:14,660
Speaker 5:  in a way. Like yes, horrible fraudulent system exposed to be

439
00:24:14,780 --> 00:24:18,500
Speaker 5:  horrible, fraudulent system by over after it was overtaken by robots. Like

440
00:24:18,500 --> 00:24:19,260
Speaker 5:  that's just funny.

441
00:24:21,170 --> 00:24:21,460
Speaker 6:  Like

442
00:24:21,920 --> 00:24:22,460
Speaker 7:  I'm very excited,

443
00:24:22,530 --> 00:24:25,700
Speaker 5:  Like I'm excited for Liz to write all of those articles when the time comes.

444
00:24:26,160 --> 00:24:30,140
Speaker 5:  But at the other side of it is, okay, well if you just increase the supply

445
00:24:30,140 --> 00:24:33,700
Speaker 5:  of garbage, you're gonna decrease the supply of good stuff from people and

446
00:24:33,700 --> 00:24:37,500
Speaker 5:  that stuff should become more valuable, right? And that is where I don't

447
00:24:37,500 --> 00:24:41,260
Speaker 5:  think we've figured out like where do the people go, right? How do they talk

448
00:24:41,260 --> 00:24:43,500
Speaker 5:  to each other and how do you make sure that you're talking to other people

449
00:24:44,080 --> 00:24:47,770
Speaker 5:  and not people armed with extremely convincing tweet generators?

450
00:24:48,470 --> 00:24:51,290
Speaker 5:  And that's when I, when I say there's something happening on the social,

451
00:24:51,290 --> 00:24:53,890
Speaker 5:  like that's gonna be the turn. And I think that's gonna be super interesting.

452
00:24:54,120 --> 00:24:58,010
Speaker 6:  Yeah, I mean like I, I do think we are barreling into the where

453
00:24:58,010 --> 00:25:00,690
Speaker 6:  do people go to find people phase. Like that's actually a really good way

454
00:25:00,690 --> 00:25:04,330
Speaker 6:  of putting it. And I think Google rolled out its perspectives thing

455
00:25:04,550 --> 00:25:07,850
Speaker 6:  now, which is just in as many words

456
00:25:08,680 --> 00:25:12,050
Speaker 6:  here is the non robot internet, right? Yeah. Like it looks through

457
00:25:12,590 --> 00:25:16,410
Speaker 6:  TikTok, it looks through Reddit, it looks through blogs, it's like hunting

458
00:25:16,510 --> 00:25:20,290
Speaker 6:  for things made by people. For other people it kind of turns

459
00:25:20,290 --> 00:25:23,130
Speaker 6:  Google search into Pinterest, which I think looks a little weird and it's

460
00:25:23,290 --> 00:25:26,370
Speaker 6:  a wacky sort of information retrieval system, but it's a really interesting

461
00:25:26,370 --> 00:25:30,330
Speaker 6:  idea. And like Reddit is, I don't know, imploding. And so the

462
00:25:30,450 --> 00:25:33,090
Speaker 6:  question of like, where do those people go? Twitter is a mess. Like where

463
00:25:33,230 --> 00:25:37,130
Speaker 6:  do people go to find people is I think the question that comes

464
00:25:37,130 --> 00:25:40,410
Speaker 6:  out of this. But the other part I think is interesting is in the same way

465
00:25:40,410 --> 00:25:43,730
Speaker 6:  that I can't think of any positives, I can't think of anyone

466
00:25:44,390 --> 00:25:48,130
Speaker 6:  who wants this to keep happening, like all the way up and down the stack,

467
00:25:48,130 --> 00:25:51,690
Speaker 6:  right? Like the ad providers don't want their stuff going on fake websites.

468
00:25:52,070 --> 00:25:55,410
Speaker 6:  Google doesn't want to index fake websites because it's a lot of work and

469
00:25:55,410 --> 00:25:58,970
Speaker 6:  degrades the quality of Google, Wikipedia and Reddit and then don't wanna

470
00:25:58,990 --> 00:26:02,970
Speaker 6:  be part of the data that turns into those fake websites. Open AI doesn't

471
00:26:02,970 --> 00:26:05,330
Speaker 6:  wanna do that because all that that ends up back in the training data and

472
00:26:05,330 --> 00:26:07,930
Speaker 6:  screws it up. And so we're in this position of like, no one wants this to

473
00:26:07,930 --> 00:26:10,610
Speaker 6:  happen, but it seems completely inevitable that is going to keep happening

474
00:26:10,670 --> 00:26:14,010
Speaker 6:  and be basically unstoppable. Yeah, that robots create the internet.

475
00:26:14,550 --> 00:26:18,330
Speaker 7:  But like people made guns, that was a

476
00:26:18,390 --> 00:26:22,010
Speaker 7:  big technological thing. True. And presumably they

477
00:26:22,270 --> 00:26:26,010
Speaker 7:  didn't want to use them to just create havoc,

478
00:26:26,710 --> 00:26:27,650
Speaker 7:  but that's still what happened.

479
00:26:27,830 --> 00:26:28,530
Speaker 6:  No, here we are.

480
00:26:28,960 --> 00:26:32,690
Speaker 5:  Well not for a lack of attempts to regulate the guns too. Like

481
00:26:32,880 --> 00:26:36,290
Speaker 7:  Yeah, there's a lot of different like technologies where we go and we develop

482
00:26:36,290 --> 00:26:39,570
Speaker 7:  these technologies and we're like, this is really cool. And then one, you

483
00:26:39,570 --> 00:26:43,290
Speaker 7:  just need like one person to be like, I'm gonna be a dick. Yeah. And then

484
00:26:43,290 --> 00:26:45,330
Speaker 7:  what happens to that technology? How like

485
00:26:45,730 --> 00:26:49,610
Speaker 5:  I feel like we're like, we're so close to the part of a video game that's

486
00:26:49,610 --> 00:26:53,330
Speaker 5:  like AI is a weapon baby. It's happening, but the greatest weapon is love

487
00:26:53,330 --> 00:26:53,570
Speaker 5:  Alex.

488
00:26:54,510 --> 00:26:57,170
Speaker 7:  Aw. I'm excited for the soundtrack for this movie.

489
00:26:57,280 --> 00:27:01,170
Speaker 5:  When I say I'm 0% prepared, what I meant was I was only

490
00:27:01,410 --> 00:27:02,930
Speaker 5:  prepared to say the greatest weapon is love.

491
00:27:02,930 --> 00:27:05,970
Speaker 6:  You've just been waiting for it. You hit it at almost exactly the 30 minute

492
00:27:06,000 --> 00:27:09,650
Speaker 6:  mark too. So congratulations on that note. We should take a break and then

493
00:27:09,670 --> 00:27:12,330
Speaker 6:  before we lose Neli, we're gonna come back and talk about some gadgets and

494
00:27:12,330 --> 00:27:13,610
Speaker 6:  do a landing round. We'll right back.

495
00:27:18,630 --> 00:27:22,520
Speaker 9:  This episode is brought to you by Shopify. That's the

496
00:27:22,520 --> 00:27:26,040
Speaker 9:  sound of switching your business to Shopify. The global commerce platform

497
00:27:26,040 --> 00:27:29,720
Speaker 9:  that supercharges your selling harness, the best converting checkout, and

498
00:27:29,720 --> 00:27:33,600
Speaker 9:  same intuitive features, trusted apps and powerful analytics used

499
00:27:33,600 --> 00:27:37,080
Speaker 9:  by the world's leading brands. Stop leaving sales on the table.

500
00:27:37,680 --> 00:27:41,520
Speaker 9:  Discover why millions trust Shopify to build, grow, and run their

501
00:27:41,720 --> 00:27:44,960
Speaker 9:  business. Sign up today for your $1 per month trial period at

502
00:27:44,960 --> 00:27:47,160
Speaker 9:  shopify.com/tech 23

503
00:27:47,790 --> 00:27:51,760
Speaker 5:  This September, the code conference enters a new era. Join me,

504
00:27:51,930 --> 00:27:55,320
Speaker 5:  Nilay Patel, editor-in-chief of The Verge and my two co-hosts,

505
00:27:55,920 --> 00:27:59,800
Speaker 5:  platformers Casey Newton and CNBC's Julia Borson is we talk to the

506
00:27:59,800 --> 00:28:03,600
Speaker 5:  leaders making headlines in tech, business, politics, and the

507
00:28:03,600 --> 00:28:07,520
Speaker 5:  messy collision between them all. We'll dig deep into the topics that

508
00:28:07,520 --> 00:28:11,280
Speaker 5:  are shaping our world, like AI streaming and the future of social

509
00:28:11,280 --> 00:28:15,160
Speaker 5:  media. And as always, with code, no one will be safe from the hot seat.

510
00:28:15,700 --> 00:28:19,600
Speaker 5:  You can apply to attend the code conference@voxmedia.com

511
00:28:19,690 --> 00:28:23,560
Speaker 5:  slash code. That's vox media.com/code.

512
00:28:27,810 --> 00:28:30,800
Speaker 6:  We're back. Let's talk about a couple of Apple things and then we're getting

513
00:28:30,800 --> 00:28:33,800
Speaker 6:  get into a little bit of Lightning round. The first one is the only story

514
00:28:33,800 --> 00:28:36,640
Speaker 6:  on the website. I'm confident you read this week Eli, so you get to tell

515
00:28:36,640 --> 00:28:40,040
Speaker 6:  us about it. Monica Chin wrote sort of a review,

516
00:28:40,470 --> 00:28:44,440
Speaker 6:  sort of a deep reported piece, a really great piece about the new Mac

517
00:28:44,460 --> 00:28:45,640
Speaker 6:  Pro. Tell us about it, Eli.

518
00:28:45,870 --> 00:28:49,720
Speaker 5:  Yeah, so we assigned Monica the thankless job of reviewing the Mac Pro,

519
00:28:49,970 --> 00:28:53,640
Speaker 5:  which comes down to we need you to review these

520
00:28:53,820 --> 00:28:57,680
Speaker 5:  PCI slots because we also had the new Mac studio with the

521
00:28:57,840 --> 00:29:01,520
Speaker 5:  M two alternate. So we're gonna, you know, benchmark the computers, the

522
00:29:01,520 --> 00:29:04,520
Speaker 5:  benchmarks are the same between the two computers. So the challenge is like,

523
00:29:04,520 --> 00:29:08,360
Speaker 5:  how do you review the Mac Pro? And the only thing this different is

524
00:29:08,960 --> 00:29:11,760
Speaker 5:  a, a case that can support these slots. And

525
00:29:12,640 --> 00:29:16,520
Speaker 5:  candidly, we have no applications for the slots at

526
00:29:16,520 --> 00:29:20,320
Speaker 5:  The Verge, it's just not, not a thing. So I kept going

527
00:29:20,320 --> 00:29:23,240
Speaker 5:  into the office and seeing Monica and she'd be like, have you figured out

528
00:29:23,240 --> 00:29:24,920
Speaker 5:  what to do with these slots? And be like, I don't know, have you figured

529
00:29:24,920 --> 00:29:28,200
Speaker 5:  out what to do these slots? And so she landed on this great idea, which we,

530
00:29:28,260 --> 00:29:31,760
Speaker 5:  we often try to do in our pro machine reviews. We're gonna go talk to actual

531
00:29:31,760 --> 00:29:35,160
Speaker 5:  professionals, put the machine in front of 'em, see what they think, and

532
00:29:35,160 --> 00:29:39,150
Speaker 5:  usually that works fine, right? We can like put, we can drop it in a

533
00:29:39,270 --> 00:29:43,140
Speaker 5:  workflow, people use it. And the weirdest thing happened in this review.

534
00:29:43,140 --> 00:29:46,580
Speaker 5:  Monica went and talked to 20 people, you know, WhatsApp developers,

535
00:29:47,000 --> 00:29:49,940
Speaker 5:  VFX people down the line and they're all like, yeah, I don't need this computer

536
00:29:50,370 --> 00:29:54,300
Speaker 5:  because I have a MacBook Pro, which is incredible to

537
00:29:54,300 --> 00:29:58,100
Speaker 5:  think about. And really what they're saying is, the MacBook Pro is fine and

538
00:29:58,100 --> 00:30:01,820
Speaker 5:  fast enough for most of what I need is they, they usually have M one Max

539
00:30:01,830 --> 00:30:05,580
Speaker 5:  chips in their Mac pros. It's fast enough for most things I need to do.

540
00:30:06,040 --> 00:30:09,660
Speaker 5:  And when I need more, what I either need is a

541
00:30:09,700 --> 00:30:13,660
Speaker 5:  discrete gpu, I need an Nvidia gpu, which the Mac Pro does

542
00:30:13,660 --> 00:30:17,060
Speaker 5:  not support, or I can just go to the cloud

543
00:30:17,840 --> 00:30:21,820
Speaker 5:  or I can just use my company's cloud services. So why do I need this thing

544
00:30:21,820 --> 00:30:25,420
Speaker 5:  at my desk? And she couldn't find anyone. And actually one of the funniest

545
00:30:25,430 --> 00:30:28,020
Speaker 5:  parts in the middle of the interview, I went, talked to her and I said, how's

546
00:30:28,020 --> 00:30:31,500
Speaker 5:  it going? She goes, everyone I talk to says they don't need this machine.

547
00:30:31,520 --> 00:30:35,420
Speaker 5:  And then they imagine someone who might, and then I go talk to

548
00:30:35,660 --> 00:30:38,540
Speaker 5:  a person in that profession and they imagine yet another person.

549
00:30:39,560 --> 00:30:43,260
Speaker 5:  And so she just, you should go read the piece. It's great. Lots of quotes,

550
00:30:43,770 --> 00:30:47,540
Speaker 5:  lots of very specific things, but that cycle is just really interesting

551
00:30:47,540 --> 00:30:51,460
Speaker 5:  because it's, you don't even need the slots, right? You can put most PCI

552
00:30:51,460 --> 00:30:55,220
Speaker 5:  cards in an external thunderbolt enclosure, which works just find the Mac

553
00:30:55,220 --> 00:30:59,180
Speaker 5:  studio. So you need the specific Ultra high speed

554
00:30:59,680 --> 00:31:03,420
Speaker 5:  of the internal slots, which only a tiny handful of cards actually need.

555
00:31:03,680 --> 00:31:07,660
Speaker 5:  And most of those are storage. And even in that case, you might end up in

556
00:31:07,660 --> 00:31:07,940
Speaker 5:  the cloud.

557
00:31:08,420 --> 00:31:11,660
Speaker 7:  I was surprised that no discrete GPUs

558
00:31:12,410 --> 00:31:16,340
Speaker 7:  work with a back pro. Like I I knew Nvidia hasn't worked with it for, for

559
00:31:16,340 --> 00:31:19,980
Speaker 7:  years now, but I I I thought like, you know, you can still use an AMD

560
00:31:20,330 --> 00:31:24,300
Speaker 7:  card with a lot of the older back pros that run Intel and you

561
00:31:24,300 --> 00:31:24,900
Speaker 7:  just, it's done.

562
00:31:24,970 --> 00:31:28,580
Speaker 5:  Yeah. So Apple's argument has been since the M series

563
00:31:28,710 --> 00:31:32,700
Speaker 5:  chips came out is that unified memory is actually the heart of the

564
00:31:32,700 --> 00:31:36,580
Speaker 5:  techno technological bet. So Emissary chips are really fast and we all

565
00:31:36,580 --> 00:31:39,900
Speaker 5:  think of them as like Apple move the Mac to arm and they're, they're building

566
00:31:39,920 --> 00:31:43,860
Speaker 5:  all their own custom chips and the price and performance is great and I

567
00:31:43,860 --> 00:31:47,540
Speaker 5:  think lost in the story, but Apple never forgets is a huge

568
00:31:47,960 --> 00:31:51,940
Speaker 5:  bet that they have made is unified memory. And the, the

569
00:31:51,940 --> 00:31:55,620
Speaker 5:  way they describe it is the GPU

570
00:31:56,330 --> 00:32:00,260
Speaker 5:  finally has a lot of memory, as much memory as a cpu,

571
00:32:00,600 --> 00:32:04,380
Speaker 5:  and the CPU has memory that's as fast as a gpu. And you're like, that's great.

572
00:32:04,720 --> 00:32:08,660
Speaker 5:  And then you're like, where's my Nvidia card? Like, and the Nvidia cards

573
00:32:08,760 --> 00:32:12,580
Speaker 5:  for a lot of industries are still blow away these gpu, these M series

574
00:32:12,810 --> 00:32:13,100
Speaker 5:  GPUs.

575
00:32:13,440 --> 00:32:17,220
Speaker 6:  But what's interesting to me is I, I think what Apple said from

576
00:32:17,240 --> 00:32:21,060
Speaker 6:  the get go basically is that this Mac Pro exists for

577
00:32:21,060 --> 00:32:24,500
Speaker 6:  people who have systems into which you need to drop this, right? They said

578
00:32:24,500 --> 00:32:27,260
Speaker 6:  like there are people out there who have like hundreds of thousands of dollars

579
00:32:27,260 --> 00:32:31,180
Speaker 6:  worth of existing gear that they don't want to throw away or buy

580
00:32:31,180 --> 00:32:34,860
Speaker 6:  enclosures or dongles for to have this thing. And what is

581
00:32:35,050 --> 00:32:38,660
Speaker 6:  seeming to be true is that that number of people is

582
00:32:39,210 --> 00:32:42,980
Speaker 6:  much smaller than we anticipated. And Apple's argument is

583
00:32:42,980 --> 00:32:46,660
Speaker 6:  also that the number of people who think they need a huge mega fast

584
00:32:46,660 --> 00:32:49,660
Speaker 6:  discreet GPU is actually much smaller than the people who really do need

585
00:32:49,660 --> 00:32:53,220
Speaker 6:  it. And I think we can debate whether or not that's actually the case.

586
00:32:53,640 --> 00:32:57,500
Speaker 6:  But the thing that just kept jumping out to me about Monica's piece is basically

587
00:32:57,500 --> 00:33:01,260
Speaker 6:  like people are good. Like people have discovered that in most professions

588
00:33:01,360 --> 00:33:05,100
Speaker 6:  we hit the actual power threshold that they need to do the things that they

589
00:33:05,100 --> 00:33:06,060
Speaker 6:  need to do day to day.

590
00:33:06,540 --> 00:33:07,420
Speaker 5:  A while ago, well

591
00:33:07,920 --> 00:33:11,900
Speaker 7:  See this, this is, this is where I I I disagree mainly

592
00:33:11,900 --> 00:33:15,260
Speaker 7:  with, with you, but also with Apple. Because

593
00:33:15,690 --> 00:33:19,140
Speaker 7:  this idea that Apple is like, oh, you know, people just wanted the PCI

594
00:33:19,390 --> 00:33:22,980
Speaker 7:  slots so they could plug their stuff in. That's not true. The reason people

595
00:33:22,980 --> 00:33:26,740
Speaker 7:  wanted PCI slots for years was because they wanted some form of

596
00:33:26,740 --> 00:33:30,660
Speaker 7:  upgradeability for the Mac Pro. If they were gonna spend $6,000 on

597
00:33:30,660 --> 00:33:34,340
Speaker 7:  a device, they wanted a bunch of it to be able to be upgraded. And Apple

598
00:33:34,440 --> 00:33:38,300
Speaker 7:  has systematically gotten rid of a lot of those ability to upgrade

599
00:33:38,300 --> 00:33:38,940
Speaker 7:  it. Yeah,

600
00:33:38,940 --> 00:33:40,740
Speaker 6:  You can't even upgrade the RAM anymore. Yeah,

601
00:33:40,740 --> 00:33:43,500
Speaker 7:  You can't upgrade anything in it. So it's like, well what's the, the pro

602
00:33:43,500 --> 00:33:47,300
Speaker 7:  doesn't need to exist. Like you're just cl like this is just clowning us

603
00:33:47,550 --> 00:33:51,340
Speaker 7:  being like, yeah, you really wanted them P C I slats, huh? What are you gonna

604
00:33:51,340 --> 00:33:55,020
Speaker 7:  upgrade now? Nothing. Maybe store

605
00:33:55,400 --> 00:33:55,620
Speaker 7:  it.

606
00:33:55,920 --> 00:33:59,660
Speaker 5:  So one industry that Monica did go and talk to that everyone guessed

607
00:33:59,930 --> 00:34:03,180
Speaker 5:  that they would would love this thing is visual effects. VFX

608
00:34:03,880 --> 00:34:07,780
Speaker 5:  and no VFX companies would talk to her about it because

609
00:34:07,930 --> 00:34:11,380
Speaker 5:  they were like, we're, we've all been all all in on Windows for years. We

610
00:34:11,380 --> 00:34:14,780
Speaker 5:  don't know anything about Max. And the reason is exactly what you're talking

611
00:34:14,780 --> 00:34:18,340
Speaker 5:  about Alex, they treat their Windows PCs like

612
00:34:18,690 --> 00:34:22,580
Speaker 5:  collections of parts. And so when one goes down they can just like swap

613
00:34:22,580 --> 00:34:25,500
Speaker 5:  in some parts and like get back up and running and like they're very modular.

614
00:34:25,770 --> 00:34:29,540
Speaker 5:  They are probably loud and noisy, you know, like all the stuff that the Mac

615
00:34:29,540 --> 00:34:32,340
Speaker 5:  Pro is not, but it's like the power supply's out, they're just gonna yank

616
00:34:32,340 --> 00:34:35,500
Speaker 5:  it and put a new power supply and keep going and they're gonna upgrade over

617
00:34:35,500 --> 00:34:38,700
Speaker 5:  time, all the pieces. So you, you basically have a, like a ship of thesis

618
00:34:39,220 --> 00:34:42,620
Speaker 5:  situation where like, what is this PC over time? And all the pieces are getting

619
00:34:42,860 --> 00:34:46,340
Speaker 5:  replaced but the studios are running and they've made the investments. And

620
00:34:46,340 --> 00:34:50,060
Speaker 5:  then the other piece of the puzzle, which I think is really fascinating is

621
00:34:50,060 --> 00:34:53,900
Speaker 5:  because Apple walked away from the market for so long, they

622
00:34:53,900 --> 00:34:56,620
Speaker 5:  have another problem that I think you're hinting at, which is no one trusts

623
00:34:56,620 --> 00:35:00,540
Speaker 5:  them to stick around. And so like Final Cut Pro, which

624
00:35:00,770 --> 00:35:04,540
Speaker 5:  they changed Final Cut Pro to Final Cut Pro 10 like 10 years ago, like

625
00:35:04,690 --> 00:35:08,660
Speaker 5:  this is ancient history that they blew up Final Cut Pro Seven

626
00:35:09,000 --> 00:35:12,860
Speaker 5:  and went with this weird iMovie interface, but they've since like dramatically

627
00:35:12,860 --> 00:35:13,340
Speaker 5:  upgraded,

628
00:35:13,660 --> 00:35:14,900
Speaker 7:  Apparently a lot of people use it now,

629
00:35:15,090 --> 00:35:18,380
Speaker 5:  It's been a decade, but they're still like open letters in this community

630
00:35:18,380 --> 00:35:21,300
Speaker 5:  about Final Cut Pro and what you need to do to make it useful. And people

631
00:35:21,300 --> 00:35:25,100
Speaker 5:  just don't trust this company to service this market. And that

632
00:35:25,280 --> 00:35:29,020
Speaker 5:  is really tough when the computer has to make you your money

633
00:35:29,370 --> 00:35:33,260
Speaker 5:  like in a very real meaningful way. And so I, I think there's just

634
00:35:33,300 --> 00:35:36,700
Speaker 5:  a gap here with the Mac Pro that quite frankly when, when, you know, when

635
00:35:36,700 --> 00:35:39,580
Speaker 5:  we are at the event and we're looking at it and they're showing us all the,

636
00:35:40,040 --> 00:35:43,100
Speaker 5:  you know, 90,000 streams of audio

637
00:35:43,840 --> 00:35:47,380
Speaker 5:  in Avatar two. Actually the amount of times we've talked about James Cameron

638
00:35:47,440 --> 00:35:51,100
Speaker 5:  in the past month has been like shockingly high for good. Keeps coming out

639
00:35:51,100 --> 00:35:52,460
Speaker 5:  reasons and really bad reasons.

640
00:35:52,960 --> 00:35:56,220
Speaker 6:  The world's foremost power user of computers apparently Jane's camera.

641
00:35:56,800 --> 00:36:00,300
Speaker 5:  But like all those demos are great and then you walk away and like actual

642
00:36:00,370 --> 00:36:04,020
Speaker 5:  pros, like I don't, I can just buy a Thunderball enclosure if I need these

643
00:36:04,070 --> 00:36:08,060
Speaker 5:  slots and actually my computer is a tool and I need to be able

644
00:36:08,060 --> 00:36:11,980
Speaker 5:  to treat it like a tool and I need the tool vendor to be there,

645
00:36:12,480 --> 00:36:15,660
Speaker 5:  not ship this computer and potentially walk away for another six years. And

646
00:36:15,660 --> 00:36:16,140
Speaker 5:  that's a hard

647
00:36:16,400 --> 00:36:19,580
Speaker 6:  Was the trash can the end of that like 2013,

648
00:36:20,210 --> 00:36:21,500
Speaker 6:  that big redesign?

649
00:36:21,730 --> 00:36:25,660
Speaker 7:  Yeah. Cuz the trash can wasn't super upgradeable because it was such a small

650
00:36:26,130 --> 00:36:28,940
Speaker 7:  chassis, so you couldn't do a lot with it. There's a bunch of stuff that

651
00:36:28,940 --> 00:36:30,820
Speaker 7:  didn't fit. You had to have custom cards and stuff

652
00:36:31,280 --> 00:36:35,020
Speaker 6:  And it didn't fit into racks. It didn't, it didn't like fit people's workflows.

653
00:36:35,020 --> 00:36:38,980
Speaker 6:  It was, that was the moment I think a lot of people went, oh, not

654
00:36:38,980 --> 00:36:42,900
Speaker 6:  only do I not want this computer, I don't think Apple understands what I

655
00:36:42,900 --> 00:36:46,100
Speaker 6:  need anymore. And then there was the big redesign with Cheese Grater where

656
00:36:46,100 --> 00:36:49,740
Speaker 6:  they were like, we're back. This is a love letter to developers. And I, it

657
00:36:49,740 --> 00:36:52,860
Speaker 6:  it just doesn't seem like some of those people came all the way back

658
00:36:53,040 --> 00:36:56,420
Speaker 7:  And even then it wasn't really like it was a all flatter maybe to developers,

659
00:36:56,560 --> 00:36:59,540
Speaker 7:  but, but, but Nilay is right. It was like, there's a whole group of people

660
00:36:59,540 --> 00:37:02,860
Speaker 7:  that were really into Max for a very long time. One of my first jobs outta

661
00:37:02,860 --> 00:37:06,700
Speaker 7:  college was working in like a, a video editing studio and they

662
00:37:06,700 --> 00:37:10,660
Speaker 7:  were all running, this was Mac o os 10 had been out for years at this

663
00:37:10,660 --> 00:37:13,860
Speaker 7:  point and they were all still running nine point like seven or something

664
00:37:14,490 --> 00:37:17,940
Speaker 7:  because it was so reliable and that was what they, they needed

665
00:37:18,050 --> 00:37:21,060
Speaker 7:  reliability, they needed, they needed to just know things were gonna work

666
00:37:21,060 --> 00:37:24,180
Speaker 7:  and they weren't gonna have to be troubleshooting. And Apple's moved away

667
00:37:24,180 --> 00:37:27,220
Speaker 7:  from that. Apple's like, yeah, we're gonna go really fast, but sometimes

668
00:37:27,220 --> 00:37:30,140
Speaker 7:  you're continuing camera won't work and so you'll have to use your garbage

669
00:37:30,500 --> 00:37:33,380
Speaker 7:  Opal C one to record the podcast with your coworkers.

670
00:37:33,860 --> 00:37:35,180
Speaker 5:  I mean I still want a Mac Pro.

671
00:37:35,450 --> 00:37:39,260
Speaker 7:  Yeah, hundred percent. No, actually I don't want one because I

672
00:37:39,260 --> 00:37:42,900
Speaker 7:  can't upgrade it like I used to. I was the person who was upgrading her G

673
00:37:42,900 --> 00:37:46,580
Speaker 7:  three Power book Pismo to a G4 processor.

674
00:37:46,650 --> 00:37:47,900
Speaker 5:  This is why we vibe Alex.

675
00:37:48,360 --> 00:37:52,340
Speaker 7:  It was not great, but it was cool that I could do it. And

676
00:37:52,340 --> 00:37:56,260
Speaker 7:  then I ended up going into Windows because I, I still, I still got that,

677
00:37:56,290 --> 00:37:59,580
Speaker 7:  that feeling and, but I like, oh, Mac os better

678
00:38:00,200 --> 00:38:04,060
Speaker 7:  and you just, you've lo like we've lost that. But between that and the fact

679
00:38:04,060 --> 00:38:07,940
Speaker 7:  that you can't do a hack OSH anymore, it's kind of

680
00:38:07,940 --> 00:38:11,500
Speaker 7:  boring from a gadget perspective in a lot of ways. They're very pretty but

681
00:38:11,500 --> 00:38:11,980
Speaker 7:  they're not like,

682
00:38:12,330 --> 00:38:15,540
Speaker 5:  Yeah, I have no idea why we're on again. I've been running this podcast

683
00:38:16,130 --> 00:38:19,700
Speaker 5:  from home on a 2015 iMac in it shows

684
00:38:19,700 --> 00:38:19,980
Speaker 6:  Fine

685
00:38:21,650 --> 00:38:23,100
Speaker 7:  Screaming at you right now.

686
00:38:23,370 --> 00:38:24,380
Speaker 5:  It's not happy. No,

687
00:38:24,540 --> 00:38:27,060
Speaker 6:  But I, I think just the one last thing I understand, we should move on. I

688
00:38:27,060 --> 00:38:30,140
Speaker 6:  think the thing that was so surprising to me about this is I expected everybody

689
00:38:30,140 --> 00:38:33,220
Speaker 6:  to say, oh no, I bought a Mac studio last year and I'm very happy. And I

690
00:38:33,220 --> 00:38:36,100
Speaker 6:  think there are people out there like that, but the number of people, like

691
00:38:36,100 --> 00:38:39,380
Speaker 6:  you said right at the top Neli who said, actually no, I'm very happy with

692
00:38:39,400 --> 00:38:43,300
Speaker 6:  my Ultra powerful MacBook Pro I thought was really fascinating

693
00:38:43,300 --> 00:38:47,020
Speaker 6:  because it's like, we've talked for years about this balance of, of

694
00:38:47,190 --> 00:38:50,420
Speaker 6:  power and portability and it's like a thing Monica spends a lot of time thinking

695
00:38:50,420 --> 00:38:53,620
Speaker 6:  about, especially as she reviews like gaming laptops and all that kinda stuff

696
00:38:53,620 --> 00:38:57,460
Speaker 6:  on the window side and on this side, like it's pretty damn close to

697
00:38:57,460 --> 00:39:01,340
Speaker 6:  being the best of both worlds if you get a pretty souped up MacBook Pro

698
00:39:01,340 --> 00:39:05,260
Speaker 6:  at this point and it's just a powerful ass desktop when you

699
00:39:05,260 --> 00:39:08,980
Speaker 6:  put it on a table with a mouse and keyboard and then you can take it

700
00:39:08,980 --> 00:39:12,820
Speaker 6:  with you. And I think that is like, that's a pretty, pretty hard

701
00:39:12,820 --> 00:39:16,500
Speaker 6:  combination to beat, especially when you're at as high a power

702
00:39:16,740 --> 00:39:18,860
Speaker 6:  threshold as the MacBook Pro can be right now.

703
00:39:19,490 --> 00:39:22,860
Speaker 5:  Yeah, I I think it's, I have a different perspective on it cuz when we test

704
00:39:22,860 --> 00:39:26,660
Speaker 5:  them we are like waiting for the thermal throttling to kick in. Yeah.

705
00:39:26,720 --> 00:39:30,500
Speaker 5:  And so we're looking for it and then we see it and I think probably in

706
00:39:30,620 --> 00:39:33,940
Speaker 5:  a regular situation you're not actively looking for it. You're like, oh,

707
00:39:33,940 --> 00:39:37,740
Speaker 5:  this computer's just really fast. Yeah. But if you know that if you put that

708
00:39:37,900 --> 00:39:41,580
Speaker 5:  ship in a Mac studio enclosure with proper cooling, it's even faster.

709
00:39:42,120 --> 00:39:45,700
Speaker 5:  But like how you can't know it until you know it. Right? Like so I I

710
00:39:45,880 --> 00:39:49,380
Speaker 5:  that's the split that I would just like push on there. It's like, yep, the

711
00:39:49,650 --> 00:39:53,380
Speaker 5:  MacBook Pro, especially M one Max is really great, but a Max studio even

712
00:39:53,380 --> 00:39:57,340
Speaker 5:  faster. And I think the real story of the Mac Pro is they couldn't fit four

713
00:39:57,340 --> 00:40:01,220
Speaker 5:  of those chips together, right? So an M two Max

714
00:40:01,320 --> 00:40:05,100
Speaker 5:  is a single chip and M two Ultra is two of them and they were supposed

715
00:40:05,240 --> 00:40:09,100
Speaker 5:  to be one with four and I don't think Apple could pull it off.

716
00:40:09,560 --> 00:40:12,700
Speaker 5:  And I think that's actually part of the story is they needed the bigger case

717
00:40:12,700 --> 00:40:15,820
Speaker 5:  for the extra cooling for the mort and they just couldn't do it. And so like

718
00:40:15,820 --> 00:40:18,940
Speaker 5:  here we are with two computers, the same chip in them and almost identical

719
00:40:18,940 --> 00:40:20,740
Speaker 5:  performance. Okay.

720
00:40:21,570 --> 00:40:22,140
Speaker 6:  PCIs and,

721
00:40:22,320 --> 00:40:24,460
Speaker 5:  And $4,000 for the slot. No one needs

722
00:40:24,640 --> 00:40:25,820
Speaker 6:  And wheels don't forget the wheels, the

723
00:40:25,820 --> 00:40:26,220
Speaker 5:  Wheels cost

724
00:40:26,220 --> 00:40:29,940
Speaker 6:  Extra. I'm just saying the M two mega with four chips is just sitting right

725
00:40:29,940 --> 00:40:33,020
Speaker 6:  there. Apple, it's just, it's just right there. One more Apple thing and

726
00:40:33,020 --> 00:40:35,980
Speaker 6:  then we should get to the Lightning round. And I, I moved this out of the

727
00:40:35,980 --> 00:40:38,860
Speaker 6:  Lightning round because it is a great personal annoyance to me right now.

728
00:40:39,110 --> 00:40:43,020
Speaker 6:  Chris Welch wrote a very good story about how the AirPods Max are

729
00:40:43,020 --> 00:40:46,980
Speaker 6:  getting sort of it deprecated, for lack of a better word, because

730
00:40:46,980 --> 00:40:50,500
Speaker 6:  of Apple's new software. So there's a bunch of new stuff coming to

731
00:40:50,610 --> 00:40:54,380
Speaker 6:  AirPods, adaptive audio and personalized volume and

732
00:40:54,730 --> 00:40:57,500
Speaker 6:  lots of sort of machine learning stuff that these devices are starting to

733
00:40:57,500 --> 00:41:01,260
Speaker 6:  do. And most of them are not coming to the AirPods

734
00:41:01,280 --> 00:41:05,060
Speaker 6:  Max, which are $549 and still very much on sale and still

735
00:41:05,060 --> 00:41:08,860
Speaker 6:  very much apple's most expensive fanciest headphones. I just bought a pair

736
00:41:08,860 --> 00:41:12,820
Speaker 6:  of third generation AirPods for $130

737
00:41:12,880 --> 00:41:16,740
Speaker 6:  at Costco because they were on sale. So I feel like I have less reason to

738
00:41:16,740 --> 00:41:20,660
Speaker 6:  be annoyed by this, but I still think it's kind of annoying and I

739
00:41:20,660 --> 00:41:24,620
Speaker 6:  had just never thought of headphones as a gadget that will

740
00:41:24,680 --> 00:41:28,420
Speaker 6:  get slowly worse and lose features over time. But now somehow

741
00:41:28,420 --> 00:41:31,740
Speaker 6:  this is the world we live in. It's like a TV now. It basically is.

742
00:41:31,740 --> 00:41:34,540
Speaker 5:  Yeah, yeah. Well you want is to turn everything into a computer with a planned

743
00:41:34,540 --> 00:41:37,700
Speaker 5:  obsolescence cycle built into it. That's my family. Yeah. Can't wait. Can

744
00:41:37,700 --> 00:41:41,660
Speaker 5:  I just, I'm gonna tell you a story that's, I'm

745
00:41:41,660 --> 00:41:42,780
Speaker 5:  a professional podcaster.

746
00:41:45,090 --> 00:41:45,460
Speaker 5:  Tell us

747
00:41:45,460 --> 00:41:45,740
Speaker 6:  The story.

748
00:41:47,010 --> 00:41:49,580
Speaker 5:  Well, I just keep thinking about like, you know, we keep turning headphones

749
00:41:49,770 --> 00:41:53,460
Speaker 5:  into computers, we keep doing spatial audio, everyone knows I feel about

750
00:41:53,460 --> 00:41:57,020
Speaker 5:  spatial audio. I went to a very high end speaker

751
00:41:57,020 --> 00:42:00,620
Speaker 5:  company. It's up here in the mountains with me. It's called Aer

752
00:42:00,840 --> 00:42:04,420
Speaker 5:  Ara. It's run by Rob Kalin. He's the co-founder of Etsy. Left.

753
00:42:04,640 --> 00:42:08,180
Speaker 5:  He started a speaker company hiring NASA engineer. He's got all these ideas

754
00:42:08,390 --> 00:42:12,260
Speaker 5:  about using horns and like really, really high-end woodworking

755
00:42:12,260 --> 00:42:15,340
Speaker 5:  to enhance speakers. And he showed me some stuff that was incredible. He

756
00:42:15,680 --> 00:42:19,460
Speaker 5:  played me like a rocket launch that

757
00:42:19,460 --> 00:42:23,340
Speaker 5:  like shook me out of my body and just the way that he constructed the speakers,

758
00:42:23,800 --> 00:42:27,740
Speaker 5:  he was using like 11 inch drivers to do. It was crazy. And then he, I don't

759
00:42:27,740 --> 00:42:31,580
Speaker 5:  like Lana Delrey, but he sat me down in front of his speakers and he played

760
00:42:31,580 --> 00:42:35,500
Speaker 5:  me a Lana Delrey song and it was like an emotional experience. I was like,

761
00:42:35,920 --> 00:42:39,860
Speaker 5:  I'm like losing my mind. And he was playing it over Bluetooth from Spotify

762
00:42:40,800 --> 00:42:43,820
Speaker 5:  and I was like, what are you doing? He's like, everyone's so focused on this.

763
00:42:43,840 --> 00:42:47,580
Speaker 5:  We all just forgot that the speakers have to be good. And like, that's the

764
00:42:47,580 --> 00:42:51,460
Speaker 5:  thing that's like in my brain is like, we are so like lossless and spatial

765
00:42:51,480 --> 00:42:54,820
Speaker 5:  da da da. And it's like, what if we just made like really kick ass speakers?

766
00:42:55,440 --> 00:42:58,580
Speaker 5:  The speakers are insanely expensive. I'm like, no one can afford them. But

767
00:42:58,580 --> 00:43:02,340
Speaker 5:  it was just like, it just occurred to me like as I was reading the AirPods

768
00:43:02,340 --> 00:43:06,220
Speaker 5:  Mac story that like turning the headphones into computers

769
00:43:06,840 --> 00:43:10,540
Speaker 5:  and shoveling software features like adaptive audio and all this stuff

770
00:43:10,610 --> 00:43:14,580
Speaker 5:  into them is absolutely taking away from

771
00:43:14,580 --> 00:43:18,180
Speaker 5:  the experience of listening to the music. And like, again, I don't like La

772
00:43:18,290 --> 00:43:21,380
Speaker 5:  LoRa. And I was like, oh my God, I get it. And then I got in my car and I

773
00:43:21,380 --> 00:43:25,300
Speaker 5:  listen, I pulled up that song and I was like, I really just like, like I

774
00:43:25,300 --> 00:43:28,740
Speaker 5:  don't like any of this. But it was just like literally like

775
00:43:28,740 --> 00:43:32,380
Speaker 5:  listening to good music and stereo from Bluetooth on Spotify

776
00:43:33,000 --> 00:43:33,580
Speaker 5:  was incredible.

777
00:43:34,080 --> 00:43:37,740
Speaker 6:  But that's all well and good because you can sit in a room with

778
00:43:37,740 --> 00:43:41,700
Speaker 6:  speakers that have 11 inch drivers, like good luck strapping that to your

779
00:43:41,700 --> 00:43:43,740
Speaker 6:  head, right? Like, I don't know if you've seen AirPods,

780
00:43:43,740 --> 00:43:44,500
Speaker 7:  I would like to see that

781
00:43:44,730 --> 00:43:47,900
Speaker 5:  AirPods Max are pretty big. They're pretty big.

782
00:43:48,170 --> 00:43:49,380
Speaker 7:  They're chunky, but

783
00:43:49,380 --> 00:43:52,740
Speaker 6:  It's just like that's, that's where we are, right? Like in order to make

784
00:43:52,740 --> 00:43:55,820
Speaker 6:  them that small, you have to take a lot of that stuff and put it in software.

785
00:43:56,010 --> 00:43:59,980
Speaker 6:  Like some at some point speaker is just about moving air and

786
00:44:00,010 --> 00:44:03,580
Speaker 6:  physics. Yeah. And Apple has to cheat that with software

787
00:44:03,920 --> 00:44:07,900
Speaker 6:  and I'm sympathetic to that, but at some point it's like, we are

788
00:44:07,900 --> 00:44:10,500
Speaker 6:  gonna get to the point where my headphones get worse over time. And that

789
00:44:10,500 --> 00:44:13,500
Speaker 6:  feels wrong in the way that like if you bought those outrageously expensive

790
00:44:13,500 --> 00:44:17,260
Speaker 6:  speakers, those are gonna sound that kick ass for an

791
00:44:17,540 --> 00:44:21,020
Speaker 6:  unbelievably long time. And I don't feel like that's the case with my AirPods

792
00:44:21,020 --> 00:44:22,900
Speaker 6:  and that's the part that bums me out. Well,

793
00:44:22,900 --> 00:44:26,620
Speaker 7:  You know, like what the guys at He Phi and a lot of these other like audio

794
00:44:26,690 --> 00:44:30,620
Speaker 7:  file places would tell you just get better headphones.

795
00:44:31,650 --> 00:44:33,900
Speaker 7:  Like don't wear the AirPods. Yeah.

796
00:44:33,900 --> 00:44:37,620
Speaker 6:  But Apple won't let them successfully connect to my phone.

797
00:44:37,720 --> 00:44:41,460
Speaker 6:  And so every time I turn my head they lose connection. Like, fix that and

798
00:44:41,460 --> 00:44:43,020
Speaker 6:  I'll buy your better headphones. you

799
00:44:43,020 --> 00:44:45,500
Speaker 5:  Know, apple would happily tell you that they have the best blueys stack on

800
00:44:45,500 --> 00:44:46,140
Speaker 5:  the market, David.

801
00:44:46,450 --> 00:44:48,460
Speaker 6:  Yeah. Because only Apple's allowed to use it.

802
00:44:49,610 --> 00:44:51,420
Speaker 7:  It's true. You could just wire it in.

803
00:44:51,480 --> 00:44:54,420
Speaker 6:  Listen wired headphones are coming back and I'm, I'm very happy about it.

804
00:44:54,420 --> 00:44:57,860
Speaker 6:  I bought a pair of Wired EarPods the other day and do you know what is easier

805
00:44:58,050 --> 00:45:01,660
Speaker 6:  than dumb Bluetooth connections is just plugging the damn thing in. It's

806
00:45:01,660 --> 00:45:01,980
Speaker 7:  Beautiful.

807
00:45:02,200 --> 00:45:03,220
Speaker 5:  I'm gonna say nothing That

808
00:45:03,220 --> 00:45:03,780
Speaker 7:  Sounds so nice.

809
00:45:03,900 --> 00:45:07,820
Speaker 5:  I want you all to just imagine it's like a text expander. I

810
00:45:07,820 --> 00:45:09,620
Speaker 5:  was super right. Anyway, I'm done. Move on.

811
00:45:12,520 --> 00:45:15,620
Speaker 6:  We can just sit here quietly and just acknowledge what you would've said

812
00:45:15,620 --> 00:45:16,300
Speaker 6:  if you were prepared

813
00:45:16,300 --> 00:45:18,860
Speaker 5:  For this podcast. That's gonna be, we should do, we should have a third episode

814
00:45:18,860 --> 00:45:22,660
Speaker 5:  a week. It's just dead silence. And just imagine what we would've said

815
00:45:22,670 --> 00:45:26,020
Speaker 5:  about various things. It would probably be our most

816
00:45:26,210 --> 00:45:27,820
Speaker 5:  popular episode every week

817
00:45:28,300 --> 00:45:31,740
Speaker 6:  Somebody just quietly comes in and says, A new camera launched and that's

818
00:45:31,740 --> 00:45:33,100
Speaker 6:  just 65 minutes of silence.

819
00:45:34,620 --> 00:45:37,980
Speaker 6:  I love this idea. All right. We should do a little Lightning round and then

820
00:45:38,090 --> 00:45:40,260
Speaker 6:  neli, you're gonna get outta here. Crayons. You go first, what do you got?

821
00:45:40,260 --> 00:45:40,660
Speaker 6:  Mine's

822
00:45:40,660 --> 00:45:43,740
Speaker 7:  Kinda a bummer. Plex is laying off 20 percent of its staff.

823
00:45:44,440 --> 00:45:45,340
Speaker 6:  That's a lot.

824
00:45:45,920 --> 00:45:49,820
Speaker 7:  That's that's a lot for, for Plex. I I did take a moment of

825
00:45:49,820 --> 00:45:53,500
Speaker 7:  being like, was that one person? Yeah, it was 37 people. That's a lot.

826
00:45:53,840 --> 00:45:57,700
Speaker 7:  So yeah. Yeah. Plex Plex has gotten, has gotten pretty, pretty big.

827
00:45:57,880 --> 00:46:01,780
Speaker 7:  And they're saying it affects every department. They're continuing to bet

828
00:46:01,880 --> 00:46:05,580
Speaker 7:  big on on Fast tv. They're, they're one of the, the many

829
00:46:05,590 --> 00:46:09,420
Speaker 7:  folks that, that are super into this space right now, which is the free ad

830
00:46:09,420 --> 00:46:12,700
Speaker 7:  sport of television. I think they were one of the first movers in that space,

831
00:46:12,960 --> 00:46:15,460
Speaker 7:  but most of us didn't understand what it was and was just like, why is there

832
00:46:15,460 --> 00:46:18,140
Speaker 7:  as a bunch of free garbage on my Plex when I wanna watch,

833
00:46:19,230 --> 00:46:22,780
Speaker 7:  definitely legally acquired copies of James Cameron's the Abyss.

834
00:46:23,040 --> 00:46:26,060
Speaker 6:  Why are you showing me reruns of the Adams family 24 hours a day?

835
00:46:26,770 --> 00:46:30,540
Speaker 7:  Yeah. And, and so the, the stuff that was happening with Fast is actually

836
00:46:30,540 --> 00:46:34,420
Speaker 7:  partially responsible for this. Their, their ad business is down, their

837
00:46:34,420 --> 00:46:37,340
Speaker 7:  ad business is like dependent on that fast stuff. And so

838
00:46:38,170 --> 00:46:41,940
Speaker 7:  yeah. 20% of folks are, are getting laid off. Plex isn't going

839
00:46:42,180 --> 00:46:43,140
Speaker 7:  anywhere. It's sticking around.

840
00:46:43,460 --> 00:46:46,140
Speaker 6:  Plex is basically a streaming service now. Right. Like that was the thing

841
00:46:46,140 --> 00:46:48,740
Speaker 6:  that really jumps out to me about, about this. I was just reading over the,

842
00:46:48,840 --> 00:46:52,220
Speaker 6:  the Slack message that the CEO, Keith Valerie sent.

843
00:46:52,680 --> 00:46:56,500
Speaker 6:  And I still think of Plex very much as like the way to get my

844
00:46:57,290 --> 00:47:01,140
Speaker 6:  legally acquired content from one device to

845
00:47:01,140 --> 00:47:04,980
Speaker 6:  another. And, and Eli's friend Yeah. Who, who

846
00:47:04,980 --> 00:47:06,300
Speaker 6:  also has a very good Plex server

847
00:47:07,840 --> 00:47:08,060
Speaker 5:  All

848
00:47:08,170 --> 00:47:11,580
Speaker 6:  Perfectly legal. But that's like Plex tried to get people to pay for that

849
00:47:11,580 --> 00:47:14,420
Speaker 6:  for a long time and it never really worked. And what it seems like happened

850
00:47:14,420 --> 00:47:17,140
Speaker 6:  is over the last couple of years, Plex was like, we are going to be a streaming

851
00:47:17,140 --> 00:47:20,660
Speaker 6:  guide and a streaming service and just has like barreled

852
00:47:20,660 --> 00:47:24,460
Speaker 6:  headlong into that. And it turns out, as any streaming service will tell

853
00:47:24,460 --> 00:47:28,100
Speaker 6:  you, there is not a lot of money in it at this particular moment in time.

854
00:47:28,120 --> 00:47:31,500
Speaker 6:  The economy's bad, bad, the ad market is bad. It's just a tough time to decide

855
00:47:31,500 --> 00:47:32,340
Speaker 6:  you're a streaming service.

856
00:47:32,690 --> 00:47:36,500
Speaker 7:  I'll always kind of admire Plex for what it's wanting to do, which is is

857
00:47:36,500 --> 00:47:39,660
Speaker 7:  be that that ag that that place where you're like, I don't know what I'm

858
00:47:39,660 --> 00:47:42,300
Speaker 7:  gonna watch. I'm gonna go to Plex and oh it's gonna have my Netflix plugged

859
00:47:42,300 --> 00:47:45,020
Speaker 7:  in and my Hulu and my Disney Plus and everything. That's good idea. That's

860
00:47:45,020 --> 00:47:48,780
Speaker 7:  perfect. None of those companies wanna work with Plex or any other

861
00:47:48,780 --> 00:47:50,020
Speaker 7:  company who has that desire.

862
00:47:50,380 --> 00:47:54,260
Speaker 5:  Plex has a bad reputation. Like if you're Disney and Plex shows

863
00:47:54,260 --> 00:47:58,220
Speaker 5:  up on your doorstep, you're like, go to jail, why are you here?

864
00:47:58,400 --> 00:47:59,100
Speaker 5:  Can you imagine like

865
00:47:59,100 --> 00:48:01,940
Speaker 6:  The Pirate Bay being like, we're starting a streaming service, would you

866
00:48:01,940 --> 00:48:02,540
Speaker 6:  like to work with us?

867
00:48:03,760 --> 00:48:07,300
Speaker 7:  But even if it wasn't Plex, even if it was a different company who had a

868
00:48:07,300 --> 00:48:11,060
Speaker 7:  bunch of support say in like, what is it, 2008 to 2009 until they decided

869
00:48:11,080 --> 00:48:15,060
Speaker 7:  to build a terrible box and their company went under, nobody wants to

870
00:48:15,220 --> 00:48:19,100
Speaker 7:  actually participate in that idea. And so Flex's other thing was like,

871
00:48:19,100 --> 00:48:21,700
Speaker 7:  okay, we're gonna do fast tv, we're just gonna make a bunch of free stuff

872
00:48:22,200 --> 00:48:26,100
Speaker 7:  so that your family, even when your server goes down, will not call and yell

873
00:48:26,100 --> 00:48:30,060
Speaker 7:  at you because they can find a bunch of stuff on their Plex account and

874
00:48:30,530 --> 00:48:31,980
Speaker 7:  there's a limited runway there.

875
00:48:32,680 --> 00:48:35,860
Speaker 5:  One thing they could do is sell software to users

876
00:48:36,760 --> 00:48:37,380
Speaker 5:  to use it.

877
00:48:37,450 --> 00:48:41,100
Speaker 7:  They already did. I bought my lifetime pass in like 2010.

878
00:48:41,400 --> 00:48:45,260
Speaker 5:  That's like a real Reddit business model like that. That sounds

879
00:48:45,260 --> 00:48:48,940
Speaker 5:  great. In the Reddit forum. And then you're like, crap. We'll never make

880
00:48:48,940 --> 00:48:49,780
Speaker 5:  any money from Alex

881
00:48:49,830 --> 00:48:53,580
Speaker 6:  Again. Yeah. Never grands Plex. $8 13 years ago.

882
00:48:53,680 --> 00:48:55,100
Speaker 6:  And it's like, why are they doing playoffs?

883
00:48:57,100 --> 00:48:57,980
Speaker 5:  I know. Oh,

884
00:48:59,760 --> 00:49:02,500
Speaker 6:  All right. I'll go next. And Nila, you can go last before we kick you off

885
00:49:02,500 --> 00:49:06,380
Speaker 6:  the show. Mine is just Reddit question

886
00:49:06,380 --> 00:49:08,020
Speaker 6:  mark is my Lightning round pick.

887
00:49:09,960 --> 00:49:12,740
Speaker 6:  So as you're hearing this, it is, it is Friday,

888
00:49:13,330 --> 00:49:16,820
Speaker 6:  June 30th. This is kind of like inflection

889
00:49:17,120 --> 00:49:19,620
Speaker 6:  day for Reddit. Reddit has been telling

890
00:49:20,530 --> 00:49:24,140
Speaker 6:  subreddit moderators that they have essentially 48 hours to

891
00:49:24,610 --> 00:49:27,820
Speaker 6:  open back up or something is gonna happen

892
00:49:28,530 --> 00:49:32,260
Speaker 6:  that expired yesterday. Today is the day that apps like

893
00:49:32,400 --> 00:49:35,780
Speaker 6:  Apollo are scheduled to shut down. I think that's supposed to happen at midnight

894
00:49:35,920 --> 00:49:39,420
Speaker 6:  Friday into Saturday. This just feels like

895
00:49:39,600 --> 00:49:43,260
Speaker 6:  Reddit is going to be one company before today and another company

896
00:49:43,430 --> 00:49:46,620
Speaker 6:  after today. And the way that it thinks about its moderators and about its

897
00:49:46,620 --> 00:49:50,610
Speaker 6:  community and about its business is about to change. And I don't

898
00:49:50,610 --> 00:49:54,010
Speaker 6:  know exactly what that's gonna look like, but moderators are just the like

899
00:49:54,890 --> 00:49:58,370
Speaker 6:  absolutely furious with everything that Reddit has done. And Reddit like

900
00:49:58,390 --> 00:50:02,130
Speaker 6:  is its moderators in a very, very real way. And I think what's about to happen

901
00:50:02,920 --> 00:50:06,610
Speaker 6:  over this July 4th holiday, which I think it is not an accident that

902
00:50:06,610 --> 00:50:10,490
Speaker 6:  Reddit scheduled all of this mess to happen on the July 4th holiday is

903
00:50:10,490 --> 00:50:11,530
Speaker 6:  gonna be really fascinating. What's

904
00:50:11,530 --> 00:50:15,330
Speaker 5:  Gonna happen in our smoking is our smoking My gosh. Open. Gosh, it's wasn't

905
00:50:16,030 --> 00:50:19,770
Speaker 5:  the means. Our smoking is the meat. Smoking I mean for July 4th. This is

906
00:50:20,110 --> 00:50:20,970
Speaker 5:  the 4th of July.

907
00:50:21,510 --> 00:50:22,490
Speaker 6:  Oh yeah. This is huge.

908
00:50:23,160 --> 00:50:26,970
Speaker 5:  This is the Super Bowl. This is a big day for the, the meat

909
00:50:27,020 --> 00:50:30,260
Speaker 5:  smoking for 'em. It looks like they, it looks like they're good. It looks

910
00:50:30,260 --> 00:50:31,820
Speaker 5:  like they're good. Okay. Okay. Whew.

911
00:50:32,160 --> 00:50:35,700
Speaker 6:  All right. Meat smokers of the world. Unite Neli. What's yours? Then we'll

912
00:50:35,700 --> 00:50:36,420
Speaker 6:  go a break. Oh,

913
00:50:36,660 --> 00:50:40,580
Speaker 5:  I got two. Of course. Of course you do. One is just very funny in, in

914
00:50:40,580 --> 00:50:44,260
Speaker 5:  like a very simple way. Ford keeps

915
00:50:44,260 --> 00:50:48,180
Speaker 5:  raising the price of the F-150 Lightning and now people don't want to

916
00:50:48,180 --> 00:50:48,540
Speaker 5:  buy it.

917
00:50:48,770 --> 00:50:52,060
Speaker 6:  Wait, didn't It used to be really expensive and every wanted to buy, everybody

918
00:50:52,060 --> 00:50:54,780
Speaker 6:  wanted to buy it and now it's really expensive and nobody wants to buy

919
00:50:54,780 --> 00:50:58,460
Speaker 5:  It. It's a, there's a weird thing going on here. So they, they launched it

920
00:50:58,460 --> 00:51:02,260
Speaker 5:  at 40 k and then the most expensive ones were like over 90, like 93 k.

921
00:51:02,260 --> 00:51:05,620
Speaker 5:  And it's just sort of been getting more expensive over time. There was a

922
00:51:05,620 --> 00:51:09,540
Speaker 5:  moment right at the end of last year when the tax

923
00:51:09,560 --> 00:51:12,660
Speaker 5:  credit was expiring. Like people who are buying a

924
00:51:12,660 --> 00:51:16,380
Speaker 5:  $93,000 truck make too much money to get the new tax

925
00:51:16,380 --> 00:51:18,940
Speaker 5:  credit. So there was a moment last year if you bought the thing, you still

926
00:51:18,940 --> 00:51:22,580
Speaker 5:  gotta 7,500. And so like everyone was gonna buy this truck. Ford

927
00:51:22,620 --> 00:51:25,660
Speaker 5:  couldn't make as many of them as they wanted. Dealers are dealers and they're

928
00:51:25,820 --> 00:51:29,700
Speaker 5:  horrible. So the markups were out of control. That whole thing

929
00:51:29,760 --> 00:51:33,700
Speaker 5:  has collapsed. Like the markups are still there, but everyone's like, why

930
00:51:33,740 --> 00:51:36,220
Speaker 5:  I don't need this truck. I, I'm not getting a tax credit. The truck's too

931
00:51:36,220 --> 00:51:40,060
Speaker 5:  expensive. There are other trucks. Rivian can't sell R one

932
00:51:40,180 --> 00:51:44,020
Speaker 5:  Ts. They set up a fake lot at their factory in normal Illinois. And

933
00:51:44,020 --> 00:51:47,820
Speaker 5:  there's, they have like one day sales events at their own factory. Part of

934
00:51:47,820 --> 00:51:51,520
Speaker 5:  the funny thing for me is like, it's just supply and demand.

935
00:51:52,230 --> 00:51:56,040
Speaker 5:  Like at some point you raise the price so high that demand drops

936
00:51:56,260 --> 00:51:57,320
Speaker 5:  and Ford did it.

937
00:52:00,030 --> 00:52:03,720
Speaker 5:  Like I learned about that in like my 11th grade economics class.

938
00:52:03,950 --> 00:52:07,920
Speaker 5:  Like there's a curve and you can go over the curve and then demand will drop

939
00:52:08,020 --> 00:52:12,000
Speaker 5:  and like great. And so I, they found it. I think Ford has to reset there.

940
00:52:12,100 --> 00:52:16,040
Speaker 5:  But then I think they have this other gigantic looming problem that

941
00:52:16,040 --> 00:52:19,880
Speaker 5:  they've all signed up for, which is they're all gonna use the

942
00:52:19,880 --> 00:52:23,600
Speaker 5:  Tesla charger, which is superior to the, the sort of like

943
00:52:24,100 --> 00:52:27,760
Speaker 5:  ccs agglomeration of a J 1 7 72 and the

944
00:52:27,980 --> 00:52:29,360
Speaker 5:  DC charger that they're using now.

945
00:52:30,050 --> 00:52:30,400
Speaker 6:  Those

946
00:52:30,400 --> 00:52:30,800
Speaker 7:  Are words

947
00:52:31,260 --> 00:52:34,600
Speaker 6:  And that's like, that's like a, that's a like a big change, right? Yeah.

948
00:52:34,600 --> 00:52:38,160
Speaker 6:  That's not like I put a slightly different thing into my car that's like,

949
00:52:38,160 --> 00:52:40,040
Speaker 6:  I need a new thing for my car. Right.

950
00:52:40,310 --> 00:52:43,720
Speaker 5:  Well it's a different plug in the car. Okay. I, I think the actual like

951
00:52:44,040 --> 00:52:48,000
Speaker 5:  electricity bit of it, right? Like a charging station isn't actually a charging

952
00:52:48,000 --> 00:52:50,840
Speaker 5:  station, it's just a supply of electricity. All the charging component tree

953
00:52:50,840 --> 00:52:51,480
Speaker 5:  is in the car.

954
00:52:52,060 --> 00:52:53,640
Speaker 6:  Are we about to get car doggles?

955
00:52:54,390 --> 00:52:58,320
Speaker 5:  Yeah. So it's like if you buy this truck now and in next year

956
00:52:58,380 --> 00:53:02,360
Speaker 5:  you're just able to use the Tesla network, you're like signing up

957
00:53:02,460 --> 00:53:05,440
Speaker 5:  for a decade of dongle usage with your

958
00:53:05,700 --> 00:53:08,960
Speaker 5:  $93,000 Ford F-150 Lightning. Is

959
00:53:08,960 --> 00:53:12,120
Speaker 7:  There like a little spot in the car where you store your dongle?

960
00:53:12,480 --> 00:53:16,400
Speaker 5:  Well it's a pickup truck, so, so there should be, there's a pretty, pretty

961
00:53:16,400 --> 00:53:18,360
Speaker 5:  significant amount of dongle storage space,

962
00:53:18,780 --> 00:53:19,840
Speaker 7:  Big space in the back.

963
00:53:20,670 --> 00:53:24,640
Speaker 6:  Alex, the way this needs to go is on the pickup truck. The spare tire is

964
00:53:24,640 --> 00:53:27,340
Speaker 6:  is on a rack. We need a second rack for the car dog

965
00:53:27,660 --> 00:53:28,420
Speaker 7:  Just for the dongle.

966
00:53:28,610 --> 00:53:31,260
Speaker 5:  Like what's, what's the franc for dongles

967
00:53:31,490 --> 00:53:34,180
Speaker 6:  Instead of truck nuts, you just carry it around into their dole.

968
00:53:35,970 --> 00:53:39,860
Speaker 5:  Well I just like it. It's not just for it like Chevy has done it, Chevy's

969
00:53:39,860 --> 00:53:43,220
Speaker 5:  about to roll out the new EV Silverado, right? Like YouTubers influencer

970
00:53:43,220 --> 00:53:46,700
Speaker 5:  already have it like this thing's about to hit and it's like GM just announced

971
00:53:46,700 --> 00:53:47,500
Speaker 5:  they're using this plug.

972
00:53:48,000 --> 00:53:51,540
Speaker 7:  Why were we spending all this time talking about AI when like

973
00:53:51,880 --> 00:53:55,300
Speaker 7:  EV dongle gate is looming like the

974
00:53:55,300 --> 00:53:58,540
Speaker 7:  existential threats of ai I don't care about Yeah. Dongle gate. Well

975
00:53:58,540 --> 00:54:02,260
Speaker 5:  So again, the weird thing, it's like the right time for this to happen,

976
00:54:02,470 --> 00:54:06,420
Speaker 5:  right? There's not a lot of EVs on the road there and the most

977
00:54:06,420 --> 00:54:09,780
Speaker 5:  popular one is the Tesla model three, which has no associated dongle gate

978
00:54:10,160 --> 00:54:13,500
Speaker 5:  cuz they're just gonna use Tesla's charger. But the government's basically

979
00:54:13,500 --> 00:54:17,300
Speaker 5:  like pushing Tesla into doing this through a a

980
00:54:17,300 --> 00:54:20,220
Speaker 5:  series of tax credit schemes, which is fine and I think appropriate for the

981
00:54:20,300 --> 00:54:23,820
Speaker 5:  government to do. Tesla's gonna turn that charging network into

982
00:54:24,130 --> 00:54:27,900
Speaker 5:  another source of revenue. Cause now it's open to more people. The S A e,

983
00:54:27,900 --> 00:54:31,140
Speaker 5:  the Society of Automotive Engineers has now like certified the Tesla plug,

984
00:54:31,140 --> 00:54:34,700
Speaker 5:  the N Aacs plug as a standard other comp Volvos signed up to use like other

985
00:54:34,860 --> 00:54:38,780
Speaker 5:  companie like basically they're winning. They're it, it is done. Everyone's

986
00:54:38,780 --> 00:54:42,060
Speaker 5:  gonna use this plug just not for a couple of years. So we're just in this

987
00:54:42,070 --> 00:54:45,260
Speaker 5:  weird moment for the EV industry where adoption

988
00:54:46,040 --> 00:54:49,780
Speaker 5:  is finally starting to pick up the cars aren't quite as

989
00:54:49,780 --> 00:54:53,380
Speaker 5:  vapor as they used to. Like you can get an F-150 Lightning, you can get an

990
00:54:53,460 --> 00:54:57,180
Speaker 5:  R one T, you might be able to get a Silverado like it's too new. I don't

991
00:54:57,340 --> 00:55:00,620
Speaker 5:  know. But like the, particularly in that market, you can like, you can go

992
00:55:00,620 --> 00:55:04,580
Speaker 5:  get one, you can go get a Polestar, you can get a Mustang Mach and

993
00:55:04,580 --> 00:55:06,780
Speaker 5:  then they've all got the wrong plug. So you've like signed up for like a

994
00:55:06,780 --> 00:55:07,780
Speaker 5:  decade of the wrong plug.

995
00:55:08,080 --> 00:55:11,390
Speaker 7:  How hard is it to just like change the plug?

996
00:55:11,980 --> 00:55:13,430
Speaker 7:  Like can't they do that in the factory?

997
00:55:13,750 --> 00:55:17,670
Speaker 5:  I suspect we're gonna see a lot of TikTok mechanics trying to third

998
00:55:17,670 --> 00:55:18,310
Speaker 6:  Party plugs.

999
00:55:18,860 --> 00:55:19,150
Speaker 7:  Yeah,

1000
00:55:19,540 --> 00:55:23,350
Speaker 5:  Like what you might call a built-in dongle. I,

1001
00:55:23,410 --> 00:55:26,550
Speaker 5:  so I don't know. At the factory you might be able to like who knows, but

1002
00:55:26,550 --> 00:55:29,830
Speaker 5:  right now off the ones sitting on lots, the dealers have to move that are

1003
00:55:29,830 --> 00:55:33,790
Speaker 5:  overpriced and have a markup and the wrong plug. Oof. Woo. That's

1004
00:55:33,790 --> 00:55:33,830
Speaker 5:  bad.

1005
00:55:33,830 --> 00:55:34,070
Speaker 6:  Not great.

1006
00:55:34,910 --> 00:55:38,310
Speaker 5:  My last one is just this TikTok thing. I just think it's funny. So TikTok,

1007
00:55:38,610 --> 00:55:41,070
Speaker 5:  we keep talking about the social web and how hard it's to be a creator and

1008
00:55:41,070 --> 00:55:44,710
Speaker 5:  how not tenable it is. TikTok, it has a new feature where

1009
00:55:44,930 --> 00:55:48,790
Speaker 5:  brands can put up money. Okay. Like we want some Ads for our

1010
00:55:48,790 --> 00:55:52,550
Speaker 5:  brand and then talkers can make Ads for

1011
00:55:52,550 --> 00:55:56,470
Speaker 5:  free and then maybe get some money. so like it's basically like a creator

1012
00:55:56,470 --> 00:55:59,590
Speaker 5:  fund. so like I'm like I want some Ads, here's like a million dollars if

1013
00:55:59,590 --> 00:56:02,670
Speaker 5:  you get the most views, I'll like pay you some percentage of the million

1014
00:56:02,670 --> 00:56:02,950
Speaker 5:  dollars.

1015
00:56:03,590 --> 00:56:07,510
Speaker 7:  I feel like that's not a good revenue stream to rely on for

1016
00:56:07,510 --> 00:56:08,030
Speaker 7:  your business.

1017
00:56:08,780 --> 00:56:12,750
Speaker 5:  Yeah, it's just basically like a new way to get free Ads from

1018
00:56:12,750 --> 00:56:13,030
Speaker 5:  kids.

1019
00:56:13,460 --> 00:56:17,430
Speaker 6:  It's weird in that I think what they're trying to do is get

1020
00:56:17,490 --> 00:56:21,310
Speaker 6:  people just starting out with fewer followers to

1021
00:56:21,310 --> 00:56:25,260
Speaker 6:  make stuff for them because like there, there has been this incredible culture

1022
00:56:25,280 --> 00:56:29,140
Speaker 6:  for so long of people wanting to seem sponsored before they're actually sponsored.

1023
00:56:29,480 --> 00:56:32,660
Speaker 6:  And now it's like, just make the spa con and we'll decide later if we wanna

1024
00:56:32,660 --> 00:56:36,620
Speaker 6:  give you money. And to some extent this is like, I think it was 99

1025
00:56:36,650 --> 00:56:40,620
Speaker 6:  designs used to do this with like graphic design. You, you could put up

1026
00:56:40,620 --> 00:56:43,820
Speaker 6:  a thing and people would submit and you'd pay the winner. And then I think

1027
00:56:43,860 --> 00:56:47,300
Speaker 6:  a lot of people have decided over the years that that is kind of gross and

1028
00:56:47,300 --> 00:56:51,060
Speaker 6:  bad and like you should pay me for my work. Not just pick a contest

1029
00:56:51,120 --> 00:56:54,860
Speaker 6:  winner and like doing my job is not entering a contest. Or

1030
00:56:54,860 --> 00:56:58,540
Speaker 7:  You could do what McDonald's did. Have you seen the grimace tos?

1031
00:56:59,000 --> 00:56:59,220
Speaker 5:  No.

1032
00:56:59,410 --> 00:57:03,180
Speaker 6:  What have you seen? The Grimace TikTok is such a menacing sentence

1033
00:57:03,280 --> 00:57:04,100
Speaker 6:  to say out loud

1034
00:57:04,470 --> 00:57:08,300
Speaker 7:  Everything about this is menacing. So so they started like tweeting and putting

1035
00:57:08,300 --> 00:57:11,940
Speaker 7:  on TikTok just like an image of grimace and being like his birthday's coming.

1036
00:57:12,640 --> 00:57:16,620
Speaker 7:  And so, and it's like very menacing with how it

1037
00:57:16,860 --> 00:57:20,580
Speaker 7:  happened. Cause you just see like the top of grimaces little purple head

1038
00:57:20,800 --> 00:57:24,780
Speaker 7:  and it's like, oh boy, lots of things happening there. And then they sold

1039
00:57:24,900 --> 00:57:28,500
Speaker 7:  a shake on his birthday and all like a lot of young

1040
00:57:28,500 --> 00:57:32,180
Speaker 7:  peoples, particularly Gen Z all shot videos that were like happy birthday

1041
00:57:32,210 --> 00:57:35,740
Speaker 7:  grimace. And they'd hold up the shake and then it would smash cut to them

1042
00:57:35,740 --> 00:57:38,740
Speaker 7:  like dead covered in grimace milkshake

1043
00:57:39,520 --> 00:57:43,100
Speaker 7:  and increasingly graphic but not technically cuz it's grimace

1044
00:57:43,100 --> 00:57:45,300
Speaker 7:  milkshake. Like Tableau's.

1045
00:57:45,880 --> 00:57:46,300
Speaker 5:  Oh my.

1046
00:57:46,480 --> 00:57:49,740
Speaker 6:  And now we're talking about it, which means someone is gonna go buy a grimace

1047
00:57:49,740 --> 00:57:52,900
Speaker 6:  shake. Someone is going to Yes, take their car for where they're right now

1048
00:57:53,120 --> 00:57:54,140
Speaker 6:  to a McDonald's drive,

1049
00:57:54,140 --> 00:57:58,020
Speaker 5:  Go to McDonald's and ask them for money. This is reverse

1050
00:57:58,340 --> 00:58:01,900
Speaker 5:  advertising pioneered on The Vergecast, but like ask nicely

1051
00:58:02,120 --> 00:58:04,380
Speaker 5:  and not like you're robbing the McDonald's. Yeah,

1052
00:58:04,380 --> 00:58:05,020
Speaker 6:  Yeah. Please

1053
00:58:05,020 --> 00:58:05,980
Speaker 7:  Don't rob McDonald's.

1054
00:58:06,170 --> 00:58:09,940
Speaker 6:  Yeah, my McDonald's take is this is a bad idea and it's absolutely a hundred

1055
00:58:09,940 --> 00:58:10,580
Speaker 6:  percent gonna work.

1056
00:58:11,080 --> 00:58:11,500
Speaker 7:  Oh yeah.

1057
00:58:12,660 --> 00:58:15,900
Speaker 5:  I think creators are looking for money. I I, this is just one of those things

1058
00:58:15,990 --> 00:58:19,580
Speaker 5:  where like most of the money, Mr. Beast did a two hour

1059
00:58:19,580 --> 00:58:23,300
Speaker 5:  interview on YouTube that Jay actually watched all of incredible.

1060
00:58:23,880 --> 00:58:27,700
Speaker 5:  And the, the big takeaway for Jay was the ad rates on a Mr.

1061
00:58:27,700 --> 00:58:30,900
Speaker 5:  Beast video are so high that most companies can't afford them or won't pay

1062
00:58:30,900 --> 00:58:34,820
Speaker 5:  them. And so Mr. Beast is like, screw it, I'm just gonna put Ads for

1063
00:58:34,820 --> 00:58:37,820
Speaker 5:  feast balls in my video because if I sell a lot of feast balls, I'll make

1064
00:58:37,820 --> 00:58:41,500
Speaker 5:  all the money. And it's like that's where we are in the creator. When I say

1065
00:58:41,500 --> 00:58:45,460
Speaker 5:  this thing is about to change, like it, none of it feels

1066
00:58:45,460 --> 00:58:49,020
Speaker 5:  sustainable right now across any platform. And setting up letting

1067
00:58:49,080 --> 00:58:53,020
Speaker 5:  brands set up creator funds on TikTok instead of TikTok figuring out how

1068
00:58:53,020 --> 00:58:56,740
Speaker 5:  to make all of its creators more money. It, it just feels like evidence of

1069
00:58:56,740 --> 00:58:57,100
Speaker 5:  this thing.

1070
00:58:57,610 --> 00:58:59,020
Speaker 6:  Yeah. Yeah. Agree

1071
00:58:59,050 --> 00:59:02,300
Speaker 5:  More people should work for free is like not the right choice for you as

1072
00:59:02,300 --> 00:59:03,180
Speaker 5:  a company. Yeah.

1073
00:59:03,410 --> 00:59:06,180
Speaker 6:  Well we should have ended on card Dons. I liked that better. But we

1074
00:59:07,460 --> 00:59:08,980
Speaker 5:  I could do another 20 on card

1075
00:59:09,560 --> 00:59:13,020
Speaker 6:  I'm sure. Yeah, I'm sure we will to be honest. But right now Neli, you need

1076
00:59:13,020 --> 00:59:16,020
Speaker 6:  to go, we need to take a break. When we come back, we're gonna catch up on

1077
00:59:16,040 --> 00:59:19,780
Speaker 6:  all of The FTC versus Microsoft stuff going on. If you

1078
00:59:19,780 --> 00:59:23,260
Speaker 6:  haven't listened to the Wednesday show, I spent a bunch of time with Tom

1079
00:59:23,780 --> 00:59:27,140
Speaker 6:  catching up on stuff. Pause this episode, go listen to that if you haven't

1080
00:59:27,320 --> 00:59:31,020
Speaker 6:  and then come back and we're gonna catch up. Satya Nadella and Bobby Kodak

1081
00:59:31,020 --> 00:59:34,620
Speaker 6:  were both on the stand on Wednesday and both had some surprisingly and

1082
00:59:35,120 --> 00:59:38,900
Speaker 6:  you know, it was spicy importantly spicy things to say. Yeah, it was,

1083
00:59:38,920 --> 00:59:42,100
Speaker 6:  it was quite a day. So we're gonna get to all of that without Nili because

1084
00:59:42,100 --> 00:59:45,180
Speaker 6:  we hate Nili. Gotta go get lost plane. We'll be right back.

1085
00:59:53,350 --> 00:59:57,300
Speaker 6:  We're back by the magic of time travel. I guess it is now. Thursday

1086
00:59:57,300 --> 01:00:01,180
Speaker 6:  morning everybody got some sleep, took a break. Neli is, I

1087
01:00:01,180 --> 01:00:04,780
Speaker 6:  don't somewhere else in America, but he's not here. But instead we have much

1088
01:00:04,780 --> 01:00:08,140
Speaker 6:  better people. Tom Warren's here. Addie's here. Hi guys. Hey.

1089
01:00:08,400 --> 01:00:08,620
Speaker 10:  Hey.

1090
01:00:08,760 --> 01:00:11,380
Speaker 6:  How are you both feeling? Are you alive Tom? I was worried about you when

1091
01:00:11,380 --> 01:00:12,100
Speaker 6:  we finished the other day.

1092
01:00:12,690 --> 01:00:15,620
Speaker 10:  What did I look, did I look like I was dying because I felt like I was

1093
01:00:15,800 --> 01:00:18,900
Speaker 6:  By the end there was just a real, like all of your sentence sort of ended

1094
01:00:19,010 --> 01:00:22,020
Speaker 6:  with like a one of those. It's just like, you know what I mean?

1095
01:00:22,650 --> 01:00:26,420
Speaker 10:  Like aha, I haven't had enough sleep. Yeah, no, I'm good. I

1096
01:00:26,480 --> 01:00:29,660
Speaker 10:  I'm a feeling better today because it's the, it's the final day of this

1097
01:00:30,170 --> 01:00:33,860
Speaker 10:  epic saga. Well it's the fi it's the final day of the hearing, but it's

1098
01:00:33,860 --> 01:00:35,340
Speaker 10:  definitely not the end of it. So yeah.

1099
01:00:35,340 --> 01:00:38,380
Speaker 6:  So by the time you're hearing this, this will be over. But we, we felt pretty

1100
01:00:38,380 --> 01:00:42,340
Speaker 6:  good doing this now because the bulk of the interesting testimony

1101
01:00:42,440 --> 01:00:45,660
Speaker 6:  we can fairly safely say is over at this point. Addie, are you expecting

1102
01:00:46,180 --> 01:00:47,300
Speaker 6:  fireworks today? Thursday?

1103
01:00:47,800 --> 01:00:50,540
Speaker 11:  Not unless something really strange gets disclosed.

1104
01:00:50,690 --> 01:00:52,380
Speaker 10:  Yeah, like a Sony document.

1105
01:00:53,590 --> 01:00:56,140
Speaker 11:  Maybe I don't, I don't know if we're ever going to see a document getting

1106
01:00:56,140 --> 01:00:56,980
Speaker 11:  disclosed again.

1107
01:00:58,750 --> 01:01:02,300
Speaker 6:  We're gonna get to that. But let's, let's just kind of take Wednesday's

1108
01:01:02,580 --> 01:01:06,260
Speaker 6:  testimony in order because it was, it was the CEOs of the two

1109
01:01:06,500 --> 01:01:09,940
Speaker 6:  companies involved in this. Bobby Kodak, the CEO of Activision Blizzard and

1110
01:01:10,180 --> 01:01:13,100
Speaker 6:  Sachin Nadella, the CEO e of Microsoft. Let's start with Bobby cuz he went

1111
01:01:13,260 --> 01:01:16,860
Speaker 6:  first. Tom, what were we expecting to learn from Bobby?

1112
01:01:17,080 --> 01:01:20,740
Speaker 6:  He, he ended up talking a lot about like the history of Activision Blizzard,

1113
01:01:20,740 --> 01:01:23,100
Speaker 6:  which is not what I expected. Was that what you were expecting

1114
01:01:23,500 --> 01:01:27,180
Speaker 10:  I mean? It's, it's, it's basically the basic questions of, of the, the lawyers

1115
01:01:27,240 --> 01:01:30,460
Speaker 10:  in these cases. They were sort of like established the facts. So I'm not

1116
01:01:30,460 --> 01:01:34,260
Speaker 10:  surprised that he kind of dug into the history a bit. But I was expecting,

1117
01:01:34,540 --> 01:01:38,020
Speaker 10:  I must admit that he was gonna get a bit more grilled that by, by the,

1118
01:01:38,270 --> 01:01:41,340
Speaker 10:  especially by the judge as well. I thought she would be firing off questions

1119
01:01:41,340 --> 01:01:44,620
Speaker 10:  to him and around specifics of some of the stuff that's come up in the case.

1120
01:01:44,710 --> 01:01:47,900
Speaker 10:  There wasn't much of that. I was actually kind of surprised at that. They,

1121
01:01:47,980 --> 01:01:51,740
Speaker 10:  I think both Bobby Kotick and Nadella both got off

1122
01:01:51,740 --> 01:01:55,300
Speaker 10:  pretty lightly if I, if I'm honest from from the questioning from, from

1123
01:01:55,300 --> 01:01:56,140
Speaker 10:  what we saw. I

1124
01:01:56,140 --> 01:01:56,420
Speaker 6:  Agree.

1125
01:01:56,600 --> 01:02:00,380
Speaker 10:  But we did get some Nintendo Switch insight from Bobby.

1126
01:02:00,640 --> 01:02:01,260
Speaker 10:  If anything

1127
01:02:01,570 --> 01:02:04,660
Speaker 6:  They just can't stop talking about Nintendo. This is like sneakily a trial

1128
01:02:04,660 --> 01:02:07,900
Speaker 6:  about like, is Nintendo as cool as everybody thinks? That's like basically

1129
01:02:07,900 --> 01:02:08,940
Speaker 6:  the main question of Yeah,

1130
01:02:08,940 --> 01:02:11,860
Speaker 10:  It's like everyone's got like some sort of advertising marketing agreement

1131
01:02:11,860 --> 01:02:15,740
Speaker 10:  with Nintendo. They have to talk about the Switch. It's like the sponsored

1132
01:02:15,740 --> 01:02:19,500
Speaker 10:  segment. Yeah, like we got a little bit of the Nintendo Switch stuff,

1133
01:02:19,670 --> 01:02:22,540
Speaker 10:  which has, as you say has been an ongoing sort of theme throughout this

1134
01:02:22,540 --> 01:02:26,340
Speaker 10:  trial. But before that we kind of got to Bobby Kotick

1135
01:02:26,340 --> 01:02:30,020
Speaker 10:  talking about deals and cloud stuff and

1136
01:02:30,020 --> 01:02:33,940
Speaker 10:  whether he hated subscriptions, which was kind of interesting because

1137
01:02:34,520 --> 01:02:38,500
Speaker 10:  the day before, or Jim Ryan had said that he thought all publishers just

1138
01:02:38,500 --> 01:02:41,500
Speaker 10:  basically thought that that game pass and subscriptions

1139
01:02:42,380 --> 01:02:45,620
Speaker 10:  just ruined the value of their, of their businesses and stuff. And he kind

1140
01:02:45,620 --> 01:02:48,980
Speaker 10:  of, he didn't exactly say what Jim Ryan said, but he kind of backed that

1141
01:02:48,980 --> 01:02:52,180
Speaker 10:  up. He was like, yeah, it, it's, it's not something that I'm interested

1142
01:02:52,200 --> 01:02:52,700
Speaker 10:  in. So

1143
01:02:53,290 --> 01:02:57,060
Speaker 11:  Yeah, his analogy was, I've watched, he didn't use these words

1144
01:02:57,080 --> 01:02:59,340
Speaker 11:  but like I've watched Netflix, the story Hollywood basically.

1145
01:02:59,730 --> 01:03:03,220
Speaker 10:  Yeah. He's like living in Los Angeles, I see all these companies and the

1146
01:03:03,220 --> 01:03:05,300
Speaker 10:  value going down and all this sort of stuff. So

1147
01:03:05,470 --> 01:03:07,660
Speaker 6:  Tease that out for me actually, cuz this is one of the things I've been trying

1148
01:03:07,660 --> 01:03:10,780
Speaker 6:  to figure out is like everybody's sort of talking in circles about cloud

1149
01:03:10,920 --> 01:03:14,460
Speaker 6:  and there's like game pass and there's X Cloud and like

1150
01:03:14,630 --> 01:03:18,380
Speaker 6:  Satya brought in Xbox live just to make everything more confusing. But there's

1151
01:03:18,380 --> 01:03:21,660
Speaker 6:  this, there's this thing that Bobby in particular was talking about

1152
01:03:22,130 --> 01:03:26,020
Speaker 6:  that it was like there there's like one set of ideas

1153
01:03:26,230 --> 01:03:30,220
Speaker 6:  about cloud gaming that he thinks are just like a non-starter. I, but

1154
01:03:30,220 --> 01:03:33,900
Speaker 6:  I had trouble figuring out exactly what it was that he seems to hate about

1155
01:03:33,900 --> 01:03:34,460
Speaker 6:  cloud gaming.

1156
01:03:35,260 --> 01:03:36,340
Speaker 11:  Anything Nvidia does, right?

1157
01:03:36,860 --> 01:03:37,580
Speaker 6:  Yeah. There's that.

1158
01:03:38,810 --> 01:03:42,380
Speaker 10:  Yeah, he did mention I think, what was the quote a do you remember it was

1159
01:03:42,380 --> 01:03:43,140
Speaker 10:  like the refrigerator

1160
01:03:43,900 --> 01:03:47,260
Speaker 11:  I was playing Modern warfare too on your phone would be like using your

1161
01:03:47,260 --> 01:03:48,820
Speaker 11:  refrigerator as a safe. Yeah.

1162
01:03:49,000 --> 01:03:52,740
Speaker 10:  And that was, that was kind of in reference to cloud but also

1163
01:03:52,840 --> 01:03:56,700
Speaker 10:  to mobile and to the fact that you have all these like Call of

1164
01:03:56,700 --> 01:04:00,300
Speaker 10:  Duty games, they don't run on phone right now. Would they run

1165
01:04:00,610 --> 01:04:04,420
Speaker 10:  like on a phone eventually? He said maybe. But also it's obviously

1166
01:04:04,420 --> 01:04:07,500
Speaker 10:  like a kind of a segue into the sort of like cloud stuff as well because

1167
01:04:07,500 --> 01:04:10,860
Speaker 10:  the answer also Microsoft says the answer is cloud, right?

1168
01:04:11,280 --> 01:04:14,540
Speaker 10:  Or they used to say, and now they're, they're kind of like, oh dad we're

1169
01:04:14,540 --> 01:04:17,300
Speaker 10:  just chilling with the cloud. you know, don't worry about the cloud, don't

1170
01:04:17,300 --> 01:04:20,940
Speaker 10:  look there. But I don't think he has like a specific opinion

1171
01:04:20,990 --> 01:04:24,620
Speaker 10:  about cloud. He's just like, we're not putting our games on there. But The

1172
01:04:24,960 --> 01:04:28,860
Speaker 10:  FTC was kind of like arguing, but you would, if there was

1173
01:04:29,020 --> 01:04:32,100
Speaker 10:  like a good commercial agreement and he was like, we don't have plans for

1174
01:04:32,100 --> 01:04:34,540
Speaker 10:  that. You know, like we don't have plans to put stuff on the subscriptions

1175
01:04:34,600 --> 01:04:38,500
Speaker 10:  as well. And they were like, yeah, but if you had an agreement and

1176
01:04:38,500 --> 01:04:41,300
Speaker 10:  then they brought up some evidence and and tried to sort of sway it that

1177
01:04:41,300 --> 01:04:44,820
Speaker 10:  way. And it's, it's clear obviously that he probably would, cuz we had Sarah Bond

1178
01:04:44,930 --> 01:04:48,700
Speaker 10:  testify on, was it Thursday, but he'd been pushing, specifically

1179
01:04:48,700 --> 01:04:52,660
Speaker 10:  pushing Microsoft just before the export series X launch to not use the

1180
01:04:52,660 --> 01:04:56,540
Speaker 10:  dev kits to on the export side unless they sort of agreed to

1181
01:04:56,540 --> 01:05:00,460
Speaker 10:  pay less or more revenue to Activision unless

1182
01:05:00,530 --> 01:05:03,980
Speaker 10:  less the exports keep. So there's obviously when he can get an agreement

1183
01:05:04,000 --> 01:05:05,540
Speaker 10:  out of a company, he'll do it.

1184
01:05:05,640 --> 01:05:09,460
Speaker 6:  But like is that revelatory I mean if, if I called Bobby

1185
01:05:09,480 --> 01:05:13,260
Speaker 6:  Codick and said I will give you $1 trillion to just

1186
01:05:13,480 --> 01:05:17,260
Speaker 6:  put Call of Duty on my computer specifically, like they do it, right? Like

1187
01:05:17,260 --> 01:05:21,060
Speaker 6:  yeah this is, we're gonna make good business decisions. It's like not

1188
01:05:21,180 --> 01:05:23,980
Speaker 6:  a surprising statement to me. And I think,

1189
01:05:24,160 --> 01:05:27,380
Speaker 10:  But I think where where it matters though is that in, in Microsoft like

1190
01:05:27,660 --> 01:05:30,780
Speaker 10:  document where they have like their like back and forth of like, why, why

1191
01:05:30,780 --> 01:05:34,460
Speaker 10:  this is good is is one of the things is like Call of Duty will be available

1192
01:05:34,520 --> 01:05:37,820
Speaker 10:  to like a hundred more million more people and that's because of the cloud,

1193
01:05:37,820 --> 01:05:41,660
Speaker 10:  right? And Call of Duty would be available on Switch. But The FTC is saying,

1194
01:05:41,660 --> 01:05:44,900
Speaker 10:  well but these things would happen anyway because you'd, you'd commercially,

1195
01:05:44,900 --> 01:05:48,540
Speaker 10:  like the market would change and then you'd commercially make that deal.

1196
01:05:48,600 --> 01:05:51,780
Speaker 10:  So why, why does Mikes have to have to buy you to for that to happen? you

1197
01:05:51,780 --> 01:05:55,420
Speaker 10:  know? And Bobby Co is saying, bye, I hate this stuff. And it's like, yeah,

1198
01:05:55,420 --> 01:05:58,820
Speaker 10:  yeah, he might hate it now, but would you hate it in five years? Like that's,

1199
01:05:58,820 --> 01:06:01,500
Speaker 10:  that's what The FTC is asking really. And that, that kind of comes down

1200
01:06:01,500 --> 01:06:03,420
Speaker 10:  to the very like crux of this case as well.

1201
01:06:03,750 --> 01:06:07,740
Speaker 6:  Addie, what do you make of all of that? Is there is, is there anything I,

1202
01:06:07,740 --> 01:06:10,540
Speaker 6:  I feel like to some extent everybody's just talking in circles here. Like

1203
01:06:10,540 --> 01:06:13,540
Speaker 6:  did you pull anything out of what Kodak was saying that struck you?

1204
01:06:13,970 --> 01:06:17,500
Speaker 11:  It's really hard for me to tell because it's really, I don't feel like I

1205
01:06:17,500 --> 01:06:21,460
Speaker 11:  have a bead on how sympathetic the judge is toward either side

1206
01:06:21,460 --> 01:06:25,180
Speaker 11:  here. Like the thing that I keep comparing this to is

1207
01:06:25,570 --> 01:06:29,060
Speaker 11:  Epic versus Apple where there were a bunch of really clear problems on the

1208
01:06:29,060 --> 01:06:32,860
Speaker 11:  table, a bunch of really clear technical questions. A bunch of, okay well

1209
01:06:32,860 --> 01:06:36,820
Speaker 11:  how much of a big deal is it if iOS has to open up, how

1210
01:06:36,820 --> 01:06:40,460
Speaker 11:  much of a security risk is that here? A bunch of it just seems to be them

1211
01:06:40,460 --> 01:06:44,380
Speaker 11:  grilling executives and talking about would you make these

1212
01:06:44,380 --> 01:06:48,220
Speaker 11:  deals if you weren't part of Microsoft A

1213
01:06:48,220 --> 01:06:51,500
Speaker 11:  And it's all pertinent like this is what antitrust law is supposed to be

1214
01:06:51,500 --> 01:06:55,140
Speaker 11:  about, is about whether it's going to foreclose competition. But it makes

1215
01:06:55,140 --> 01:06:58,820
Speaker 11:  it really hard for me to tell what we should be looking for when an

1216
01:06:58,820 --> 01:07:00,300
Speaker 11:  actual decision comes down.

1217
01:07:01,120 --> 01:07:04,940
Speaker 7:  How often is this the case with these antitrust hearings where it's just

1218
01:07:04,940 --> 01:07:08,180
Speaker 7:  like a judge being like, I'm gonna give you a billion hypotheticals. Tell

1219
01:07:08,180 --> 01:07:11,580
Speaker 7:  me what you think. Cuz that just seems like wild as a way to like

1220
01:07:11,960 --> 01:07:14,500
Speaker 7:  decide if a company can be purchased or not.

1221
01:07:15,100 --> 01:07:18,860
Speaker 11:  I mean that's gotta be what kind of what it always will boil down to eventually.

1222
01:07:19,320 --> 01:07:23,060
Speaker 11:  But a bunch of this just, there aren't the kind of very, very clear

1223
01:07:23,250 --> 01:07:27,100
Speaker 11:  hypotheticals. I think that there were in say Epic

1224
01:07:27,100 --> 01:07:30,860
Speaker 11:  versus Apple, which was just these very discreet technical, alright,

1225
01:07:31,080 --> 01:07:35,020
Speaker 11:  is this thing going to work on this platform? Are you going to charge

1226
01:07:35,020 --> 01:07:38,940
Speaker 11:  this amount of money for it? And this is much more about how, what

1227
01:07:38,940 --> 01:07:42,660
Speaker 11:  does this mean for the larger state of the games industry? Is it

1228
01:07:42,660 --> 01:07:46,020
Speaker 11:  going to change the way that companies make deals with each other?

1229
01:07:46,490 --> 01:07:50,260
Speaker 11:  What does it mean for these like 10 years down the road market

1230
01:07:50,320 --> 01:07:50,740
Speaker 11:  shares?

1231
01:07:51,120 --> 01:07:54,780
Speaker 6:  It also seems like, Tom jog my memory on this a bit. There was a point in

1232
01:07:54,780 --> 01:07:58,580
Speaker 6:  the hearing where the judge got really annoyed with the line of questioning

1233
01:07:58,580 --> 01:08:01,700
Speaker 6:  about basically like, will you commit to putting this game on PlayStation?

1234
01:08:02,370 --> 01:08:05,300
Speaker 6:  Just like over and over and over. The lawyers are, and I think it was when

1235
01:08:05,300 --> 01:08:07,060
Speaker 6:  Phil Spencer was on the stand, but maybe I'm wrong.

1236
01:08:07,160 --> 01:08:07,900
Speaker 10:  It was, yeah,

1237
01:08:08,050 --> 01:08:10,740
Speaker 6:  That was one of the moments, that was one of the few moments I think we've

1238
01:08:10,740 --> 01:08:14,140
Speaker 6:  seen where the judge was basically like just shut up with this line of questioning.

1239
01:08:14,160 --> 01:08:17,180
Speaker 6:  And so I think it seems like the lawyers have had to then kind of go another

1240
01:08:17,180 --> 01:08:20,940
Speaker 6:  way and be like, okay, how do we ask essentially that

1241
01:08:21,180 --> 01:08:24,540
Speaker 6:  question with different enough words that the judge is not gonna like throw

1242
01:08:24,540 --> 01:08:25,340
Speaker 6:  us outta the courtroom.

1243
01:08:25,600 --> 01:08:29,020
Speaker 11:  And the point being that they just can't like that, right? You can't

1244
01:08:29,330 --> 01:08:32,980
Speaker 11:  make that promise. That's an impossible promise. And The FTC is like, well

1245
01:08:32,980 --> 01:08:33,860
Speaker 11:  yes, obviously.

1246
01:08:34,050 --> 01:08:37,900
Speaker 10:  Yeah. And, and I think that frustration has happened multiple times and

1247
01:08:37,900 --> 01:08:41,580
Speaker 10:  it usually is to do with the FTCs questioning or whoever's

1248
01:08:41,580 --> 01:08:45,100
Speaker 10:  answering the Ft FTCs questioning because obviously when Microsoft's up

1249
01:08:45,100 --> 01:08:48,260
Speaker 10:  there, they're just letting the, the witness just talk away. you know, like

1250
01:08:48,480 --> 01:08:52,060
Speaker 10:  you, you make your case. Whereas The FTC is very pointed

1251
01:08:52,440 --> 01:08:56,100
Speaker 10:  and some of their questioning can be irritating to witnesses. Right?

1252
01:08:56,240 --> 01:09:00,100
Speaker 10:  So that particular example was interesting because she

1253
01:09:00,160 --> 01:09:03,980
Speaker 10:  cut in, she intervened, she's done that, she did that quite a lot in at

1254
01:09:03,980 --> 01:09:07,020
Speaker 10:  the end of that Phil Spencer testimony and then she actually cut the testimony

1255
01:09:07,020 --> 01:09:10,540
Speaker 10:  off I mean they were kind of at time anyway, but I think she was just like,

1256
01:09:11,130 --> 01:09:14,540
Speaker 10:  okay, you're done, you know, your question is done, your time's done. But

1257
01:09:14,540 --> 01:09:18,300
Speaker 10:  yeah, she has, she has jumped in a bunch. She jumped in with some of the

1258
01:09:18,300 --> 01:09:22,180
Speaker 10:  expert testimony from, from yesterday and

1259
01:09:22,180 --> 01:09:25,820
Speaker 10:  the day before just to sort of like get a grip on it. And I think it's

1260
01:09:26,100 --> 01:09:29,940
Speaker 10:  interesting to, to watch when she does cut in because she wants to

1261
01:09:29,940 --> 01:09:33,820
Speaker 10:  know predominantly from what I've seen, she wants to know about Call of

1262
01:09:33,820 --> 01:09:37,180
Speaker 10:  Duty because that's obviously been the big thing here.

1263
01:09:37,780 --> 01:09:41,260
Speaker 10:  So she's always jumping in to be like, well like she asked Bobby Kotek like,

1264
01:09:41,260 --> 01:09:45,020
Speaker 10:  well why would someone buy, why would someone subscribe, sorry

1265
01:09:45,040 --> 01:09:48,940
Speaker 10:  to World of Warcraft and not just pay $70 for a game? So she's

1266
01:09:48,940 --> 01:09:52,380
Speaker 10:  trying to get an understanding of that dynamic in the market as well. And

1267
01:09:52,380 --> 01:09:55,180
Speaker 10:  it all comes obviously I think it all comes down to court of duty and I

1268
01:09:55,180 --> 01:09:58,980
Speaker 10:  think that's gonna come from what I can tell, that's gonna come become part

1269
01:09:58,980 --> 01:10:02,060
Speaker 10:  of her decision. That's that, that's cuz that's, that's the way The FTC

1270
01:10:02,060 --> 01:10:05,100
Speaker 10:  have framed it as well and Sony's framed it. So of course it's gonna be

1271
01:10:05,570 --> 01:10:06,700
Speaker 10:  part of her thinking.

1272
01:10:07,040 --> 01:10:10,060
Speaker 7:  That's just incredible to me. That Call of Duty is so

1273
01:10:10,650 --> 01:10:13,940
Speaker 7:  important now to antitrust law and not just

1274
01:10:14,560 --> 01:10:14,980
Speaker 7:  gamers.

1275
01:10:15,460 --> 01:10:18,460
Speaker 10:  I think it's just cuz it's like, and Jim Ryan was saying this the other

1276
01:10:18,460 --> 01:10:21,620
Speaker 10:  day that it is unique, right? Like they do pump out, we, we were talking

1277
01:10:21,620 --> 01:10:25,020
Speaker 10:  about this the other day as well, it, they pump out a new game every year.

1278
01:10:25,360 --> 01:10:29,300
Speaker 10:  So, and that's kind of unusual unless you're like a Madden or a fifa

1279
01:10:29,300 --> 01:10:31,300
Speaker 10:  which are just iterative stuff. So

1280
01:10:31,980 --> 01:10:35,620
Speaker 6:  I loved KO's answer to basically how do you put out a game every year

1281
01:10:36,260 --> 01:10:38,740
Speaker 6:  which amounted to him basically just being like, well there've been a lot

1282
01:10:38,740 --> 01:10:39,180
Speaker 6:  of wars.

1283
01:10:40,810 --> 01:10:41,540
Speaker 11:  Yeah, there's

1284
01:10:41,800 --> 01:10:43,180
Speaker 6:  So many for us to choose from.

1285
01:10:43,950 --> 01:10:44,300
Speaker 7:  Sorry.

1286
01:10:44,930 --> 01:10:47,860
Speaker 11:  Yeah. Which is funnier because they're not even than half of 'em in their,

1287
01:10:48,080 --> 01:10:49,340
Speaker 11:  in the future now. Well

1288
01:10:49,340 --> 01:10:51,580
Speaker 6:  And the other half are World War ii like what are we doing here?

1289
01:10:52,410 --> 01:10:53,020
Speaker 10:  They run out.

1290
01:10:53,730 --> 01:10:57,180
Speaker 6:  Yeah. Yeah we did all the wars already. I guess who knows what's gonna happen

1291
01:10:57,180 --> 01:11:00,420
Speaker 6:  next. But I do think you're right that it seems like this came back over

1292
01:11:00,420 --> 01:11:04,380
Speaker 6:  and over and over too, like how Big Call of Duty is and how much it

1293
01:11:04,380 --> 01:11:08,260
Speaker 6:  matters to the game industry and also it seems to be

1294
01:11:08,260 --> 01:11:11,140
Speaker 6:  the kind of thing that you can argue fairly successfully from both sides.

1295
01:11:11,140 --> 01:11:14,500
Speaker 6:  Whereas The FTC is like, well we can't just have this one company owned Call

1296
01:11:14,500 --> 01:11:18,340
Speaker 6:  of Duty. It'll give it too much sway to the game industry. And then both

1297
01:11:18,340 --> 01:11:22,220
Speaker 6:  Activision and Microsoft come out and say basically no

1298
01:11:22,290 --> 01:11:26,140
Speaker 6:  Call of Duty is so big that we would be ridiculous to foreclose on the rest

1299
01:11:26,140 --> 01:11:29,940
Speaker 6:  of the game industry. There's too much money people would revolt. It's

1300
01:11:29,940 --> 01:11:33,580
Speaker 6:  like, it's so big that it's kind of a winning argument on

1301
01:11:33,650 --> 01:11:35,620
Speaker 6:  both sides in a strange way

1302
01:11:35,720 --> 01:11:37,860
Speaker 11:  Is Call of Duty too big to fail?

1303
01:11:39,840 --> 01:11:43,380
Speaker 11:  That's the rare correct use of too big to fail. Yes, yes.

1304
01:11:45,610 --> 01:11:49,540
Speaker 11:  Call of Duty is such as important institution that we're it to fail, we

1305
01:11:49,540 --> 01:11:52,980
Speaker 11:  must do anything in our power to prop it up because it would be catastrophic

1306
01:11:52,980 --> 01:11:53,620
Speaker 11:  to society

1307
01:11:54,920 --> 01:11:57,740
Speaker 6:  How we'd have to go back to doing real wars if we didn't have Call of Duty

1308
01:11:57,740 --> 01:12:01,540
Speaker 6:  to play. Alright, so any, anything else Addie that jumped out to you from,

1309
01:12:01,540 --> 01:12:03,820
Speaker 6:  from Bobby before we get to Sachin Nadella? I

1310
01:12:03,820 --> 01:12:07,420
Speaker 11:  Think that the part where the judge did jump in and say, well alright, clearly

1311
01:12:07,600 --> 01:12:11,100
Speaker 11:  if you say you regret not putting this thing on Switch, you don't need Microsoft

1312
01:12:11,120 --> 01:12:14,140
Speaker 11:  to come and like make you make this thing cross-platform. Like you're going

1313
01:12:14,140 --> 01:12:17,180
Speaker 11:  to be making dec decisions in your own rational self-interest.

1314
01:12:18,040 --> 01:12:21,460
Speaker 11:  And that seems like another sort of little bit of a hint.

1315
01:12:22,200 --> 01:12:26,100
Speaker 11:  But otherwise, yeah, I, I think we've, he talked for a very, very long time

1316
01:12:26,100 --> 01:12:28,820
Speaker 11:  and we learned a lot about Call of Duty and the Switch.

1317
01:12:29,290 --> 01:12:32,260
Speaker 6:  Yeah, I did think it was interesting that he said he thought the Switch was

1318
01:12:32,460 --> 01:12:36,300
Speaker 6:  probably the second most successful game console ever. Assuming

1319
01:12:36,360 --> 01:12:39,620
Speaker 6:  the first one is the we, which he's also spoken very highly of in the past.

1320
01:12:39,890 --> 01:12:40,180
Speaker 6:  Yeah.

1321
01:12:40,180 --> 01:12:43,900
Speaker 10:  He regretted not bringing Call of Duty to the Switch. Right. So that's why

1322
01:12:43,900 --> 01:12:47,020
Speaker 10:  the Judge Cayenne as well. Cause she was like, well you know, you regretted

1323
01:12:47,020 --> 01:12:50,340
Speaker 10:  it so you of course she'd bring it to this next generation Nintendo console.

1324
01:12:50,600 --> 01:12:50,820
Speaker 10:  So

1325
01:12:50,930 --> 01:12:54,860
Speaker 6:  Yeah. Yeah, he had a good line about seeing the thing and it was like, Nintendo

1326
01:12:54,860 --> 01:12:57,820
Speaker 6:  can't possibly pull off all of the stuff it's trying to do, which I think

1327
01:12:57,820 --> 01:13:00,340
Speaker 6:  is how a lot of people felt about the switch. And then just slowly but truly

1328
01:13:00,340 --> 01:13:03,540
Speaker 6:  it was like, oh no, this, they did the thing, this works. Yeah, it's, it's

1329
01:13:03,540 --> 01:13:07,460
Speaker 6:  a Zelda machine. Here we go. Alright, let's, let's switch to Satya

1330
01:13:07,460 --> 01:13:11,340
Speaker 6:  Nadella who came on later. Tom run us through the greatest hits there. What,

1331
01:13:11,340 --> 01:13:12,180
Speaker 6:  what did we learn from him?

1332
01:13:12,530 --> 01:13:16,220
Speaker 10:  Yeah, so he, he kind of started off it was The

1333
01:13:16,400 --> 01:13:20,300
Speaker 10:  FTC question Him, him at the beginning. So he started off with

1334
01:13:20,570 --> 01:13:24,340
Speaker 10:  some of the stuff to do with the cloud market. So he started off very quietly

1335
01:13:24,340 --> 01:13:27,940
Speaker 10:  as well. Like, he was very Oh really? Yeah, very quiet. I think Bobbie did

1336
01:13:27,940 --> 01:13:30,180
Speaker 10:  as well. But Nadella was pretty, pretty quiet.

1337
01:13:30,210 --> 01:13:31,780
Speaker 6:  He's like, well Microsoft is a company.

1338
01:13:32,080 --> 01:13:35,460
Speaker 10:  He was like, yes. And usually when you'd answer, if you ask ask Nadella

1339
01:13:35,460 --> 01:13:37,660
Speaker 10:  a question, you'll answer like for an hour. Yeah, he's

1340
01:13:37,780 --> 01:13:39,740
Speaker 6:  A pretty like boisterous guy. Yeah. Yeah.

1341
01:13:39,740 --> 01:13:43,580
Speaker 10:  So that was, that was kind of unusual. But yeah, he started off like some

1342
01:13:43,580 --> 01:13:47,140
Speaker 10:  exports market share stuff. The FTC dragged up some of the statements about

1343
01:13:47,140 --> 01:13:50,780
Speaker 10:  how they, they were winning in North America for like three months

1344
01:13:50,890 --> 01:13:54,660
Speaker 10:  when Sony didn't have good PlayStation shipments. So it was a bit weird,

1345
01:13:54,760 --> 01:13:58,300
Speaker 10:  but, but they brought that into the record then they talked about cloud

1346
01:13:58,300 --> 01:14:02,220
Speaker 10:  gaming and Nadella said that he thought that exports

1347
01:14:02,220 --> 01:14:05,460
Speaker 10:  live was part of that and that streaming was a minor part of that. I'm like,

1348
01:14:06,280 --> 01:14:09,660
Speaker 10:  mm. I wouldn't say, I don't know like I mean

1349
01:14:10,150 --> 01:14:14,140
Speaker 10:  kinda, but I get where he's coming from but that's not

1350
01:14:14,300 --> 01:14:18,180
Speaker 10:  strictly true. And then they were like, well so you think that, do you

1351
01:14:18,400 --> 01:14:22,260
Speaker 10:  The FTC was like, okay well we're gonna bring up all these emails where

1352
01:14:22,260 --> 01:14:25,860
Speaker 10:  you've said you wanna take cloud mainstream. So then, then it was like,

1353
01:14:26,050 --> 01:14:30,020
Speaker 10:  okay so one of the emails was Microsoft's I

1354
01:14:30,020 --> 01:14:33,500
Speaker 10:  think leadership team in Nadella. They were talking to Meta or talking about

1355
01:14:33,500 --> 01:14:37,420
Speaker 10:  meta the, the event where they had, they had META'S new headset

1356
01:14:37,630 --> 01:14:37,980
Speaker 10:  which

1357
01:14:37,980 --> 01:14:40,060
Speaker 6:  Had a bunch of Microsoft integrations that came with

1358
01:14:40,060 --> 01:14:40,580
Speaker 10:  It. Right? Yeah.

1359
01:14:40,650 --> 01:14:44,420
Speaker 11:  They had that, that past tense is awfully generous. I believe

1360
01:14:44,420 --> 01:14:46,420
Speaker 11:  they've still not, I'm not sure they've rolled it all of it out.

1361
01:14:46,530 --> 01:14:49,620
Speaker 6:  That is fair. Which which said words about

1362
01:14:50,300 --> 01:14:50,980
Speaker 6:  Microsoft products.

1363
01:14:51,390 --> 01:14:54,780
Speaker 10:  Let's hear some stuff, some made some promises. So hasn't happened. Yeah.

1364
01:14:54,780 --> 01:14:57,740
Speaker 10:  Which actually happens weirdly a lot with Meta and Microsoft. They were

1365
01:14:57,820 --> 01:15:01,220
Speaker 10:  supposed to do some experts cloud gaming stuff on Facebook, but that never

1366
01:15:01,420 --> 01:15:05,140
Speaker 10:  happened. So weird. But yeah, so they were talking, discussing amongst themselves

1367
01:15:05,140 --> 01:15:08,380
Speaker 10:  about this event and saying Nadella was basically saying look, this is how

1368
01:15:08,380 --> 01:15:10,980
Speaker 10:  we want the world to perceive us. That we're like, you know, the leaders

1369
01:15:11,040 --> 01:15:15,020
Speaker 10:  in cloud essentially we're going out with teams and X Cloud and

1370
01:15:15,360 --> 01:15:19,020
Speaker 10:  and everything else. I want us to push on on cloud here. So that

1371
01:15:19,360 --> 01:15:22,460
Speaker 10:  FC is obviously using that to say, look, cloud is clearly important to,

1372
01:15:22,460 --> 01:15:25,980
Speaker 10:  to Microsoft and that this is part of your thinking for this deal. Cause

1373
01:15:25,980 --> 01:15:28,300
Speaker 10:  that was in October I think around about then.

1374
01:15:28,720 --> 01:15:32,540
Speaker 7:  But since then they've pulled back from cloud, right?

1375
01:15:33,180 --> 01:15:35,900
Speaker 10:  Apparently. Yeah. I think they've publicly have,

1376
01:15:36,080 --> 01:15:38,780
Speaker 6:  So they say in this particular trial, yeah.

1377
01:15:39,020 --> 01:15:42,580
Speaker 10:  Yeah. But behind the scenes they just hit Paul's buttons like Right, we're

1378
01:15:42,580 --> 01:15:43,740
Speaker 10:  not doing emails no more.

1379
01:15:44,330 --> 01:15:47,500
Speaker 7:  Well good. No cause I mean, I think even on our side of things, when we were

1380
01:15:47,660 --> 01:15:50,780
Speaker 7:  covering this like slow pullback from cloud, we were like, is this because

1381
01:15:50,800 --> 01:15:54,660
Speaker 7:  of AI and like they're now going all in on ai. Yeah. Or is it more

1382
01:15:54,880 --> 01:15:58,780
Speaker 7:  likely that it was because they were about to go to like have to

1383
01:15:58,780 --> 01:16:00,300
Speaker 7:  prove that they could buy Activision.

1384
01:16:00,840 --> 01:16:04,540
Speaker 10:  So the cma, the UK regulator was fine with the call of GE

1385
01:16:04,550 --> 01:16:08,300
Speaker 10:  stuff. The EU was fine with the call of GE stuff, but they both weren't

1386
01:16:08,300 --> 01:16:11,740
Speaker 10:  fine with the cloud stuff. The EU worked out a remedy

1387
01:16:11,740 --> 01:16:15,460
Speaker 10:  essentially with Microsoft for some, some licensing stuff to allow cloud

1388
01:16:15,460 --> 01:16:19,260
Speaker 10:  competitors to just automatically get their games and stuff. The CMA

1389
01:16:19,260 --> 01:16:21,700
Speaker 10:  were like, no, we're not interested in that sort of stuff. We're gonna block

1390
01:16:21,700 --> 01:16:25,460
Speaker 10:  you deal. So, so they've definitely obviously been talking to those regulators

1391
01:16:25,490 --> 01:16:28,780
Speaker 10:  over the months and, and gradually pulled back. They probably pulled back

1392
01:16:28,780 --> 01:16:32,660
Speaker 10:  when they first got that set of like details from these regulators to

1393
01:16:32,660 --> 01:16:35,300
Speaker 10:  say this, this is what we're concerned about. So they probably thought let's

1394
01:16:35,300 --> 01:16:35,420
Speaker 10:  pull.

1395
01:16:35,880 --> 01:16:38,940
Speaker 7:  Is that more likely why that cloud console never came out?

1396
01:16:39,300 --> 01:16:42,140
Speaker 10:  I wouldn't be surprised if it's, if it's linked, I think, I think it's probably

1397
01:16:42,140 --> 01:16:44,860
Speaker 10:  half and half impacted by it. Yeah. Yeah. It's definitely impacted. Yeah.

1398
01:16:45,110 --> 01:16:48,540
Speaker 10:  Cause they were very close to launching that so it was, it was like close

1399
01:16:48,560 --> 01:16:51,220
Speaker 10:  as in they'd made them and, and all that sort of stuff. Yeah.

1400
01:16:51,220 --> 01:16:55,140
Speaker 6:  It's also, you've gotta figure some lawyer at Microsoft was like, hey, if

1401
01:16:55,140 --> 01:16:58,740
Speaker 6:  we have a box that is called the Xbox cloud, the argument

1402
01:16:58,740 --> 01:17:01,980
Speaker 6:  you're about to try to make in court is going to get a lot more complicated.

1403
01:17:02,410 --> 01:17:05,980
Speaker 6:  Yeah. But I do think I mean all these things can be true at the same time.

1404
01:17:05,980 --> 01:17:08,700
Speaker 6:  Right? Like the, the AI thing happened really fast and Microsoft clearly

1405
01:17:08,700 --> 01:17:12,660
Speaker 6:  pushed resources at it. The business of cloud gaming has very much not

1406
01:17:12,660 --> 01:17:16,300
Speaker 6:  been solved. Publishers do seem to have, if not

1407
01:17:16,300 --> 01:17:20,180
Speaker 6:  problems with it then at least questions about it. And there's this big regulatory

1408
01:17:20,180 --> 01:17:23,260
Speaker 6:  fight. So I think it, it would not be hard for Microsoft to kind of read

1409
01:17:23,280 --> 01:17:26,940
Speaker 6:  the room and realize, okay, we're still betting on this big in the long run

1410
01:17:27,080 --> 01:17:30,780
Speaker 6:  but maybe this is a moment for us to like slow the pace down just a

1411
01:17:30,900 --> 01:17:33,860
Speaker 6:  smidge. And I don't think that's an unreasonable thing to have done in this

1412
01:17:33,860 --> 01:17:34,420
Speaker 6:  moment at all.

1413
01:17:34,760 --> 01:17:38,220
Speaker 10:  No, I think it makes sense, right? Like we, we, we were talking the other

1414
01:17:38,220 --> 01:17:41,340
Speaker 10:  day David, about some of the other stuff that's, that's been missing over

1415
01:17:41,340 --> 01:17:43,820
Speaker 10:  the past year with the cloud stuff. They made like all these commitments

1416
01:17:43,960 --> 01:17:47,900
Speaker 10:  to bring its TVs and to bring your cloud

1417
01:17:47,900 --> 01:17:50,460
Speaker 10:  library and all that sort of stuff. And that stuff hasn't happened. So there's

1418
01:17:50,460 --> 01:17:53,540
Speaker 10:  obviously been some stuff going on behind the scenes. So, but yeah, so that

1419
01:17:53,540 --> 01:17:57,500
Speaker 10:  was like the, the cloud section of Mandela's testimony.

1420
01:17:57,680 --> 01:18:01,660
Speaker 10:  So I think it's clear he is all about cloud cuz he's all about

1421
01:18:01,660 --> 01:18:04,540
Speaker 10:  cloud in all of Microsoft's businesses. So he's definitely all about cloud

1422
01:18:04,540 --> 01:18:04,900
Speaker 10:  and gaming.

1423
01:18:05,010 --> 01:18:08,220
Speaker 6:  Yeah, I was gonna say anyone who has paid attention to him speak the last

1424
01:18:08,220 --> 01:18:09,820
Speaker 6:  several years can tell you that.

1425
01:18:10,210 --> 01:18:14,140
Speaker 10:  Yeah, exactly. And then we moved on to, I wouldn't say this is

1426
01:18:14,140 --> 01:18:17,980
Speaker 10:  a bombshell, but it was just a kind of funny thing he said was that he would

1427
01:18:17,980 --> 01:18:21,740
Speaker 10:  get rid of exclusives in gaming if he could. But the market leader,

1428
01:18:21,990 --> 01:18:25,140
Speaker 10:  which will assume is Sony here and not Nintendo

1429
01:18:26,090 --> 01:18:29,140
Speaker 10:  even though Nintendo's been doing exclusives for, for quite some time as

1430
01:18:29,140 --> 01:18:33,060
Speaker 10:  well. So you could argue between yourselves who that refers to, but I

1431
01:18:33,060 --> 01:18:36,500
Speaker 10:  think it's Sony cause I don't think Microsoft's trying to bury Nintendo

1432
01:18:36,500 --> 01:18:38,260
Speaker 10:  in this case. They're definitely trying to bury Sony.

1433
01:18:38,600 --> 01:18:42,180
Speaker 6:  No, Microsoft is trying to remind you how terrific Nintendo is. That's Microsoft's

1434
01:18:42,180 --> 01:18:43,300
Speaker 6:  whole endgame here. Exactly.

1435
01:18:43,610 --> 01:18:47,580
Speaker 10:  Yeah. So he said he wanted to end those exclusives, but you know,

1436
01:18:47,580 --> 01:18:49,620
Speaker 10:  the market leader doesn't let us do that.

1437
01:18:49,880 --> 01:18:52,060
Speaker 11:  God poor downtrodden Microsoft. Poor

1438
01:18:52,340 --> 01:18:52,620
Speaker 10:  Microsoft.

1439
01:18:54,340 --> 01:18:57,620
Speaker 10:  Yeah. And, and so that, that was that and obviously you don't think get

1440
01:18:57,780 --> 01:19:01,140
Speaker 10:  questioned cuz it was under Microsoft's lawyers questioning him. So he,

1441
01:19:01,200 --> 01:19:04,420
Speaker 10:  he was free to just say stuff like that and then he

1442
01:19:04,770 --> 01:19:08,660
Speaker 10:  committed again to shipping Call of Duty on PlayStation.

1443
01:19:09,460 --> 01:19:13,340
Speaker 10:  So similar to not as strong as Phil Spencer's sort of oath,

1444
01:19:13,730 --> 01:19:17,020
Speaker 10:  I'll put my hand up sort of thing. But you know, it was still significant

1445
01:19:17,020 --> 01:19:20,980
Speaker 10:  that the CEOs saying it as well. And we also didn't get any back and forth

1446
01:19:20,980 --> 01:19:24,900
Speaker 10:  with The FTC questioning him again to see if, if he would

1447
01:19:25,000 --> 01:19:28,820
Speaker 10:  put Diablo on there, all the other questions that we The FTC fired

1448
01:19:28,880 --> 01:19:32,780
Speaker 10:  at Spencer and then that was, that was kind of most of his testimony really.

1449
01:19:32,840 --> 01:19:36,820
Speaker 10:  So I, but like I say, Like, he got off lightly. He really did.

1450
01:19:37,160 --> 01:19:38,500
Speaker 11:  It was quieter than I expected.

1451
01:19:38,770 --> 01:19:42,220
Speaker 6:  Yeah, I was just gonna ask, were you surprised Addie at, at kind of how

1452
01:19:42,410 --> 01:19:45,540
Speaker 6:  lightly both CEOs were treated? Tom mentioned being surprised. Were you

1453
01:19:46,060 --> 01:19:49,220
Speaker 11:  I was a little bit, and I I mean, I'm wondering how much of it is that the

1454
01:19:49,220 --> 01:19:52,260
Speaker 11:  people who went earlier just popping up all over the evidence, they're in

1455
01:19:52,260 --> 01:19:55,860
Speaker 11:  all of these email chains that they get to asked about Jim Ryan

1456
01:19:56,110 --> 01:19:59,980
Speaker 11:  while not there was, you know, there in spirit. These are all of these

1457
01:19:59,980 --> 01:20:02,700
Speaker 11:  people who have been saying all of these really contentious things about

1458
01:20:02,700 --> 01:20:06,420
Speaker 11:  each other on these first two days. And now like Satya Nadel is not

1459
01:20:06,610 --> 01:20:10,340
Speaker 11:  primarily an Xbox guy. He is a guy who goes and sits back and

1460
01:20:10,340 --> 01:20:14,260
Speaker 11:  manages the rest of Microsoft and he's there because he owns Microsoft,

1461
01:20:14,680 --> 01:20:17,860
Speaker 11:  but he's just not knee deep in these decisions the way that a bunch of the

1462
01:20:17,860 --> 01:20:18,940
Speaker 11:  people from the early days were.

1463
01:20:19,130 --> 01:20:22,260
Speaker 6:  Yeah, it's a good reminder that CEOs mostly don't actually know what's going

1464
01:20:22,280 --> 01:20:23,460
Speaker 6:  on inside of their company.

1465
01:20:24,920 --> 01:20:28,700
Speaker 10:  That's very true. Yeah. And I think that's probably why they got off lightly.

1466
01:20:28,700 --> 01:20:32,500
Speaker 10:  Right. So, but but, but I swear in other cases, like federal

1467
01:20:32,500 --> 01:20:36,300
Speaker 10:  cases, we usually see judges going out out CEOs. so like

1468
01:20:36,330 --> 01:20:38,020
Speaker 10:  meta and I know it was strange.

1469
01:20:38,130 --> 01:20:41,700
Speaker 6:  Fair enough. All right. Well the, the newsiest and funniest and

1470
01:20:41,890 --> 01:20:45,540
Speaker 6:  best and most interesting thing that happened yesterday was,

1471
01:20:45,840 --> 01:20:49,540
Speaker 6:  was this document that came out from Sony

1472
01:20:49,570 --> 01:20:52,300
Speaker 6:  that was supposed to be Addie. You're the one who figured out what was going

1473
01:20:52,300 --> 01:20:55,180
Speaker 6:  on here. So e explain what happened somehow it just ended up in our Slack

1474
01:20:55,240 --> 01:20:58,900
Speaker 6:  and like everyone mobilized to figure out what to do about this terribly

1475
01:20:59,060 --> 01:21:00,660
Speaker 6:  redacted document, what happened here.

1476
01:21:01,280 --> 01:21:04,980
Speaker 11:  So Sony at some point filed a sworn testimony. It's like,

1477
01:21:04,980 --> 01:21:08,820
Speaker 11:  here's our statement of facts pertinent. And at some point

1478
01:21:08,930 --> 01:21:12,740
Speaker 11:  this was brought up in court and the courts agreed there were going to be

1479
01:21:12,740 --> 01:21:16,020
Speaker 11:  portions of it that were sealed. These were like financial details largely.

1480
01:21:16,600 --> 01:21:19,980
Speaker 11:  And there various ways that you can redact things.

1481
01:21:20,920 --> 01:21:24,500
Speaker 11:  You can just paste black bars over them. They did not do this as far as

1482
01:21:24,500 --> 01:21:27,860
Speaker 11:  we can tell. They went over it with an actual pen, which

1483
01:21:28,370 --> 01:21:31,500
Speaker 11:  left just enough of the document, original text,

1484
01:21:32,030 --> 01:21:35,940
Speaker 11:  still visible with the, like there was imagine drawing a sharpie

1485
01:21:35,940 --> 01:21:39,780
Speaker 11:  over a piece of printed text that if you

1486
01:21:39,820 --> 01:21:43,740
Speaker 11:  messed with the gamma from the pdf you could actually find out all

1487
01:21:43,740 --> 01:21:47,580
Speaker 11:  of these top secret things. We don't know who did this,

1488
01:21:47,760 --> 01:21:48,740
Speaker 11:  we don't know who

1489
01:21:49,110 --> 01:21:52,820
Speaker 7:  Their penmanship was impeccable though, right? Like, like the

1490
01:21:52,820 --> 01:21:56,180
Speaker 7:  actual marking out was nearly perfect to the point where we're like, did

1491
01:21:56,180 --> 01:21:56,940
Speaker 7:  a computer do this?

1492
01:21:57,560 --> 01:22:01,260
Speaker 11:  See, I wanna know, I would love to hear from someone who is a listener who

1493
01:22:01,500 --> 01:22:04,660
Speaker 11:  actually knows how this stuff goes down. Because I don't know if someone

1494
01:22:04,720 --> 01:22:08,580
Speaker 11:  is just really amazing at handling a pen if there's some kind of semi-automated

1495
01:22:08,580 --> 01:22:08,860
Speaker 11:  process.

1496
01:22:09,730 --> 01:22:12,500
Speaker 6:  It's like you, you like take calligraphy for years and then you're like,

1497
01:22:12,500 --> 01:22:15,140
Speaker 6:  what am I gonna do with this professionally? And then you just, you just

1498
01:22:15,140 --> 01:22:16,420
Speaker 6:  get into redacting for a living.

1499
01:22:16,860 --> 01:22:20,100
Speaker 11:  I will say it is not the worst redaction I've seen there have been much,

1500
01:22:20,100 --> 01:22:22,300
Speaker 11:  much worse. Redaction That's definitely true. Including ones where you can

1501
01:22:22,300 --> 01:22:25,140
Speaker 11:  just highlight the text from behind it because they just put a black bar

1502
01:22:25,140 --> 01:22:29,060
Speaker 11:  over something that had already been scanned. So it's not

1503
01:22:29,060 --> 01:22:31,500
Speaker 11:  like, it's not that embarrassing, but it's not great. Yeah,

1504
01:22:31,500 --> 01:22:35,140
Speaker 10:  I like's think that they did it all like line by line with ruler and a Sharpie

1505
01:22:35,410 --> 01:22:36,540
Speaker 10:  like old school. Oh yeah.

1506
01:22:36,650 --> 01:22:39,220
Speaker 6:  Yeah. I hope so. It I mean it's, it sure looks like it

1507
01:22:39,220 --> 01:22:42,460
Speaker 10:  Because some of it is slightly off, but you can tell, you can tell what,

1508
01:22:42,460 --> 01:22:46,180
Speaker 10:  you can tell where the, the pen ends on the sides of it. So yeah.

1509
01:22:46,960 --> 01:22:50,780
Speaker 6:  Tom, we, we asked you to make us a list of everything we learned because

1510
01:22:50,780 --> 01:22:54,580
Speaker 6:  it was some of the most revelatory stuff from the whole trial. You wanna

1511
01:22:54,580 --> 01:22:55,660
Speaker 6:  give it to us? Give us the rundown?

1512
01:22:55,770 --> 01:22:58,380
Speaker 10:  Yeah, well firstly there's a bunch of stuff in there that people have been

1513
01:22:58,380 --> 01:23:01,820
Speaker 10:  tweeting about that we can't exactly see the figures. So we're not sure

1514
01:23:01,850 --> 01:23:05,530
Speaker 10:  it's like percentages and all sorts of stuff. But the stuff that we're fairly

1515
01:23:05,530 --> 01:23:08,780
Speaker 10:  sure on is that Horizon, forbidden West

1516
01:23:09,260 --> 01:23:12,940
Speaker 10:  apparently costs 212 million to,

1517
01:23:13,040 --> 01:23:17,020
Speaker 10:  to make over five years with 300 employees. The last of us

1518
01:23:17,020 --> 01:23:20,780
Speaker 10:  part two was 220 million and around 200

1519
01:23:20,980 --> 01:23:24,740
Speaker 10:  employees. And then Sony says that 1 million PlayStation shooting gamers

1520
01:23:24,740 --> 01:23:27,060
Speaker 10:  play nothing but Call of Duty, which is,

1521
01:23:27,060 --> 01:23:27,340
Speaker 6:  That's

1522
01:23:27,340 --> 01:23:31,180
Speaker 10:  Nuts. Interesting. So just literally Call of

1523
01:23:31,180 --> 01:23:35,020
Speaker 10:  Duty console. And then there's suggestions that in the, the

1524
01:23:35,220 --> 01:23:39,020
Speaker 10:  document that was around about 800 million in PlayStation revenue

1525
01:23:39,530 --> 01:23:42,460
Speaker 10:  just in the US alone during 2021 for Call of Duty. Just

1526
01:23:42,460 --> 01:23:42,980
Speaker 6:  For Call of Duty.

1527
01:23:43,160 --> 01:23:47,100
Speaker 10:  Wow. Yeah. And we think the document says 1.5 billion globally

1528
01:23:47,600 --> 01:23:51,460
Speaker 10:  in 2021 alone. And then when you count, so Sony

1529
01:23:51,490 --> 01:23:54,420
Speaker 10:  says like when you count accessories, subscriptions and everything else

1530
01:23:54,420 --> 01:23:58,340
Speaker 10:  that comes with that franchise, then this jumps up to what we

1531
01:23:58,340 --> 01:24:02,020
Speaker 10:  think is either 13.9 billion or 15.9 billion

1532
01:24:02,260 --> 01:24:03,860
Speaker 10:  a year. But either way it is

1533
01:24:04,360 --> 01:24:05,220
Speaker 7:  Too big to fail.

1534
01:24:05,880 --> 01:24:06,900
Speaker 10:  It is, it's

1535
01:24:06,920 --> 01:24:07,620
Speaker 7:  So much money.

1536
01:24:08,120 --> 01:24:09,020
Speaker 10:  Wow. There's a lot of

1537
01:24:09,020 --> 01:24:12,620
Speaker 6:  Money. There's the, this is why you have to have Call of Duty

1538
01:24:12,620 --> 01:24:16,140
Speaker 6:  everywhere thing like the Yes. Numbers don't lie on that front.

1539
01:24:16,690 --> 01:24:20,580
Speaker 10:  Yeah. Like it, it that, that should immediately say to

1540
01:24:20,580 --> 01:24:24,500
Speaker 10:  you it has to be on all platforms. Like, and Bobby Kotek said like why

1541
01:24:24,810 --> 01:24:27,980
Speaker 10:  that they asked him Yes. Yesterday. Like why wouldn't you make it exclusive?

1542
01:24:28,040 --> 01:24:28,260
Speaker 10:  And

1543
01:24:28,330 --> 01:24:32,260
Speaker 6:  Okay, here's my new theory is that somebody redacted this badly

1544
01:24:32,360 --> 01:24:36,020
Speaker 6:  on purpose hoping that everyone would see it because those numbers

1545
01:24:36,770 --> 01:24:40,260
Speaker 6:  tell such a story about why you would not make this game exclusive.

1546
01:24:41,060 --> 01:24:44,020
Speaker 6:  I have no evidence for this. There's no chance this is true, but this is

1547
01:24:44,020 --> 01:24:45,260
Speaker 6:  now my, my conspiracy.

1548
01:24:45,520 --> 01:24:49,460
Speaker 10:  So what you're saying is an Xbox fanboy in the court or in The

1549
01:24:49,720 --> 01:24:51,340
Speaker 10:  FTC or something that's, that's done this?

1550
01:24:52,380 --> 01:24:55,660
Speaker 6:  Absolutely. Potentially. Yes. Yes. Someone with great calligraphy skills

1551
01:24:55,960 --> 01:24:59,060
Speaker 6:  who loves Call of Duty on their Xbox.

1552
01:24:59,900 --> 01:25:01,900
Speaker 10:  I mean it's not beyond, you never know

1553
01:25:02,880 --> 01:25:04,900
Speaker 6:  If you email us Vergecast the british do com,

1554
01:25:05,520 --> 01:25:09,020
Speaker 7:  But that, that, that news about the last of us in Horizon forbidden rest.

1555
01:25:09,020 --> 01:25:12,260
Speaker 7:  That was kind of like big news too. Cause we usually don't know how much

1556
01:25:12,260 --> 01:25:15,620
Speaker 7:  these games cost to make. Like we get rough estimates from the outside.

1557
01:25:15,650 --> 01:25:19,460
Speaker 10:  Yeah. Rough estimate. Yeah. PE and I think it is roughly always around the

1558
01:25:19,460 --> 01:25:23,420
Speaker 10:  sort of 200 million mark really for, for those sort of expensive big

1559
01:25:23,420 --> 01:25:27,220
Speaker 10:  games. But it also wasn't just Sony that had redaction

1560
01:25:27,220 --> 01:25:30,460
Speaker 10:  problems in all of this. So, so obviously we got all that information out

1561
01:25:30,460 --> 01:25:34,380
Speaker 10:  there and that was like, oh cool. And then we realized, oh, hang on a

1562
01:25:34,380 --> 01:25:37,700
Speaker 10:  minute. Like the courts pulled all the documents on this folder that they

1563
01:25:37,700 --> 01:25:40,540
Speaker 10:  have apart from one. And I was like, car, let me have a look at this document.

1564
01:25:40,560 --> 01:25:44,260
Speaker 10:  It was the exports one that revealed that Microsoft was obvious going after

1565
01:25:44,260 --> 01:25:47,940
Speaker 10:  to Seger Bungee and a bunch of other studios. And they had redacted all

1566
01:25:47,940 --> 01:25:51,260
Speaker 10:  of that stuff in this new document Wow. Had been uploaded.

1567
01:25:51,760 --> 01:25:55,420
Speaker 10:  And then as the hours went by, that document disappeared. The link to the

1568
01:25:55,620 --> 01:25:58,420
Speaker 10:  documents disappeared from the court website and it's all just gone very

1569
01:25:58,420 --> 01:26:01,900
Speaker 10:  quiet. So I'm hoping today that we hear

1570
01:26:02,050 --> 01:26:05,220
Speaker 10:  exactly what's going, going on there. Cause there hasn't been any filings

1571
01:26:05,220 --> 01:26:08,780
Speaker 10:  mentioned in it just yet. So, but it's obviously a bit of a mess. So,

1572
01:26:09,200 --> 01:26:12,460
Speaker 10:  and who knows when we're gonna see the other exhibits in this, in this case

1573
01:26:12,520 --> 01:26:12,740
Speaker 10:  now

1574
01:26:12,930 --> 01:26:15,620
Speaker 6:  It's just all gonna be redacted. They're just gonna be like, nevermind. You

1575
01:26:15,620 --> 01:26:15,900
Speaker 6:  don't get to

1576
01:26:15,900 --> 01:26:17,060
Speaker 10:  Read anything. Just a giant sharpie.

1577
01:26:18,370 --> 01:26:20,340
Speaker 11:  Just a picture of a Sharpie. Yeah.

1578
01:26:20,440 --> 01:26:24,020
Speaker 6:  All right, well speaking of, y'all need to get to this thing which starts

1579
01:26:24,020 --> 01:26:27,780
Speaker 6:  in just a few minutes. Addie, any, what are you looking for today? Any, anything

1580
01:26:27,780 --> 01:26:30,580
Speaker 6:  that's still sort of outstanding that you're curious about to see at

1581
01:26:30,580 --> 01:26:33,340
Speaker 11:  This point? The thing I'm most curious about is just what more questions

1582
01:26:33,340 --> 01:26:35,940
Speaker 11:  the judge has, especially because we're gonna have closing statements and

1583
01:26:35,940 --> 01:26:39,820
Speaker 11:  the closing statements. I believe she said earlier that's

1584
01:26:39,820 --> 01:26:43,180
Speaker 11:  not just going to be a one way. It's where she gets to kind of do her last

1585
01:26:43,180 --> 01:26:46,860
Speaker 11:  grilling of both sides. And so I'm hoping that will at least give us a little

1586
01:26:46,860 --> 01:26:49,900
Speaker 11:  bit more of a sense of what we could be looking for in a ruling.

1587
01:26:50,490 --> 01:26:52,940
Speaker 6:  Yeah, that'll be, it's, that was one of the things we really learned from

1588
01:26:52,970 --> 01:26:55,780
Speaker 6:  Epic versus Apple too, was like the more you pay attention to the questions

1589
01:26:55,800 --> 01:26:59,260
Speaker 6:  the judge is asking, the better equipped you're gonna be to figure out where

1590
01:26:59,260 --> 01:27:01,340
Speaker 6:  this is actually gonna go. Tom, what about you?

1591
01:27:01,600 --> 01:27:04,900
Speaker 10:  So I, it's gonna be a lot of financial stuff in the morning, I think with

1592
01:27:04,920 --> 01:27:08,900
Speaker 10:  the CFO of exports and CFO at Microsoft appearing. So I think

1593
01:27:08,900 --> 01:27:12,380
Speaker 10:  we're gonna probably get some stuff there. I'm curious to see what Nintendo

1594
01:27:12,380 --> 01:27:15,980
Speaker 10:  has to say as well. Cause they're appearing today and it will mean that

1595
01:27:15,980 --> 01:27:19,140
Speaker 10:  we will have five days of constant arguments about the switch. I'm sure

1596
01:27:19,140 --> 01:27:23,020
Speaker 10:  of that. So hopefully Nintendo will sort of answer the question,

1597
01:27:23,020 --> 01:27:26,940
Speaker 10:  whether it's a game console or not, the switch and then the closing statements.

1598
01:27:26,940 --> 01:27:29,820
Speaker 10:  Yeah, I think they'll kind of hint at where they're trying to sway the judge,

1599
01:27:29,980 --> 01:27:33,580
Speaker 10:  I guess. Which they kind of started off at the beginning with the opening

1600
01:27:33,580 --> 01:27:37,220
Speaker 10:  statements. And then I'm interested, like you say, to see what

1601
01:27:37,400 --> 01:27:40,540
Speaker 10:  if the judge cuts in at any point again today and if it's about court of

1602
01:27:40,540 --> 01:27:40,700
Speaker 10:  duty,

1603
01:27:41,040 --> 01:27:43,900
Speaker 6:  All right. And Alex, you and I are just gonna screw off and go play some

1604
01:27:43,950 --> 01:27:46,340
Speaker 6:  co-op Call of Duty. Right? That's that's my day. Yeah.

1605
01:27:46,460 --> 01:27:49,380
Speaker 11:  I mean it's too big to fail. We gotta play it. I mean you're Americans,

1606
01:27:49,380 --> 01:27:53,340
Speaker 11:  you love shooting things and you love Yeah. Multiplayer love shooting your

1607
01:27:53,340 --> 01:27:53,500
Speaker 11:  friends.

1608
01:27:54,050 --> 01:27:57,500
Speaker 6:  Yeah, that's, and there we go on that note. Awesome. Thank you all.

1609
01:27:57,640 --> 01:28:00,740
Speaker 6:  That's it. That's The Vergecast. The website was really good this week. We

1610
01:28:00,740 --> 01:28:04,220
Speaker 6:  had a thing on the Best Tech books that is both very good and

1611
01:28:04,220 --> 01:28:07,940
Speaker 6:  surprisingly controversial. Like it's sort of encouraging to know that the

1612
01:28:08,060 --> 01:28:10,820
Speaker 6:  internet has strong opinions about books at this moment in time. But they

1613
01:28:10,820 --> 01:28:14,340
Speaker 6:  do. And we have a form where you can tell us all the books that you missed

1614
01:28:14,760 --> 01:28:17,580
Speaker 6:  and you'll be, you'll be wrong about that. But that's okay because we picked

1615
01:28:17,580 --> 01:28:21,100
Speaker 6:  all the best books. Go read some of those books. I wrote a thing about Google

1616
01:28:21,100 --> 01:28:25,060
Speaker 6:  Reader that just went live on Friday and it made

1617
01:28:25,060 --> 01:28:27,940
Speaker 6:  me sad that Google Reader has been gone for 10 years. But here we are. The

1618
01:28:27,960 --> 01:28:31,300
Speaker 6:  Reddit stuff continues to be chaotic. Today's gonna be a crazy day. And Reddit,

1619
01:28:31,340 --> 01:28:34,300
Speaker 6:  like we were talking about, lots to talk about next week. We're off next

1620
01:28:34,300 --> 01:28:37,900
Speaker 6:  Wednesday, but we'll be back next Friday. Have an awesome weekend. That's

1621
01:28:38,020 --> 01:28:38,940
Speaker 6:  Vergecast Rock and roll.

1622
01:28:44,000 --> 01:28:47,340
Speaker 12:  And that's a wrap for Vergecast this week. We'd love to hear from you. Shoot

1623
01:28:47,340 --> 01:28:51,300
Speaker 12:  us an email at Vergecast at The Verge dot com. The Vergecast is a

1624
01:28:51,300 --> 01:28:54,900
Speaker 12:  production of The Verge and the Vox Media podcast network. The show is produced

1625
01:28:54,920 --> 01:28:58,860
Speaker 12:  by me, Liam James, and our senior audio director, Andrew Marino. Our

1626
01:28:58,860 --> 01:29:02,540
Speaker 12:  editorial director is Brooke Minters. That's it. We'll see you next week.

