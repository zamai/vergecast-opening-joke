1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: a4b5f264-eb00-436f-9386-72eb97765e3d
Status: Done
Stage: Done
Title: The great Pixel 8 camera debate
Audio URL: https://jfe93e.s3.amazonaws.com/8697035540414822973/3210311571309025029/s93290-US-5633s-1696584628.mp3
Description: The Verge’s Nilay Patel, David Pierce, and Alex Cranz discuss the announcement of Google’s Pixel 8 phone, along with the new AI tools that raise lots of questions.

2
00:01:25,320 --> 00:01:29,070
Speaker 4:  Hello and welcome to Our Chest, the flagship podcast of extra

3
00:01:29,070 --> 00:01:30,110
Speaker 4:  large cups and bowls.

4
00:01:30,500 --> 00:01:32,910
Speaker 1:  Yeah, you never know when you need one.

5
00:01:33,610 --> 00:01:37,030
Speaker 4:  No. You always know when you one, which is always, yeah. All right, look,

6
00:01:37,030 --> 00:01:40,870
Speaker 4:  here's what happened before we started the show. David held up his new Ember

7
00:01:40,870 --> 00:01:44,150
Speaker 4:  mug, which is gigantic. It's huge. It's, it's so big. It

8
00:01:44,150 --> 00:01:47,110
Speaker 5:  Weighs so much. It was so much work to hold it up just now,

9
00:01:47,490 --> 00:01:51,350
Speaker 4:  Our own James Vincent, who's currently on book leave again, he

10
00:01:51,350 --> 00:01:54,850
Speaker 4:  wrote an entire book called Beyond Measure

11
00:01:55,270 --> 00:01:58,530
Speaker 4:  about the Pol, essentially the politics and culture of measuring

12
00:01:59,240 --> 00:02:02,610
Speaker 4:  things. This is one of the Verst Verge ideas that anyone has ever had. And

13
00:02:02,610 --> 00:02:05,250
Speaker 4:  it's obviously, and you should go read this book, it's called Beyond Measure.

14
00:02:05,250 --> 00:02:08,930
Speaker 4:  Go read it. And it occurred to me as we were talking about David's

15
00:02:08,940 --> 00:02:12,810
Speaker 4:  comically large electric cup. That's what it is. Let's be

16
00:02:13,480 --> 00:02:17,450
Speaker 4:  honest that I have no idea how much a cup of coffee is like,

17
00:02:17,450 --> 00:02:21,360
Speaker 4:  not price like volume. Right. And David pointed out to me

18
00:02:21,360 --> 00:02:25,040
Speaker 4:  that the word cup has a third additional

19
00:02:25,040 --> 00:02:27,000
Speaker 4:  secret meeting in the context of coffee.

20
00:02:27,870 --> 00:02:31,520
Speaker 5:  Yeah. So when you, when you have a coffee maker

21
00:02:31,940 --> 00:02:35,360
Speaker 5:  and you see the lines on the side that's like this many cups of coffee, you

22
00:02:35,360 --> 00:02:38,520
Speaker 5:  would think that would be like a cup, like a measuring cup cup.

23
00:02:39,150 --> 00:02:42,880
Speaker 5:  It's not. It's a coffee cup, which in most cases is

24
00:02:42,880 --> 00:02:46,040
Speaker 5:  defined to be either five or six ounces. So what you're getting

25
00:02:46,660 --> 00:02:50,480
Speaker 5:  is not eight cups measured by mugs or eight cup

26
00:02:50,540 --> 00:02:54,240
Speaker 5:  me measured cups. You're getting eight cups by this random

27
00:02:54,240 --> 00:02:57,320
Speaker 5:  measure of a tiny amount of coffee that no one actually wants.

28
00:02:57,470 --> 00:02:59,920
Speaker 4:  This is another book, a whole other book.

29
00:03:00,180 --> 00:03:02,480
Speaker 1:  How many, how many cup cups of coffee do you

30
00:03:02,510 --> 00:03:06,440
Speaker 4:  This is okay. The problem with the history of Europe, oh boy, is that

31
00:03:06,440 --> 00:03:10,320
Speaker 4:  a bunch of kings got to invent a bunch of Right. And now we just

32
00:03:10,320 --> 00:03:14,050
Speaker 4:  live in their world. King Harold was like, I don't know, it's this many

33
00:03:15,210 --> 00:03:19,130
Speaker 4:  whatever. All I'm saying is you should get extraordinary. The biggest

34
00:03:19,130 --> 00:03:22,530
Speaker 4:  coffee cups you can, the biggest mugs. And then we bought

35
00:03:22,930 --> 00:03:26,410
Speaker 4:  gigantic bowls at our new house. Yeah, they're, they're so big.

36
00:03:27,320 --> 00:03:30,290
Speaker 4:  They make an average amount of soup look like not a lot of soup.

37
00:03:30,470 --> 00:03:32,170
Speaker 1:  You've consumed a lot of soup. You

38
00:03:32,170 --> 00:03:35,930
Speaker 4:  Move into new place, you're like, what? What large format foods when you

39
00:03:35,970 --> 00:03:39,170
Speaker 4:  fill it, can we acquire so we don't have to think about this for the next

40
00:03:39,170 --> 00:03:39,650
Speaker 4:  several weeks

41
00:03:39,920 --> 00:03:43,170
Speaker 1:  When you fill it with ice cream, is there any ice cream left in the carton?

42
00:03:44,260 --> 00:03:46,540
Speaker 4:  I prefer not to answer that question this time. This

43
00:03:46,540 --> 00:03:50,100
Speaker 5:  Makes me think of like, so my wife and I eat cereal as a lot of people do,

44
00:03:50,460 --> 00:03:54,380
Speaker 5:  and her definition of a bowl of cereal is just whatever the

45
00:03:54,380 --> 00:03:57,700
Speaker 5:  size of the receptacle is. So like if you give her, yeah, that's right. Like

46
00:03:57,740 --> 00:04:01,580
Speaker 5:  a relatively small human sized bowl, she'll fill

47
00:04:01,580 --> 00:04:05,420
Speaker 5:  it with the, the You know normal amount of Cheerios, put some milk in and

48
00:04:05,420 --> 00:04:08,340
Speaker 5:  move on with her life. If you give that woman like a bucket, she'll fill

49
00:04:08,340 --> 00:04:12,100
Speaker 5:  the bucket with Cheerios and be like, this is a serving of Cheerios. So like,

50
00:04:12,100 --> 00:04:15,940
Speaker 5:  I love a big bowl. It's, they're, they're better than plates at everything.

51
00:04:16,570 --> 00:04:20,460
Speaker 5:  Including No, just everything. It's just all the things. But if I give

52
00:04:20,460 --> 00:04:24,260
Speaker 5:  my wife a large bowl, she will, she will eat like a deathly amount of

53
00:04:24,260 --> 00:04:25,580
Speaker 5:  Cheerios. And I can't do this.

54
00:04:26,220 --> 00:04:28,860
Speaker 4:  I want you to know there's a trend in plateware where the, the plates are

55
00:04:29,060 --> 00:04:32,500
Speaker 4:  becoming bowls and they're like, they're half bowls. There's a designer out

56
00:04:32,500 --> 00:04:35,020
Speaker 4:  there who invented this who's furious that they're being ripped off.

57
00:04:35,930 --> 00:04:38,620
Speaker 1:  It's fiestaware, my fiestaware bulls. Are you

58
00:04:38,620 --> 00:04:42,460
Speaker 4:  Just, you just randomly said the word fiestaware. It's huge. The bulls are,

59
00:04:42,570 --> 00:04:45,540
Speaker 4:  okay, we gotta stop this. There's actual tech news. I'm just saying if you

60
00:04:45,540 --> 00:04:49,020
Speaker 4:  wanna write the inside story of how plates are becoming

61
00:04:49,190 --> 00:04:52,940
Speaker 4:  bulls there, there's but one publication that will be like, yes, that's correct.

62
00:04:53,040 --> 00:04:54,500
Speaker 4:  Yes. This belongs to us.

63
00:04:54,560 --> 00:04:57,380
Speaker 5:  And if they're temperature regulated like my ember mug, all the better.

64
00:04:58,560 --> 00:05:01,220
Speaker 4:  If your bowls, it's very have wifi in them. Call me.

65
00:05:02,170 --> 00:05:05,220
Speaker 4:  There's a lot of tech news this week, by the way. I'm your friend Neli. Hello.

66
00:05:05,390 --> 00:05:06,700
Speaker 4:  David Pierce is here. Hi.

67
00:05:06,920 --> 00:05:08,540
Speaker 5:  Me and my mug. We're both here.

68
00:05:09,900 --> 00:05:13,460
Speaker 4:  Electric mugs. I drink the coffee too fast. I wanna be, I've always wanted

69
00:05:13,460 --> 00:05:15,980
Speaker 4:  an ember mug and there's just no hope for me. Like

70
00:05:15,980 --> 00:05:19,340
Speaker 5:  Why? Oh, I'm the opposite. I drink the first five sips really fast and then

71
00:05:19,360 --> 00:05:23,180
Speaker 5:  the last two thirds, six and a half hours later, this is why this thing is

72
00:05:23,180 --> 00:05:23,620
Speaker 5:  perfect for me.

73
00:05:23,730 --> 00:05:24,820
Speaker 4:  Alex Francis is here. Yeah,

74
00:05:24,840 --> 00:05:27,780
Speaker 1:  I'm with her Fiesta. I'm I'm, I'm still thinking about the Fiesta wear bowls.

75
00:05:27,850 --> 00:05:30,300
Speaker 1:  They're huge. I've eaten so much. Chill

76
00:05:30,300 --> 00:05:31,660
Speaker 4:  Is Fiesta wear the radioactive ones?

77
00:05:32,120 --> 00:05:35,180
Speaker 1:  The old ones. Okay. I don't think mine are very

78
00:05:35,180 --> 00:05:35,380
Speaker 4:  Good.

79
00:05:35,670 --> 00:05:36,740
Speaker 1:  We'll see you in a couple of months.

80
00:05:37,000 --> 00:05:40,620
Speaker 4:  If You know why I just said is Fiestaware the radioactive ones. That's a

81
00:05:40,620 --> 00:05:42,620
Speaker 4:  little Google adventure for you today. Yeah, that's a good one. That's a

82
00:05:42,620 --> 00:05:45,700
Speaker 4:  little Wikipedia hold that you could fall down on a service we provide here

83
00:05:45,940 --> 00:05:48,300
Speaker 4:  on The Vergecast stuff to Google. You're

84
00:05:48,300 --> 00:05:49,340
Speaker 1:  Gonna have a great time. Welcome

85
00:05:49,360 --> 00:05:53,300
Speaker 4:  To our newest segment. How to Google about old plates. Alright,

86
00:05:53,300 --> 00:05:57,140
Speaker 4:  there is a lot of tech news this week, like absurd amount

87
00:05:57,200 --> 00:06:01,120
Speaker 4:  of tech news. There are some philosophical quandaries. A bunch of people

88
00:06:01,140 --> 00:06:05,120
Speaker 4:  are mad at me on Instagram threads for suggesting that words have meanings.

89
00:06:05,630 --> 00:06:08,520
Speaker 4:  It's a lot. There's just a lot going on. There is The Pixel event that had

90
00:06:08,520 --> 00:06:12,080
Speaker 4:  Android 14 The Pixel Watch two new Pixel Buds price of every

91
00:06:12,080 --> 00:06:15,960
Speaker 4:  Streaming service is going up and the actors are still on

92
00:06:15,960 --> 00:06:19,800
Speaker 4:  strike. Lots going on there. And then we have a

93
00:06:19,860 --> 00:06:23,480
Speaker 4:  deeply hilarious lightning round just of things,

94
00:06:24,510 --> 00:06:28,480
Speaker 4:  just people continuing to make choices. We should obviously start with The

95
00:06:28,480 --> 00:06:30,360
Speaker 4:  Pixel. David, what? What happened to this event? Sure.

96
00:06:30,460 --> 00:06:34,040
Speaker 5:  So this event as ever with Google recently has been

97
00:06:34,400 --> 00:06:37,960
Speaker 5:  absolutely leaked to death. So we kind of knew what was going on coming in,

98
00:06:37,960 --> 00:06:41,280
Speaker 5:  but basically we got two new phones, The Pixel eight and The Pixel eight

99
00:06:41,300 --> 00:06:44,920
Speaker 5:  Pro, which have a lot in common but

100
00:06:45,300 --> 00:06:48,920
Speaker 5:  are differentiated, especially along camera lines. It's a little bit like

101
00:06:48,920 --> 00:06:52,240
Speaker 5:  the way Apple does it, but Google split it in a bunch of ways that I don't

102
00:06:52,240 --> 00:06:56,000
Speaker 5:  think make a lot of sense. But anyway, new tensor chips normal

103
00:06:56,110 --> 00:06:58,880
Speaker 5:  sort of upgrades over time. The screens got better, especially on the, on

104
00:06:58,880 --> 00:07:02,840
Speaker 5:  the eight Pro. But the camera is the thing and we're gonna talk a lot about

105
00:07:02,840 --> 00:07:05,280
Speaker 5:  the camera, so I'll leave that for now. We also got The Pixel watch two,

106
00:07:05,290 --> 00:07:08,000
Speaker 5:  which seems to have much longer battery life, which is awesome. It seems

107
00:07:08,000 --> 00:07:11,600
Speaker 5:  to also have better performance, which is exciting. This seems like

108
00:07:11,990 --> 00:07:15,920
Speaker 5:  kind of The Pixel watch. We knew Google was

109
00:07:15,920 --> 00:07:19,160
Speaker 5:  gonna make after the first one, which is last year's was like cool first

110
00:07:19,220 --> 00:07:22,840
Speaker 5:  try like call us when you've sort of finished the job. This one feels potentially

111
00:07:22,840 --> 00:07:26,280
Speaker 5:  closer to having finished the job. It has some good safety features, new

112
00:07:26,390 --> 00:07:30,120
Speaker 5:  sensors. I think a heart rate thing for the first time or

113
00:07:30,120 --> 00:07:33,640
Speaker 5:  some like zone training in your heart rate, which is a good thing. And then

114
00:07:33,640 --> 00:07:37,440
Speaker 5:  the other one is, if I'm not mistaken, not new Pixel

115
00:07:37,750 --> 00:07:41,640
Speaker 5:  Buds Pros, but new features for The, Pixel, Buds Pro, some cool

116
00:07:42,000 --> 00:07:45,680
Speaker 5:  software stuff to make your calls sound better. There's the conversation

117
00:07:45,910 --> 00:07:49,480
Speaker 5:  ducking thing, which is like when you start talking, it'll lower the volume

118
00:07:49,480 --> 00:07:52,800
Speaker 5:  and turn on transparency mode, which I hate and think absolutely everyone

119
00:07:52,800 --> 00:07:55,440
Speaker 5:  should turn off because it doesn't work on anything. It's a disaster of a

120
00:07:55,440 --> 00:07:58,720
Speaker 5:  feature. Cool idea, bad feature, terrible. Hopefully Google got it right

121
00:07:59,220 --> 00:08:02,640
Speaker 5:  and some Bluetooth super wideband, which is obviously a thing we have to

122
00:08:02,640 --> 00:08:06,440
Speaker 5:  talk about today on the first cast. And yeah, again, just a bunch of new

123
00:08:06,840 --> 00:08:10,760
Speaker 5:  software stuff coming and I feel like software was sort of the story of

124
00:08:10,760 --> 00:08:14,520
Speaker 5:  this event. Like Google still makes phones and still makes watches and still

125
00:08:14,520 --> 00:08:18,400
Speaker 5:  makes headphones, but is increasingly taking all of the stuff that happens

126
00:08:18,500 --> 00:08:22,160
Speaker 5:  on those devices and moving it somewhere else. A lot of the camera

127
00:08:22,170 --> 00:08:25,640
Speaker 5:  stuff that happened is not happening on the camera, it's happening in Google

128
00:08:25,660 --> 00:08:29,520
Speaker 5:  Photos and that's not nothing. And there's a lot of cloud stuff

129
00:08:29,520 --> 00:08:33,240
Speaker 5:  happening and they talked a lot about assistant with Bard and Android

130
00:08:33,440 --> 00:08:37,280
Speaker 5:  14 came out and it's getting generative AI stuff like Google is really leaned

131
00:08:37,280 --> 00:08:40,840
Speaker 5:  into this idea that it can basically make you a decent piece of hardware

132
00:08:40,990 --> 00:08:44,600
Speaker 5:  with a whole lot of stuff happening in the cloud. And that

133
00:08:44,620 --> 00:08:48,520
Speaker 5:  that's the correct balance from now on. And

134
00:08:48,580 --> 00:08:50,880
Speaker 5:  I'm deeply fascinated to see if that turns out to be true.

135
00:08:51,150 --> 00:08:55,120
Speaker 4:  Yeah, and and this is obviously the flip of Apple's approach, right? Where

136
00:08:55,180 --> 00:08:58,200
Speaker 4:  everything is happening locally on the phone and the phone processors are

137
00:08:58,660 --> 00:09:02,640
Speaker 4:  the fastest processors in human history. And how dare you even suggest that

138
00:09:02,640 --> 00:09:04,200
Speaker 4:  other processors may be faster at any time.

139
00:09:04,200 --> 00:09:04,480
Speaker 5:  Yeah,

140
00:09:04,480 --> 00:09:05,080
Speaker 1:  Nothing's faster.

141
00:09:05,220 --> 00:09:08,640
Speaker 4:  Do You know what makes the iPhone Pro Pro? It's really good at games

142
00:09:09,430 --> 00:09:10,840
Speaker 5:  Freight tracing baby. It's the most pro

143
00:09:10,840 --> 00:09:11,040
Speaker 1:  Feature.

144
00:09:11,990 --> 00:09:15,040
Speaker 4:  Very confusing. Google has always kind of walked this line though, right?

145
00:09:15,220 --> 00:09:17,840
Speaker 4:  How much is happening on your phone? How much is happening on a Google service?

146
00:09:18,660 --> 00:09:21,880
Speaker 4:  You know people have been sharing all these home screens lately, and I think

147
00:09:21,880 --> 00:09:25,320
Speaker 4:  David, somebody shared one to you and me on Threads and it was their entire

148
00:09:25,320 --> 00:09:28,360
Speaker 4:  home home screen was Google apps and you're like, You know Android exists.

149
00:09:28,390 --> 00:09:28,680
Speaker 4:  Yeah.

150
00:09:29,380 --> 00:09:29,800
Speaker 5:  Wasn't

151
00:09:29,800 --> 00:09:31,000
Speaker 1:  That a coworker of ours?

152
00:09:31,620 --> 00:09:35,440
Speaker 4:  No. I mean I'm sure it's many coworkers of ours are like that, but

153
00:09:35,640 --> 00:09:38,520
Speaker 4:  I think Google recognizes this, that a lot of people have iPhones that are

154
00:09:38,520 --> 00:09:42,400
Speaker 4:  just vessels of Google services and Google can just sell you a phone.

155
00:09:42,400 --> 00:09:43,160
Speaker 4:  That is that thing.

156
00:09:43,950 --> 00:09:44,240
Speaker 1:  Yeah.

157
00:09:45,120 --> 00:09:48,600
Speaker 5:  I will say it doesn't give me great confidence in Google wanting to

158
00:09:48,960 --> 00:09:52,830
Speaker 5:  continue to make hardware for a long time, but I already

159
00:09:52,830 --> 00:09:55,030
Speaker 5:  didn't have great confidence in that. So whatever

160
00:09:55,550 --> 00:09:59,350
Speaker 1:  I kind of feel like we need to start and by we Google, not

161
00:09:59,350 --> 00:10:02,750
Speaker 1:  us. I think we're pretty realistic about it. Be realistic about the fact

162
00:10:02,750 --> 00:10:06,470
Speaker 1:  that these are always gonna be like the surface. It's always a

163
00:10:06,470 --> 00:10:10,390
Speaker 1:  device to show OEMs what they can do with the, the stuff that's

164
00:10:10,390 --> 00:10:11,710
Speaker 1:  available and the software that's available.

165
00:10:12,030 --> 00:10:12,750
Speaker 4:  I disagree with that.

166
00:10:12,890 --> 00:10:15,510
Speaker 1:  Really fight me nuts.

167
00:10:16,350 --> 00:10:20,110
Speaker 4:  I think that might've been, I mean we've been through 50 rounds of what is

168
00:10:20,150 --> 00:10:23,790
Speaker 4:  a Pixel phone for? That's what the Nexus line was for. Yeah. Like a piece

169
00:10:23,790 --> 00:10:27,550
Speaker 4:  of reference hardware billion years ago that might've been

170
00:10:27,710 --> 00:10:31,430
Speaker 4:  what the first Pixel phones were for. But there's only one

171
00:10:32,070 --> 00:10:34,150
Speaker 4:  o e m in Android world that matters.

172
00:10:34,410 --> 00:10:34,990
Speaker 1:  That's fair.

173
00:10:35,120 --> 00:10:38,950
Speaker 4:  Right? They're Samsung. Yeah. Yeah. And so I, I think the idea that

174
00:10:38,950 --> 00:10:42,750
Speaker 4:  Google needs to show Samsung how to do it is

175
00:10:42,750 --> 00:10:46,590
Speaker 4:  just not reality. I think Google needs to show Samsung that if Samsung

176
00:10:46,810 --> 00:10:50,710
Speaker 4:  screws up or steps out of line, that it will go and get

177
00:10:50,710 --> 00:10:51,870
Speaker 4:  the marketing deal with Verizon.

178
00:10:52,370 --> 00:10:53,670
Speaker 1:  But did it do that to,

179
00:10:54,100 --> 00:10:56,510
Speaker 4:  Well, I think, I think Samsung has stayed in line. Okay,

180
00:10:56,510 --> 00:11:00,070
Speaker 5:  Wait, Neli, go with me here. Google is to Samsung.

181
00:11:00,700 --> 00:11:04,510
Speaker 5:  What binging is to Google like a, a substantially less

182
00:11:04,510 --> 00:11:08,150
Speaker 5:  successful thing that exists both to try and be a good product,

183
00:11:08,290 --> 00:11:12,270
Speaker 5:  but mostly as a check on the other thing, just saying if you blow

184
00:11:12,270 --> 00:11:14,140
Speaker 5:  it, we will come for you.

185
00:11:14,690 --> 00:11:18,340
Speaker 4:  Yeah. Only the, the slight difference is that

186
00:11:18,600 --> 00:11:22,340
Speaker 4:  if Samsung didn't exist, Android would not be as important

187
00:11:22,400 --> 00:11:22,860
Speaker 4:  as it is.

188
00:11:22,970 --> 00:11:23,780
Speaker 5:  Yeah, that's fair.

189
00:11:23,960 --> 00:11:27,540
Speaker 4:  At least in this country and in in, in Europe, in

190
00:11:27,630 --> 00:11:31,540
Speaker 4:  India and Africa, like Android is a big deal all on its own.

191
00:11:31,540 --> 00:11:34,700
Speaker 4:  Like my favorite thing about the Indian smartphone market is that like 14

192
00:11:34,700 --> 00:11:38,500
Speaker 4:  phones get released every single week. And people in Android there

193
00:11:38,500 --> 00:11:42,100
Speaker 4:  is expressed very differently. Like Android is globally important. Whether

194
00:11:42,100 --> 00:11:45,900
Speaker 4:  or not Samsung does it in this country, Android doesn't exist

195
00:11:45,900 --> 00:11:49,180
Speaker 4:  without Samsung. That's right. Which is just at, at least in smartphones,

196
00:11:49,480 --> 00:11:53,320
Speaker 4:  that's crazy. Right. And I think a lot of The, Pixel is

197
00:11:53,320 --> 00:11:56,520
Speaker 4:  there to just repeatedly show Samsung,

198
00:11:57,660 --> 00:12:01,640
Speaker 4:  Hey, we can do it too. Right? And so if you pull, if you put ties in

199
00:12:01,640 --> 00:12:04,600
Speaker 4:  on your phone, which they've threatened to do. Yeah. Or you skin the phone

200
00:12:04,620 --> 00:12:08,280
Speaker 4:  too hard, which they have done and like there's a whole history there. You

201
00:12:08,280 --> 00:12:10,880
Speaker 4:  know Google's there. So I, I think The Pixel is just different than the service.

202
00:12:10,980 --> 00:12:14,680
Speaker 1:  So you're saying The Pixel is basically Google just staring Samsung

203
00:12:14,860 --> 00:12:18,040
Speaker 1:  in the eye and saying try it. Yeah.

204
00:12:18,630 --> 00:12:19,560
Speaker 1:  Like just to some

205
00:12:19,560 --> 00:12:22,520
Speaker 4:  Extent uncomfortable to some extent. I think they also need a vessel for

206
00:12:22,520 --> 00:12:25,480
Speaker 4:  their services. They wanna show people what it would be like. They also want

207
00:12:25,480 --> 00:12:29,440
Speaker 4:  to compete head on with Apple. Right. There's a, there's a lot there, but

208
00:12:29,510 --> 00:12:33,440
Speaker 4:  what is the thing it accomplishes the most? It's not sales. Right? It's

209
00:12:33,440 --> 00:12:36,880
Speaker 4:  true. It is a lot of interest from phone nerds.

210
00:12:37,780 --> 00:12:41,400
Speaker 4:  It is crazy to me the disproportionate amount of traffic we get

211
00:12:41,740 --> 00:12:44,120
Speaker 4:  on Pixel coverage versus Samsung coverage. Yeah.

212
00:12:44,380 --> 00:12:47,880
Speaker 1:  And I think that's why I think of it like the surface devices. 'cause it's

213
00:12:47,880 --> 00:12:50,520
Speaker 1:  the same thing. Like people care about the surface devices and the way they

214
00:12:50,520 --> 00:12:53,640
Speaker 1:  do not care about a se unless it's appearing in court.

215
00:12:55,060 --> 00:12:58,120
Speaker 1:  But yeah, like, like that's that's really because people really

216
00:12:58,360 --> 00:13:02,320
Speaker 1:  passionately like the Googled devices. Yeah. But it's a very

217
00:13:02,320 --> 00:13:05,040
Speaker 1:  small group of people according to sales. Yeah, yeah.

218
00:13:05,420 --> 00:13:08,200
Speaker 4:  And the answer, right? We are not, we're fully into talking about this is

219
00:13:08,200 --> 00:13:10,880
Speaker 4:  the thing with talking about Pixel. It's more fun to talk about the, the

220
00:13:10,880 --> 00:13:13,880
Speaker 4:  concepts and the ideas embedded in The Pixel line. But there was cool stuff

221
00:13:13,880 --> 00:13:15,080
Speaker 4:  than the phones. There

222
00:13:15,080 --> 00:13:16,240
Speaker 1:  Was cool stuff this time. Yeah.

223
00:13:16,480 --> 00:13:20,000
Speaker 5:  I don't know. I mean the thing that really stuck out to me was

224
00:13:20,580 --> 00:13:24,480
Speaker 5:  how much the people who were otherwise primed to like

225
00:13:24,480 --> 00:13:28,400
Speaker 5:  a phone like these did not like it this year. Like typically

226
00:13:29,300 --> 00:13:31,840
Speaker 5:  the, the overwhelming reaction I've seen, and I'm sure this is not true of

227
00:13:31,840 --> 00:13:33,520
Speaker 5:  everybody, and I'm sure there are people who are excited about it, and I

228
00:13:33,520 --> 00:13:36,480
Speaker 5:  think there are cool things worth getting excited about, but like a bunch

229
00:13:36,480 --> 00:13:40,000
Speaker 5:  of people on like the Android blogs were mad about

230
00:13:40,260 --> 00:13:43,640
Speaker 5:  the, the sort of perceived lack of interesting stuff Google did. Here are

231
00:13:43,690 --> 00:13:47,600
Speaker 5:  commenters, were not psyched about this in a big way. The general

232
00:13:47,600 --> 00:13:51,560
Speaker 5:  feeling of this launch that I got was a lot of people kind of

233
00:13:51,560 --> 00:13:55,280
Speaker 5:  being like this. Like that's, they seem like fine phones, but like that's,

234
00:13:55,280 --> 00:13:59,080
Speaker 5:  that's all you got. And I kind of agree like Google made a good phone

235
00:13:59,230 --> 00:14:02,680
Speaker 5:  here that really doesn't seem to make a particularly interesting case for

236
00:14:02,680 --> 00:14:05,080
Speaker 5:  itself outside of some of the camera stuff.

237
00:14:05,180 --> 00:14:07,200
Speaker 4:  And they also raised prices by a hundred bucks.

238
00:14:07,510 --> 00:14:10,880
Speaker 1:  Yeah. It's, it's like a boring looking phone.

239
00:14:10,980 --> 00:14:14,880
Speaker 1:  Extraordinarily boring. But the the Magic Editor thing is cool. Right.

240
00:14:14,940 --> 00:14:15,240
Speaker 1:  All right.

241
00:14:15,240 --> 00:14:18,600
Speaker 4:  We, we, we, the camera is such a six hour, I I

242
00:14:18,600 --> 00:14:19,720
Speaker 1:  Know I wanna just go talk about it.

243
00:14:20,100 --> 00:14:24,040
Speaker 4:  Can you just read the specs of the phone? So we've done, we we've,

244
00:14:24,040 --> 00:14:25,320
Speaker 4:  we've done our job, we've talked

245
00:14:25,590 --> 00:14:29,520
Speaker 5:  Sure, I can do this. Okay. So main things to know. The Pixel

246
00:14:29,520 --> 00:14:33,040
Speaker 5:  eight starts at 700 bucks. The Pixel eight Pro starts at a thousand dollars.

247
00:14:33,190 --> 00:14:37,160
Speaker 5:  They both run Android 14. The Pixel eight has a 6.2 inch

248
00:14:37,160 --> 00:14:40,760
Speaker 5:  screen. The Pixel eight Pro has a 6.7 inch screen that's

249
00:14:40,870 --> 00:14:44,720
Speaker 5:  also a little better. 12 gigs of Ram on the Pro eight gigs of Ram on

250
00:14:44,720 --> 00:14:48,640
Speaker 5:  the eight more storage options in the high end on the pro you can get I

251
00:14:48,640 --> 00:14:51,640
Speaker 5:  think five 12 and a terabyte, which you can't get on The Pixel eight. They

252
00:14:51,640 --> 00:14:55,360
Speaker 5:  both have U S B C. They both do more face unlock

253
00:14:55,360 --> 00:14:58,080
Speaker 5:  stuff than they have in the past, which I think is interesting. Google's

254
00:14:58,080 --> 00:15:01,360
Speaker 5:  like letting you do more with face unlock but didn't really explain if it

255
00:15:01,360 --> 00:15:05,320
Speaker 5:  has made face lock unlock more secure, which I thought was interesting. A

256
00:15:05,320 --> 00:15:08,920
Speaker 5:  million outrageously complicated differences in the

257
00:15:09,160 --> 00:15:13,120
Speaker 5:  camera. Allison Johnson made a spreadsheet of which features are coming

258
00:15:13,120 --> 00:15:17,080
Speaker 5:  to which that even now doesn't make any sense. But basically

259
00:15:17,180 --> 00:15:21,000
Speaker 5:  the Pro has four cameras that you can

260
00:15:21,000 --> 00:15:23,920
Speaker 5:  use. It has the 50 megapixel main camera,

261
00:15:24,850 --> 00:15:28,800
Speaker 5:  48 megapixel ultra wide, 48 megapixel telephoto. And the

262
00:15:28,820 --> 00:15:32,640
Speaker 5:  10 and a half megapixel camera on the front. The Pixel eight has the same

263
00:15:33,070 --> 00:15:36,400
Speaker 5:  main camera. A 12 megapixel ultra wide, no

264
00:15:36,470 --> 00:15:40,360
Speaker 5:  telephoto, and the same selfie cam. And then again, there's a million software

265
00:15:40,600 --> 00:15:44,560
Speaker 5:  features. Fundamentally The Pixel eight Pro is like a dramatically better

266
00:15:44,570 --> 00:15:47,800
Speaker 5:  phone in the way that I think it's kind of like comparing,

267
00:15:48,540 --> 00:15:51,360
Speaker 5:  not like the iPhone Pro and the pro Max, but it's kinda like comparing the

268
00:15:51,390 --> 00:15:55,280
Speaker 5:  base iPhone to the iPhone pro Max. It's like none of the things versus

269
00:15:55,420 --> 00:15:58,720
Speaker 5:  all of the things. And it's a $300 price difference, which I guess You know

270
00:15:58,860 --> 00:16:01,800
Speaker 5:  belies that, but that's essentially the rundown. Like they have the same

271
00:16:01,910 --> 00:16:04,480
Speaker 5:  chip, which has been very important to Google, to have all of this stuff

272
00:16:04,480 --> 00:16:07,880
Speaker 5:  kind of on the same generation of tensor as much as possible to do the AI

273
00:16:07,880 --> 00:16:11,320
Speaker 5:  stuff they wanna do. But then it just gets real chaotic from there. And they

274
00:16:11,320 --> 00:16:15,080
Speaker 5:  come in a bunch of cool colors that are actual colors. Like

275
00:16:15,260 --> 00:16:19,040
Speaker 5:  Google didn't do the thing Apple did and just put like one drop of

276
00:16:19,040 --> 00:16:22,480
Speaker 5:  color dye near the phone and call that a color. They actually colored the

277
00:16:22,480 --> 00:16:23,440
Speaker 5:  phone. It's very exciting.

278
00:16:24,060 --> 00:16:25,480
Speaker 4:  All right, so we talk about the camera. Yeah,

279
00:16:25,480 --> 00:16:27,360
Speaker 1:  Let's talk about the camera Magic editor. Neither

280
00:16:27,360 --> 00:16:29,480
Speaker 5:  Of you were listening to that. Were you, you were just sitting there going

281
00:16:29,510 --> 00:16:32,000
Speaker 5:  shut up so I can talk about the camera. I get it. I was

282
00:16:32,000 --> 00:16:32,560
Speaker 1:  Reading along

283
00:16:33,090 --> 00:16:36,800
Speaker 4:  Along, if we don't, if we don't say the spec, like when I was a little baby

284
00:16:36,900 --> 00:16:40,360
Speaker 4:  gadget blogger, I was told for just get the specs in the price out of the

285
00:16:40,360 --> 00:16:43,520
Speaker 4:  way. And you can say whatever you want, but if you don't deliver those up

286
00:16:43,520 --> 00:16:46,840
Speaker 4:  front, people are gonna get mad. 'cause all anybody cares about, so now you

287
00:16:46,840 --> 00:16:48,920
Speaker 4:  have the specs here of the price I was talking about the camera

288
00:16:49,290 --> 00:16:51,000
Speaker 1:  Magic editor. Magic editor.

289
00:16:51,500 --> 00:16:55,280
Speaker 4:  So this is like a, this is a, I think a watershed moment in these

290
00:16:55,280 --> 00:16:58,600
Speaker 4:  cameras and it isn't at all at the same time.

291
00:16:59,060 --> 00:17:02,600
Speaker 4:  And I, I wanna be clear on that. There's a tension here because of what David

292
00:17:02,710 --> 00:17:05,920
Speaker 4:  said earlier, the features are built into

293
00:17:06,400 --> 00:17:10,320
Speaker 4:  Google's cloud services and their apps. But the

294
00:17:10,320 --> 00:17:14,200
Speaker 4:  way they are marketing this camera, the way they're showing it off on

295
00:17:14,200 --> 00:17:18,080
Speaker 4:  stage, the the the thing they want you to think it can do is

296
00:17:18,180 --> 00:17:22,120
Speaker 4:  lie to you. Like I, there's no other word

297
00:17:22,180 --> 00:17:25,560
Speaker 4:  for it. The camera has generative AI tools

298
00:17:26,620 --> 00:17:30,200
Speaker 4:  not built into it, but parked on the front lawn. Sure.

299
00:17:30,500 --> 00:17:33,280
Speaker 4:  You know what I mean? Yeah. It's like, it's not, it's not, they're not the

300
00:17:33,280 --> 00:17:36,480
Speaker 4:  same. They're not, it's not in the house. Yeah. You know,

301
00:17:37,310 --> 00:17:38,800
Speaker 1:  It's like a Photoshop editor,

302
00:17:38,900 --> 00:17:42,280
Speaker 4:  But you can open the window and and yell at the generative AI tool and it

303
00:17:42,280 --> 00:17:46,160
Speaker 4:  will definitely hear you. Eventually the gap is gonna close. So

304
00:17:46,160 --> 00:17:48,920
Speaker 5:  You're, you're talking about Magic Editor, right? Like essentially there's

305
00:17:48,920 --> 00:17:50,200
Speaker 5:  a lot, a lot of features, but that's the one,

306
00:17:50,340 --> 00:17:54,200
Speaker 4:  That's the one. And in particular in Magic Editor, the one I am talking

307
00:17:54,200 --> 00:17:57,240
Speaker 4:  about is the one where you can take a bunch of frames of people

308
00:17:57,920 --> 00:18:01,800
Speaker 4:  altogether and it will assemble a composite. You can pick the faces of the

309
00:18:01,800 --> 00:18:05,680
Speaker 4:  people that like, I don't care if you lie to me, we'll call it the

310
00:18:05,690 --> 00:18:09,680
Speaker 4:  Skies You know, like whatever, it's green you Yeah. The sky's green.

311
00:18:09,820 --> 00:18:13,720
Speaker 4:  You great. You're you're crazy. Like I know man. Like you have all

312
00:18:13,720 --> 00:18:17,560
Speaker 4:  the right, you, you moved a salt shaker from the left to the right of the

313
00:18:17,560 --> 00:18:20,960
Speaker 4:  picture. That's great. I'm glad that happened For you You know, be you're

314
00:18:20,960 --> 00:18:24,880
Speaker 4:  an artist, you start changing the way people look at

315
00:18:24,880 --> 00:18:28,600
Speaker 4:  each other in photos. You are playing with fire

316
00:18:29,310 --> 00:18:30,960
Speaker 4:  like in the realist way.

317
00:18:31,740 --> 00:18:34,840
Speaker 5:  You're, you're like, you're kidding about the line there. But I actually

318
00:18:34,840 --> 00:18:37,400
Speaker 5:  think the line there is really important and I want you to tease out why

319
00:18:37,400 --> 00:18:41,040
Speaker 5:  that bothers you in the way that the sky doesn't. Because if the, if your

320
00:18:41,040 --> 00:18:44,800
Speaker 5:  fundamental problem is these cameras and their editing

321
00:18:45,080 --> 00:18:48,960
Speaker 5:  software let you lie about what you're seeing in front of you, I

322
00:18:48,960 --> 00:18:52,800
Speaker 5:  think that's like slightly sort of chicken little sky is falling ish

323
00:18:52,800 --> 00:18:56,560
Speaker 5:  about the world. But also, like I can understand that take, I can also understand

324
00:18:56,560 --> 00:19:00,280
Speaker 5:  the take that just says, most people take photos of their family

325
00:19:00,420 --> 00:19:04,360
Speaker 5:  and would like everyone to be smiling. And that's basically fine. I

326
00:19:04,360 --> 00:19:07,840
Speaker 5:  can also see that, but you're trying to find this middle that I don't

327
00:19:08,270 --> 00:19:11,640
Speaker 5:  necessarily know how to find, where it's like changing

328
00:19:11,950 --> 00:19:15,920
Speaker 5:  certain things is fine. Are you okay with the thing where you

329
00:19:15,920 --> 00:19:19,560
Speaker 5:  can like move stuff over in the scene where like if you shot them

330
00:19:19,700 --> 00:19:23,240
Speaker 5:  off center, you can slide them over on the bench to be in the center? Like

331
00:19:23,240 --> 00:19:27,160
Speaker 5:  where does this go from silly changes to like, you're lying

332
00:19:27,160 --> 00:19:27,440
Speaker 5:  to me.

333
00:19:28,040 --> 00:19:31,840
Speaker 4:  I mean first of all, all of it's lying to you. Sure. Like, did this actually

334
00:19:31,840 --> 00:19:35,360
Speaker 4:  happen? Is now a threshold question for

335
00:19:35,360 --> 00:19:39,200
Speaker 4:  smartphone photography. If you look at a photo that came off a smartphone,

336
00:19:39,720 --> 00:19:43,360
Speaker 4:  a good question you can ask is, did this happen?

337
00:19:43,940 --> 00:19:47,840
Speaker 4:  Did this look this way? Sure. We've been doing what is a photo on the

338
00:19:47,840 --> 00:19:51,360
Speaker 4:  show since, for years now, since the first Pixel

339
00:19:52,130 --> 00:19:56,120
Speaker 4:  since the, what's the iPhone 11 did was the first one with H D

340
00:19:56,120 --> 00:19:59,480
Speaker 4:  R and it looked kind of bad You know, like we've been talking about what

341
00:19:59,480 --> 00:20:03,120
Speaker 4:  a photo is for a long time because the, what

342
00:20:03,230 --> 00:20:07,160
Speaker 4:  most people believe a photo to be is you open

343
00:20:07,240 --> 00:20:11,120
Speaker 4:  a shutter, a bunch of light hits a sensor, the shutter closes, and that's

344
00:20:11,120 --> 00:20:11,640
Speaker 4:  the end of that.

345
00:20:11,640 --> 00:20:15,600
Speaker 5:  Right? And the, the goal is how accurately did this capture reality? That's

346
00:20:15,600 --> 00:20:17,080
Speaker 5:  the, like, that's the metric of success,

347
00:20:17,890 --> 00:20:21,840
Speaker 4:  Right? And there's a whole other brand of photography and we can get into

348
00:20:21,840 --> 00:20:25,000
Speaker 4:  it that's like very creative where you are trying to manipulate light and

349
00:20:25,000 --> 00:20:28,720
Speaker 4:  textures and you're proposing people, like all this stuff is happening, you're

350
00:20:28,720 --> 00:20:32,640
Speaker 4:  doing wild edits in Photoshop. But the, the thing that we rely

351
00:20:32,640 --> 00:20:36,160
Speaker 4:  on photography to be is an accurate

352
00:20:36,160 --> 00:20:39,680
Speaker 4:  representation of light in a moment. And that You know those moments can

353
00:20:39,680 --> 00:20:42,800
Speaker 4:  be long or short, you can do long exposure, but whatever, it's like this

354
00:20:42,800 --> 00:20:46,680
Speaker 4:  light occurred, like these things happened and you

355
00:20:46,820 --> 00:20:50,680
Speaker 4:  get to a place with computational photography where it's where it's at even

356
00:20:50,680 --> 00:20:54,520
Speaker 4:  now, before you start moving faces around with ai where you're like, man,

357
00:20:54,610 --> 00:20:56,400
Speaker 4:  we're like right at the boundary of this.

358
00:20:57,080 --> 00:21:00,000
Speaker 1:  I, I think I, I think I know where the boundary is. I think I I think I got

359
00:21:00,000 --> 00:21:03,440
Speaker 1:  it. Yeah, I think so. We've always been able to manipulate,

360
00:21:03,440 --> 00:21:07,240
Speaker 1:  manipulate photos, right? Like you've been able to manipulate photos since

361
00:21:07,340 --> 00:21:10,880
Speaker 1:  the first person took the photograph. Like they were, they were putting

362
00:21:10,880 --> 00:21:14,400
Speaker 1:  fairies in in photos in the 18 hundreds and saying, yeah, it's a real fairy.

363
00:21:14,400 --> 00:21:18,240
Speaker 1:  And it's like, no, it's a person wearing a stupid outfit. What? That's always

364
00:21:18,720 --> 00:21:22,600
Speaker 1:  shocking. I know, but that's always been there. But the, it

365
00:21:22,600 --> 00:21:26,440
Speaker 1:  was hard to do, like in order to do the stuff that you can now

366
00:21:26,440 --> 00:21:29,200
Speaker 1:  do on a phone in Photoshop 20 years

367
00:21:29,200 --> 00:21:31,600
Speaker 4:  Ago, wait, no back up. You're about to make, you're about to make the argument

368
00:21:31,600 --> 00:21:33,680
Speaker 4:  that Google's making, which is like, we just made the tool more accessible.

369
00:21:33,700 --> 00:21:35,160
Speaker 4:  Let me just back up No. Saying up for one second.

370
00:21:35,160 --> 00:21:37,440
Speaker 1:  I'm saying, I'm saying this, the accessibility itself is the line.

371
00:21:37,500 --> 00:21:40,240
Speaker 4:  No, no, no, no. I disagree. You disagree. That's like, that's basically Google

372
00:21:40,250 --> 00:21:43,280
Speaker 4:  slide, to be honest, that's like what Google is saying, right? It's, and

373
00:21:43,280 --> 00:21:46,640
Speaker 4:  it's our piece. I just wanna back up for a second. Even if you stop moving

374
00:21:46,640 --> 00:21:50,600
Speaker 4:  faces around, the thing that we are doing now with basic H D R and

375
00:21:50,600 --> 00:21:54,520
Speaker 4:  phones where you're doing complicated exposure stacking to make

376
00:21:54,520 --> 00:21:58,400
Speaker 4:  things look better is right on the blurry edge of the

377
00:21:58,400 --> 00:22:02,120
Speaker 4:  line because it never looked that way. Right?

378
00:22:02,230 --> 00:22:06,200
Speaker 4:  Like, and like the goal is to make it like look

379
00:22:06,200 --> 00:22:09,640
Speaker 4:  even or even out the tones or expose everything evenly and like, that's not

380
00:22:09,640 --> 00:22:13,160
Speaker 4:  how your eyes work. And like, this is a very philosophical, very abstract,

381
00:22:13,160 --> 00:22:16,360
Speaker 4:  very Vergecast conversation about like, okay, it's a photo

382
00:22:17,060 --> 00:22:20,640
Speaker 4:  and like, it's not, maybe not what you saw or maybe not what old film looked

383
00:22:20,640 --> 00:22:24,280
Speaker 4:  like and You know. We've had Mark LaVoy from The Pixel team on the show talking

384
00:22:24,280 --> 00:22:28,040
Speaker 4:  about what artists The Pixel was meant to evoke. And every year the art,

385
00:22:28,070 --> 00:22:31,760
Speaker 4:  like we've done it and that's just exposure,

386
00:22:32,170 --> 00:22:32,520
Speaker 1:  Right?

387
00:22:32,650 --> 00:22:35,480
Speaker 4:  Right. We've done years of conversation about

388
00:22:36,150 --> 00:22:40,000
Speaker 4:  computational exposure, manipulation, And, what

389
00:22:40,000 --> 00:22:43,920
Speaker 4:  that means for photography and that's smart hdr r and the iPhone and

390
00:22:43,920 --> 00:22:47,160
Speaker 4:  that's pixel's, computational photography and all of that is just,

391
00:22:48,100 --> 00:22:52,050
Speaker 4:  boy, should there be shadows and photos. Yes. And literally

392
00:22:52,050 --> 00:22:55,970
Speaker 4:  years of conversation about the blurry line of computational and

393
00:22:55,970 --> 00:22:59,730
Speaker 4:  photography and ex and exposure. And I still

394
00:22:59,780 --> 00:23:03,770
Speaker 4:  dunno the answer. Like I I I think that's very subjective. I think

395
00:23:03,790 --> 00:23:06,570
Speaker 4:  You know it hit the mainstream. Like the New Yorker, Kyle Chika had a piece

396
00:23:06,570 --> 00:23:10,210
Speaker 4:  in the New Yorker about iPhone photos not looking real and there being

397
00:23:10,410 --> 00:23:14,370
Speaker 4:  backlash against it. That's crazy. The, the, the just exposure,

398
00:23:14,880 --> 00:23:18,730
Speaker 4:  computational manipulation of exposure Okay. Is now a,

399
00:23:18,850 --> 00:23:22,810
Speaker 4:  a mainstream conversation about what photography means. So that, that

400
00:23:22,910 --> 00:23:26,690
Speaker 4:  the line already is blurry before we get to adding various in the 18

401
00:23:26,690 --> 00:23:30,530
Speaker 4:  hundreds. Yeah. Now I'm gonna tell you that inside the new phone, inside

402
00:23:30,530 --> 00:23:33,890
Speaker 4:  The Pixel eight, it is marketed having a tool where you can take a bunch

403
00:23:33,890 --> 00:23:37,770
Speaker 4:  of shots. It won't just stack exposure, it will change people's faces

404
00:23:38,150 --> 00:23:42,050
Speaker 4:  at your discretion to create a reality that never existed.

405
00:23:42,890 --> 00:23:43,170
Speaker 1:  I just

406
00:23:43,520 --> 00:23:45,400
Speaker 4:  Like a thing that never happened

407
00:23:46,140 --> 00:23:49,560
Speaker 1:  And I still see that as Photoshop. I still see that as the exact same

408
00:23:49,990 --> 00:23:53,440
Speaker 4:  Fine. But this is where the accessibility, this is where your thing about

409
00:23:53,440 --> 00:23:57,400
Speaker 4:  accessibility comes in. Yeah. Okay, fine. That's Photoshop. But the

410
00:23:57,400 --> 00:24:01,040
Speaker 4:  camera is marketed as that thing. So I, I spent the morning on the train

411
00:24:01,040 --> 00:24:03,280
Speaker 4:  in coming up with like very silly examples.

412
00:24:04,020 --> 00:24:05,320
Speaker 1:  Go through 'em. Let's hear 'em

413
00:24:05,630 --> 00:24:06,840
Speaker 4:  Like very silly examples.

414
00:24:06,900 --> 00:24:09,280
Speaker 1:  I'm, I'm, so, I talked about fairies. It's okay. Okay.

415
00:24:09,740 --> 00:24:13,720
Speaker 4:  You are at a chiefs game and you see the box

416
00:24:14,550 --> 00:24:18,480
Speaker 4:  with Taylor Swift and Ryan Reynolds and Blake Lively and whoever

417
00:24:18,590 --> 00:24:22,280
Speaker 4:  else. Hugh Jackman. Thank you. Can erase Hugh Jackman from this, please.

418
00:24:22,400 --> 00:24:25,240
Speaker 4:  Yeah, he was there. He was right. Okay. It looks like Travis Kels gonna score

419
00:24:25,240 --> 00:24:27,760
Speaker 4:  a touchdown. So they're all cheering, but then he fumbles in your and they

420
00:24:27,760 --> 00:24:31,680
Speaker 4:  all, they're all sad. Yeah. All right. Well now you can just tweak

421
00:24:31,680 --> 00:24:35,520
Speaker 4:  the faces. So you got happy Hugh. Happy Ryan.

422
00:24:35,570 --> 00:24:37,040
Speaker 4:  Happy Blake. Sad Taylor.

423
00:24:37,820 --> 00:24:39,480
Speaker 1:  But I can do that in Photoshop. You

424
00:24:39,480 --> 00:24:42,840
Speaker 4:  Can do that in Photoshop. But you wouldn't like the

425
00:24:42,840 --> 00:24:46,720
Speaker 4:  accessibility is the thing. Yeah. Like now you can lie more easily right

426
00:24:46,720 --> 00:24:50,600
Speaker 4:  off your phone and send it. Right. Okay. Should there be some

427
00:24:50,810 --> 00:24:54,720
Speaker 4:  norms about whether or not that tool should be widely available to you,

428
00:24:55,220 --> 00:24:56,440
Speaker 4:  to anyone? I think

429
00:24:56,490 --> 00:24:57,160
Speaker 1:  There should be,

430
00:24:57,160 --> 00:25:01,000
Speaker 4:  Should it be built into the camera? Should it be so close to the camera

431
00:25:01,520 --> 00:25:05,360
Speaker 4:  that the camera is marketed? The line is best take uses of

432
00:25:05,360 --> 00:25:09,240
Speaker 4:  photos you did take to get the photo you thought you took. Mm. That

433
00:25:09,240 --> 00:25:12,960
Speaker 4:  is the marketing of this camera. That is the positioning of this camera.

434
00:25:13,660 --> 00:25:16,200
Speaker 4:  And it's like, yes, you can do anything in Photoshop. You could have done

435
00:25:16,200 --> 00:25:20,120
Speaker 4:  this a hundred years ago in Photoshop. But the effort to do

436
00:25:20,120 --> 00:25:23,960
Speaker 4:  it, the effort to lie was high. It was a barrier

437
00:25:24,020 --> 00:25:27,080
Speaker 4:  to entry. So lowering the barrier to entry to

438
00:25:27,820 --> 00:25:30,800
Speaker 4:  now they're not photos, they're just memories. And we all know how infallible

439
00:25:31,000 --> 00:25:33,720
Speaker 4:  memories are. It's like, oh, you're gonna change the very nature of photography

440
00:25:34,740 --> 00:25:38,480
Speaker 4:  to make it so you can't trust any pictures. And in my my mentions on threads,

441
00:25:38,710 --> 00:25:40,880
Speaker 4:  it's a lot of people saying, well, you could never trust pictures.

442
00:25:41,540 --> 00:25:43,320
Speaker 1:  See, that's where I, I'm, I won't put there my

443
00:25:43,320 --> 00:25:47,240
Speaker 4:  Back. Okay. When I say it's a watershed moment in cameras, this is the camera

444
00:25:47,510 --> 00:25:50,800
Speaker 4:  that is making people say you cannot trust pictures.

445
00:25:51,680 --> 00:25:54,560
Speaker 1:  I I don't think that, I think, I don't think, I think there was a bunch of

446
00:25:54,570 --> 00:25:58,480
Speaker 1:  memes about sharks swimming through the streets of Houston that said I

447
00:25:58,480 --> 00:25:59,480
Speaker 1:  can never trust pictures.

448
00:26:00,020 --> 00:26:03,080
Speaker 4:  But those, but those people were like, but that it's like so obviously silly.

449
00:26:03,270 --> 00:26:03,560
Speaker 4:  Yeah,

450
00:26:03,560 --> 00:26:04,280
Speaker 1:  Yeah, that's true.

451
00:26:04,310 --> 00:26:08,000
Speaker 4:  This is like, the camera is just, it will be sold to you

452
00:26:08,540 --> 00:26:11,920
Speaker 4:  so that when you take a bunch of frames of your family, it will

453
00:26:12,250 --> 00:26:15,880
Speaker 4:  synthesize a thing that never happened by

454
00:26:15,880 --> 00:26:19,720
Speaker 4:  design. And that is a selling point of the camera, let me off and the

455
00:26:19,720 --> 00:26:22,720
Speaker 4:  market will not correct this. The market will say, that's great because I

456
00:26:22,720 --> 00:26:23,920
Speaker 4:  want better photos of my family. Yeah.

457
00:26:23,940 --> 00:26:24,920
Speaker 1:  It does feel very like,

458
00:26:25,180 --> 00:26:28,120
Speaker 4:  Let me offer you this other very silly example. I love it. And this is like

459
00:26:28,120 --> 00:26:30,760
Speaker 4:  based on a, a meme that I saw today. I'm

460
00:26:30,760 --> 00:26:31,240
Speaker 1:  So excited.

461
00:26:31,670 --> 00:26:34,520
Speaker 4:  It's really bad. You're you're gonna, can you pre forgive me for this? Okay.

462
00:26:34,830 --> 00:26:38,200
Speaker 4:  It's a camera and it takes a photo and ingests the photo.

463
00:26:39,660 --> 00:26:43,600
Speaker 4:  It processes the photo and throws the original photo away. Yeah. And so it's

464
00:26:43,600 --> 00:26:47,160
Speaker 4:  gone. You can never recover that photo. Yeah. And at the other end, everyone

465
00:26:47,160 --> 00:26:48,720
Speaker 4:  in your photo just has huge boobs.

466
00:26:50,860 --> 00:26:54,120
Speaker 4:  The market buys this, this is the most successful camera in human history.

467
00:26:54,400 --> 00:26:54,520
Speaker 4:  I

468
00:26:54,520 --> 00:26:57,320
Speaker 1:  Would do that all the time. Everyone's got big naturals.

469
00:26:57,820 --> 00:26:58,280
Speaker 4:  I'm just saying

470
00:26:58,390 --> 00:26:59,440
Speaker 1:  Just across the

471
00:26:59,570 --> 00:27:02,320
Speaker 4:  Board. I don't know. I think you probably have some sliders. Look, it doesn't

472
00:27:02,320 --> 00:27:06,160
Speaker 4:  matter. My point is the market does

473
00:27:06,160 --> 00:27:09,960
Speaker 4:  not reject this camera. Like the people of the United

474
00:27:10,020 --> 00:27:11,880
Speaker 4:  States are like, that's the camera we want.

475
00:27:12,150 --> 00:27:13,360
Speaker 1:  Yeah. 'cause big natural

476
00:27:13,970 --> 00:27:17,520
Speaker 4:  Jesus Christ. I should have agreed not to get mad at you.

477
00:27:17,920 --> 00:27:18,040
Speaker 4:  I,

478
00:27:18,110 --> 00:27:20,320
Speaker 1:  Yeah. Sorry David. We're all

479
00:27:20,370 --> 00:27:24,000
Speaker 4:  Judge. No, just like, if you just think about this like people want lies,

480
00:27:24,000 --> 00:27:25,320
Speaker 4:  they want pleasing lies.

481
00:27:25,880 --> 00:27:28,000
Speaker 1:  I didn't realize it gets rid of the old photo. Like

482
00:27:28,100 --> 00:27:29,480
Speaker 4:  No, this is my hypothetical saying.

483
00:27:29,480 --> 00:27:30,440
Speaker 1:  Oh yeah. 'cause like terrifying.

484
00:27:30,440 --> 00:27:31,560
Speaker 4:  This is just a made up example. I have

485
00:27:31,560 --> 00:27:34,720
Speaker 5:  You're also dead wrong about that by the way. Just like that thing you just

486
00:27:34,720 --> 00:27:38,680
Speaker 5:  described. No one want, they're like four horny teenagers on

487
00:27:38,720 --> 00:27:40,240
Speaker 5:  the internet. Who would be psyched about that? There's

488
00:27:40,240 --> 00:27:43,480
Speaker 4:  A Kickstarter for that camera right now. I guarantee you. And I want to cut.

489
00:27:44,500 --> 00:27:48,360
Speaker 4:  All I'm saying is if you But the market isn't gonna correct for truth. I,

490
00:27:48,440 --> 00:27:51,240
Speaker 4:  I made it that example. 'cause it's silly and it honestly, I wanted to see

491
00:27:51,820 --> 00:27:52,560
Speaker 4:  how I should react to it.

492
00:27:53,070 --> 00:27:54,440
Speaker 1:  It's my favorite idea. But

493
00:27:54,440 --> 00:27:58,200
Speaker 4:  Like in general, in the information ecosystem, we are

494
00:27:58,260 --> 00:28:02,200
Speaker 4:  in the, the market. The free market is not like truth

495
00:28:02,300 --> 00:28:06,200
Speaker 4:  is important to us. It's more like pictures of sharks. Yeah. Well,

496
00:28:06,250 --> 00:28:10,120
Speaker 4:  right. And so now you ma you've made this thing. Google

497
00:28:10,380 --> 00:28:13,040
Speaker 4:  has made this thing, the company that wants to organize the world's information

498
00:28:13,060 --> 00:28:16,960
Speaker 4:  has made a camera where right next to it camped out on the front lawn

499
00:28:18,190 --> 00:28:22,040
Speaker 4:  marketing the phone with we will synthesize pictures. That never

500
00:28:22,360 --> 00:28:26,280
Speaker 4:  happened. It never happened. We are gonna make you images that

501
00:28:26,280 --> 00:28:29,680
Speaker 4:  never occurred without a seconds hesitation

502
00:28:30,500 --> 00:28:33,280
Speaker 4:  on how that might go completely sideways for them. I mean,

503
00:28:33,280 --> 00:28:36,080
Speaker 1:  They're not just doing it with photos though, right? Like they're doing it

504
00:28:36,080 --> 00:28:37,600
Speaker 1:  across the board with generative ai.

505
00:28:38,200 --> 00:28:41,640
Speaker 4:  I mean, but this is like, when you talk about the AI apocalypse Yeah. Like

506
00:28:41,640 --> 00:28:45,400
Speaker 4:  the AI misinformation apocalypse is right on their front door. It's on

507
00:28:45,400 --> 00:28:49,040
Speaker 4:  YouTube. Deep fakes are on YouTube. Right. And at the same time, the argument

508
00:28:49,040 --> 00:28:52,440
Speaker 4:  is, well, we're just making, these tools are already available. We're just

509
00:28:52,440 --> 00:28:55,520
Speaker 4:  making them widely accessible to everyone. It's like, yeah, letting people

510
00:28:55,580 --> 00:28:59,120
Speaker 4:  lie at scale is actually, I dunno if you've been around the past five or

511
00:28:59,120 --> 00:28:59,440
Speaker 4:  10 years.

512
00:29:00,070 --> 00:29:00,480
Speaker 5:  Well, and

513
00:29:00,480 --> 00:29:01,440
Speaker 4:  It's, it's, it's something great.

514
00:29:01,550 --> 00:29:05,360
Speaker 5:  It's very funny to watch this happen as you see companies like open AI

515
00:29:05,370 --> 00:29:09,240
Speaker 5:  start to deliberately pull back on the thing that their

516
00:29:09,240 --> 00:29:12,840
Speaker 5:  stuff can do. Like Yeah. OpenAI put out the, the

517
00:29:12,890 --> 00:29:16,400
Speaker 5:  image recognition thing that you can, you can upload a picture to chat G

518
00:29:16,400 --> 00:29:19,560
Speaker 5:  P T and it'll try to sort of guess what you want from it and query it. And

519
00:29:20,180 --> 00:29:22,960
Speaker 5:  the thing you would want from that is the thing that you keep talking about

520
00:29:23,030 --> 00:29:25,840
Speaker 5:  Neli the thing where I'm just like, who is this person that's like, that's

521
00:29:25,840 --> 00:29:29,680
Speaker 5:  the feature. It won't do that. It could do that, but it won't because they

522
00:29:29,680 --> 00:29:33,400
Speaker 5:  know that that's a bad idea. That asking an AI chatbot about

523
00:29:33,720 --> 00:29:37,640
Speaker 5:  a person is a real bad idea. And it's just very funny to

524
00:29:37,640 --> 00:29:40,680
Speaker 5:  watch Google of all companies, the company that is like preaching You know,

525
00:29:40,680 --> 00:29:44,320
Speaker 5:  bold, but responsible. Just pull all of that apart. Wait,

526
00:29:44,320 --> 00:29:46,840
Speaker 4:  Where where in this camera is responsible?

527
00:29:46,840 --> 00:29:50,520
Speaker 5:  Nowhere. That's my point. Yeah. Just full bold. But if

528
00:29:50,550 --> 00:29:54,360
Speaker 5:  Photoshop was doing this and not Google Photos, and it was just as easy and

529
00:29:54,360 --> 00:29:56,640
Speaker 5:  just as good, but you had to pay for a creative Cloud membership to have

530
00:29:56,640 --> 00:29:58,200
Speaker 5:  it, would you have a problem with it? It,

531
00:29:58,200 --> 00:30:00,200
Speaker 4:  So it isn't effectively in Photoshop,

532
00:30:00,350 --> 00:30:03,160
Speaker 5:  It's this much harder in Photoshop, but not that much harder.

533
00:30:03,550 --> 00:30:07,400
Speaker 4:  It's it's that much harder. There's face swap exists on, I

534
00:30:07,420 --> 00:30:11,320
Speaker 4:  You know there's a million of these things that exist, but

535
00:30:11,340 --> 00:30:15,120
Speaker 4:  the image pipeline from camera to editing software to

536
00:30:15,520 --> 00:30:19,360
Speaker 4:  distribution is important. Right. Because the thing you

537
00:30:19,360 --> 00:30:22,880
Speaker 4:  need to trust is the camera and Google is right on the

538
00:30:23,140 --> 00:30:26,680
Speaker 4:  blurry edge. The camera's still fine. The camera's still just doing h D R

539
00:30:26,740 --> 00:30:30,000
Speaker 4:  as far as I can tell. Right. And maybe it's like recovering some detail.

540
00:30:30,780 --> 00:30:34,320
Speaker 4:  But if you just look at how they're talking about the camera, the

541
00:30:34,720 --> 00:30:38,440
Speaker 4:  features in Google, Photos in Google's marketing are

542
00:30:39,160 --> 00:30:42,280
Speaker 4:  integral to the experience of the camera. That's weird.

543
00:30:43,260 --> 00:30:47,240
Speaker 4:  And when I say the line is blurry, yes. And a, on a very basic

544
00:30:47,350 --> 00:30:51,200
Speaker 4:  technical level, some features are in one app and some features are in another

545
00:30:51,200 --> 00:30:54,640
Speaker 4:  app. But then on a much more like

546
00:30:55,230 --> 00:30:59,080
Speaker 4:  intuitive level, Google Photos is the gallery app on your phone.

547
00:30:59,190 --> 00:31:01,800
Speaker 4:  Like, it's just like the, when you look at the photos, it's like, do you

548
00:31:01,800 --> 00:31:05,040
Speaker 4:  wanna edit them? Do you wanna combine these? Like it's driving you towards

549
00:31:05,410 --> 00:31:09,320
Speaker 4:  these choices in a way that having Photoshop

550
00:31:09,320 --> 00:31:12,880
Speaker 4:  is like, I'm intentionally gonna go do this thing. Okay. I don't know. I

551
00:31:12,950 --> 00:31:13,360
Speaker 4:  it's because

552
00:31:13,590 --> 00:31:16,680
Speaker 1:  It's suggesting rather like, like it's removing that intention.

553
00:31:17,220 --> 00:31:19,560
Speaker 1:  That's kind of, it feels like where the issue is, right?

554
00:31:19,560 --> 00:31:23,440
Speaker 4:  You it, it is. Right. Google to its credit. Right Google, I know

555
00:31:23,440 --> 00:31:26,160
Speaker 4:  that they're thinking about this and they've made this choice in another

556
00:31:26,160 --> 00:31:28,760
Speaker 4:  year. Another Android, o e m,

557
00:31:29,850 --> 00:31:33,200
Speaker 4:  which is either less responsible, more aggressive, or however you wanna say

558
00:31:33,200 --> 00:31:35,880
Speaker 4:  it, is gonna build it right into the shutter button of the phone. Right.

559
00:31:36,260 --> 00:31:39,040
Speaker 4:  And they're gonna say, take a bunch of pictures and we will AI generate everyone

560
00:31:39,040 --> 00:31:41,920
Speaker 4:  looking at, at you right away. And, and we will throw away the original

561
00:31:42,180 --> 00:31:43,520
Speaker 1:  And you're gonna look super hot.

562
00:31:43,790 --> 00:31:45,280
Speaker 5:  Yeah. That's the camera that would sell

563
00:31:45,500 --> 00:31:48,720
Speaker 4:  In one, in one specific way. Alex, if that's what you're into. My, my

564
00:31:48,720 --> 00:31:52,560
Speaker 5:  Other question is, I think this, this question of like taking a bunch of

565
00:31:52,560 --> 00:31:56,480
Speaker 5:  photos that exist in order to make one that never did, there is

566
00:31:56,480 --> 00:32:00,200
Speaker 5:  something there that is sort of icky, but if, if that was just saying

567
00:32:00,430 --> 00:32:03,200
Speaker 5:  like if Google said we're gonna do the same thing, but we're gonna treat

568
00:32:03,200 --> 00:32:06,960
Speaker 5:  it essentially as like merging burst mode where we're not going to use a

569
00:32:06,960 --> 00:32:10,680
Speaker 5:  single picture that wasn't captured. We're just gonna use the best Pixel

570
00:32:11,620 --> 00:32:15,600
Speaker 5:  in every part of the photo. From the 10 images that you took, would you still

571
00:32:15,600 --> 00:32:19,280
Speaker 5:  have a problem with that? Every Pixel of that is real. It's just not all

572
00:32:19,280 --> 00:32:19,800
Speaker 5:  at the same time.

573
00:32:20,230 --> 00:32:23,600
Speaker 4:  Yeah. Everything happened, but not all at the same time. Travis Kelsey never

574
00:32:23,600 --> 00:32:25,800
Speaker 4:  looked at Taylor Swift, but we've invented a photo. No,

575
00:32:25,800 --> 00:32:28,640
Speaker 5:  That's what I'm saying. Travis Kelsey looked at Taylor Swift, but not at

576
00:32:28,640 --> 00:32:31,200
Speaker 5:  the same moment. Hugh Jackman was also making that funny face.

577
00:32:31,530 --> 00:32:34,960
Speaker 4:  Right. But so now we've now, but now we've created a moment that does, this

578
00:32:34,960 --> 00:32:35,680
Speaker 4:  is dangerous

579
00:32:36,000 --> 00:32:38,520
Speaker 5:  Territory. You're but go with me here philosophically. Go with me here. Yeah.

580
00:32:38,520 --> 00:32:42,440
Speaker 5:  If, if that last step of, we've used all that data cr to create a

581
00:32:42,440 --> 00:32:43,200
Speaker 5:  thing that never happened,

582
00:32:43,820 --> 00:32:44,960
Speaker 1:  But every single bit of that

583
00:32:44,960 --> 00:32:47,800
Speaker 4:  Data, so what you're gring by the way is just, this is just composite photography.

584
00:32:47,800 --> 00:32:50,280
Speaker 4:  Yeah. The number, again, the number of people that I mentioned who are just

585
00:32:50,280 --> 00:32:53,400
Speaker 4:  like recapitulating the history. I, I know, right? I, I know that all these

586
00:32:53,400 --> 00:32:55,480
Speaker 4:  ideas have existed and all these tools have existed before. Right. But if

587
00:32:55,480 --> 00:32:55,560
Speaker 4:  you

588
00:32:55,560 --> 00:32:59,120
Speaker 5:  Take composite photography and make it that easy and that accessible, do

589
00:32:59,120 --> 00:33:00,400
Speaker 5:  you have the same problem with it? Yes.

590
00:33:00,510 --> 00:33:04,200
Speaker 4:  Like, without question. Interesting. Okay. Because your expectations

591
00:33:04,900 --> 00:33:08,320
Speaker 4:  of these images are different. You have to change

592
00:33:08,630 --> 00:33:12,480
Speaker 4:  society's norms and maybe the norm is never trust a

593
00:33:12,480 --> 00:33:15,760
Speaker 4:  photo again. Yeah. Okay. Big decision. Yeah.

594
00:33:16,190 --> 00:33:20,080
Speaker 4:  Just putting that out there. Huge decision. Have you been

595
00:33:20,080 --> 00:33:23,560
Speaker 4:  around, have you looked at the internet and how quickly and easily people

596
00:33:23,560 --> 00:33:27,440
Speaker 4:  will believe photos. Okay, I need you instead

597
00:33:27,440 --> 00:33:31,160
Speaker 4:  of tweeting at me to go talk to all of them. Let me know when you're done.

598
00:33:31,560 --> 00:33:35,240
Speaker 4:  Right. Like fine. Like I, that's a huge

599
00:33:35,390 --> 00:33:39,040
Speaker 4:  norm shift in society. It is not as simple as waving it away by saying you

600
00:33:39,040 --> 00:33:42,480
Speaker 4:  shouldn't trust anything. If you would like to get to a place

601
00:33:42,850 --> 00:33:46,360
Speaker 4:  where no one trusts anything, then you should play that all the way out.

602
00:33:46,980 --> 00:33:50,880
Speaker 4:  And so You know the flip side of this is, I think this is fine for creative

603
00:33:50,880 --> 00:33:54,560
Speaker 4:  use. I think this is probably fine for family photos. Like where the

604
00:33:54,580 --> 00:33:58,160
Speaker 4:  stakes of the images are low, probably fine.

605
00:33:58,330 --> 00:34:01,840
Speaker 5:  Which is the vast majority of images. The vast majority

606
00:34:02,290 --> 00:34:05,880
Speaker 4:  Maybe like, I don't know. I just, we, we now create,

607
00:34:06,150 --> 00:34:09,160
Speaker 4:  when you say the vast majority, we create so many photos as a people

608
00:34:09,990 --> 00:34:13,760
Speaker 4:  that like the vast majority of everything in all photos is nothing

609
00:34:14,470 --> 00:34:18,200
Speaker 5:  Fair. But I mean, think about even your own life, right? Like the, the number

610
00:34:18,200 --> 00:34:21,360
Speaker 5:  of pictures in your camera roll. And I suspect this is true of most people

611
00:34:21,510 --> 00:34:25,120
Speaker 5:  that will be viewed by frankly just you and no one else ever,

612
00:34:25,540 --> 00:34:29,440
Speaker 5:  or you and 10 or fewer people ever

613
00:34:30,180 --> 00:34:34,120
Speaker 5:  is Sure. All practically all of them. Yeah. And you're a pretty

614
00:34:34,140 --> 00:34:37,000
Speaker 5:  public person in the scheme of things, right? Like Yeah. Right

615
00:34:37,000 --> 00:34:40,520
Speaker 4:  Now most of the photos in my camera roll are of like light switches that

616
00:34:40,520 --> 00:34:43,680
Speaker 4:  I need to rewire and I just need to remember what colors are going on. Yeah.

617
00:34:43,900 --> 00:34:47,280
Speaker 4:  And it's like, boy, I hope The Pixel doesn't lie to me about that. That seems

618
00:34:47,280 --> 00:34:50,920
Speaker 4:  like a fire hazard. Like You know, like whatever. You're, you're not wrong.

619
00:34:51,180 --> 00:34:54,280
Speaker 4:  I'm just saying like, it'd be crazy, right? Like

620
00:34:54,280 --> 00:34:57,920
Speaker 1:  The idea is like You know what that red light like cable, that's not pretty.

621
00:34:57,990 --> 00:34:58,440
Speaker 1:  Yeah, exactly.

622
00:34:58,440 --> 00:35:02,120
Speaker 4:  Yellow. What are we, oh, what? Break that up a little bit. Sorry. Black

623
00:35:02,150 --> 00:35:05,960
Speaker 4:  wire. No, no, no. Right. Like it's not gonna do that. Fine. The

624
00:35:06,320 --> 00:35:09,960
Speaker 4:  revolution in photography is that more people use the tool

625
00:35:10,070 --> 00:35:14,000
Speaker 4:  than ever before in phones. Right? That, that's just so you

626
00:35:14,000 --> 00:35:17,440
Speaker 4:  have this like shared understanding, worldwide

627
00:35:18,280 --> 00:35:22,120
Speaker 4:  cultural understanding of how the camera works. First time

628
00:35:22,120 --> 00:35:26,080
Speaker 4:  in human history, that you have had a tool where everyone is using

629
00:35:26,100 --> 00:35:29,400
Speaker 4:  it the same way to make the same thing. And then

630
00:35:29,830 --> 00:35:33,080
Speaker 4:  importantly closing the loop, consuming it on the same device.

631
00:35:33,950 --> 00:35:36,320
Speaker 4:  That piece is really important. We talked about that with the vision pro

632
00:35:36,320 --> 00:35:40,240
Speaker 4:  and spatial photos the other day. Like it breaks a chain for Apple where

633
00:35:40,240 --> 00:35:42,920
Speaker 4:  you're taking pictures on a phone that you can't view. 'cause they're designed

634
00:35:42,920 --> 00:35:46,840
Speaker 4:  for the vision Pro right now with phones, the chain is, the loop is complete.

635
00:35:47,380 --> 00:35:50,840
Speaker 4:  You, you have a tool, you can take photos, you can look at photos other people

636
00:35:50,840 --> 00:35:54,480
Speaker 4:  took on largely the same tool on on your, that's very important. That's a

637
00:35:54,480 --> 00:35:58,000
Speaker 4:  shared. So if you break that for people and you say

638
00:35:58,420 --> 00:36:02,160
Speaker 4:  you understand how unreliable your tool is, right?

639
00:36:02,190 --> 00:36:05,800
Speaker 4:  That you can, you are often using this tool to make things that never

640
00:36:06,120 --> 00:36:09,240
Speaker 4:  happened. Well then you change everyone else's understanding of everyone

641
00:36:09,240 --> 00:36:12,880
Speaker 4:  else's tools. And that is like a long process, a slow

642
00:36:12,880 --> 00:36:16,480
Speaker 4:  process, ideally a very considered process.

643
00:36:17,180 --> 00:36:20,080
Speaker 4:  And like here we are on the doorstep and Google's answer is, well, we put

644
00:36:20,080 --> 00:36:22,760
Speaker 4:  some metadata in it and sort of everyone else's response is, well you could

645
00:36:22,760 --> 00:36:25,840
Speaker 4:  always do this with Photoshop. And it's like, yes, I could always lie.

646
00:36:26,880 --> 00:36:30,690
Speaker 4:  What I did not do is culturally recontextualize the very

647
00:36:30,690 --> 00:36:34,530
Speaker 4:  nature of lying spread across every phone in existence. Sure.

648
00:36:34,910 --> 00:36:38,770
Speaker 4:  And like that's this moment. Like we should just be honest

649
00:36:38,770 --> 00:36:42,410
Speaker 4:  with ourselves. The next phone or the phone after this

650
00:36:42,870 --> 00:36:46,490
Speaker 4:  is gonna have it in the shutter button. And when it's in the shutter button,

651
00:36:46,550 --> 00:36:50,490
Speaker 4:  the camera becomes totally unreliable and unreliable narrator of

652
00:36:50,490 --> 00:36:54,450
Speaker 4:  the events. And we are now just producing memories and not photographs

653
00:36:55,070 --> 00:36:58,850
Speaker 4:  and all of the norms around photos about what is acceptable editing,

654
00:36:59,750 --> 00:37:03,610
Speaker 4:  how much skin smoothing is acceptable. I, I don't know what should,

655
00:37:03,610 --> 00:37:07,450
Speaker 4:  what should people, like people with darker skin tones, what should our complexions

656
00:37:07,450 --> 00:37:11,290
Speaker 4:  look like? All those norms get tossed away in favor of well, what

657
00:37:11,290 --> 00:37:15,210
Speaker 4:  did you want? Yeah. And this camera will not Neli deliver you what you want

658
00:37:15,210 --> 00:37:16,610
Speaker 4:  instead of what is true. I mean,

659
00:37:17,020 --> 00:37:20,530
Speaker 1:  We've been seeing that for a while now in, in phones in China where they

660
00:37:20,530 --> 00:37:24,210
Speaker 1:  would lighten skin and smooth everybody out automatically and you couldn't

661
00:37:24,430 --> 00:37:27,610
Speaker 1:  escape it. I think even one of the early iPhones did that, right? No. So,

662
00:37:27,710 --> 00:37:31,170
Speaker 4:  So this has been a conversation that I've had with the various

663
00:37:31,170 --> 00:37:33,970
Speaker 4:  photography teams. Yeah. All these, again, we've been having this conversation

664
00:37:33,970 --> 00:37:37,490
Speaker 4:  about something as simple as exposure for a long time,

665
00:37:37,680 --> 00:37:41,290
Speaker 4:  like for 10 generations of the phone. Yeah.

666
00:37:41,400 --> 00:37:45,250
Speaker 4:  Like exposure, just how bright and light are

667
00:37:45,250 --> 00:37:49,130
Speaker 4:  things. And then you add on top of that skin smoothing And, what you have

668
00:37:49,130 --> 00:37:52,970
Speaker 4:  found, what we have all found, what the major companies have found is

669
00:37:53,390 --> 00:37:57,090
Speaker 4:  the Chinese market and the South Korean market want built-in beauty

670
00:37:57,090 --> 00:38:00,570
Speaker 4:  filters. And the American market thinks that is a moral

671
00:38:00,600 --> 00:38:04,530
Speaker 4:  catastrophe. And it probably is, I don't know. It

672
00:38:04,530 --> 00:38:08,410
Speaker 4:  is. That hasn't doesn't stop the people on TikTok from using them all the

673
00:38:08,410 --> 00:38:11,770
Speaker 4:  time, but like Yeah. Whatever. And so they've had to split the difference.

674
00:38:12,230 --> 00:38:15,610
Speaker 1:  But I'm saying that gives us like a framework as we go into this next

675
00:38:16,180 --> 00:38:19,610
Speaker 1:  phase with Google and, and, and people who follow after Google is we kind

676
00:38:19,610 --> 00:38:22,810
Speaker 1:  of have that framework. We've seen how it happens in China. We've seen how

677
00:38:22,810 --> 00:38:25,210
Speaker 1:  it happens in South Korea. Yeah. To some extent.

678
00:38:25,360 --> 00:38:28,170
Speaker 4:  Yeah. We'll see, I mean like, should, again, this is my point.

679
00:38:28,290 --> 00:38:29,130
Speaker 1:  I mean, I, I, this

680
00:38:29,130 --> 00:38:32,810
Speaker 4:  Is my point about my fake camera. Like the market in this country,

681
00:38:33,550 --> 00:38:37,400
Speaker 4:  the market rejected the idea of aggressive skin smoothing.

682
00:38:37,530 --> 00:38:41,440
Speaker 4:  Right? That's actually like, maybe not what you would've expected.

683
00:38:42,020 --> 00:38:46,000
Speaker 4:  And you have another, you have a counter example with a market in Asian countries

684
00:38:46,360 --> 00:38:49,880
Speaker 4:  actually wanted it. And You know Samsung basically gives it to them.

685
00:38:50,570 --> 00:38:53,520
Speaker 4:  Apple kind of doesn't You know. Like they're, they're like, you can get the

686
00:38:53,520 --> 00:38:56,120
Speaker 4:  apps. Like that's more or less their answer when you ask them like, You know

687
00:38:56,620 --> 00:38:59,720
Speaker 4:  we face two, we see face two and like all this stuff exists. And they're

688
00:38:59,720 --> 00:39:03,320
Speaker 4:  like, you can just get it. We have blue bubbles. We're good. Right? I don't

689
00:39:03,320 --> 00:39:06,720
Speaker 4:  know what the market is gonna say about the camera can swap faces around.

690
00:39:07,240 --> 00:39:08,760
Speaker 4:  I mean, I, I don't, I thought

691
00:39:08,760 --> 00:39:12,240
Speaker 1:  It was kind of terrifying. And there's a video on our TikTok right now

692
00:39:12,660 --> 00:39:16,160
Speaker 1:  of Allison and Becca using it and it's

693
00:39:16,160 --> 00:39:19,560
Speaker 1:  horrifying if You know Allison because her face just keeps switching

694
00:39:20,060 --> 00:39:21,960
Speaker 1:  but her body doesn't change. And you're like,

695
00:39:22,090 --> 00:39:25,280
Speaker 4:  Right. So here's some important caveats to this entire conversation. Yeah.

696
00:39:25,380 --> 00:39:27,560
Speaker 4:  One, we haven't reviewed the phones. I don't know how good this stuff is.

697
00:39:27,760 --> 00:39:31,680
Speaker 4:  I have a sense of how good it is. Two, I've said nothing about like the audio

698
00:39:31,790 --> 00:39:35,400
Speaker 4:  filters where like n video, it just makes people's voices stand out against,

699
00:39:35,500 --> 00:39:38,960
Speaker 4:  I'm like, that's great. You know. Like I maybe I'm morally compromised in

700
00:39:38,960 --> 00:39:42,920
Speaker 4:  that front too, and it's not built into the camera and Photoshop exists and

701
00:39:42,920 --> 00:39:46,840
Speaker 4:  all this stuff and like whatever. I'm just saying there's not a lick

702
00:39:46,840 --> 00:39:49,720
Speaker 4:  of introspection here. There's Google saying we put some shit in the exif

703
00:39:49,720 --> 00:39:52,600
Speaker 4:  data to market as edited. And it's like, guys,

704
00:39:53,230 --> 00:39:55,920
Speaker 1:  Well it's really funny that this is coming from Google who's been very vocal

705
00:39:55,920 --> 00:39:59,560
Speaker 1:  about like, we're doing ai, but Right. And it's like, okay, but you just,

706
00:40:00,060 --> 00:40:03,560
Speaker 1:  how is this any different than the gen like putting out a generative

707
00:40:03,790 --> 00:40:06,880
Speaker 1:  text AI that can just lie. Well, yeah,

708
00:40:07,380 --> 00:40:10,840
Speaker 5:  That's what I keep coming back to during this conversation is like the, the

709
00:40:10,840 --> 00:40:14,040
Speaker 5:  thing we're describing here is what's happening with everything everywhere

710
00:40:14,040 --> 00:40:17,280
Speaker 5:  on the internet, right? Like this idea that it's, we've made it trivially

711
00:40:17,280 --> 00:40:21,200
Speaker 5:  easy to lie to a lot of people is like the story of the social era on

712
00:40:21,200 --> 00:40:25,080
Speaker 5:  the internet. Yes. And first it was easy to lie in a text

713
00:40:25,080 --> 00:40:28,280
Speaker 5:  post and then it was easy to lie by making a website that looked like a news

714
00:40:28,280 --> 00:40:31,240
Speaker 5:  post. And then it was easy to lie because you were associated with the Russian

715
00:40:31,240 --> 00:40:34,600
Speaker 5:  government and now it's easy to lie in photos.

716
00:40:35,080 --> 00:40:38,920
Speaker 5:  So like, I think the, the arc of that is not that different, it's just

717
00:40:38,920 --> 00:40:41,200
Speaker 5:  that it's getting sort of higher fidelity. And we're gonna go through this

718
00:40:41,200 --> 00:40:45,040
Speaker 5:  with video in a very real way at some point very soon too, as

719
00:40:45,180 --> 00:40:47,640
Speaker 5:  all of these image generators get better. I mean, we should be talking about

720
00:40:47,810 --> 00:40:50,560
Speaker 5:  Dolly three and the fact that it's able to do things that look like photos

721
00:40:50,780 --> 00:40:54,440
Speaker 5:  now, like we are now past the point where you can quickly glance

722
00:40:54,620 --> 00:40:58,600
Speaker 5:  at a generated image that is wholly generated and immediately go, oh,

723
00:40:58,600 --> 00:41:02,560
Speaker 5:  that's ai. Like we, we are past that. You can no longer do that unless

724
00:41:02,760 --> 00:41:06,680
Speaker 5:  somebody wanted it to not look like a photo. And all, all of this

725
00:41:06,700 --> 00:41:10,400
Speaker 5:  is I think, wrapped up together. And I think like the means of

726
00:41:10,720 --> 00:41:14,680
Speaker 5:  creation in in Dolly three is in a lot of ways easier than, I

727
00:41:14,680 --> 00:41:17,400
Speaker 5:  don't even have to buy a Pixel to do that. I can just go to a, I go to Bing

728
00:41:17,420 --> 00:41:20,960
Speaker 5:  and I can just do that. Right. So I think yeah, these big questions of like

729
00:41:21,330 --> 00:41:24,400
Speaker 5:  trust and rules, and I think, I think you posted about this earlier, but

730
00:41:24,400 --> 00:41:28,320
Speaker 5:  like I've been going back and rereading like the APS rules about

731
00:41:28,340 --> 00:41:32,040
Speaker 5:  how they do photos and Getty's rules about how they do photos. And like for

732
00:41:32,040 --> 00:41:34,880
Speaker 5:  people who take the kinds of high stake photos that you're talking about,

733
00:41:35,090 --> 00:41:39,000
Speaker 5:  there have always been rules about what you're allowed to edit and how.

734
00:41:39,420 --> 00:41:43,280
Speaker 5:  And basically like you can like crop and you can vignette and

735
00:41:43,280 --> 00:41:47,160
Speaker 5:  that's essentially it. And you can't even really mess with things

736
00:41:47,180 --> 00:41:50,520
Speaker 5:  to make them more beautiful. The idea is like you took the photo, that is

737
00:41:50,520 --> 00:41:52,200
Speaker 5:  what it looked like. We are going to publish that.

738
00:41:52,540 --> 00:41:55,840
Speaker 4:  If you look at the, the New York Times and, and sort of their

739
00:41:55,910 --> 00:41:59,840
Speaker 4:  feature photography, the vignette is out of control because it's,

740
00:41:59,840 --> 00:42:01,040
Speaker 4:  it's the one knob that they can turn.

741
00:42:02,080 --> 00:42:05,800
Speaker 5:  Yeah. And so I think you're gonna start to see these folks, like in the same

742
00:42:05,800 --> 00:42:09,680
Speaker 5:  way that we have more and more people being more and more outspoken

743
00:42:09,770 --> 00:42:13,600
Speaker 5:  about things that they're using, like Ja G P T people are coming out with

744
00:42:13,880 --> 00:42:16,440
Speaker 5:  policies about how they do and don't use it. I think you're gonna start to

745
00:42:16,500 --> 00:42:20,400
Speaker 5:  see the same thing with images. I do wonder if the question is going to be

746
00:42:20,400 --> 00:42:23,840
Speaker 5:  like we're going from everything is now possible on a smartphone to like,

747
00:42:23,880 --> 00:42:27,640
Speaker 5:  I wonder if smartphone photos get outlawed from some of these rules

748
00:42:27,670 --> 00:42:29,880
Speaker 5:  over time for exactly the reason you're describing.

749
00:42:30,140 --> 00:42:33,920
Speaker 4:  So you brought up Getty. So You know these high stakes image pipelines. You

750
00:42:33,920 --> 00:42:36,800
Speaker 4:  can't solve it in technology. You have to solve it by getting a bunch of

751
00:42:36,800 --> 00:42:40,680
Speaker 4:  people to agree that this is what will happen. You norms, right? You

752
00:42:40,680 --> 00:42:44,080
Speaker 4:  will do this, you will not do this. So if you are a Getty News

753
00:42:44,080 --> 00:42:48,040
Speaker 4:  photographer, you, you just agree these are the guidelines and all we

754
00:42:48,040 --> 00:42:50,640
Speaker 4:  can really do is crop and all we can really do is maybe a little dodging

755
00:42:50,640 --> 00:42:53,880
Speaker 4:  and burning. I think Getty allows you to remove sensor dust or the AP allows

756
00:42:53,880 --> 00:42:57,160
Speaker 4:  you to remove sensor, but then they're like, but only if it doesn't change

757
00:42:57,160 --> 00:43:00,600
Speaker 4:  the photo. And if you have a question like ask us. So

758
00:43:00,940 --> 00:43:04,600
Speaker 4:  you can't enforce cultural norms in software. You just

759
00:43:04,650 --> 00:43:08,480
Speaker 4:  can't. You can influence them. You can, you can do some stuff, but

760
00:43:08,780 --> 00:43:11,680
Speaker 4:  at the end of the day, it's just a bunch of people that have to agree on

761
00:43:11,680 --> 00:43:15,480
Speaker 4:  how things work. So that's the news side of it. On the creative side, I think

762
00:43:15,480 --> 00:43:18,120
Speaker 4:  it's like wide open. Do whatever you want. Have Photoshop's great. Generate

763
00:43:18,200 --> 00:43:20,960
Speaker 4:  a cat, like have all the time. That's what they, that was the demo they did

764
00:43:20,960 --> 00:43:24,280
Speaker 4:  with us on, on stage at code. Like fine, I think that stuff is exciting

765
00:43:24,780 --> 00:43:28,640
Speaker 4:  on the high stakes. Are you representing the truth to a lot of people?

766
00:43:29,340 --> 00:43:32,840
Speaker 4:  Is Donald Trump looking at the protesters or is he looking at his shoes and

767
00:43:32,840 --> 00:43:36,680
Speaker 4:  did you remove like this matters, right? These like really some of these

768
00:43:36,680 --> 00:43:40,640
Speaker 4:  images really matter what they say. There is an entire like

769
00:43:40,640 --> 00:43:43,720
Speaker 4:  ecosystem of talkers who just look at pictures and tell you what Taylor Swift

770
00:43:43,720 --> 00:43:47,640
Speaker 4:  is thinking. Yep. It matters what the pictures say. Okay. So

771
00:43:47,640 --> 00:43:51,280
Speaker 4:  on those high stakes pipelines, you have to set some rules. So I, you mentioned

772
00:43:51,370 --> 00:43:54,600
Speaker 4:  Getty. Getty, c e o, Craig Peters was on the code stage with me talking about

773
00:43:55,030 --> 00:43:57,240
Speaker 4:  AI last week. I just hit him up and said, Hey, did you look at The Pixel

774
00:43:57,240 --> 00:44:00,480
Speaker 4:  slate? What do you think of it? Would you ever like ban a camera?

775
00:44:01,260 --> 00:44:04,160
Speaker 4:  And he said, he obviously did not answer The Pixel lady. He's like, I haven't

776
00:44:04,160 --> 00:44:07,200
Speaker 4:  played with you yet. He said, we will largely continue to navigate along

777
00:44:07,200 --> 00:44:10,680
Speaker 4:  the following lines within our editorial coverage. This is the

778
00:44:10,680 --> 00:44:14,560
Speaker 4:  editorial coverage at Getti, the news coverage, strict avoidance

779
00:44:14,580 --> 00:44:18,480
Speaker 4:  of any modifications to the image. Camera manufacturers are already making

780
00:44:18,480 --> 00:44:22,240
Speaker 4:  some adjustments to light on the sensor, but we are as strict the image as

781
00:44:22,440 --> 00:44:26,320
Speaker 4:  possible. So that's, that's it. No changes. And then he said

782
00:44:26,320 --> 00:44:30,080
Speaker 4:  within creative, the sort of artistic side of Getty, we allow for a bunch

783
00:44:30,080 --> 00:44:33,960
Speaker 4:  of stuff. He said, I see us allowing things within creative context,

784
00:44:34,100 --> 00:44:37,640
Speaker 4:  but we will not allow misrepresentation. So then my next question to him

785
00:44:37,660 --> 00:44:40,920
Speaker 4:  was, if there's a hypothetical phone

786
00:44:41,470 --> 00:44:45,320
Speaker 4:  that just like aggressively smooth skin outta the box, is that

787
00:44:45,320 --> 00:44:49,280
Speaker 4:  allowed? And he said, we might, there aren't any, we we don't

788
00:44:49,360 --> 00:44:53,120
Speaker 4:  have this problem today, but if things move in that direction, we could.

789
00:44:54,020 --> 00:44:57,840
Speaker 4:  So even like Getty is at the point where they're like, we're watching it.

790
00:44:58,380 --> 00:45:01,880
Speaker 1:  So theoretically they could be like, next Pixel phone. No, you can't submit

791
00:45:01,880 --> 00:45:05,400
Speaker 4:  Phone or whatever. Unnamed You know o e m

792
00:45:05,530 --> 00:45:09,160
Speaker 4:  makes a phone and their big camera feature is like, you, you're always beautiful.

793
00:45:09,780 --> 00:45:13,400
Speaker 4:  I'm not gonna say it. Big naturals, god dammit, You know. Like Getty would

794
00:45:13,400 --> 00:45:16,880
Speaker 4:  be like this, this camera, this pipeline is not acceptable to us. Yeah. And

795
00:45:16,880 --> 00:45:20,440
Speaker 4:  I think just the fact that one, I can ask the c e o of Getty that question

796
00:45:21,180 --> 00:45:25,040
Speaker 4:  and he's like, this is the answer. Like, we are considering it.

797
00:45:25,140 --> 00:45:28,200
Speaker 4:  That's why I'm saying this is a watershed moment. I'm not saying this phone,

798
00:45:28,300 --> 00:45:31,080
Speaker 4:  I'm not saying you need to be outraged about this phone. I'm not saying you

799
00:45:31,080 --> 00:45:34,720
Speaker 4:  shouldn't buy the phone seven years of software updates. You know, buy It

800
00:45:34,720 --> 00:45:38,560
Speaker 4:  seems nice, have a time, but go nuts. Yeah. I'm saying that in the context

801
00:45:38,740 --> 00:45:42,160
Speaker 4:  of what is a photo Google has decided

802
00:45:42,990 --> 00:45:46,120
Speaker 4:  that they don't make photos anymore. That they make memories. That's the

803
00:45:46,120 --> 00:45:49,960
Speaker 4:  quote they gave to Allison Johnson in her interview about with a Google

804
00:45:50,100 --> 00:45:53,800
Speaker 4:  PM about this, these tools. And they're like, these are memories.

805
00:45:53,980 --> 00:45:57,480
Speaker 4:  Mm. And I'm just would remind people that

806
00:45:57,800 --> 00:46:01,640
Speaker 4:  memories are famously fallible, famously unreliable,

807
00:46:02,380 --> 00:46:06,360
Speaker 4:  not actually allowed in many objective contexts where people need to

808
00:46:06,360 --> 00:46:09,840
Speaker 4:  know what happened. Like that's crazy right? To, to make that

809
00:46:10,120 --> 00:46:13,560
Speaker 4:  rhetorical shift. Yeah. It's not to say photos don't lie. It's not to say

810
00:46:13,560 --> 00:46:17,440
Speaker 4:  photos can't be biased. It's not to say photos can't be added. It's this

811
00:46:17,940 --> 00:46:21,720
Speaker 4:  one camera. Okay. We should probably start having a really

812
00:46:21,960 --> 00:46:25,800
Speaker 4:  serious conversation about AI and cameras and how they

813
00:46:25,800 --> 00:46:29,400
Speaker 4:  collide and whether or not we as a culture, as a

814
00:46:29,400 --> 00:46:33,240
Speaker 4:  society expect our photos to be representations

815
00:46:33,240 --> 00:46:36,800
Speaker 4:  of moments in time. Whether we expect cameras to produce that. And if we

816
00:46:36,800 --> 00:46:40,740
Speaker 4:  don't, we don't think that's true, then we should

817
00:46:41,020 --> 00:46:43,660
Speaker 4:  probably have a huge conversation about how we start need to start training

818
00:46:43,900 --> 00:46:46,380
Speaker 4:  children from a young age to never believe their own eyes.

819
00:46:47,180 --> 00:46:48,460
Speaker 1:  I mean, I think we should do that anyway, but

820
00:46:48,460 --> 00:46:51,620
Speaker 4:  Yeah, fine. I mean if you wanna like, look in the end, we're all gonna be

821
00:46:51,620 --> 00:46:52,260
Speaker 4:  in headsets. I'm,

822
00:46:52,640 --> 00:46:54,460
Speaker 1:  I'm have a real seminar for five year olds. It's gonna be great.

823
00:46:54,560 --> 00:46:57,540
Speaker 4:  But, but that's like, those are the stakes. You know, and, and I, I think

824
00:46:57,540 --> 00:47:01,460
Speaker 4:  there's a lot of people kind of like doing nihilism in

825
00:47:01,460 --> 00:47:04,300
Speaker 4:  my mentions that are like, well, nothing was any or anything like this is

826
00:47:04,300 --> 00:47:08,060
Speaker 4:  just, and it's like, no, there's actually stakes. Like the, it means something

827
00:47:08,060 --> 00:47:11,700
Speaker 4:  to give everyone the tool. Yeah. If I was like, I'm just making guns easier

828
00:47:11,700 --> 00:47:13,740
Speaker 4:  to buy, you wouldn't be like, well, everyone's always been able to buy. Like

829
00:47:14,080 --> 00:47:16,340
Speaker 4:  no, it actually means something to give everyone the tool where they can

830
00:47:16,400 --> 00:47:20,380
Speaker 4:  lie at scale and it to brush it off is these aren't photos, these are

831
00:47:20,620 --> 00:47:23,100
Speaker 4:  memories. And if that's what you wanna say, then okay, you haven't made a

832
00:47:23,100 --> 00:47:26,940
Speaker 4:  camera. But it's, it's, I'm not, not like outraged.

833
00:47:27,080 --> 00:47:30,560
Speaker 4:  I'm, we've been doing what is a photo for so long that I'm just like, oh,

834
00:47:30,560 --> 00:47:32,560
Speaker 4:  this is, this is the culmination of my life's work

835
00:47:32,700 --> 00:47:34,320
Speaker 1:  And we're gonna keep going. We got, we

836
00:47:34,320 --> 00:47:37,640
Speaker 4:  Got reviews now, by the way, now I have to buy a Pixel eight. Yeah. Like

837
00:47:37,720 --> 00:47:40,760
Speaker 4:  I have to have one just to be like, this is a watershed. We should all buy

838
00:47:40,760 --> 00:47:44,280
Speaker 4:  Pixel, eights, Pixel eight sails through the roof. And when people are like,

839
00:47:44,380 --> 00:47:48,280
Speaker 4:  why? We're like, because this phone is philosophically important to

840
00:47:48,280 --> 00:47:51,240
Speaker 4:  the nature of imagery, not because it's any good.

841
00:47:52,240 --> 00:47:53,680
Speaker 1:  Honestly. That, that made me wanna go buy one.

842
00:47:53,880 --> 00:47:57,320
Speaker 4:  I mean that, can you imagine if like ver Verizon had a meeting

843
00:47:57,970 --> 00:48:00,960
Speaker 4:  where they're like, Pixel eight sales are through the roof. And they're like,

844
00:48:00,960 --> 00:48:04,080
Speaker 4:  have we done any audience data? And people, and they're like, it's it's,

845
00:48:04,080 --> 00:48:07,560
Speaker 4:  is it battery life? Is it performance? And people are like, no, it's philosophical

846
00:48:07,920 --> 00:48:11,400
Speaker 4:  quandary. Boy, have we gone over? Sorry David, he knew this was happening.

847
00:48:11,800 --> 00:48:14,520
Speaker 5:  I did. I I walked right into this and I'm okay with it.

848
00:48:14,760 --> 00:48:17,240
Speaker 4:  We barely talked about that. The phone has a screen.

849
00:48:18,100 --> 00:48:18,720
Speaker 5:  It does You

850
00:48:18,720 --> 00:48:20,840
Speaker 4:  Know you can pay dollars for it. A hundred more than last year.

851
00:48:21,350 --> 00:48:21,880
Speaker 1:  Tens are three.

852
00:48:22,420 --> 00:48:23,480
Speaker 4:  All right, we'll stop. Can

853
00:48:23,480 --> 00:48:27,320
Speaker 5:  I just say one more cool thing about the, the phones or about the Google

854
00:48:27,520 --> 00:48:30,120
Speaker 5:  announcement and then we should move on. Google did this weird thing where

855
00:48:30,120 --> 00:48:33,720
Speaker 5:  they announced that Bard, the, the l l m powering Bard is gonna be

856
00:48:33,720 --> 00:48:36,720
Speaker 5:  powering Google Assistant. And they did it in this very sketchy way that

857
00:48:36,720 --> 00:48:39,360
Speaker 5:  made it seem like they had like, just thought of this idea five minutes before

858
00:48:39,540 --> 00:48:43,280
Speaker 5:  the end event started. But that's a super big deal.

859
00:48:43,620 --> 00:48:46,600
Speaker 5:  And Alex, you and I were at the Amazon event when they started talking about

860
00:48:46,700 --> 00:48:50,120
Speaker 5:  the putting LLMs underneath Alexa to do more stuff. And

861
00:48:50,880 --> 00:48:54,040
Speaker 5:  I think if, if voice assistants are ever going to be good,

862
00:48:54,990 --> 00:48:58,840
Speaker 5:  this is the moment we're gonna start to see it. And the hope these

863
00:48:58,840 --> 00:49:02,680
Speaker 5:  people have is super high. The case for why this

864
00:49:02,680 --> 00:49:05,160
Speaker 5:  can work and why it can understand context better and why it can be more

865
00:49:05,160 --> 00:49:09,000
Speaker 5:  reactive is real. Like the chat g p t voice stuff is like

866
00:49:09,210 --> 00:49:13,160
Speaker 5:  alarmingly good. And so I think if, if suddenly this becomes baked

867
00:49:13,230 --> 00:49:17,040
Speaker 5:  into Google Assistant in a way that is like accessible and cool

868
00:49:17,100 --> 00:49:21,080
Speaker 5:  and useful and has the same control over your phone and access to the internet,

869
00:49:21,180 --> 00:49:24,240
Speaker 5:  that's all gonna be really cool. They were super sketchy about the announcement.

870
00:49:24,240 --> 00:49:26,560
Speaker 5:  I have no idea how any of it's gonna work or when any of it's gonna come,

871
00:49:26,580 --> 00:49:28,200
Speaker 5:  but I think it could be a really big deal.

872
00:49:28,660 --> 00:49:32,040
Speaker 1:  I'm very excited, especially if it can answer the cup question Neil, I had

873
00:49:32,040 --> 00:49:35,240
Speaker 1:  earlier, like if it knows the difference between the cups for you.

874
00:49:36,020 --> 00:49:37,520
Speaker 1:  Oh my God, it's impossible. God. Game changer.

875
00:49:38,030 --> 00:49:41,360
Speaker 4:  When you ask Google Assistant, like, hey, hey,

876
00:49:42,100 --> 00:49:44,720
Speaker 4:  I'm trying, I'm trying to be kind to the audience. Yeah. When you ask Google

877
00:49:44,720 --> 00:49:48,600
Speaker 4:  Assistant how much is a cup, it's so confidently not useful. Like in

878
00:49:48,600 --> 00:49:52,160
Speaker 4:  the context of a coffee machine, it's just like a cup is an ounce. It's like,

879
00:49:52,160 --> 00:49:53,400
Speaker 4:  no, I know that's not the answer. Yeah,

880
00:49:53,510 --> 00:49:54,640
Speaker 1:  It's gonna give you the real one.

881
00:49:55,320 --> 00:49:58,120
Speaker 4:  Alright, we're gonna take a break. We're gonna come back and we're gonna

882
00:49:58,120 --> 00:50:01,560
Speaker 4:  do another two hours on the conceptual nature of photography.

883
00:50:02,250 --> 00:50:06,040
Speaker 4:  We're not gonna do that, but if you send me but one email asking us to do

884
00:50:06,040 --> 00:50:07,240
Speaker 4:  it, we'll definitely do it. We'll be right back.

885
00:51:40,500 --> 00:51:43,840
Speaker 4:  All right, we're back. There's other stuff at the, at The Pixel event. There

886
00:51:43,840 --> 00:51:47,320
Speaker 4:  was Android 14 got released, there's Pixel Watch two. All that stuff

887
00:51:47,490 --> 00:51:51,160
Speaker 4:  isn't gonna be in the review cycle, so we will go into that stuff in

888
00:51:51,160 --> 00:51:54,720
Speaker 4:  detail when we've actually used it. I just can't resist it.

889
00:51:54,870 --> 00:51:57,840
Speaker 4:  What is a photo? I like still want to, I'm like ready to keep talking to

890
00:51:57,900 --> 00:51:59,280
Speaker 1:  It. Just like we could do another four hours

891
00:51:59,510 --> 00:52:03,280
Speaker 4:  Anytime you want. That's what we, that's why we made the website. Hey,

892
00:52:03,280 --> 00:52:05,480
Speaker 5:  Like guess what? Netflix is more expensive now. Oh

893
00:52:05,480 --> 00:52:08,680
Speaker 4:  My God. All right, pivot. It's to Streaming and Alex talked the whole time

894
00:52:09,430 --> 00:52:11,040
Speaker 4:  here. Alex, what's up with your Plex server?

895
00:52:11,230 --> 00:52:13,920
Speaker 1:  It's, it's doing fine. I watched Top Hat on it this weekend. It was lovely.

896
00:52:14,110 --> 00:52:14,400
Speaker 1:  Nice.

897
00:52:14,430 --> 00:52:18,400
Speaker 5:  Alex. Your pl server is like famous on the internet, by the way. I posted

898
00:52:18,400 --> 00:52:21,760
Speaker 5:  something on Threads the other day basically saying like, the only possible

899
00:52:22,080 --> 00:52:25,720
Speaker 5:  response to the the unending stream

900
00:52:25,860 --> 00:52:29,360
Speaker 5:  of Streaming price hikes is just to delete all your Streaming services and

901
00:52:29,380 --> 00:52:32,200
Speaker 5:  buy box sets of your four favorite TV shows that solves all your problems.

902
00:52:32,460 --> 00:52:36,200
Speaker 5:  And I mean, a dozen different people we're like, or just get on Alex

903
00:52:36,210 --> 00:52:37,040
Speaker 5:  Cranz Plex server.

904
00:52:37,870 --> 00:52:38,160
Speaker 4:  Yeah,

905
00:52:38,430 --> 00:52:42,200
Speaker 1:  Look, it's a, I I worked really hard on that server. I'm not gonna

906
00:52:42,300 --> 00:52:45,400
Speaker 1:  lie. I, I worked really hard when I was unemployed.

907
00:52:47,420 --> 00:52:51,400
Speaker 4:  Are you in like our home lab? There's, oh, we, you gotta get, you got into

908
00:52:51,400 --> 00:52:55,000
Speaker 4:  it. It's gonna be bad. We, we gotta get you like a rack mount sonology situation.

909
00:52:55,000 --> 00:52:55,360
Speaker 4:  Oh God.

910
00:52:55,360 --> 00:52:59,080
Speaker 1:  That's what I want so bad. Yeah, I have to figure out where to put it.

911
00:52:59,780 --> 00:53:03,600
Speaker 1:  But what I'm not gonna be doing is subscribing to Netflix.

912
00:53:03,740 --> 00:53:07,680
Speaker 1:  No, that's not true. I continue to subscribe to Netflix because I

913
00:53:07,880 --> 00:53:10,920
Speaker 1:  I like it and there's a bunch of shows on it I like and I obsessively consume

914
00:53:10,930 --> 00:53:14,920
Speaker 1:  media. But Netflix is, there's a rumor that Netflix is gonna be raising its

915
00:53:14,920 --> 00:53:18,720
Speaker 1:  prices again. It's waiting until after the,

916
00:53:18,780 --> 00:53:22,600
Speaker 1:  the sag after strike is resolved, which everybody

917
00:53:22,600 --> 00:53:25,810
Speaker 1:  on both sides seems pretty confident is gonna happen, happen fairly soon.

918
00:53:25,810 --> 00:53:29,640
Speaker 1:  So that's lovely to hear. The reason it's doing that is not because, oh,

919
00:53:29,640 --> 00:53:32,680
Speaker 1:  they signed all these new contracts and they're spending way more money on,

920
00:53:32,860 --> 00:53:35,840
Speaker 1:  on actors and writers. They're spending a little bit more money on actors

921
00:53:35,840 --> 00:53:39,520
Speaker 1:  and writers, like a fraction Tiny, tiny fraction. The reason is because they

922
00:53:39,520 --> 00:53:41,840
Speaker 1:  were always gonna have to raise the prices. Yeah. Because like

923
00:53:42,830 --> 00:53:45,920
Speaker 1:  0% interest rates are gone and, and you have to pay for things,

924
00:53:46,700 --> 00:53:50,560
Speaker 1:  so they need money. So, so yeah. It's, it's happening

925
00:53:50,560 --> 00:53:53,360
Speaker 1:  again. And, and David and I were talking about this beforehand. We, we've

926
00:53:53,360 --> 00:53:55,960
Speaker 1:  seen a bunch of price hikes this year and, and we're gonna keep seeing some,

927
00:53:55,960 --> 00:53:58,920
Speaker 1:  we saw another one this week. In fact, the ad free version of Discovery Plus

928
00:53:59,060 --> 00:54:02,960
Speaker 1:  is also getting a big price hike. Sorry, David, one of the

929
00:54:02,960 --> 00:54:04,360
Speaker 1:  few people I know that subscribes.

930
00:54:04,870 --> 00:54:08,520
Speaker 5:  It's okay. How are you feeling? I'm fine. We, we have finally moved all of

931
00:54:08,520 --> 00:54:12,360
Speaker 5:  our Streaming to Max, so Okay. We just, we just leave

932
00:54:12,360 --> 00:54:13,800
Speaker 5:  inside of Max Now. Wait,

933
00:54:13,890 --> 00:54:17,600
Speaker 4:  Leave her alone. I said she's fine. She's in

934
00:54:17,600 --> 00:54:18,600
Speaker 4:  elementary school now. She's gonna

935
00:54:18,620 --> 00:54:19,560
Speaker 1:  Let her own Plex server.

936
00:54:20,330 --> 00:54:23,640
Speaker 4:  She's got her own, she's got a little raspberry pie, like a little, a little

937
00:54:23,640 --> 00:54:25,040
Speaker 4:  five-year-olds size. Er, it's

938
00:54:25,040 --> 00:54:26,440
Speaker 5:  Just bluey on the server all day. Just frozen.

939
00:54:26,590 --> 00:54:30,520
Speaker 4:  Yeah, it's just, just Bluey. No, right now we're, it's Minecraft videos

940
00:54:30,580 --> 00:54:34,240
Speaker 4:  on YouTube. She, this kid has taught herself, she's like building Oh wow.

941
00:54:34,240 --> 00:54:37,520
Speaker 4:  In Minecraft. Yeah. And it's all because of Minecraft videos on YouTube kids.

942
00:54:38,140 --> 00:54:41,320
Speaker 4:  And I, it's like, I, I'm worried about it. You know. It's, it's fine.

943
00:54:41,990 --> 00:54:44,720
Speaker 4:  It's like, is my child gonna fall on the radicalization funnel? But like

944
00:54:44,720 --> 00:54:46,800
Speaker 4:  right now it's like she's building houses, so that's

945
00:54:46,800 --> 00:54:49,520
Speaker 1:  Fine. My godson wants to be a dictator because of YouTube. It's fine.

946
00:54:50,110 --> 00:54:53,720
Speaker 4:  Look, there's an argument that we would be better served. He

947
00:54:53,720 --> 00:54:54,800
Speaker 1:  Said he would be a gentle dictator.

948
00:54:54,800 --> 00:54:56,080
Speaker 4:  He'd be like, don't trust photography.

949
00:54:56,900 --> 00:54:59,400
Speaker 1:  He would like you get in early with him, you'll be fine.

950
00:54:59,650 --> 00:55:02,440
Speaker 4:  He'll all the rules. Anyway. Continue. You why wait, everything's on Max.

951
00:55:02,510 --> 00:55:04,640
Speaker 4:  Yeah. Have you watched The Naked Show? No.

952
00:55:04,680 --> 00:55:08,600
Speaker 5:  I, it just, naked Attraction is not family friendly for my 10

953
00:55:08,600 --> 00:55:08,920
Speaker 5:  month old.

954
00:55:09,120 --> 00:55:11,080
Speaker 4:  I watched one episode for

955
00:55:11,080 --> 00:55:11,560
Speaker 5:  Journalism.

956
00:55:12,330 --> 00:55:15,560
Speaker 4:  Becky and I were like, sat there and we watched it together just, and we

957
00:55:15,560 --> 00:55:18,920
Speaker 4:  both were like, do you want to, do you wanna keep going? You can stop you,

958
00:55:19,140 --> 00:55:19,840
Speaker 4:  you want going?

959
00:55:20,580 --> 00:55:23,880
Speaker 5:  No, it's okay. Anna and I are currently deep in the ultimatum on Netflix

960
00:55:24,180 --> 00:55:27,200
Speaker 5:  and the Golden Bachelor on Hulu. So like, we'll, we'll get around to Naked

961
00:55:27,200 --> 00:55:29,040
Speaker 5:  Attraction. But the the thing

962
00:55:29,190 --> 00:55:31,680
Speaker 4:  Then don't No, no, no, no, no. Don't run,

963
00:55:33,100 --> 00:55:36,040
Speaker 5:  Run. Yeah. I will say the Golden Bachelor makes me feel a lot of feelings

964
00:55:36,040 --> 00:55:39,680
Speaker 5:  about the world, but that's for a different podcast. The, the Streaming prices

965
00:55:39,680 --> 00:55:43,040
Speaker 5:  thing. So I went back and like I, I made a story stream for us just like

966
00:55:43,040 --> 00:55:46,920
Speaker 5:  compiling all of the recent stories about the price hikes and like,

967
00:55:46,940 --> 00:55:50,920
Speaker 5:  man, it's brutal. It's just everywhere you look, it's just like one, two,

968
00:55:50,920 --> 00:55:54,880
Speaker 5:  $3 more expensive all the time. And I've been

969
00:55:55,060 --> 00:55:59,040
Speaker 5:  trying to figure out there one of two things is happening and I'm

970
00:55:59,040 --> 00:56:02,600
Speaker 5:  trying to figure out which either all of these companies are just testing

971
00:56:03,420 --> 00:56:07,200
Speaker 5:  to see how high they can go. And eventually

972
00:56:07,550 --> 00:56:10,680
Speaker 5:  they're hoping that you'll cancel everybody else and stick around with them,

973
00:56:10,680 --> 00:56:14,280
Speaker 5:  which will make your price flexibility higher for that one platform that

974
00:56:14,280 --> 00:56:17,720
Speaker 5:  you like best. And they'll be able to get away with charging you $30 a piece

975
00:56:18,380 --> 00:56:22,320
Speaker 5:  or everyone thinks ad plus cheap

976
00:56:22,600 --> 00:56:26,400
Speaker 5:  subscription is the future. So they are just going to keep pricing ad-free

977
00:56:26,720 --> 00:56:30,600
Speaker 5:  services through the roof until you go to the ads because they've realized

978
00:56:30,600 --> 00:56:32,360
Speaker 5:  that that's how they're gonna make their money.

979
00:56:32,590 --> 00:56:33,920
Speaker 4:  This is my theory that think

980
00:56:33,920 --> 00:56:34,080
Speaker 5:  That's

981
00:56:34,080 --> 00:56:37,120
Speaker 1:  The one that's the right one. Yeah, that's the one. This entire business

982
00:56:37,500 --> 00:56:40,240
Speaker 1:  was entirely ad dependent until like that

983
00:56:40,240 --> 00:56:41,400
Speaker 5:  15 minutes ago. Yeah.

984
00:56:41,830 --> 00:56:45,640
Speaker 1:  Yeah. Like everybody was onto ads. And it's really, really

985
00:56:45,640 --> 00:56:48,880
Speaker 1:  funny in the last couple of months you've really started to see Netflix take

986
00:56:48,880 --> 00:56:52,280
Speaker 1:  a backseat as it's trying to roll out ads. And Max is taking a backseat as

987
00:56:52,280 --> 00:56:56,000
Speaker 1:  it tries to roll out ads. Max famously like this comes from HBO o that was

988
00:56:56,000 --> 00:56:59,480
Speaker 1:  not a very advertiser heavy, like heavily subsidized place,

989
00:57:00,300 --> 00:57:03,160
Speaker 1:  but a bunch of the other parts of that thing and a bunch of things that David

990
00:57:03,390 --> 00:57:07,240
Speaker 1:  Zalo did before this was very advertising place and you watch

991
00:57:07,240 --> 00:57:10,960
Speaker 1:  like Paramount Plus and everything that they're doing right now and those

992
00:57:10,960 --> 00:57:14,800
Speaker 1:  guys know ads that was c b s. Yeah. Those were, that was like the biggest

993
00:57:15,030 --> 00:57:18,880
Speaker 1:  network and it was really, really good at taking all of your

994
00:57:18,880 --> 00:57:22,840
Speaker 1:  money and, and, and wants to do that again and watching

995
00:57:22,840 --> 00:57:26,320
Speaker 1:  this like sleeping giant slowly get out of bed

996
00:57:26,750 --> 00:57:30,600
Speaker 1:  because he's old. Sorry. No. Watching him slowly get out of bed

997
00:57:30,660 --> 00:57:34,480
Speaker 1:  and like go out and, and, and start to win back all those eyes. As,

998
00:57:34,500 --> 00:57:38,400
Speaker 1:  as this, this whole landscape starts to shift from

999
00:57:38,530 --> 00:57:42,480
Speaker 1:  gotta be on Netflix to gotta have my own service to, oh God, we're just inventing

1000
00:57:42,480 --> 00:57:46,320
Speaker 1:  cable again and now we have to figure out what part of cable we are Paramount's

1001
00:57:46,320 --> 00:57:49,560
Speaker 1:  gonna crush in this because that's what it's done for years and it knows

1002
00:57:49,560 --> 00:57:50,040
Speaker 1:  how to do it.

1003
00:57:50,230 --> 00:57:53,960
Speaker 5:  Alex that continues to be your craziest Streaming take. I just want you to

1004
00:57:53,960 --> 00:57:54,640
Speaker 1:  Know it's my crazy thing.

1005
00:57:54,640 --> 00:57:58,400
Speaker 5:  Paramount Plus is good is your single wildest stance in the

1006
00:57:58,400 --> 00:57:58,880
Speaker 5:  Streaming war.

1007
00:57:58,900 --> 00:58:00,920
Speaker 1:  But then what did they do this week on TikTok Super?

1008
00:58:00,960 --> 00:58:03,600
Speaker 5:  I don't know because who care? Oh, they're the ones who did, okay, nevermind.

1009
00:58:03,600 --> 00:58:04,920
Speaker 5:  I take it back. Paramount's good again.

1010
00:58:05,340 --> 00:58:07,600
Speaker 1:  See? See, you're seeing it. You're seeing it.

1011
00:58:07,780 --> 00:58:10,080
Speaker 5:  You should explain what happened. But you're right. I take it all back. What

1012
00:58:10,080 --> 00:58:10,840
Speaker 4:  Did they, okay, so,

1013
00:58:10,940 --> 00:58:14,880
Speaker 1:  So Paramount this week uploaded all of mean girls to TikTok, right?

1014
00:58:14,940 --> 00:58:17,480
Speaker 1:  And so you can watch it in little sections because that's how everybody's

1015
00:58:17,480 --> 00:58:19,160
Speaker 1:  been watching old shows is they'll go on TikTok.

1016
00:58:19,180 --> 00:58:21,880
Speaker 4:  No, I've watched so many movies in

1017
00:58:22,950 --> 00:58:25,760
Speaker 4:  what is essentially the Plex, Alex's plex server of TikTok.

1018
00:58:25,980 --> 00:58:27,400
Speaker 1:  You just watch, I've watched Sully Land

1019
00:58:27,400 --> 00:58:30,320
Speaker 4:  That I watched Whiplash at what is obviously two x speed. I also watch, which

1020
00:58:30,320 --> 00:58:33,760
Speaker 4:  makes that movie, right? It was everywhere. It was like, oh we're gonna,

1021
00:58:34,460 --> 00:58:38,400
Speaker 4:  JK Simmons is gonna be a dick. But super fast. The only one that didn't

1022
00:58:38,400 --> 00:58:40,800
Speaker 4:  get me was Inception where I was like, I didn't know what was happening in

1023
00:58:40,800 --> 00:58:44,480
Speaker 4:  this movie when I was watching it in real time on a big screen. And I certainly

1024
00:58:44,700 --> 00:58:47,600
Speaker 4:  do not know what is happening on TikTok that was

1025
00:58:47,600 --> 00:58:51,240
Speaker 1:  So smart of Paramount to do because everybody does it that way. That's the

1026
00:58:51,240 --> 00:58:54,160
Speaker 1:  new like TV street. Like you go and you sit in front of your TV and you flip

1027
00:58:54,160 --> 00:58:54,480
Speaker 1:  the channel,

1028
00:58:55,060 --> 00:58:57,960
Speaker 4:  But they took it down, they took mean girls off at TikTok. It was just a

1029
00:58:57,960 --> 00:58:58,960
Speaker 4:  one day marketing style. Yeah.

1030
00:58:58,960 --> 00:59:01,560
Speaker 1:  They did it as a one day marketing thing and that's a really smart marketing

1031
00:59:01,560 --> 00:59:01,920
Speaker 1:  thing. Like

1032
00:59:02,060 --> 00:59:04,320
Speaker 4:  Did they make any money on Paramount Plus? But it's

1033
00:59:04,320 --> 00:59:04,680
Speaker 5:  Mean Girls

1034
00:59:04,680 --> 00:59:07,360
Speaker 1:  Day. But it was all about the memes. Yeah, it was Mean Girls Day. 'cause

1035
00:59:07,360 --> 00:59:10,520
Speaker 1:  October 3rd and that's when he talked to her. It was

1036
00:59:11,140 --> 00:59:13,320
Speaker 1:  October 3rd. So I watched that movie a lot.

1037
00:59:13,430 --> 00:59:13,920
Speaker 4:  Clearly.

1038
00:59:14,520 --> 00:59:15,680
Speaker 1:  I unfortunately,

1039
00:59:15,860 --> 00:59:19,200
Speaker 4:  Oh also almost all of the Ocean's movies. Ocean's 11.

1040
00:59:19,430 --> 00:59:23,360
Speaker 4:  Ocean's 12 Ocean's almost fully. I think I watched all of Ocean's 11

1041
00:59:23,360 --> 00:59:23,760
Speaker 4:  on TikTok.

1042
00:59:23,760 --> 00:59:26,600
Speaker 1:  Your F Y P is much nice. Mine was just the s sully

1043
00:59:27,390 --> 00:59:30,560
Speaker 1:  landing the plane and then young Sheldon. Oh

1044
00:59:30,560 --> 00:59:34,000
Speaker 4:  No, that's worse. Yeah, that's a lot. I watched so much young

1045
00:59:34,070 --> 00:59:37,800
Speaker 4:  Sheldon. So mine is people complaining about the lack of

1046
00:59:38,280 --> 00:59:41,920
Speaker 4:  switch options for Philips Hu which I, I'm gonna start making these tos.

1047
00:59:41,920 --> 00:59:42,400
Speaker 4:  Yeah, it's

1048
00:59:42,400 --> 00:59:42,480
Speaker 1:  Like

1049
00:59:42,780 --> 00:59:43,920
Speaker 4:  On it and, and like movies

1050
00:59:45,060 --> 00:59:48,680
Speaker 1:  Way more entertaining. Yeah. But yeah, I I think, I think Paramount's on

1051
00:59:48,740 --> 00:59:49,040
Speaker 1:  to it

1052
00:59:49,270 --> 00:59:52,080
Speaker 4:  That this is crazier than the time you said big naturals 50 times. Gotcha.

1053
00:59:52,320 --> 00:59:53,160
Speaker 4:  I dunno what naturals,

1054
00:59:53,600 --> 00:59:57,120
Speaker 5:  I just like the, the tags we're gonna get on, like

1055
00:59:57,390 --> 01:00:00,800
Speaker 5:  everybody does like automated transcriptions of these podcasts and we're

1056
01:00:00,800 --> 01:00:03,560
Speaker 5:  just gonna instantly get banned from every podcast platform. Like we're toast.

1057
01:00:03,860 --> 01:00:07,440
Speaker 5:  But I think the thing that's so funny about this to me is everybody has discovered

1058
01:00:07,940 --> 01:00:11,440
Speaker 5:  how good a business cable was. And, and like a bunch of people have been

1059
01:00:11,440 --> 01:00:14,200
Speaker 5:  talking about this recently with the way the economics changed that have

1060
01:00:14,220 --> 01:00:18,000
Speaker 5:  led to a lot of the stuff in the writer strike is they made money

1061
01:00:18,000 --> 01:00:21,840
Speaker 5:  from you twice. You paid a lot of money for your cable bill and then

1062
01:00:21,840 --> 01:00:25,640
Speaker 5:  they showed you ads and that was, that was the deal we made and

1063
01:00:25,640 --> 01:00:29,600
Speaker 5:  agreed to for decades. And then a bunch of people thought

1064
01:00:29,600 --> 01:00:33,280
Speaker 5:  it was a super good idea to just take one of those revenue streams away.

1065
01:00:33,780 --> 01:00:37,120
Speaker 5:  And they did that and it seemed very smart in a time when you could get

1066
01:00:37,400 --> 01:00:41,000
Speaker 5:  infinity investment because you just kept showing user growth.

1067
01:00:41,380 --> 01:00:44,640
Speaker 5:  And then we come to the last two years and all of a sudden it's like, you

1068
01:00:44,640 --> 01:00:46,720
Speaker 5:  have to start making money again. And they're like, well how did we make

1069
01:00:46,720 --> 01:00:49,680
Speaker 5:  money the last time? And it's like, oh, by making money twice

1070
01:00:50,540 --> 01:00:53,920
Speaker 5:  and they're just going to do the same thing again.

1071
01:00:54,260 --> 01:00:58,240
Speaker 5:  Except instead of paying one $70 bill and seeing nine

1072
01:00:58,240 --> 01:01:02,000
Speaker 5:  minutes of ads every half hour, I'm gonna pay ten seven dollars bills

1073
01:01:02,000 --> 01:01:05,320
Speaker 5:  and see nine minutes of ads every half hour. But the user experience is gonna

1074
01:01:05,410 --> 01:01:08,320
Speaker 5:  be worse. We're literally in the middle of rebuilding worse cable and it's

1075
01:01:08,320 --> 01:01:09,320
Speaker 5:  starting to drive me totally nuts.

1076
01:01:10,070 --> 01:01:13,760
Speaker 1:  Wait, but we don't have to deal with a cable box. I don't know that

1077
01:01:13,760 --> 01:01:14,200
Speaker 5:  That's better.

1078
01:01:14,620 --> 01:01:15,680
Speaker 1:  Oh, I do. Instead

1079
01:01:15,680 --> 01:01:17,560
Speaker 5:  I get to deal with my Roku tv, which sucks.

1080
01:01:18,390 --> 01:01:21,880
Speaker 4:  Yeah, I just want to Yeah, there's a cable box in there. It's a Roku,

1081
01:01:22,110 --> 01:01:25,680
Speaker 1:  It's an Apple tv. It has, that's like go get a better, get better system,

1082
01:01:26,240 --> 01:01:29,640
Speaker 4:  Whatever. It's still 95,000 different user experiences. Yeah.

1083
01:01:30,100 --> 01:01:33,480
Speaker 4:  And a series of video players that are not very good. Yeah.

1084
01:01:33,480 --> 01:01:37,240
Speaker 1:  And I think that's what we're like at some point

1085
01:01:37,240 --> 01:01:41,040
Speaker 1:  they're gonna have to start reckoning with is right now everybody's big way

1086
01:01:41,040 --> 01:01:44,760
Speaker 1:  of getting you on their service is, well we've got this sport or we've got

1087
01:01:44,760 --> 01:01:47,600
Speaker 1:  this, this TV show you really wanna watch or this movie you wanna really

1088
01:01:47,600 --> 01:01:51,440
Speaker 1:  watch. Max has been very clear about we're gonna sell our stuff.

1089
01:01:51,580 --> 01:01:54,720
Speaker 1:  So you're seeing a whole bunch of Max products on Netflix this week. And

1090
01:01:54,720 --> 01:01:57,440
Speaker 1:  if you log into Netflix this week, it feels like old school Netflix. You're

1091
01:01:57,440 --> 01:02:00,320
Speaker 1:  like, oh, I can watch Dune. Yeah, I can just do that. That's nice.

1092
01:02:00,460 --> 01:02:03,360
Speaker 4:  So Casey boy is on stage a code and he was like basically saying what David

1093
01:02:03,360 --> 01:02:05,480
Speaker 4:  is saying is, oh, we used to sell our things like a lot of times.

1094
01:02:05,980 --> 01:02:09,080
Speaker 1:  And, and that's everybody's wanting to get back to that. They

1095
01:02:09,090 --> 01:02:10,600
Speaker 4:  Wanna get back. He called syndication the brass

1096
01:02:10,600 --> 01:02:10,720
Speaker 5:  Ring.

1097
01:02:10,750 --> 01:02:14,640
Speaker 1:  Syndication was huge. Syndication was what everybody worked

1098
01:02:14,660 --> 01:02:17,400
Speaker 1:  for and they haven't had to do it in the last few years. And now they're

1099
01:02:17,400 --> 01:02:20,960
Speaker 1:  realizing, wait, no, this is like all the writers and actors realize it.

1100
01:02:20,980 --> 01:02:24,840
Speaker 1:  All of the, the directors and producers realize it. Everybody wants to make

1101
01:02:24,860 --> 01:02:28,760
Speaker 1:  as many much money as possible, dipping as many times as they can

1102
01:02:29,540 --> 01:02:33,320
Speaker 1:  and just going all in on any one like form is

1103
01:02:33,320 --> 01:02:34,320
Speaker 1:  dumb. Yeah. The,

1104
01:02:34,380 --> 01:02:38,200
Speaker 4:  The big difference with cable and I, it's, I think

1105
01:02:38,200 --> 01:02:40,880
Speaker 4:  it's important to say this out loud, there was value

1106
01:02:41,480 --> 01:02:45,080
Speaker 4:  pre-internet in paying money for cable. One,

1107
01:02:45,780 --> 01:02:49,600
Speaker 4:  it had all this stuff. Yeah. And you couldn't get the stuff at all

1108
01:02:49,660 --> 01:02:53,480
Speaker 4:  unless you paid for cable. Like the cable channels were not available to

1109
01:02:53,480 --> 01:02:57,240
Speaker 4:  you in any way, shape or form unless you paid for cable two.

1110
01:02:57,420 --> 01:03:01,280
Speaker 4:  And I, I think we forget that this is where this industry came from. They

1111
01:03:01,280 --> 01:03:05,200
Speaker 4:  were better than antennas. Yeah. Right. The

1112
01:03:05,200 --> 01:03:08,640
Speaker 4:  like the, the wild west of the early cable industry

1113
01:03:09,100 --> 01:03:12,960
Speaker 4:  was a bunch of people who got in trouble from broadcast networks

1114
01:03:13,150 --> 01:03:17,120
Speaker 4:  Yeah. For setting up big antennas and then running wires to everybody's

1115
01:03:17,120 --> 01:03:21,000
Speaker 4:  houses. You know they used to call it cable Cat tv, C A T V, it was

1116
01:03:21,000 --> 01:03:23,160
Speaker 4:  like community access television. Yeah.

1117
01:03:23,230 --> 01:03:24,040
Speaker 1:  That was the good shit.

1118
01:03:24,040 --> 01:03:27,480
Speaker 4:  Right. And this, there's like, there was a thing under there where they were

1119
01:03:27,480 --> 01:03:31,320
Speaker 4:  like, we will centralize the hard part, which is getting the signal off

1120
01:03:31,320 --> 01:03:34,760
Speaker 4:  the broadcast towers and then we'll, we'll make it good.

1121
01:03:35,580 --> 01:03:38,880
Speaker 4:  And you were paying for that thing very specifically.

1122
01:03:39,510 --> 01:03:42,920
Speaker 4:  Like DirecTV used to advertise that it was higher quality video

1123
01:03:43,670 --> 01:03:47,560
Speaker 4:  than cable operators. Right. That's gone. This is gone. It's like

1124
01:03:47,700 --> 01:03:50,320
Speaker 4:  no one gives a shit about that part anymore. So the part where you're paying

1125
01:03:50,320 --> 01:03:52,640
Speaker 4:  twice is like you're paying for access and then you're getting ads. Like,

1126
01:03:53,020 --> 01:03:54,800
Speaker 4:  we should just demand that the access be great.

1127
01:03:55,070 --> 01:03:58,520
Speaker 1:  Well, and the other part of this is cable didn't just die because Netflix,

1128
01:03:58,670 --> 01:04:02,520
Speaker 1:  like Netflix certainly participated in the death of cable that is still

1129
01:04:02,520 --> 01:04:06,440
Speaker 1:  currently happening, but YouTube is a mu was a much bigger impact. Sure.

1130
01:04:06,530 --> 01:04:10,320
Speaker 1:  Right. And and YouTube hasn't gone away. YouTube is still here. So these

1131
01:04:10,400 --> 01:04:14,160
Speaker 1:  people are all gonna be out here recreating cable, but

1132
01:04:14,160 --> 01:04:16,760
Speaker 1:  they still aren't gonna be, they're not gonna get the audience back. The

1133
01:04:16,960 --> 01:04:19,800
Speaker 1:  audience is so fractured now. You're not getting that little audience back

1134
01:04:20,020 --> 01:04:23,520
Speaker 1:  the days where you could go and watch. I remember like my favorite example

1135
01:04:23,520 --> 01:04:26,520
Speaker 1:  is Grey's Anatomy because Grey's Anatomy season two, that was still when

1136
01:04:26,520 --> 01:04:28,080
Speaker 1:  cable was really strong and

1137
01:04:28,080 --> 01:04:29,640
Speaker 5:  When Grey's Anatomy was still really strong,

1138
01:04:29,910 --> 01:04:32,880
Speaker 1:  It's still really strong. Which is terrifying. Mm. No. Like it does well

1139
01:04:32,880 --> 01:04:34,000
Speaker 1:  in the Nielsen still. Oh

1140
01:04:34,000 --> 01:04:35,920
Speaker 5:  Well sure. It's a bad show, but it still does well.

1141
01:04:36,470 --> 01:04:39,800
Speaker 1:  Yeah, yeah, yeah. So like, so the Nielsen's back in like

1142
01:04:39,860 --> 01:04:43,600
Speaker 1:  2006, 2007 for most watched episode would be like

1143
01:04:43,600 --> 01:04:47,000
Speaker 1:  30 million people tuned in. Nowadays, Grey's

1144
01:04:47,330 --> 01:04:50,960
Speaker 1:  Anatomy still a major success, considered a major success for a, B, C still

1145
01:04:51,000 --> 01:04:54,640
Speaker 1:  making a lot of money. Average episode gets like 6 million

1146
01:04:55,190 --> 01:04:59,120
Speaker 1:  people. And it's, that's good. That's considered good. Like we saw a

1147
01:04:59,120 --> 01:05:02,720
Speaker 1:  huge drop off in audience and all that audience went and watched stuff elsewhere

1148
01:05:02,720 --> 01:05:06,000
Speaker 1:  and that's not coming back. And that's the thing that the streamers are actually

1149
01:05:06,000 --> 01:05:09,840
Speaker 1:  gonna have to reckon with. And I think that's why the prices are gonna keep

1150
01:05:10,430 --> 01:05:14,400
Speaker 1:  going up even past where we think is reasonable

1151
01:05:14,400 --> 01:05:17,560
Speaker 1:  because they're having to compete with YouTube where everybody makes the

1152
01:05:17,560 --> 01:05:19,920
Speaker 1:  content for quote unquote free. That's gonna be hard for them.

1153
01:05:20,080 --> 01:05:22,840
Speaker 5:  I think that's right in the sense that there's only like 24 hours in a day.

1154
01:05:22,840 --> 01:05:26,400
Speaker 5:  Right. And, and the idea is where are people spending their time? And like

1155
01:05:26,400 --> 01:05:29,880
Speaker 5:  the younger you get, the less likely you are to be even spending that time

1156
01:05:29,880 --> 01:05:33,720
Speaker 5:  on Streaming services. Like I, I spent a bunch of time this past

1157
01:05:33,720 --> 01:05:37,680
Speaker 5:  weekend with my nephews, it was homecoming weekend, so I was like

1158
01:05:37,680 --> 01:05:39,640
Speaker 5:  talking to him and all his friends and we're talking about all the stuff

1159
01:05:39,640 --> 01:05:43,440
Speaker 5:  they watch and he was like, yeah, I used to like sit and watch tv, but then

1160
01:05:43,440 --> 01:05:46,240
Speaker 5:  I realized I was just looking at TikTok the whole time. So now I don't even

1161
01:05:46,240 --> 01:05:49,640
Speaker 5:  turn the TV on, I just sit and watch TikTok. And I was like, that is telling

1162
01:05:49,740 --> 01:05:53,200
Speaker 5:  my friend like, your second screen is the television. Yeah.

1163
01:05:53,230 --> 01:05:57,120
Speaker 4:  This is how I ended up buying a Frame tv. This is like, I, I've

1164
01:05:57,120 --> 01:06:00,720
Speaker 4:  talked to people like TV installers and then like some Samsung people

1165
01:06:00,900 --> 01:06:04,840
Speaker 4:  who are like, the frame is a hit because people know that it will be off.

1166
01:06:05,830 --> 01:06:09,720
Speaker 4:  They're just gonna leave it off. And then I bought one because I

1167
01:06:09,720 --> 01:06:12,920
Speaker 4:  was like, it'll be in our bedroom and we'll just leave it off. But I now

1168
01:06:13,190 --> 01:06:16,880
Speaker 4:  it's currently our tv. Yeah. And I'm just like, this is not even local dimming.

1169
01:06:16,880 --> 01:06:18,000
Speaker 4:  Like why did I buy this tv?

1170
01:06:18,520 --> 01:06:20,760
Speaker 1:  I really want one for my bedroom just because it's pretty.

1171
01:06:20,870 --> 01:06:23,600
Speaker 4:  Yeah. We're gonna eventually be in the bedroom and You know Becky will watch

1172
01:06:23,620 --> 01:06:25,080
Speaker 4:  comedy specials on it. I

1173
01:06:25,080 --> 01:06:28,920
Speaker 5:  Have always wondered they like, all of the advertising material you see for

1174
01:06:28,920 --> 01:06:32,080
Speaker 5:  the frame is when it's off. And I've always wondered like, is this, is this

1175
01:06:32,120 --> 01:06:35,080
Speaker 5:  a good TV when you like turn it on? 'cause at some point you're gonna turn

1176
01:06:35,080 --> 01:06:35,200
Speaker 5:  it

1177
01:06:35,200 --> 01:06:39,080
Speaker 4:  On. No, I wanna be very clear. No, no. Okay.

1178
01:06:39,270 --> 01:06:42,600
Speaker 4:  It's the, the mat screen is like really cool.

1179
01:06:43,150 --> 01:06:47,120
Speaker 4:  It's it really cool. And we have one in the other studio here and

1180
01:06:47,120 --> 01:06:51,080
Speaker 4:  when it has like art on it with like detail, it's so convincing.

1181
01:06:51,940 --> 01:06:55,080
Speaker 4:  So cool. Yeah. And then you turn it on and you're like, all right, it's just

1182
01:06:55,860 --> 01:06:59,840
Speaker 4:  one big ass l e d backlight. Like it's 1956

1183
01:07:00,550 --> 01:07:01,480
Speaker 4:  viewing angles.

1184
01:07:01,810 --> 01:07:02,400
Speaker 1:  How's the bloom?

1185
01:07:02,620 --> 01:07:05,880
Speaker 4:  The whole thing is bloom. It's just, there's just like one

1186
01:07:06,090 --> 01:07:08,440
Speaker 4:  flashlight behind the Just I,

1187
01:07:08,580 --> 01:07:09,920
Speaker 1:  It does not know the color. Black.

1188
01:07:10,230 --> 01:07:13,400
Speaker 4:  It's just gray baby. It's gray. Yeah. Why did I watch Whiplash on my phone?

1189
01:07:14,330 --> 01:07:15,720
Speaker 4:  Phone can do True Blacks.

1190
01:07:16,270 --> 01:07:19,520
Speaker 5:  Well the good news is it's really expensive. So you got that going for you.

1191
01:07:19,700 --> 01:07:22,440
Speaker 4:  No, it's not. They're actually, the little ones are cheap. I think this is

1192
01:07:22,440 --> 01:07:24,040
Speaker 4:  the other re because it's a cheap panel. Yeah,

1193
01:07:24,040 --> 01:07:24,880
Speaker 5:  That's fair. So

1194
01:07:25,030 --> 01:07:28,160
Speaker 4:  It's all priced on margin, which means they get discounted like crazy.

1195
01:07:28,750 --> 01:07:29,640
Speaker 5:  Okay, fair enough.

1196
01:07:30,000 --> 01:07:33,680
Speaker 4:  I, but, but, but I was like, we should buy, I should buy a gallery series

1197
01:07:34,120 --> 01:07:37,240
Speaker 4:  LG like a G two. It's real nice, very thin. You can mount it just like a

1198
01:07:37,240 --> 01:07:40,880
Speaker 4:  frame, but it doesn't have the math screen doesn't, you can't do the art.

1199
01:07:41,200 --> 01:07:43,600
Speaker 4:  I was like, crap, well buy this thing. And then I'm looking at it, I'm like,

1200
01:07:43,600 --> 01:07:47,480
Speaker 4:  why did I buy it? Why, why do I have a, a single

1201
01:07:47,510 --> 01:07:50,040
Speaker 4:  backlight L L C D in my home?

1202
01:07:50,500 --> 01:07:53,200
Speaker 1:  You can put it as a background for your podcast. Right.

1203
01:07:53,310 --> 01:07:55,960
Speaker 4:  It's gonna be great when it's on the wall in the bedroom and we never watch

1204
01:07:55,960 --> 01:07:59,560
Speaker 4:  it. Yeah. Which is an insane thing to say about a tv. It'll be great when

1205
01:07:59,560 --> 01:08:01,000
Speaker 4:  we stop watching it. Just

1206
01:08:01,000 --> 01:08:01,800
Speaker 1:  Pictures of Max.

1207
01:08:02,830 --> 01:08:05,600
Speaker 4:  It's gonna be pictures of Max and it's gonna look great. Yeah. It's gonna

1208
01:08:05,600 --> 01:08:08,360
Speaker 4:  be great at that thing. But as an actual

1209
01:08:08,930 --> 01:08:12,720
Speaker 4:  television, it's like Samsung got me again. This is the second time I've

1210
01:08:12,720 --> 01:08:15,440
Speaker 4:  bought a Samsung tv 'cause it's beautiful. And I've been like, like what

1211
01:08:15,440 --> 01:08:19,440
Speaker 4:  am I looking at and why do I on this? And I'm gonna have it for 15 more

1212
01:08:19,440 --> 01:08:19,720
Speaker 4:  years.

1213
01:08:20,210 --> 01:08:20,720
Speaker 1:  Gotta go

1214
01:08:20,920 --> 01:08:24,520
Speaker 4:  LG. No, we're gonna buy, we're I'm waiting to buy. Okay. I think you can

1215
01:08:24,520 --> 01:08:28,040
Speaker 4:  get them now. But the official on sale date is the Sony A 95 L.

1216
01:08:28,710 --> 01:08:31,720
Speaker 4:  It's so expensive. It's pretty. But we're gonna, it's gonna be on the wall

1217
01:08:31,720 --> 01:08:34,880
Speaker 4:  for 15 years, so I'm like, I I should spend the money. Yeah.

1218
01:08:35,620 --> 01:08:38,600
Speaker 4:  And it's gonna, it's gonna be great. It's, it better be great because after

1219
01:08:38,600 --> 01:08:42,360
Speaker 4:  this frame experience, I don't know if I can take it all. Sorry David.

1220
01:08:42,500 --> 01:08:44,040
Speaker 4:  We were talking about the future of Streaming.

1221
01:08:44,380 --> 01:08:47,960
Speaker 5:  All I know is I have the, I have a T C L TV back here.

1222
01:08:48,390 --> 01:08:52,160
Speaker 5:  It's 40 some inches because I accidentally clicked the wrong button on

1223
01:08:52,160 --> 01:08:55,880
Speaker 5:  Amazon. So You know what it is. Is it's fine. It is

1224
01:08:56,070 --> 01:08:56,680
Speaker 5:  just fine.

1225
01:08:57,180 --> 01:08:58,800
Speaker 1:  I'm, I'm still on my LG. B seven.

1226
01:08:59,060 --> 01:09:00,040
Speaker 4:  No, see I have a B six

1227
01:09:00,100 --> 01:09:01,280
Speaker 1:  2007. Yeah. Yeah. I

1228
01:09:01,280 --> 01:09:02,600
Speaker 4:  Have a B six. Yeah, it's, it's

1229
01:09:03,000 --> 01:09:06,000
Speaker 1:  14. Whatever this is years ago. 2017. That's

1230
01:09:06,000 --> 01:09:08,480
Speaker 4:  Right. 17. 'cause I bought the B six and 16. Okay. I was like,

1231
01:09:08,500 --> 01:09:09,280
Speaker 1:  Wow, my TV's only

1232
01:09:09,430 --> 01:09:11,400
Speaker 4:  TV's great, but it's gonna be there forever. I'm like, I should upgrade.

1233
01:09:11,420 --> 01:09:14,800
Speaker 4:  I'm like, why spend the money? There's one thing you should spend the money

1234
01:09:14,800 --> 01:09:17,280
Speaker 4:  on. It's a nice TV in a, you're gonna have it forever.

1235
01:09:17,560 --> 01:09:18,360
Speaker 5:  I do actually agree with that.

1236
01:09:18,670 --> 01:09:21,160
Speaker 4:  This is the problem with Streaming is I'd rather talk about the TVs than

1237
01:09:21,160 --> 01:09:21,880
Speaker 4:  the content. Good

1238
01:09:21,880 --> 01:09:24,520
Speaker 5:  Teaser for Monday's Vergecast, by the way, before we take a break, which

1239
01:09:24,520 --> 01:09:27,400
Speaker 5:  we should do, we're, our whole episode is with a person who would like to

1240
01:09:27,400 --> 01:09:28,720
Speaker 5:  give you a television for free.

1241
01:09:29,220 --> 01:09:32,960
Speaker 4:  Ooh. It's coming. I cannot wait for this episode. That's great. All right.

1242
01:09:32,960 --> 01:09:35,200
Speaker 4:  You're right. We should take a break. We should do a lightning round and

1243
01:09:35,200 --> 01:09:39,160
Speaker 4:  we should get outta here. And then after this we'll have a bonus round of

1244
01:09:39,220 --> 01:09:41,960
Speaker 4:  six more hours talking about the nature of photography. We'll be right back.

1245
01:10:58,630 --> 01:11:02,310
Speaker 4:  All right, we're back landing around, we we're like already over.

1246
01:11:02,460 --> 01:11:05,870
Speaker 4:  It's craziest episode in a long time. People

1247
01:11:05,870 --> 01:11:07,590
Speaker 1:  Sitting in their cars waiting for us to finish

1248
01:11:08,050 --> 01:11:11,910
Speaker 4:  The the phone is good. It's a piece of har it's

1249
01:11:11,910 --> 01:11:14,800
Speaker 4:  not just a bundle of philosophical quandaries, but it

1250
01:11:14,800 --> 01:11:17,160
Speaker 1:  Is also that. But you buy it because it is that. Yeah, yeah, yeah.

1251
01:11:17,160 --> 01:11:20,520
Speaker 4:  Please. I'm telling you, if you have the money, go to your local wireless

1252
01:11:20,520 --> 01:11:24,480
Speaker 4:  carrier and be like, I want to, I want you to mark down that

1253
01:11:24,520 --> 01:11:28,320
Speaker 4:  I am buying this phone because it is a philosophical dilemma.

1254
01:11:29,560 --> 01:11:33,440
Speaker 4:  I want it move the needle. Right. It's not speed,

1255
01:11:33,440 --> 01:11:37,080
Speaker 4:  it's not camera. It's I want to, I want to be horrible at parties.

1256
01:11:38,380 --> 01:11:39,560
Speaker 4:  All right. Lightning round, David.

1257
01:11:40,070 --> 01:11:43,960
Speaker 5:  Okay. I have two because sometimes I'm allowed to do that. My first

1258
01:11:43,980 --> 01:11:47,720
Speaker 5:  one is, it appears we've now seen the entire Humane

1259
01:11:48,300 --> 01:11:52,040
Speaker 5:  ai Pin. It was at Paris Fashion Week and Naomi

1260
01:11:52,360 --> 01:11:56,000
Speaker 5:  Campbell. And then I think a couple of other people were wearing it on their,

1261
01:11:56,180 --> 01:11:57,800
Speaker 5:  on their sort of breast pocket area.

1262
01:11:58,090 --> 01:12:00,560
Speaker 4:  Loggerfeld the shit out of this thing. So we

1263
01:12:00,560 --> 01:12:03,320
Speaker 5:  Have now seen what it looks like. I think it was John Gruber who described

1264
01:12:03,320 --> 01:12:05,360
Speaker 5:  it as an iPhone. You would get in a happy meal.

1265
01:12:07,420 --> 01:12:11,240
Speaker 5:  We still don't know anything about what this device does. Their original

1266
01:12:11,240 --> 01:12:14,920
Speaker 5:  launch date was the 14th, which is just next week. But there's been some

1267
01:12:15,150 --> 01:12:18,000
Speaker 5:  reporting out there that says that has slipped to November. So I'm not exactly

1268
01:12:18,000 --> 01:12:21,960
Speaker 5:  sure what's going on there, but we've now seen the Pin. Is it real? Does

1269
01:12:21,960 --> 01:12:25,560
Speaker 5:  it do anything? Who's to say? We have had many questions about this for a

1270
01:12:25,560 --> 01:12:29,480
Speaker 5:  very long time. I think if they, they had a couple of videos

1271
01:12:29,870 --> 01:12:33,480
Speaker 5:  from Paris Fashion Week where it was another one of those like projecting

1272
01:12:33,480 --> 01:12:37,400
Speaker 5:  it onto their hands as they were doing stuff, kinds of videos. I have

1273
01:12:37,400 --> 01:12:41,360
Speaker 5:  never had so many questions about what something is than I do

1274
01:12:41,360 --> 01:12:42,880
Speaker 5:  about this, but I find it deeply fascinating.

1275
01:12:43,170 --> 01:12:47,120
Speaker 4:  Again, I just, I'm gonna start with a baseline question. There's a look at

1276
01:12:47,120 --> 01:12:50,600
Speaker 4:  our post and we'll go look at the post It's in the show notes. Pull over

1277
01:12:50,600 --> 01:12:53,840
Speaker 4:  in your car and I want you to look at the picture of the, of it mounted on

1278
01:12:53,840 --> 01:12:57,280
Speaker 4:  the lady's pants. Okay. How much does this thing weigh?

1279
01:12:57,800 --> 01:13:00,400
Speaker 4:  'cause it is not pulling on this fabric because

1280
01:13:00,400 --> 01:13:02,160
Speaker 1:  She's got her hand in her pockets. No,

1281
01:13:02,160 --> 01:13:03,360
Speaker 4:  I don't know. No,

1282
01:13:03,360 --> 01:13:05,360
Speaker 1:  That's, she's got her hand in her pockets.

1283
01:13:05,460 --> 01:13:06,880
Speaker 4:  You think she's holding it up? She's

1284
01:13:06,880 --> 01:13:09,480
Speaker 1:  Ke she's keeping it. She's keeping up Because look at the, that fabric that's

1285
01:13:09,480 --> 01:13:09,560
Speaker 1:  not,

1286
01:13:09,630 --> 01:13:13,320
Speaker 4:  Have you ever worn so much as like a, a boutin air Yeah. To like

1287
01:13:13,430 --> 01:13:16,400
Speaker 4:  high school and you're like, oh, this is kind of saggy. Yeah. Right.

1288
01:13:16,400 --> 01:13:17,280
Speaker 1:  Well, you have to think about it.

1289
01:13:17,380 --> 01:13:20,960
Speaker 4:  I'm just saying, have you ever anything like has any weight it's gonna pull

1290
01:13:21,060 --> 01:13:23,960
Speaker 4:  and here it's not doing. And you think she's pulling that fabric taut?

1291
01:13:24,020 --> 01:13:27,200
Speaker 1:  Oh yeah, she's too. She's pulling, pulling it t because you look at the other

1292
01:13:27,200 --> 01:13:30,600
Speaker 1:  ones, it's on a, it's on a suede jacket. Yeah. And then it's on this, this

1293
01:13:30,600 --> 01:13:31,880
Speaker 1:  white shirt where she's also the

1294
01:13:31,880 --> 01:13:32,160
Speaker 4:  Lapel.

1295
01:13:32,390 --> 01:13:35,920
Speaker 1:  Yeah. It's on the lapel. And, and and she's also, again, styling imposing

1296
01:13:35,920 --> 01:13:39,840
Speaker 1:  herself. And this was all coming from the CAPI show, which caper

1297
01:13:40,160 --> 01:13:43,840
Speaker 1:  apparently loves to, to do some little fashion stuff. So like, their, their

1298
01:13:44,080 --> 01:13:48,040
Speaker 1:  previous, a recent show was like all the robot dogs. Yeah. Running

1299
01:13:48,040 --> 01:13:48,280
Speaker 4:  Around

1300
01:13:48,420 --> 01:13:52,360
Speaker 1:  On the stage pretty good. Like my dogs. So like they, they love to do

1301
01:13:52,360 --> 01:13:56,320
Speaker 1:  like weird tech stuff. Our reporter Mia Sato

1302
01:13:56,320 --> 01:13:59,280
Speaker 1:  is a big fashion nerd. And I was like, oh, Kearny's doing it. She's like,

1303
01:13:59,480 --> 01:14:02,560
Speaker 1:  oh, of course it's confirmed. That would do the Humane thing.

1304
01:14:03,160 --> 01:14:06,120
Speaker 4:  I still have a number of questions. For example, how do you load contacts

1305
01:14:06,120 --> 01:14:09,600
Speaker 4:  into this phone? Is there a phone app? Real, real question. How do you

1306
01:14:09,760 --> 01:14:10,200
Speaker 1:  Interact with it?

1307
01:14:10,550 --> 01:14:13,640
Speaker 4:  Well, you, you just talk to it with your hand out in front of you. Okay.

1308
01:14:14,180 --> 01:14:14,950
Speaker 1:  Well you got no hand.

1309
01:14:15,550 --> 01:14:19,030
Speaker 5:  I just, to me it was very funny that this came out the same week that Apple

1310
01:14:19,030 --> 01:14:22,590
Speaker 5:  announced it's no longer supporting and repairing the

1311
01:14:22,790 --> 01:14:26,750
Speaker 5:  $17,000 Apple Watch. And it's like those two things. It's like the, the

1312
01:14:26,750 --> 01:14:30,390
Speaker 5:  beginning of one thing in the end of a very similar thing. It's like

1313
01:14:30,650 --> 01:14:34,270
Speaker 5:  you can just see Humane trying to go this like, cool

1314
01:14:35,230 --> 01:14:39,150
Speaker 5:  fashiony make this a luxury item route. Which is

1315
01:14:39,230 --> 01:14:43,110
Speaker 5:  a thing I think people try to do before they have a product. That's good.

1316
01:14:43,270 --> 01:14:43,390
Speaker 5:  I

1317
01:14:43,390 --> 01:14:45,310
Speaker 1:  Was like, how did that work for Google Glass?

1318
01:14:46,120 --> 01:14:49,990
Speaker 5:  Badly. It works pretty badly for everybody because it

1319
01:14:49,990 --> 01:14:52,350
Speaker 5:  turns out it's very hard to make something fashion.

1320
01:14:53,150 --> 01:14:57,110
Speaker 4:  I just, I want to take you back to the photo of the lady in

1321
01:14:57,110 --> 01:15:00,590
Speaker 4:  the pants. Every time I look at this product, I have more questions.

1322
01:15:01,500 --> 01:15:05,270
Speaker 4:  Okay. We've seen how the thing works, right? You can call Bethany,

1323
01:15:05,370 --> 01:15:08,790
Speaker 4:  you hold that, where if you have it mounted on your pants,

1324
01:15:09,610 --> 01:15:11,070
Speaker 1:  You can see her knuckles,

1325
01:15:11,790 --> 01:15:15,710
Speaker 4:  Whatever, whatever it Look, wait aside, I I still don't know

1326
01:15:15,710 --> 01:15:18,430
Speaker 4:  how this works. Yeah, I think, I think there's cardboard in the pants is

1327
01:15:18,430 --> 01:15:21,710
Speaker 4:  what I'm trying to say. Let's say this is an acceptable mounting position

1328
01:15:21,770 --> 01:15:23,830
Speaker 4:  for the Humane eye. Pin

1329
01:15:24,410 --> 01:15:25,390
Speaker 1:  Not a dorky one.

1330
01:15:25,660 --> 01:15:28,470
Speaker 4:  Well, let's say, let's say this is where they think you should have it. Okay.

1331
01:15:28,850 --> 01:15:32,750
Speaker 4:  And you need to call Bethany. Where are you gonna put your hand?

1332
01:15:33,210 --> 01:15:36,150
Speaker 1:  You like squatting? Yeah. You're like, you're holding down here. Hand out

1333
01:15:36,150 --> 01:15:36,870
Speaker 1:  down here. Yeah. All

1334
01:15:36,870 --> 01:15:40,150
Speaker 4:  Right. Hold it out. Just the basics of this product or so confusing to me.

1335
01:15:40,160 --> 01:15:40,750
Speaker 4:  Maybe you pop

1336
01:15:40,790 --> 01:15:43,150
Speaker 1:  A squat and maybe that's like part of their, their Oh,

1337
01:15:43,150 --> 01:15:45,230
Speaker 4:  You're like, you're like doing fashion rap squats

1338
01:15:45,330 --> 01:15:48,870
Speaker 1:  And they're like, pop a squat. Call your friend, pop a squat.

1339
01:15:49,160 --> 01:15:50,230
Speaker 1:  Check your mail. The

1340
01:15:50,230 --> 01:15:53,510
Speaker 5:  Humane marketing slogan is pop a squat. I'm all in.

1341
01:15:54,010 --> 01:15:54,430
Speaker 5:  All in.

1342
01:15:54,830 --> 01:15:58,430
Speaker 4:  Yeah. All right. Right. I'm just saying look at the photo

1343
01:15:58,620 --> 01:16:02,390
Speaker 4:  with the late where it, where it's on the pants and just be like, how would

1344
01:16:02,390 --> 01:16:05,670
Speaker 4:  you just run? Just run that to its logical

1345
01:16:05,820 --> 01:16:09,750
Speaker 4:  conclusions. You have to, one of the ways in which you use it is

1346
01:16:09,750 --> 01:16:12,830
Speaker 4:  that you talk to it and it talks to people. You're,

1347
01:16:13,890 --> 01:16:16,430
Speaker 4:  you're just like, and suddenly it's, you're like, da da da. And it's like

1348
01:16:17,050 --> 01:16:18,830
Speaker 4:  in French from your, your pants pocket,

1349
01:16:19,820 --> 01:16:21,150
Speaker 1:  From your crotch. Basically

1350
01:16:21,260 --> 01:16:22,550
Speaker 4:  What happens when you sit down?

1351
01:16:22,860 --> 01:16:26,150
Speaker 5:  Does French sound like the adults in Charlie Brown? Is that what that just

1352
01:16:26,170 --> 01:16:26,390
Speaker 5:  was?

1353
01:16:27,810 --> 01:16:28,790
Speaker 4:  Try not to be racist.

1354
01:16:29,730 --> 01:16:30,430
Speaker 5:  All right. Just checking.

1355
01:16:31,450 --> 01:16:32,190
Speaker 4:  You know. There

1356
01:16:32,190 --> 01:16:35,350
Speaker 5:  You go. My other lightning round thing real fast is this weird

1357
01:16:36,100 --> 01:16:40,070
Speaker 5:  kind of scandal about the iPhone 15 overheating. A bunch of

1358
01:16:40,070 --> 01:16:43,230
Speaker 5:  people noticed that their iPhone fifteens were getting really hot and draining

1359
01:16:43,230 --> 01:16:47,150
Speaker 5:  the battery super fast. Apple basically came out and said it's the App's

1360
01:16:47,150 --> 01:16:51,030
Speaker 5:  fault. Which is like a, a delightful follow onto, you're holding it

1361
01:16:51,030 --> 01:16:54,910
Speaker 5:  wrong from You know all those years ago, apple then released

1362
01:16:55,330 --> 01:16:59,190
Speaker 5:  an iOS update that it said, quote addresses an issue

1363
01:16:59,190 --> 01:17:02,470
Speaker 5:  that may cause iPhone to run warmer than expected, but also

1364
01:17:03,260 --> 01:17:06,790
Speaker 5:  said that it was the app's fault and a bunch of apps updated and

1365
01:17:06,990 --> 01:17:10,900
Speaker 5:  supposedly solved the problem. So what is going on

1366
01:17:10,900 --> 01:17:14,620
Speaker 5:  here? It's whatever it is. It seems like it's been fixed. Terrific. The o

1367
01:17:14,640 --> 01:17:17,580
Speaker 5:  the overheating problem from some of these apps that were running wild seems

1368
01:17:17,600 --> 01:17:21,540
Speaker 5:  to be solved, but whose fault was this and how did they fix it? We

1369
01:17:21,540 --> 01:17:25,340
Speaker 5:  don't know. And it's driving me nuts. I just want know what's going. It's

1370
01:17:25,340 --> 01:17:29,140
Speaker 5:  very weird. Yeah, because it's a point update that doesn't, like, ordinarily

1371
01:17:29,140 --> 01:17:32,860
Speaker 5:  when Apple adds new features in like a 0.0 0.3

1372
01:17:32,860 --> 01:17:35,940
Speaker 5:  update, it's like little bug fixes and security updates and stuff. Like this

1373
01:17:35,940 --> 01:17:37,780
Speaker 5:  is not a moment you do much, but they named this

1374
01:17:37,780 --> 01:17:40,740
Speaker 4:  One. Well 'cause I think they, there was a new cycle around it, but I guess

1375
01:17:41,240 --> 01:17:45,220
Speaker 4:  my guess here is that with iOS 17 there came some new frameworks that Instagram

1376
01:17:45,220 --> 01:17:48,660
Speaker 4:  and Uber and a bunch of other apps relied on. They updated to use those

1377
01:17:48,660 --> 01:17:52,620
Speaker 4:  frameworks that was causing the problem. So they then they

1378
01:17:52,620 --> 01:17:55,860
Speaker 4:  could blame it on Instagram. And now here we are where they've updated the

1379
01:17:55,860 --> 01:17:58,300
Speaker 4:  thing, they've updated the oss that was maybe using the framework. They've

1380
01:17:58,300 --> 01:18:00,500
Speaker 4:  updated the framework that the apps are using. The apps can't do it anymore.

1381
01:18:01,320 --> 01:18:04,140
Speaker 4:  It, I, my guess is that it's much more innocent

1382
01:18:05,700 --> 01:18:09,300
Speaker 4:  it seems. But it is, it is very funny that they're like, I don't know

1383
01:18:09,300 --> 01:18:09,700
Speaker 4:  Instagram.

1384
01:18:10,450 --> 01:18:11,780
Speaker 1:  Everybody made a boo booo.

1385
01:18:12,280 --> 01:18:13,620
Speaker 4:  All right. Cranz, what's yours?

1386
01:18:14,100 --> 01:18:17,460
Speaker 1:  LG is dropping ATSC 3.0 from its TVs next year.

1387
01:18:17,980 --> 01:18:21,180
Speaker 1:  I, I got a lot of, a lot of messages about this over the weekend. This is

1388
01:18:21,180 --> 01:18:21,460
Speaker 1:  the r

1389
01:18:21,540 --> 01:18:23,340
Speaker 5:  I p Cranz story of the week.

1390
01:18:23,690 --> 01:18:27,660
Speaker 1:  Yeah. And I think it was telling LG was one of the

1391
01:18:27,660 --> 01:18:31,620
Speaker 1:  first to adopt the technology and put it in its TVs and now it's one of the

1392
01:18:31,620 --> 01:18:35,380
Speaker 1:  first to drop it. Yeah. And it's because nobody's adopting ATSC

1393
01:18:35,380 --> 01:18:39,260
Speaker 1:  3.0. Like the stations aren't, aren't converting over to it.

1394
01:18:39,480 --> 01:18:39,700
Speaker 1:  So

1395
01:18:39,700 --> 01:18:43,380
Speaker 4:  I had a conversation with a fancy person that code Okay.

1396
01:18:44,250 --> 01:18:47,270
Speaker 4:  To about television. Yeah. Not the person who was on stage. Not by Allen,

1397
01:18:47,290 --> 01:18:50,550
Speaker 4:  who by the way, a superstar at Code overshadowed by Lindy Yacker You know.

1398
01:18:50,550 --> 01:18:52,950
Speaker 4:  But he was like, here's what I'm doing. I'm suing McDonald's 'cause they're

1399
01:18:52,950 --> 01:18:56,550
Speaker 4:  racist. Every french fries are racist. Like it was crazy. But he's trying

1400
01:18:56,550 --> 01:18:59,550
Speaker 4:  to buy a B, C. Yeah. And one of the reasons that he was trying to buy a B,

1401
01:18:59,550 --> 01:19:02,630
Speaker 4:  C is he thinks CS stations are poorly managed this whole thing. He's trying

1402
01:19:02,630 --> 01:19:06,270
Speaker 4:  to buy a bbc. So I was at code talking to other people about that

1403
01:19:06,270 --> 01:19:09,510
Speaker 4:  conversation 'cause it was a, a superstar performance at code.

1404
01:19:10,090 --> 01:19:14,070
Speaker 4:  And they were like, we keep trying to work with local

1405
01:19:14,070 --> 01:19:17,870
Speaker 4:  stations to make them better. There's a lot of things we wanna do. Particularly

1406
01:19:17,870 --> 01:19:20,550
Speaker 4:  if you're interested in sports rights. All the big streamers are interested

1407
01:19:20,550 --> 01:19:24,390
Speaker 4:  in sports. There's a lot of local broadcast deals to, to make any sort of

1408
01:19:24,390 --> 01:19:27,950
Speaker 4:  upgrades. So get sports in four K for example, that the local stations

1409
01:19:28,180 --> 01:19:31,990
Speaker 4:  have to do it. Yep. And they won't. Yep. They just will.

1410
01:19:31,990 --> 01:19:35,870
Speaker 4:  Like there is not a single less innovative part of

1411
01:19:35,870 --> 01:19:37,550
Speaker 4:  the media. It's ecosystem.

1412
01:19:38,010 --> 01:19:38,950
Speaker 5:  Nobody wants local

1413
01:19:38,950 --> 01:19:39,790
Speaker 4:  Television stations.

1414
01:19:39,900 --> 01:19:42,950
Speaker 1:  Yeah. And everybody who's been buying up the local stations, 'cause they're

1415
01:19:42,950 --> 01:19:46,470
Speaker 1:  largely a couple of different monopolies that that own the majority of them,

1416
01:19:46,780 --> 01:19:50,390
Speaker 1:  they have no interest in it. They, they are effectively like private equity

1417
01:19:50,450 --> 01:19:52,910
Speaker 1:  did to journalism where they're like, we wanna just buy it and

1418
01:19:52,910 --> 01:19:54,390
Speaker 4:  Milk and run the See

1419
01:19:54,390 --> 01:19:54,950
Speaker 5:  If we get out. Yep.

1420
01:19:55,090 --> 01:19:58,990
Speaker 1:  Run it to the ground. In some places, like in New York, it's, it's pretty

1421
01:19:58,990 --> 01:20:02,630
Speaker 1:  expensive to, to put up the towers and stuff for ATSC 3.0

1422
01:20:03,510 --> 01:20:06,310
Speaker 1:  and a lot of the, I guess a lot of the buildings around here don't wanna

1423
01:20:06,310 --> 01:20:08,920
Speaker 1:  deal with that. Yeah. And deal with the leases and everything like that.

1424
01:20:08,980 --> 01:20:12,880
Speaker 1:  So it is, it's challenging. And it was just a bummer. 'cause like at S

1425
01:20:12,880 --> 01:20:16,720
Speaker 1:  three, 3.0 is cool technology. Like the technology itself is

1426
01:20:16,770 --> 01:20:20,200
Speaker 1:  super, super cool and it could do a lot and it could really,

1427
01:20:20,800 --> 01:20:23,840
Speaker 1:  really democratize access to a lot of stuff.

1428
01:20:24,940 --> 01:20:28,800
Speaker 1:  But no one wants to actually do it. So it's

1429
01:20:28,800 --> 01:20:32,000
Speaker 1:  all just kind of a bummer. And it's like, I don't blame LG for not wanting

1430
01:20:32,250 --> 01:20:35,960
Speaker 1:  to spend the time and resources to continue to use this. If nobody's adopting

1431
01:20:35,980 --> 01:20:39,920
Speaker 1:  it. Like You know, come back in a couple of years, but by then

1432
01:20:39,920 --> 01:20:43,360
Speaker 1:  broadcast TV will effectively probably be dead. So that's a,

1433
01:20:43,630 --> 01:20:45,520
Speaker 1:  it's just a real bummer

1434
01:20:45,690 --> 01:20:46,760
Speaker 4:  Right at the end. The text.

1435
01:20:47,240 --> 01:20:49,760
Speaker 1:  Yeah. It's all over. Yeah. They they got it. They're like, we, we made it.

1436
01:20:49,760 --> 01:20:52,160
Speaker 1:  And I was like, yeah, you're 10 years too late. You should have done this

1437
01:20:52,360 --> 01:20:52,880
Speaker 1:  a while back.

1438
01:20:53,180 --> 01:20:55,640
Speaker 4:  But this is You know there's a, there's a balance here. Remember Vizio took

1439
01:20:55,640 --> 01:20:58,680
Speaker 4:  the tuners out of its TVs and there was huge backlash and they put them back

1440
01:20:58,680 --> 01:20:59,280
Speaker 4:  because that was

1441
01:20:59,280 --> 01:21:01,640
Speaker 1:  Too soon. If they did it now, nobody would care. I think

1442
01:21:01,640 --> 01:21:02,480
Speaker 4:  That's right. I know.

1443
01:21:02,880 --> 01:21:06,400
Speaker 1:  I mean, well they, Vizio was cheap though. Visio, that, that was the whole

1444
01:21:06,540 --> 01:21:09,280
Speaker 1:  get of Vizio is you'd get a really good TV for really inexpensive price.

1445
01:21:09,380 --> 01:21:13,160
Speaker 1:  That's changed a little nowadays. But like, so I get like their

1446
01:21:13,400 --> 01:21:16,720
Speaker 1:  audience probably did use tuners a lot more than they thought. And most people

1447
01:21:17,000 --> 01:21:20,840
Speaker 1:  probably don't actually. Especially LG. Like somebody going out to buy the

1448
01:21:20,880 --> 01:21:21,600
Speaker 1:  nice LGS just

1449
01:21:21,610 --> 01:21:24,960
Speaker 4:  Right to us. I need to know the answer. We're both guessing. Yeah.

1450
01:21:25,090 --> 01:21:25,680
Speaker 1:  We're guessing.

1451
01:21:25,900 --> 01:21:29,680
Speaker 4:  I'm, I think a lot of people still use our tuners a lot. Especially our audience.

1452
01:21:30,140 --> 01:21:32,560
Speaker 1:  Our audience does that is because I use my tuner still.

1453
01:21:32,560 --> 01:21:35,960
Speaker 4:  Yep. That's why I said it. But we, the audience a small asked us to talk

1454
01:21:36,010 --> 01:21:38,920
Speaker 4:  about ATSC 3.0. I have a feeling. I

1455
01:21:38,920 --> 01:21:42,240
Speaker 1:  Know. I'm sorry guys. I I want it too. No, but I don't think we're getting

1456
01:21:42,240 --> 01:21:42,440
Speaker 1:  it

1457
01:21:42,860 --> 01:21:46,680
Speaker 4:  At S 3.0 is going to perform upstate. It's gonna be very happy there.

1458
01:21:46,910 --> 01:21:47,280
Speaker 4:  It's gonna

1459
01:21:47,280 --> 01:21:47,800
Speaker 1:  Be really closed.

1460
01:21:48,260 --> 01:21:50,920
Speaker 4:  All right. I have two as well, but my two are like the same thing. Okay.

1461
01:21:51,420 --> 01:21:55,000
Speaker 4:  So one X the platform formerly known as Twitter announced

1462
01:21:55,910 --> 01:21:59,720
Speaker 4:  deal with Paris Hilton. I would just, I would describe this

1463
01:21:59,720 --> 01:22:03,400
Speaker 4:  press release as calorie free. It contains no

1464
01:22:03,400 --> 01:22:06,920
Speaker 4:  information. It's a lot of words. It's so many words. And one thing I learned

1465
01:22:06,920 --> 01:22:10,600
Speaker 4:  about Lindy Aino is she loves words. Just

1466
01:22:10,650 --> 01:22:13,640
Speaker 4:  loves word. And you can see why she was a great marketing executive at N

1467
01:22:13,680 --> 01:22:17,320
Speaker 4:  B C. 'cause she had N B C, she had the Olympics.

1468
01:22:17,780 --> 01:22:21,680
Speaker 4:  And if you just wanna wander around talking about how people feel inspired

1469
01:22:22,220 --> 01:22:25,920
Speaker 4:  and communities are brought together and look at the world and

1470
01:22:25,940 --> 01:22:29,560
Speaker 4:  here's a young girl singing and bio Amex, N B C is a great

1471
01:22:30,000 --> 01:22:33,800
Speaker 4:  platform to be that personality. When you have Twitter, it's like,

1472
01:22:34,550 --> 01:22:38,240
Speaker 4:  what are you talking about? Like, you've got

1473
01:22:38,240 --> 01:22:42,040
Speaker 4:  nothing. And it's all the same sort of like hopey changing

1474
01:22:42,560 --> 01:22:46,160
Speaker 4:  language for lack of a better word. So anyway, so they've announced the Seal

1475
01:22:46,160 --> 01:22:49,520
Speaker 4:  Paris Hilton, which she will, she will post on X, they'll work together to

1476
01:22:49,520 --> 01:22:52,800
Speaker 4:  create four original video content programs per year. That includes live

1477
01:22:53,000 --> 01:22:56,880
Speaker 4:  shopping across, along with a host of other activations across

1478
01:22:57,020 --> 01:23:00,400
Speaker 4:  all surfaces of X. The live shopping experience will allow you to browse

1479
01:23:00,400 --> 01:23:03,160
Speaker 4:  through a catalog of products and then click through the site to make a purchase

1480
01:23:03,700 --> 01:23:05,440
Speaker 4:  via the in-app browser. Okay.

1481
01:23:05,950 --> 01:23:08,040
Speaker 1:  This could be cool. And I'm gonna give you the

1482
01:23:08,040 --> 01:23:11,680
Speaker 4:  One caveat unless Paris Hilton is, is actually selling ecstasy

1483
01:23:11,900 --> 01:23:15,240
Speaker 4:  on this platform? No, because zero people will buy anything.

1484
01:23:15,530 --> 01:23:19,480
Speaker 1:  Paris is super into radios. This is like the, the lore

1485
01:23:19,480 --> 01:23:23,440
Speaker 1:  about Paris Hilton. Is she super into to traditional radios? Can I CB

1486
01:23:23,440 --> 01:23:27,160
Speaker 1:  radio? What if she one of the stores is just CB radio stuff. All right.

1487
01:23:27,160 --> 01:23:27,520
Speaker 1:  That would

1488
01:23:27,520 --> 01:23:31,320
Speaker 4:  Be cool. If Paris Hilton does a CB radio activation, that'd be great. X

1489
01:23:31,320 --> 01:23:34,760
Speaker 4:  will work to secure brand sponsorship to support each of the activations.

1490
01:23:35,080 --> 01:23:38,160
Speaker 4:  So they're paying Paris, they dunno how they're gonna pay for paying Paris.

1491
01:23:38,590 --> 01:23:42,280
Speaker 4:  Very good. X will also support amplifying other efforts that

1492
01:23:42,280 --> 01:23:45,960
Speaker 4:  1111 Media, which is our company 1111 Media in Paris Hilton will be involved

1493
01:23:45,960 --> 01:23:48,440
Speaker 4:  in throughout the year. This is nothing.

1494
01:23:49,110 --> 01:23:49,680
Speaker 1:  What is it?

1495
01:23:50,060 --> 01:23:53,280
Speaker 4:  So I, the only reason I say it's the same thing is, my second thing is

1496
01:23:53,670 --> 01:23:57,560
Speaker 4:  there's a little bit of an existential crisis on threads this week because

1497
01:23:57,590 --> 01:24:01,240
Speaker 4:  it's the heart of the N F L season. The government shut down all this stuff

1498
01:24:01,260 --> 01:24:05,200
Speaker 4:  is speaker, the house is getting fired. This is the stuff you you would

1499
01:24:05,340 --> 01:24:08,800
Speaker 4:  use Twitter for, right? Like these moments

1500
01:24:09,130 --> 01:24:12,840
Speaker 4:  where, where Twitter, there was a flood in New York. Do You know what Twitter

1501
01:24:12,840 --> 01:24:16,640
Speaker 4:  is great at telling people who don't care about the weather in New York City?

1502
01:24:16,700 --> 01:24:20,280
Speaker 4:  What exactly is happening with the weather in New York

1503
01:24:20,280 --> 01:24:24,200
Speaker 4:  City? Yeah. And Threads has, I would say flatly failed to live up

1504
01:24:24,200 --> 01:24:28,040
Speaker 4:  to any of these occasions as a real-time new service. Yep. I've, I've tried

1505
01:24:28,040 --> 01:24:31,000
Speaker 4:  and I'm rooting for Threads. It's the one I use. And everyone gets mad at

1506
01:24:31,000 --> 01:24:33,520
Speaker 4:  me when I'm like, threads is bad. They're like, you, what's the one I'm using?

1507
01:24:34,070 --> 01:24:37,920
Speaker 4:  It's, I believe in activity. The whole thing. It's not good

1508
01:24:38,000 --> 01:24:41,840
Speaker 4:  at this stuff. Watching football with threads is

1509
01:24:41,840 --> 01:24:45,640
Speaker 4:  like not watching football with anything. Like, it's the

1510
01:24:45,800 --> 01:24:48,520
Speaker 4:  opposite of watching football with threads. So there's been just a lot of

1511
01:24:48,520 --> 01:24:52,440
Speaker 4:  consternation about threads and whether they care about news and meta is

1512
01:24:52,440 --> 01:24:55,800
Speaker 4:  they, there was a report in the information that Meta is doing, creator councils,

1513
01:24:55,800 --> 01:24:58,800
Speaker 4:  whether they're gathering all the creators together to see what they want,

1514
01:24:59,330 --> 01:25:03,240
Speaker 4:  which is historically in the history of social platforms,

1515
01:25:03,240 --> 01:25:06,280
Speaker 4:  a thing you do right before you die. But You know on the flip side, You know

1516
01:25:06,350 --> 01:25:09,840
Speaker 4:  beta does have Instagram and like they, they they have a they have a thing

1517
01:25:09,840 --> 01:25:13,080
Speaker 4:  there. Yeah. But You know, it's like they're, they're doing committees to

1518
01:25:13,080 --> 01:25:16,040
Speaker 4:  figure out what they should do to, to kick threads back in the gear. And

1519
01:25:16,040 --> 01:25:17,240
Speaker 4:  it's like, oh, you don't need a committee,

1520
01:25:17,780 --> 01:25:18,200
Speaker 1:  Do news.

1521
01:25:18,430 --> 01:25:21,800
Speaker 4:  Just do Twitter. Just do what Twitter was good at, which is realtime news.

1522
01:25:22,020 --> 01:25:25,880
Speaker 4:  So I posted on threads. It feels like they need to make every

1523
01:25:25,900 --> 01:25:29,840
Speaker 4:  mis they were so burned by news. Adam er was so burned by news. 'cause he

1524
01:25:29,840 --> 01:25:33,000
Speaker 4:  used to run the newsfeed. He was so burned by news. They have to try everything

1525
01:25:33,080 --> 01:25:36,800
Speaker 4:  else first. And so Missery actually wrote back to me, we are not

1526
01:25:36,800 --> 01:25:40,080
Speaker 4:  anti news. News is already in threads. We're simply trying to avoid

1527
01:25:40,080 --> 01:25:43,840
Speaker 4:  overpromising and under-delivering to an incredibly powerful group, which

1528
01:25:43,840 --> 01:25:47,400
Speaker 4:  is a mistake we've made as a company many times in our past on its face.

1529
01:25:47,470 --> 01:25:50,120
Speaker 4:  This is a totally reasonable thing to say. Right? We don't want the news

1530
01:25:50,120 --> 01:25:53,760
Speaker 4:  media to think that threads will save it. Yeah. So we're, we're not gonna

1531
01:25:53,760 --> 01:25:57,440
Speaker 4:  pretend two things about this. One, one

1532
01:25:57,720 --> 01:26:00,520
Speaker 4:  solution to this problem is to make promises and keep them

1533
01:26:01,990 --> 01:26:05,200
Speaker 4:  just putting that out there. Little one as, as somebody who lived through

1534
01:26:05,480 --> 01:26:09,240
Speaker 4:  something called the pivot to video, not lying about metrics and

1535
01:26:09,520 --> 01:26:10,600
Speaker 4:  actually developing revenue.

1536
01:26:11,190 --> 01:26:11,680
Speaker 5:  It's a concept

1537
01:26:12,040 --> 01:26:14,760
Speaker 4:  I think that would've solved your problems. Yeah. It's just a, it's just

1538
01:26:14,840 --> 01:26:17,760
Speaker 4:  a thing. So I get where that's coming from. Like everyone yelled at him,

1539
01:26:18,100 --> 01:26:18,520
Speaker 4:  but that's

1540
01:26:18,520 --> 01:26:18,800
Speaker 5:  Not what people

1541
01:26:18,800 --> 01:26:21,160
Speaker 4:  Asking. But he's saying like, we didn't keep our promises, so we don't wanna

1542
01:26:21,160 --> 01:26:24,360
Speaker 4:  make any promises this time. Which is like backwards fine, but like fine.

1543
01:26:24,360 --> 01:26:27,800
Speaker 4:  Like I I actually understand it. Yeah. Like one rational

1544
01:26:28,600 --> 01:26:32,440
Speaker 4:  response to that is to say, we are making no promises. Sure. Great.

1545
01:26:32,660 --> 01:26:36,520
Speaker 4:  The problem is you're using emergency going down. So good luck. Like

1546
01:26:36,540 --> 01:26:40,200
Speaker 5:  Forget being burned by making bad decisions. Hasn't meta learned that

1547
01:26:40,300 --> 01:26:43,960
Speaker 5:  caring about the news business is actually just all downside.

1548
01:26:44,150 --> 01:26:47,760
Speaker 4:  Yeah. Right. And th and the regulators are there. And You know Canada's gonna

1549
01:26:47,760 --> 01:26:51,360
Speaker 4:  make you pay for links. And an Australian person's going to You know

1550
01:26:51,360 --> 01:26:53,840
Speaker 4:  Rupert Murdoch is gonna be like, are you willing to pay for my links? Like,

1551
01:26:53,920 --> 01:26:56,840
Speaker 4:  I think like it all happens. There's a law in this country that would make

1552
01:26:56,840 --> 01:27:00,760
Speaker 4:  him do, it's all bad, but the product isn't

1553
01:27:00,760 --> 01:27:04,680
Speaker 4:  sticky because what you desperately need is people posting about

1554
01:27:04,680 --> 01:27:06,320
Speaker 4:  the news on a product that looks like Twitter.

1555
01:27:06,470 --> 01:27:10,240
Speaker 5:  Okay. Yes. Except Twitter was a bad business was

1556
01:27:10,240 --> 01:27:13,480
Speaker 5:  always a bad business and was a bad product that most people didn't use.

1557
01:27:13,480 --> 01:27:17,200
Speaker 5:  And the people who did hated themselves for it. Like, I'm not sure you

1558
01:27:17,480 --> 01:27:20,080
Speaker 5:  actually should look at Twitter and be like, let's do what Twitter did. True.

1559
01:27:20,080 --> 01:27:23,880
Speaker 5:  That's how you get a bunch of like psychotic maniacs like us who spent

1560
01:27:24,060 --> 01:27:27,280
Speaker 5:  all together too much time for 15 years on Twitter. Like,

1561
01:27:27,780 --> 01:27:31,400
Speaker 4:  Oh, my brain is healed. I I feel I'm a new person now that I'm not doing

1562
01:27:31,400 --> 01:27:35,040
Speaker 4:  it. Mine's like I have threads in my phone. I don't even open it. Like

1563
01:27:35,320 --> 01:27:38,840
Speaker 4:  I forgot. I, I like threw a bomb on threads. I was like, this camera's a

1564
01:27:38,840 --> 01:27:39,920
Speaker 4:  liar. Like walked away

1565
01:27:40,300 --> 01:27:43,600
Speaker 5:  To me. I think, I think the thing threads will regret

1566
01:27:44,230 --> 01:27:47,920
Speaker 5:  more than not being more like Twitter is

1567
01:27:48,120 --> 01:27:51,160
Speaker 5:  positioning itself against Twitter the way that it has. Yeah. 'cause what

1568
01:27:51,160 --> 01:27:54,520
Speaker 5:  they did, mark Zuckerberg and Adam is Harry both did this like

1569
01:27:54,520 --> 01:27:58,400
Speaker 5:  aggressively and sort of loudly at the beginning was say in as

1570
01:27:58,400 --> 01:28:02,080
Speaker 5:  many words, we want to be better Twitter. And we all kind of internally

1571
01:28:02,080 --> 01:28:05,720
Speaker 5:  understand what that might be. And that comes with certain things and certain

1572
01:28:05,720 --> 01:28:09,480
Speaker 5:  ideas. If they had just said, we think it's time for a new

1573
01:28:09,630 --> 01:28:12,400
Speaker 5:  kind of text-based social networking for sharing.

1574
01:28:12,830 --> 01:28:14,120
Speaker 4:  They sort of did

1575
01:28:14,760 --> 01:28:18,040
Speaker 5:  A little. But you, if you say the first thing, everybody's gonna hold you

1576
01:28:18,040 --> 01:28:21,720
Speaker 5:  the first thing. Right. Like, I, I I think it's true. But also if you, if

1577
01:28:21,720 --> 01:28:25,560
Speaker 5:  you say like we, we think the world deserves Twitter that is sanely

1578
01:28:25,560 --> 01:28:29,040
Speaker 5:  run. Like in essentially that many words. People are going to hold you to

1579
01:28:29,040 --> 01:28:32,960
Speaker 5:  Twitter standards. But I think like meta's out here to build a big thing

1580
01:28:32,960 --> 01:28:35,880
Speaker 5:  that makes money and Twitter was never any of those things. Well

1581
01:28:36,040 --> 01:28:39,520
Speaker 4:  Actually we have like 10,000 disclosures. So the first one is criticism of

1582
01:28:39,520 --> 01:28:42,440
Speaker 4:  the current. Twitter is in no way praise for the previous administration

1583
01:28:42,440 --> 01:28:45,960
Speaker 4:  of Twitter. I think it's important to say they were bad at this. There is

1584
01:28:45,960 --> 01:28:49,720
Speaker 4:  an argument to be made now. I think that that

1585
01:28:50,380 --> 01:28:54,040
Speaker 4:  was the best. It could be sort of chaotic, horrible.

1586
01:28:54,170 --> 01:28:55,240
Speaker 4:  Right. Was like the best.

1587
01:28:55,380 --> 01:28:57,360
Speaker 5:  It certainly was the best it has been. Yeah. Maybe,

1588
01:28:57,410 --> 01:29:01,160
Speaker 4:  Maybe they had stumbled into a sort of steady state of misery.

1589
01:29:01,840 --> 01:29:05,720
Speaker 4:  Like, I don't know. Maybe. So I mean like the, this is a company that sold

1590
01:29:05,720 --> 01:29:09,400
Speaker 4:  itself for $44 billion at 54, 20 a share. 'cause it

1591
01:29:09,940 --> 01:29:13,480
Speaker 4:  could not figure out a way to get to 54 20 a share on

1592
01:29:13,800 --> 01:29:17,120
Speaker 4:  the open market. Like they're like, I don't know. That seems high. Take the

1593
01:29:17,180 --> 01:29:21,080
Speaker 4:  money. Like we got nothing like our, we not, is it, it we're, we're gonna

1594
01:29:21,080 --> 01:29:22,760
Speaker 4:  let more people tweet like whatever. And

1595
01:29:22,760 --> 01:29:25,800
Speaker 5:  Now the new report said that it's worth $8 billion. So everybody's doing

1596
01:29:25,800 --> 01:29:26,120
Speaker 5:  great over there.

1597
01:29:26,220 --> 01:29:28,800
Speaker 4:  So there's I just that. And then the other dis discussion we talked about

1598
01:29:28,800 --> 01:29:32,600
Speaker 4:  Streaming and cable. So Comcast and invest in our company. Whatever. They

1599
01:29:32,600 --> 01:29:35,720
Speaker 4:  don't, they're not into me. We didn't say news. I'm I'm a Netflix producer.

1600
01:29:36,420 --> 01:29:39,760
Speaker 4:  That's a real thing. That's true. I have at t as my personal phone service.

1601
01:29:40,430 --> 01:29:42,560
Speaker 1:  Yeah. I, I Verizon. Yeah.

1602
01:29:42,560 --> 01:29:46,320
Speaker 5:  Alex Cranz watches every Star Trek show on Paramount Plus it's

1603
01:29:46,320 --> 01:29:50,160
Speaker 4:  True broadcast tv. RF energy is flowing through our bodies right

1604
01:29:50,160 --> 01:29:50,360
Speaker 4:  now.

1605
01:29:52,460 --> 01:29:52,880
Speaker 1:  That's

1606
01:29:52,880 --> 01:29:54,520
Speaker 4:  A real thing that's happening to you too.

1607
01:29:55,060 --> 01:29:56,480
Speaker 1:  Grey's Anatomy's going right now. Hey

1608
01:29:56,520 --> 01:30:00,400
Speaker 4:  Look that the, the five G alert went off. Huh? All of

1609
01:30:00,400 --> 01:30:04,360
Speaker 4:  you are Aaron Rogers. Now I wonder to saw, I dunno what that means, but it

1610
01:30:04,360 --> 01:30:08,120
Speaker 4:  sounds great. Come back around to threads. I'm just saying if they had solved

1611
01:30:08,280 --> 01:30:11,920
Speaker 4:  the smallest problem, f they're all I know a bunch of them are F one fans.

1612
01:30:11,920 --> 01:30:15,760
Speaker 4:  'cause I see them posting about f one days after they've posted because of

1613
01:30:15,760 --> 01:30:19,000
Speaker 4:  the algorithmic feed. But I see 'em doing it. If they had just solved, we

1614
01:30:19,000 --> 01:30:22,120
Speaker 4:  wanna make this great, they, they would've solved it. They wanna make it

1615
01:30:22,120 --> 01:30:24,880
Speaker 4:  great for watching football. The problem is that solving those

1616
01:30:25,510 --> 01:30:29,200
Speaker 4:  problems necessarily makes you great at, oh, there's a government shutdown,

1617
01:30:29,200 --> 01:30:32,320
Speaker 4:  liming, oh, the speaker of the house got fired or whatever. Right. And that

1618
01:30:32,320 --> 01:30:36,280
Speaker 4:  means they're good at news. And I don't think that they have a plan to square

1619
01:30:36,280 --> 01:30:40,200
Speaker 4:  that circle. It cannot be that a bunch of famous, you, you're gonna pay

1620
01:30:40,200 --> 01:30:44,160
Speaker 4:  Paris Hilton some money to post about radios in

1621
01:30:44,160 --> 01:30:46,720
Speaker 4:  her shop. It's coming. That's not gonna do it for you.

1622
01:30:47,260 --> 01:30:48,240
Speaker 1:  You don't know. Or

1623
01:30:48,240 --> 01:30:51,400
Speaker 5:  That you're just gonna block some topics entirely the way France has done

1624
01:30:51,400 --> 01:30:54,800
Speaker 5:  with Covid. Right. Like you can't just, you can't just write certain things

1625
01:30:54,800 --> 01:30:57,880
Speaker 5:  off and then be like, come hang out. But only about the nice things like

1626
01:30:58,600 --> 01:31:02,560
Speaker 5:  I I think you might be right that what Twitter was might be the best of all

1627
01:31:02,560 --> 01:31:05,640
Speaker 5:  the bad outcomes. And maybe they're all bad outcomes. Well,

1628
01:31:05,720 --> 01:31:09,640
Speaker 1:  'cause it was, it was like the NC 17 social media and threads is like

1629
01:31:09,640 --> 01:31:09,840
Speaker 1:  seriously

1630
01:31:10,280 --> 01:31:12,320
Speaker 4:  Legitimately the NC 17 social media.

1631
01:31:12,390 --> 01:31:14,880
Speaker 1:  Yeah. And, and this is like G rated, this

1632
01:31:14,880 --> 01:31:15,600
Speaker 5:  Is Pixar Twitter

1633
01:31:15,660 --> 01:31:16,680
Speaker 1:  And this is not G

1634
01:31:16,760 --> 01:31:20,280
Speaker 4:  Rated. I just don't think, like, even if, again, I I I would just come back

1635
01:31:20,280 --> 01:31:22,760
Speaker 4:  to some of the stickiest things that Twitter did for me that weren't news.

1636
01:31:23,070 --> 01:31:26,640
Speaker 4:  Like hard news, like political news sports, watching sports

1637
01:31:26,790 --> 01:31:30,520
Speaker 4:  with Twitter open is great or was great. Now it's weird because

1638
01:31:30,790 --> 01:31:33,760
Speaker 4:  like half people aren't there and then they're doing the weird programmatic

1639
01:31:33,760 --> 01:31:37,720
Speaker 4:  insertions of like, ads for buying gold. It's like, I

1640
01:31:37,720 --> 01:31:40,880
Speaker 4:  love gold. I don't love any of this and I don't have the app on my phone,

1641
01:31:40,880 --> 01:31:43,840
Speaker 4:  so it's on the web, whatever. It's not great. I would love to switch to threads

1642
01:31:43,840 --> 01:31:47,680
Speaker 4:  for this. It's just not there. And again, I'm rooting for threads

1643
01:31:47,840 --> 01:31:51,360
Speaker 4:  because I want a company at Meta Scale to bet on

1644
01:31:51,360 --> 01:31:54,760
Speaker 4:  federation and interoperability and push all of that forward.

1645
01:31:55,110 --> 01:31:58,600
Speaker 4:  They seem super committed to it. I am totally in support of that.

1646
01:31:59,160 --> 01:32:02,760
Speaker 4:  I just think they've lost sight of what makes it sticky,

1647
01:32:03,130 --> 01:32:06,680
Speaker 4:  which is actually real-time information. And like, I don't know that you

1648
01:32:06,680 --> 01:32:09,640
Speaker 4:  can solve that without accidentally solving news. And I think they're paralyzed

1649
01:32:09,640 --> 01:32:12,560
Speaker 4:  for that. Yes. Right. That sounds a hundred percent right. On the other hand,

1650
01:32:12,950 --> 01:32:15,000
Speaker 4:  Twitter's paying Paris Hilton money. The fact that

1651
01:32:15,010 --> 01:32:18,520
Speaker 5:  Linda Ya Carino had to tweet the word SL is just,

1652
01:32:18,860 --> 01:32:20,920
Speaker 5:  it just makes it all worthwhile. It's slaving.

1653
01:32:21,140 --> 01:32:24,760
Speaker 4:  Is it? S slaving. Slaving. I always say slaving. Slaving.

1654
01:32:24,930 --> 01:32:27,080
Speaker 5:  While, while we're living. Nila is slaving. Guys,

1655
01:32:27,180 --> 01:32:30,800
Speaker 4:  That's I'm slaving while we're right out the door. All right. I think that's

1656
01:32:30,800 --> 01:32:33,840
Speaker 4:  it. We're way over time. I just wanna point out this episode of the verse

1657
01:32:33,840 --> 01:32:37,760
Speaker 4:  has started with coffee cups. It, it did philosophical quandaries. And

1658
01:32:37,760 --> 01:32:41,400
Speaker 4:  we ended with slicing. When people say we don't have the range, this is,

1659
01:32:41,460 --> 01:32:45,440
Speaker 4:  we made a whole publication for ourselves. Yeah. For this. This

1660
01:32:45,440 --> 01:32:48,600
Speaker 4:  is where we live. All right. That's it. I got some things to call out. We

1661
01:32:48,600 --> 01:32:50,840
Speaker 4:  have a partnership with Dbrand. You should go buy the skin. They're really

1662
01:32:50,840 --> 01:32:53,240
Speaker 4:  cool. They're cool. They're sick as hell, actually. They look beautiful.

1663
01:32:53,620 --> 01:32:57,480
Speaker 4:  And then speaking of cameras, Becca's video, a thousand Photos with the

1664
01:32:57,480 --> 01:33:01,160
Speaker 4:  iPhone 15 Pro Max, one of the best videos we've ever done. It's on

1665
01:33:01,160 --> 01:33:04,080
Speaker 4:  YouTube. Lots of photos. Becca has lots of thoughts about this camera. She

1666
01:33:04,080 --> 01:33:08,000
Speaker 4:  compares it to the first iPhone camera. Cool. Which is actually, it is more

1667
01:33:08,000 --> 01:33:11,360
Speaker 4:  complicated than you might think. Again, just exposure,

1668
01:33:11,360 --> 01:33:15,040
Speaker 4:  philosophical quandary on these phones. Go watch that video. Okay. That's

1669
01:33:15,040 --> 01:33:17,040
Speaker 4:  it. That's Vergecast Rock Roll.

1670
01:33:22,060 --> 01:33:25,480
Speaker 8:  And that's a wrap for Vergecast this week. We'd love to hear from you. Shoot

1671
01:33:25,480 --> 01:33:29,280
Speaker 8:  us an email at Vergecast at The Verge dot com. The Vergecast is a

1672
01:33:29,280 --> 01:33:33,000
Speaker 8:  production of The Verge and the Vox Media Podcast network. The show is produced

1673
01:33:33,000 --> 01:33:36,920
Speaker 8:  by me, Liam James, and our senior audio director, Andrew Marino. Our

1674
01:33:36,920 --> 01:33:40,480
Speaker 8:  editorial director is Brooke Miners. That's it. We'll see you next week.

