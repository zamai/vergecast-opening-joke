1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 0376376a-fc94-4b6c-b841-483be4437f40
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/165604127018391832/522953423619683794/s93290-US-2500s-1721554357.mp3
Description: On this episode of The Vergecast, senior producer Will Poor explores the AI-tinged worlds of Dries Depoorter. Depoorter has built all manner of quirky and provocative installations and online experiments. There’s a clock that tells you how much of your life you’ve already lived; a phone charger that only works when your eyes are closed; a mobile chat app that you can only use when your phone has less than 5% battery.
His most eyebrow-raising work, though, is around AI and surveillance. In his projects Depoorter takes publicly available webcam footage from around the world, and uses it to stalk celebrities, catch jaywalkers in the act, keep politicians honest, and generally make you wonder about your own privacy and anonymity.
We talked with Depoorter about how he creates his work, how he thinks about the future of AI, and how he responds to the people who see his art and want to turn it into commerce. It’s a wild conversation, so check it out above. To see all of Dries’ work, head over to his portfolio.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (21 ads detected)

2
00:01:14,295 --> 00:01:18,135
Speaker 3:  let's, let's see where AI is being used that isn't just to like

3
00:01:18,135 --> 00:01:21,615
Speaker 3:  write slightly better business emails. Yeah. What'd you find?

4
00:01:22,215 --> 00:01:25,535
Speaker 4:  I went as far afield as I could go. I wanted to get

5
00:01:26,005 --> 00:01:29,895
Speaker 4:  away from the hype and skepticism cycle around all

6
00:01:29,895 --> 00:01:33,655
Speaker 4:  the AI tools that we're talking about, get away from all the product announcements

7
00:01:34,125 --> 00:01:37,335
Speaker 4:  that we're following. I wanted to make AI

8
00:01:38,005 --> 00:01:41,495
Speaker 4:  kind of unfamiliar to myself again and

9
00:01:42,015 --> 00:01:45,695
Speaker 4:  a coworker as I was chatting about this reminded me of this thing that got

10
00:01:45,695 --> 00:01:49,335
Speaker 4:  shared around The Verge a ton a couple of years ago. It was a project

11
00:01:49,405 --> 00:01:52,775
Speaker 4:  that was called The Follower and it used facial

12
00:01:52,885 --> 00:01:56,415
Speaker 4:  recognition to match social media photos

13
00:01:57,035 --> 00:02:00,815
Speaker 4:  to open webcam footage. So basically this thing

14
00:02:01,155 --> 00:02:04,575
Speaker 4:  looked at unsecured webcams in like Times Square

15
00:02:05,235 --> 00:02:09,015
Speaker 4:  and then it looked at public Instagram photos taken at the same time in the

16
00:02:09,015 --> 00:02:12,815
Speaker 4:  same place and used facial recognition to match up

17
00:02:12,815 --> 00:02:16,615
Speaker 4:  the people that appeared in both feeds. So you got this creepy

18
00:02:16,875 --> 00:02:20,735
Speaker 4:  second angle on social media posts that came from these

19
00:02:20,885 --> 00:02:22,855
Speaker 4:  unsecured webcams. Oh

20
00:02:22,915 --> 00:02:26,175
Speaker 3:  Man, that's one of those things that it's like you put all this information

21
00:02:26,175 --> 00:02:28,895
Speaker 3:  on the internet and you sort of exist in the world knowing that there are

22
00:02:29,095 --> 00:02:32,335
Speaker 3:  lots of cameras around, but I feel like nobody ever thinks about it And like

23
00:02:32,545 --> 00:02:34,935
Speaker 3:  we've talked about this a lot on this show that like having

24
00:02:36,455 --> 00:02:39,775
Speaker 3:  computers collect information about you is one thing and it feels bad, but

25
00:02:39,775 --> 00:02:42,855
Speaker 3:  you don't really think about it. But then when it shows it back to you, yes

26
00:02:42,955 --> 00:02:46,935
Speaker 3:  it becomes, it, it feels different, right? Like it becomes visceral in a

27
00:02:46,935 --> 00:02:50,415
Speaker 3:  way that feels almost like horrifying even though you're like, well yeah,

28
00:02:50,495 --> 00:02:52,255
Speaker 3:  I took that picture and put it on the internet

29
00:02:52,455 --> 00:02:56,415
Speaker 4:  Completely. And for it to be paired with, as you say, these webcam

30
00:02:56,625 --> 00:03:00,205
Speaker 4:  feeds that you know in the abstract are out there everywhere and

31
00:03:00,205 --> 00:03:03,405
Speaker 4:  following you around, but you just don't have evidence of

32
00:03:04,145 --> 00:03:08,125
Speaker 4:  you out there in them. And that's what this does. So if you

33
00:03:08,305 --> 00:03:12,285
Speaker 4:  scroll through this project, I I, I saw this one photo of

34
00:03:12,685 --> 00:03:16,405
Speaker 4:  a guy in a leather jacket who's casually leaning against this famous

35
00:03:16,665 --> 00:03:20,405
Speaker 4:  bar in Dublin. It's a very standard sort of social media

36
00:03:20,475 --> 00:03:24,365
Speaker 4:  post, but then in the camera footage you get to watch him try

37
00:03:24,365 --> 00:03:28,285
Speaker 4:  out different poses and angles. He, he ab tests jacket or

38
00:03:28,345 --> 00:03:32,205
Speaker 4:  no jacket. When he is taking this picture, you, you see how the social media

39
00:03:32,235 --> 00:03:36,045
Speaker 4:  post came to be. There's this other shot of these

40
00:03:36,075 --> 00:03:39,885
Speaker 4:  very two cool hip guys strolling through Times Square.

41
00:03:39,995 --> 00:03:42,925
Speaker 4:  It's really epic looking, but then in the webcam footage

42
00:03:43,785 --> 00:03:47,085
Speaker 4:  you see them just walking through busy times Square, they almost bump into

43
00:03:47,085 --> 00:03:50,885
Speaker 4:  someone who's wheeling luggage around. It is just like, it totally changes

44
00:03:51,035 --> 00:03:55,005
Speaker 4:  your view on social media posts and how they came

45
00:03:55,005 --> 00:03:55,365
Speaker 4:  to be.

46
00:03:56,075 --> 00:03:59,885
Speaker 3:  That thing is like a shtick of some social media posts. Like I saw a

47
00:03:59,885 --> 00:04:03,725
Speaker 3:  TikTok the other day that was a person explaining how to

48
00:04:03,775 --> 00:04:07,765
Speaker 3:  sleep in an airport and she like, you know, put her bags on and

49
00:04:07,965 --> 00:04:09,525
Speaker 3:  attached everything to her in such a way that it would be hard to get off

50
00:04:09,525 --> 00:04:12,965
Speaker 3:  of her. Puts on her eye mask and puts up her hood and goes to sleep. And

51
00:04:13,165 --> 00:04:16,965
Speaker 3:  then it was stitched by somebody who like walks over waves in her face and

52
00:04:16,965 --> 00:04:20,925
Speaker 3:  then just steals her phone. And it's like the, the the like pulling

53
00:04:21,075 --> 00:04:24,365
Speaker 3:  back the curtain on how this stuff actually works is like part of how this

54
00:04:24,365 --> 00:04:27,925
Speaker 3:  all works on the internet but the people doing it are still the ones in control.

55
00:04:27,955 --> 00:04:31,805
Speaker 3:  This is just like the world is telling on you in a way

56
00:04:31,925 --> 00:04:34,765
Speaker 3:  that it kind of like, like there are people who would make those behind the

57
00:04:34,765 --> 00:04:37,805
Speaker 3:  scenes video of them doing the 10 different tries, the blooper reel and whatever,

58
00:04:38,145 --> 00:04:41,285
Speaker 3:  but when it's somebody else doing it to you because they saw it happen,

59
00:04:41,515 --> 00:04:44,965
Speaker 4:  Yeah, it's like if that person stitching the TikTok was actually there for

60
00:04:44,965 --> 00:04:48,645
Speaker 4:  the recording of the first TikTok and messing with the person as they were

61
00:04:48,645 --> 00:04:51,805
Speaker 4:  doing it, which is just next level weird and creepy. Seriously

62
00:04:52,225 --> 00:04:56,085
Speaker 4:  that's what I gravitated towards. 'cause it's all of this very

63
00:04:56,405 --> 00:04:59,645
Speaker 4:  publicly available data plus AI equals

64
00:05:00,295 --> 00:05:04,045
Speaker 4:  weird disconcerting stuff and I, I just, I figured whoever made

65
00:05:04,275 --> 00:05:08,125
Speaker 4:  made that, whoever made the follower would have a lot of

66
00:05:08,635 --> 00:05:11,805
Speaker 4:  very different, very interesting thoughts on ai. So I called him up and talked

67
00:05:11,805 --> 00:05:12,325
Speaker 4:  to him about it.

68
00:05:16,035 --> 00:05:19,525
Speaker 4:  Just to get started, I'll start with the hardest question first, just tell

69
00:05:19,525 --> 00:05:20,925
Speaker 4:  me your name and what you do.

70
00:05:22,345 --> 00:05:26,045
Speaker 6:  My name is DRIs Porter and I'm an artist working with

71
00:05:26,045 --> 00:05:26,645
Speaker 6:  technology.

72
00:05:27,475 --> 00:05:31,285
Speaker 4:  DRIs is 33 years old. He lives in Belgium and for

73
00:05:31,285 --> 00:05:34,925
Speaker 4:  more than a decade he's been using technology in ways that are

74
00:05:34,925 --> 00:05:38,805
Speaker 4:  inspiring silly and kind of disturbing his

75
00:05:38,805 --> 00:05:42,645
Speaker 4:  work is this unique mix of technical proficiency and outlandish

76
00:05:42,645 --> 00:05:43,645
Speaker 4:  ideas. So

77
00:05:43,885 --> 00:05:47,365
Speaker 6:  I have a background in electronics. I

78
00:05:47,435 --> 00:05:50,805
Speaker 6:  studied electronics for six years. Really

79
00:05:51,375 --> 00:05:51,925
Speaker 6:  loved it.

80
00:05:53,565 --> 00:05:57,080
Speaker 6:  I was really enjoying experimenting with the the

81
00:05:57,080 --> 00:06:00,005
Speaker 6:  electronics but I missed a sort of creativity.

82
00:06:00,985 --> 00:06:04,605
Speaker 6:  So after I was graduated

83
00:06:05,085 --> 00:06:08,805
Speaker 6:  I went to an art school and that was like a, a really

84
00:06:09,335 --> 00:06:12,685
Speaker 6:  weird switch. I was studying media arts

85
00:06:13,545 --> 00:06:17,485
Speaker 6:  and I really loved it. It was something

86
00:06:17,485 --> 00:06:20,965
Speaker 6:  totally different. After that I went to an

87
00:06:21,245 --> 00:06:24,765
Speaker 6:  advertising company and I worked for two years as an net concept

88
00:06:25,125 --> 00:06:29,005
Speaker 6:  provider. That's someone that's come up with with IDs

89
00:06:29,005 --> 00:06:30,805
Speaker 6:  for the, for the agency. So

90
00:06:30,805 --> 00:06:34,645
Speaker 4:  That's the fuel mixture for this guy. Electronics plus art

91
00:06:34,795 --> 00:06:38,645
Speaker 4:  plus weird concepts. When I scrolled through Dre's portfolio,

92
00:06:39,145 --> 00:06:42,965
Speaker 4:  the first concepts that caught my eye weren't actually AI powered.

93
00:06:43,435 --> 00:06:47,245
Speaker 4:  Lots of his ideas are these simple, clever conceits brought to

94
00:06:47,245 --> 00:06:51,205
Speaker 4:  life. They're little bobble universes that play by one or two really simple

95
00:06:51,415 --> 00:06:55,365
Speaker 4:  rules like he built this interactive exhibit called Recharge.

96
00:06:55,795 --> 00:06:58,925
Speaker 4:  It's a phone charger that only works when your eyes are closed

97
00:06:59,705 --> 00:07:03,365
Speaker 4:  in the gallery. You lie back in a lawn chair and plug your phone into an

98
00:07:03,365 --> 00:07:07,125
Speaker 4:  outlet, but there's a camera pointed at your head and a computer

99
00:07:07,125 --> 00:07:10,885
Speaker 4:  running face tracking software. So the exhibit only sends

100
00:07:11,405 --> 00:07:15,365
Speaker 4:  electricity to the outlet if your eyes are closed. He also made a

101
00:07:15,365 --> 00:07:19,285
Speaker 4:  website that You can take over and post anything you want, but only

102
00:07:19,305 --> 00:07:22,925
Speaker 4:  if you have more followers on X than the last person To do that.

103
00:07:23,705 --> 00:07:27,485
Speaker 4:  The latest person to post is a guy named Kev Adams who just wrote Subscribe

104
00:07:27,485 --> 00:07:31,405
Speaker 4:  to Kev Adams. One other project that I still can't

105
00:07:31,405 --> 00:07:35,325
Speaker 4:  get outta my head is a little gadgety looking LED clock in

106
00:07:35,365 --> 00:07:39,205
Speaker 4:  a classy wooden frame that dries actually makes and sells.

107
00:07:39,825 --> 00:07:43,325
Speaker 6:  So I have a project called Short Life. It's

108
00:07:43,725 --> 00:07:47,245
Speaker 6:  a clock. It's a clock that shows a percentage

109
00:07:47,825 --> 00:07:51,565
Speaker 6:  and that's a personal percentage because it shows

110
00:07:51,865 --> 00:07:55,725
Speaker 6:  how much of your life that is already completed based on

111
00:07:55,795 --> 00:07:59,725
Speaker 6:  your life dependency in your country. I have like different people

112
00:07:59,755 --> 00:08:02,565
Speaker 6:  that order it, like sometimes I have

113
00:08:03,675 --> 00:08:07,605
Speaker 6:  fathers that bought it for the newborn or

114
00:08:08,475 --> 00:08:12,205
Speaker 6:  this year I sold a clock for someone that has more than

115
00:08:12,385 --> 00:08:15,965
Speaker 6:  120% so that that person was

116
00:08:16,025 --> 00:08:17,605
Speaker 6:  108 years old.

117
00:08:17,785 --> 00:08:18,285
Speaker 4:  Oh my gosh.

118
00:08:18,505 --> 00:08:20,165
Speaker 6:  And I will only sell 1 million,

119
00:08:20,675 --> 00:08:21,965
Speaker 4:  Just one, just 1 million.

120
00:08:23,275 --> 00:08:26,485
Speaker 6:  It's a limited edition, only 1 million

121
00:08:26,775 --> 00:08:30,725
Speaker 4:  Order Now before it's too late if it's not clear from all these

122
00:08:30,725 --> 00:08:34,285
Speaker 4:  projects, DRIs loves the elegance of a good simple idea

123
00:08:34,315 --> 00:08:36,485
Speaker 4:  brought to life through lots of tinkering.

124
00:08:36,865 --> 00:08:39,685
Speaker 6:  The first idea is most of the time really complex,

125
00:08:41,425 --> 00:08:42,525
Speaker 6:  but then I try to

126
00:08:44,275 --> 00:08:48,245
Speaker 6:  make them really small. For me it's important that I can explain the

127
00:08:48,385 --> 00:08:52,005
Speaker 6:  ID in one sentence. I also have this notes app

128
00:08:52,305 --> 00:08:56,005
Speaker 6:  in my phone and I note a lot of random ideas.

129
00:08:56,595 --> 00:08:58,725
Speaker 6:  Most of the time they're really bad ID

130
00:09:00,815 --> 00:09:04,355
Speaker 6:  and what I do is I always go back

131
00:09:05,545 --> 00:09:09,365
Speaker 6:  to the list and what I do is combining two bad ideas

132
00:09:09,365 --> 00:09:11,725
Speaker 6:  with together that makes like a better idea.

133
00:09:12,025 --> 00:09:15,925
Speaker 4:  Do you have an example of two bad ideas that made one good idea?

134
00:09:16,525 --> 00:09:20,005
Speaker 6:  I think a good example here was an app that I

135
00:09:20,005 --> 00:09:23,845
Speaker 6:  developed named Die With Me And Die

136
00:09:23,845 --> 00:09:27,365
Speaker 6:  With Me is an app which You can only use when you have less than

137
00:09:27,365 --> 00:09:31,125
Speaker 6:  5% battery on your phone. If you have less than 5% battery

138
00:09:31,225 --> 00:09:35,045
Speaker 6:  on your phone, You can start chatting to other people

139
00:09:35,145 --> 00:09:39,045
Speaker 6:  having a a battery below 5%. How I came up with

140
00:09:39,045 --> 00:09:42,845
Speaker 6:  this idea, I was in a city where I was not able to

141
00:09:42,845 --> 00:09:43,925
Speaker 6:  find my way back

142
00:09:45,465 --> 00:09:49,445
Speaker 6:  to my hotel because my phone died. And for me that

143
00:09:49,445 --> 00:09:53,365
Speaker 6:  was really inspiring and I was like, okay. I'm like really?

144
00:09:54,685 --> 00:09:58,605
Speaker 6:  I really, really depended on on my phone. I am even not

145
00:09:58,605 --> 00:10:01,485
Speaker 6:  able to find, find my way, my way back.

146
00:10:02,625 --> 00:10:06,525
Speaker 6:  And then I, I had this idea like, okay, I'll make an app

147
00:10:06,525 --> 00:10:10,245
Speaker 6:  that You can only use when you have a low battery. And I

148
00:10:10,295 --> 00:10:14,205
Speaker 6:  noted that later that night in my phone and I

149
00:10:14,205 --> 00:10:17,925
Speaker 6:  think it was on my phone for like more than a year because

150
00:10:18,085 --> 00:10:21,965
Speaker 6:  I was not able to find like a good ID with it

151
00:10:21,965 --> 00:10:25,885
Speaker 6:  because I had like only this concept of, okay, let's

152
00:10:26,135 --> 00:10:29,805
Speaker 6:  build an app which You can only use when you have less than 5% battery.

153
00:10:30,185 --> 00:10:34,165
Speaker 6:  But I was like, really? Okay, it needs to be like a dating app

154
00:10:34,225 --> 00:10:37,765
Speaker 6:  or sort of a game. But that was really not

155
00:10:38,315 --> 00:10:41,805
Speaker 6:  perfect for me. It didn't click or something.

156
00:10:42,185 --> 00:10:45,765
Speaker 6:  And then I had like a vague idea of like a chat app

157
00:10:46,425 --> 00:10:50,405
Speaker 6:  and then I just, one day I was reading through all the

158
00:10:50,405 --> 00:10:54,285
Speaker 6:  ideas and then it clicked like, okay Die With Me is like, and

159
00:10:54,355 --> 00:10:56,445
Speaker 6:  chat app. That's the best concept

160
00:10:57,065 --> 00:11:00,925
Speaker 4:  Of all the ideas that DRIs mixes and matches in his work. There are

161
00:11:00,925 --> 00:11:04,245
Speaker 4:  two that just seem to keep coming up over and over again,

162
00:11:05,185 --> 00:11:09,005
Speaker 4:  AI and surveillance. He says he got started down

163
00:11:09,005 --> 00:11:12,965
Speaker 4:  this path after making a discovery on one of the sketchier corners of the

164
00:11:13,125 --> 00:11:13,325
Speaker 4:  internet.

165
00:11:13,625 --> 00:11:17,205
Speaker 6:  During my work at the advertising

166
00:11:17,425 --> 00:11:20,885
Speaker 6:  agency, I was really into exploring

167
00:11:22,555 --> 00:11:26,525
Speaker 6:  open cameras. When I talk about open cameras, I'm talking about

168
00:11:26,915 --> 00:11:30,365
Speaker 6:  cameras that are connected to the internet with a standard password

169
00:11:31,065 --> 00:11:32,365
Speaker 6:  or no password at all.

170
00:11:32,665 --> 00:11:36,045
Speaker 4:  How did you discover them and what was so exciting about

171
00:11:36,755 --> 00:11:37,605
Speaker 4:  that resource?

172
00:11:38,125 --> 00:11:41,845
Speaker 6:  I discovered them through website called show them.

173
00:11:42,355 --> 00:11:46,165
Speaker 6:  It's sort of a search engine

174
00:11:46,985 --> 00:11:50,405
Speaker 6:  but it doesn't index website

175
00:11:50,785 --> 00:11:54,645
Speaker 6:  but it index machines on the internet. So with

176
00:11:54,645 --> 00:11:57,965
Speaker 6:  this tool you are able to find open cameras,

177
00:11:58,545 --> 00:12:02,405
Speaker 6:  you find cameras and living rooms and and places where people

178
00:12:03,925 --> 00:12:06,765
Speaker 6:  I think don't want to, to have that publicly open.

179
00:12:07,445 --> 00:12:11,165
Speaker 4:  I checked out this website Shodan and it is exactly how

180
00:12:11,325 --> 00:12:14,965
Speaker 4:  DRIs describes it. After just a couple of clicks I found

181
00:12:14,965 --> 00:12:18,765
Speaker 4:  myself browsing webcams that definitely aren't supposed to be public.

182
00:12:19,475 --> 00:12:23,365
Speaker 4:  Some were innocuous, like closeup bird feeder cams, but I

183
00:12:23,365 --> 00:12:26,885
Speaker 4:  also watched people playing with dogs inside a doggy daycare

184
00:12:27,285 --> 00:12:30,925
Speaker 4:  business. There was another camera in a living room that I could

185
00:12:31,075 --> 00:12:34,925
Speaker 4:  control, like pan it around. I got really weirded out

186
00:12:34,925 --> 00:12:38,645
Speaker 4:  and closed all the tabs. Shoan is a site that you could do deeply

187
00:12:38,755 --> 00:12:42,725
Speaker 4:  sketchy things with. And this is a personal nightmare

188
00:12:42,725 --> 00:12:46,525
Speaker 4:  of mine that I don't know enough about how my own gadgets

189
00:12:46,525 --> 00:12:50,325
Speaker 4:  work or have enough of a handle on the tech around me generally

190
00:12:50,755 --> 00:12:54,685
Speaker 4:  that I'm somehow broadcasting myself out into the world without realizing

191
00:12:54,685 --> 00:12:57,805
Speaker 4:  it. Like I'm a sticky note on my webcam person.

192
00:12:58,465 --> 00:12:59,965
Speaker 4:  I'm superstitious about all of it.

193
00:13:00,515 --> 00:13:03,965
Speaker 3:  Okay, but wait, hold on. Will hang on. Let's talk about how

194
00:13:03,965 --> 00:13:07,885
Speaker 3:  superstitious this actually is, right? Because there is a thing

195
00:13:08,155 --> 00:13:11,485
Speaker 3:  happening with all of these AI companies that there is this incredible race

196
00:13:11,485 --> 00:13:15,445
Speaker 3:  to get tons of data, a vast amount of that data that is just like

197
00:13:15,565 --> 00:13:18,205
Speaker 3:  a thing that someone made that is finished on the internet, right? Yeah.

198
00:13:18,505 --> 00:13:22,405
Speaker 3:  We kind of know what to do with an article or even like a

199
00:13:22,405 --> 00:13:25,765
Speaker 3:  YouTube video. But the idea of all of these webcams around

200
00:13:26,875 --> 00:13:30,325
Speaker 3:  just collecting data all the time, like if you think about live video as

201
00:13:30,325 --> 00:13:33,925
Speaker 3:  just a live stream of data, it feels very different to me. Yeah.

202
00:13:34,115 --> 00:13:36,285
Speaker 3:  Like you're saying, like you're, you're finding these things where I think

203
00:13:36,285 --> 00:13:40,085
Speaker 3:  lots of people are setting up webcams and don't know, like you, we've all

204
00:13:40,085 --> 00:13:42,405
Speaker 3:  heard the horror stories over the years about people who are hacking into

205
00:13:42,405 --> 00:13:46,365
Speaker 3:  other people's like baby monitors and all kinds of stuff. And so part of

206
00:13:46,365 --> 00:13:50,285
Speaker 3:  me is, I'm not really a tinfoil hat kind of person, but the idea of

207
00:13:51,135 --> 00:13:55,045
Speaker 3:  these are unbelievable fountains of data, plus there

208
00:13:55,065 --> 00:13:58,685
Speaker 3:  are more and more compelling reasons for people to want to get access to

209
00:13:58,745 --> 00:14:02,645
Speaker 3:  and store forever. All of that data feels kind of terrifying.

210
00:14:02,645 --> 00:14:06,525
Speaker 3:  Especially then if You can figure out how to ask kind of the right

211
00:14:06,805 --> 00:14:09,885
Speaker 3:  questions of all of that stuff. Like maybe that's the challenge there is

212
00:14:10,255 --> 00:14:14,125
Speaker 3:  we're gonna end up with just a hundred years of Times Square and then the

213
00:14:14,205 --> 00:14:16,365
Speaker 3:  question is like, well what do we, what do we do with any of this? But it's

214
00:14:16,365 --> 00:14:18,485
Speaker 3:  all there, you just have to know what to do with it, right? Yeah,

215
00:14:18,485 --> 00:14:21,925
Speaker 4:  That's, well first of all, thank you for co-signing my paranoia

216
00:14:22,915 --> 00:14:23,405
Speaker 4:  because

217
00:14:23,405 --> 00:14:26,605
Speaker 3:  This is, I mean it's funny, I'm not, I'm not a sticky note on the webcam

218
00:14:26,605 --> 00:14:30,445
Speaker 3:  convers, yeah. Like it's, I never really have been. My sense is like I'm

219
00:14:30,445 --> 00:14:34,165
Speaker 3:  not quite in the, I have nothing to hide camp, but

220
00:14:34,225 --> 00:14:37,965
Speaker 3:  I'm in the, like, as long as I'm reasonably responsible, it's probably gonna

221
00:14:37,965 --> 00:14:41,805
Speaker 3:  be fine camp. But like I've been to Times Square, there is

222
00:14:42,085 --> 00:14:43,245
Speaker 3:  probably's, lots of footage of you there. Footage of you

223
00:14:43,505 --> 00:14:43,725
Speaker 4:  In

224
00:14:43,725 --> 00:14:47,525
Speaker 3:  Times, times Square or some Yeah. And I, that is where it's like, okay, it's,

225
00:14:47,525 --> 00:14:51,485
Speaker 3:  it's no longer just like a thing that I'm in relative control of and

226
00:14:51,485 --> 00:14:54,845
Speaker 3:  should avoid making mistakes. It's just like being a person in the world.

227
00:14:55,585 --> 00:14:56,765
Speaker 3:  You become training data.

228
00:14:56,915 --> 00:15:00,845
Speaker 4:  Yeah. And I mean that just fans. I, I don't, I think you're right that that

229
00:15:01,225 --> 00:15:05,205
Speaker 4:  the vulnerability of my MacBook camera probably has not

230
00:15:05,205 --> 00:15:08,845
Speaker 4:  changed dramatically with the advent of all of these AI companies who bring

231
00:15:08,845 --> 00:15:12,685
Speaker 4:  up data, but it just fans the flames so much

232
00:15:12,685 --> 00:15:16,405
Speaker 4:  more. Totally. And so like these are when I, when I look

233
00:15:16,405 --> 00:15:20,285
Speaker 4:  through all of Dre's projects, this is all what comes up for

234
00:15:20,285 --> 00:15:23,805
Speaker 4:  me. And so that's like, that's my reaction to it. And I'm so interested in

235
00:15:23,805 --> 00:15:27,325
Speaker 4:  his work because I think he has all of the same questions

236
00:15:27,955 --> 00:15:31,845
Speaker 4:  that we are asking, but instead of putting sticky notes on

237
00:15:31,845 --> 00:15:35,765
Speaker 4:  his webcams, he made a lot of really fascinating stuff around

238
00:15:35,765 --> 00:15:35,965
Speaker 4:  it.

239
00:15:36,565 --> 00:15:39,805
Speaker 6:  I was really inspired by this and it and it started a lot of

240
00:15:40,745 --> 00:15:43,365
Speaker 6:  my projects around surveillance.

241
00:15:43,745 --> 00:15:47,165
Speaker 4:  We need to take a quick break, but when we come back, DRE starts

242
00:15:47,485 --> 00:15:51,405
Speaker 4:  mashing together. AI and sketchy webcam feeds and things

243
00:15:51,425 --> 00:15:52,485
Speaker 4:  get real weird fast.

244
00:17:55,395 --> 00:17:58,965
Speaker 6:  Yeah, I was exploring a lot, but

245
00:17:59,185 --> 00:18:03,125
Speaker 6:  I'm, I always explored with open source too. So if I

246
00:18:03,125 --> 00:18:07,045
Speaker 6:  need in a project and face recognition, I don't start writing

247
00:18:07,185 --> 00:18:11,005
Speaker 6:  it from zero. I use something that is out there that I'm able

248
00:18:11,005 --> 00:18:14,725
Speaker 6:  to use and, and, and I combine all those

249
00:18:15,815 --> 00:18:19,605
Speaker 6:  tools that are out there. So most of the time

250
00:18:19,885 --> 00:18:22,525
Speaker 6:  I don't need to write everything from zero.

251
00:18:22,985 --> 00:18:26,925
Speaker 4:  Do you have ideas first and then you go and find the tools

252
00:18:27,035 --> 00:18:30,765
Speaker 4:  that will help you? Or do you learn about a new tool and have an

253
00:18:30,765 --> 00:18:32,645
Speaker 4:  idea? Which comes first?

254
00:18:33,805 --> 00:18:37,325
Speaker 6:  Hmm. Yeah, it's, most of the time it's me

255
00:18:37,715 --> 00:18:41,525
Speaker 6:  exploring new, new technologies and just playing

256
00:18:41,525 --> 00:18:45,165
Speaker 6:  around, even checking out

257
00:18:46,085 --> 00:18:49,885
Speaker 6:  documentation of software documentation of a new

258
00:18:50,045 --> 00:18:53,965
Speaker 6:  API. That is really interesting. Checking out the documentation of the

259
00:18:54,085 --> 00:18:58,045
Speaker 6:  GPT, the, the API that's, that sparks a lot of ideas for

260
00:18:58,045 --> 00:18:58,205
Speaker 6:  me.

261
00:18:58,825 --> 00:19:02,645
Speaker 4:  In this case, he taught himself how to use some computer vision algorithms

262
00:19:02,645 --> 00:19:06,325
Speaker 4:  from an open source library called Open cv, which

263
00:19:06,425 --> 00:19:10,165
Speaker 4:  by the way is a dizzyingly huge and powerful set of tools.

264
00:19:10,995 --> 00:19:14,365
Speaker 4:  Chat GPT might get more attention these days, but open source

265
00:19:14,515 --> 00:19:18,485
Speaker 4:  libraries like Open CV are putting things like facial recognition into

266
00:19:18,765 --> 00:19:22,005
Speaker 4:  a lot more hands for better or worse.

267
00:19:22,825 --> 00:19:26,685
Speaker 4:  Anyway, DRIs gave himself a crash course in Python and then

268
00:19:26,715 --> 00:19:30,365
Speaker 4:  used OpenCV to hack together this funny little machine.

269
00:19:30,925 --> 00:19:33,965
Speaker 6:  I selected an area where the traffic light is

270
00:19:35,065 --> 00:19:39,005
Speaker 6:  and then it sort of counts up all

271
00:19:39,025 --> 00:19:42,605
Speaker 6:  the, the pixels and it searches for the, the main color

272
00:19:43,385 --> 00:19:46,885
Speaker 6:  in that area. If that main color is red

273
00:19:47,545 --> 00:19:50,865
Speaker 6:  or if it's more red than green, then

274
00:19:51,765 --> 00:19:55,345
Speaker 6:  the software thinks it's like, okay, this is a green or red light

275
00:19:55,965 --> 00:19:59,345
Speaker 6:  and then it checks in a specific zone

276
00:19:59,845 --> 00:20:03,465
Speaker 6:  if someone is crossing over with the yeah, person detection.

277
00:20:03,845 --> 00:20:07,825
Speaker 4:  But he still needed something to do with his auto jaywalk detector.

278
00:20:08,525 --> 00:20:12,385
Speaker 4:  He needed the second idea to combine with this one. So

279
00:20:12,385 --> 00:20:15,425
Speaker 4:  he added a few more steps and debuted jaywalking.

280
00:20:15,935 --> 00:20:18,945
Speaker 6:  It's an interactive installation where You can report

281
00:20:19,435 --> 00:20:23,265
Speaker 6:  jaywalkers from all over the world with the help of

282
00:20:23,935 --> 00:20:27,505
Speaker 6:  open cameras that are pointed to crossroads

283
00:20:27,985 --> 00:20:31,945
Speaker 4:  Visitors to the installation stand in a dark room in front of a bright screen

284
00:20:31,975 --> 00:20:35,825
Speaker 4:  playing a stream from an unsecured traffic camera. The

285
00:20:35,985 --> 00:20:39,425
Speaker 4:  screen also displays a big graphic showing whether the stoplight in view

286
00:20:39,485 --> 00:20:43,345
Speaker 4:  is red or green. So participants stand and they watch

287
00:20:43,495 --> 00:20:47,445
Speaker 4:  live as unsuspecting people cross the street, the street.

288
00:20:47,445 --> 00:20:51,205
Speaker 4:  But all the while there's one other thing in front of them, a big

289
00:20:51,255 --> 00:20:55,165
Speaker 4:  round button on a pedestal, the kind of button that looks so

290
00:20:55,255 --> 00:20:59,045
Speaker 4:  satisfying to hit. What happens if it identifies a Jay

291
00:20:59,045 --> 00:21:00,205
Speaker 4:  walker? What happens next?

292
00:21:00,665 --> 00:21:03,685
Speaker 6:  You get the question, would you like to report to Jay Walker?

293
00:21:04,585 --> 00:21:08,125
Speaker 6:  And then the, the visitor of the, the interactive installation

294
00:21:08,385 --> 00:21:11,685
Speaker 6:  can report the jaywalker by pressing the

295
00:21:12,075 --> 00:21:15,765
Speaker 6:  emergency button and then it'll send an email to the closest police

296
00:21:15,765 --> 00:21:19,645
Speaker 6:  station Okay. With a screenshot of the jaywalker. And

297
00:21:19,645 --> 00:21:22,525
Speaker 6:  that email address only exists for 10 minutes.

298
00:21:23,385 --> 00:21:26,565
Speaker 4:  Gotcha. Okay. The system is automatically

299
00:21:26,875 --> 00:21:30,805
Speaker 4:  detecting Jay walker's and it's, you're, you're forcing a decision

300
00:21:30,905 --> 00:21:34,885
Speaker 4:  by the viewer of the the installation. Yes. What they want to

301
00:21:34,885 --> 00:21:38,485
Speaker 4:  do with that, if you've like done any machine vision on the installations

302
00:21:38,585 --> 00:21:41,565
Speaker 4:  to know how many people press the button of, of all the viewers,

303
00:21:43,545 --> 00:21:47,085
Speaker 6:  In the beginning it was not saving this data. Then later,

304
00:21:47,845 --> 00:21:51,485
Speaker 6:  I mean like the last couple of years I saved this data and

305
00:21:51,625 --> 00:21:55,445
Speaker 6:  it was a lot exhibited in Belgium and the Netherlands.

306
00:21:55,865 --> 00:21:59,685
Speaker 6:  And what I saw that is that more people in the Netherlands

307
00:21:59,875 --> 00:22:01,405
Speaker 6:  presses than in Belgium.

308
00:22:02,665 --> 00:22:06,445
Speaker 4:  AI plus surveillance turned out to be a pretty deep well for

309
00:22:06,455 --> 00:22:10,165
Speaker 4:  Drees. He made an exhibit that tracks celebrities using

310
00:22:10,315 --> 00:22:13,805
Speaker 4:  unsecured camera footage. Another that constantly searches

311
00:22:14,155 --> 00:22:17,485
Speaker 4:  webcams for an anonymous man that his sister met once by chance.

312
00:22:18,635 --> 00:22:22,365
Speaker 4:  Both of those are physical installations that are built to run indefinitely

313
00:22:22,425 --> 00:22:25,885
Speaker 4:  in galleries. They're these little purpose-built boxes of

314
00:22:25,955 --> 00:22:29,805
Speaker 4:  electronics with screens that are forever showing live webcam footage.

315
00:22:30,235 --> 00:22:33,725
Speaker 4:  They're constantly on the lookout for whatever their algorithms have been

316
00:22:33,725 --> 00:22:37,645
Speaker 4:  trained to track. They look harmless and contained on their

317
00:22:37,645 --> 00:22:40,405
Speaker 4:  own until you think about what they're actually doing.

318
00:22:41,625 --> 00:22:44,965
Speaker 4:  My favorite DRE project though is the entirely virtual

319
00:22:45,765 --> 00:22:46,565
Speaker 4:  Flemish scrollers.

320
00:22:46,985 --> 00:22:50,965
Speaker 6:  So the Flemish Scrollers is a project that checks the live

321
00:22:50,985 --> 00:22:54,645
Speaker 6:  stream of my government. They have meetings and they

322
00:22:54,875 --> 00:22:58,405
Speaker 6:  live stream this on YouTube and I made

323
00:22:58,725 --> 00:23:02,485
Speaker 6:  software that is constantly checking the live stream to

324
00:23:02,485 --> 00:23:06,365
Speaker 6:  find as a politician is on their phone. If there for

325
00:23:06,565 --> 00:23:10,245
Speaker 6:  a certain amount of minutes on their phone, it'll tweet them

326
00:23:11,145 --> 00:23:14,845
Speaker 6:  and tag them with the help of face recognition and the

327
00:23:15,005 --> 00:23:18,965
Speaker 6:  tweets look like, Hey dear, distracted, please

328
00:23:18,965 --> 00:23:22,725
Speaker 6:  stay focused. Then you have like a short video of of that

329
00:23:22,725 --> 00:23:22,965
Speaker 6:  person.

330
00:23:23,505 --> 00:23:27,365
Speaker 4:  The idea of using these creepy tools for accountability is just

331
00:23:27,465 --> 00:23:31,405
Speaker 4:  so surprising and hilarious to me. I, I want this in America, I

332
00:23:31,405 --> 00:23:35,325
Speaker 4:  want congress fully surveilled by AI while they're debating tech

333
00:23:35,325 --> 00:23:39,125
Speaker 4:  policy. It's, it's just too good. So that's my

334
00:23:39,325 --> 00:23:42,845
Speaker 4:  personal favorite. But the piece I wanted to talk the most about

335
00:23:43,265 --> 00:23:47,085
Speaker 4:  was the follower, the one one that matches social media posts with

336
00:23:47,085 --> 00:23:50,925
Speaker 4:  camera footage. Dre says that one day he and his

337
00:23:50,925 --> 00:23:54,565
Speaker 4:  friends were hanging out on a terrace in a touristy area in Portugal

338
00:23:55,025 --> 00:23:58,925
Speaker 4:  and they were watching people on the street take selfies and they

339
00:23:58,925 --> 00:24:02,245
Speaker 4:  thought, well we know where and when those photos are being taken,

340
00:24:03,185 --> 00:24:07,165
Speaker 4:  can we go find them on social media? They poked around for a while

341
00:24:07,265 --> 00:24:10,245
Speaker 4:  and came up empty, but it gave drees an idea

342
00:24:10,505 --> 00:24:14,445
Speaker 6:  And then actually I did the same but automated the whole thing. So

343
00:24:14,925 --> 00:24:18,845
Speaker 6:  I found cameras again, open cameras where

344
00:24:18,845 --> 00:24:22,725
Speaker 6:  people are taking a lot of photos and I saved videos

345
00:24:23,745 --> 00:24:26,525
Speaker 6:  for I think two or three weeks, something like that.

346
00:24:27,945 --> 00:24:31,485
Speaker 6:  And then at the same time I saved all the images

347
00:24:32,355 --> 00:24:36,325
Speaker 6:  from that location on Instagram. So all the

348
00:24:36,325 --> 00:24:39,445
Speaker 6:  photos that were tagged with the location

349
00:24:40,025 --> 00:24:43,995
Speaker 6:  around the camera was saved. And then the hardest part of

350
00:24:43,995 --> 00:24:47,915
Speaker 6:  course was finding that the, the people in the videos at

351
00:24:47,915 --> 00:24:48,635
Speaker 6:  the right moment,

352
00:24:49,295 --> 00:24:53,235
Speaker 4:  The system did its best to match faces between the camera footage and the

353
00:24:53,235 --> 00:24:57,075
Speaker 4:  Instagram photos. Dre says he went through and weed out false

354
00:24:57,435 --> 00:25:00,955
Speaker 4:  positives and then he cut the best examples together into a video

355
00:25:01,295 --> 00:25:03,995
Speaker 4:  and he posted it online. And what was the response to it?

356
00:25:04,475 --> 00:25:08,235
Speaker 6:  I mean, a lot of people were checking it out the day I, I launched

357
00:25:08,335 --> 00:25:10,795
Speaker 6:  it, people were really surprised

358
00:25:11,215 --> 00:25:15,075
Speaker 4:  It was a hit. And I get why it is this one, two

359
00:25:15,165 --> 00:25:19,155
Speaker 4:  punch of a reminder that anonymity is not what you

360
00:25:19,155 --> 00:25:22,515
Speaker 4:  think it is, especially if you're on social media, but

361
00:25:22,595 --> 00:25:25,595
Speaker 4:  increasingly if you're just alive in the world,

362
00:25:26,615 --> 00:25:30,515
Speaker 4:  the whole thing started a conversation that DRIs wasn't totally

363
00:25:30,875 --> 00:25:31,235
Speaker 4:  prepared for.

364
00:25:31,365 --> 00:25:35,355
Speaker 6:  There was also like a lot of false information. Like there was

365
00:25:35,435 --> 00:25:39,395
Speaker 6:  a video saying like, okay, it's an open source

366
00:25:39,545 --> 00:25:43,395
Speaker 6:  tool that works perfectly and you

367
00:25:43,395 --> 00:25:47,275
Speaker 6:  just need to upload an Instagram photo and it'll check all

368
00:25:47,275 --> 00:25:51,235
Speaker 6:  the cameras in the world. Yeah. That was really not, I mean

369
00:25:51,235 --> 00:25:55,075
Speaker 6:  that's not the tool I produced, right? It's only

370
00:25:55,105 --> 00:25:58,555
Speaker 6:  working with like specific

371
00:25:58,705 --> 00:25:59,195
Speaker 6:  cameras.

372
00:25:59,955 --> 00:26:03,675
Speaker 4:  Although if part of your goal is to make people think about what's possible,

373
00:26:04,895 --> 00:26:07,835
Speaker 4:  the version in their head that's bigger and more powerful

374
00:26:08,895 --> 00:26:12,795
Speaker 4:  is also possible technically. So in some senses do you feel like

375
00:26:13,155 --> 00:26:16,875
Speaker 4:  you're, you're making people think about the bigger scarier tool even if

376
00:26:16,875 --> 00:26:18,195
Speaker 4:  the one that you made is smaller?

377
00:26:18,705 --> 00:26:22,475
Speaker 6:  Yeah, and I think what is also important to to

378
00:26:22,915 --> 00:26:26,675
Speaker 6:  remember is that I am only one person

379
00:26:26,825 --> 00:26:29,995
Speaker 6:  producing this. I have limited

380
00:26:31,955 --> 00:26:35,435
Speaker 6:  resources. My skillset is just a basic one.

381
00:26:36,315 --> 00:26:40,235
Speaker 6:  I mean like, I'm not, it's not a team that built this. It's just like a

382
00:26:40,235 --> 00:26:42,155
Speaker 6:  bit of playing around I guess.

383
00:26:42,635 --> 00:26:46,085
Speaker 4:  I mean do, do you want people to think, well, alright, if this was just one

384
00:26:46,085 --> 00:26:49,725
Speaker 4:  person who built this, imagine what a whole team of people

385
00:26:50,025 --> 00:26:51,965
Speaker 4:  can do. Are those questions that you think about?

386
00:26:52,515 --> 00:26:56,045
Speaker 6:  Yeah, I think that was something that I saw a lot in the

387
00:26:56,285 --> 00:26:59,925
Speaker 6:  reaction. Like, okay, it's just only one

388
00:27:00,345 --> 00:27:03,005
Speaker 6:  person. Imagine what a government can do.

389
00:27:03,395 --> 00:27:06,165
Speaker 3:  Okay. Wait, will I have, I have just a really quick thought on this, which

390
00:27:06,165 --> 00:27:09,725
Speaker 3:  is there's this thing that Dres is doing where he's making all this sound

391
00:27:09,725 --> 00:27:12,925
Speaker 3:  really like theoretical and sort of, I just did a project. Yes. What if other

392
00:27:12,925 --> 00:27:16,645
Speaker 3:  people do this project, but like this stuff is happening, this is so like

393
00:27:16,955 --> 00:27:20,325
Speaker 3:  ingrained in the product of all of this stuff. I mean we spent all this time

394
00:27:20,325 --> 00:27:23,805
Speaker 3:  talking about Clearview ai, which was just a search engine of people's faces.

395
00:27:23,915 --> 00:27:27,805
Speaker 3:  Like that's, that is what he's talking about here. Like it's the same kind

396
00:27:27,805 --> 00:27:30,965
Speaker 3:  of thing and used for I would say largely

397
00:27:31,495 --> 00:27:35,365
Speaker 3:  terrifying purposes. We think about, like Neli on the show always

398
00:27:35,365 --> 00:27:39,125
Speaker 3:  talks all the time about how his favorite use case for AR is

399
00:27:39,185 --> 00:27:43,125
Speaker 3:  to have it identify people when he's walking past them. So

400
00:27:43,125 --> 00:27:45,565
Speaker 3:  it can be like, oh, who is this person that I met before? Like, that's the

401
00:27:45,565 --> 00:27:48,845
Speaker 3:  same thing, right? It's a, it's a less nefarious version of exactly the same

402
00:27:48,845 --> 00:27:52,405
Speaker 3:  thing, just reminding you of who a person is. It's like

403
00:27:52,625 --> 00:27:55,565
Speaker 3:  it, these are all two sides of the same coin, but this is the kind of stuff

404
00:27:55,565 --> 00:27:59,245
Speaker 3:  that like isn't just happening on, on

405
00:27:59,245 --> 00:28:03,045
Speaker 3:  Andres's computer to make us wonder about who we are in the world. This is

406
00:28:03,045 --> 00:28:06,205
Speaker 3:  like, this is the product in so many ways and

407
00:28:06,945 --> 00:28:10,925
Speaker 3:  he is right that I think he's at, at the like edge of it in

408
00:28:10,925 --> 00:28:14,805
Speaker 3:  certain ways, but also is like, dude is just building an app that

409
00:28:14,805 --> 00:28:16,325
Speaker 3:  other people are gonna want to use.

410
00:28:17,945 --> 00:28:21,765
Speaker 3:  Is he thinking about the, the, how this all feels to him? Like he's

411
00:28:21,885 --> 00:28:25,605
Speaker 3:  accidentally becoming like a pretty impressive tech developer. Somebody's

412
00:28:25,605 --> 00:28:29,405
Speaker 3:  gonna like want to download his apps on the app store here at some point.

413
00:28:29,405 --> 00:28:30,845
Speaker 3:  How does he feel about all that? Yeah,

414
00:28:30,875 --> 00:28:34,605
Speaker 4:  It's so this is what is so fascinating and kind of frustrating

415
00:28:34,825 --> 00:28:38,605
Speaker 4:  to talk to him is he's doing all of this incredibly relevant stuff with a

416
00:28:38,605 --> 00:28:40,925
Speaker 4:  lot of parallels to things that we talk about daily,

417
00:28:42,865 --> 00:28:46,765
Speaker 4:  but when you talk to him, he likes to talk exclusively

418
00:28:46,765 --> 00:28:50,645
Speaker 4:  about how other people respond to this stuff or what other people

419
00:28:51,035 --> 00:28:54,965
Speaker 4:  read into it. And this is where I feel like I completely

420
00:28:54,965 --> 00:28:58,205
Speaker 4:  failed as an interviewer. 'cause after an hour of talking to this guy, I

421
00:28:58,205 --> 00:29:01,605
Speaker 4:  couldn't get him to say why he's so interested in surveillance

422
00:29:02,025 --> 00:29:05,165
Speaker 4:  or what he finds so powerful,

423
00:29:05,775 --> 00:29:09,405
Speaker 4:  scary, interesting about these tools, what he wants

424
00:29:09,585 --> 00:29:13,445
Speaker 4:  to say about ai. That's just not where he's at as an

425
00:29:13,445 --> 00:29:13,645
Speaker 4:  artist.

426
00:29:14,525 --> 00:29:18,445
Speaker 6:  I think it's me exploring. I think the most important thing is, I

427
00:29:18,445 --> 00:29:21,925
Speaker 6:  don't want to put like a big message on the projects itself. Like

428
00:29:22,305 --> 00:29:26,085
Speaker 6:  you need to, to, to find, I know which

429
00:29:26,235 --> 00:29:29,165
Speaker 6:  kind of questions it raises, but

430
00:29:30,195 --> 00:29:32,445
Speaker 6:  yeah, I don't answer them.

431
00:29:32,955 --> 00:29:36,925
Speaker 4:  DRIs is so circumspect about his own intentions that he doesn't

432
00:29:36,925 --> 00:29:40,125
Speaker 4:  always seem to know how to handle the response to his work.

433
00:29:40,955 --> 00:29:44,725
Speaker 4:  Take for example, that project that shows Belgian politicians on their phones.

434
00:29:45,255 --> 00:29:48,125
Speaker 6:  There was a person from France,

435
00:29:49,305 --> 00:29:53,085
Speaker 6:  the person had a company and that person wanted to

436
00:29:53,345 --> 00:29:56,925
Speaker 6:  buy the software to, to use on their employees.

437
00:29:57,945 --> 00:29:58,485
Speaker 4:  Oh my gosh.

438
00:29:58,905 --> 00:30:02,445
Speaker 6:  And I was so surprised about this email. Like, I mean,

439
00:30:03,955 --> 00:30:07,925
Speaker 6:  just showing what is possible. But then there was a

440
00:30:07,925 --> 00:30:11,725
Speaker 6:  long email, I I, I think it was not a joke at first at thought,

441
00:30:11,725 --> 00:30:15,565
Speaker 6:  like this is a joke, but he had like a lot of cameras in the

442
00:30:15,565 --> 00:30:19,405
Speaker 6:  offices and it like, can I buy your software because I want to

443
00:30:19,905 --> 00:30:23,205
Speaker 6:  use this in my company. That was pretty crazy.

444
00:30:23,595 --> 00:30:25,325
Speaker 4:  Yeah. What did you tell him?

445
00:30:26,405 --> 00:30:29,965
Speaker 6:  I I, to be honest, I didn't reply on this email because

446
00:30:30,645 --> 00:30:32,245
Speaker 6:  I didn't know what to react.

447
00:30:32,605 --> 00:30:36,165
Speaker 4:  I I have to imagine that was not the,

448
00:30:36,625 --> 00:30:40,205
Speaker 4:  the idea that you wanted to give people necessarily or

449
00:30:40,465 --> 00:30:41,685
Speaker 4:  do you not think in those terms?

450
00:30:42,395 --> 00:30:42,965
Speaker 6:  Well, I

451
00:32:05,445 --> 00:32:09,045
Speaker 1:  your fingertips. Expand your world with

452
00:32:09,245 --> 00:32:10,005
Speaker 1:  meta ai.

453
00:33:01,305 --> 00:33:05,045
Speaker 4:  you. It just in the, in the sense that there are

454
00:33:05,115 --> 00:33:07,845
Speaker 4:  many, many tools being

455
00:33:08,675 --> 00:33:12,365
Speaker 4:  refined all the time. New, new AI related

456
00:33:12,415 --> 00:33:16,365
Speaker 4:  tools coming out, people talking about them, people interested in

457
00:33:16,365 --> 00:33:20,005
Speaker 4:  them, people scared of them. What tools are you interested in?

458
00:33:20,155 --> 00:33:21,765
Speaker 4:  What are you playing around with right now?

459
00:33:22,825 --> 00:33:26,765
Speaker 6:  I'm playing around with voice cloning a lot. I think that's really

460
00:33:27,125 --> 00:33:31,085
Speaker 6:  interesting. For the last couple of months, I'm exploring a lot of

461
00:33:31,515 --> 00:33:35,165
Speaker 6:  open source models that I can run

462
00:33:35,165 --> 00:33:38,805
Speaker 6:  locally. Sometimes they're better than commercial

463
00:33:38,855 --> 00:33:42,005
Speaker 6:  tools. You are way more able to

464
00:33:42,925 --> 00:33:46,735
Speaker 6:  configurate them in your style or something. You,

465
00:33:46,955 --> 00:33:50,855
Speaker 6:  you have more parameters. That's something I'm exploring

466
00:33:51,125 --> 00:33:54,615
Speaker 4:  Most of his work with generative AI is still in the tinkering stage

467
00:33:55,235 --> 00:33:58,135
Speaker 4:  though. He posted one very funny demo to his site.

468
00:33:58,815 --> 00:34:02,695
Speaker 6:  I did a project named the selfie

469
00:34:02,695 --> 00:34:06,575
Speaker 6:  coach. I, I don't want to call it a project, I don't see

470
00:34:06,575 --> 00:34:10,055
Speaker 6:  it as a final project. It's more like an experiment

471
00:34:11,115 --> 00:34:14,455
Speaker 6:  and it's a tool that I developed

472
00:34:14,785 --> 00:34:17,735
Speaker 6:  where You can take selfies

473
00:34:18,755 --> 00:34:21,815
Speaker 6:  but it'll give you advice in what to change

474
00:34:22,835 --> 00:34:26,575
Speaker 6:  in the photo. So it takes a photo and then

475
00:34:26,795 --> 00:34:30,535
Speaker 6:  you are smiling and then you get advice from Kylie

476
00:34:30,715 --> 00:34:32,575
Speaker 6:  Jenner with her voice.

477
00:34:32,725 --> 00:34:36,535
Speaker 7:  Alright babe, let's get another angle. Okay, ready? 3, 2, 1

478
00:34:36,775 --> 00:34:36,895
Speaker 7:  pose.

479
00:34:37,685 --> 00:34:41,615
Speaker 4:  DRIs posted a video of himself testing this out. He poses in front of

480
00:34:41,615 --> 00:34:45,495
Speaker 4:  his webcam wearing big headphones and looking out of his window. The

481
00:34:45,495 --> 00:34:49,055
Speaker 4:  camera snaps a shot. There's a short pause and then

482
00:34:49,505 --> 00:34:51,455
Speaker 4:  robot Kylie Jenner does her thing.

483
00:34:51,845 --> 00:34:55,535
Speaker 7:  Okay, love the candid vibe, but let's add some drama, turn towards the light,

484
00:34:55,565 --> 00:34:59,175
Speaker 7:  lose the headphones and think mysterious thoughts to spice it up. 'cause

485
00:34:59,255 --> 00:35:00,255
Speaker 7:  lighting is everything babe.

486
00:35:00,595 --> 00:35:03,895
Speaker 6:  It uses Chachi PT and a voice cloning service.

487
00:35:04,395 --> 00:35:07,935
Speaker 6:  So first it takes a photo and then

488
00:35:08,715 --> 00:35:12,455
Speaker 6:  the prompt is you are a selfie coach and

489
00:35:12,915 --> 00:35:14,895
Speaker 6:  you are also Kylie Jenner

490
00:35:18,155 --> 00:35:22,055
Speaker 6:  and say, what I need to change in the,

491
00:35:22,475 --> 00:35:26,375
Speaker 6:  the photo in a really funny way. And then

492
00:35:26,535 --> 00:35:30,495
Speaker 6:  I have text and then it sends it to this voice cloning service.

493
00:35:31,495 --> 00:35:35,335
Speaker 6:  I trained it on I think half an hour of

494
00:35:35,335 --> 00:35:38,965
Speaker 6:  voice. So you give the system like half an hour

495
00:35:39,465 --> 00:35:43,245
Speaker 6:  of that person talking and then

496
00:35:43,355 --> 00:35:46,725
Speaker 6:  it's able to produce any

497
00:35:47,165 --> 00:35:50,965
Speaker 6:  sentence in audio with, with that person's

498
00:35:50,965 --> 00:35:53,725
Speaker 6:  voice. It's quite spectacular. Add

499
00:35:53,725 --> 00:35:57,285
Speaker 7:  A pop of color by holding some bright fashion accessories. It'll draw more

500
00:35:57,285 --> 00:35:57,845
Speaker 7:  eyes babe.

501
00:35:58,635 --> 00:36:02,485
Speaker 6:  Just me exploring a bit how those tools work

502
00:36:03,665 --> 00:36:07,645
Speaker 6:  and it's quite fun. Also, the intonation is really good and

503
00:36:08,835 --> 00:36:12,805
Speaker 6:  also the way she talks is it's in there. For example, she

504
00:36:12,805 --> 00:36:16,525
Speaker 6:  is using a lot vape, so sometimes Chachi PT

505
00:36:16,525 --> 00:36:17,485
Speaker 6:  reacts with like, ba,

506
00:36:17,655 --> 00:36:19,565
Speaker 7:  Let's snap another one. You're gonna kill it.

507
00:36:20,355 --> 00:36:24,005
Speaker 4:  Next up Dre says he's working on some kind of generative

508
00:36:24,005 --> 00:36:27,845
Speaker 4:  dating app. Knowing him Lord only knows what that'll turn into.

509
00:36:28,545 --> 00:36:32,405
Speaker 4:  But beyond that, he's pretty coy. Do you think that the fundamentals

510
00:36:32,475 --> 00:36:35,445
Speaker 4:  here, the voice cloning tools will lead to some

511
00:36:36,225 --> 00:36:38,455
Speaker 4:  bigger project?

512
00:36:40,475 --> 00:36:44,455
Speaker 6:  Not, I don't have any good ideas at this moment with voice cloning.

513
00:36:44,565 --> 00:36:48,495
Speaker 6:  It's me again, exploring a bit, and

514
00:36:48,495 --> 00:36:50,175
Speaker 6:  this is something I put online.

515
00:36:50,755 --> 00:36:54,575
Speaker 4:  So you're waiting, you've got lots of bad ideas and you're waiting to combine

516
00:36:54,575 --> 00:36:57,175
Speaker 4:  them into some, some good ideas with voice cloning.

517
00:36:57,955 --> 00:36:58,775
Speaker 6:  Yes, exactly.

518
00:36:59,705 --> 00:37:03,495
Speaker 4:  Dre's work is undeniably relevant right now and it's

519
00:37:03,495 --> 00:37:06,895
Speaker 4:  showing there are lots more AI themed exhibitions popping up.

520
00:37:07,405 --> 00:37:11,135
Speaker 4:  He's asked to give a lot more talks these days. His ideas are just

521
00:37:11,135 --> 00:37:15,055
Speaker 4:  generally around more. And I get it, I watch

522
00:37:15,235 --> 00:37:19,215
Speaker 4:  as each new AI service blows up the internet and I

523
00:37:19,365 --> 00:37:22,775
Speaker 4:  fret about what it's gonna do to my work, my identity.

524
00:37:23,715 --> 00:37:27,335
Speaker 4:  and more than anything, I want someone who's not Sam

525
00:37:27,655 --> 00:37:31,215
Speaker 4:  Altman to tell me what to think about all these hairy questions.

526
00:37:32,095 --> 00:37:35,775
Speaker 4:  I probably shouldn't admit that as a tech reporter, but it's so true.

527
00:37:37,145 --> 00:37:41,015
Speaker 4:  Sadly, for Meris is emphatically not here to

528
00:37:41,015 --> 00:37:44,735
Speaker 4:  clear anything up. As you do more talks and do

529
00:37:44,735 --> 00:37:48,255
Speaker 4:  more exhibitions, are lots of people coming to you and saying,

530
00:37:49,125 --> 00:37:51,535
Speaker 4:  what do you think? What? How should I feel about all this?

531
00:37:52,205 --> 00:37:56,055
Speaker 6:  Yeah, sometimes after a talk there's like the small q

532
00:37:56,055 --> 00:37:59,775
Speaker 6:  and a and that's one of the most asked questions because after

533
00:37:59,975 --> 00:38:03,695
Speaker 6:  a talk, I leave this open. Like I, I, yeah, here are my

534
00:38:03,695 --> 00:38:07,175
Speaker 6:  projects. I explain them all. I do live experiments

535
00:38:08,235 --> 00:38:11,655
Speaker 6:  on stage. I make sure it's like entertaining and fun,

536
00:38:12,475 --> 00:38:16,215
Speaker 6:  but then they're like, Hey, but what's, what's your message? And

537
00:38:16,315 --> 00:38:20,095
Speaker 6:  I'm like, okay, I, there's really no message at all.

538
00:38:20,415 --> 00:38:22,735
Speaker 6:  I don't have any clue why I am interested in this.

539
00:38:24,015 --> 00:38:27,775
Speaker 4:  I honestly can't tell if I buy that or not. It's

540
00:38:28,015 --> 00:38:31,975
Speaker 4:  possible that Dres is focused on finding unique raw material like

541
00:38:32,075 --> 00:38:35,365
Speaker 4:  webcam footage and dreaming up clever ways to look at it.

542
00:38:36,025 --> 00:38:39,405
Speaker 4:  All the inevitable questions around privacy and anonymity.

543
00:38:39,985 --> 00:38:43,645
Speaker 4:  He might have just stumbled into those, but I think

544
00:38:43,645 --> 00:38:47,445
Speaker 4:  there's a clue to be found in who he's choosing to watch

545
00:38:48,195 --> 00:38:51,685
Speaker 4:  celebrities, politicians, social media users,

546
00:38:52,185 --> 00:38:56,045
Speaker 4:  the pettiest possible scoff laws. That's just too

547
00:38:56,445 --> 00:39:00,285
Speaker 4:  interesting and provocative. A spread of people, at least

548
00:39:00,285 --> 00:39:03,885
Speaker 4:  that's where my brain goes. The AI tools he's playing with,

549
00:39:03,995 --> 00:39:07,965
Speaker 4:  they're powerful but agnostic on their own. They need a subject

550
00:39:08,385 --> 00:39:12,285
Speaker 4:  or a target and training them to keep politicians

551
00:39:12,285 --> 00:39:16,205
Speaker 4:  honest versus prosecute Jay Walkers versus tail famous

552
00:39:16,205 --> 00:39:19,885
Speaker 4:  people. Each example changes my emotional

553
00:39:20,245 --> 00:39:24,085
Speaker 4:  response to the whole concept of ai. Not just the one example of it,

554
00:39:24,695 --> 00:39:27,845
Speaker 4:  which makes the body of his work greater than the sum of its parts.

555
00:39:29,105 --> 00:39:32,805
Speaker 4:  But again, that's just me. You're not gonna get

556
00:39:33,125 --> 00:39:37,045
Speaker 4:  anything like that from DRIs. I hope he forgives me for this

557
00:39:37,245 --> 00:39:40,805
Speaker 4:  metaphor, but he's like a crow that leaves a weird

558
00:39:40,975 --> 00:39:44,485
Speaker 4:  shiny trinket at your doorstep. Is it a gift,

559
00:39:45,285 --> 00:39:48,645
Speaker 4:  a challenge, a threat? He just

560
00:39:48,935 --> 00:39:52,885
Speaker 4:  drops these funny and slightly upsetting tech mysteries at

561
00:39:52,885 --> 00:39:56,205
Speaker 4:  our feet and the only note he ever leaves is,

562
00:39:56,715 --> 00:39:57,605
Speaker 4:  look what You can do.

563
00:39:59,145 --> 00:40:00,245
Speaker 4:  The rest is up to us.

564
00:40:08,475 --> 00:40:12,375
Speaker 3:  All right, that is it for The Vergecast today. Thanks to Will Andres for

565
00:40:12,375 --> 00:40:15,055
Speaker 3:  putting this whole story together. And thank you as always for listening.

566
00:40:15,285 --> 00:40:19,175
Speaker 3:  There's lots more on all of the AI stuff we've been talking about at The

567
00:40:19,255 --> 00:40:22,935
Speaker 3:  Verge dot com. It turns out kind of a newsy week in the

568
00:40:23,075 --> 00:40:26,655
Speaker 3:  AI universe, like seemingly every week. So we'll put something in the show

569
00:40:26,655 --> 00:40:30,575
Speaker 3:  notes, but as always, go read the website. We'll have some links to Dre's

570
00:40:30,575 --> 00:40:34,015
Speaker 3:  work too if you wanna go check it out. As always, if you have thoughts, questions,

571
00:40:34,075 --> 00:40:38,015
Speaker 3:  or feelings or AI art projects you'd like us to check out or

572
00:40:38,295 --> 00:40:40,815
Speaker 3:  surveillance footage of me in Times Square, you'd like me to see, You can

573
00:40:40,815 --> 00:40:41,815
Speaker 3:  always email us Vergecast

