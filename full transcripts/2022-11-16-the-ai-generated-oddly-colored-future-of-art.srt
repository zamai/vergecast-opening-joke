1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 480b4ca0-68db-11ed-bc19-f11eee35a173
Status: Done
Stage: Done
Title: The AI-generated, oddly colored future of art
Audio URL: https://jfe93e.s3.amazonaws.com/-5334973589042054592/6244297833171967696/s93290-US-3775s-1668952576.mp3
Description: Today on the flagship podcast of the difference between CMYK and RGB colors:

02:19 - David talks about the future of Photoshop with Adobe's Chief Product Officer Scott Belsky.


13:37 - Verge senior reporter James Vincent joins the show to discuss generative AI art and all its possibilities and complications.


43:05 - The Verge's Kristen Radtke and Jess Weatherbed chat with David about Pantone's new subscription service and what it means for artists and designers.


Email us at vergecast@theverge.com or call us at 866-VERGE11, we'd love to hear from you.
We are conducting a short audience survey to help plan for our future and hear from you. To participate, head toÂ vox.com/podsurvey, and thank you!
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (6 ads detected)

2
00:00:02,690 --> 00:00:06,160
Speaker 2:  Welcome to the Verge Cast, the flagship podcast of the difference between

3
00:00:06,160 --> 00:00:09,920
Speaker 2:  C M Y K and RGB Colors. I'm your friend David Pierce and I am at

4
00:00:09,920 --> 00:00:13,080
Speaker 2:  Ace Hardware Shopping for Paint, which is both very fitting for the episode

5
00:00:13,080 --> 00:00:16,760
Speaker 2:  we're about to do and also just a life necessity because apparently the fact

6
00:00:16,760 --> 00:00:20,570
Speaker 2:  that I'm having a baby soon means I am required to repaint every single surface

7
00:00:20,570 --> 00:00:24,550
Speaker 2:  in my house. So I'm here trying to figure out the difference between

8
00:00:24,550 --> 00:00:28,320
Speaker 2:  Maui Mist and Moonlit Snow and Austrian

9
00:00:28,320 --> 00:00:32,200
Speaker 2:  Ice, which are apparently all different colors despite looking absolutely

10
00:00:32,200 --> 00:00:35,720
Speaker 2:  exactly the same to me. Anyway, we have a super fun show coming up for you

11
00:00:35,720 --> 00:00:39,080
Speaker 2:  today, all about the future of art. We're gonna talk about Photoshop and

12
00:00:39,080 --> 00:00:42,360
Speaker 2:  what it means to make a 30 year old piece of software work for the next 30

13
00:00:42,360 --> 00:00:46,100
Speaker 2:  years. We're also gonna talk about AI art, things like Dolly and Mid Journey

14
00:00:46,100 --> 00:00:49,880
Speaker 2:  and all the possibilities and complications that those bring up. And then

15
00:00:49,880 --> 00:00:53,200
Speaker 2:  we're gonna talk about a strange beef in the world of colors and how it came

16
00:00:53,200 --> 00:00:56,840
Speaker 2:  to be that Pantone is the authority on what colors actually

17
00:00:56,840 --> 00:01:00,600
Speaker 2:  are. It's crazy world out there. All that is coming up right after the break.

18
00:01:00,600 --> 00:01:04,380
Speaker 2:  But first, I need to actually like pick a paint color and buy something.

19
00:01:04,380 --> 00:01:08,360
Speaker 2:  I'm leaning toward Arctic Dawn personally, but also I have literally

20
00:01:08,360 --> 00:01:11,320
Speaker 2:  no idea what I'm talking about. Here goes nothing. This is a verge cast scene

21
00:01:11,320 --> 00:01:14,980
Speaker 2:  in a sec.

22
00:02:19,740 --> 00:02:23,530
Speaker 2:  Welcome back. As I said a minute ago, we're gonna spend a lot of this

23
00:02:23,530 --> 00:02:27,490
Speaker 2:  episode talking about digital art because in a world where everyone's a

24
00:02:27,490 --> 00:02:31,170
Speaker 2:  creator, most of our life happens on screens and

25
00:02:31,170 --> 00:02:35,010
Speaker 2:  we're all buying and selling JPEGs for millions of dollars. It turns

26
00:02:35,010 --> 00:02:38,880
Speaker 2:  out art is pretty important and you really, really,

27
00:02:38,880 --> 00:02:42,530
Speaker 2:  really cannot talk about digital art without talking about Adobe's

28
00:02:42,530 --> 00:02:46,370
Speaker 2:  Photoshop. Photoshop is to art

29
00:02:46,370 --> 00:02:49,850
Speaker 2:  what Google is to search or what Kleenex is to facial

30
00:02:49,850 --> 00:02:53,810
Speaker 2:  tissues. What I don't know, Mario, is to games where you jump

31
00:02:53,810 --> 00:02:57,090
Speaker 2:  on top of things that look like turtles. I'm getting away from the point

32
00:02:57,090 --> 00:03:00,640
Speaker 2:  here. The point is Photoshop is a mission critical,

33
00:03:00,640 --> 00:03:04,360
Speaker 2:  essential tool for artists around the world and it has been for

34
00:03:04,360 --> 00:03:08,120
Speaker 2:  decades. You know what's wild by the way, I went back

35
00:03:08,120 --> 00:03:11,880
Speaker 2:  and looked at some fun videos of Photoshop 1.0 which was

36
00:03:11,880 --> 00:03:15,760
Speaker 2:  released in 19 90, 32 years ago and it's kind of incredible

37
00:03:15,760 --> 00:03:19,240
Speaker 2:  how much has stayed the same. There was even this really fun video I found

38
00:03:19,240 --> 00:03:22,720
Speaker 2:  from a couple of years ago where a graphic designer named Will Patterson

39
00:03:22,720 --> 00:03:24,990
Speaker 2:  tried to use Photoshop 1.0,

40
00:03:24,990 --> 00:03:27,360
Speaker 6:  This is weird playing with

41
00:03:27,360 --> 00:03:31,320
Speaker 6:  Photoshop on here, right? Let's just get on

42
00:03:31,320 --> 00:03:32,200
Speaker 6:  with the design anyway,

43
00:03:32,200 --> 00:03:33,280
Speaker 2:  And he made it work.

44
00:03:33,280 --> 00:03:37,160
Speaker 6:  There we go. We got our first design in Photoshop One. It doesn't look

45
00:03:37,160 --> 00:03:37,840
Speaker 6:  great. I mean,

46
00:03:37,840 --> 00:03:40,920
Speaker 2:  Looks like it's legitimately impressive and it's good evidence that Adobe

47
00:03:40,920 --> 00:03:44,880
Speaker 2:  got a lot of things right even really early on. But in a way it's

48
00:03:44,880 --> 00:03:48,640
Speaker 2:  also kind of a bad sign. Technology changes, users

49
00:03:48,640 --> 00:03:52,400
Speaker 2:  change the things that we want and can do with technology

50
00:03:52,400 --> 00:03:56,360
Speaker 2:  change and apps that don't change along with all of that tend to

51
00:03:56,360 --> 00:04:00,200
Speaker 2:  turn out like Microsoft Word or Lotus Notes and they

52
00:04:00,200 --> 00:04:03,570
Speaker 2:  eventually just kind of go by the wayside as a new generation of tools

53
00:04:03,570 --> 00:04:07,160
Speaker 2:  emerges. It's this eternal struggle for a lot of successful

54
00:04:07,160 --> 00:04:10,960
Speaker 2:  products. How do you make an app super successful and then change it to

55
00:04:10,960 --> 00:04:14,880
Speaker 2:  appeal to new users without irritating the existing ones? If you were

56
00:04:14,880 --> 00:04:18,520
Speaker 2:  to start Photoshop from scratch right now, it probably wouldn't

57
00:04:18,520 --> 00:04:22,280
Speaker 2:  have a thousand menus and all those icons that haven't

58
00:04:22,280 --> 00:04:26,160
Speaker 2:  changed in more than three decades. But if you made those changes now you'd

59
00:04:26,160 --> 00:04:30,080
Speaker 2:  piss off millions of people. Scott Beski is the

60
00:04:30,080 --> 00:04:32,960
Speaker 2:  person charged with threading this very careful needle.

61
00:04:32,960 --> 00:04:36,560
Speaker 7:  Scott Beski, chief product officer of receiving creative products at

62
00:04:36,560 --> 00:04:37,280
Speaker 7:  Adobe.

63
00:04:37,280 --> 00:04:41,160
Speaker 2:  Scott told me he knows very acutely how important it

64
00:04:41,160 --> 00:04:44,860
Speaker 2:  is to not screw up Photoshop, which is good. You don't wanna screw up Photoshop

65
00:04:44,860 --> 00:04:48,400
Speaker 2:  and the way Adobe has avoided doing it so far is pretty

66
00:04:48,400 --> 00:04:52,260
Speaker 2:  simple. It just does what customers want, which is mostly,

67
00:04:52,260 --> 00:04:56,160
Speaker 2:  you know, don't touch anything. People want more features, but they

68
00:04:56,160 --> 00:05:00,120
Speaker 2:  also really, really, really don't want to have to change the way that

69
00:05:00,120 --> 00:05:01,800
Speaker 2:  they use a tool for no good reason.

70
00:05:01,800 --> 00:05:05,490
Speaker 7:  Photoshop has one of the greatest product market fits ever.

71
00:05:05,490 --> 00:05:09,400
Speaker 7:  It is a name brand that is also in some ways a verb. And

72
00:05:09,400 --> 00:05:13,240
Speaker 7:  when you talk to a customer who loves Photoshop, what do they say

73
00:05:13,240 --> 00:05:16,960
Speaker 7:  first? Don't change anything. Like don't screw with it. Like don't

74
00:05:16,960 --> 00:05:20,880
Speaker 7:  redesign the interface or whatever. Now, of course, like as a modern

75
00:05:20,880 --> 00:05:24,640
Speaker 7:  product builder, I look at the interface and the way that it's been

76
00:05:24,640 --> 00:05:28,520
Speaker 7:  built. You know, there's 30 years of decisions that

77
00:05:28,520 --> 00:05:32,160
Speaker 7:  have had to accumulate and you know, it's almost like the difference between

78
00:05:32,160 --> 00:05:35,720
Speaker 7:  going to a European city that has been built over hundreds

79
00:05:35,720 --> 00:05:39,120
Speaker 7:  and thousands of years of architecture and then you go to like a

80
00:05:39,120 --> 00:05:42,800
Speaker 7:  modern American city that was dope from a desert or

81
00:05:42,800 --> 00:05:45,100
Speaker 7:  something. It's just completely different.

82
00:05:45,100 --> 00:05:48,600
Speaker 2:  The more I think about that city's analogy, the more I like it

83
00:05:48,600 --> 00:05:52,280
Speaker 2:  because like those old cities are amazing, right? Nobody

84
00:05:52,280 --> 00:05:55,840
Speaker 2:  thinks about it and goes, ah, Venice versus Phoenix, give me Phoenix all

85
00:05:55,840 --> 00:05:59,480
Speaker 2:  day. But if your, your goal is just to get where you're going, I

86
00:05:59,480 --> 00:06:03,400
Speaker 2:  suspect a more modern city helps. There are no obvious answers

87
00:06:03,400 --> 00:06:07,240
Speaker 2:  here, especially as people keep wanting more and more of these new modern

88
00:06:07,240 --> 00:06:10,720
Speaker 2:  things. But that's where it gets tricky, right? You give people everything

89
00:06:10,720 --> 00:06:14,360
Speaker 2:  they want by just sort of shoving new features into Photoshop. You

90
00:06:14,360 --> 00:06:17,880
Speaker 2:  eventually build an app that is big and powerful and

91
00:06:17,880 --> 00:06:21,640
Speaker 2:  borderline unusable. It's like if you had a city that was both

92
00:06:21,640 --> 00:06:25,380
Speaker 2:  Phoenix and Venice, I'm not sure you'd have a super good time there.

93
00:06:25,380 --> 00:06:28,840
Speaker 2:  And I think a lot of designers would argue that Photoshop has been teetering

94
00:06:28,840 --> 00:06:32,520
Speaker 2:  on that line for a long time. One way Adobe has tried to

95
00:06:32,520 --> 00:06:36,160
Speaker 2:  combat this is by splitting off apps like Lightroom when it became obvious

96
00:06:36,160 --> 00:06:39,860
Speaker 2:  that a huge number of people really needed a way to manage their photos.

97
00:06:39,860 --> 00:06:43,480
Speaker 2:  But doing that in the confines of Photoshop, along with

98
00:06:43,480 --> 00:06:47,240
Speaker 2:  everything else Photoshop can do, didn't really make sense. So Lightroom

99
00:06:47,240 --> 00:06:50,760
Speaker 2:  became a thing. More recently, Scott told me the rise of social media

100
00:06:50,760 --> 00:06:53,210
Speaker 2:  creators has gone kind of the same way.

101
00:06:53,210 --> 00:06:57,080
Speaker 7:  An observation we've made over the last 10 years or so in Photoshop is

102
00:06:57,080 --> 00:07:00,600
Speaker 7:  that there were so many customers coming in creating things for social

103
00:07:00,600 --> 00:07:04,350
Speaker 7:  media and you know, and the subset of features that they were asking for

104
00:07:04,350 --> 00:07:08,220
Speaker 7:  were easier interfaces, ways of making for mixed media.

105
00:07:08,220 --> 00:07:11,940
Speaker 7:  Cuz most platforms, every social platform these days is mixed media.

106
00:07:11,940 --> 00:07:15,440
Speaker 7:  You come in thinking you're making a graphic and then you realize you want

107
00:07:15,440 --> 00:07:18,520
Speaker 7:  to add some animation and some text and maybe a little motion and then you

108
00:07:18,520 --> 00:07:22,030
Speaker 7:  put a time on, you throw in some music and before you know it, it's like

109
00:07:22,030 --> 00:07:26,000
Speaker 7:  a fully featured mixed media creation. So we realized that

110
00:07:26,000 --> 00:07:29,240
Speaker 7:  Photoshop is not the right tool for that and that actually was one of the

111
00:07:29,240 --> 00:07:32,520
Speaker 7:  reasons why we embarked on Express and the previous

112
00:07:32,520 --> 00:07:36,360
Speaker 7:  incarnations of Express because we recognized that we needed to build something

113
00:07:36,360 --> 00:07:39,920
Speaker 7:  specifically for the social media creator that probably had a lower

114
00:07:39,920 --> 00:07:43,760
Speaker 7:  willingness to pay, had a lower tolerance for a learning curve, and wanted

115
00:07:43,760 --> 00:07:47,480
Speaker 7:  mixed media creation. And so that's a story repeating itself all over

116
00:07:47,480 --> 00:07:48,190
Speaker 7:  again.

117
00:07:48,190 --> 00:07:52,160
Speaker 2:  Ultimately the business model here is pretty clear, right? What Adobe

118
00:07:52,160 --> 00:07:56,120
Speaker 2:  really wants to do is introduce you to its suite through tools like Express

119
00:07:56,120 --> 00:07:59,760
Speaker 2:  and Figma and then slowly upsell you all the way to

120
00:07:59,760 --> 00:08:03,200
Speaker 2:  Photoshop and the whole creative suite. But one thing Scott said to me a

121
00:08:03,200 --> 00:08:07,120
Speaker 2:  few times was that Adobe has to let Photoshop be Photoshop, but

122
00:08:07,120 --> 00:08:10,520
Speaker 2:  even that means something different than it used to. Adobe is now bringing

123
00:08:10,520 --> 00:08:14,360
Speaker 2:  Photoshop to the web, it's embedding some of Photoshop's functionality

124
00:08:14,360 --> 00:08:17,920
Speaker 2:  inside of other apps and it's thinking about what collaboration

125
00:08:17,920 --> 00:08:21,880
Speaker 2:  looks like. Again, technology changes, users change and Adobe has to

126
00:08:21,880 --> 00:08:25,150
Speaker 2:  figure out how to keep up while letting Photoshop be Photoshop.

127
00:08:25,150 --> 00:08:28,680
Speaker 2:  Thankfully it doesn't sound like Scott wants to do the whole

128
00:08:28,680 --> 00:08:32,400
Speaker 2:  let's all edit a photo together thing, which honestly sounds like my

129
00:08:32,400 --> 00:08:32,870
Speaker 2:  nightmare.

130
00:08:32,870 --> 00:08:36,760
Speaker 7:  Yeah, I've seen very little evidence of customers saying, I want to have

131
00:08:36,760 --> 00:08:40,520
Speaker 7:  three people editing my image at the same time. So I don't think that that's

132
00:08:40,520 --> 00:08:44,120
Speaker 7:  where we're gonna take Photoshop. What we want to do with Photoshop is we

133
00:08:44,120 --> 00:08:47,840
Speaker 7:  want it to be easy to work with every stakeholder, you know,

134
00:08:47,840 --> 00:08:51,750
Speaker 7:  every single creative of our products. Unless you're an artist working

135
00:08:51,750 --> 00:08:55,440
Speaker 7:  solely for yourself and you're never gonna share your creations. Everyone

136
00:08:55,440 --> 00:08:59,240
Speaker 7:  has stakeholders, copy editors, clients, marketers, you know,

137
00:08:59,240 --> 00:09:03,200
Speaker 7:  how do you name it? And currently the workflows to engage with those

138
00:09:03,200 --> 00:09:05,670
Speaker 7:  stakeholders are extraordinarily antiquated.

139
00:09:05,670 --> 00:09:09,600
Speaker 2:  That basically means making it easier to show stuff, easier to share stuff,

140
00:09:09,600 --> 00:09:13,320
Speaker 2:  easier to leave comments on stuff, which fair enough, I can get

141
00:09:13,320 --> 00:09:17,040
Speaker 2:  behind that. Anything that means fewer email attachments is a huge win

142
00:09:17,040 --> 00:09:20,720
Speaker 2:  in my book. To me, the biggest question for the future of Photoshop

143
00:09:20,720 --> 00:09:24,640
Speaker 2:  comes from AI advances in AI have made all kinds of

144
00:09:24,640 --> 00:09:27,950
Speaker 2:  image editing, the stuff people used to do professionally in Photoshop,

145
00:09:27,950 --> 00:09:31,680
Speaker 2:  suddenly easy, even automatic. Think of the Magic

146
00:09:31,680 --> 00:09:33,180
Speaker 2:  eraser on the pixel phones.

147
00:09:33,180 --> 00:09:36,560
Speaker 8:  All you do is outline what you don't want in the picture and it's

148
00:09:36,560 --> 00:09:40,440
Speaker 2:  Gone. Or the iPhone's action mode, which automatically tries to steady

149
00:09:40,440 --> 00:09:43,640
Speaker 2:  your video even as you run and jump and move around.

150
00:09:43,640 --> 00:09:46,840
Speaker 9:  Normally these shots would only be possible with a ton of stabilizing equipment,

151
00:09:46,840 --> 00:09:49,160
Speaker 9:  but with action mode, we didn't need any of that.

152
00:09:49,160 --> 00:09:52,800
Speaker 2:  Photoshop has introduced a bunch of automatic filters recently.

153
00:09:52,800 --> 00:09:56,720
Speaker 2:  It's trying to help people make their stuff better without requiring

154
00:09:56,720 --> 00:10:00,640
Speaker 2:  a huge amount of repetitive work. And if you've ever spent what feels

155
00:10:00,640 --> 00:10:04,560
Speaker 2:  like a thousand hours carefully masking around a part of your

156
00:10:04,560 --> 00:10:08,360
Speaker 2:  image just so you can edit it, it's definitely good news that you can

157
00:10:08,360 --> 00:10:12,120
Speaker 2:  now just like click on the tennis ball you're trying to remove and poof it's

158
00:10:12,120 --> 00:10:12,920
Speaker 2:  gone. You

159
00:10:12,920 --> 00:10:16,760
Speaker 7:  Can immaculately apply color to a black and white photograph or you can

160
00:10:16,760 --> 00:10:20,400
Speaker 7:  just touch the button and it makes a color. I mean, why would you not do

161
00:10:20,400 --> 00:10:22,770
Speaker 7:  that? And then make final edits and changes.

162
00:10:22,770 --> 00:10:26,440
Speaker 2:  Adobe's also starting to really lean into generative ai, the tools that you

163
00:10:26,440 --> 00:10:30,160
Speaker 2:  can use to not just edit your photos in art but actually create stuff

164
00:10:30,160 --> 00:10:33,680
Speaker 2:  from scratch. What if the future of Photoshop is just like a

165
00:10:33,680 --> 00:10:37,600
Speaker 2:  text box? You type what you want, the AI creates it and then

166
00:10:37,600 --> 00:10:41,480
Speaker 2:  you apply the edits you want it. The whole text box thing is

167
00:10:41,480 --> 00:10:45,160
Speaker 2:  probably a bit much, but that idea I think is actually where Scott and Adobe

168
00:10:45,160 --> 00:10:46,070
Speaker 2:  are headed.

169
00:10:46,070 --> 00:10:49,200
Speaker 7:  I think that's right and I think if you look at our fu our tools into the

170
00:10:49,200 --> 00:10:52,840
Speaker 7:  future, they will all be AI assisted tools. But at the end of the day,

171
00:10:52,840 --> 00:10:56,560
Speaker 7:  the creative control resides with the person and the AI is really

172
00:10:56,560 --> 00:11:00,240
Speaker 7:  their co-pilot. It's working for them layer by layer by layer or

173
00:11:00,240 --> 00:11:04,080
Speaker 7:  suggestion by suggestion and the creatives will either take it or leave it,

174
00:11:04,080 --> 00:11:06,390
Speaker 7:  it'll make them feel more productive.

175
00:11:06,390 --> 00:11:10,160
Speaker 2:  This sounds right to me in a lot of ways and also sounds messy and

176
00:11:10,160 --> 00:11:12,920
Speaker 2:  complicated in a thousand ways we haven't even gotten into

177
00:11:12,920 --> 00:11:16,400
Speaker 2:  yet. Generative AI art,

178
00:11:16,400 --> 00:11:20,360
Speaker 2:  generative AI in general is gonna change things in bigger ways

179
00:11:20,360 --> 00:11:23,960
Speaker 2:  than I think we even realize. So actually let's leave Scott and

180
00:11:23,960 --> 00:11:27,720
Speaker 2:  Photoshop and Adobe for now and let's get into that. We're gonna take a quick

181
00:11:27,720 --> 00:11:31,640
Speaker 2:  break and then we're gonna dive into the weeds of all things generative ai.

182
00:11:31,640 --> 00:11:32,360
Speaker 2:  We'll be right

183
00:11:32,360 --> 00:11:38,870
Speaker 2:  back.

184
00:13:39,840 --> 00:13:43,790
Speaker 2:  Welcome back. Like we were just talking about, generative AI is

185
00:13:43,790 --> 00:13:47,750
Speaker 2:  kind of having a moment. Everyone's out there using Dolly

186
00:13:47,750 --> 00:13:51,630
Speaker 2:  to make weird pictures of like the moon made of spaghetti

187
00:13:51,630 --> 00:13:55,590
Speaker 2:  painted in the style of Andy Warhol or whatever. And

188
00:13:55,590 --> 00:13:59,270
Speaker 2:  Stable diffusion and Mid Journey and a bunch of other companies and

189
00:13:59,270 --> 00:14:03,070
Speaker 2:  organizations are building generative tools of their own. It's

190
00:14:03,070 --> 00:14:06,640
Speaker 2:  weird and it's messy, but it's really interesting stuff happening out there.

191
00:14:06,640 --> 00:14:10,470
Speaker 2:  So is the future of art just writing words in an

192
00:14:10,470 --> 00:14:14,370
Speaker 2:  AI prompt? And if so, how does that change the way that we think about

193
00:14:14,370 --> 00:14:18,230
Speaker 2:  art and artists? This is all fascinating and it

194
00:14:18,230 --> 00:14:22,177
Speaker 2:  gets really complicated really fast. So I asked James Vincent

195
00:14:22,177 --> 00:14:25,950
Speaker 2:  the verges resident AI expert to come wade through all of it with

196
00:14:25,950 --> 00:14:28,190
Speaker 2:  me. Hey James. Hi

197
00:14:28,190 --> 00:14:29,450
Speaker 10:  David. How you doing?

198
00:14:29,450 --> 00:14:32,630
Speaker 2:  I'm good. I have been excited to talk to you about AI art for a long time

199
00:14:32,630 --> 00:14:36,230
Speaker 2:  because this is to me like the most interesting and deeply

200
00:14:36,230 --> 00:14:40,150
Speaker 2:  bizarre space in tech right now. I have no idea what to make of it. So you're,

201
00:14:40,150 --> 00:14:43,350
Speaker 2:  you're gonna make all of this make sense for me. So thank you in advance

202
00:14:43,350 --> 00:14:46,910
Speaker 2:  for doing so. I'll do my best. The first thing is like, let's just, let's

203
00:14:46,910 --> 00:14:50,310
Speaker 2:  just like lay the land a little bit here. Everybody talks about open AI and

204
00:14:50,310 --> 00:14:54,080
Speaker 2:  Dolly, but this, this is like the, the sort of generative AI space

205
00:14:54,080 --> 00:14:57,560
Speaker 2:  turns out to be like much bigger than I realized.

206
00:14:57,560 --> 00:15:00,760
Speaker 2:  Scope this industry a little bit for us right now. Yeah,

207
00:15:00,760 --> 00:15:04,520
Speaker 11:  I mean generative AI is sort of this term that's become buzzy very recently

208
00:15:04,520 --> 00:15:08,440
Speaker 11:  and we've sort of rec conned what we're a lot of older systems which

209
00:15:08,440 --> 00:15:11,540
Speaker 11:  we're doing this sort of thing. But generative AI

210
00:15:11,540 --> 00:15:15,520
Speaker 11:  technically includes stuff like G PT three, so generating

211
00:15:15,520 --> 00:15:19,230
Speaker 11:  text stuff like GitHub co-pilot generating code,

212
00:15:19,230 --> 00:15:23,160
Speaker 11:  a lot of experimental music models and art models. But the art models

213
00:15:23,160 --> 00:15:26,560
Speaker 11:  are definitely the most visible and they're the ones that are, they've become

214
00:15:26,560 --> 00:15:30,520
Speaker 11:  touchstones for a lot of the wider issues in the industry, which I'm sure

215
00:15:30,520 --> 00:15:34,200
Speaker 11:  we will talk about stuff like copyrights, stuff like safety bias, et cetera,

216
00:15:34,200 --> 00:15:38,000
Speaker 11:  et cetera. For the generative stuff it looks like these models have come

217
00:15:38,000 --> 00:15:41,640
Speaker 11:  out of nowhere but there has been a lot of work going on in this area for

218
00:15:41,640 --> 00:15:45,120
Speaker 11:  many years now. You know the, the latest sort of generation, if we're looking

219
00:15:45,120 --> 00:15:49,000
Speaker 11:  in a multi-decade span of AI art models probably goes back

220
00:15:49,000 --> 00:15:52,880
Speaker 11:  to 2014 or something where you have a type of

221
00:15:52,880 --> 00:15:56,840
Speaker 11:  method known as gans or generative adversarial networks

222
00:15:56,840 --> 00:16:00,720
Speaker 11:  which become the big new thing. And now in the years since then we've

223
00:16:00,720 --> 00:16:04,400
Speaker 11:  had a lot of different methods pop up that have been sort of tried and tested

224
00:16:04,400 --> 00:16:07,810
Speaker 11:  and you might remember some of them. Do you remember Google Deep Dream?

225
00:16:07,810 --> 00:16:08,160
Speaker 11:  Oh

226
00:16:08,160 --> 00:16:08,820
Speaker 2:  Yeah.

227
00:16:08,820 --> 00:16:12,720
Speaker 11:  And that was all the way back in 2015. But that was, you know, one of

228
00:16:12,720 --> 00:16:16,310
Speaker 11:  these early forays where researchers went down

229
00:16:16,310 --> 00:16:20,080
Speaker 11:  what turned out to be basically a sort of a bit of a cul-de-sac but

230
00:16:20,080 --> 00:16:23,920
Speaker 11:  you have to explore those things. And then the latest wave, which was

231
00:16:23,920 --> 00:16:27,560
Speaker 11:  very much kicked off by open eyes Dolly, has come about

232
00:16:27,560 --> 00:16:31,290
Speaker 11:  cause of a method called diffusion, which is just,

233
00:16:31,290 --> 00:16:35,080
Speaker 11:  it really fits with our current capabilities and it's proved to be

234
00:16:35,080 --> 00:16:38,760
Speaker 11:  very, very rich in possibility. And it's from that which

235
00:16:38,760 --> 00:16:42,640
Speaker 11:  sort of opening I first demo that we've got all these other models like

236
00:16:42,640 --> 00:16:45,430
Speaker 11:  stable diffusion, like mid journey and all the rest.

237
00:16:45,430 --> 00:16:49,120
Speaker 2:  Okay, so obviously I'm very smart and I know things and I definitely

238
00:16:49,120 --> 00:16:53,080
Speaker 2:  for sure know what diffusion is, but let's just, let's

239
00:16:53,080 --> 00:16:57,000
Speaker 2:  just for a second assume that I don't like what is it about diffusion

240
00:16:57,000 --> 00:16:59,960
Speaker 2:  that is so new and different? Cause it really seems to me, and I've only

241
00:16:59,960 --> 00:17:03,520
Speaker 2:  been following this from sort of a a like end user perspective over the last

242
00:17:03,520 --> 00:17:07,400
Speaker 2:  several years, like something clicked in the last like 12 months

243
00:17:07,400 --> 00:17:11,160
Speaker 2:  where this went from like nifty science project that people would demo on

244
00:17:11,160 --> 00:17:14,720
Speaker 2:  stage to like real thing doing real stuff in real

245
00:17:14,720 --> 00:17:18,560
Speaker 2:  people's lives and it sounds like diffusion, which again I understand

246
00:17:18,560 --> 00:17:22,440
Speaker 2:  extremely well and know exactly what it is, is responsible

247
00:17:22,440 --> 00:17:25,360
Speaker 2:  for that. So like is diffusion the thing? Like what is this thing and how

248
00:17:25,360 --> 00:17:26,710
Speaker 2:  did it sort of change that?

249
00:17:26,710 --> 00:17:30,000
Speaker 11:  I would say there are lots of factors here. Some of them are to do with

250
00:17:30,000 --> 00:17:33,520
Speaker 11:  these new methods to do with diffusion. Some of it is sort of, as I say,

251
00:17:33,520 --> 00:17:37,120
Speaker 11:  stuff that's been building for a while. You know, we've got a lot of things

252
00:17:37,120 --> 00:17:40,880
Speaker 11:  that people have been working on in terms of computing capability in terms

253
00:17:40,880 --> 00:17:44,600
Speaker 11:  of data sets that have accrued over the years. Diffusion though has

254
00:17:44,600 --> 00:17:48,480
Speaker 11:  really improved things definitely to break diffusion down in

255
00:17:48,480 --> 00:17:52,000
Speaker 11:  its sort of of simplest term is it? Is it basically you get models, you

256
00:17:52,000 --> 00:17:55,400
Speaker 11:  feed them an image on one end, imagine an image on one end complete, you

257
00:17:55,400 --> 00:17:59,030
Speaker 11:  know whatever, it's vango sunflowers. And then on the other end you have

258
00:17:59,030 --> 00:18:02,680
Speaker 11:  static, you have TV static, you have nothing discernible just random

259
00:18:02,680 --> 00:18:06,570
Speaker 11:  patterns. How do you get from vango sunflowers to complete static

260
00:18:06,570 --> 00:18:10,240
Speaker 11:  imagery? You get it through a series of steps and what a

261
00:18:10,240 --> 00:18:14,160
Speaker 11:  diffusion model does is it takes that image step by step by step by

262
00:18:14,160 --> 00:18:17,460
Speaker 11:  step by step, you know hundreds and hundreds of hundreds of steps and it

263
00:18:17,460 --> 00:18:21,090
Speaker 11:  det tunes it to nothing. And then this is the clever bit

264
00:18:21,090 --> 00:18:25,080
Speaker 11:  is is it goes in the other direction. It tries to learn how

265
00:18:25,080 --> 00:18:28,850
Speaker 11:  to get from sunflowers static to sunflowers.

266
00:18:28,850 --> 00:18:32,690
Speaker 11:  It looks at every single step and it tries to learn about the image

267
00:18:32,690 --> 00:18:36,560
Speaker 11:  by doing that. Now again that sounds like well why is that better than any

268
00:18:36,560 --> 00:18:40,240
Speaker 11:  other method of making these things? And I think part of the reason is it

269
00:18:40,240 --> 00:18:44,200
Speaker 11:  fits what has become the sort of default approach to of artificial

270
00:18:44,200 --> 00:18:47,880
Speaker 11:  intelligence at the moment, which relies on a lot of paralleled

271
00:18:47,880 --> 00:18:51,480
Speaker 11:  computation. So you know, people talk a lot

272
00:18:51,480 --> 00:18:55,440
Speaker 11:  about, for example, the utility of GPUs and how they

273
00:18:55,440 --> 00:18:58,400
Speaker 11:  have been really useful in pushing forward artificial intelligence. It's

274
00:18:58,400 --> 00:19:02,160
Speaker 11:  because they are good at doing lots and lots and lots and lots of calculations

275
00:19:02,160 --> 00:19:05,920
Speaker 11:  simultaneously now. Now if you ever imagine you are doing this diffusion

276
00:19:05,920 --> 00:19:09,570
Speaker 11:  tango from sunflowers to static to sunflowers, that

277
00:19:09,570 --> 00:19:12,680
Speaker 11:  is a process that involves lots and lots and lots and lots and lots of little

278
00:19:12,680 --> 00:19:16,330
Speaker 11:  calculations simultaneously. So that's sort of the broad strokes

279
00:19:16,330 --> 00:19:20,280
Speaker 11:  picture, the big picture to use a very appropriate metaphor of why

280
00:19:20,280 --> 00:19:24,080
Speaker 11:  we've taken off now. But yeah, people saw open AI and Dolly do this

281
00:19:24,080 --> 00:19:27,240
Speaker 11:  and they were like, right, we're gonna try that as well. And then everyone

282
00:19:27,240 --> 00:19:30,670
Speaker 11:  else has sort of improved upon their methods and tweaked them in new ways.

283
00:19:30,670 --> 00:19:33,400
Speaker 2:  Part of what's interesting to me about this now is it, it now seems like

284
00:19:33,400 --> 00:19:36,970
Speaker 2:  we're in this phase of going from like you said, sort of proofs of concept

285
00:19:36,970 --> 00:19:40,880
Speaker 2:  to like there are lots of slightly different takes on this same

286
00:19:40,880 --> 00:19:44,640
Speaker 2:  kind of thing. Is there an underlying race here to be sort of the

287
00:19:44,640 --> 00:19:48,080
Speaker 2:  technology provider for whatever's coming next? Is everybody still just kind

288
00:19:48,080 --> 00:19:52,000
Speaker 2:  of experimenting to see what's gonna happen? Like what's building out of

289
00:19:52,000 --> 00:19:54,130
Speaker 2:  all of these different projects here?

290
00:19:54,130 --> 00:19:57,720
Speaker 11:  So the really interesting thing with the current, with what we've seen over

291
00:19:57,720 --> 00:20:01,560
Speaker 11:  the last year is that we've had a new sort of approach

292
00:20:01,560 --> 00:20:05,320
Speaker 11:  evolve and it starts with text to image models, but it's something that

293
00:20:05,320 --> 00:20:08,750
Speaker 11:  was gonna, that it will certainly affect the wider AI industry.

294
00:20:08,750 --> 00:20:12,440
Speaker 11:  People sometimes call them the first generation of AI labs and this

295
00:20:12,440 --> 00:20:16,200
Speaker 11:  is deep mind and open ai. Those are the and Google

296
00:20:16,200 --> 00:20:19,460
Speaker 11:  Brain, you know those are the big, the big ones that have made an impact.

297
00:20:19,460 --> 00:20:23,440
Speaker 11:  Now they had a certain approach to this technology which is

298
00:20:23,440 --> 00:20:27,360
Speaker 11:  a little bit more cautious shall we say. So you have open ai, they

299
00:20:27,360 --> 00:20:31,240
Speaker 11:  create an API for Dolly for businesses and they put in

300
00:20:31,240 --> 00:20:35,000
Speaker 11:  a lot of safety guardrails in there and they don't give people access to

301
00:20:35,000 --> 00:20:38,760
Speaker 11:  it. Now the second wave of AI labs have sort of

302
00:20:38,760 --> 00:20:42,370
Speaker 11:  come and looked at this model and gone, right, how do we beat these guys

303
00:20:42,370 --> 00:20:46,360
Speaker 11:  because they've got so much money, they've got so much computing resources,

304
00:20:46,360 --> 00:20:49,260
Speaker 11:  they've got such a a headstart, what can we do that's wildly different?

305
00:20:49,260 --> 00:20:51,800
Speaker 11:  And they've gone, you know what, we're gonna build these and we're gonna

306
00:20:51,800 --> 00:20:55,760
Speaker 11:  give them away for free. And this has upended a lot of what

307
00:20:55,760 --> 00:20:58,240
Speaker 11:  people thought they knew about how the industry, how this particular part

308
00:20:58,240 --> 00:21:01,560
Speaker 11:  of the industry should work. The standard bearer for that approach is a

309
00:21:01,560 --> 00:21:05,440
Speaker 11:  company called Stability ai. They promote a

310
00:21:05,440 --> 00:21:08,880
Speaker 11:  model called Stable Diffusion. Now a lot of people say they made that

311
00:21:08,880 --> 00:21:12,640
Speaker 11:  model, I wanna be a little bit more specific about that. They funded that

312
00:21:12,640 --> 00:21:16,630
Speaker 11:  model. The model was a lot of the research was done by

313
00:21:16,630 --> 00:21:20,240
Speaker 11:  a university in Germany and there is a reason for that which will

314
00:21:20,240 --> 00:21:23,860
Speaker 11:  become apparent later, but it's essentially to do with legal liabilities

315
00:21:23,860 --> 00:21:27,520
Speaker 11:  and it means that st stability AI isn't legally liable

316
00:21:27,520 --> 00:21:31,400
Speaker 11:  necessarily for what people do with this model and etc. But so that is

317
00:21:31,400 --> 00:21:35,320
Speaker 11:  important but their approach has been why don't we make the model and

318
00:21:35,320 --> 00:21:38,720
Speaker 11:  give it away? And that means our model will get

319
00:21:38,720 --> 00:21:42,640
Speaker 11:  everywhere and we'll collect business as part of that. So

320
00:21:42,640 --> 00:21:46,360
Speaker 11:  they do run a commercial version of stable diffusion called Dream

321
00:21:46,360 --> 00:21:50,320
Speaker 11:  Studio but they are doing much more and they're having a much bigger impact

322
00:21:50,320 --> 00:21:54,120
Speaker 11:  by GI giving this stuff away. And I can really vouch for that first

323
00:21:54,120 --> 00:21:57,960
Speaker 11:  hand and that I've covered just in the last two months. So many companies

324
00:21:57,960 --> 00:22:01,760
Speaker 11:  who have gone, we've got a new text image model and

325
00:22:01,760 --> 00:22:04,520
Speaker 11:  we're doing new things with it and you ask them, oh you did you make this

326
00:22:04,520 --> 00:22:08,480
Speaker 11:  yourself? And they go, it's stable diffusion. And I'm talking about

327
00:22:08,480 --> 00:22:12,040
Speaker 11:  huge companies like Canva for example, you know the Australian design app,

328
00:22:12,040 --> 00:22:15,360
Speaker 11:  they came out just this week, they had it in beta a month ago. Now it's

329
00:22:15,360 --> 00:22:19,120
Speaker 11:  sort of available to all your users, a text to image model and it's

330
00:22:19,120 --> 00:22:22,920
Speaker 11:  just stable diffusion, you know that they've built some

331
00:22:22,920 --> 00:22:26,810
Speaker 11:  new UI stuff into it. But really stability AI have really sort of

332
00:22:26,810 --> 00:22:30,600
Speaker 11:  upended the industry with this. Let's just make it free and let's see

333
00:22:30,600 --> 00:22:32,440
Speaker 11:  what everyone else does as a result.

334
00:22:32,440 --> 00:22:35,600
Speaker 2:  Interesting. Okay and to that point about all these things sort of starting

335
00:22:35,600 --> 00:22:39,440
Speaker 2:  to show up in these products, it seems like there's been this run on it over

336
00:22:39,440 --> 00:22:42,970
Speaker 2:  the last few months and my sense in talking to some of these product folks

337
00:22:42,970 --> 00:22:46,680
Speaker 2:  is there is this like sense in the industry that this is something big, right?

338
00:22:46,680 --> 00:22:50,360
Speaker 2:  Like this is gonna change things. The idea that you can make, especially

339
00:22:50,360 --> 00:22:53,560
Speaker 2:  the text to image thing is like there, everybody seems to gro that there

340
00:22:53,560 --> 00:22:56,600
Speaker 2:  is like something powerful here and so you're seeing it in Canva, you're

341
00:22:56,600 --> 00:22:59,400
Speaker 2:  seeing it in TikTok, like shutter stock is doing stuff which I wanna come

342
00:22:59,400 --> 00:23:03,280
Speaker 2:  back to, but as far as I can tell there are these like big

343
00:23:03,280 --> 00:23:06,920
Speaker 2:  meaty questions about how this is supposed to work and how we understand

344
00:23:06,920 --> 00:23:10,700
Speaker 2:  content veracity and how we understand ownership and how we under

345
00:23:10,700 --> 00:23:13,840
Speaker 2:  and we've solved none of it as far as I can tell. So there's just a lot of

346
00:23:13,840 --> 00:23:16,600
Speaker 2:  people who are like, who are like, oh these are interesting questions by

347
00:23:16,600 --> 00:23:19,100
Speaker 2:  the way, it's available in our app right now. And it's like,

348
00:23:19,100 --> 00:23:19,520
Speaker 11:  Yes

349
00:23:19,520 --> 00:23:23,120
Speaker 2:  This feels slightly backwards to me. Is there, is there more progress being

350
00:23:23,120 --> 00:23:26,750
Speaker 2:  made than I'm giving it credit for? Or is this just like total abject chaos?

351
00:23:26,750 --> 00:23:27,360
Speaker 2:  That's

352
00:23:27,360 --> 00:23:31,200
Speaker 11:  A tricky one. That's a tricky one to answer. I mean I think there

353
00:23:31,200 --> 00:23:34,760
Speaker 11:  has been progress about this stuff and in a way some of the

354
00:23:34,760 --> 00:23:38,480
Speaker 11:  debates that you alluded to about, for example,

355
00:23:38,480 --> 00:23:42,160
Speaker 11:  copyright infringement about safety, our

356
00:23:42,160 --> 00:23:45,560
Speaker 11:  debates we've had before in different guys. So for example, the copyright

357
00:23:45,560 --> 00:23:48,800
Speaker 11:  debate is one we've sort of had with search engines, you know, when they

358
00:23:48,800 --> 00:23:52,020
Speaker 11:  use a snippet of someone's text, is that fair? Is it a transformative use?

359
00:23:52,020 --> 00:23:55,760
Speaker 11:  The debate we've had about content veracity is one we've been having for

360
00:23:55,760 --> 00:23:59,720
Speaker 11:  years with deep fakes and there have been, you know, approaches put

361
00:23:59,720 --> 00:24:03,360
Speaker 11:  forward by people like Adobe. You know, they have their content authenticity

362
00:24:03,360 --> 00:24:06,680
Speaker 11:  initiative where they're discussing, you know, watermarks where they're

363
00:24:06,680 --> 00:24:10,520
Speaker 11:  discussing ways to sort of embed veracity into metadata. So

364
00:24:10,520 --> 00:24:14,080
Speaker 11:  there are big things happening, but we have been talking about some of these

365
00:24:14,080 --> 00:24:14,760
Speaker 11:  problems for a while

366
00:24:14,760 --> 00:24:17,960
Speaker 2:  Now. Well, yeah, I guess the difference to me is I think like the, the deep

367
00:24:17,960 --> 00:24:21,240
Speaker 2:  fix thing is an interesting one, right? Where it's like, it's been possible

368
00:24:21,240 --> 00:24:25,200
Speaker 2:  to do for a long time, but it's not like TikTok had a

369
00:24:25,200 --> 00:24:29,160
Speaker 2:  deep fake filter, right? And now we're getting to the point where this

370
00:24:29,160 --> 00:24:32,960
Speaker 2:  stuff is becoming like first class features of these things and

371
00:24:32,960 --> 00:24:36,440
Speaker 2:  they're, you're actually being like they're easy to use and being made easier

372
00:24:36,440 --> 00:24:39,680
Speaker 2:  to use all the time and being just made so available to you. It feels like

373
00:24:39,680 --> 00:24:43,520
Speaker 2:  before we've answered some of these questions and part of me wonders if they're

374
00:24:43,520 --> 00:24:47,440
Speaker 2:  gonna be big new answers about copyright and ownership and all of

375
00:24:47,440 --> 00:24:50,760
Speaker 2:  a sudden all of these companies are gonna be like, oh dear God, you know,

376
00:24:50,760 --> 00:24:54,720
Speaker 2:  stability AI owns all of the art that exists because some judge

377
00:24:54,720 --> 00:24:58,440
Speaker 2:  decided so we're so screwed. I think you're right that the

378
00:24:58,440 --> 00:25:02,320
Speaker 2:  problems are not actually necessarily as different as they've been made

379
00:25:02,320 --> 00:25:06,090
Speaker 2:  out to be, it's just that like they are being put in front of people

380
00:25:06,090 --> 00:25:08,230
Speaker 2:  at such greater scale so quickly.

381
00:25:08,230 --> 00:25:11,880
Speaker 11:  Yeah, absolutely. I mean, and let, let's take copyright infringement as

382
00:25:11,880 --> 00:25:15,800
Speaker 11:  a sort of example of that. So, so the big thing for these models and

383
00:25:15,800 --> 00:25:19,400
Speaker 11:  copyright is that they are trained on usually vast amounts of

384
00:25:19,400 --> 00:25:23,360
Speaker 11:  data scraped from the open web. Now we, we don't always have a clear

385
00:25:23,360 --> 00:25:27,340
Speaker 11:  insight into what that data is. Open AI for example, has never,

386
00:25:27,340 --> 00:25:31,140
Speaker 11:  has never disclosed what the complete contents of its training data.

387
00:25:31,140 --> 00:25:34,560
Speaker 11:  And I have asked them multiple, multiple times and they always are like,

388
00:25:34,560 --> 00:25:38,400
Speaker 11:  well why do we need to tell you? We don't get anything from telling

389
00:25:38,400 --> 00:25:41,800
Speaker 11:  you, which is whatever, it's their prerogative. But I think it's, you know,

390
00:25:41,800 --> 00:25:45,680
Speaker 11:  deeply suspicious and also completely counter to their name open

391
00:25:45,680 --> 00:25:48,800
Speaker 11:  ai. That's been a misnomer for a long time but don't get me started. Don't

392
00:25:48,800 --> 00:25:52,560
Speaker 11:  get me started on that David. So they scrape huge amounts of

393
00:25:52,560 --> 00:25:56,000
Speaker 11:  internet, of internet content and they scrape it from, from personal

394
00:25:56,000 --> 00:25:59,720
Speaker 11:  blogs, from stock image sites, from all these places. Their

395
00:25:59,720 --> 00:26:03,360
Speaker 11:  defense, these companies is that using this material in this way is covered

396
00:26:03,360 --> 00:26:07,240
Speaker 11:  particularly in the US by fair use doctrine. Now it's funny

397
00:26:07,240 --> 00:26:10,920
Speaker 11:  you should ask about this cuz I've been working all week on a report trying

398
00:26:10,920 --> 00:26:14,680
Speaker 11:  to answer that question. Is this covered by fair use? And I've spoken to

399
00:26:14,680 --> 00:26:18,280
Speaker 11:  several lawyers over the past week and technologists and

400
00:26:18,280 --> 00:26:22,040
Speaker 11:  risk analysis and the general consensus is we don't

401
00:26:22,040 --> 00:26:25,800
Speaker 11:  know for sure which is a really interesting place

402
00:26:25,800 --> 00:26:29,480
Speaker 11:  to be in. Okay? We're pretty sure that the training of these

403
00:26:29,480 --> 00:26:33,320
Speaker 11:  systems that collecting this data and creating a model is covered by fair

404
00:26:33,320 --> 00:26:37,120
Speaker 11:  use that most people are pretty confident on that. Where the questions

405
00:26:37,120 --> 00:26:40,780
Speaker 11:  come in is how are you using these systems? So for example,

406
00:26:40,780 --> 00:26:44,720
Speaker 11:  you know if I train an art model on a hundred million images and I use

407
00:26:44,720 --> 00:26:47,880
Speaker 11:  it to create a new picture that looks kind of like nothing, that's fine.

408
00:26:47,880 --> 00:26:51,720
Speaker 11:  Like nothing in the training data set, that's fine. If I train it on a

409
00:26:51,720 --> 00:26:55,300
Speaker 11:  hundred images by artist X who happens to be living

410
00:26:55,300 --> 00:26:59,030
Speaker 11:  and I tell the model to produce more images that look like artist

411
00:26:59,030 --> 00:27:02,760
Speaker 11:  X made them, then we have a much stronger case for this

412
00:27:02,760 --> 00:27:06,750
Speaker 11:  not being fair use and for it being copyright infringement. What lawyers

413
00:27:06,750 --> 00:27:10,640
Speaker 11:  told me was that this is stuff that's gonna be

414
00:27:10,640 --> 00:27:14,280
Speaker 11:  settled in the courts eventually. It probably will be anyway.

415
00:27:14,280 --> 00:27:18,160
Speaker 11:  And the interesting thing is we've already seen the first legal case

416
00:27:18,160 --> 00:27:22,000
Speaker 11:  launched in this area, the first sort of legal case aimed at a

417
00:27:22,000 --> 00:27:25,880
Speaker 11:  generative AI model and it wasn't at an art model but at a

418
00:27:25,880 --> 00:27:29,700
Speaker 11:  code model. It was a model built by open AI

419
00:27:29,700 --> 00:27:33,680
Speaker 11:  and Microsoft and Microsoft subsidiary GitHub and it's

420
00:27:33,680 --> 00:27:37,200
Speaker 11:  called GitHub co-pilot. And it basically think of it like an auto-complete

421
00:27:37,200 --> 00:27:40,320
Speaker 11:  for code. You are typing your programming away and it will suggest what

422
00:27:40,320 --> 00:27:44,080
Speaker 11:  the next line should be. The problem is it's trained on a lot of public

423
00:27:44,080 --> 00:27:48,040
Speaker 11:  repositories of code, many of which have licenses that say if

424
00:27:48,040 --> 00:27:51,880
Speaker 11:  you reproduce my code then you need to credit it to me in some way because

425
00:27:51,880 --> 00:27:55,650
Speaker 11:  this is the sort of ethos that the open source community is built on. Now

426
00:27:55,650 --> 00:27:59,420
Speaker 11:  copilot does not do that. It never credits where it gets the code from.

427
00:27:59,420 --> 00:28:03,400
Speaker 11:  And so this lawsuit has been launched saying this is in violation of the

428
00:28:03,400 --> 00:28:07,200
Speaker 11:  dmca, the lawsuit says a lot. It also says that there's a conspiracy that

429
00:28:07,200 --> 00:28:10,800
Speaker 11:  there's unfair competition, but this could be something that really

430
00:28:10,800 --> 00:28:14,720
Speaker 11:  changes the entire gen generative AI landscape if they get this

431
00:28:14,720 --> 00:28:18,560
Speaker 11:  certified as a class action lawsuit, if it goes ahead, which it, it may

432
00:28:18,560 --> 00:28:21,560
Speaker 11:  not, we don't know about that yet, then it could really rewrite the rules

433
00:28:21,560 --> 00:28:24,760
Speaker 11:  about using copyrighted material.

434
00:28:24,760 --> 00:28:28,600
Speaker 2:  Interesting. And it seems to me that part of the problem or just the mess

435
00:28:28,600 --> 00:28:32,560
Speaker 2:  of all of this is that it's so hard to know, right? Like you really

436
00:28:32,560 --> 00:28:36,480
Speaker 2:  can't draw a straight line from I fed these 10

437
00:28:36,480 --> 00:28:40,120
Speaker 2:  images to this is why the output looks the way that it does. Like we have

438
00:28:40,120 --> 00:28:43,720
Speaker 2:  some sense but all of this stuff is, is like black boxy to

439
00:28:43,720 --> 00:28:45,590
Speaker 2:  some extent or or another.

440
00:28:45,590 --> 00:28:49,050
Speaker 11:  Okay so have you heard of a tool called Clip Interrogator?

441
00:28:49,050 --> 00:28:49,920
Speaker 2:  No, I've

442
00:28:49,920 --> 00:28:53,640
Speaker 11:  Been meaning to write about it so you can blame me for not knowing about

443
00:28:53,640 --> 00:28:56,330
Speaker 11:  it cuz I should have written about it by now. It was a fascinating thing.

444
00:28:56,330 --> 00:29:00,320
Speaker 11:  So basically you feed Clip interrogator and clip is an

445
00:29:00,320 --> 00:29:04,120
Speaker 11:  acronym that is another AI image generation tool, a module

446
00:29:04,120 --> 00:29:07,800
Speaker 11:  component. You feed it an image and it tries to extrapolate what

447
00:29:07,800 --> 00:29:11,720
Speaker 11:  words were fed into the system to create that image. So if you feed it

448
00:29:11,720 --> 00:29:15,400
Speaker 11:  a picture of yourself, it'll be like how would I describe this if I wanted

449
00:29:15,400 --> 00:29:19,040
Speaker 11:  to generate this image anew from the system? And so people are now

450
00:29:19,040 --> 00:29:22,840
Speaker 11:  suggesting that with something like that you could feed someone's art into

451
00:29:22,840 --> 00:29:26,200
Speaker 11:  that that they've generated with the help of mid journey or would it stable

452
00:29:26,200 --> 00:29:30,080
Speaker 11:  diffusion or whatever it is. And you could get a clue about whether they'd

453
00:29:30,080 --> 00:29:33,920
Speaker 11:  say I want this to look like a, a drawing by Greg Rotowski or

454
00:29:33,920 --> 00:29:37,720
Speaker 11:  Rebecca Sugar or whoever it is now whether that will

455
00:29:37,720 --> 00:29:41,440
Speaker 11:  ever be useful in court, whether that would ever be admissible evidence

456
00:29:41,440 --> 00:29:45,000
Speaker 11:  in any way we don't know. And again, this comes back to the big thing here

457
00:29:45,000 --> 00:29:46,440
Speaker 11:  that's like a lot of this is up in the

458
00:29:46,440 --> 00:29:49,880
Speaker 2:  Air, right? That's really interesting when you say open AI doesn't wanna

459
00:29:49,880 --> 00:29:52,880
Speaker 2:  share training data, my immediate response is like of course open AI doesn't

460
00:29:52,880 --> 00:29:55,360
Speaker 2:  wanna share training data because a, that's like, it's like a trade secret

461
00:29:55,360 --> 00:29:58,640
Speaker 2:  to some extent. And also yeah like there's a lot to be gained for these

462
00:29:58,640 --> 00:30:02,520
Speaker 2:  companies by having it appear to be sort of

463
00:30:02,520 --> 00:30:06,480
Speaker 2:  unknowable magic, right? And I think part of what we're trying to

464
00:30:06,480 --> 00:30:10,360
Speaker 2:  figure out as a like society grappling with this stuff is like we just

465
00:30:10,360 --> 00:30:13,280
Speaker 2:  need to understand this stuff better, right? Like yeah we need to be able

466
00:30:13,280 --> 00:30:17,080
Speaker 2:  to understand how algorithms rank the stuff that we see on social networks.

467
00:30:17,080 --> 00:30:20,000
Speaker 2:  We need to understand why this image looks the way that it does and where

468
00:30:20,000 --> 00:30:23,590
Speaker 2:  it came from and whether it's original. Like all these things are just so

469
00:30:23,590 --> 00:30:27,110
Speaker 2:  much more complicated and so much more intentionally

470
00:30:27,110 --> 00:30:30,720
Speaker 2:  like opaque to us and we actually need to go in the other direction. But

471
00:30:30,720 --> 00:30:34,480
Speaker 2:  like if I'm open ai, nothing is to be gained by helping you

472
00:30:34,480 --> 00:30:37,320
Speaker 2:  understand how it works. All they wanna say is just like look at this nifty

473
00:30:37,320 --> 00:30:41,090
Speaker 2:  thing that it does when you type in a frog writing a scooter

474
00:30:41,090 --> 00:30:44,640
Speaker 2:  in the middle of the ocean like look isn't that great? Why do you need to

475
00:30:44,640 --> 00:30:48,040
Speaker 2:  ask any more questions? And I think that tension to me is gonna be so, so

476
00:30:48,040 --> 00:30:49,650
Speaker 2:  hard to resolve over time.

477
00:30:49,650 --> 00:30:53,400
Speaker 11:  Mm, absolutely. And yeah I should say like the reason I press open AI on

478
00:30:53,400 --> 00:30:56,440
Speaker 11:  this stuff is not necessarily because some people think journalists are

479
00:30:56,440 --> 00:30:59,200
Speaker 11:  very like they want to break down this technology and criticize it in some

480
00:30:59,200 --> 00:31:01,600
Speaker 11:  ways and it's like, no it's exactly the reason you've just said, I just

481
00:31:01,600 --> 00:31:04,880
Speaker 11:  think it's beneficial to everyone if we knew a little bit more about how

482
00:31:04,880 --> 00:31:08,800
Speaker 11:  this stuff works and for open AI a company that is

483
00:31:08,800 --> 00:31:12,600
Speaker 11:  gonna make a lot, a lot, a lot of money and that has a lot of big tax money

484
00:31:12,600 --> 00:31:15,480
Speaker 11:  in there. You know, Microsoft put a billion dollars in open AI and that's

485
00:31:15,480 --> 00:31:18,360
Speaker 11:  why we, that's what we know about. There could be, you know, more big investments

486
00:31:18,360 --> 00:31:22,240
Speaker 11:  like that. This is why it's important to be able to have some insight into

487
00:31:22,240 --> 00:31:25,760
Speaker 11:  these systems. Now interestingly the one thing that Open Eye has told us

488
00:31:25,760 --> 00:31:29,720
Speaker 11:  about is training data is that it bought hundreds of millions of images

489
00:31:29,720 --> 00:31:33,280
Speaker 11:  licensed them along with the metadata from shutter stock. And this is

490
00:31:33,280 --> 00:31:37,160
Speaker 11:  fascinating because a lot of people have been saying what's the first casualty

491
00:31:37,160 --> 00:31:40,920
Speaker 11:  of text to image systems gonna be, it's gonna be stock photo

492
00:31:40,920 --> 00:31:44,440
Speaker 11:  companies. Because if you can just generate, you know, pictures of

493
00:31:44,440 --> 00:31:48,320
Speaker 11:  women laughing with salad or whatever it might be on the fly rather than

494
00:31:48,320 --> 00:31:51,800
Speaker 11:  licensing it, you're you're gonna be in Clover. So it's interesting that

495
00:31:51,800 --> 00:31:55,120
Speaker 11:  Shut stock has sort of preemptively done a deal and as part of this deal

496
00:31:55,120 --> 00:31:59,080
Speaker 11:  they are now going to add text to image systems to shut a

497
00:31:59,080 --> 00:32:02,900
Speaker 11:  stock's website. So you'll be able to generate new stock photos

498
00:32:02,900 --> 00:32:06,040
Speaker 11:  and they've set up, or they're going to set up, we'll see how far they get

499
00:32:06,040 --> 00:32:09,680
Speaker 11:  with it, something called a contributor's fund where they are gonna pay

500
00:32:09,680 --> 00:32:13,520
Speaker 11:  royalties to the creators of copyrighted images used to train

501
00:32:13,520 --> 00:32:16,840
Speaker 11:  Dolly. So they're gonna pay people for using their images in this

502
00:32:16,840 --> 00:32:17,760
Speaker 11:  work.

503
00:32:17,760 --> 00:32:20,390
Speaker 2:  Interesting. So the way they're thinking about it to some extent is like

504
00:32:20,390 --> 00:32:24,320
Speaker 2:  your art is still helping make the art even when the art

505
00:32:24,320 --> 00:32:28,020
Speaker 2:  is AI generated. So you should be compensated in some way.

506
00:32:28,020 --> 00:32:28,440
Speaker 2:  Yes,

507
00:32:28,440 --> 00:32:32,320
Speaker 11:  Exactly. But then the artists, the photographers, they've

508
00:32:32,320 --> 00:32:36,200
Speaker 11:  said a one off payment, a royalty. How is that gonna replace the loss

509
00:32:36,200 --> 00:32:40,110
Speaker 11:  of my job? You know, there's lots and lots of unanswered questions here.

510
00:32:40,110 --> 00:32:43,840
Speaker 11:  I personally think the contributors fund from shut stock is not going to

511
00:32:43,840 --> 00:32:47,720
Speaker 11:  satisfy people. I think there's no way it will be able to give out

512
00:32:47,720 --> 00:32:51,560
Speaker 11:  enough money in order to make people think or feel and to have it be

513
00:32:51,560 --> 00:32:55,280
Speaker 11:  worth their time that that their job has been cloned by this machine. And

514
00:32:55,280 --> 00:32:58,920
Speaker 11:  it's very interesting cuz they stand in opposition shut stock to Getty

515
00:32:58,920 --> 00:33:02,600
Speaker 11:  Images, which is of course the other big stock image site. Getty Images

516
00:33:02,600 --> 00:33:05,310
Speaker 11:  has gone the other way. They have banned it

517
00:33:05,310 --> 00:33:09,080
Speaker 11:  entirely, AI generated imagery. And I've spoken to their

518
00:33:09,080 --> 00:33:12,600
Speaker 11:  CEO about this and he's been very critical of these

519
00:33:12,600 --> 00:33:16,240
Speaker 11:  companies. He's saying that we would never sell AI generated

520
00:33:16,240 --> 00:33:20,200
Speaker 11:  imagery on our sites because we think it would put our customers

521
00:33:20,200 --> 00:33:23,320
Speaker 11:  in legal liability that we don't know about. So they've gone exactly the

522
00:33:23,320 --> 00:33:25,680
Speaker 11:  other way and they're sort of wanting to differentiate themselves

523
00:33:25,680 --> 00:33:29,400
Speaker 2:  Here. But it sounds like by that logic then Getty is not saying we think

524
00:33:29,400 --> 00:33:32,280
Speaker 2:  this is a bad idea and bad for, you know, the business and the art community.

525
00:33:32,280 --> 00:33:36,070
Speaker 2:  So we're out, they're saying who knows where this is gonna land.

526
00:33:36,070 --> 00:33:39,880
Speaker 2:  We'd rather not be roped into whatever weird stuff is gonna happen

527
00:33:39,880 --> 00:33:40,480
Speaker 2:  in the meantime.

528
00:33:40,480 --> 00:33:44,080
Speaker 11:  Yeah, they're protecting their customers rather than their contributors

529
00:33:44,080 --> 00:33:44,950
Speaker 11:  should we say.

530
00:33:44,950 --> 00:33:48,760
Speaker 2:  Yeah, that's that's very fair. So yes. What are you seeing in terms of

531
00:33:48,760 --> 00:33:52,640
Speaker 2:  like sensible guardrails on how this stuff is being done? Cuz right now we're

532
00:33:52,640 --> 00:33:55,240
Speaker 2:  at this moment now that feels very much to me like the the sort of chatbot

533
00:33:55,240 --> 00:33:59,160
Speaker 2:  era when everybody was like, here's a chatbot, do whatever you want

534
00:33:59,160 --> 00:34:02,720
Speaker 2:  with it. And everybody was like, I'm gonna make it racist. And they

535
00:34:02,720 --> 00:34:06,420
Speaker 2:  did and, and it was like, well this was obviously a bad idea

536
00:34:06,420 --> 00:34:09,880
Speaker 2:  and so everybody has sort of tamped it down and made them smaller and these

537
00:34:09,880 --> 00:34:12,280
Speaker 2:  things are much narrower now. And now they will like book you a flight but

538
00:34:12,280 --> 00:34:15,680
Speaker 2:  it's very hard to make them be racist while they book you a flight. But now

539
00:34:15,680 --> 00:34:19,120
Speaker 2:  it, it feels like we're in the kind of here's the text box, do whatever you

540
00:34:19,120 --> 00:34:22,720
Speaker 2:  want with it phase with a lot of the image stuff. Are you seeing people sort

541
00:34:22,720 --> 00:34:24,680
Speaker 2:  of putting rails on this in ways that make sense?

542
00:34:24,680 --> 00:34:28,400
Speaker 11:  There have been a lot of guardrails put in and the guardrails

543
00:34:28,400 --> 00:34:32,280
Speaker 11:  really differ based on the company doing it. So I mentioned

544
00:34:32,280 --> 00:34:36,120
Speaker 11:  earlier that sort of, we had this first gen wave of AI labs which included

545
00:34:36,120 --> 00:34:40,000
Speaker 11:  places like open AI and they have become, you know, because they're

546
00:34:40,000 --> 00:34:43,920
Speaker 11:  now the incumbents, right? They've now got a reputation, they've got huge

547
00:34:43,920 --> 00:34:47,240
Speaker 11:  amounts of investment in them, they've become much more cautious and they

548
00:34:47,240 --> 00:34:51,200
Speaker 11:  do a lot of things in terms of safety. They filter a lot of keywords, you

549
00:34:51,200 --> 00:34:55,000
Speaker 11:  know, so you can't explicitly ask for naked pictures of people or whatever

550
00:34:55,000 --> 00:34:58,880
Speaker 11:  it might be. Sure. And they filter a lot of stuff that would result

551
00:34:58,880 --> 00:35:02,460
Speaker 11:  in obvious bad images and they also have a look at the output as well

552
00:35:02,460 --> 00:35:05,960
Speaker 11:  and they also study, it's not clear how, again, they're not very open about

553
00:35:05,960 --> 00:35:09,840
Speaker 11:  it. They study to some degree what users are asking. So you

554
00:35:09,840 --> 00:35:13,440
Speaker 11:  know, if you put in a lot of dodgy requests that keep on getting bounced

555
00:35:13,440 --> 00:35:16,480
Speaker 11:  back, they might have a look at your request and go, well this guy's clearly

556
00:35:16,480 --> 00:35:20,240
Speaker 11:  trying to mess up our systems and they will boot you out. And

557
00:35:20,240 --> 00:35:23,770
Speaker 11:  not coincidentally I have been booted out of darli

558
00:35:23,770 --> 00:35:27,000
Speaker 11:  because I was trying to, I was trying to stress test the

559
00:35:27,000 --> 00:35:28,790
Speaker 2:  System. Damn it James,

560
00:35:28,790 --> 00:35:32,590
Speaker 11:  I was trying to break it and, and they, they kicked, they banned my account.

561
00:35:32,590 --> 00:35:35,600
Speaker 2:  That's good. That feels like the correct response I would say. Yeah,

562
00:35:35,600 --> 00:35:39,080
Speaker 11:  No, I mean in in fairness fair play to them that was, that was the right

563
00:35:39,080 --> 00:35:42,920
Speaker 11:  thing to do. But on the other hand you have places like civility, AI

564
00:35:42,920 --> 00:35:46,760
Speaker 11:  making stable diffusion. They put stable diffusion out there with a sort

565
00:35:46,760 --> 00:35:50,680
Speaker 11:  of a very basic tier of filtering. And I know it's basic because

566
00:35:50,680 --> 00:35:54,440
Speaker 11:  they are used it myself and b I was speaking to Canva, this company that

567
00:35:54,440 --> 00:35:58,090
Speaker 11:  integrated stable diffusion and they were like, it was trivial

568
00:35:58,090 --> 00:36:01,800
Speaker 11:  to overcome their filters and we had to add another layer of our own cuz

569
00:36:01,800 --> 00:36:04,840
Speaker 11:  we just thought it was so easy to bypass them. So that's not me saying it,

570
00:36:04,840 --> 00:36:07,680
Speaker 11:  that's Canvas saying it, you know, they're putting their tool in front of

571
00:36:07,680 --> 00:36:11,340
Speaker 11:  school kids. They were like this is not enough, we gotta protect these guys.

572
00:36:11,340 --> 00:36:14,480
Speaker 11:  The thing with stable diffusion is that it's very easy to turn off these

573
00:36:14,480 --> 00:36:18,130
Speaker 11:  filters all together and their CEO has said,

574
00:36:18,130 --> 00:36:21,840
Speaker 11:  we just think that this is, you know, it's up to individuals,

575
00:36:21,840 --> 00:36:25,160
Speaker 11:  it's up to people to make the right choices that, you know, they, they come

576
00:36:25,160 --> 00:36:28,640
Speaker 11:  up with the same argument. You hear a lot in the tech industry, which is

577
00:36:28,640 --> 00:36:32,080
Speaker 11:  technology is neutral and I'm very suspicious of that myself. I think technology

578
00:36:32,080 --> 00:36:35,720
Speaker 11:  always has affordances that guides people in one direction or another, but

579
00:36:35,720 --> 00:36:38,920
Speaker 11:  it's, you know, something of a mantra people say they've got, people are

580
00:36:38,920 --> 00:36:42,880
Speaker 11:  gonna do good stuff with it, they'll do bad stuff with it. A moak the

581
00:36:42,880 --> 00:36:46,480
Speaker 11:  stability AI ceo, he says, I think the good's gonna outweigh the bad. And

582
00:36:46,480 --> 00:36:50,240
Speaker 11:  also if the model is out there completely accessible, we'll be able to create

583
00:36:50,240 --> 00:36:54,080
Speaker 11:  better filters more quickly Now, you know, we yet to see what

584
00:36:54,080 --> 00:36:58,000
Speaker 11:  this is gonna play out. Like I've personally think we're gonna see some

585
00:36:58,000 --> 00:37:01,850
Speaker 11:  nasty stuff over the coming months. One thing that

586
00:37:01,850 --> 00:37:05,560
Speaker 11:  is sort of just beginning to become an issue is a model called Dream

587
00:37:05,560 --> 00:37:09,400
Speaker 11:  Booth. Now Dream Booth basically lets you fine tune very

588
00:37:09,400 --> 00:37:13,320
Speaker 11:  easily that have stable diffusion on a specific set of images and you can

589
00:37:13,320 --> 00:37:16,040
Speaker 11:  use that. A lot of companies have already implemented this or a few companies

590
00:37:16,040 --> 00:37:20,020
Speaker 11:  anyway as selfie generators. So you feed the system,

591
00:37:20,020 --> 00:37:23,880
Speaker 11:  you know, 10 of your pictures and it'll do a range of pictures of you

592
00:37:23,880 --> 00:37:27,600
Speaker 11:  looking like a soldier, you looking like a monk, you looking like a whatever

593
00:37:27,600 --> 00:37:31,480
Speaker 11:  it is a cyberpunk villain. However, this is gonna be very easy for people

594
00:37:31,480 --> 00:37:34,960
Speaker 11:  to use for targeted harassment. And I think this is when a lot of the discussions

595
00:37:34,960 --> 00:37:38,640
Speaker 11:  about deep fakes about non-consensual pornography, these are gonna start

596
00:37:38,640 --> 00:37:42,410
Speaker 11:  coming back into prominence again because we're gonna see a lot of people

597
00:37:42,410 --> 00:37:45,800
Speaker 11:  mocking up nudes of people essentially. It's, it's not gonna be pretty at

598
00:37:45,800 --> 00:37:45,920
Speaker 11:  all.

599
00:37:45,920 --> 00:37:49,760
Speaker 2:  Yeah. And I think it's that kind of stuff. Like I, I

600
00:37:49,760 --> 00:37:53,160
Speaker 2:  just, I'm permanently suspicious of the argument that people use that are

601
00:37:53,160 --> 00:37:55,720
Speaker 2:  like, let's just figure out what happens and then we'll back our way into

602
00:37:55,720 --> 00:37:59,360
Speaker 2:  successful answers. And we hear it from everybody in every industry all the

603
00:37:59,360 --> 00:38:01,920
Speaker 2:  time, right? Like let's, let's let all the bad stuff happen so we can learn

604
00:38:01,920 --> 00:38:05,870
Speaker 2:  all the bad stuff and that doesn't work. It turns out

605
00:38:05,870 --> 00:38:08,960
Speaker 2:  it's like it just doesn't work. But I I I do think you're right and I think

606
00:38:08,960 --> 00:38:12,160
Speaker 2:  we're, we're about to enter this really interesting phase where like we as

607
00:38:12,160 --> 00:38:15,000
Speaker 2:  people are gonna start, have to have to figure out how we respond to this

608
00:38:15,000 --> 00:38:18,840
Speaker 2:  stuff, right? Like we're still in the phase where like people just do a dolly

609
00:38:18,840 --> 00:38:22,080
Speaker 2:  search and like take a screenshot of it and share it on Twitter and all of

610
00:38:22,080 --> 00:38:25,000
Speaker 2:  our responses like ha ha that's sort of funny. But this stuff is gonna start

611
00:38:25,000 --> 00:38:28,840
Speaker 2:  to end up in like, like it's, it's the new clip Art in Office is working

612
00:38:28,840 --> 00:38:32,720
Speaker 2:  with this kind of AI art, AI art in business presentations is

613
00:38:32,720 --> 00:38:35,320
Speaker 2:  like a whole weird thing that we're gonna have to figure out what we, we

614
00:38:35,320 --> 00:38:38,600
Speaker 2:  think about. And art is really interesting cuz it seems to people like it's

615
00:38:38,600 --> 00:38:41,800
Speaker 2:  very low stakes but it's actually everywhere and in a lot of ways it's really

616
00:38:41,800 --> 00:38:44,840
Speaker 2:  high stakes and I think figuring out all the different things it means and

617
00:38:44,840 --> 00:38:48,560
Speaker 2:  how we're supposed to approach each one feels like a big

618
00:38:48,560 --> 00:38:51,880
Speaker 2:  challenge to go through kind of all at the same time. But it's gonna be super,

619
00:38:51,880 --> 00:38:52,990
Speaker 2:  super interesting watch.

620
00:38:52,990 --> 00:38:56,840
Speaker 11:  Yeah, absolutely. But I think you mentioned this stuff is clip

621
00:38:56,840 --> 00:39:00,620
Speaker 11:  art and I think that's a really important thing to remember

622
00:39:00,620 --> 00:39:04,560
Speaker 11:  and specifically, you know, Microsoft has created this app called

623
00:39:04,560 --> 00:39:07,560
Speaker 11:  Designer that this part of the office suite it uses Dolly and they want

624
00:39:07,560 --> 00:39:10,720
Speaker 11:  to, as you said, use it to generate clip art. Whenever I think about this

625
00:39:10,720 --> 00:39:14,560
Speaker 11:  technology there's, you know, I flip between in this sort of quantum

626
00:39:14,560 --> 00:39:17,700
Speaker 11:  superposition between thinking this stuff is the mad stuff there is possible,

627
00:39:17,700 --> 00:39:20,640
Speaker 11:  you type in image and it exists, blah blah blah gonna change the world.

628
00:39:20,640 --> 00:39:23,760
Speaker 11:  And then I look at what people are doing with it right now and I go, it's

629
00:39:23,760 --> 00:39:27,720
Speaker 11:  clip art. They've just redone clip art, you know, and

630
00:39:27,720 --> 00:39:31,040
Speaker 11:  it doesn't seem like that big a deal when you put it like that. And you

631
00:39:31,040 --> 00:39:34,600
Speaker 11:  know, I think our, our job as journalists is to sort of say what we see,

632
00:39:34,600 --> 00:39:38,240
Speaker 11:  right? It's to just not get too carried away by what could be

633
00:39:38,240 --> 00:39:42,040
Speaker 11:  and just say what is and what is right now is a lot of clip

634
00:39:42,040 --> 00:39:45,960
Speaker 11:  art, right? There's definitely stuff that's gonna bad stuff and amazing

635
00:39:45,960 --> 00:39:48,880
Speaker 11:  stuff that's gonna happen with it. But actually the way we are gonna get

636
00:39:48,880 --> 00:39:52,680
Speaker 11:  the impact of it really soon and right now is gonna be just people putting

637
00:39:52,680 --> 00:39:56,400
Speaker 11:  new images and presentations and that's gonna be really good in many ways.

638
00:39:56,400 --> 00:39:59,440
Speaker 11:  I think that's gonna be really fun. You know, if I was a six, seven year

639
00:39:59,440 --> 00:40:02,320
Speaker 11:  old right now and I was making a mock up about, you know, why turtles are

640
00:40:02,320 --> 00:40:05,440
Speaker 11:  the best animals in the sea and actually I do that anyway on a weekend,

641
00:40:05,440 --> 00:40:05,920
Speaker 11:  but that's, you

642
00:40:05,920 --> 00:40:06,970
Speaker 2:  Know, that one does. Yeah.

643
00:40:06,970 --> 00:40:10,680
Speaker 11:  As one does then I would be so overjoyed that I could like ask for an image

644
00:40:10,680 --> 00:40:14,240
Speaker 11:  of a turtle wearing a crown and I would be like, yeah, great. You know,

645
00:40:14,240 --> 00:40:17,760
Speaker 11:  there's a lot of good stuff in there that I think is, is gonna be beneficial

646
00:40:17,760 --> 00:40:20,400
Speaker 11:  and is low stakes. So we should remember that. Definitely.

647
00:40:20,400 --> 00:40:23,930
Speaker 2:  Totally. I agree. Thank you for doing this James. I appreciate it.

648
00:40:23,930 --> 00:40:27,620
Speaker 11:  My, my pleasure. My pleasure guys.

649
00:40:27,620 --> 00:40:31,000
Speaker 2:  All right, we need to take a break. When we come back we're gonna dive into

650
00:40:31,000 --> 00:40:34,440
Speaker 2:  the deeply confusing and surprisingly very important

651
00:40:34,440 --> 00:40:38,360
Speaker 2:  world of digital colors and why designers are so mad

652
00:40:38,360 --> 00:40:40,480
Speaker 2:  at Adobe and Panton right now. We'll be right

653
00:40:40,480 --> 00:40:49,180
Speaker 2:  back.

654
00:42:57,760 --> 00:43:01,710
Speaker 2:  Welcome back. A couple of weeks ago a bit of small seeming

655
00:43:01,710 --> 00:43:05,510
Speaker 2:  news kind of shook up the art world. Pantone and Adobe announced

656
00:43:05,510 --> 00:43:09,230
Speaker 2:  that if you wanted to use Pantone colors in Adobe products, you now

657
00:43:09,230 --> 00:43:12,510
Speaker 2:  need to subscribe to a service called Pantone Connect, which costs

658
00:43:12,510 --> 00:43:15,790
Speaker 2:  $15 a month. Adobe kind of blamed

659
00:43:15,790 --> 00:43:19,310
Speaker 2:  Pantone and sort of backhandedly accused it of money

660
00:43:19,310 --> 00:43:23,090
Speaker 2:  gouging while Pantone said that Adobe wasn't supporting its latest stuff

661
00:43:23,090 --> 00:43:27,030
Speaker 2:  and so it needed to go its own way. Whatever the reason, this was a big

662
00:43:27,030 --> 00:43:30,910
Speaker 2:  deal for designers, many of whom have been relying on Pantone colors to do

663
00:43:30,910 --> 00:43:34,630
Speaker 2:  their job for years. Personally all it did was

664
00:43:34,630 --> 00:43:38,150
Speaker 2:  confuse me aggressively, aren't colors just like

665
00:43:38,150 --> 00:43:41,950
Speaker 2:  codes on a computer? How did one company come to

666
00:43:41,950 --> 00:43:45,830
Speaker 2:  own those colors? How do you even own a color? Like I said, I'm

667
00:43:45,830 --> 00:43:49,190
Speaker 2:  very confused. So I grabbed Kristen Radke, a creative director at the Verge

668
00:43:49,190 --> 00:43:52,590
Speaker 2:  and Verge reporter Jess Weathered to help me figure it out. Hi

669
00:43:52,590 --> 00:43:56,550
Speaker 2:  Kristen. Hi. Hi Jess. Hi. This is gonna

670
00:43:56,550 --> 00:43:59,790
Speaker 2:  be delightful because we are now entering a world I know absolutely nothing

671
00:43:59,790 --> 00:44:03,670
Speaker 2:  about. So bear with me as we go through this, but I think the place

672
00:44:03,670 --> 00:44:07,190
Speaker 2:  I wanna start is I think Kristen, we, we have to do like a weird amount of

673
00:44:07,190 --> 00:44:11,040
Speaker 2:  defining like what colors are in order to understand what's going on here.

674
00:44:11,040 --> 00:44:14,680
Speaker 2:  So can you just sort of walk me through like what

675
00:44:14,680 --> 00:44:18,670
Speaker 2:  Pantone is and what sort of purpose it serves in the

676
00:44:18,670 --> 00:44:19,640
Speaker 2:  design community?

677
00:44:19,640 --> 00:44:23,500
Speaker 18:  So basically in the sixties Pantone created what they called

678
00:44:23,500 --> 00:44:27,310
Speaker 18:  a universal color language, which is basically just a giant and

679
00:44:27,310 --> 00:44:30,710
Speaker 18:  very expensive swatch book of colored ink. So

680
00:44:30,710 --> 00:44:34,600
Speaker 18:  Pantone is only used for print design. Like if you're making a poster or

681
00:44:34,600 --> 00:44:37,520
Speaker 18:  a product and you want to know for sure what the color is gonna look like

682
00:44:37,520 --> 00:44:40,920
Speaker 18:  before you go into production, Panton is the best way to do that because

683
00:44:40,920 --> 00:44:44,080
Speaker 18:  other ways of printing produce a lot more variation. Okay.

684
00:44:44,080 --> 00:44:47,960
Speaker 2:  So the ideas basically like what I think of as blue and what you

685
00:44:47,960 --> 00:44:51,480
Speaker 2:  think of as blue are different. But if we can agree on like

686
00:44:51,480 --> 00:44:54,760
Speaker 2:  Pantone's version of blue, we can accomplish the same thing

687
00:44:54,760 --> 00:44:58,640
Speaker 18:  Together. So I mean like in any version of color, whether it's on the

688
00:44:58,640 --> 00:45:02,330
Speaker 18:  internet or in print, there are a million different ways to make that color.

689
00:45:02,330 --> 00:45:05,760
Speaker 18:  If you look at something like black for example, the range of black is extreme.

690
00:45:05,760 --> 00:45:08,840
Speaker 18:  There's like true black, there's rich black and then there's a million other

691
00:45:08,840 --> 00:45:12,320
Speaker 18:  varieties because black can either be built just out of black or out of

692
00:45:12,320 --> 00:45:15,880
Speaker 18:  all the colors kind of shoved together, different inks pushed

693
00:45:15,880 --> 00:45:19,800
Speaker 18:  together. So Pantone is a way like there are a huge variety of black, there's

694
00:45:19,800 --> 00:45:22,680
Speaker 18:  a huge variety of blue and Pantone just like there's a huge variety of any

695
00:45:22,680 --> 00:45:26,040
Speaker 18:  color, but you can pick the exact one so you know exactly what it will look

696
00:45:26,040 --> 00:45:30,000
Speaker 18:  like because it's an ink. It's not just, it's not just like a combination

697
00:45:30,000 --> 00:45:32,020
Speaker 18:  of other inks, it's a specific ink.

698
00:45:32,020 --> 00:45:35,760
Speaker 2:  Got it. Okay. And it's like figuring out how what Pantone

699
00:45:35,760 --> 00:45:39,120
Speaker 2:  sort of owns and doesn't has been very confusing to me cuz I've like I've

700
00:45:39,120 --> 00:45:42,200
Speaker 2:  read enough about this to know that like you can't own a color, which is

701
00:45:42,200 --> 00:45:46,120
Speaker 2:  a thing. Yes. But Pantone is sort of the, we've just all agreed that it is

702
00:45:46,120 --> 00:45:49,070
Speaker 2:  like as good a standard as any, so we kind of went with it, right,

703
00:45:49,070 --> 00:45:52,960
Speaker 18:  A standard only for print. So I think that's one thing that some

704
00:45:52,960 --> 00:45:56,280
Speaker 18:  people don't necessarily realize is that Pantone is really only used for

705
00:45:56,280 --> 00:45:59,080
Speaker 18:  print. So like would it be helpful if I just like told you about different

706
00:45:59,080 --> 00:46:00,990
Speaker 18:  color profiles and different color spaces

707
00:46:00,990 --> 00:46:02,030
Speaker 2:  Please.

708
00:46:02,030 --> 00:46:05,880
Speaker 18:  Okay. So there are like three main color spaces that designers use most

709
00:46:05,880 --> 00:46:09,000
Speaker 18:  often. So like if you're using a color for the internet, if we're designing

710
00:46:09,000 --> 00:46:12,880
Speaker 18:  something for the Verge, we're gonna use a six digit hex code that signifies

711
00:46:12,880 --> 00:46:16,680
Speaker 18:  a mixture of red, blue, and green. So that's within the RGB

712
00:46:16,680 --> 00:46:20,500
Speaker 18:  color space because it's built outta a formula of three primary colors.

713
00:46:20,500 --> 00:46:24,400
Speaker 18:  But the way things look on a screen is a lot of times super different from

714
00:46:24,400 --> 00:46:27,680
Speaker 18:  how they look when they're printed. Screens are back lit. Screens have the

715
00:46:27,680 --> 00:46:31,440
Speaker 18:  ability to kind of communicate a certain range of color. So sometimes that

716
00:46:31,440 --> 00:46:35,320
Speaker 18:  means there's a wider range and sometimes it's more limited depending on

717
00:46:35,320 --> 00:46:38,640
Speaker 18:  the color that you're using. So that doesn't translate exactly to physical

718
00:46:38,640 --> 00:46:42,160
Speaker 18:  printing. So there's also like added variables like what kind of printer

719
00:46:42,160 --> 00:46:45,000
Speaker 18:  are you using and what kind of paper because different paper sucks up in

720
00:46:45,000 --> 00:46:48,840
Speaker 18:  in different ways. So for print, a lot of times we use more advanced

721
00:46:48,840 --> 00:46:52,800
Speaker 18:  color space, which is called CM Y K and that stands for the four colors

722
00:46:52,800 --> 00:46:56,060
Speaker 18:  you're using, which are cyan, magenta, yellow and key.

723
00:46:56,060 --> 00:46:59,640
Speaker 18:  And everyone's always confused what the case stands for. It's just black

724
00:46:59,640 --> 00:47:02,840
Speaker 18:  and it got its name from, in traditional printing you would call the black

725
00:47:02,840 --> 00:47:05,960
Speaker 18:  plate the key plate because you'd run it through last and that's how you

726
00:47:05,960 --> 00:47:09,760
Speaker 18:  get all the detail in the art. But then we have Pantone and Pantone

727
00:47:09,760 --> 00:47:13,040
Speaker 18:  isn't so much a color space like I said, but it's a specific kind of ink

728
00:47:13,040 --> 00:47:16,680
Speaker 18:  so it ensures you know exactly the kind of color you're gonna get. And it

729
00:47:16,680 --> 00:47:20,480
Speaker 18:  also lets you print a lot of stuff that C M Y K can't do. It can be a

730
00:47:20,480 --> 00:47:23,880
Speaker 18:  lot brighter, a lot more vibrant. It can do neon, it can do

731
00:47:23,880 --> 00:47:26,190
Speaker 18:  metallics and a lot of times it just looks better.

732
00:47:26,190 --> 00:47:29,880
Speaker 2:  Okay, this all is very helpful. It also makes me realize like we are down

733
00:47:29,880 --> 00:47:32,540
Speaker 2:  the deepest rabbit hole of things. I have never, I, if you had asked me what

734
00:47:32,540 --> 00:47:36,160
Speaker 2:  CNY case stood for and we just like sat here in silence for an hour, I would've

735
00:47:36,160 --> 00:47:40,040
Speaker 2:  never come up with it. But Jess, walk me through this like fight

736
00:47:40,040 --> 00:47:43,630
Speaker 2:  between Adobe and Pantone because as I understand it, there's like

737
00:47:43,630 --> 00:47:47,240
Speaker 2:  some slight backstory to this beef that predates this

738
00:47:47,240 --> 00:47:47,990
Speaker 2:  news.

739
00:47:47,990 --> 00:47:51,840
Speaker 19:  Yeah, we don't necessarily have the entire story. We only have kind of like

740
00:47:51,840 --> 00:47:55,240
Speaker 19:  the sides that they're telling us from their perspective, well,

741
00:47:55,240 --> 00:47:59,070
Speaker 19:  perspectives and that. So as far as Panton is concerned,

742
00:47:59,070 --> 00:48:02,480
Speaker 19:  they have been in a partnership with Adobe now for, for years and years

743
00:48:02,480 --> 00:48:06,280
Speaker 19:  and years. They do not feel that Adobe has either allowed them

744
00:48:06,280 --> 00:48:10,120
Speaker 19:  to substantially update their product within the Adobe Creative Suite and

745
00:48:10,120 --> 00:48:14,030
Speaker 19:  then subsequently the creative cloud or that Adobe has not

746
00:48:14,030 --> 00:48:17,280
Speaker 19:  pulled its own weight and done those updates by ourselves. So they basically

747
00:48:17,280 --> 00:48:20,610
Speaker 19:  turned around and said we no longer trust Adobe to maintain our product.

748
00:48:20,610 --> 00:48:24,360
Speaker 19:  We are removing it from Adobe in order to preserve it and

749
00:48:24,360 --> 00:48:27,800
Speaker 19:  keep it maintained to our own standards. Adobe

750
00:48:27,800 --> 00:48:31,630
Speaker 19:  has for whatever reason, not fought against that and gone. Okay, sweet.

751
00:48:31,630 --> 00:48:35,400
Speaker 19:  I can't see that anyone else is gonna have an issue with this buy and it's

752
00:48:35,400 --> 00:48:38,920
Speaker 19:  all come to a head. There seems to be kind of rumors, speculating that either

753
00:48:38,920 --> 00:48:42,120
Speaker 19:  Pantone has turned around and asked Adobe for more

754
00:48:42,120 --> 00:48:46,080
Speaker 19:  money or vice versa. Either party has then turned around and gone,

755
00:48:46,080 --> 00:48:49,520
Speaker 19:  no. And now they're just expecting the the consumer to absorb that cost

756
00:48:49,520 --> 00:48:50,470
Speaker 19:  at face value.

757
00:48:50,470 --> 00:48:54,160
Speaker 2:  Okay, so where this lands then is now if you want to use Pantone

758
00:48:54,160 --> 00:48:57,800
Speaker 2:  colors, if I'm understanding this correctly, you have to pay an extra subscription

759
00:48:57,800 --> 00:49:01,280
Speaker 2:  on top of your Creative Cloud subscription. And if you don't, was I reading

760
00:49:01,280 --> 00:49:03,840
Speaker 2:  this right earlier that basically everything that use Pantone colors, if

761
00:49:03,840 --> 00:49:07,520
Speaker 2:  you stop paying it just turns black. That is like the most bonkers, heavy

762
00:49:07,520 --> 00:49:10,560
Speaker 2:  handed execution of this that like, oh, what do you want? Colors? You get

763
00:49:10,560 --> 00:49:11,330
Speaker 2:  no colors.

764
00:49:11,330 --> 00:49:14,280
Speaker 19:  It kind of goes against what Pantone were promising cuz they were saying

765
00:49:14,280 --> 00:49:18,000
Speaker 19:  that any kind of legacy documents that already used Pantone colors would

766
00:49:18,000 --> 00:49:21,040
Speaker 19:  preserve the access to those, those files or whatever books that they were

767
00:49:21,040 --> 00:49:24,800
Speaker 19:  using at the time. That wasn't the case. Whoever got that initial update,

768
00:49:24,800 --> 00:49:28,040
Speaker 19:  I have no idea whether those have been restored. I haven't seen any instances

769
00:49:28,040 --> 00:49:31,320
Speaker 19:  of people saying that they've come back. But yeah, when that update went

770
00:49:31,320 --> 00:49:35,000
Speaker 19:  live, existing Lexi documents that were opened that were using Pantone colors

771
00:49:35,000 --> 00:49:37,360
Speaker 19:  initially are now just displaying complete black where they were.

772
00:49:37,360 --> 00:49:40,980
Speaker 2:  Kristin, how big a deal is this like in your day to day life? Like

773
00:49:40,980 --> 00:49:44,520
Speaker 2:  you, you work mostly digitally? We're we're not a print magazine. Yeah, so

774
00:49:44,520 --> 00:49:47,800
Speaker 2:  I hopefully this doesn't like ruin your day to day but like in in your world,

775
00:49:47,800 --> 00:49:51,150
Speaker 2:  how big a deal is losing this thing kind of overnight.

776
00:49:51,150 --> 00:49:54,930
Speaker 18:  Okay. So anyone who's like a, who's working as a full-time designer

777
00:49:54,930 --> 00:49:57,920
Speaker 18:  or who's working at a, you know a company where you're doing a lot of print

778
00:49:57,920 --> 00:50:00,600
Speaker 18:  products, obviously they're gonna pay for the service, it's gonna be no

779
00:50:00,600 --> 00:50:04,120
Speaker 18:  big deal. I think it is a much bigger deal when you're a freelance artist,

780
00:50:04,120 --> 00:50:08,080
Speaker 18:  partially because working with Pantos is already a pain. When I work with

781
00:50:08,080 --> 00:50:11,610
Speaker 18:  artists for print product, like for example we've been working at the Verge

782
00:50:11,610 --> 00:50:14,720
Speaker 18:  on this big package almost all year about the 20 year anniversary of the

783
00:50:14,720 --> 00:50:18,280
Speaker 18:  Department of Homeland Security that's mostly online where we're using this

784
00:50:18,280 --> 00:50:21,840
Speaker 18:  really great electric blue but we are doing a print product that we really

785
00:50:21,840 --> 00:50:25,680
Speaker 18:  couldn't replicate that blue in Cnyk. It just, we couldn't get it

786
00:50:25,680 --> 00:50:29,600
Speaker 18:  as bright as it was online. So that's when you might turn

787
00:50:29,600 --> 00:50:32,760
Speaker 18:  to something like Pantone. The problem is we're working with a lot of artists

788
00:50:32,760 --> 00:50:36,640
Speaker 18:  who are used to working in digital spaces and converting art that was

789
00:50:36,640 --> 00:50:40,440
Speaker 18:  built for the internet into Pantone is like the nightmare

790
00:50:40,440 --> 00:50:44,280
Speaker 18:  that you can't believe. Like it takes more time than makes and is logical

791
00:50:44,280 --> 00:50:48,040
Speaker 18:  sense and I feel like that's always been a broken part of the Adobe suite,

792
00:50:48,040 --> 00:50:51,520
Speaker 18:  that translation because the way you're making something might change, the

793
00:50:51,520 --> 00:50:54,660
Speaker 18:  application might change, you might want to shift and do something for print.

794
00:50:54,660 --> 00:50:58,240
Speaker 18:  And so it's always been really broken. It's always been like there's been

795
00:50:58,240 --> 00:51:01,960
Speaker 18:  this separate program with inside of Adobe Photoshop or Illustrator

796
00:51:01,960 --> 00:51:05,600
Speaker 18:  or whatever you're using. So this just makes that a much more complicated

797
00:51:05,600 --> 00:51:06,310
Speaker 18:  mess.

798
00:51:06,310 --> 00:51:09,160
Speaker 2:  Yeah, this is the part of all of this that has been the hardest for me to

799
00:51:09,160 --> 00:51:12,120
Speaker 2:  wrap my head around, which is like I think of it as like there is a whole

800
00:51:12,120 --> 00:51:15,240
Speaker 2:  spectrum of colors and all of them are identifiable with hex codes, right?

801
00:51:15,240 --> 00:51:18,760
Speaker 2:  And it's like I have the slider in RGB that I can use and I have hex codes

802
00:51:18,760 --> 00:51:22,120
Speaker 2:  and between those I always thought I could access all the colors that like

803
00:51:22,120 --> 00:51:25,280
Speaker 2:  goof around with that enough and you should be able to accomplish that. But

804
00:51:25,280 --> 00:51:28,280
Speaker 2:  now we're in this place where it seems like what we actually have is a bunch

805
00:51:28,280 --> 00:51:31,640
Speaker 2:  of standards that are overlapping but slightly different and none of them

806
00:51:31,640 --> 00:51:33,760
Speaker 2:  actually understand each other. Yes. And now somebody is saying well this

807
00:51:33,760 --> 00:51:37,560
Speaker 2:  one is ours and we are taking it away from you. And that I like, I can see

808
00:51:37,560 --> 00:51:39,160
Speaker 2:  why that would be a big problem for people,

809
00:51:39,160 --> 00:51:42,560
Speaker 18:  Which has always been kind of a problem with Pantone. Like Pantone updates

810
00:51:42,560 --> 00:51:45,960
Speaker 18:  their color books like every year or two and they do that so they can charge

811
00:51:45,960 --> 00:51:49,680
Speaker 18:  like another $175 for a swatch book and

812
00:51:49,680 --> 00:51:53,400
Speaker 18:  that means that sometimes if your swatch book on your Adobe suite is out

813
00:51:53,400 --> 00:51:57,020
Speaker 18:  of date, you have to pay to download that new color book.

814
00:51:57,020 --> 00:51:59,880
Speaker 18:  Now what's really happening though is that Panton is just turning into a

815
00:51:59,880 --> 00:52:02,590
Speaker 18:  subscription service rather than something you do every couple of years.

816
00:52:02,590 --> 00:52:06,280
Speaker 2:  Jess, this seems like the kind of thing that it's like a funny match to when

817
00:52:06,280 --> 00:52:08,800
Speaker 2:  Adobe went to Creative Cloud in the first place and everybody got really

818
00:52:08,800 --> 00:52:11,080
Speaker 2:  up in arms about it and they're like, well I used to just pay for a thing

819
00:52:11,080 --> 00:52:14,160
Speaker 2:  and then I would have the thing and now you're basically telling me to like

820
00:52:14,160 --> 00:52:17,560
Speaker 2:  pay you a large amount of money every year or like the tools I need to do

821
00:52:17,560 --> 00:52:21,320
Speaker 2:  my job disappear on me and I don't own them anymore. Like is

822
00:52:21,320 --> 00:52:25,240
Speaker 2:  this just the inexorable path that all of this stuff is heading and to

823
00:52:25,240 --> 00:52:29,200
Speaker 2:  be a designer or anyone is just gonna be a million subscriptions that you

824
00:52:29,200 --> 00:52:31,770
Speaker 2:  basically have no choice but to pay for. Like is this where we're headed?

825
00:52:31,770 --> 00:52:34,960
Speaker 19:  It kind of feels like that is the way it's going. There are a couple of

826
00:52:34,960 --> 00:52:37,960
Speaker 19:  services recently that have also switched from a one time purchase model

827
00:52:37,960 --> 00:52:41,600
Speaker 19:  over to subscription. I can think of kind of like a clip paint studio off

828
00:52:41,600 --> 00:52:45,280
Speaker 19:  the top of my head a couple of months back. Their fan base felt quite significantly

829
00:52:45,280 --> 00:52:48,960
Speaker 19:  betrayed by that. But by kind of standing their ground and saying,

830
00:52:48,960 --> 00:52:52,400
Speaker 19:  well we as Adobe don't want to front this cost for our consumers, you can

831
00:52:52,400 --> 00:52:55,300
Speaker 19:  now pay for it. They've kind of shot themselves in the foot by proving that

832
00:52:55,300 --> 00:52:59,200
Speaker 19:  the software as a service model is as bad as people were worried it was

833
00:52:59,200 --> 00:53:03,040
Speaker 19:  going to be. Because inevitably people that still have legacy Adobe

834
00:53:03,040 --> 00:53:06,920
Speaker 19:  products, if you still have a copy of Creative Suite for example CS six

835
00:53:06,920 --> 00:53:10,480
Speaker 19:  knocking around that will still have access to the Pantone color box within

836
00:53:10,480 --> 00:53:14,400
Speaker 19:  there because it doesn't receive any legacy updates. So if you have Adobe

837
00:53:14,400 --> 00:53:17,800
Speaker 19:  consists to be an updated product that's no longer up to industry standards,

838
00:53:17,800 --> 00:53:20,680
Speaker 19:  you're actually better off if you're still using Pantone colors despite

839
00:53:20,680 --> 00:53:23,330
Speaker 19:  the fact that they might be out of date by a significant margin.

840
00:53:23,330 --> 00:53:27,200
Speaker 2:  So are we gonna get like a black market for old versions of

841
00:53:27,200 --> 00:53:30,840
Speaker 2:  Adobe products? Like I, this, this is just, this all gets so crazy to me

842
00:53:30,840 --> 00:53:34,720
Speaker 2:  where it's like suddenly several years old software becomes a really useful

843
00:53:34,720 --> 00:53:35,430
Speaker 2:  thing to have.

844
00:53:35,430 --> 00:53:38,840
Speaker 19:  It's already kind of happening. So there's a couple of things are like noteworthy

845
00:53:38,840 --> 00:53:42,720
Speaker 19:  here. So one is that Adobe software in general is just one of the most

846
00:53:42,720 --> 00:53:46,360
Speaker 19:  powered softwares in the world alongside Microsoft Office Suite. The two

847
00:53:46,360 --> 00:53:49,840
Speaker 19:  of them are just like, they are the top ranking in terms of what people

848
00:53:49,840 --> 00:53:53,160
Speaker 19:  will go and legally crack online for those reasons they don't want to pay

849
00:53:53,160 --> 00:53:57,080
Speaker 19:  an ongoing subscription fee for the product. Yes it might be initial

850
00:53:57,080 --> 00:54:00,720
Speaker 19:  hefty cash lump sum to begin with, but if you are paying that and you are

851
00:54:00,720 --> 00:54:04,120
Speaker 19:  consistently working with the creative industry, I think it's within like

852
00:54:04,120 --> 00:54:07,840
Speaker 19:  three or four years or something, you would've already paid for the most

853
00:54:07,840 --> 00:54:11,800
Speaker 19:  premium monthly subscription tier of Adobe. And at that point

854
00:54:11,800 --> 00:54:14,840
Speaker 19:  you are just handing Adobe money for a continual product that may not be

855
00:54:14,840 --> 00:54:18,160
Speaker 19:  necessarily like getting updates that are worth your time. The other thing

856
00:54:18,160 --> 00:54:21,440
Speaker 19:  from that is that people are finding that if they either had a preUpdate

857
00:54:21,440 --> 00:54:25,330
Speaker 19:  version of Adobe products or an outdated version of

858
00:54:25,330 --> 00:54:29,000
Speaker 19:  CS six, they can pull the the Pantone color box

859
00:54:29,000 --> 00:54:32,280
Speaker 19:  from those, I'm not condoning this by the way, I'm just noting it's happening.

860
00:54:32,280 --> 00:54:35,920
Speaker 19:  But they can pull the, the Pantone color box from those versions of the

861
00:54:35,920 --> 00:54:39,840
Speaker 19:  software and then manually implant them into the updated version. It may

862
00:54:39,840 --> 00:54:42,880
Speaker 19:  not work forever, it may not work well but it is what people are doing in

863
00:54:42,880 --> 00:54:45,740
Speaker 19:  order to try and avoid paying Pantone a monthly subscription.

864
00:54:45,740 --> 00:54:49,520
Speaker 2:  The other thing we would need here is a better version of this

865
00:54:49,520 --> 00:54:52,400
Speaker 2:  kind of standard system. Cause I think part of the reason this story is interesting

866
00:54:52,400 --> 00:54:55,400
Speaker 2:  to me is this is the kind of thing that has come up over and over across

867
00:54:55,400 --> 00:54:58,680
Speaker 2:  a lot of tech is like everybody looks around and it's like, oh we've relied

868
00:54:58,680 --> 00:55:02,560
Speaker 2:  essentially on a for-profit company to provide what we see

869
00:55:02,560 --> 00:55:05,520
Speaker 2:  as sort of a necessary service across a lot of the things that we use. And

870
00:55:05,520 --> 00:55:08,560
Speaker 2:  then that company turns around and says, oh we'd like to make more money.

871
00:55:08,560 --> 00:55:12,480
Speaker 2:  And everybody goes, well that sucks but also fair enough. Like that's

872
00:55:12,480 --> 00:55:16,400
Speaker 2:  what they do. And it seems to me like if I'm an enterprising

873
00:55:16,400 --> 00:55:19,880
Speaker 2:  designer right now with some like, I don't know, coding skills and time in

874
00:55:19,880 --> 00:55:23,640
Speaker 2:  my hands, like somebody should start building like open source color repositories

875
00:55:23,640 --> 00:55:27,560
Speaker 2:  that start to replace some of this stuff in more functional accessible

876
00:55:27,560 --> 00:55:29,860
Speaker 2:  ways. Like is that, is that a thing? Is that happening?

877
00:55:29,860 --> 00:55:32,880
Speaker 19:  Yep, that that's happening. There are other kind of color books whereas

878
00:55:32,880 --> 00:55:36,480
Speaker 19:  we, we consider Pantone to be the industry standard because of its longevity

879
00:55:36,480 --> 00:55:39,640
Speaker 19:  and how many people already use the service, how many printers are already

880
00:55:39,640 --> 00:55:43,120
Speaker 19:  calibrated to use Pantone inks. There are other color books available, they're

881
00:55:43,120 --> 00:55:46,960
Speaker 19:  just not as popular. You've got like focal tone, true match and interesting

882
00:55:46,960 --> 00:55:50,870
Speaker 19:  development recently there's an artist called Stuart Sample who has

883
00:55:50,870 --> 00:55:54,600
Speaker 19:  kind of gained no notoriety for beefing with another artist, Anish Kapur

884
00:55:54,600 --> 00:55:58,150
Speaker 19:  over gatekeeping art. He developed a paint called

885
00:55:58,150 --> 00:56:02,090
Speaker 19:  like the Black Is Black and then sold that because Anish Kapur

886
00:56:02,090 --> 00:56:05,760
Speaker 19:  notably painted the rights to Van to Black at the time and he didn't think

887
00:56:05,760 --> 00:56:09,720
Speaker 19:  that was fair. He's now done a similar thing where he has created a open

888
00:56:09,720 --> 00:56:13,200
Speaker 19:  source color book of his own and he is making it unavailable for

889
00:56:13,200 --> 00:56:17,040
Speaker 19:  Adobe and Panton as some kind of like protesting that. But it, it's quite

890
00:56:17,040 --> 00:56:20,240
Speaker 19:  easy to develop these, these color standards. It's getting everyone else

891
00:56:20,240 --> 00:56:22,040
Speaker 19:  to use and that's the hard part.

892
00:56:22,040 --> 00:56:24,680
Speaker 2:  Yeah. Kristen, what are, are you seeing? What do you make of all the other

893
00:56:24,680 --> 00:56:26,030
Speaker 2:  stuff that's out there right now?

894
00:56:26,030 --> 00:56:29,320
Speaker 18:  I think the most important thing here is to remember that Pantone is only

895
00:56:29,320 --> 00:56:32,960
Speaker 18:  for print products. So I think sometimes like the confusion over how

896
00:56:32,960 --> 00:56:36,080
Speaker 18:  colors can be used. Like if you're not designing for print, this is kind

897
00:56:36,080 --> 00:56:39,600
Speaker 18:  of like a totally irrelevant thing. Which is I think why because fewer and

898
00:56:39,600 --> 00:56:43,390
Speaker 18:  fewer people are designing for print products over time. I thinks probably

899
00:56:43,390 --> 00:56:47,200
Speaker 18:  that would be my guess is why Panton is now trying to gather more

900
00:56:47,200 --> 00:56:50,920
Speaker 18:  money from the people who are using and printing that way. I mean Panton

901
00:56:50,920 --> 00:56:54,880
Speaker 18:  also similar to Adobe has a lot of piloting. There's a lot of fake panton

902
00:56:54,880 --> 00:56:58,170
Speaker 18:  ink out there that's being used at various printers across the world.

903
00:56:58,170 --> 00:57:02,160
Speaker 2:  Do you see any of these kind of new color books and tools out there

904
00:57:02,160 --> 00:57:05,520
Speaker 2:  that have a chance of being like just said the ones that everybody actually

905
00:57:05,520 --> 00:57:08,160
Speaker 2:  latches onto? Cause that seems like, like you're saying, the biggest problem

906
00:57:08,160 --> 00:57:11,600
Speaker 2:  here is that we need one that everybody looks at and goes, okay, this is

907
00:57:11,600 --> 00:57:15,150
Speaker 2:  now the one. And as we've seen, that's a very hard thing to do.

908
00:57:15,150 --> 00:57:18,000
Speaker 18:  I mean the, the hardest thing is to, as Jess mentioned, is to get all of

909
00:57:18,000 --> 00:57:21,600
Speaker 18:  the printers to stock all of this ink. I mean printers across the world

910
00:57:21,600 --> 00:57:25,120
Speaker 18:  are using this as the standard. So it's about kind of like that shift

911
00:57:25,120 --> 00:57:28,950
Speaker 18:  happening in a, in an industry that is not often quick to make changes.

912
00:57:28,950 --> 00:57:32,080
Speaker 2:  Yeah. As we've discussed before on this podcast, relying on the printing

913
00:57:32,080 --> 00:57:35,810
Speaker 2:  industry to catch up to things really fast is a, a recipe for disaster. Yeah.

914
00:57:35,810 --> 00:57:39,680
Speaker 2:  Is there a version of this in the digital world? Like as we move toward

915
00:57:39,680 --> 00:57:43,440
Speaker 2:  digital stuff? Like do do hex codes and RGB sliders solve this problem

916
00:57:43,440 --> 00:57:47,320
Speaker 2:  for people who work digitally native? Or are we due for another one of

917
00:57:47,320 --> 00:57:48,550
Speaker 2:  these at some point in the future?

918
00:57:48,550 --> 00:57:51,680
Speaker 18:  I don't know. I mean the main most important thing to remember I think is

919
00:57:51,680 --> 00:57:54,800
Speaker 18:  to understand the distinction between RGB and C M Y K if you're working

920
00:57:54,800 --> 00:57:58,000
Speaker 18:  on the computer. Because you're basically only gonna be working in rgb,

921
00:57:58,000 --> 00:58:01,280
Speaker 18:  which is using hex codes. So those colors will continue to exist. Those

922
00:58:01,280 --> 00:58:04,600
Speaker 18:  are just codes, you know, there's nothing, there's no ink attached to those

923
00:58:04,600 --> 00:58:05,900
Speaker 18:  things. To those colors.

924
00:58:05,900 --> 00:58:09,040
Speaker 2:  And that seems like good news, right? That's a hard thing for someone to

925
00:58:09,040 --> 00:58:12,520
Speaker 2:  decide that they own and take away from you. It's just a bunch of six digit.

926
00:58:12,520 --> 00:58:16,080
Speaker 18:  Yeah, I suppose unless Adobe was like, if you use more than seven colors

927
00:58:16,080 --> 00:58:19,920
Speaker 18:  you have to pay us additional 9 99 a month or something like that.

928
00:58:19,920 --> 00:58:23,710
Speaker 2:  Don't give them ideas, Kristen, don't give them ideas.

929
00:58:23,710 --> 00:58:27,560
Speaker 2:  Jess, what's your sense of where this goes next? This fight is, I think it's

930
00:58:27,560 --> 00:58:30,280
Speaker 2:  a really interesting one because like Kristen's saying this applies to a

931
00:58:30,280 --> 00:58:32,920
Speaker 2:  smaller and smaller group of people, but this is the kind of thing that a

932
00:58:32,920 --> 00:58:36,280
Speaker 2:  lot of people care really deeply about and Adobe has already been under fire

933
00:58:36,280 --> 00:58:40,120
Speaker 2:  for years for what I think people perceive as like nickling and diving people

934
00:58:40,120 --> 00:58:43,970
Speaker 2:  out of a lot of money for the tools they need to do their jobs. Like

935
00:58:43,970 --> 00:58:47,720
Speaker 2:  is this coming to a head in a meaningful way? Is this just the

936
00:58:47,720 --> 00:58:50,790
Speaker 2:  cost of progress? Like where, where do you think we go from here?

937
00:58:50,790 --> 00:58:54,000
Speaker 19:  I don't necessarily, as much as I'd love to say that I think that something

938
00:58:54,000 --> 00:58:57,400
Speaker 19:  is gonna change in the next few years, I think that everything is so standardized

939
00:58:57,400 --> 00:59:01,160
Speaker 19:  at this point and the captive market that this affects is so

940
00:59:01,160 --> 00:59:04,960
Speaker 19:  small and to an extent kind of so able to

941
00:59:04,960 --> 00:59:08,880
Speaker 19:  absorb those additional costs that it's not really gonna affect the industry

942
00:59:08,880 --> 00:59:12,560
Speaker 19:  as a whole. So like illustrators and photographers aren't really gonna notice

943
00:59:12,560 --> 00:59:15,000
Speaker 19:  that the Pantone colors are disappearing cuz they simply don't need to use

944
00:59:15,000 --> 00:59:18,180
Speaker 19:  them. But if you own a huge manufacturing business and you work with several

945
00:59:18,180 --> 00:59:22,000
Speaker 19:  big notable brands, you need to use Coca-Cola Red, you need to use

946
00:59:22,000 --> 00:59:25,880
Speaker 19:  like whatever like brand specific colors that additional subscription charge

947
00:59:25,880 --> 00:59:29,200
Speaker 19:  every month on top of what you inevitably are already paying for their,

948
00:59:29,200 --> 00:59:32,920
Speaker 19:  their physical color books, watches and like any color matching systems

949
00:59:32,920 --> 00:59:36,800
Speaker 19:  that they provide is negligible and it's frustrating that companies

950
00:59:36,800 --> 00:59:40,760
Speaker 19:  will have to absorb those costs. But it, I think Adobe and Panton, as much

951
00:59:40,760 --> 00:59:43,760
Speaker 19:  as they're beefing with each other, they are aware that the people that

952
00:59:43,760 --> 00:59:47,120
Speaker 19:  this effects can afford to absorb that. So unless it starts to

953
00:59:47,120 --> 00:59:50,960
Speaker 19:  affect the vast majority of creatives, we're not gonna see any kind of difference

954
00:59:50,960 --> 00:59:54,560
Speaker 19:  here. There are kind of discussions of small manufacturing business that

955
00:59:54,560 --> 00:59:58,490
Speaker 19:  are moving over to using different like color matching systems like CM

956
00:59:58,490 --> 01:00:01,880
Speaker 19:  yk because with certain stuff you can get it

957
01:00:01,880 --> 01:00:05,320
Speaker 19:  close, there are inconsistencies, you can't necessarily

958
01:00:05,320 --> 01:00:08,840
Speaker 19:  guarantee that you're gonna have consistent results with whatever you're

959
01:00:08,840 --> 01:00:12,120
Speaker 19:  printing. But if you are a one time printer and you just need to, I dunno,

960
01:00:12,120 --> 01:00:14,800
Speaker 19:  make a couple of screen printed t-shirts, then you can get something as

961
01:00:14,800 --> 01:00:18,030
Speaker 19:  close as you need to. But in terms of like big, yeah, big branded projects,

962
01:00:18,030 --> 01:00:20,620
Speaker 19:  I don't think you're moving away from Panto and anytime soon

963
01:00:20,620 --> 01:00:24,220
Speaker 18:  And a lot of times you're printing with C M Y K and Pantone

964
01:00:24,220 --> 01:00:27,360
Speaker 18:  for the example for the Homeland project that we talked about, you know,

965
01:00:27,360 --> 01:00:30,320
Speaker 18:  that's how we print it. That's how I printed it. And you know, when I worked

966
01:00:30,320 --> 01:00:33,200
Speaker 18:  in books, when I worked at other magazines before coming to the Verge, you

967
01:00:33,200 --> 01:00:35,680
Speaker 18:  would use the Pantone if you needed to accomplish something you couldn't

968
01:00:35,680 --> 01:00:39,160
Speaker 18:  in C M Y K. So that's something like Jess said, that's a really recognizable

969
01:00:39,160 --> 01:00:42,440
Speaker 18:  color, like a Coca-Cola red, that's something that's metallic, that's something

970
01:00:42,440 --> 01:00:45,200
Speaker 18:  that's neon or that's something that's just like really bright and vibrant.

971
01:00:45,200 --> 01:00:48,800
Speaker 18:  Those are things that you cannot get with C Y K, but you can maybe print,

972
01:00:48,800 --> 01:00:51,520
Speaker 18:  if you were printing a poster, you might need some neon elements that you

973
01:00:51,520 --> 01:00:54,640
Speaker 18:  print and Pantone and then the rest is C y K. So a lot of times, so that

974
01:00:54,640 --> 01:00:57,040
Speaker 18:  means you're running something through the press five times rather than

975
01:00:57,040 --> 01:00:57,500
Speaker 18:  four.

976
01:00:57,500 --> 01:01:00,600
Speaker 19:  The inclusion, the brands is actually quite important here as well because

977
01:01:00,600 --> 01:01:04,400
Speaker 19:  whereas the system like CMI K can replicate those colors

978
01:01:04,400 --> 01:01:07,920
Speaker 19:  accurately. Pantone will actually let brands patent their branded colors

979
01:01:07,920 --> 01:01:10,920
Speaker 19:  so in Pantone so that no one else can use it and provided that it's something

980
01:01:10,920 --> 01:01:14,560
Speaker 19:  on a gamut that none of the other color matching systems can reach. No one

981
01:01:14,560 --> 01:01:17,560
Speaker 19:  else can use that Coca-Cola red or like whatever other branded color you

982
01:01:17,560 --> 01:01:21,240
Speaker 19:  have. So it's within a brand's best interest to keep Pantone as a color

983
01:01:21,240 --> 01:01:24,960
Speaker 19:  standard. Like yes, you could use everything else, but there aren't

984
01:01:24,960 --> 01:01:27,160
Speaker 19:  necessarily brands that would want you to do that.

985
01:01:27,160 --> 01:01:30,560
Speaker 2:  Interesting. All right. Well you have both succeeded in successfully convincing

986
01:01:30,560 --> 01:01:34,120
Speaker 2:  me that colors don't make sense and aren't real and no one actually knows

987
01:01:34,120 --> 01:01:36,240
Speaker 2:  anything about what a color actually is.

988
01:01:36,240 --> 01:01:39,560
Speaker 18:  That's true. Actually, no one knows anything about color. Yeah,

989
01:01:39,560 --> 01:01:40,990
Speaker 2:  Apparently.

990
01:01:40,990 --> 01:01:41,480
Speaker 18:  Yeah.

991
01:01:41,480 --> 01:01:44,240
Speaker 2:  All right, well thank you both. I, I appreciate this was really fun.

992
01:01:44,240 --> 01:01:44,720
Speaker 19:  Thank

993
01:01:44,720 --> 01:01:47,060
Speaker 18:  You. Cool. I'm gonna send you some color books, David.

994
01:01:47,060 --> 01:01:50,200
Speaker 2:  All right, that's it for the Vergecast today. As always, thank you so much

995
01:01:50,200 --> 01:01:53,200
Speaker 2:  for listening. There is tons more coverage on everything we talked

996
01:01:53,200 --> 01:01:56,440
Speaker 2:  about@verge.com. It's website, it's cool, you should go check it out. You

997
01:01:56,440 --> 01:02:00,240
Speaker 2:  can follow all of us on Twitter. Jess is zombie underscore retch.

998
01:02:00,240 --> 01:02:04,020
Speaker 2:  Kristin is Kristin Radke. James is JJ Vincent,

999
01:02:04,020 --> 01:02:07,550
Speaker 2:  and I'm Pierce. This show is produced by Andrew Marino and Liam James

1000
01:02:07,550 --> 01:02:11,200
Speaker 2:  Nore. Donovan is our executive producer and Brooke Miners is our editorial

1001
01:02:11,200 --> 01:02:14,440
Speaker 2:  director of Audio Vergecast is Verge Production and part of the Fox Media

1002
01:02:14,440 --> 01:02:18,360
Speaker 2:  podcast network. If you have thoughts, feedback, feelings, ideas about

1003
01:02:18,360 --> 01:02:21,760
Speaker 2:  colors or just six digit hex codes you wanna send me, you can always email

1004
01:02:21,760 --> 01:02:25,680
Speaker 2:  vergecast@theverge.com. And if you have tech questions, call the hotline

1005
01:02:25,680 --> 01:02:29,480
Speaker 2:  eight sixty six Verge 11, send us all your big thoughts and

1006
01:02:29,480 --> 01:02:32,200
Speaker 2:  questions about all things tech. We're gonna do a bunch more hotline between

1007
01:02:32,200 --> 01:02:35,000
Speaker 2:  now and the end of the year, so please keep calling with all your questions.

1008
01:02:35,000 --> 01:02:38,640
Speaker 2:  We'll be back on Friday to discuss Elon and Twitter because that just keeps

1009
01:02:38,640 --> 01:02:42,480
Speaker 2:  happening. Plus the ongoing crypto craziness, new GPUs, and

1010
01:02:42,480 --> 01:02:44,440
Speaker 2:  much more. We'll see you then. Rock and roll.

