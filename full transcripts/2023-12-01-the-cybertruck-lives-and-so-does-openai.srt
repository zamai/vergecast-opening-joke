1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: ebdb0b61-dd18-49c3-865b-debfdbbff869
Status: Done
Stage: Done
Title: The Cybertruck lives, and so does OpenAI
Audio URL: https://jfe93e.s3.amazonaws.com/-167697093418164831/1256546866988706350/s93290-US-5077s-1701438838.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and David Pierce discuss the Tesla Cybertruck event, the latest with OpenAI leadership, and what Elon Musk and Bob Iger said at Dealbook Summit 2023.

Getting close — but not too close — to the Tesla Cybertruck

Tesla Cybertruck delivery event: Elon Musk hands over the first trucks to customers

Elon Musk tells advertisers: ‘Go fuck yourself’

Sam Altman is back, so what’s next for OpenAI and ChatGPT?


Interview: Sam Altman on being fired and rehired by OpenAI 


ChatGPT is winning the future — but what future is that? 

Ikea debuts a trio of affordable smart home sensors

DealBook Summit 2023: Elon Musk, Bob Iger, and more

How to find your 2023 Spotify Wrapped


Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (7 ads detected)

2
00:01:21,595 --> 00:01:25,145
Speaker 5:  Hello and Welcome To Vergecast, America's number one source of Cybertruck

3
00:01:25,145 --> 00:01:27,825
Speaker 5:  wiper news. And it's all thanks to readers like you.

4
00:01:28,165 --> 00:01:32,105
Speaker 6:  That's actually true. Now I think like, I think that was, we spoke into existence

5
00:01:32,335 --> 00:01:35,865
Speaker 6:  that we are the world's number one source of Cybertruck news.

6
00:01:35,935 --> 00:01:38,865
Speaker 5:  Yeah. Yeah. People believe us. The thing is, we still dunno the answer to

7
00:01:38,865 --> 00:01:39,305
Speaker 5:  this question.

8
00:01:39,685 --> 00:01:40,065
Speaker 7:  Not at

9
00:01:40,065 --> 00:01:42,745
Speaker 5:  All. We'll get into it though. I'm your friend Neli. David Pierce is here.

10
00:01:42,875 --> 00:01:43,225
Speaker 6:  Hello?

11
00:01:43,775 --> 00:01:44,705
Speaker 5:  Alex. Cranz is here.

12
00:01:45,005 --> 00:01:48,945
Speaker 7:  I'm your friend who wishes they'd used a crossbow, like checked it with a

13
00:01:48,945 --> 00:01:52,505
Speaker 7:  crossbow. I wanna see how does it with Stan Crossbo fire. Yeah. The

14
00:01:52,505 --> 00:01:52,985
Speaker 7:  Cybertruck.

15
00:01:53,655 --> 00:01:54,905
Speaker 5:  Well, we know the answer now.

16
00:01:55,405 --> 00:01:57,225
Speaker 7:  Do not cross bows. Just all

17
00:01:57,225 --> 00:01:59,985
Speaker 5:  The Oh, oh, I see. Just Yeah. We know a regular bone arrow.

18
00:01:59,985 --> 00:02:02,665
Speaker 7:  Yeah. I want like cross bows. Catapults,

19
00:02:03,015 --> 00:02:06,545
Speaker 6:  Alex. one of the reasons that I know that like I like you and we're friends

20
00:02:06,685 --> 00:02:10,625
Speaker 6:  is that no part of me would be surprised if that was a reveal that you're

21
00:02:10,625 --> 00:02:14,545
Speaker 6:  like sick with a cross crossbo. Yeah. Like nothing. I wouldn't

22
00:02:14,595 --> 00:02:14,945
Speaker 6:  blink

23
00:02:15,285 --> 00:02:15,625
Speaker 3:  If you

24
00:02:16,175 --> 00:02:19,305
Speaker 6:  Just like pulled a crossbow out from underneath the table and were like,

25
00:02:19,305 --> 00:02:19,905
Speaker 6:  just like this.

26
00:02:19,975 --> 00:02:23,265
Speaker 7:  Yeah. Yeah. That's, that's what I do on my, my off time. Yeah. Is practice

27
00:02:23,265 --> 00:02:23,945
Speaker 7:  with my crossbow

28
00:02:24,385 --> 00:02:25,665
Speaker 5:  Axe throwing crossbows. I

29
00:02:25,665 --> 00:02:28,545
Speaker 7:  Am actually really good with it. Axe throwing like the axe throwing knife,

30
00:02:28,705 --> 00:02:31,625
Speaker 7:  throwing terrible axe throwing. It's like I got it

31
00:02:32,245 --> 00:02:34,745
Speaker 5:  All. right. Well, let's explain why we're talking about this.

32
00:02:35,815 --> 00:02:39,625
Speaker 5:  There's quite a lot of news this week, but we are coming to you

33
00:02:40,225 --> 00:02:43,345
Speaker 5:  directly after the Tesla Cybertruck launch event,

34
00:02:44,155 --> 00:02:47,145
Speaker 5:  which was very odd. That itself followed

35
00:02:48,055 --> 00:02:51,745
Speaker 5:  Elon Musk at the Dealbook conference yesterday, which was even

36
00:02:51,815 --> 00:02:55,805
Speaker 5:  weirder. So we are literally live reacting

37
00:02:55,805 --> 00:02:59,715
Speaker 5:  to you post Cybertruck reveal in which we learned nothing.

38
00:02:59,865 --> 00:03:03,635
Speaker 5:  Yeah. I want to be very clear, it was a 25 minute

39
00:03:03,765 --> 00:03:07,545
Speaker 5:  event. It started 25 minutes late after

40
00:03:07,545 --> 00:03:11,345
Speaker 5:  playing an enormous amount of just ominous

41
00:03:11,575 --> 00:03:15,545
Speaker 5:  ambient music That was strange. Elon comes out,

42
00:03:15,575 --> 00:03:19,305
Speaker 5:  they, they go through the Cybertruck specs. Here's how much it can pull.

43
00:03:19,305 --> 00:03:22,585
Speaker 5:  It can pull more than a Ford F-150 Lightning and a Rivian and an F three

44
00:03:22,585 --> 00:03:25,545
Speaker 5:  50 diesel. Great. Cool. They threw a baseball at it

45
00:03:26,525 --> 00:03:26,945
Speaker 7:  Poorly

46
00:03:27,445 --> 00:03:29,185
Speaker 5:  The first time friends missed.

47
00:03:29,445 --> 00:03:31,185
Speaker 7:  Yes. And then the second time he was just like,

48
00:03:31,605 --> 00:03:34,625
Speaker 5:  He was, it was very weak. Yeah. But the windows didn't shatter this time.

49
00:03:34,625 --> 00:03:35,265
Speaker 5:  They were happy about it.

50
00:03:36,455 --> 00:03:39,665
Speaker 7:  Well, yeah. If you're throwing it that like he threw it like, I throw a ball.

51
00:03:39,805 --> 00:03:41,945
Speaker 7:  I'm terrible at throwing a ball. And he was just like,

52
00:03:42,625 --> 00:03:46,505
Speaker 6:  I sincerely believe that if you threw a baseball at that speed at

53
00:03:47,005 --> 00:03:49,185
Speaker 6:  my car's window, it would not break.

54
00:03:49,185 --> 00:03:51,785
Speaker 5:  That's kind of what I said. I was like, I've seen Honda Civics with Stan

55
00:03:51,785 --> 00:03:52,705
Speaker 5:  more impact than this.

56
00:03:52,705 --> 00:03:56,665
Speaker 6:  Yeah. And it was just so funny 'cause it was clearly that moment was

57
00:03:57,525 --> 00:04:01,185
Speaker 6:  it sort of set up to be like a big applause moment. 'cause it was

58
00:04:01,825 --> 00:04:05,265
Speaker 6:  I think feasted four years ago. Yeah. Which is nuts that this was four years

59
00:04:05,265 --> 00:04:08,545
Speaker 6:  ago. The first time this happened, Franz got up there and threw a ball at

60
00:04:08,585 --> 00:04:11,705
Speaker 6:  a supposedly baseball proof window, which is a

61
00:04:11,705 --> 00:04:15,665
Speaker 5:  Thing be clear. He threw a metal ball at a supposedly bulletproof window.

62
00:04:16,265 --> 00:04:19,785
Speaker 6:  Oh, okay. That's a good distinction. Yeah. And it, it

63
00:04:19,855 --> 00:04:20,465
Speaker 6:  shattered

64
00:04:20,565 --> 00:04:23,505
Speaker 5:  And then he did it again and the other window shattered. Yeah.

65
00:04:23,955 --> 00:04:25,985
Speaker 5:  Incredible moment in product history.

66
00:04:26,365 --> 00:04:30,345
Speaker 6:  Oh, it was fantastic. It was one of the great like live demos or a disaster

67
00:04:30,345 --> 00:04:34,185
Speaker 6:  moments. I really enjoyed it. But he sets this up as this

68
00:04:34,185 --> 00:04:37,785
Speaker 6:  sort of redemptive moment four years later and like kudos to them for,

69
00:04:38,005 --> 00:04:41,625
Speaker 6:  you know, laughing about this, but then didn't rerun the test.

70
00:04:41,945 --> 00:04:45,785
Speaker 6:  Yeah. He had him lobb a baseball at the window and he

71
00:04:45,785 --> 00:04:49,545
Speaker 6:  missed the first time he hit like the underneath of the window and then

72
00:04:50,255 --> 00:04:53,305
Speaker 6:  Yeah, like Alex was saying, sort of gently lobbed it. Yeah,

73
00:04:53,365 --> 00:04:55,545
Speaker 5:  You can really tell this man did not want to do this thing.

74
00:04:55,765 --> 00:04:59,545
Speaker 6:  No. And it was this, the funniest part was they got to the end and he didn't

75
00:04:59,545 --> 00:05:03,345
Speaker 6:  get any of the like applause breaks or laughs or anything he was hoping for.

76
00:05:03,345 --> 00:05:07,265
Speaker 6:  And he was just like, it's good is my point. It was

77
00:05:07,265 --> 00:05:07,745
Speaker 6:  like All. right.

78
00:05:08,405 --> 00:05:12,025
Speaker 7:  And the bulb bounced on the ground at one point. Very, very like higher than

79
00:05:12,025 --> 00:05:15,545
Speaker 7:  I would expect a baseball to bounce. So I'm, I'm, I'm, I'm now a ball

80
00:05:15,545 --> 00:05:18,625
Speaker 5:  Truthfully. Well, there are now 10 owners of a Cybertruck. We have been told

81
00:05:18,695 --> 00:05:18,985
Speaker 5:  Yeah,

82
00:05:19,565 --> 00:05:19,785
Speaker 7:  At

83
00:05:19,785 --> 00:05:23,545
Speaker 5:  Least 10. They were not allowed to drive their own Cybertruck away. So the

84
00:05:23,545 --> 00:05:26,385
Speaker 5:  event happens, we, and we can talk about all the specs and, and whatever.

85
00:05:26,405 --> 00:05:29,025
Speaker 5:  And we've got some pricing now for the Cybertruck, which is interesting.

86
00:05:29,805 --> 00:05:33,345
Speaker 5:  But the event ends with Elon one by one

87
00:05:33,345 --> 00:05:37,305
Speaker 5:  shaking a person's hand and putting them in a Cybertruck in the

88
00:05:37,305 --> 00:05:39,625
Speaker 5:  passenger seat and having them driven away.

89
00:05:39,805 --> 00:05:43,465
Speaker 6:  Did you guys recognize anybody? I saw Alexis Hanian was the one person I

90
00:05:43,745 --> 00:05:46,625
Speaker 6:  recognized the, the Reddit co-founder and husband of Serena Williams.

91
00:05:47,525 --> 00:05:50,705
Speaker 6:  I'm sure there were other people in this world that I should have known.

92
00:05:50,885 --> 00:05:53,065
Speaker 6:  Did you guys see any faces you recognize other than

93
00:05:53,125 --> 00:05:55,585
Speaker 5:  No. I, I didn't, I didn't catch Alexis. He took a Cybertruck.

94
00:05:55,585 --> 00:05:59,245
Speaker 6:  Yeah. Alexis with like big m like wolverine, mutton chops

95
00:06:00,145 --> 00:06:02,125
Speaker 6:  got into a Cybertruck. It was, it was a cool

96
00:06:02,125 --> 00:06:05,045
Speaker 7:  Look. Do you think they were real or do you think he was like in disguise

97
00:06:05,465 --> 00:06:07,885
Speaker 6:  Sneaky Alexis getting his Cybertruck?

98
00:06:08,475 --> 00:06:08,765
Speaker 7:  Yeah.

99
00:06:09,235 --> 00:06:12,925
Speaker 5:  What, what I caught was that after the second one, Elon was

100
00:06:13,365 --> 00:06:15,805
Speaker 5:  visibly annoyed that he had to keep shaking people's hands.

101
00:06:16,245 --> 00:06:17,205
Speaker 6:  People wanted hugs.

102
00:06:17,305 --> 00:06:20,605
Speaker 5:  And he said on the camera, well it's a delivery event because I think he

103
00:06:20,605 --> 00:06:24,325
Speaker 5:  knew that people were watching a live stream of just 10 people

104
00:06:24,325 --> 00:06:27,885
Speaker 5:  getting their hand shook and driven away. And he said it like three more

105
00:06:27,885 --> 00:06:30,605
Speaker 5:  times. He's like, well this is what's gonna keep happening. And then he tried

106
00:06:30,605 --> 00:06:34,525
Speaker 5:  to end it early after nine I think. He was like, well, we're

107
00:06:34,525 --> 00:06:38,125
Speaker 5:  done. And someone was like, there's one more. And he had to shake one more

108
00:06:38,125 --> 00:06:41,285
Speaker 5:  hand and these people were getting rushed off stage. I think only one family

109
00:06:41,345 --> 00:06:45,245
Speaker 5:  got like the picture. Yeah, whatever. It doesn't matter. It's,

110
00:06:45,425 --> 00:06:47,125
Speaker 5:  it was an odd event. But

111
00:06:47,195 --> 00:06:49,965
Speaker 6:  Wait, did you notice that nobody could figure out how to get into the car?

112
00:06:49,985 --> 00:06:51,205
Speaker 6:  Yes, this was my favorite part of

113
00:06:51,205 --> 00:06:51,845
Speaker 7:  The whole thing. That was

114
00:06:51,845 --> 00:06:54,965
Speaker 6:  Great. He kept having to tell people. I think almost every single person

115
00:06:55,025 --> 00:06:58,925
Speaker 6:  who got into the car had trouble figuring out how to open the passenger

116
00:06:58,925 --> 00:07:02,885
Speaker 6:  side door to get in. And at first he, he was saying

117
00:07:02,885 --> 00:07:05,245
Speaker 6:  things like, oh, you just press the, and then they would sort of figure it

118
00:07:05,245 --> 00:07:08,765
Speaker 6:  out and then by the end he just sort of reflexively was going, just press

119
00:07:08,765 --> 00:07:11,365
Speaker 6:  the button on the top as they're walking over to the thing and they would

120
00:07:11,365 --> 00:07:14,725
Speaker 6:  press it and get in. And it's just, this just makes me so happy. Like the

121
00:07:14,725 --> 00:07:18,005
Speaker 6:  Tesla has lots of interesting ideas about cars and just

122
00:07:18,155 --> 00:07:22,005
Speaker 6:  exclusively bad ideas about how to get into them. And it

123
00:07:22,005 --> 00:07:25,845
Speaker 6:  just makes me happy that the Cybertruck continues that pace.

124
00:07:25,875 --> 00:07:27,285
Speaker 6:  Yeah. but it was great. It

125
00:07:27,285 --> 00:07:31,125
Speaker 5:  Was great. And hopefully we'll see these actually around, what we did

126
00:07:31,125 --> 00:07:34,965
Speaker 5:  not learn was whether the wiper blade is one blade or two in a line.

127
00:07:35,525 --> 00:07:36,085
Speaker 7:  Shameful.

128
00:07:36,405 --> 00:07:40,005
Speaker 5:  I haven't received so many photos now and I

129
00:07:40,215 --> 00:07:43,125
Speaker 5:  think I've been clear about my theory. Yeah,

130
00:07:43,305 --> 00:07:45,605
Speaker 7:  You've assigned someone to go even I,

131
00:07:45,605 --> 00:07:49,245
Speaker 5:  We sent someone to the Tesla showroom. Mandy Hawkins went with our

132
00:07:49,245 --> 00:07:53,205
Speaker 5:  photographer Amelia kras. I told Andy that I would pay his bail if he got

133
00:07:53,445 --> 00:07:56,645
Speaker 5:  arrested for lifting the wiper up and he chickened out fine.

134
00:07:57,585 --> 00:08:01,005
Speaker 5:  But I've now people have just like reinterpreted what they think I mean by,

135
00:08:01,005 --> 00:08:04,805
Speaker 5:  is it two wipers? One person wrote me an email that said, I I prefer the

136
00:08:04,805 --> 00:08:07,965
Speaker 5:  two wiper design on a regular card. I was like, that's not,

137
00:08:08,395 --> 00:08:09,165
Speaker 7:  Most people do.

138
00:08:11,185 --> 00:08:13,845
Speaker 5:  So does the free market like I don Dunno what, what to tell you.

139
00:08:15,435 --> 00:08:19,245
Speaker 6:  Wait, but le let's, let's just take this back Neli. What is, what

140
00:08:19,245 --> 00:08:22,085
Speaker 6:  is your current belief about the Cybertruck wiper? Because like you said,

141
00:08:22,085 --> 00:08:26,045
Speaker 6:  we've gotten, I would say like the 10 best pictures

142
00:08:26,055 --> 00:08:29,965
Speaker 6:  we've ever gotten of the Cybertruck wiper in the last eight days.

143
00:08:30,155 --> 00:08:33,165
Speaker 6:  Most of them thanks to Verge Cat's listeners. So thank you to everyone who

144
00:08:33,165 --> 00:08:36,805
Speaker 6:  sent those in. What is your current belief about the

145
00:08:36,805 --> 00:08:37,245
Speaker 6:  Cybertruck

146
00:08:37,245 --> 00:08:40,445
Speaker 5:  Wiper? I think it might be one wiper, I think it might be one extremely weird

147
00:08:40,545 --> 00:08:44,085
Speaker 5:  custom wiper blade. I'd not, I don't know for sure.

148
00:08:45,005 --> 00:08:48,925
Speaker 5:  I, I've now seen reporting of other people who have seen the, the truck

149
00:08:49,345 --> 00:08:53,205
Speaker 5:  and they in YouTube videos like Tesla fans confidently saying it's two

150
00:08:53,205 --> 00:08:56,725
Speaker 5:  wipers in a line. And I've asked, do you know, or do you,

151
00:08:57,265 --> 00:09:00,085
Speaker 5:  do you know and got no

152
00:09:00,365 --> 00:09:00,605
Speaker 7:  Response.

153
00:09:03,145 --> 00:09:07,125
Speaker 5:  So I, I don't know. But now that someone owns, I'm sure Alexis has like an

154
00:09:07,205 --> 00:09:11,165
Speaker 5:  NDA about this truck, but I'm definitely gonna write him a note and be

155
00:09:11,165 --> 00:09:14,925
Speaker 5:  like, can you pick up wiper? Because

156
00:09:15,125 --> 00:09:19,045
Speaker 5:  I no one knows. No one knows. And it is the, well someone knows the most

157
00:09:19,125 --> 00:09:22,965
Speaker 5:  enduring mystery about this truck. Like, did Tesla make a very

158
00:09:23,075 --> 00:09:26,765
Speaker 5:  fast triangle? They did. Is it so fast

159
00:09:26,955 --> 00:09:30,885
Speaker 5:  that it can out accelerate a Porsche nine 11 while towing

160
00:09:31,005 --> 00:09:34,245
Speaker 5:  a Porsche nine 11? Yes. That's very cool.

161
00:09:34,945 --> 00:09:37,805
Speaker 5:  Is it bulletproof? Who knows sort of

162
00:09:38,025 --> 00:09:38,605
Speaker 7:  The bottom is

163
00:09:38,745 --> 00:09:42,525
Speaker 5:  The bot. We saw someone firing a, a Tommy gun into the side of it

164
00:09:42,525 --> 00:09:45,765
Speaker 5:  during this event. So if you're a 1920s mobster, like you're, you're good

165
00:09:45,765 --> 00:09:49,565
Speaker 5:  to go. Can it withstand Joe Rogan firing a metal tipped armor

166
00:09:49,685 --> 00:09:53,525
Speaker 5:  piercing arrow at the side? Sure. Can we know that answer? How

167
00:09:53,525 --> 00:09:57,405
Speaker 5:  do the wipers work remains a deep

168
00:09:57,505 --> 00:10:01,485
Speaker 5:  and defining mystery about this vehicle. We've seen photos like

169
00:10:01,765 --> 00:10:05,565
Speaker 5:  overhead, photos of extremely muddy and dirty Teslas just a

170
00:10:05,565 --> 00:10:05,845
Speaker 7:  Huge,

171
00:10:06,665 --> 00:10:10,445
Speaker 5:  And it's, there's just a huge dirty spot. I've seen videos

172
00:10:10,705 --> 00:10:14,605
Speaker 5:  of extremely muddy Teslas parked at hotels where it

173
00:10:14,605 --> 00:10:18,445
Speaker 5:  is obvious the passenger could not see very well and they've reached their

174
00:10:18,445 --> 00:10:22,125
Speaker 5:  hand over the side and wiped off the mud manually.

175
00:10:22,615 --> 00:10:26,205
Speaker 5:  There was an argument in our comments over whether or not the Ford

176
00:10:26,365 --> 00:10:30,325
Speaker 5:  F-150 actually wipes proportionally less of the

177
00:10:30,325 --> 00:10:33,925
Speaker 5:  windshield than the Cybertruck. And I had to be like, look,

178
00:10:33,985 --> 00:10:37,325
Speaker 5:  here's one thing I know for sure. Maybe the Cybertruck

179
00:10:37,725 --> 00:10:41,605
Speaker 5:  proportionally mathematically covers more of the windshield than the

180
00:10:41,605 --> 00:10:45,125
Speaker 5:  F-150. No passenger in my F-150

181
00:10:45,665 --> 00:10:49,605
Speaker 5:  has ever reached over the side and wiped off some more of the

182
00:10:49,605 --> 00:10:50,085
Speaker 5:  windshield.

183
00:10:53,185 --> 00:10:56,965
Speaker 5:  It it, it is just such a bonkers design problem that

184
00:10:56,965 --> 00:11:00,925
Speaker 5:  they've created for themselves and now it's here. So eventually the mystery

185
00:11:00,925 --> 00:11:04,485
Speaker 5:  will expire. Right. Eventually this will all be over and we'll know it's

186
00:11:04,765 --> 00:11:07,645
Speaker 5:  one wiper or two. I just think it's amazing

187
00:11:08,925 --> 00:11:12,615
Speaker 5:  that for weeks now we've

188
00:11:12,615 --> 00:11:15,575
Speaker 5:  propelled a number of tech and car enthusiasts

189
00:11:16,405 --> 00:11:18,535
Speaker 5:  into not knowing because no one knows.

190
00:11:21,515 --> 00:11:22,375
Speaker 7:  I'm really excited.

191
00:11:24,395 --> 00:11:28,295
Speaker 5:  One thing we do know, we do know some pricing. So the base model,

192
00:11:28,295 --> 00:11:31,975
Speaker 5:  which is available in 2025. Oh oof.

193
00:11:32,235 --> 00:11:34,975
Speaker 5:  I'm just gonna point out we, we we're at the launch event

194
00:11:35,435 --> 00:11:36,455
Speaker 7:  In 2023. In

195
00:11:36,455 --> 00:11:39,975
Speaker 5:  2023. Yeah. The base model in 2025

196
00:11:40,635 --> 00:11:44,335
Speaker 5:  is $60,990, $61,000. That's

197
00:11:44,335 --> 00:11:48,015
Speaker 5:  significantly more than we thought. Right? It was supposed to be 49. So it's

198
00:11:48,225 --> 00:11:51,095
Speaker 5:  crept up about $10,000, $11,000.

199
00:11:51,275 --> 00:11:52,695
Speaker 7:  But inflation has like,

200
00:11:52,845 --> 00:11:53,805
Speaker 5:  Yeah, thanks Biden.

201
00:11:54,035 --> 00:11:54,765
Speaker 7:  Yeah, thanks Biden.

202
00:11:55,345 --> 00:11:58,605
Speaker 5:  That's, you know, you know he did it just so he could say inflation Biden

203
00:11:58,675 --> 00:12:02,565
Speaker 5:  made inflation happen. We'll get to that. That's gonna have a 250

204
00:12:02,595 --> 00:12:05,925
Speaker 5:  mile range. 6.5 seconds, zero to 60.

205
00:12:06,545 --> 00:12:10,085
Speaker 5:  The all-wheel drive model, which should deliver next year. It says delivery

206
00:12:10,105 --> 00:12:14,045
Speaker 5:  in 2024. By the way, big difference between available in 2025

207
00:12:14,105 --> 00:12:17,725
Speaker 5:  and delivery in 2024. I would tell you

208
00:12:18,055 --> 00:12:21,925
Speaker 5:  based on what we know of the model three and the cheap model three,

209
00:12:21,995 --> 00:12:23,365
Speaker 5:  just sort of never arriving,

210
00:12:23,985 --> 00:12:25,845
Speaker 7:  Go for the delivery in 2024. Model

211
00:12:25,955 --> 00:12:28,645
Speaker 5:  This, this base model railroad drive. Don't,

212
00:12:28,645 --> 00:12:29,325
Speaker 7:  Don't buy it. Just

213
00:12:29,325 --> 00:12:33,045
Speaker 5:  Buy any other, just be like whatever. I'll buy another truck. And if that

214
00:12:33,045 --> 00:12:33,885
Speaker 5:  thing ever comes out,

215
00:12:34,225 --> 00:12:38,205
Speaker 7:  You, you go, you just keep leasing trucks until the Cybertruck

216
00:12:38,405 --> 00:12:39,725
Speaker 7:  actually comes out and you can buy it. Yeah.

217
00:12:40,285 --> 00:12:43,205
Speaker 5:  This one is here to, to anchor the price for it's I Yeah.

218
00:12:43,425 --> 00:12:45,605
Speaker 7:  One of your three year periods, it'll finally be available.

219
00:12:45,975 --> 00:12:49,565
Speaker 5:  There was a meeting at Tesla where they were deciding what copy to put on

220
00:12:49,565 --> 00:12:53,405
Speaker 5:  the website and they put delivery in 2024 for the all-wheel drive

221
00:12:53,545 --> 00:12:57,125
Speaker 5:  and for the cyber beast. And then someone was like, what do we say about

222
00:12:57,125 --> 00:13:00,865
Speaker 5:  the base model? And they all looked at each other and brainstormed

223
00:13:00,865 --> 00:13:03,185
Speaker 5:  words possible in 2025.

224
00:13:05,585 --> 00:13:09,545
Speaker 5:  Apparently in 2025. And they went with available seemingly

225
00:13:09,565 --> 00:13:09,785
Speaker 7:  In

226
00:13:09,785 --> 00:13:13,105
Speaker 5:  2020. We don't. So then the, then the all wheel drive, which I think is the

227
00:13:13,105 --> 00:13:16,865
Speaker 5:  true base model, that's $80,000, 79, 990,

228
00:13:17,015 --> 00:13:20,785
Speaker 5:  delivered in 20 24, 340 miles range. 4.1

229
00:13:20,785 --> 00:13:24,705
Speaker 5:  seconds, zero to 60. You'll note they don't list the top speed on the

230
00:13:24,705 --> 00:13:28,585
Speaker 5:  illusory base model. But this one has 112 mile an hour top speed,

231
00:13:28,585 --> 00:13:32,385
Speaker 5:  600 horsepower, 7,400 pound feet of

232
00:13:32,385 --> 00:13:35,785
Speaker 5:  torque and 11,000 pounds of towing capacity. Just big silly truck numbers.

233
00:13:35,935 --> 00:13:37,745
Speaker 5:  Yeah. And then the cyber beast.

234
00:13:38,255 --> 00:13:42,065
Speaker 7:  Yeah, the beast. That was my brother's high school

235
00:13:42,385 --> 00:13:46,065
Speaker 7:  nickname. He got it as a tattoo. He no longer has that tattoo. Wow.

236
00:13:46,485 --> 00:13:47,465
Speaker 7:  We, we call him the

237
00:13:47,465 --> 00:13:49,665
Speaker 5:  Beast. Well now we're gonna call him the cyber beast.

238
00:13:50,765 --> 00:13:50,985
Speaker 7:  I'm

239
00:13:50,985 --> 00:13:53,825
Speaker 5:  Not doing that. This is a hundred thousand dollars 99. Nine 90.

240
00:13:55,265 --> 00:13:59,065
Speaker 5:  Slightly lower range. Three 20 miles much faster. 2.6 seconds.

241
00:13:59,225 --> 00:14:02,425
Speaker 5:  130 mile hour top speed. 845 horsepower,

242
00:14:03,045 --> 00:14:05,545
Speaker 5:  10 beast pound, feet of fork. I'm guessing

243
00:14:06,945 --> 00:14:10,715
Speaker 5:  it's hard to know. I'm guessing that this is the triple motor

244
00:14:10,855 --> 00:14:14,835
Speaker 5:  one. Okay. I'm guessing. I I don't know. There there are no specs. It doesn't

245
00:14:14,835 --> 00:14:16,315
Speaker 5:  quite tell you what. Oh,

246
00:14:16,315 --> 00:14:19,475
Speaker 7:  Do you think it's got one of those like mufflers? The speaker, the, the speakers

247
00:14:19,475 --> 00:14:20,435
Speaker 7:  and the mufflers? Well,

248
00:14:20,495 --> 00:14:23,435
Speaker 5:  Teslas make sounds, you know, they, they can do it. You can make like the

249
00:14:23,505 --> 00:14:23,795
Speaker 5:  Yeah,

250
00:14:23,795 --> 00:14:25,755
Speaker 7:  But the beast has like a special edition one.

251
00:14:25,955 --> 00:14:26,635
Speaker 5:  It's growling at

252
00:14:26,635 --> 00:14:27,475
Speaker 7:  You. Yeah. The whole time.

253
00:14:27,735 --> 00:14:30,435
Speaker 5:  If I know anything about Tesla, they will make it make growling sounds when

254
00:14:30,435 --> 00:14:34,115
Speaker 5:  it runs. So this is all of the new information that we have is

255
00:14:34,185 --> 00:14:37,845
Speaker 5:  this much specs and this much pricing. And then

256
00:14:38,035 --> 00:14:41,685
Speaker 5:  Elon being like it's good at truck stuff, which they really insisted upon.

257
00:14:41,935 --> 00:14:45,565
Speaker 5:  Right, right. It's good at work stuff. It's good at towing.

258
00:14:46,345 --> 00:14:50,015
Speaker 5:  At one point he was like, as you all know, a tractor

259
00:14:50,165 --> 00:14:52,805
Speaker 5:  pole is the most important metric for a truck.

260
00:14:53,385 --> 00:14:53,685
Speaker 7:  Is it?

261
00:14:54,165 --> 00:14:57,805
Speaker 5:  That's exactly. I was like, is it, is that, is it? And they

262
00:14:58,045 --> 00:15:02,005
Speaker 5:  demonstrated that a Tesla can pull a sled loaded down with weight

263
00:15:02,195 --> 00:15:03,685
Speaker 5:  much farther than other trucks.

264
00:15:03,945 --> 00:15:07,925
Speaker 7:  How is the braking? Like that's what I care about at a tractor pull.

265
00:15:08,205 --> 00:15:11,725
Speaker 7:  I don't do those, but if I'm pulling a trailer, I do care about like

266
00:15:11,865 --> 00:15:13,445
Speaker 5:  If you're pulling a tra an actual trailer.

267
00:15:13,515 --> 00:15:17,285
Speaker 7:  Yeah. You want good brakes. Yeah. You want it to like not have it slam

268
00:15:17,285 --> 00:15:18,045
Speaker 7:  into you and stuff.

269
00:15:18,275 --> 00:15:21,205
Speaker 5:  Well if I know anything about Teslas, we're gonna, we're all gonna learn

270
00:15:21,205 --> 00:15:22,725
Speaker 5:  something about the Cybertruck brakes.

271
00:15:24,275 --> 00:15:28,205
Speaker 5:  Regular Teslas, because they use regenerative braking so much to recapture

272
00:15:28,205 --> 00:15:32,135
Speaker 5:  energy. They often have weird brake issues. That's

273
00:15:32,135 --> 00:15:32,495
Speaker 5:  just a thing.

274
00:15:32,735 --> 00:15:36,095
Speaker 6:  I have two questions about this Cybertruck being for truck stuff thing

275
00:15:36,605 --> 00:15:39,895
Speaker 6:  that I've been thinking about since they started talking about this. And

276
00:15:39,895 --> 00:15:43,815
Speaker 6:  the first is like, why frame it that way?

277
00:15:44,035 --> 00:15:48,015
Speaker 6:  If you're Tesla on the one hand there are a lot of people who do not

278
00:15:48,015 --> 00:15:51,055
Speaker 6:  do truck stuff with their trucks who buy trucks anyway.

279
00:15:51,835 --> 00:15:55,495
Speaker 6:  On the other hand, competing for the truck stuff

280
00:15:55,495 --> 00:15:59,415
Speaker 6:  market is probably more complicated than just being like, look, we built

281
00:15:59,415 --> 00:16:02,815
Speaker 6:  this cool new thing with a big truck bed.

282
00:16:03,205 --> 00:16:06,935
Speaker 6:  It's something completely different. It will do like light truck stuff, but

283
00:16:06,955 --> 00:16:10,215
Speaker 6:  not full truck stuff. But fundamentally, like he kept saying over and over

284
00:16:10,215 --> 00:16:13,255
Speaker 6:  their, their tagline is the future should look like the future. Which is

285
00:16:13,255 --> 00:16:17,095
Speaker 6:  very funny and doesn't mean anything. But he, and he said

286
00:16:17,095 --> 00:16:19,655
Speaker 6:  something to the effect of like, this is gonna change the look of the road.

287
00:16:19,805 --> 00:16:23,735
Speaker 6:  Yeah. Like why not call this a different thing as opposed to talking to a

288
00:16:23,735 --> 00:16:26,855
Speaker 6:  bunch of people who probably don't want a car that looks like this, don't

289
00:16:26,855 --> 00:16:30,415
Speaker 6:  want an electric car right now and just aren't

290
00:16:30,635 --> 00:16:34,455
Speaker 6:  really the Tesla target market at all. I just don't, I don't get

291
00:16:34,455 --> 00:16:35,775
Speaker 6:  that part of this. Yeah,

292
00:16:35,885 --> 00:16:39,215
Speaker 7:  It's kinda like an El Camino. Like that's, that's like where it's existing

293
00:16:39,215 --> 00:16:43,055
Speaker 7:  in the the truck ecosystem. Right. Is that too

294
00:16:43,055 --> 00:16:43,255
Speaker 7:  mean?

295
00:16:43,595 --> 00:16:46,215
Speaker 5:  It, that's too mean. Okay. I mean's a That's fair. It's a truck. It's as

296
00:16:46,215 --> 00:16:50,015
Speaker 5:  big as an F-150. It an El Camino. It's a very different,

297
00:16:50,255 --> 00:16:52,775
Speaker 5:  I think they should bring back the El Camino if you're listening to this,

298
00:16:53,035 --> 00:16:56,255
Speaker 5:  an ev El Camino I think would be welcome on America's roads. Talk about changing

299
00:16:56,255 --> 00:16:59,855
Speaker 5:  the look of the roads. This I think is different. I think if you already

300
00:16:59,855 --> 00:17:03,135
Speaker 5:  make the best selling car in the country, the model y

301
00:17:03,995 --> 00:17:07,775
Speaker 5:  to move the needle, you've gotta make something else that's really popular.

302
00:17:08,045 --> 00:17:11,935
Speaker 5:  Yeah. And the Ford F-150 is the most popular vehicle in

303
00:17:11,935 --> 00:17:15,855
Speaker 5:  America for the last 400 years. Like the first job that

304
00:17:15,885 --> 00:17:19,775
Speaker 5:  chat GBT is gonna take is the writing, the

305
00:17:19,775 --> 00:17:23,695
Speaker 5:  press release that says the Ford F-150 is once again America's bestselling

306
00:17:23,695 --> 00:17:27,575
Speaker 5:  vehicle. Like I've gotten that press release every year that I've

307
00:17:27,575 --> 00:17:31,495
Speaker 5:  been alive. And that's, that's great for Ford. but it is

308
00:17:31,495 --> 00:17:35,455
Speaker 5:  the biggest car market in the country. It's the

309
00:17:35,455 --> 00:17:39,215
Speaker 5:  most ferocious. It is where the prices have steadily gone up. If you wanna

310
00:17:39,215 --> 00:17:42,605
Speaker 5:  sell a hundred thousand dollars car, you're selling a truck. That's fair.

311
00:17:42,605 --> 00:17:45,925
Speaker 5:  That's just the way it's going. You're selling a giant SUV or a pickup truck.

312
00:17:46,035 --> 00:17:49,365
Speaker 7:  Most of the people buying those trucks, like just regular

313
00:17:49,765 --> 00:17:53,365
Speaker 7:  consumers, are they actually buying it to be a truck or are they buying it

314
00:17:54,225 --> 00:17:55,045
Speaker 7:  to be a truck?

315
00:17:55,825 --> 00:17:58,205
Speaker 5:  You are the one from Texas Alex. You tell me.

316
00:17:58,595 --> 00:18:01,725
Speaker 7:  Like I tend to feel it's the other, i I know a lot of people who buy trucks

317
00:18:01,725 --> 00:18:04,725
Speaker 7:  because they need them to haul. My friend called me the day and was like,

318
00:18:04,725 --> 00:18:05,725
Speaker 7:  I'm hauling a mire,

319
00:18:06,465 --> 00:18:07,405
Speaker 5:  I'm hauling a bear.

320
00:18:07,695 --> 00:18:11,565
Speaker 7:  Mayor. Mayor. But Siri thought it was mayor and I was like,

321
00:18:12,505 --> 00:18:15,045
Speaker 7:  is Eric Adams in your car? What's happening?

322
00:18:15,505 --> 00:18:19,405
Speaker 5:  You need an F three 50 to haul Eric Adams. Yeah. He

323
00:18:19,405 --> 00:18:20,285
Speaker 5:  comes with a lot of baggage.

324
00:18:26,465 --> 00:18:30,325
Speaker 7:  But yeah, like I feel like the majority of people I see who just

325
00:18:30,325 --> 00:18:33,445
Speaker 7:  want a truck, want a truck just 'cause it's like, it's cool. It's fun. Yeah.

326
00:18:33,445 --> 00:18:36,845
Speaker 7:  It's, it's neat to drive and sometimes you're like, well I gotta move a bunch

327
00:18:36,845 --> 00:18:38,085
Speaker 7:  of crap and now I can,

328
00:18:38,725 --> 00:18:40,965
Speaker 5:  I mean I I own a pickup truck for these reasons. That's

329
00:18:40,965 --> 00:18:41,805
Speaker 7:  Why my mom has one. Yeah.

330
00:18:41,805 --> 00:18:45,445
Speaker 6:  The most most people need to do is like, they have a pile of dirt in their

331
00:18:45,645 --> 00:18:48,765
Speaker 6:  backyard that they would like to not be in their backyard anymore. And that's

332
00:18:48,765 --> 00:18:52,445
Speaker 6:  like the most truck stuff I think a lot of truck owners get into. Yeah,

333
00:18:52,445 --> 00:18:55,205
Speaker 5:  Yeah. No, when we in the pandemic, we moved to the country and I had to take

334
00:18:55,205 --> 00:18:59,045
Speaker 5:  my own trash to the dump once a week. And I convinced myself that owning

335
00:18:59,045 --> 00:19:02,925
Speaker 5:  a pickup truck was the only solution to this problem. I mean, and now don

336
00:19:02,925 --> 00:19:05,765
Speaker 5:  don't, now I don't live in the, in the country

337
00:19:07,035 --> 00:19:10,685
Speaker 5:  then I still own a pickup truck and I haven't figured out

338
00:19:11,385 --> 00:19:14,085
Speaker 5:  why, but I know that we're not getting rid of it.

339
00:19:14,105 --> 00:19:16,885
Speaker 7:  Do you need to do some sod work or something in No, we

340
00:19:16,885 --> 00:19:18,965
Speaker 5:  Don't have, we, no. We live in a very urban area.

341
00:19:21,875 --> 00:19:22,165
Speaker 5:  This

342
00:19:22,355 --> 00:19:23,805
Speaker 7:  Furniture refinishing fully

343
00:19:23,805 --> 00:19:27,765
Speaker 5:  Incompatible with the area I live in now and yet, and yet we still

344
00:19:27,765 --> 00:19:30,845
Speaker 5:  have it. That said, look, a lot of people like pickup trucks and you can

345
00:19:30,845 --> 00:19:33,805
Speaker 5:  have a lot of feelings about that. Are they appropriate cars for most places?

346
00:19:35,385 --> 00:19:35,605
Speaker 5:  No.

347
00:19:35,675 --> 00:19:36,605
Speaker 7:  Depends. Right.

348
00:19:36,625 --> 00:19:40,325
Speaker 5:  Do cars need to be this big? At one point in this Cybertruck event, Elon

349
00:19:40,325 --> 00:19:43,525
Speaker 5:  said if you get into a fight with another car, you will win. And he showed

350
00:19:43,605 --> 00:19:45,245
Speaker 5:  a car crashing into a tough truck.

351
00:19:45,245 --> 00:19:45,685
Speaker 7:  Isn't that

352
00:19:46,465 --> 00:19:50,045
Speaker 5:  Bad? And our transportation reporter, Andy Hawkins, who has been on a tear

353
00:19:50,045 --> 00:19:53,085
Speaker 5:  pointing out that cars are too big, they're too dangerous, they're dangerous

354
00:19:53,085 --> 00:19:55,965
Speaker 5:  to other pedestrians, especially when they start going this fast. They're

355
00:19:55,965 --> 00:19:59,645
Speaker 5:  incredibly dangerous to other cars. That's not a good thing.

356
00:19:59,715 --> 00:20:03,365
Speaker 7:  Yeah. Like we used to have a Thunderbird 66 Thunderbird when I was a kid

357
00:20:03,365 --> 00:20:06,645
Speaker 7:  and stuff and it was really cool. And if you got in an accident you were

358
00:20:06,645 --> 00:20:10,485
Speaker 7:  totally fine. And now, but like the knowledge was if you got in an accident

359
00:20:10,485 --> 00:20:13,845
Speaker 7:  with a modern car, you would annihilate it and that's actually horrible.

360
00:20:13,945 --> 00:20:17,165
Speaker 7:  And then you'd also get batted around inside the car. Yeah. And just pinging

361
00:20:17,165 --> 00:20:21,085
Speaker 7:  pong. And I'm like, there was not a lot of safety discussion at

362
00:20:21,085 --> 00:20:24,245
Speaker 7:  this event. And that was a little concerning for me, given that they have

363
00:20:24,325 --> 00:20:28,245
Speaker 7:  a giant stainless steel box they're putting on the road. Yeah. Like

364
00:20:28,245 --> 00:20:30,925
Speaker 7:  what, what, what are crumple zones like on this thing? Because everybody

365
00:20:30,925 --> 00:20:33,725
Speaker 7:  else is like crumple zones. They should do that. And this guy's like, you

366
00:20:33,725 --> 00:20:36,325
Speaker 7:  know what, small arms fire

367
00:20:36,945 --> 00:20:39,045
Speaker 6:  In a Cybertruck, you crumble everything

368
00:20:39,405 --> 00:20:43,365
Speaker 5:  They did, they did show some crash testing. We will see. Who knows, right?

369
00:20:43,365 --> 00:20:47,285
Speaker 5:  Yeah. It's a million Who knows questions. But but

370
00:20:47,635 --> 00:20:51,565
Speaker 5:  back to David's point, which is why market this thing

371
00:20:51,565 --> 00:20:55,165
Speaker 5:  as a truck? People love them. I it's because it's the last market,

372
00:20:55,345 --> 00:20:59,325
Speaker 5:  the last big section in the market to go get. I think they

373
00:20:59,325 --> 00:21:02,725
Speaker 5:  could have picked one or two others. Right. Luxury SUVs

374
00:21:03,395 --> 00:21:07,005
Speaker 5:  that are not the Model X, which is a little long in the tooth.

375
00:21:07,745 --> 00:21:11,725
Speaker 5:  That's a big market that's growing. Who knows if you can build an

376
00:21:11,825 --> 00:21:15,445
Speaker 5:  ev in that form factor. But this, I think they wanted to build this 'cause

377
00:21:15,445 --> 00:21:18,925
Speaker 5:  it is the statement piece. It is the big bold design.

378
00:21:19,555 --> 00:21:22,885
Speaker 5:  They're not counting on it winning, which I think they've now set on some

379
00:21:23,125 --> 00:21:26,965
Speaker 5:  earnings calls. Like the model Y is the volume Tesla, it's gonna

380
00:21:26,965 --> 00:21:30,405
Speaker 5:  keep doing what it's doing even as pri they, they're slashing prices left

381
00:21:30,405 --> 00:21:33,325
Speaker 5:  and right. This is the one that's gonna be the the halo car.

382
00:21:33,795 --> 00:21:37,725
Speaker 6:  Yeah. There's a way to look at this that it's like the successor to the

383
00:21:37,725 --> 00:21:40,645
Speaker 6:  Roadster in a pretty real way. That, that this is like

384
00:21:42,265 --> 00:21:46,085
Speaker 6:  the car hardly anybody has but reliably turns

385
00:21:46,085 --> 00:21:49,005
Speaker 6:  heads when you see it on the street. And in that sense, I sort of buy it,

386
00:21:49,305 --> 00:21:53,165
Speaker 6:  but I, I just keep going back, back to this idea of like, if you are a person

387
00:21:53,265 --> 00:21:57,205
Speaker 6:  who like uses the hell out of your truck, what

388
00:21:57,205 --> 00:22:01,045
Speaker 6:  on earth would compel you to buy a Cybertruck at this moment in time? I just

389
00:22:01,205 --> 00:22:02,405
Speaker 6:  don't know that there are good answers.

390
00:22:04,085 --> 00:22:06,295
Speaker 5:  This is where we have to talk about Elon and Dealbook.

391
00:22:06,755 --> 00:22:06,975
Speaker 6:  We,

392
00:22:07,195 --> 00:22:08,135
Speaker 5:  We should not dwell on

393
00:22:08,135 --> 00:22:09,055
Speaker 6:  Yeah, okay. No, we should,

394
00:22:09,055 --> 00:22:11,295
Speaker 5:  You're right. And know David like does not wanna talk about this.

395
00:22:11,495 --> 00:22:13,415
Speaker 6:  I don't, I super don't. But we should.

396
00:22:13,675 --> 00:22:17,655
Speaker 5:  But you can't actually right now, divorce Tesla the car company

397
00:22:17,655 --> 00:22:20,775
Speaker 5:  from Elon Musk the man, which is a real problem.

398
00:22:21,475 --> 00:22:25,135
Speaker 5:  So in some academic argument about market share and

399
00:22:25,135 --> 00:22:27,095
Speaker 5:  products, you're like, why would you make this product? Why would you market

400
00:22:27,095 --> 00:22:30,525
Speaker 5:  it this way? And you can like come to an answer. And part of the answer right

401
00:22:30,525 --> 00:22:34,205
Speaker 5:  now is why would you buy a car from Elon Musk? Like there's some enormous

402
00:22:34,205 --> 00:22:38,125
Speaker 5:  segment of the population that is just over his antics,

403
00:22:38,235 --> 00:22:42,085
Speaker 5:  over his weird racist and anti-Semitic tweets just over

404
00:22:42,225 --> 00:22:44,375
Speaker 5:  him. Right. And what we saw.

405
00:22:44,595 --> 00:22:48,535
Speaker 6:  But to be fair, there's also a chunk of people on the opposite, the exact

406
00:22:48,775 --> 00:22:51,975
Speaker 6:  opposite end of that spectrum, right. Who will, who will follow him to the

407
00:22:51,975 --> 00:22:53,735
Speaker 6:  ends of the earth and buy anything he tells them to buy?

408
00:22:54,485 --> 00:22:54,775
Speaker 5:  Yeah.

409
00:22:55,005 --> 00:22:58,135
Speaker 6:  Like he could have a podcast selling them supplements and they would buy

410
00:22:58,135 --> 00:23:01,175
Speaker 6:  the supplements. I think that group is substantially smaller than it was

411
00:23:01,315 --> 00:23:02,375
Speaker 6:  24 months ago.

412
00:23:02,575 --> 00:23:05,135
Speaker 5:  I also don't think they have a hundred thousand dollars to spend on the Cybertruck

413
00:23:05,165 --> 00:23:09,055
Speaker 5:  that I just wanna be very clear, right. Because the, the people you

414
00:23:09,055 --> 00:23:12,935
Speaker 5:  need to spend the money are often very educated.

415
00:23:12,995 --> 00:23:16,975
Speaker 5:  And this is like a real problem for Elon. Like he lives in a

416
00:23:16,975 --> 00:23:20,455
Speaker 5:  Twitter bubble and he thinks the Twitter bubble is real

417
00:23:20,835 --> 00:23:24,095
Speaker 5:  and he thinks the thing that gets him engagement on Twitter will get him

418
00:23:24,095 --> 00:23:28,055
Speaker 5:  engagement in real life. And then he goes to conferences. Like we

419
00:23:28,075 --> 00:23:31,535
Speaker 5:  saw him this week, the Dealbook conference with Andrew Ross Sorkin. And he

420
00:23:31,975 --> 00:23:35,415
Speaker 5:  delivers what he, what are like bangers on Twitter?

421
00:23:36,395 --> 00:23:37,855
Speaker 5:  It, it's dead silence.

422
00:23:38,155 --> 00:23:41,935
Speaker 7:  It was just like Twitter come to life. And then

423
00:23:42,315 --> 00:23:45,645
Speaker 7:  the the silence afterwards was so aw. Like every time he'd do a punchline

424
00:23:45,645 --> 00:23:48,845
Speaker 7:  and he'd ape to the camera and be like, eh, and it's just like crickets.

425
00:23:49,035 --> 00:23:52,365
Speaker 5:  Yeah. Our, our transportation reporter Omar pointed out that he would often

426
00:23:52,365 --> 00:23:55,685
Speaker 5:  deliver the punchlines twice. And the second time he got the pity laugh,

427
00:23:55,975 --> 00:23:58,885
Speaker 5:  which is a, if you go watch it again, it's a, it's a very brutal, real dynamic.

428
00:24:00,405 --> 00:24:04,165
Speaker 5:  I mean that that joke, that Twitter, that thing that Twitter

429
00:24:04,165 --> 00:24:07,965
Speaker 5:  isn't real life. His, his appearance at Dealbook

430
00:24:08,025 --> 00:24:12,005
Speaker 5:  was just, oh, Twitter's not real life like fully encapsulated

431
00:24:12,005 --> 00:24:15,325
Speaker 5:  in that event. And so we should talk about what he said. There's a few things

432
00:24:15,325 --> 00:24:19,085
Speaker 5:  that are just worth pulling apart. The first and most important

433
00:24:19,465 --> 00:24:23,325
Speaker 5:  is that advertisers are leaving Twitter. After yet

434
00:24:23,325 --> 00:24:27,125
Speaker 5:  more of Musk's antics and weird anti-Semitism and

435
00:24:27,215 --> 00:24:31,165
Speaker 5:  Media Matters had a report about ads showing

436
00:24:31,165 --> 00:24:34,365
Speaker 5:  up next to bad content. The advertisers started leaving because that's what

437
00:24:34,565 --> 00:24:37,485
Speaker 5:  advertisers do. Whenever there are brand safety concerns on any platforms.

438
00:24:37,885 --> 00:24:41,765
Speaker 5:  I would just offer you the example of something called

439
00:24:41,765 --> 00:24:45,245
Speaker 5:  the Apocalypse on YouTube, which was exactly this thing.

440
00:24:46,335 --> 00:24:50,175
Speaker 5:  And Susan Waki, who was the CEO of YouTube at the time, did not

441
00:24:50,175 --> 00:24:53,935
Speaker 5:  tell a bunch of advertisers to go fuck themselves. They like actively tried

442
00:24:53,935 --> 00:24:57,655
Speaker 5:  to fix it. Elon's response was to look at the DealBook

443
00:24:58,055 --> 00:25:01,535
Speaker 5:  audience and say, if you think you can blackmail me with money

444
00:25:02,855 --> 00:25:06,635
Speaker 5:  Go fuck yourself. And he was like, Bob, if you're here meaning Bob, Iger

445
00:25:07,495 --> 00:25:11,395
Speaker 5:  Go, fuck yourself. I, I don't know. Like again,

446
00:25:11,395 --> 00:25:15,115
Speaker 5:  that's, it's not, I don't sell the ads for us. It's not my side of the house.

447
00:25:15,575 --> 00:25:19,315
Speaker 5:  I'm fairly confident at the people who sold our ads announced at a

448
00:25:19,315 --> 00:25:23,195
Speaker 5:  conference, their clients could go fuck themselves. Like I

449
00:25:23,195 --> 00:25:26,155
Speaker 5:  don't think the advertisers are coming back. And then he follows up with

450
00:25:26,905 --> 00:25:30,875
Speaker 5:  this advertising boycott will kill the company and the world will

451
00:25:30,875 --> 00:25:34,795
Speaker 5:  know it. We will document it. And I think Andrew followed

452
00:25:34,795 --> 00:25:38,655
Speaker 5:  up by asking, isn't that a bad thing? And he is like, earth will

453
00:25:38,655 --> 00:25:39,215
Speaker 5:  know. Yeah.

454
00:25:39,475 --> 00:25:43,415
Speaker 7:  He, he was earth, he really thinks that everyone is gonna come out and

455
00:25:43,525 --> 00:25:47,375
Speaker 7:  save X. He was very, he that was one thing he was very clear on. He

456
00:25:47,375 --> 00:25:50,855
Speaker 7:  constantly corrected Andrew on calling it X versus Twitter.

457
00:25:51,155 --> 00:25:55,055
Speaker 7:  And he really thinks that like the uproar will save X And it's like, no,

458
00:25:55,055 --> 00:25:58,895
Speaker 7:  but the uproar caused this to happen. Yeah. Everybody's leaving

459
00:25:58,895 --> 00:26:01,295
Speaker 7:  because there was an uproar. 'cause everybody's mad at you.

460
00:26:02,005 --> 00:26:05,215
Speaker 5:  Yeah. I mean we, we have talked about in the show, there was a time when

461
00:26:05,215 --> 00:26:09,185
Speaker 5:  Twitter was a particular kind of monopoly and no matter what

462
00:26:09,205 --> 00:26:13,185
Speaker 5:  the company did and that I will issue our disclaimer again, criticism

463
00:26:13,185 --> 00:26:16,905
Speaker 5:  of Elon Musk as CEO of Twitter is no way praise for the previous

464
00:26:16,905 --> 00:26:20,265
Speaker 5:  administration of Twitter who did a horrible job running this company.

465
00:26:20,965 --> 00:26:24,945
Speaker 5:  But they had a kind of monopoly where no matter how

466
00:26:25,035 --> 00:26:28,625
Speaker 5:  badly they screwed up, everyone kept coming back to Twitter.

467
00:26:28,965 --> 00:26:32,825
Speaker 5:  And I'm pretty sure Elon thought that he was buying that monopoly. Yes.

468
00:26:33,255 --> 00:26:37,145
Speaker 5:  Because if I was addicted to cigarettes, I'm not saying I ever was and I

469
00:26:37,145 --> 00:26:40,185
Speaker 5:  bought a cigarette company, I'd be like, this rules, everyone's addicted

470
00:26:40,205 --> 00:26:44,085
Speaker 5:  to my product just as much as I am. And then everyone was like, now you

471
00:26:44,085 --> 00:26:47,885
Speaker 5:  are an asshole. And we vape now. And that is more or less what has happened

472
00:26:47,885 --> 00:26:48,525
Speaker 5:  to Elon Musk.

473
00:26:48,795 --> 00:26:51,885
Speaker 6:  Yeah. Yeah. And I think, I mean, it it's wild how

474
00:26:52,715 --> 00:26:56,525
Speaker 6:  even a year ago that seemed like a

475
00:26:56,525 --> 00:27:00,325
Speaker 6:  reasonable bet, right? I think there was a time when it looked like

476
00:27:01,425 --> 00:27:05,325
Speaker 6:  for all of the chaos and for all of the bad stuff, everybody was just

477
00:27:05,325 --> 00:27:08,805
Speaker 6:  gonna kind of hold their nose and keep using Twitter because to some extent

478
00:27:08,805 --> 00:27:12,685
Speaker 6:  there was nowhere to go. And to a potentially larger extent, the

479
00:27:12,735 --> 00:27:16,645
Speaker 6:  chaos and the fun are so close to each other on Twitter that

480
00:27:16,905 --> 00:27:20,405
Speaker 6:  you, you can ramp up one and kind of

481
00:27:20,765 --> 00:27:24,525
Speaker 6:  accidentally ramp up the, like the, the mess is the point. And

482
00:27:25,285 --> 00:27:28,485
Speaker 6:  I think I, like 12 months ago, I'm sure on this show

483
00:27:29,465 --> 00:27:32,645
Speaker 6:  we were having this same debate. And to me at this point, I think it feels

484
00:27:32,645 --> 00:27:36,485
Speaker 6:  like a foregone conclusion that at some point or another, X is gonna go

485
00:27:36,555 --> 00:27:40,445
Speaker 6:  away in a pretty real way. I don't know if the company's gonna die just 'cause

486
00:27:40,445 --> 00:27:42,765
Speaker 6:  he has so much money he could float the thing forever if he felt like it.

487
00:27:42,765 --> 00:27:43,325
Speaker 6:  But also to he's got

488
00:27:43,325 --> 00:27:45,685
Speaker 5:  Investors he has to pay off. Like there's a lot going on there.

489
00:27:45,715 --> 00:27:49,485
Speaker 6:  Yeah. But I also get the increasing sense that he is

490
00:27:49,485 --> 00:27:52,765
Speaker 6:  trying to set it up in such a way that if X goes away,

491
00:27:53,475 --> 00:27:56,685
Speaker 6:  goes outta business, declares bankruptcy, whatever it turns out to be,

492
00:27:58,305 --> 00:28:02,205
Speaker 6:  he wants to be a martyr and he, he wants to have, that

493
00:28:02,205 --> 00:28:06,005
Speaker 6:  will be his like full proof positive that the world

494
00:28:06,065 --> 00:28:09,925
Speaker 6:  is against him and has gone woken. Everything is a disaster. And now he kind

495
00:28:09,925 --> 00:28:13,485
Speaker 6:  of wins coming and going. Right. It's either like a business victory or a

496
00:28:13,485 --> 00:28:16,645
Speaker 6:  moral victory or it's kind of a moral victory on either side. No matter what

497
00:28:16,645 --> 00:28:20,165
Speaker 6:  happens. He's trying to set up this thing that I find very strange.

498
00:28:20,525 --> 00:28:24,005
Speaker 7:  I mean, it's, it's not that strange 'cause it's a, it's a pretty common playbook

499
00:28:24,105 --> 00:28:27,725
Speaker 7:  now. Like that this was the exact same playbook Trump has and, and

500
00:28:27,925 --> 00:28:30,565
Speaker 7:  continues to have. And I, I think it's just a really, it's, it's difficult

501
00:28:30,565 --> 00:28:34,405
Speaker 7:  to counter. And I think that's why it's a really like useful and

502
00:28:34,545 --> 00:28:35,645
Speaker 7:  deeply irritating.

503
00:28:35,745 --> 00:28:38,565
Speaker 5:  Let me make the distinction for you a comparison and a distinction. Okay.

504
00:28:38,565 --> 00:28:42,045
Speaker 5:  You're right. It's the same playbook. Yeah. I the very different dudes, the

505
00:28:42,125 --> 00:28:45,765
Speaker 5:  election was lost. Everyone is corrupt. It was stolen from me. The bad forces

506
00:28:45,875 --> 00:28:49,085
Speaker 5:  came and took this away from me. That's voting with your votes. Yep.

507
00:28:50,115 --> 00:28:53,365
Speaker 5:  Very few people get to make that argument when it's voting with your dollars.

508
00:28:53,545 --> 00:28:57,205
Speaker 5:  That's true. And so Elon is not making the argument that these woke

509
00:28:57,205 --> 00:29:01,045
Speaker 5:  corporations are no longer buying advertising to, to spite him. And

510
00:29:01,045 --> 00:29:04,765
Speaker 5:  it is very hard to tell anyone, particularly

511
00:29:05,325 --> 00:29:07,805
Speaker 5:  Americans, that they have an obligation to spend money.

512
00:29:09,185 --> 00:29:12,485
Speaker 5:  It is just not a thing you can do in this country culturally. Just not a

513
00:29:12,485 --> 00:29:16,445
Speaker 5:  thing you can do to say you have to spend money advertising dollars on

514
00:29:16,445 --> 00:29:20,245
Speaker 5:  this platform because it's important that this thing exists. And

515
00:29:20,475 --> 00:29:23,805
Speaker 5:  most people are like, what? You can't tell me how to spend my money. Well

516
00:29:23,805 --> 00:29:27,725
Speaker 7:  That was like the immediate reaction on social media. On, on and in our own

517
00:29:27,725 --> 00:29:31,565
Speaker 7:  like comments and stuff. Everybody's just like, no, it's

518
00:29:31,565 --> 00:29:31,965
Speaker 7:  my money.

519
00:29:32,305 --> 00:29:35,725
Speaker 5:  Oh yeah. No, it's, it's very like, we talk a lot about free speech on the

520
00:29:35,725 --> 00:29:39,445
Speaker 5:  show and like when it's anything but money, like everyone immediately

521
00:29:39,445 --> 00:29:42,925
Speaker 5:  sees the gray areas. There's like nuance when it's like you have to spend

522
00:29:42,925 --> 00:29:46,645
Speaker 5:  money. People are like, no, I don't. No, no sir. No, thank you.

523
00:29:46,795 --> 00:29:50,565
Speaker 5:  Yeah. Go fuck yourself. And it's like you just,

524
00:29:50,745 --> 00:29:54,245
Speaker 5:  you just watching him run into this, right? Yeah. Like he had this argument

525
00:29:54,245 --> 00:29:57,085
Speaker 5:  about Tesla's at, at the double conference as well, where he was like, I

526
00:29:57,085 --> 00:29:59,765
Speaker 5:  think Andrew asked him, are you worried that your tweets and your antics

527
00:29:59,765 --> 00:30:02,085
Speaker 5:  are gonna hurt the company? He said, no, we make the best car. If you want

528
00:30:02,085 --> 00:30:06,005
Speaker 5:  the best car, you have to buy a Tesla. Which is debatable, but it

529
00:30:06,265 --> 00:30:10,005
Speaker 5:  is a market driven argument. Right? Right.

530
00:30:10,095 --> 00:30:13,045
Speaker 5:  We're we're putting our product in the market and if people are gonna look

531
00:30:13,045 --> 00:30:15,165
Speaker 5:  at an evaluation and maybe they'll evaluate me, but really they're gonna

532
00:30:15,165 --> 00:30:17,365
Speaker 5:  want the best car and they're gonna pick my car, we make the bestselling

533
00:30:17,465 --> 00:30:21,325
Speaker 5:  car. Okay. You're competing in the market. You, you've, you're weighing all

534
00:30:21,325 --> 00:30:24,885
Speaker 5:  the variables with Twitter. It's like, if you don't advertise here,

535
00:30:25,105 --> 00:30:28,965
Speaker 5:  you're destroying America. And it's like, that's just not, that's not

536
00:30:28,965 --> 00:30:32,925
Speaker 5:  how that works. That's just a weird argument man. And to say like,

537
00:30:32,925 --> 00:30:36,245
Speaker 5:  my product is objectively worse or my product is not what the market

538
00:30:36,485 --> 00:30:40,365
Speaker 5:  demands for me or what the market says it wants and not buying it anyway

539
00:30:40,385 --> 00:30:44,365
Speaker 5:  is the problem. I don't know man. Like, it's just odd and

540
00:30:44,365 --> 00:30:47,645
Speaker 5:  like what's gonna happen right next to it is threads is gonna light up advertising.

541
00:30:48,175 --> 00:30:52,005
Speaker 5:  We're gonna go into a holiday season where TikTok and Amazon and Google

542
00:30:52,105 --> 00:30:55,965
Speaker 5:  and Meta are all in Q4. They all have salespeople with

543
00:30:55,965 --> 00:30:59,285
Speaker 5:  real relationships. They all have content moderation teams. And they're gonna

544
00:30:59,285 --> 00:31:02,805
Speaker 5:  say, every dollar you were spending on Twitter, you should spend that dollar

545
00:31:02,875 --> 00:31:05,885
Speaker 5:  here and we'll treat you nicely. We wont tell

546
00:31:05,885 --> 00:31:06,365
Speaker 6:  You to fuck off.

547
00:31:06,435 --> 00:31:10,245
Speaker 5:  Yeah. And like, we'll, we'll send you some swag. Whatever salespeople do

548
00:31:10,245 --> 00:31:11,485
Speaker 5:  I don know I don know this works.

549
00:31:12,035 --> 00:31:15,165
Speaker 6:  It's swag. I just spend money. It's just t-shirts from company to company

550
00:31:15,485 --> 00:31:19,325
Speaker 5:  Floating around. Yeah. And poor Linda Yao, like her job is to run

551
00:31:19,395 --> 00:31:23,285
Speaker 5:  this playbook and she can't because her boss has just told all these

552
00:31:23,285 --> 00:31:26,965
Speaker 5:  people to, to f off I that will kill the company.

553
00:31:27,265 --> 00:31:31,205
Speaker 5:  And this attempt to run the martyr playbook, it

554
00:31:31,205 --> 00:31:35,125
Speaker 5:  will fail because you just cannot be a martyr because people

555
00:31:35,125 --> 00:31:39,065
Speaker 5:  didn't buy your shit. You, you can't be a like a

556
00:31:39,385 --> 00:31:43,025
Speaker 5:  musician and be like, people didn't buy my album. The world is corrupt. Like

557
00:31:43,965 --> 00:31:45,025
Speaker 5:  no, I mean a lot

558
00:31:45,025 --> 00:31:46,705
Speaker 6:  Of musicians, I totally disagree with that. Well

559
00:31:46,705 --> 00:31:48,785
Speaker 5:  Sure. but it never hits. It never hits, but it's

560
00:31:49,155 --> 00:31:52,585
Speaker 6:  Never works. Literally. I like, this is gonna sound like I'm joking, but

561
00:31:52,585 --> 00:31:56,345
Speaker 6:  I'm not. It's because no one else is trying hard enough. Like the thing

562
00:31:56,455 --> 00:31:59,865
Speaker 6:  Elon Musk is doing is, is making it his life's mission

563
00:32:00,365 --> 00:32:03,865
Speaker 6:  to tell everyone how X is the last

564
00:32:03,935 --> 00:32:07,865
Speaker 6:  bastion of free speech. Anything against X is against free

565
00:32:07,865 --> 00:32:11,745
Speaker 6:  speech. Like Lindy Yao's tweet said, like, what was it? X

566
00:32:11,765 --> 00:32:14,945
Speaker 6:  is standing at a unique and amazing intersection of free speech and Main

567
00:32:14,945 --> 00:32:18,505
Speaker 6:  Street. Which like what on earth does that mean? Yeah. But there no

568
00:32:18,575 --> 00:32:19,585
Speaker 5:  Idea what that means, by

569
00:32:19,585 --> 00:32:22,105
Speaker 6:  The way. It's pure nonsense. But we're now in this position where like he,

570
00:32:22,385 --> 00:32:22,625
Speaker 6:  I I,

571
00:32:22,645 --> 00:32:25,665
Speaker 5:  By the way, can I just point out speech notably free on Main Street,

572
00:32:26,605 --> 00:32:30,585
Speaker 5:  you can find the main street in your town and stand on that corner and

573
00:32:30,645 --> 00:32:31,905
Speaker 5:  do whatever you want.

574
00:32:34,015 --> 00:32:37,945
Speaker 5:  Like that's the whole point. That's the whole idea. Yeah. Like you can

575
00:32:37,945 --> 00:32:40,485
Speaker 5:  yell, cops arrest me and the cops would be like, no,

576
00:32:40,715 --> 00:32:42,205
Speaker 7:  Depends on how many clothes you're wearing.

577
00:32:42,525 --> 00:32:45,685
Speaker 5:  That's true. I'll be like, one nipple's fine,

578
00:32:47,225 --> 00:32:49,925
Speaker 6:  But he's gonna be able to do this whole thing and get to the end of this

579
00:32:49,925 --> 00:32:53,525
Speaker 6:  process and say, by by killing X you have killed free speech

580
00:32:53,945 --> 00:32:57,845
Speaker 6:  in America. And there it's a ridiculous argument. But he has said

581
00:32:57,845 --> 00:33:00,725
Speaker 6:  it so many times that at this point some people are going to believe him

582
00:33:01,305 --> 00:33:05,085
Speaker 6:  and sure. It's just gonna be ridiculous. Like, like

583
00:33:05,265 --> 00:33:09,205
Speaker 6:  Donald Trump says it about truth social that it is and, and nobody believes

584
00:33:09,205 --> 00:33:12,925
Speaker 6:  him. Like part of it is, is Elon took this thing that was so much

585
00:33:12,925 --> 00:33:16,725
Speaker 6:  bigger and destroyed it so much faster than, in a funny way, it's gonna make

586
00:33:16,725 --> 00:33:20,525
Speaker 6:  his argument more credible as he does this. Like

587
00:33:20,525 --> 00:33:23,485
Speaker 6:  he, he's gonna be able to track those two things against each other in a

588
00:33:23,485 --> 00:33:26,885
Speaker 6:  way that is absolutely unrelated to the truth of the world,

589
00:33:27,705 --> 00:33:30,605
Speaker 6:  but it's, it's gonna work for him. For some people. You, you,

590
00:33:30,605 --> 00:33:34,405
Speaker 7:  You've also got Twitter brain saying that like I I I think like maybe,

591
00:33:34,445 --> 00:33:38,245
Speaker 7:  maybe spicy people on Twitter, like his fans on Twitter

592
00:33:38,245 --> 00:33:41,685
Speaker 7:  will probably believe that. I don't think anybody else out in the the general

593
00:33:41,735 --> 00:33:45,165
Speaker 7:  world is gonna be like, man, can you believe what happened? He was right.

594
00:33:45,195 --> 00:33:49,085
Speaker 7:  There's something to it. Yeah. Like some stuff he says,

595
00:33:49,085 --> 00:33:49,285
Speaker 7:  sure

596
00:33:49,745 --> 00:33:53,485
Speaker 6:  Don don't think you can make the Donald Trump comparison you made a few minutes

597
00:33:53,485 --> 00:33:54,725
Speaker 6:  ago and then make that argument.

598
00:33:55,285 --> 00:33:58,685
Speaker 7:  I mean, they're both, look, they're both, they both have very passionate

599
00:33:58,905 --> 00:34:02,525
Speaker 7:  groups of followers and those, those passionate followers are gonna follow

600
00:34:02,525 --> 00:34:05,565
Speaker 7:  them regardless of what they say. And, and they're both really, really good

601
00:34:05,565 --> 00:34:09,165
Speaker 7:  at that. And then they use that, that that very vocal small group of followers

602
00:34:09,905 --> 00:34:13,405
Speaker 7:  as as as as cudgels, right? Like to as and as barriers

603
00:34:13,745 --> 00:34:17,485
Speaker 7:  to to all other discourse and all other like actual

604
00:34:17,555 --> 00:34:20,845
Speaker 7:  reality. That's what he's really good at. That's, that's what Trump's really

605
00:34:20,845 --> 00:34:24,565
Speaker 7:  good at. And it's interesting to watch it first play out in politics and

606
00:34:24,565 --> 00:34:28,085
Speaker 7:  now in in tech. Yeah. It's a kind of a bummer, but

607
00:34:28,475 --> 00:34:29,325
Speaker 7:  it's also interesting.

608
00:34:29,865 --> 00:34:33,605
Speaker 5:  It is definitely a bummer. I think the world might be better off that Twitter

609
00:34:33,665 --> 00:34:37,245
Speaker 5:  is on the decline as we go into an election year. Mm. Just putting that out

610
00:34:37,245 --> 00:34:40,925
Speaker 5:  there. Yeah. I'm not saying that the ends justify the means. Yeah. I think

611
00:34:40,925 --> 00:34:44,045
Speaker 5:  the means have been very destructive. Lots of people have lost their jobs.

612
00:34:44,235 --> 00:34:47,365
Speaker 5:  Like all of it's been bad racism and antisemitism

613
00:34:47,885 --> 00:34:51,765
Speaker 5:  skyrocketing on platform. The, i, in no way am I saying that the

614
00:34:51,765 --> 00:34:55,325
Speaker 5:  means have been appropriate or effective. I just think this argument

615
00:34:55,595 --> 00:34:59,365
Speaker 5:  that he represents free speech while he's suing media matters for

616
00:34:59,645 --> 00:35:03,165
Speaker 5:  criticizing him. Like this is the stuff that just begins to fall on deaf

617
00:35:03,165 --> 00:35:07,005
Speaker 5:  ears. Yeah. Like over and over and over again. And I, I think

618
00:35:07,315 --> 00:35:11,085
Speaker 5:  that when we talk about Tesla, like he has to sell

619
00:35:11,115 --> 00:35:14,645
Speaker 5:  more cars and his antics with Twitter are

620
00:35:15,045 --> 00:35:18,205
Speaker 5:  actively making people not want to give him money. Whether that's

621
00:35:18,685 --> 00:35:22,605
Speaker 5:  advertisers, whether it's car customers, whatever it is. The last monopoly

622
00:35:22,605 --> 00:35:25,125
Speaker 5:  he has is actually SpaceX. Yeah.

623
00:35:25,625 --> 00:35:26,885
Speaker 7:  And it's a

624
00:35:26,885 --> 00:35:30,405
Speaker 5:  Really good one. It's a really good one. Like there's no, the United Launch

625
00:35:30,645 --> 00:35:34,445
Speaker 5:  Alliance is nowhere close to shipping a Ford F-150 lightning like

626
00:35:34,745 --> 00:35:38,445
Speaker 5:  the, you know, where there's like any other choice. There's no pole star

627
00:35:38,585 --> 00:35:39,925
Speaker 5:  in the rocket business. Yeah.

628
00:35:40,125 --> 00:35:43,325
Speaker 6:  I don Dunno. Bezos is out here kind of doing stuff. Blue Origin is like

629
00:35:44,075 --> 00:35:47,805
Speaker 5:  Lingering. Bezos can't put a payload into space. He can put like four

630
00:35:47,805 --> 00:35:48,845
Speaker 5:  rich people into space.

631
00:35:49,105 --> 00:35:51,405
Speaker 7:  But like Subspace, it's not even literal payload full space.

632
00:35:52,425 --> 00:35:55,405
Speaker 5:  He is like my girlfriend's in the sky. Like that's what he can do.

633
00:35:57,075 --> 00:36:00,645
Speaker 5:  Like, like maybe he will get there. It's a long way away before

634
00:36:00,665 --> 00:36:03,885
Speaker 5:  SpaceX has real competition. Yeah. And that's like, you can think about that

635
00:36:03,885 --> 00:36:07,765
Speaker 5:  any way you want, but Tesla has real competition and Twitter has real competition

636
00:36:08,225 --> 00:36:12,085
Speaker 5:  and in both cases he's reacting very differently. And I just think

637
00:36:12,985 --> 00:36:16,645
Speaker 5:  as we go into an election year, as things get more heated

638
00:36:17,345 --> 00:36:21,165
Speaker 5:  on social media, his decisions about what he amplifies, what he moderates

639
00:36:21,165 --> 00:36:25,045
Speaker 5:  all this stuff, and whether or not the platform is a

640
00:36:25,075 --> 00:36:28,005
Speaker 5:  safe place for advertisers. 'cause advertisers want brand safety.

641
00:36:29,555 --> 00:36:32,155
Speaker 5:  I think you're right. I think there's, there's not a world in which a year

642
00:36:32,155 --> 00:36:35,075
Speaker 5:  from now we are talking about a Twitter, the company

643
00:36:35,895 --> 00:36:39,345
Speaker 5:  that exists in the form that even this diminished company exists right now.

644
00:36:39,455 --> 00:36:41,305
Speaker 5:  Yeah. One more thing I'll say All,

645
00:36:41,305 --> 00:36:43,585
Speaker 6:  Right? We've officially talked too much about this so do this quickly.

646
00:36:43,855 --> 00:36:47,025
Speaker 5:  It's just copy, it's just copyright law. I promise you. It's just, it's just

647
00:36:47,025 --> 00:36:50,465
Speaker 5:  me talking about copy law. So, so can ask about ai. Oh

648
00:36:50,945 --> 00:36:54,825
Speaker 5:  go watch this clip. Addie quick posted tech chart writeup of this clip. It

649
00:36:54,825 --> 00:36:58,705
Speaker 5:  is amazing. So's like ai, it's a lot of training data. You have a lot of

650
00:36:58,825 --> 00:37:02,745
Speaker 5:  training data like these copyright lawsuits might just end it all. And Musk

651
00:37:02,745 --> 00:37:06,425
Speaker 5:  looks at Andrew and says, by the time these lawsuits come to a conclusion,

652
00:37:06,425 --> 00:37:09,225
Speaker 5:  we'll have a digital God. And you should ask him

653
00:37:10,665 --> 00:37:14,475
Speaker 5:  like, like I know Andrew, you know, like he's a

654
00:37:14,475 --> 00:37:16,555
Speaker 5:  pretty rational guy and he's just like, what,

655
00:37:18,185 --> 00:37:22,115
Speaker 5:  what, what are you talking about? He's like, by the time

656
00:37:22,115 --> 00:37:24,715
Speaker 5:  these lawsuits get sorted out, we'll have a digital God and it won't matter.

657
00:37:25,215 --> 00:37:29,035
Speaker 5:  And that was the end. Like there's nothing more you can do with that

658
00:37:29,035 --> 00:37:32,875
Speaker 5:  statement. Like he's like, AI will have become a God.

659
00:37:33,335 --> 00:37:37,315
Speaker 5:  And so copyright lawsuits, who cares? And it's like, dude, those

660
00:37:37,315 --> 00:37:39,435
Speaker 5:  lawsuits are gonna take like two or three years

661
00:37:40,835 --> 00:37:44,645
Speaker 7:  Digital God's right around the corner. He's ascending as we speak. It's

662
00:37:44,645 --> 00:37:47,525
Speaker 6:  Okay. Neli digital God

663
00:37:48,635 --> 00:37:49,885
Speaker 6:  base model Cybertruck,

664
00:37:50,215 --> 00:37:51,085
Speaker 5:  Which comes first?

665
00:37:54,345 --> 00:37:57,245
Speaker 5:  That's tough. That's tough. I gotta go digital. God on this one.

666
00:37:58,845 --> 00:37:59,565
Speaker 5:  I mean honestly,

667
00:38:00,445 --> 00:38:01,165
Speaker 7:  I I would agree.

668
00:38:01,165 --> 00:38:04,045
Speaker 5:  Yeah. I, and I think if you're working at one of these generative AI companies,

669
00:38:04,045 --> 00:38:07,525
Speaker 5:  you're hoping it's digital God too. Because these copyright lawsuits are

670
00:38:07,525 --> 00:38:11,325
Speaker 5:  an existential threat. And the Cybertruck, you know, it's just a truck.

671
00:38:11,745 --> 00:38:13,045
Speaker 7:  Get 'em ascending, get going.

672
00:38:13,605 --> 00:38:16,925
Speaker 5:  Everyone at opening eyes is like furiously ordering a base model. Cybertruck

673
00:38:16,925 --> 00:38:20,405
Speaker 5:  right now. All right? We David's right. We've talked enough about Elon. We

674
00:38:20,405 --> 00:38:23,445
Speaker 5:  gotta take a break. We'll right back talk about some other drama. We'll right

675
00:38:23,445 --> 00:38:23,605
Speaker 5:  back

676
00:40:34,475 --> 00:40:38,095
Speaker 5:  All right, we're back. It feels like we should update everybody on what's

677
00:40:38,095 --> 00:40:41,495
Speaker 5:  going on with OpenAI this week. Yeah, it feels like there was a run of instantly

678
00:40:41,575 --> 00:40:45,535
Speaker 5:  obsolete emergency podcasts last week. Including

679
00:40:45,535 --> 00:40:49,495
Speaker 5:  our own, including our own. It's a great podcast. It wass 24 hours. It

680
00:40:49,495 --> 00:40:50,055
Speaker 5:  was obsolete. We

681
00:40:50,055 --> 00:40:50,975
Speaker 1:  Did our best everybody. I enjoyed

682
00:40:50,975 --> 00:40:54,615
Speaker 5:  Listening to it. Yeah, it was fun people, people liked it instantly

683
00:40:54,615 --> 00:40:58,535
Speaker 5:  obsolete. So here's what we know and we can kind

684
00:40:58,535 --> 00:41:01,575
Speaker 5:  of unpack what we think it means because there's some stuff we don't know.

685
00:41:02,205 --> 00:41:06,175
Speaker 5:  What we know is that Sam Altman is officially back as CEO. He

686
00:41:07,225 --> 00:41:10,725
Speaker 5:  is not on the board of directors. Ilya, the

687
00:41:10,935 --> 00:41:14,845
Speaker 5:  chief scientist, does not appear to be working at OpenAI again, is not

688
00:41:14,865 --> 00:41:16,085
Speaker 5:  on the new board of directors.

689
00:41:17,615 --> 00:41:20,965
Speaker 5:  Craig Bachman, the president, is back as president. The new board of directors

690
00:41:20,965 --> 00:41:24,885
Speaker 5:  has begun. Brett Taylor, the former CEO of Salesforce on the

691
00:41:24,885 --> 00:41:27,485
Speaker 5:  board, former treasury secretary, Larry Summers

692
00:41:27,905 --> 00:41:29,205
Speaker 7:  Has interesting ideas about women.

693
00:41:29,385 --> 00:41:30,765
Speaker 5:  Yes. He thinks they're bad at math.

694
00:41:31,485 --> 00:41:34,965
Speaker 7:  I mean, I have said I'm bad at math on this very podcast. That's that's true.

695
00:41:35,465 --> 00:41:38,605
Speaker 7:  I'm sorry to all other women. There are many very women who are excellent

696
00:41:38,605 --> 00:41:39,845
Speaker 7:  at math. I'm okay.

697
00:41:40,315 --> 00:41:43,325
Speaker 5:  Yeah. But Larry, Larry. So there's just some weird ideas. But he's apparently

698
00:41:43,325 --> 00:41:47,125
Speaker 5:  been in the mix as an adult. And then Adam D'Angelo, who was on the old board,

699
00:41:47,585 --> 00:41:51,285
Speaker 5:  one of the guys who voted to remove Sam and Greg

700
00:41:51,385 --> 00:41:54,085
Speaker 5:  in the first place. Yeah. And apparently Adam,

701
00:41:55,195 --> 00:41:59,045
Speaker 5:  they've like Adam and Sam have like hung out. Cool. So the new

702
00:41:59,045 --> 00:42:03,005
Speaker 5:  board of directors, its job is to make the actual board of directors and

703
00:42:03,005 --> 00:42:06,805
Speaker 5:  fill out this board and then Microsoft is taking an observer seat

704
00:42:07,585 --> 00:42:11,565
Speaker 5:  and Microsoft will not tell us who is taking that seat. We have asked and

705
00:42:11,565 --> 00:42:13,845
Speaker 5:  they said we're not, we're not gonna tell you. Do

706
00:42:13,845 --> 00:42:16,925
Speaker 7:  You think it could just be some like guy at mic or girl at Microsoft?

707
00:42:17,085 --> 00:42:20,845
Speaker 5:  I think the only rational answer is Kevin Scott, the head of AI at

708
00:42:21,085 --> 00:42:23,925
Speaker 5:  Microsoft. But for some reason Microsoft, I don't, I don't know this. Yeah,

709
00:42:24,225 --> 00:42:25,925
Speaker 5:  I'm just saying if I had to look at Microsoft,

710
00:42:26,655 --> 00:42:28,725
Speaker 7:  Maybe they're all on vacation. They haven't decided yet.

711
00:42:28,875 --> 00:42:29,765
Speaker 5:  It's not Phil Spencer.

712
00:42:30,305 --> 00:42:34,245
Speaker 6:  Is it possible Microsoft doesn't know? I mean it, I I just keep going back

713
00:42:34,245 --> 00:42:37,805
Speaker 6:  to, so where we left our emergency podcast was

714
00:42:38,545 --> 00:42:41,925
Speaker 6:  we thought that Sam and Greg were

715
00:42:42,725 --> 00:42:45,725
Speaker 6:  Microsoft employees who were going to start a new thing at Microsoft. And

716
00:42:45,725 --> 00:42:48,805
Speaker 6:  that was the thing that became most instantly obsolete. Yep. Because suddenly

717
00:42:48,825 --> 00:42:52,085
Speaker 6:  it became very clear that actually they were probably going back to OpenAI

718
00:42:52,105 --> 00:42:55,925
Speaker 6:  and it was never really official. But then Satya Nadella does

719
00:42:56,005 --> 00:42:59,765
Speaker 6:  a round of media interviews in which I don't wanna say it didn't sound like

720
00:42:59,765 --> 00:43:02,725
Speaker 6:  he knew what was going on, but it kind of sounded like he didn't know what

721
00:43:02,725 --> 00:43:06,365
Speaker 6:  was going on. It was, it, it got to the point where it was moving

722
00:43:06,585 --> 00:43:10,165
Speaker 6:  so fast that it seemed like even he, the person

723
00:43:10,425 --> 00:43:13,245
Speaker 6:  who was by all accounts kind of brokering this whole thing

724
00:43:15,025 --> 00:43:17,765
Speaker 6:  was caught off guard by some of the turns of it.

725
00:43:17,945 --> 00:43:21,685
Speaker 5:  Yes. I can call her in some there. Okay. I, I think there was a, a

726
00:43:21,685 --> 00:43:24,965
Speaker 5:  notion that Nadela was trying to make it happen.

727
00:43:25,925 --> 00:43:29,425
Speaker 5:  I think that ultimately he was involved in some

728
00:43:29,425 --> 00:43:33,265
Speaker 5:  conversations. But we, we now know that Airbnb, CEO, Brian

729
00:43:33,425 --> 00:43:36,945
Speaker 5:  Chesky, who's a friend of Sam Emmett, she who's the former CEO of Twitch

730
00:43:37,005 --> 00:43:38,585
Speaker 5:  who became the interim CEO of OpenAI

731
00:43:38,965 --> 00:43:39,745
Speaker 6:  For five minutes

732
00:43:40,885 --> 00:43:43,825
Speaker 5:  And Larry Summers and some other folks, they were the ones actually sort

733
00:43:43,825 --> 00:43:47,185
Speaker 5:  of like mediating this conversation. And Nadella was just like around

734
00:43:48,225 --> 00:43:50,865
Speaker 5:  'cause he wanted to protect Microsoft's interest. Okay. The other thing I

735
00:43:50,865 --> 00:43:53,825
Speaker 5:  know is that that round of media tours is our fault

736
00:43:55,345 --> 00:43:58,505
Speaker 5:  straight up. Oh, interesting. That's, that's our fault. It's because

737
00:43:59,055 --> 00:44:02,865
Speaker 5:  Alex and I reported that Sam and Greg

738
00:44:02,865 --> 00:44:06,785
Speaker 5:  were trying to go back to OpenAI after Microsoft had made its big announcement.

739
00:44:06,885 --> 00:44:09,145
Speaker 5:  So Microsoft had made its big announcement they're all gonna come work at,

740
00:44:09,165 --> 00:44:12,345
Speaker 5:  at Microsoft. We're gonna start a new AI thing, the stock goes up,

741
00:44:13,215 --> 00:44:17,025
Speaker 5:  then we report, ah, maybe this won't happen. And now there's like instability.

742
00:44:17,025 --> 00:44:20,785
Speaker 5:  We got jittery investors and of course Nadela goes on CNBC and

743
00:44:20,895 --> 00:44:24,785
Speaker 5:  Bloomberg to say it's all good. Like doesn't matter

744
00:44:24,785 --> 00:44:28,065
Speaker 5:  which way's happening, Microsoft is moving forward with Sam Altman, which

745
00:44:28,065 --> 00:44:31,965
Speaker 5:  was very much his message. And he kept saying, and I thought this was

746
00:44:31,965 --> 00:44:35,285
Speaker 5:  just the funniest thing, he kept saying, irrespective of configuration,

747
00:44:36,175 --> 00:44:39,525
Speaker 5:  which just sounds like he was announcing plug and play device drivers.

748
00:44:40,595 --> 00:44:43,525
Speaker 5:  Yeah. It doesn't matter irrespective of how your dip switches are set. Like

749
00:44:43,525 --> 00:44:47,285
Speaker 5:  we will find this USB device, we will find time

750
00:44:47,485 --> 00:44:50,925
Speaker 5:  Albin like he's not wrong. It doesn't matter what order you put the cards

751
00:44:50,945 --> 00:44:54,885
Speaker 5:  in with PCI express for, you know, it's like irrespective

752
00:44:55,145 --> 00:44:58,445
Speaker 5:  con configuration. He just kept saying it. It was amazing.

753
00:45:00,025 --> 00:45:03,965
Speaker 5:  So I. I think that was that, right? Like we had, because of

754
00:45:03,965 --> 00:45:07,165
Speaker 5:  our reporting, we had made it clear like this isn't a done deal

755
00:45:07,865 --> 00:45:11,645
Speaker 5:  And So I think he was compelled to go out and

756
00:45:11,705 --> 00:45:15,605
Speaker 5:  say, yeah, I actually dunno what's going on. Like it's gonna land one

757
00:45:15,605 --> 00:45:18,845
Speaker 5:  way or the other. But our investment in OpenAI is safe because either we

758
00:45:18,845 --> 00:45:22,765
Speaker 5:  will take all of these employees at very high rates, which is

759
00:45:22,845 --> 00:45:26,525
Speaker 5:  a massive investment for Microsoft to make, to protect this investment. Or

760
00:45:26,535 --> 00:45:30,205
Speaker 5:  we'll find a way to keep working with OpenAI. And they asked him, I think

761
00:45:30,205 --> 00:45:33,165
Speaker 5:  it was Emily Chang and Bloomberg asked him, who will be the CEO of OpenAI

762
00:45:33,365 --> 00:45:36,725
Speaker 5:  tomorrow? And he looked exhausted and he said

763
00:45:37,625 --> 00:45:39,045
Speaker 5:  that's for the board to decide.

764
00:45:41,665 --> 00:45:45,485
Speaker 5:  Don don't know man. Oh he also did, he also did on

765
00:45:45,485 --> 00:45:49,405
Speaker 5:  with Kara Swisher. Yeah. And it was, it was, you know, I interviewed a lot

766
00:45:49,405 --> 00:45:53,255
Speaker 5:  of CEOs. Sometimes CEOs just have a thing they're gonna say and he

767
00:45:53,255 --> 00:45:56,855
Speaker 5:  said the thing in all three places. Yeah. But I know that what was happening

768
00:45:56,855 --> 00:46:00,775
Speaker 5:  there was the market thought that this thing

769
00:46:00,775 --> 00:46:04,735
Speaker 5:  was a done deal and it became quickly apparent that it

770
00:46:04,735 --> 00:46:07,735
Speaker 5:  was actually not a done deal and actively trying to go the other way. Which

771
00:46:07,735 --> 00:46:11,255
Speaker 5:  it did. So I that, which it did. Yeah. In the end. Which in the end it did.

772
00:46:11,775 --> 00:46:15,735
Speaker 5:  And Microsoft seems very happy and Sam notably like tweeted

773
00:46:16,355 --> 00:46:19,975
Speaker 5:  my thanks to Satya for his support, So I. I think everybody wanted

774
00:46:20,205 --> 00:46:23,335
Speaker 5:  this to happen. And I think at one point

775
00:46:23,925 --> 00:46:27,175
Speaker 5:  Nadella said to to Kara or Emily, like, surprises are bad.

776
00:46:27,765 --> 00:46:30,575
Speaker 5:  Like we don't wanna be surprised like this Anyway. Yeah, we'll fix the governance

777
00:46:30,575 --> 00:46:34,455
Speaker 5:  of the company. So you've got the board observer seat, which again, I don't

778
00:46:34,455 --> 00:46:38,415
Speaker 5:  know who it will be. Yeah. but it just feels like

779
00:46:38,415 --> 00:46:42,215
Speaker 5:  you want that to be Kevin Scott, your head of AI or maybe Brad Smith or

780
00:46:42,215 --> 00:46:45,935
Speaker 5:  their chief legal officer. Like that's who you want in that spot. Just some

781
00:46:45,955 --> 00:46:49,935
Speaker 5:  pm But I don't think you can, I I personally do not think you could put

782
00:46:50,175 --> 00:46:53,935
Speaker 5:  Scha there. Yeah. I think you need to a little bit of distance from the company

783
00:46:54,035 --> 00:46:56,855
Speaker 5:  to Yeah. To claim that you're not running the company. So if you put your

784
00:46:56,915 --> 00:47:00,695
Speaker 5:  famous high profile CEO on this board, now you've just got another,

785
00:47:01,165 --> 00:47:04,535
Speaker 5:  another part of Microsoft, which I think Microsoft wants to keep a little

786
00:47:04,535 --> 00:47:08,455
Speaker 5:  bit of that distance. That's what we know so far. Then there's this

787
00:47:08,455 --> 00:47:12,015
Speaker 5:  weirdness where Alex Heath got a quick interview with Sam

788
00:47:12,255 --> 00:47:16,135
Speaker 5:  Altman and Mira Mirati, the former CTO who was the

789
00:47:16,135 --> 00:47:20,055
Speaker 5:  first interim CEO who is now the CTO again. You did it God at

790
00:47:20,055 --> 00:47:22,495
Speaker 5:  that time. Yes. Nailed it. Just so you know, that was like the third take

791
00:47:22,875 --> 00:47:23,255
Speaker 5:  of that.

792
00:47:25,635 --> 00:47:27,975
Speaker 5:  And you know, Alex is like, what should I ask? I was like only one question,

793
00:47:27,975 --> 00:47:30,685
Speaker 5:  like why'd you get fired? And Alex asked this question five times in a row

794
00:47:30,705 --> 00:47:32,165
Speaker 5:  and there's no answer to this question

795
00:47:32,705 --> 00:47:36,605
Speaker 7:  That's continues to kind of baffle me. I could understand why the

796
00:47:36,605 --> 00:47:39,525
Speaker 7:  board wasn't saying anything, but like, why isn't he

797
00:47:40,005 --> 00:47:43,845
Speaker 5:  I, don Dunno. We do know that the board has been talking like, I think

798
00:47:43,855 --> 00:47:46,605
Speaker 5:  Helen Toner, who's a member of the board, has been talking to folks saying

799
00:47:46,605 --> 00:47:50,165
Speaker 5:  like, we feel confident in our decision, all this stuff.

800
00:47:50,705 --> 00:47:54,485
Speaker 5:  It is abundantly clear that these ideas that it was about

801
00:47:54,505 --> 00:47:58,485
Speaker 5:  safety or that there was a new model that could destroy the world

802
00:47:58,585 --> 00:48:01,925
Speaker 5:  or do basic math. It's one or the other. It's do basic math or destroy the

803
00:48:01,925 --> 00:48:02,045
Speaker 5:  world.

804
00:48:02,315 --> 00:48:02,765
Speaker 7:  Same thing.

805
00:48:03,265 --> 00:48:06,925
Speaker 5:  But like the notion that that spooked the board and they had to get rid of

806
00:48:06,945 --> 00:48:10,085
Speaker 5:  Sam so they could put in their own person that none of that appears to be

807
00:48:10,085 --> 00:48:13,565
Speaker 5:  true. Right. Like no one on either side is saying this was the reason. It

808
00:48:13,565 --> 00:48:17,285
Speaker 5:  is, it really comes down to interpersonal conflicts

809
00:48:17,435 --> 00:48:21,205
Speaker 5:  between a group of four people and Sam Altman. Yeah. And I think the board

810
00:48:22,395 --> 00:48:26,285
Speaker 5:  basically thinks like they couldn't control him and then whenever

811
00:48:26,285 --> 00:48:30,245
Speaker 5:  they tried he would do something to get out of it. Yeah. And there will be

812
00:48:30,245 --> 00:48:33,995
Speaker 5:  an investigation. We are told, I think the results of that

813
00:48:34,275 --> 00:48:37,915
Speaker 5:  investigation should likely be made public, given the dollar amounts and

814
00:48:37,915 --> 00:48:40,675
Speaker 5:  investors and you know, employees involved.

815
00:48:40,695 --> 00:48:42,035
Speaker 7:  Oh, we're gonna find out. Anyway.

816
00:48:42,355 --> 00:48:45,835
Speaker 5:  I even, I even for open AI's employees, you need to make that investigation.

817
00:48:45,905 --> 00:48:49,195
Speaker 5:  Like why did this happen to us? Right. Yeah. I think there's skittishness

818
00:48:49,195 --> 00:48:52,955
Speaker 5:  on both sides there. And we'll see, and that's another job of this new board

819
00:48:53,135 --> 00:48:55,235
Speaker 5:  run by Brett Taylor is to run that investigation.

820
00:48:55,755 --> 00:48:59,595
Speaker 7:  I think my question is if it was something like we're upset

821
00:48:59,595 --> 00:49:03,475
Speaker 7:  that he's, he's moving too fast, he he's being too commercial and which

822
00:49:03,535 --> 00:49:07,235
Speaker 7:  seemed from the, from the beginning to be one of the issues there. Particularly

823
00:49:07,235 --> 00:49:10,835
Speaker 7:  like his, his conflict with Helen Toner was particularly about that and how

824
00:49:10,835 --> 00:49:14,715
Speaker 7:  she'd written a paper about that. If, if that's the case and they come

825
00:49:14,715 --> 00:49:18,675
Speaker 7:  back and they say, oh, we, we found that was the reason why. What,

826
00:49:18,675 --> 00:49:22,275
Speaker 7:  what do they do? Because they now have a board that is very much for

827
00:49:22,275 --> 00:49:25,835
Speaker 7:  commercialization. They now have their biggest investor in an observer's

828
00:49:25,835 --> 00:49:29,075
Speaker 7:  seat on that board. And the guy who was doing all of that, that they had

829
00:49:29,235 --> 00:49:32,835
Speaker 7:  conflict with is now still CEO of the company. Right. Like,

830
00:49:33,265 --> 00:49:34,195
Speaker 7:  what happens then?

831
00:49:34,465 --> 00:49:38,435
Speaker 5:  Well, they're gone. They, they wiped their hands of this, by

832
00:49:38,435 --> 00:49:41,755
Speaker 5:  the way that Helen Toner paper is worth reading. We'll link to it in the

833
00:49:41,755 --> 00:49:45,155
Speaker 5:  show notes or something. The, you know, the part that everyone is up that

834
00:49:45,435 --> 00:49:49,195
Speaker 5:  apparently Sam was up in arms about is she just points out that the industry

835
00:49:49,215 --> 00:49:52,995
Speaker 5:  had been very cautious until the success of chat GPT

836
00:49:53,415 --> 00:49:56,795
Speaker 5:  and then everyone started moving really fast, which, and they had abandoned

837
00:49:56,855 --> 00:50:00,595
Speaker 5:  the caution in favor of whatever commercial outcomes they could see,

838
00:50:00,685 --> 00:50:01,035
Speaker 5:  which

839
00:50:01,035 --> 00:50:01,795
Speaker 7:  Is exactly what

840
00:50:01,915 --> 00:50:05,155
Speaker 5:  Happened, which is exactly what happened. And it, it's actually really hard

841
00:50:05,155 --> 00:50:08,955
Speaker 5:  to read a criticism of OpenAI into it. She's just like,

842
00:50:08,955 --> 00:50:12,715
Speaker 5:  compare and contrast. Like we had, you know, the very famously philanthropic

843
00:50:12,765 --> 00:50:16,355
Speaker 5:  split from OpenAI over safety concerns and they're like, they waited

844
00:50:16,975 --> 00:50:20,885
Speaker 5:  but everyone else rushed. Yeah. After we had our success. That's

845
00:50:20,885 --> 00:50:21,085
Speaker 5:  true.

846
00:50:21,175 --> 00:50:24,805
Speaker 6:  Right. Well, and what's what's funny about that is it, there's a version

847
00:50:24,825 --> 00:50:28,765
Speaker 6:  of that story that I think is true that says OpenAI didn't do

848
00:50:28,765 --> 00:50:31,965
Speaker 6:  that on purpose, right? Like I think Yeah. What what

849
00:50:32,565 --> 00:50:36,045
Speaker 6:  Microsoft did with binging was just ruthlessly

850
00:50:36,045 --> 00:50:39,125
Speaker 6:  capitalistic, right? They just said like, yeah, I gotta figure out how to

851
00:50:39,125 --> 00:50:42,925
Speaker 6:  make money off of this. And that's, that's what they did. But like today,

852
00:50:44,005 --> 00:50:46,925
Speaker 6:  Thursday, as we're recording, this is the one year anniversary of the chat

853
00:50:47,045 --> 00:50:50,845
Speaker 6:  GPT launch. And like the, the thing we

854
00:50:51,075 --> 00:50:54,685
Speaker 6:  know beyond a shadow of a doubt at this point is that no one at

855
00:50:54,865 --> 00:50:58,725
Speaker 6:  OpenAI or Microsoft or anywhere launched chat GPT thinking

856
00:50:58,725 --> 00:51:02,005
Speaker 6:  it was gonna take over the world. There was this like long term plan that

857
00:51:02,005 --> 00:51:04,725
Speaker 6:  everybody was on. And it's funny if you rewind back to

858
00:51:06,005 --> 00:51:09,485
Speaker 6:  pre-chat GPT and sort of asked a bunch of people at these companies what

859
00:51:09,485 --> 00:51:12,285
Speaker 6:  they thought the future of AI was gonna look like. They all would've been

860
00:51:12,315 --> 00:51:15,805
Speaker 6:  more or less right, just wrong about the timeline. And then

861
00:51:16,035 --> 00:51:19,125
Speaker 6:  chat GPT happened and everybody went, oh crap, this stuff that we've been

862
00:51:19,125 --> 00:51:22,805
Speaker 6:  working on in the background towards this interesting long future

863
00:51:23,265 --> 00:51:26,805
Speaker 6:  we have to do today. And that's, that's what Google did. It was like the,

864
00:51:26,805 --> 00:51:29,845
Speaker 6:  what was it? The code red within Google that that Yeah. Sergey and Larry

865
00:51:29,905 --> 00:51:33,845
Speaker 6:  put up. Microsoft starts running, everybody just starts running. And it

866
00:51:33,865 --> 00:51:37,725
Speaker 6:  did it, it lit this fire in the tech industry that took it down a bunch

867
00:51:37,745 --> 00:51:41,485
Speaker 6:  of how do we make money off of AI questions

868
00:51:42,035 --> 00:51:45,805
Speaker 6:  from people who had previously been saying, we have to do this

869
00:51:45,805 --> 00:51:48,165
Speaker 6:  carefully. We have to be thoughtful about this, we have to be smart about

870
00:51:48,165 --> 00:51:52,085
Speaker 6:  this. but it all happened like by accident. It's the

871
00:51:52,085 --> 00:51:55,725
Speaker 6:  weirdest sort of twist of fate that it wasn't like somebody launched the

872
00:51:55,885 --> 00:51:59,085
Speaker 6:  revolutionary product knowing it was the revolutionary product and like turned

873
00:51:59,085 --> 00:52:02,685
Speaker 6:  everything on its head. It's just like somebody just like farted out a blog

874
00:52:02,685 --> 00:52:04,925
Speaker 6:  post and then the world changed. Like it's nuts.

875
00:52:06,515 --> 00:52:09,965
Speaker 5:  Well yeah, that's the ChatGPT side of it, which is,

876
00:52:10,545 --> 00:52:13,285
Speaker 5:  you should read David's piece. It's the fastest growing consumer product

877
00:52:14,185 --> 00:52:18,165
Speaker 5:  we should reckon with it. But you know, satin Anella did

878
00:52:18,165 --> 00:52:20,965
Speaker 5:  look me in the eye and say, I want Google to know I made them dance. Yeah,

879
00:52:20,965 --> 00:52:24,725
Speaker 5:  yeah. Oh yeah. Like he was, he was headed that way regardless, right?

880
00:52:24,835 --> 00:52:28,445
Speaker 6:  Yeah. But I don't think he would've gotten there in February of 2023. Right?

881
00:52:28,765 --> 00:52:31,365
Speaker 5:  I, don Dunno. He did make the giant investment, right? Like, yeah, this,

882
00:52:31,515 --> 00:52:33,205
Speaker 5:  it's hard, it's hard to know. But like

883
00:52:34,725 --> 00:52:38,565
Speaker 5:  Microsoft do was doing co-pilots already. They had started doing this stuff

884
00:52:38,565 --> 00:52:41,685
Speaker 5:  in a very small way. Yeah. And I think the part of it where

885
00:52:42,745 --> 00:52:46,155
Speaker 5:  the stack overflows of the world were just

886
00:52:46,265 --> 00:52:49,755
Speaker 5:  overrun with, oh, this is better if I just ask an AI to do it. And that's

887
00:52:49,755 --> 00:52:52,675
Speaker 5:  a real thing that's going on. Yeah. Maybe that would've happened anyway,

888
00:52:52,675 --> 00:52:56,035
Speaker 5:  but the sort of like, I'll have it right in my resume for me.

889
00:52:56,785 --> 00:53:00,275
Speaker 5:  Like just sort of like, whatever's gonna go on with the LLMs and

890
00:53:00,275 --> 00:53:03,315
Speaker 7:  You shouldn't, by the way, have it write your resume.

891
00:53:04,065 --> 00:53:06,635
Speaker 5:  Well, no. It'll hallucinate some grant on. Exactly. That's

892
00:53:08,175 --> 00:53:11,515
Speaker 7:  If it's gonna be FactCheck, you shouldn't, if it's not, do it. Yeah.

893
00:53:12,585 --> 00:53:13,275
Speaker 5:  Make me look good.

894
00:53:14,895 --> 00:53:17,555
Speaker 5:  And that's actually the, the p the part of your piece, David, that I think

895
00:53:17,555 --> 00:53:21,275
Speaker 5:  is the most resonant for me, which is now we have these tools and everyone

896
00:53:21,275 --> 00:53:24,275
Speaker 5:  thinks they're gonna change everything. I am personally still stuck at

897
00:53:25,115 --> 00:53:28,445
Speaker 5:  Yeah. But they're not very good yet. Like they're good at some things.

898
00:53:29,325 --> 00:53:33,295
Speaker 5:  Programming a computer, everyone constantly telling me they're great

899
00:53:33,295 --> 00:53:37,215
Speaker 5:  at this in a way that is revolutionary. And that's because it's programming

900
00:53:37,375 --> 00:53:41,055
Speaker 5:  computers effectively just delivering instructions. Right. Make me a set

901
00:53:41,055 --> 00:53:44,895
Speaker 5:  of instructions such that some input in the computer generates some output.

902
00:53:45,885 --> 00:53:49,815
Speaker 5:  Yeah. That's a deterministic task. You can like set an AI to

903
00:53:49,815 --> 00:53:53,615
Speaker 5:  it. Many people have done it before. The internet is full of this knowledge.

904
00:53:53,615 --> 00:53:57,415
Speaker 5:  You can get there, write me a new idea. Something like

905
00:53:57,415 --> 00:54:01,175
Speaker 5:  inherently creative. I still think they fall down all the time. These

906
00:54:01,175 --> 00:54:05,095
Speaker 5:  non-deterministic tasks, they fall down all the time. Yeah. And, and I continue

907
00:54:05,095 --> 00:54:09,055
Speaker 5:  to point out they're all these companies are one copyright decision away

908
00:54:09,525 --> 00:54:13,415
Speaker 5:  from just non existing. Yeah. And that's re and like no

909
00:54:13,415 --> 00:54:16,575
Speaker 5:  one, it's just like, well, let's just not look at that time bomb.

910
00:54:17,495 --> 00:54:17,785
Speaker 7:  Yeah.

911
00:54:17,895 --> 00:54:19,745
Speaker 5:  That seems Nah. Yeah.

912
00:54:20,145 --> 00:54:23,145
Speaker 7:  I, I love it as a, as a person who loves to do that sort of thing, just be

913
00:54:23,145 --> 00:54:26,185
Speaker 7:  like, oh, I'm not gonna worry about that for now. Just put my blinders on

914
00:54:26,185 --> 00:54:26,865
Speaker 7:  and go straight ahead.

915
00:54:26,865 --> 00:54:27,665
Speaker 5:  Yeah. That car's on fire.

916
00:54:27,975 --> 00:54:29,905
Speaker 7:  It's fine. Don't worry about it.

917
00:54:30,035 --> 00:54:33,985
Speaker 6:  There was a great, Corey au the, the author and EFF activist who's

918
00:54:33,985 --> 00:54:37,505
Speaker 6:  been on this show a few times, wrote a really good thing this week that I'll,

919
00:54:37,505 --> 00:54:40,385
Speaker 6:  I'll try and make sure we link to in the show notes, basically saying he

920
00:54:40,385 --> 00:54:43,665
Speaker 6:  quoted this guy named Lee Vinzel who talks about this concept of crita hype,

921
00:54:43,955 --> 00:54:47,585
Speaker 6:  which is basically taking the sort of

922
00:54:47,765 --> 00:54:51,705
Speaker 6:  truths that the most optimistic people who are usually the people

923
00:54:51,705 --> 00:54:55,305
Speaker 6:  making products, believing that they're going to be right

924
00:54:55,605 --> 00:54:58,945
Speaker 6:  and then covering it with hell scapes is the way he describes it. So it's

925
00:54:58,945 --> 00:55:02,585
Speaker 6:  like to talk about this world in which AI takes over everything and

926
00:55:02,595 --> 00:55:06,505
Speaker 6:  ruins everything, assumes that AI is going to

927
00:55:06,605 --> 00:55:10,505
Speaker 6:  get as good as some of these people say that it is, which they have

928
00:55:10,585 --> 00:55:14,265
Speaker 6:  a massive vested interest in doing. And so like, it's the digital

929
00:55:14,525 --> 00:55:18,385
Speaker 6:  God thing, right? Like Yeah. Just to talk about whether we think a digital

930
00:55:18,525 --> 00:55:22,385
Speaker 6:  God is a good idea or a bad idea is, is to assume that

931
00:55:22,385 --> 00:55:26,185
Speaker 6:  it's gonna happen. Yeah. And we're, we're, there is just no

932
00:55:26,465 --> 00:55:30,305
Speaker 6:  evidence in the AI that exists now that we are headed toward that place

933
00:55:30,305 --> 00:55:33,585
Speaker 6:  anytime soon. And this is like what OpenAI talks about with the artificial

934
00:55:33,585 --> 00:55:37,185
Speaker 6:  general intelligence and a GI and this idea that like, we're on the brink

935
00:55:37,185 --> 00:55:40,265
Speaker 6:  of something that is like you're talking about Eli creative and thoughtful

936
00:55:40,285 --> 00:55:44,265
Speaker 6:  and human and better than us at everything. And there's just no reason

937
00:55:44,285 --> 00:55:47,865
Speaker 6:  to believe right now in the world in that we live in

938
00:55:48,055 --> 00:55:50,745
Speaker 6:  that that's actually coming. And it's like Corey's piece was very smart and

939
00:55:50,745 --> 00:55:53,945
Speaker 6:  I've been thinking about it all week because it's like we, we have to criticize

940
00:55:53,945 --> 00:55:57,265
Speaker 6:  the thing that exists, not the thing that the people who make the thing exist

941
00:55:57,265 --> 00:55:59,185
Speaker 6:  promise us is coming someday soon. Yeah.

942
00:55:59,695 --> 00:56:03,425
Speaker 7:  Well, and I think what's really interesting about that is, is

943
00:56:03,565 --> 00:56:07,265
Speaker 7:  it ignores the stuff that it, that the harms and stuff like the actual concern

944
00:56:07,285 --> 00:56:09,505
Speaker 7:  you should have right now about it, which totally is the stuff we talk about

945
00:56:09,505 --> 00:56:12,865
Speaker 7:  all the time. How it's it's destroying the internet like that's happening

946
00:56:12,865 --> 00:56:16,825
Speaker 7:  right now. Yeah. This second, somebody is writing garb, having

947
00:56:16,825 --> 00:56:20,585
Speaker 7:  it write garbage and putting, publishing it on a website and, and that gets

948
00:56:20,585 --> 00:56:21,945
Speaker 7:  totally ignored to talk

949
00:56:21,945 --> 00:56:25,485
Speaker 5:  About. This week Sports Illustrated got caught. Yeah. Publishing AI

950
00:56:25,485 --> 00:56:29,165
Speaker 5:  generated SEO affiliate Garbage. And they took it down and they

951
00:56:29,165 --> 00:56:32,765
Speaker 5:  blamed a company and like we didn't know they were using ai and then somebody

952
00:56:32,765 --> 00:56:36,525
Speaker 5:  looked up the company on LinkedIn and like literally companies described

953
00:56:36,525 --> 00:56:40,245
Speaker 5:  as using AI and ML to do SEO affiliate. That's like don don't know man.

954
00:56:40,605 --> 00:56:44,325
Speaker 5:  I I think Aon commerce, the AI ML commerce

955
00:56:44,325 --> 00:56:48,245
Speaker 5:  company was probably gonna use AI and ML to make your commerce

956
00:56:48,325 --> 00:56:48,885
Speaker 5:  articles.

957
00:56:49,115 --> 00:56:51,965
Speaker 7:  They actually asked AI what companies should we hire for this?

958
00:56:52,395 --> 00:56:56,285
Speaker 5:  Like it was the most circular excuse in the world, but that's

959
00:56:56,285 --> 00:56:59,365
Speaker 5:  happening. Like story media institutions are chasing

960
00:57:00,635 --> 00:57:04,245
Speaker 5:  just the bottom of the barrel SEO content. Yeah. In a way that will

961
00:57:04,315 --> 00:57:08,165
Speaker 5:  pollute Google and Well, and the internet lots of sort of just like

962
00:57:08,195 --> 00:57:11,925
Speaker 5:  mid-level creatives are gonna lose their jobs over this stuff at the place

963
00:57:11,925 --> 00:57:15,555
Speaker 5:  it is now. David, your point about we can't criticize what's to come is like

964
00:57:15,555 --> 00:57:19,515
Speaker 5:  our oldest rule in reviewing products is

965
00:57:19,515 --> 00:57:23,355
Speaker 5:  you review what comes in the box. Yep. And when the company promises you

966
00:57:23,515 --> 00:57:26,475
Speaker 5:  a software update, we'll fix it. You just, we always say All, right? Let

967
00:57:26,475 --> 00:57:29,715
Speaker 5:  us know when it comes out. Yeah. We're not gonna review this promise against

968
00:57:29,735 --> 00:57:33,155
Speaker 5:  the promises. Sorry. So harsh. Sorry.

969
00:57:34,255 --> 00:57:34,475
Speaker 5:  We

970
00:57:35,285 --> 00:57:35,635
Speaker 7:  Sorry.

971
00:57:35,635 --> 00:57:38,795
Speaker 5:  They just put out a next product. Yeah, that's a great example actually.

972
00:57:38,795 --> 00:57:41,835
Speaker 5:  Yeah. We are, we are like, we will not review a product against the promise

973
00:57:41,855 --> 00:57:44,555
Speaker 5:  of a software update and hope bill's like here's a new camera,

974
00:57:45,895 --> 00:57:47,235
Speaker 5:  no software updates for you.

975
00:57:48,375 --> 00:57:48,595
Speaker 7:  Aw,

976
00:57:48,905 --> 00:57:52,795
Speaker 5:  It's brutal. Yeah. Yeah. And so here we are with the LLMs and some

977
00:57:52,795 --> 00:57:54,835
Speaker 5:  of them are good and some of they're bad and like they all have different

978
00:57:54,835 --> 00:57:58,715
Speaker 5:  capabilities, but GBT four right now is not a, a threat

979
00:57:58,775 --> 00:57:59,355
Speaker 5:  to the world.

980
00:58:00,245 --> 00:58:00,535
Speaker 6:  Yeah,

981
00:58:01,135 --> 00:58:02,335
Speaker 7:  I agree with that. Just a threat to bloggers.

982
00:58:02,605 --> 00:58:05,455
Speaker 5:  It's just, yeah. It's just a threat to the people who make affiliate commerce

983
00:58:05,505 --> 00:58:05,855
Speaker 5:  links

984
00:58:05,855 --> 00:58:08,415
Speaker 6:  And our ability to do functional Google searches.

985
00:58:09,465 --> 00:58:12,355
Speaker 7:  Yeah. Well I think we're gonna see it like in YouTube and we're seeing those

986
00:58:12,355 --> 00:58:15,835
Speaker 7:  other mediums too, right? Like just the ability to make, to generate

987
00:58:16,585 --> 00:58:20,475
Speaker 7:  mass amounts of garbage into a world that is largely driven by

988
00:58:20,475 --> 00:58:24,075
Speaker 7:  algorithms that are predisposed to that garbage because they're algorithms

989
00:58:24,075 --> 00:58:27,715
Speaker 7:  and they think like computers just the way this garbage being generated does,

990
00:58:27,715 --> 00:58:30,955
Speaker 7:  like it's just this weird gross snake eating its tail

991
00:58:31,655 --> 00:58:33,195
Speaker 7:  and we all have to sit in it. I

992
00:58:33,355 --> 00:58:37,155
Speaker 5:  Need to, I'm, I'm assigning myself a story. So, I have this idea that

993
00:58:37,155 --> 00:58:40,115
Speaker 5:  you can make a list of all of Google's platforms

994
00:58:40,975 --> 00:58:44,355
Speaker 5:  and then you can make a list of all of Google's tools and you can just draw

995
00:58:44,355 --> 00:58:46,315
Speaker 5:  a line from one to the other and be like, here's the problem.

996
00:58:48,295 --> 00:58:52,035
Speaker 5:  You know, like you can draw a line from the Pixel camera using

997
00:58:52,335 --> 00:58:56,235
Speaker 5:  AI to do photos Yeah. To YouTube. It's has a rule.

998
00:58:56,265 --> 00:58:59,795
Speaker 5:  It's like you need to disclose the AI in the pho in the video.

999
00:59:00,375 --> 00:59:04,075
Speaker 5:  And you're like, well, have you guys talked? And they're like, no,

1000
00:59:04,145 --> 00:59:04,595
Speaker 5:  that should

1001
00:59:04,595 --> 00:59:07,915
Speaker 6:  Be the headline. This is a column that you just described, NELI called, have

1002
00:59:07,915 --> 00:59:08,555
Speaker 6:  these guys talked.

1003
00:59:10,895 --> 00:59:14,315
Speaker 5:  And it's just literally drawing a line between any Google platform and any

1004
00:59:14,315 --> 00:59:18,195
Speaker 5:  Google tool. Like Google Docs now let's you do AI generated thing and

1005
00:59:18,585 --> 00:59:22,205
Speaker 5:  like you go talk to the, the search team and he'd be like, do you know about

1006
00:59:22,205 --> 00:59:26,125
Speaker 5:  this? And they're like, huh. It's just like a big circle. And like

1007
00:59:26,335 --> 00:59:30,125
Speaker 5:  every part of Google has this problem, which I think is utterly

1008
00:59:30,405 --> 00:59:33,885
Speaker 5:  fascinating. But that, and that's just one company that's,

1009
00:59:34,115 --> 00:59:37,645
Speaker 5:  that runs both platforms. Another one, just this last week,

1010
00:59:38,485 --> 00:59:42,365
Speaker 5:  the Bard team extended Bard such that you can just give

1011
00:59:42,365 --> 00:59:46,285
Speaker 5:  it a YouTube video and it'll summarize it for you. You know, like if if it's

1012
00:59:46,285 --> 00:59:50,245
Speaker 5:  a recipe, it'll just like pull out the recipe. Does the creator get a a YouTube

1013
00:59:50,245 --> 00:59:51,765
Speaker 5:  view when that happens? Does

1014
00:59:51,765 --> 00:59:52,525
Speaker 7:  YouTube know about that?

1015
00:59:52,635 --> 00:59:56,405
Speaker 5:  Does this, does YouTube know? That's weird, right? Like the, the YouTube

1016
00:59:56,405 --> 01:00:00,265
Speaker 5:  is an economy, like it is the

1017
01:00:00,415 --> 01:00:04,385
Speaker 5:  gold standard in creator platforms where people can make a living and bard's

1018
01:00:04,385 --> 01:00:07,625
Speaker 5:  like, what if we just pull everything off of it and summarize the videos

1019
01:00:07,725 --> 01:00:08,465
Speaker 5:  for people. Little

1020
01:00:08,465 --> 01:00:12,425
Speaker 7:  12 year olds watching, like reading Mr Beast videos in Bard.

1021
01:00:12,425 --> 01:00:12,825
Speaker 7:  Now he

1022
01:00:12,825 --> 01:00:15,065
Speaker 5:  Needs to start feeding Mr. Beast videos to Bard. And Bard would be like,

1023
01:00:15,335 --> 01:00:16,905
Speaker 5:  this is not a good use of your time.

1024
01:00:17,365 --> 01:00:18,865
Speaker 6:  Google summarize. It

1025
01:00:18,865 --> 01:00:19,145
Speaker 5:  Shuts down.

1026
01:00:20,045 --> 01:00:23,945
Speaker 6:  No. Eli, you just described a decoder series and, and

1027
01:00:23,945 --> 01:00:26,185
Speaker 6:  you know how much I love talking. Have you guys talked, have you guys talked?

1028
01:00:26,485 --> 01:00:30,065
Speaker 6:  You just bring two people in and you just say, have you guys talked? And

1029
01:00:30,065 --> 01:00:30,745
Speaker 6:  then you just leave

1030
01:00:32,645 --> 01:00:36,265
Speaker 6:  and, and it's like, it's like a big brother style. We would just record them

1031
01:00:36,265 --> 01:00:38,665
Speaker 6:  with security cameras for an hour and that's decoder.

1032
01:00:39,025 --> 01:00:42,825
Speaker 5:  That's perfect. It's just us making different parts of Google

1033
01:00:42,855 --> 01:00:43,585
Speaker 5:  talk to each other.

1034
01:00:44,565 --> 01:00:45,865
Speaker 6:  You could run that show for a decade.

1035
01:00:46,135 --> 01:00:49,945
Speaker 5:  This is one of my favorite stories about early Verge,

1036
01:00:49,945 --> 01:00:53,705
Speaker 5:  just like us learning how to do reporting, how everything

1037
01:00:53,705 --> 01:00:57,625
Speaker 5:  worked. And remember early Verge is like early in lifecycle all

1038
01:00:57,625 --> 01:01:00,465
Speaker 5:  these companies too. Yeah. So there was just one time

1039
01:01:01,435 --> 01:01:05,345
Speaker 5:  where basically we told Google that they had scheduled

1040
01:01:05,345 --> 01:01:07,345
Speaker 5:  two events on the same day because

1041
01:01:07,345 --> 01:01:07,865
Speaker 7:  They didn't

1042
01:01:07,865 --> 01:01:11,625
Speaker 5:  Know because they hadn't talked. And like, and we didn't, we were

1043
01:01:11,985 --> 01:01:15,705
Speaker 5:  babies. So we were like, did you, do you have the same,

1044
01:01:15,965 --> 01:01:18,945
Speaker 5:  is that okay? Is that how we normally? And they're like, no, we have to talk

1045
01:01:18,945 --> 01:01:20,265
Speaker 5:  to each other. I'm like, are we in trouble?

1046
01:01:24,965 --> 01:01:28,065
Speaker 5:  But no, they were just, Google was so big and sprawling that the two teams

1047
01:01:28,305 --> 01:01:32,185
Speaker 5:  hadn't talked and they had both scheduled events in the same day. Oh

1048
01:01:32,205 --> 01:01:35,305
Speaker 5:  boy. And and it was us. We had told him. Yeah, it was very good.

1049
01:01:35,975 --> 01:01:39,505
Speaker 5:  Have you talked the extension of Decoder coming soon to a podcast player?

1050
01:01:40,565 --> 01:01:42,745
Speaker 5:  All right? We should take a break. We're gonna come back. We'll do a lightning

1051
01:01:42,745 --> 01:01:42,905
Speaker 5:  Run.

1052
01:03:47,815 --> 01:03:51,235
Speaker 5:  All, right? We're back with the Cybertruck Lightning round sponsored by no

1053
01:03:51,235 --> 01:03:55,155
Speaker 5:  one. This could be you. I'm looking directly

1054
01:03:55,155 --> 01:03:58,915
Speaker 5:  at you advertisers by not sponsoring the lightning round. You are

1055
01:03:58,915 --> 01:03:59,515
Speaker 5:  blackmailing

1056
01:04:03,745 --> 01:04:04,235
Speaker 7:  Stop it.

1057
01:04:04,575 --> 01:04:08,395
Speaker 5:  And I, you know, the Woke Mind virus is overtaken America and

1058
01:04:08,395 --> 01:04:11,755
Speaker 5:  that's why the Lightning Round does not have a sponsor and that's the first

1059
01:04:11,785 --> 01:04:12,075
Speaker 5:  cast

1060
01:04:13,995 --> 01:04:14,475
Speaker 5:  possibly forever.

1061
01:04:18,125 --> 01:04:19,595
Speaker 5:  There you go. All, right? Alex what you got?

1062
01:04:20,345 --> 01:04:23,875
Speaker 7:  Okay. So. I. I'm gonna continue our theme of talking a little bit of boardroom,

1063
01:04:24,105 --> 01:04:25,515
Speaker 7:  boardroom drama. That's,

1064
01:04:25,515 --> 01:04:26,555
Speaker 5:  That's David's favorite. Yeah.

1065
01:04:26,555 --> 01:04:30,235
Speaker 7:  David, get excited. Okay. But this one is fun because it's Disney

1066
01:04:30,415 --> 01:04:34,315
Speaker 7:  and Yeah. And what actually happens here could really affect Disney

1067
01:04:34,905 --> 01:04:38,835
Speaker 7:  because, so there's an activist investor, his name is Nelson Peltz, and

1068
01:04:38,865 --> 01:04:42,515
Speaker 7:  he's really good friends with Ike Perlmutter. And Ike Perlmutter used to

1069
01:04:42,515 --> 01:04:45,755
Speaker 7:  run Marvel back in the day and he got fired earlier this year. Another

1070
01:04:45,755 --> 01:04:46,915
Speaker 5:  Guy with ideas and he,

1071
01:04:47,255 --> 01:04:50,315
Speaker 7:  Boy does he have a lot of ideas. He was like, you know what, we don't need

1072
01:04:50,765 --> 01:04:54,635
Speaker 7:  women not a fan of them. Yeah, yeah.

1073
01:04:54,735 --> 01:04:57,315
Speaker 7:  You know, he, he didn't want a Black Widow movie, he didn't want a captive

1074
01:04:57,315 --> 01:05:00,075
Speaker 7:  Marvel movie, a Black Panther movie. He didn't think any of It works. He's

1075
01:05:00,075 --> 01:05:02,235
Speaker 7:  like, you know what you need, you need a bunch of scrapping young white guys

1076
01:05:02,615 --> 01:05:03,595
Speaker 7:  out there doing stuff.

1077
01:05:03,635 --> 01:05:05,275
Speaker 5:  I feel like we're learning something else about Ike promo.

1078
01:05:05,775 --> 01:05:09,555
Speaker 7:  We are. Yeah. Interesting. But they're, they're very good friends. Ike was

1079
01:05:09,555 --> 01:05:13,475
Speaker 7:  fired earlier this year from Disney by Bob Iger. They are, they are not friends.

1080
01:05:13,615 --> 01:05:17,600
Speaker 7:  Nelson Peltz is an activist investor trying to get two board seats on

1081
01:05:17,600 --> 01:05:20,805
Speaker 7:  the Disney board because he is mad at everything. And the Disney board is

1082
01:05:20,805 --> 01:05:24,165
Speaker 7:  like, you just wanna bring Ike back and all the voting shares you're using

1083
01:05:24,165 --> 01:05:28,045
Speaker 7:  like 78% of the voting shares you're using are Ike's voting shares.

1084
01:05:28,875 --> 01:05:32,525
Speaker 7:  It's good. No, so, so there's a little fight there. And, and, and right now

1085
01:05:32,525 --> 01:05:36,365
Speaker 7:  there is ammunition, there's a lot of garbage. Bob Iger was

1086
01:05:36,365 --> 01:05:39,885
Speaker 7:  also at that Dealbook conference and he talked about it and he was like,

1087
01:05:39,885 --> 01:05:43,125
Speaker 7:  yeah, captain Marvel didn't work because we didn't have enough, like essentially

1088
01:05:43,265 --> 01:05:46,285
Speaker 7:  adults in the room. We didn't have enough executives watching over this.

1089
01:05:46,285 --> 01:05:49,445
Speaker 7:  We've been making bad films. And in the case of the Marvels, it was really

1090
01:05:49,685 --> 01:05:53,045
Speaker 7:  interesting to hear him say that, given that there were a lot of reasons

1091
01:05:53,235 --> 01:05:57,045
Speaker 7:  that movies struggled. Yeah. Box offices down across the board this

1092
01:05:57,045 --> 01:06:00,085
Speaker 7:  year there was a strike. It was a film. There

1093
01:06:00,085 --> 01:06:01,485
Speaker 5:  Was Yeah. They couldn't promote the movie.

1094
01:06:01,595 --> 01:06:05,445
Speaker 7:  Yeah. They just straight up couldn't promote the movie. And, and part of

1095
01:06:05,445 --> 01:06:08,725
Speaker 7:  it was like, yeah, it had a mess. It was had, it was a messy production.

1096
01:06:08,915 --> 01:06:11,685
Speaker 7:  They did have to go back and do a whole bunch of reshoots. And the third,

1097
01:06:12,225 --> 01:06:15,885
Speaker 7:  the back third of the film is like not as good as the first two parts.

1098
01:06:16,945 --> 01:06:20,845
Speaker 7:  That's brutal. That like, it is true. But also they've been releasing a

1099
01:06:20,925 --> 01:06:24,285
Speaker 7:  ton of garbage for the last few years. The the fatigue is, is set in. And

1100
01:06:24,285 --> 01:06:27,005
Speaker 7:  so for them to be like, oh, that one just didn't work because we didn't try

1101
01:06:27,005 --> 01:06:30,925
Speaker 7:  hard enough was really interesting for him to say. And so now he's like,

1102
01:06:30,925 --> 01:06:34,885
Speaker 7:  don't worry, we're gonna try harder. And that's not what you need to

1103
01:06:34,885 --> 01:06:37,845
Speaker 7:  be saying when you've got activists, investors breathing down your neck,

1104
01:06:37,845 --> 01:06:41,685
Speaker 7:  particularly activist investors who think it's a mistake that

1105
01:06:41,685 --> 01:06:43,845
Speaker 7:  you've made all these films starring, you

1106
01:06:43,845 --> 01:06:47,765
Speaker 5:  Know, and and Peltz was in the mix, right? With Chap Ppac. So you got, you

1107
01:06:47,765 --> 01:06:50,965
Speaker 5:  got your dual and Bobs Yeah, a lot of Bobs. So you get Bob Iger, he hands

1108
01:06:50,965 --> 01:06:53,925
Speaker 5:  it out to Chap Pack, the pandemic hits. He doesn't like how Chap Ppac is

1109
01:06:53,925 --> 01:06:56,125
Speaker 5:  running anything and he, he brings himself back,

1110
01:06:56,145 --> 01:06:57,205
Speaker 7:  Brings himself back.

1111
01:06:57,625 --> 01:07:01,115
Speaker 5:  But in there, in somewhere in there, right. Nelson

1112
01:07:01,285 --> 01:07:05,035
Speaker 5:  Peltz shows up and like causes trouble for Disney along the way.

1113
01:07:05,145 --> 01:07:08,715
Speaker 7:  Yeah. He's, because he's been friends with Ike for this whole time. So he's

1114
01:07:08,715 --> 01:07:12,435
Speaker 7:  been really supportive of Ike and, and supportive of like kind of Ike's way

1115
01:07:12,435 --> 01:07:16,315
Speaker 7:  of doing things. And, and so he's, he's really wanted a

1116
01:07:16,315 --> 01:07:19,835
Speaker 7:  seat on that board. He's wanted to maintain a seat on that board and, and

1117
01:07:19,935 --> 01:07:23,195
Speaker 7:  he and Iger don't like each other. And and we saw that at the Dealbook conference.

1118
01:07:23,655 --> 01:07:27,075
Speaker 7:  Andrew was at kept asking him, tell me about Pels, what's going on with him?

1119
01:07:27,075 --> 01:07:29,835
Speaker 7:  And he's like, ah, don't worry about it. Yeah, it's fine. Really blowing

1120
01:07:29,855 --> 01:07:33,835
Speaker 7:  him off there. So it's gonna be really interesting to see how, how they

1121
01:07:33,835 --> 01:07:37,595
Speaker 7:  go about this. And I think it is very concerning when you have the largest

1122
01:07:37,645 --> 01:07:41,555
Speaker 7:  media company in the world and you've got these guys who are,

1123
01:07:41,945 --> 01:07:45,595
Speaker 7:  have a really strong reputation for not liking half of the

1124
01:07:45,595 --> 01:07:49,355
Speaker 7:  population of the world wanting to go back and wanting a seat at the table

1125
01:07:49,615 --> 01:07:53,555
Speaker 7:  to dictate kind of how that company operates and including the kind of

1126
01:07:53,555 --> 01:07:57,515
Speaker 7:  films they make for me as a, as a woman, not, not my favorite thing

1127
01:07:57,515 --> 01:08:00,595
Speaker 7:  to see, but it's happening. Yeah. So we'll see how it lands.

1128
01:08:00,855 --> 01:08:04,195
Speaker 5:  It it is also true. I, I mean I agree with you, but it is also true. Disney

1129
01:08:04,255 --> 01:08:05,875
Speaker 5:  is not doing great. That's

1130
01:08:05,875 --> 01:08:08,795
Speaker 7:  True too. Yeah. Disney is not doing great and and there's a lot of reasons

1131
01:08:08,795 --> 01:08:11,995
Speaker 7:  for that. Iger likes to put everything on jpeg and I think that's stupid

1132
01:08:12,015 --> 01:08:12,355
Speaker 7:  of him.

1133
01:08:12,355 --> 01:08:13,875
Speaker 5:  Yeah. That clock has run out. He's been back

1134
01:08:13,875 --> 01:08:16,565
Speaker 7:  Long enough, right? Yeah. He's been back way too long for that. I think there

1135
01:08:16,565 --> 01:08:19,725
Speaker 7:  is a lot of problems with that company and, and, and there's a lot of different

1136
01:08:19,725 --> 01:08:22,845
Speaker 7:  reasons for that. I saw a really great story the other day about how Disney

1137
01:08:22,955 --> 01:08:26,325
Speaker 7:  Plus was kind of like antithetical to the way that Disney has always done

1138
01:08:26,565 --> 01:08:29,805
Speaker 7:  business because Disney's business has always been circular. Every single

1139
01:08:29,805 --> 01:08:33,085
Speaker 7:  thing builds gets you to go buy something else that Disney makes.

1140
01:08:33,985 --> 01:08:37,925
Speaker 7:  And when you put suddenly everything in one spot, why,

1141
01:08:37,945 --> 01:08:40,485
Speaker 7:  why are you going anywhere else? It's a great deal for us, the consumer.

1142
01:08:40,555 --> 01:08:44,445
Speaker 7:  It's a terrible deal for Disney and Disney's gotta figure out a way out

1143
01:08:44,445 --> 01:08:48,245
Speaker 7:  of that. And, and their idea is, okay, well we're gonna put more into our

1144
01:08:48,245 --> 01:08:50,605
Speaker 7:  parks. We're gonna, we're gonna make them bigger, we're gonna make them more

1145
01:08:50,605 --> 01:08:53,685
Speaker 7:  fun, make them more accessible. They changed a whole lot of the rules there

1146
01:08:53,745 --> 01:08:57,085
Speaker 7:  to like kind of get all those park goers happy again. 'cause that was one

1147
01:08:57,085 --> 01:09:00,325
Speaker 7:  of the things jpeg really screwed up was he pissed off all of those. He

1148
01:09:00,325 --> 01:09:00,885
Speaker 5:  Raised all the prices.

1149
01:09:00,955 --> 01:09:04,125
Speaker 7:  Yeah, he raised the prices. He, he changed like how you could get into the

1150
01:09:04,125 --> 01:09:08,045
Speaker 7:  park and stuff like that. So like Iris's got a lot

1151
01:09:08,065 --> 01:09:11,725
Speaker 7:  of work to do and I don't think he's moving fast enough.

1152
01:09:12,365 --> 01:09:12,605
Speaker 5:  Interesting.

1153
01:09:13,005 --> 01:09:16,445
Speaker 7:  I I think he, he he's gotta move a lot faster and he's also gotta start like

1154
01:09:17,595 --> 01:09:21,325
Speaker 7:  letting off the reins and more importantly, like trusting

1155
01:09:21,355 --> 01:09:25,125
Speaker 7:  your directors, trusting your writers, trusting the creatives you

1156
01:09:25,125 --> 01:09:28,605
Speaker 7:  hire to do their job. The, the fact that he was like, oh yeah, we just didn't

1157
01:09:28,925 --> 01:09:32,525
Speaker 7:  have enough executives watching over the Marvels is a wild statement to make

1158
01:09:32,525 --> 01:09:35,685
Speaker 7:  because one of the big reasons that Marvel has been struggling is that there's

1159
01:09:35,685 --> 01:09:39,245
Speaker 7:  one guy at the top, Kevin Vay and he stretched too thin and he was like kind

1160
01:09:39,245 --> 01:09:42,925
Speaker 7:  of, he had a Midas touch, he could fix anything and he can't fix

1161
01:09:42,925 --> 01:09:46,805
Speaker 7:  everything. Like delegate my man. So there, there needs to be a lot more

1162
01:09:46,805 --> 01:09:47,565
Speaker 7:  delegation over there.

1163
01:09:47,745 --> 01:09:51,725
Speaker 5:  Can I tell you a story about Dakota really quickly? Yeah. I had IBM's head

1164
01:09:51,725 --> 01:09:54,045
Speaker 5:  of quantum computing on Dakota this week. I don Dunno know when that upset,

1165
01:09:54,105 --> 01:09:55,925
Speaker 5:  but it's coming out in the very end. I was like, what do you think of those

1166
01:09:55,925 --> 01:09:57,365
Speaker 5:  Antman movies? And it was just like, ugh.

1167
01:10:01,115 --> 01:10:04,885
Speaker 5:  Just, just letting you know. Brutal. So I. She get Jerry Chow

1168
01:10:05,125 --> 01:10:07,765
Speaker 5:  watching this. I'll say as a person who was just a Disney world, yeah.

1169
01:10:08,835 --> 01:10:12,725
Speaker 5:  They have not lowered the prices. My friend. I bought a $35 bubble wand

1170
01:10:13,465 --> 01:10:16,845
Speaker 5:  and boy did we not get $35 in value on that thing.

1171
01:10:18,305 --> 01:10:22,085
Speaker 5:  Not even a little bit. No. Also Disney World. Very entertaining because

1172
01:10:23,595 --> 01:10:27,335
Speaker 5:  you know, people from all walks of life are there. Yeah. And there's a monorail,

1173
01:10:27,335 --> 01:10:31,135
Speaker 5:  which is like public transit and washing big people in maga hats like

1174
01:10:31,135 --> 01:10:34,375
Speaker 5:  happily ride the train is like very entertaining. And you're like, wait,

1175
01:10:34,375 --> 01:10:37,335
Speaker 5:  this train doesn't work nearly as well as it should.

1176
01:10:38,445 --> 01:10:41,375
Speaker 5:  It's like really doesn't. It's really that great. It's, I would describe

1177
01:10:41,375 --> 01:10:45,055
Speaker 5:  it my, my sister and I, my sister's like a Disney adult and she can like

1178
01:10:45,055 --> 01:10:47,375
Speaker 5:  work the app and she knows the vocabulary to get

1179
01:10:47,375 --> 01:10:49,175
Speaker 7:  Your lines. You're in here, all your lines in lightning

1180
01:10:49,175 --> 01:10:53,135
Speaker 5:  Lanes. It was incredible. No adult should ever say

1181
01:10:53,135 --> 01:10:56,975
Speaker 5:  to another adult, have you checked your My Genie? Plus that should

1182
01:10:56,975 --> 01:10:57,335
Speaker 5:  not happen.

1183
01:10:57,915 --> 01:10:58,895
Speaker 7:  But it does when you go to Disney.

1184
01:10:58,955 --> 01:11:02,935
Speaker 5:  But it, you're like, oh this is, this is the most like incredible

1185
01:11:04,055 --> 01:11:07,895
Speaker 5:  technocratic neoliberal bureaucracy in the world. Right? Like, if you can

1186
01:11:07,895 --> 01:11:11,775
Speaker 5:  work the system, the world is your oyster. There's a sheen of

1187
01:11:11,825 --> 01:11:14,965
Speaker 5:  light corruption that makes, makes everything a little exciting. Yeah. You're

1188
01:11:14,965 --> 01:11:17,445
Speaker 5:  like, is Bradley Cooper here? Like does he have to stand in mind?

1189
01:11:17,585 --> 01:11:19,245
Speaker 7:  Did you get the, did you get the little band?

1190
01:11:19,465 --> 01:11:22,765
Speaker 5:  Oh we had the band. The band flew off my wrist during Guardians of the Galaxy

1191
01:11:22,765 --> 01:11:25,765
Speaker 5:  and I caught it. That is by the way, one of the best rollercoasters I've

1192
01:11:25,765 --> 01:11:28,965
Speaker 5:  been on my entire life. Incredible. And then I somehow I lost the band during

1193
01:11:29,195 --> 01:11:32,245
Speaker 5:  Pirates of the Caribbean, which is a slow boat ride.

1194
01:11:33,385 --> 01:11:35,045
Speaker 5:  No idea. Not idea. Not the one

1195
01:11:35,045 --> 01:11:36,445
Speaker 7:  Where you're doing like a spiral.

1196
01:11:36,825 --> 01:11:40,765
Speaker 5:  No, no idea how I lost it on that one. And I caught

1197
01:11:40,765 --> 01:11:44,405
Speaker 5:  it during Guardians. Very cool. Loved every minute of it, but Max was happy.

1198
01:11:44,545 --> 01:11:47,605
Speaker 6:  Did they make you a guardian? If you catch it in midair, they're just like,

1199
01:11:47,605 --> 01:11:49,645
Speaker 6:  Chris Pratt comes out and he's like, you did it.

1200
01:11:51,545 --> 01:11:54,365
Speaker 7:  No. That's how the ride starts. Is your guardian basically?

1201
01:11:55,425 --> 01:11:59,365
Speaker 5:  No. Kind of, no. Yeah. I mean there's, it's like it's a ride. There's a story.

1202
01:11:59,625 --> 01:12:03,285
Speaker 5:  The thing is, the story comes to its conclusion during the rollercoaster

1203
01:12:03,285 --> 01:12:07,245
Speaker 5:  while everyone is screaming and they're blasting eighties music at you. Perfect.

1204
01:12:07,345 --> 01:12:11,285
Speaker 5:  So I have no idea what hap I wrote it twice and I'm like, what happens?

1205
01:12:11,865 --> 01:12:15,765
Speaker 7:  The first time I wrote it I was maybe really enjoying it was,

1206
01:12:15,765 --> 01:12:18,605
Speaker 7:  it was a, it was, you know, it was kind of an April 20th kind of day for

1207
01:12:18,605 --> 01:12:18,725
Speaker 7:  me.

1208
01:12:18,725 --> 01:12:20,085
Speaker 5:  Would you say you were in the Magic Kingdom?

1209
01:12:20,485 --> 01:12:24,365
Speaker 7:  I was in the Magic Kingdom and I didn't enjoy it very much 'cause I spent

1210
01:12:24,385 --> 01:12:27,805
Speaker 7:  the entire time being like, what if my glasses fly off? Well

1211
01:12:28,325 --> 01:12:31,405
Speaker 7:  I just spent like this and the video of me was just holding my glasses.

1212
01:12:31,635 --> 01:12:33,085
Speaker 5:  Just be more careful on Yeah.

1213
01:12:33,145 --> 01:12:34,845
Speaker 7:  On Tired being second time I just didn't wear it.

1214
01:12:36,025 --> 01:12:38,885
Speaker 5:  It was good. And the Tron Rider was good. It was fun. But you're just like

1215
01:12:38,885 --> 01:12:42,005
Speaker 5:  in this place where're like, oh this is what, this is what you want. The

1216
01:12:42,005 --> 01:12:45,285
Speaker 5:  taxes are very high, the services are available, but it somewhat

1217
01:12:45,445 --> 01:12:46,005
Speaker 5:  mediocre

1218
01:12:46,945 --> 01:12:48,245
Speaker 7:  And sometimes they just carry a man

1219
01:12:48,245 --> 01:12:51,805
Speaker 5:  Up and if you can work this app, you can get health insurance. Yes. Like

1220
01:12:51,905 --> 01:12:52,605
Speaker 5:  Disney World,

1221
01:12:52,865 --> 01:12:53,765
Speaker 7:  Really good health insurance.

1222
01:12:53,845 --> 01:12:57,805
Speaker 5:  I would not, I was not the most fun at Disney World. Like everyone's

1223
01:12:57,805 --> 01:13:00,205
Speaker 5:  like, yes, we understand that you have ideas about politics,

1224
01:13:02,185 --> 01:13:05,765
Speaker 5:  But it was very fun. We had a good time. And the enterprise wifi network

1225
01:13:07,145 --> 01:13:11,005
Speaker 5:  is WLAN dash T wdc, which it

1226
01:13:11,005 --> 01:13:13,685
Speaker 5:  has been apparently for over a decade. What?

1227
01:13:14,825 --> 01:13:18,005
Speaker 5:  And it's on, it's like this at every Disney property. And Disney employees

1228
01:13:18,005 --> 01:13:21,285
Speaker 5:  were posting on threads at me that it is so cumbersome that they installed

1229
01:13:21,285 --> 01:13:25,125
Speaker 5:  network devices in their house so they can get on the corporate network instead

1230
01:13:25,125 --> 01:13:29,045
Speaker 5:  of using the VPN all Very good. I like to think that Disney World has so

1231
01:13:29,045 --> 01:13:32,845
Speaker 5:  many devices on the wifi, they can never change the network name. Like

1232
01:13:33,075 --> 01:13:36,325
Speaker 5:  they have to walk around the entire park like hitting the reset button.

1233
01:13:36,865 --> 01:13:39,485
Speaker 5:  All very good. All, right? David, what's your lighting around? That was mine.

1234
01:13:39,555 --> 01:13:40,245
Speaker 5:  I'll just do that one.

1235
01:13:41,245 --> 01:13:41,565
Speaker 6:  I just,

1236
01:13:41,905 --> 01:13:43,285
Speaker 7:  Eli went to Disney World. Yeah.

1237
01:13:43,985 --> 01:13:47,765
Speaker 6:  At very, very, I just like, I'm now imagining Eli with like his 16 inch

1238
01:13:47,765 --> 01:13:50,525
Speaker 6:  MacBook Pro just like sitting in line for a rollercoaster,

1239
01:13:51,235 --> 01:13:54,965
Speaker 5:  Waiting for OpenAI News to break a real thing that I thought was gonna happen.

1240
01:13:56,395 --> 01:13:56,685
Speaker 7:  Yeah.

1241
01:13:58,475 --> 01:14:01,325
Speaker 5:  Have you ever been on the phone with a source being like, is this gonna get

1242
01:14:01,325 --> 01:14:03,845
Speaker 5:  Wrapped up tonight because I have to go to Disney World in the morning.

1243
01:14:04,235 --> 01:14:08,205
Speaker 6:  It's a cool line, honestly. Yeah. I support it. Mine is just

1244
01:14:08,425 --> 01:14:11,245
Speaker 6:  to, as I occasionally do, implore

1245
01:14:12,135 --> 01:14:15,685
Speaker 6:  every website and app and person on earth

1246
01:14:16,185 --> 01:14:20,085
Speaker 6:  to do your version of Spotify Wrapped. So Spotify Wrapped came out this week.

1247
01:14:20,115 --> 01:14:24,045
Speaker 6:  This is the week of like everybody gets their year in review stuff.

1248
01:14:24,045 --> 01:14:27,445
Speaker 6:  YouTube music did it. Apple Music did it. Has Reddit's come out yet? Reddit

1249
01:14:27,445 --> 01:14:30,205
Speaker 6:  does one every year. I haven't seen mine. I don't know if it came out's yet.

1250
01:14:30,205 --> 01:14:34,125
Speaker 6:  Think it's Spotify is his out PocketCasts, my podcast app did one.

1251
01:14:34,465 --> 01:14:38,085
Speaker 6:  All kinds of stuff coming out. This is my favorite genre of thing on the

1252
01:14:38,205 --> 01:14:40,845
Speaker 6:  internet. I saw somebody who just posted one that was like a screenshot of

1253
01:14:40,845 --> 01:14:43,925
Speaker 6:  their Chrome history just showing all the webpage they go to the most. I

1254
01:14:43,925 --> 01:14:47,605
Speaker 6:  was like, my browser should have this. I want my email to show me like who

1255
01:14:47,605 --> 01:14:48,805
Speaker 6:  I email the most. Should

1256
01:14:48,805 --> 01:14:49,645
Speaker 7:  Your browser have

1257
01:14:49,645 --> 01:14:53,325
Speaker 6:  This? I it does like it or not. Like it has that information. It might as

1258
01:14:53,325 --> 01:14:56,725
Speaker 6:  well show it to me in a funny way. I just think everything that exists,

1259
01:14:57,355 --> 01:15:01,325
Speaker 6:  like Mike, I want like Google Calendar Wrapped at the end of every

1260
01:15:01,325 --> 01:15:02,485
Speaker 6:  year just to tell me

1261
01:15:02,995 --> 01:15:04,245
Speaker 5:  Like, that's so brutal.

1262
01:15:04,685 --> 01:15:06,445
Speaker 7:  That's depressing. don don't want that.

1263
01:15:06,715 --> 01:15:09,725
Speaker 6:  Like David, who have you talked shit about in Slack most this year?

1264
01:15:11,065 --> 01:15:13,285
Speaker 5:  Oh, slack should do that. That would actually be great. This is what

1265
01:15:13,285 --> 01:15:16,045
Speaker 7:  I'm saying, slack didn't Slack used to, I feel like there used to be a way

1266
01:15:16,045 --> 01:15:18,765
Speaker 7:  where you could just see like who talks the most.

1267
01:15:19,025 --> 01:15:22,845
Speaker 5:  No, it it this, this was like a weird enterprise privacy

1268
01:15:24,195 --> 01:15:28,045
Speaker 5:  nightmare debate where like your Slack administrator could see who the, the

1269
01:15:28,045 --> 01:15:30,965
Speaker 5:  biggest user of Slack was in your organization. Oh yeah. And people were

1270
01:15:30,965 --> 01:15:33,405
Speaker 5:  like, this is a privacy violation. And I was like, no, you work at a company

1271
01:15:33,505 --> 01:15:33,725
Speaker 5:  bro.

1272
01:15:34,505 --> 01:15:35,965
Speaker 7:  You don't have privacy in the company stuff.

1273
01:15:35,965 --> 01:15:38,165
Speaker 5:  No, no, no, no. Like not on the work laptop.

1274
01:15:40,545 --> 01:15:41,285
Speaker 5:  No, no, no, no, no.

1275
01:15:43,075 --> 01:15:43,365
Speaker 6:  Yeah.

1276
01:15:43,715 --> 01:15:44,365
Speaker 7:  Shut it down

1277
01:15:44,625 --> 01:15:48,525
Speaker 5:  All. right. My last, my real lightning round item, which is

1278
01:15:48,525 --> 01:15:48,885
Speaker 5:  not about my

1279
01:15:49,055 --> 01:15:49,845
Speaker 7:  World is Lightning Lane

1280
01:15:50,685 --> 01:15:53,925
Speaker 5:  Technocratic Neoliberal nightmare of Disney World.

1281
01:15:55,825 --> 01:15:59,525
Speaker 5:  No, it's the Epic versus Google trial. So the Epic versus

1282
01:15:59,525 --> 01:16:03,365
Speaker 5:  Google antitrust trials ongoing. That again is about Fortnite and

1283
01:16:03,365 --> 01:16:07,225
Speaker 5:  billing on app stores. This is the sequel to the Apple

1284
01:16:07,365 --> 01:16:10,905
Speaker 5:  one. We, we mentioned this during the, while we were covering the Apple one,

1285
01:16:10,905 --> 01:16:14,105
Speaker 5:  I think we've mentioned this show since then. Apple case was very straightforward.

1286
01:16:14,155 --> 01:16:16,825
Speaker 5:  Apple doesn't allow other app stores, they require you to use their billing

1287
01:16:16,825 --> 01:16:20,595
Speaker 5:  system. This whole thing. There's no emails

1288
01:16:20,615 --> 01:16:24,555
Speaker 5:  except Apple executives being like, this is stupid go away. And so

1289
01:16:24,555 --> 01:16:28,195
Speaker 5:  like Epic had to concoct this entire case about Apple and its

1290
01:16:28,355 --> 01:16:31,995
Speaker 5:  monopoly and Lock-in. And you can believe it or not, Google is an

1291
01:16:31,995 --> 01:16:35,115
Speaker 5:  ecosystem provider. They have lots and lots of deals. They have lots and

1292
01:16:35,115 --> 01:16:38,195
Speaker 5:  lots of relationships. They have hardware vendors. They have hardware vendors

1293
01:16:38,195 --> 01:16:42,035
Speaker 5:  trying to differentiate their products. There is a ton of evidence about

1294
01:16:42,035 --> 01:16:45,875
Speaker 5:  how Google makes deals in this case and how people try to make deals

1295
01:16:45,875 --> 01:16:49,795
Speaker 5:  with Google. And it is not pretty. And I think that's like the thing that

1296
01:16:49,795 --> 01:16:53,355
Speaker 5:  we have learned throughout this entire case and throughout the DOJ case with

1297
01:16:53,355 --> 01:16:57,235
Speaker 5:  Google is Google does a lot of deal making. So the one that

1298
01:16:57,235 --> 01:17:01,115
Speaker 5:  came out this week that is I think really, really revealing is

1299
01:17:01,165 --> 01:17:05,115
Speaker 5:  Activision tried to start its own mobile game app store

1300
01:17:05,135 --> 01:17:08,515
Speaker 5:  on Android. So Activision proposed with Epic

1301
01:17:09,305 --> 01:17:12,405
Speaker 5:  and with the company that makes Clash of Clan super solid, they would start

1302
01:17:12,405 --> 01:17:16,365
Speaker 5:  their own app store on Android, it would be Steam for Android. And they called

1303
01:17:16,365 --> 01:17:19,405
Speaker 5:  this project Boston and they were running it with one track and they had

1304
01:17:19,405 --> 01:17:22,445
Speaker 5:  another track with Google that was just give us a hundred million dollars

1305
01:17:22,465 --> 01:17:26,285
Speaker 5:  and we won't do this. And Google just paid them the money

1306
01:17:26,885 --> 01:17:29,965
Speaker 5:  because they don't want competition for their app store and Android. And

1307
01:17:29,965 --> 01:17:32,925
Speaker 5:  they're, you know, you can, we'll link our story, there's all these emails

1308
01:17:32,935 --> 01:17:35,445
Speaker 5:  about it and whether they're really gonna do it and whether Google is really

1309
01:17:35,445 --> 01:17:39,285
Speaker 5:  afraid of it, but Google paid them the money to foreclose

1310
01:17:39,285 --> 01:17:43,005
Speaker 5:  the competition. And that there's just a lot of that in this case

1311
01:17:43,275 --> 01:17:47,125
Speaker 5:  because there are so many companies in the Android ecosystem that

1312
01:17:47,125 --> 01:17:49,405
Speaker 5:  are trying to get leverage over one another. And Google's sitting in the

1313
01:17:49,405 --> 01:17:53,385
Speaker 5:  middle of it. And you can put that right next to, hey,

1314
01:17:53,385 --> 01:17:57,245
Speaker 5:  you know, apple Maps is pretty good lately, right? Like Apple

1315
01:17:58,185 --> 01:18:01,205
Speaker 5:  had to build a competitor to Google Maps for all kinds of reasons and they

1316
01:18:01,205 --> 01:18:04,085
Speaker 5:  made a really good one. And there's a lot of conversation lately about how

1317
01:18:04,085 --> 01:18:07,445
Speaker 5:  it is superior to Google Maps. And I think if you live in the right places,

1318
01:18:08,005 --> 01:18:11,965
Speaker 5:  particularly for driving directions for turn by turn vastly superior to

1319
01:18:11,965 --> 01:18:12,685
Speaker 5:  Google Maps. I

1320
01:18:12,685 --> 01:18:13,845
Speaker 6:  Agree. It's really good now.

1321
01:18:14,635 --> 01:18:17,045
Speaker 5:  Okay, well if Apple wants to make a competitor a Google product, they can

1322
01:18:17,045 --> 01:18:19,405
Speaker 5:  definitely do it. Yeah. And they definitely have the install base to do it

1323
01:18:19,405 --> 01:18:22,605
Speaker 5:  and Google does not want that to happen to search and pays Apple the money.

1324
01:18:23,685 --> 01:18:27,065
Speaker 5:  And you can put that right next to Blizzard said, we might start an app store

1325
01:18:27,065 --> 01:18:30,665
Speaker 5:  and Google said, eh, we don't want you to do that. Pay them the money. And

1326
01:18:30,665 --> 01:18:34,505
Speaker 5:  we're just, I I I don't want to tell you like, I think most people

1327
01:18:34,505 --> 01:18:37,905
Speaker 5:  think that the world they live in is not done by

1328
01:18:37,905 --> 01:18:41,785
Speaker 5:  handshakes and backroom deals. Or maybe they do now Maybe it, gen Z

1329
01:18:41,785 --> 01:18:43,185
Speaker 5:  is wiser to the waist of

1330
01:18:43,185 --> 01:18:46,105
Speaker 7:  The, they all watch, they all watch Hamilton's. So they all, they all know

1331
01:18:46,765 --> 01:18:46,985
Speaker 7:  the

1332
01:18:47,205 --> 01:18:48,345
Speaker 5:  Is it Hamilton or Suits?

1333
01:18:49,535 --> 01:18:50,265
Speaker 7:  Both. It's,

1334
01:18:50,375 --> 01:18:51,865
Speaker 5:  It's, if you wanna know how the world works,

1335
01:18:52,485 --> 01:18:53,345
Speaker 6:  Two of the two shows,

1336
01:18:54,315 --> 01:18:57,665
Speaker 7:  Those are the two shows core to the your internet. No, I think, I

1337
01:18:57,665 --> 01:18:58,225
Speaker 5:  Think there has been,

1338
01:18:59,845 --> 01:19:03,465
Speaker 5:  at least on the internet, the idea that the internet is, is democratizing

1339
01:19:03,855 --> 01:19:07,105
Speaker 5:  that the distribution is free and open and equal.

1340
01:19:08,565 --> 01:19:11,225
Speaker 5:  And then you get to these app stores and you get to these mobile platforms

1341
01:19:11,405 --> 01:19:15,265
Speaker 5:  and that has instantly broken down. Yeah. So read that

1342
01:19:15,265 --> 01:19:18,425
Speaker 5:  on the site. It is, it's great. Really, really illuminating coverage about

1343
01:19:18,425 --> 01:19:21,865
Speaker 5:  how all the stuff works. And then when, when the trial's over, we will

1344
01:19:22,295 --> 01:19:25,945
Speaker 5:  have Sean on and he can synthesize what he's learned. but it is like

1345
01:19:25,945 --> 01:19:27,905
Speaker 5:  gleeful coverage from the courtroom from Sean. It is very,

1346
01:19:28,005 --> 01:19:31,745
Speaker 6:  Oh yeah, this one a great time. The USV Google was so locked

1347
01:19:31,775 --> 01:19:35,585
Speaker 6:  down and so redacted and so carefully

1348
01:19:35,605 --> 01:19:39,385
Speaker 6:  in everything that anyone revealed about anything. This one is the opposite.

1349
01:19:39,495 --> 01:19:43,465
Speaker 6:  They're just like, everybody is just throwing signed contracts like into

1350
01:19:43,465 --> 01:19:46,985
Speaker 6:  the wind every morning in the courtroom. Like you really get the sense the

1351
01:19:46,985 --> 01:19:50,865
Speaker 6:  discovery process here has been a, a joyful one for a lot of people. Yeah.

1352
01:19:51,125 --> 01:19:53,505
Speaker 7:  I'm really just bummed. I haven't come up with an idea that Google will pay

1353
01:19:53,505 --> 01:19:54,065
Speaker 7:  me not to do.

1354
01:19:54,705 --> 01:19:58,505
Speaker 6:  Honestly, it really feels like if you just go like park your car

1355
01:19:59,165 --> 01:20:02,825
Speaker 6:  in Google's parking lot and just start going, maybe I'll open an app store.

1356
01:20:02,825 --> 01:20:06,745
Speaker 6:  Someone will just like fire a money cannon into your car. Yeah. Up until

1357
01:20:06,745 --> 01:20:07,925
Speaker 6:  you drive. Let's head out. Yeah.

1358
01:20:08,435 --> 01:20:11,605
Speaker 5:  Yeah. You know what's, what's interesting about that point David is, you

1359
01:20:11,605 --> 01:20:15,415
Speaker 5:  know, USV, Google, the United States government doing an

1360
01:20:15,415 --> 01:20:17,695
Speaker 5:  antitrust case, they need to be buttoned up. They were in a very buttoned

1361
01:20:17,695 --> 01:20:21,295
Speaker 5:  up courtroom. The United States government right now does not have a great

1362
01:20:21,295 --> 01:20:24,815
Speaker 5:  history of pursuing these cases or a lot of pressure on them. They're making

1363
01:20:24,855 --> 01:20:28,375
Speaker 5:  a very complicated argument about search lockin Epic is just like

1364
01:20:29,505 --> 01:20:33,425
Speaker 5:  f go fuck yourselves. Yeah. Like straight up like that is

1365
01:20:33,425 --> 01:20:36,225
Speaker 5:  Tim Sweeney's attitude towards about this is a moral mission for him as well

1366
01:20:36,225 --> 01:20:39,985
Speaker 5:  as an economic one. And then you have the ecosystem

1367
01:20:40,375 --> 01:20:44,135
Speaker 5:  that is never happy with these players and

1368
01:20:44,335 --> 01:20:47,415
Speaker 5:  a constant looking for every little ounce of leverage. And you can, you're

1369
01:20:47,415 --> 01:20:51,335
Speaker 5:  right, the tenor of this case is very different. Yeah. Because Epic has

1370
01:20:51,335 --> 01:20:52,495
Speaker 5:  already gone, they have nothing to

1371
01:20:52,495 --> 01:20:56,175
Speaker 6:  Lose. Right. You get the definite sense that even if they lose, they're prepared

1372
01:20:56,275 --> 01:21:00,175
Speaker 6:  to lose having landed a bunch of punches against

1373
01:21:00,175 --> 01:21:04,055
Speaker 6:  Google. And that is, that is pretty clearly the

1374
01:21:04,375 --> 01:21:07,655
Speaker 6:  tactic here. And so far I think it's working like don don't know if they'll

1375
01:21:07,655 --> 01:21:11,135
Speaker 6:  win the case, but boy have they made Google look bad over and over and over

1376
01:21:11,135 --> 01:21:11,375
Speaker 6:  here.

1377
01:21:11,405 --> 01:21:15,095
Speaker 5:  Yeah. What's interesting to me, just as a person who looks at how our website

1378
01:21:15,095 --> 01:21:18,695
Speaker 5:  is doing is when they're up against Apple, like every

1379
01:21:18,955 --> 01:21:22,895
Speaker 5:  one of our stories was a hit. And here we're with, with Google,

1380
01:21:22,925 --> 01:21:26,695
Speaker 5:  like with the evidence that the store, like there's shenanigans

1381
01:21:26,695 --> 01:21:30,575
Speaker 5:  afoot and we, we have to sell the stories a little bit harder. And

1382
01:21:30,575 --> 01:21:34,455
Speaker 5:  I, I think that just speaks to honestly how many people have iPhones versus

1383
01:21:34,515 --> 01:21:37,695
Speaker 5:  how many people care about how Android works. But I will remind you that

1384
01:21:37,695 --> 01:21:41,615
Speaker 5:  Android is the most used operating system on the planet. And how Google manages

1385
01:21:41,615 --> 01:21:45,575
Speaker 5:  it affects basically how computing is done. Yep. And so like

1386
01:21:45,995 --> 01:21:49,895
Speaker 5:  all these little shenanigans have like massive downstream effects across

1387
01:21:49,895 --> 01:21:53,415
Speaker 5:  the entire Android ecosystem because if Google stops making money in this

1388
01:21:53,415 --> 01:21:57,335
Speaker 5:  way, their case for making Android, the open source

1389
01:21:57,335 --> 01:22:00,835
Speaker 5:  operating system goes a little sideways. So

1390
01:22:01,585 --> 01:22:04,765
Speaker 5:  I'm like I said, you should read Sean's cover. It is very fun. I enjoy catching

1391
01:22:04,765 --> 01:22:08,165
Speaker 5:  up on it every day. This is why we invented story streams with like the quick,

1392
01:22:08,165 --> 01:22:12,005
Speaker 5:  like it's all happening. And then we, we will have Sean at the

1393
01:22:12,005 --> 01:22:14,965
Speaker 5:  conclusion on trial, which should be soon. I think we'll have Sean on set

1394
01:22:14,965 --> 01:22:18,605
Speaker 5:  to to talk about it. Yeah. All right? We have gone long. I gotta go throw

1395
01:22:18,615 --> 01:22:19,485
Speaker 5:  rocks at a triangle,

1396
01:22:20,945 --> 01:22:21,645
Speaker 6:  But not too hard.

1397
01:22:22,675 --> 01:22:26,165
Speaker 5:  NE's about to get arrested. I'm watching the social video team make comparison

1398
01:22:26,165 --> 01:22:29,525
Speaker 5:  videos of the metal ball versus the baseball. It's a very good Go on

1399
01:22:29,785 --> 01:22:33,085
Speaker 5:  TikTok and look at that. Thank you to everybody who's been posting about

1400
01:22:33,085 --> 01:22:36,365
Speaker 5:  your Spotify Wrapped with The Vergecast and with Decoder. We love it.

1401
01:22:36,805 --> 01:22:40,685
Speaker 5:  We've also got a lot, a lot of emails from people sh telling us about Spotify

1402
01:22:40,685 --> 01:22:44,605
Speaker 5:  Wrapped, which is great. We love it. Our hotline episode next week.

1403
01:22:44,675 --> 01:22:45,605
Speaker 5:  David's buying advice.

1404
01:22:45,605 --> 01:22:49,205
Speaker 6:  Yeah, we're doing a whole, we got a bunch of questions kind of organically

1405
01:22:49,205 --> 01:22:51,365
Speaker 6:  from people who are like, I'm trying to figure out what to buy or I'm trying

1406
01:22:51,365 --> 01:22:54,005
Speaker 6:  to buy this thing or this thing. So we're gonna do a whole bunch of those.

1407
01:22:54,095 --> 01:22:57,005
Speaker 6:  We're just gonna grab a bunch of folks and sit around and try to help people

1408
01:22:57,025 --> 01:23:00,845
Speaker 6:  buy stuff for the holidays. So if you have holiday questions, email us

1409
01:23:01,085 --> 01:23:04,365
Speaker 6:  Vergecast at virgin.com, which you can do about all of your feelings in general,

1410
01:23:04,665 --> 01:23:08,565
Speaker 6:  but also buying questions or call the hotline. Eight, six, six version

1411
01:23:08,585 --> 01:23:11,605
Speaker 6:  one, one. Send them all in. We're gonna answer a whole bunch of 'em on next

1412
01:23:11,605 --> 01:23:12,125
Speaker 6:  Wednesday show.

1413
01:23:12,395 --> 01:23:14,605
Speaker 10:  Love it. All, right? That's it. That's Vergecast McElroy.

1414
01:23:20,425 --> 01:23:23,765
Speaker 1:  And that's a wrap for Vergecast this week. Hey, we'd love to hear from you.

1415
01:23:23,835 --> 01:23:27,805
Speaker 1:  Give us a call at eight six six Verge one. One The Vergecast is

1416
01:23:27,805 --> 01:23:31,645
Speaker 1:  a production of The Verge and Vox Media Podcast Network. The show is produced

1417
01:23:31,645 --> 01:23:35,605
Speaker 1:  by Andrew Marino and Liam James. This episode was mixed and edited by

1418
01:23:35,655 --> 01:23:37,805
Speaker 1:  Xandr Adams. And that's it. We'll see you next week.

