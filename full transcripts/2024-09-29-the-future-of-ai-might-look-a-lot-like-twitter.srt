1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 161728a4-c6d0-4db9-9fda-053fa8ffa6d9
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/6611727241075618694/5555030108530425342/s93290-US-3202s-1727609720.mp3
Description: Michael Sayman, the creator of a viral new app called SocialAI, joins the show to discuss why he built a social network where you're the only human around. He tells us how he thinks about AI interfaces, what's next for ChatGPT and other chatbots, and why posting to a language model might be better than posting on a social network.

2
00:00:02,715 --> 00:00:06,325
Speaker 2:  Welcome To The Vergecast, the flagship podcast of Bot Farms.

3
00:00:06,625 --> 00:00:10,405
Speaker 2:  I'm your friend, David Pierce, and this is the second episode in our miniseries

4
00:00:10,505 --> 00:00:14,405
Speaker 2:  all about AI in the real world. AI is

5
00:00:14,465 --> 00:00:18,165
Speaker 2:  so abstract, and it's a term that we now use in so many

6
00:00:18,235 --> 00:00:22,125
Speaker 2:  ways that honestly it can feel sort of meaningless. So we've

7
00:00:22,125 --> 00:00:25,565
Speaker 2:  been on a quest to find actual examples of actual AI

8
00:00:25,755 --> 00:00:29,365
Speaker 2:  showing up and being useful, or at the very least interesting

9
00:00:29,665 --> 00:00:33,645
Speaker 2:  in our actual lives for this episode. The last one

10
00:00:33,645 --> 00:00:36,645
Speaker 2:  in our little series, for now, I have a feeling we'll come back to this subject,

11
00:00:36,905 --> 00:00:40,845
Speaker 2:  but last one for now. I'm talking to Michael Sayman, who recently

12
00:00:41,005 --> 00:00:44,365
Speaker 2:  launched an app called Social Ai that has become kind of a viral

13
00:00:44,925 --> 00:00:48,765
Speaker 2:  phenomenon on the internet. We'll get into what it is and how it works in

14
00:00:48,765 --> 00:00:52,725
Speaker 2:  pretty serious detail here. But basically, I'd explain social AI

15
00:00:52,755 --> 00:00:56,685
Speaker 2:  this way. Imagine a social network, Twitter or threads or whatever,

16
00:00:57,305 --> 00:01:01,125
Speaker 2:  but every user other than you, every single one other than you

17
00:01:01,385 --> 00:01:04,925
Speaker 2:  is a bot. Does that sound interesting? Pointless,

18
00:01:05,645 --> 00:01:08,605
Speaker 2:  terrible, dystopian amazing. Maybe all of those things.

19
00:01:09,565 --> 00:01:13,325
Speaker 2:  I wasn't sure where I fell on that line when I started

20
00:01:13,325 --> 00:01:17,085
Speaker 2:  talking to Michael, but we ended up having, frankly, one of the most

21
00:01:17,265 --> 00:01:21,165
Speaker 2:  fun conversations I've had in a while, all about how

22
00:01:21,545 --> 00:01:24,525
Speaker 2:  AI works and how we're actually supposed to use it.

23
00:01:25,155 --> 00:01:29,085
Speaker 2:  Spoiler alert, he thinks about this less as a network and more as an

24
00:01:29,085 --> 00:01:32,965
Speaker 2:  interface, and I find that fascinating. We happen to

25
00:01:32,975 --> 00:01:36,685
Speaker 2:  agree, actually, Michael and I, that a chat bot cannot possibly

26
00:01:36,785 --> 00:01:40,765
Speaker 2:  be the future of everything in technology, and Michael has

27
00:01:40,765 --> 00:01:44,645
Speaker 2:  some big ideas about what else we might be able to do. All

28
00:01:44,645 --> 00:01:48,165
Speaker 2:  that is coming up in just a second. But. first, I have to tell my bots what's

29
00:01:48,165 --> 00:01:51,765
Speaker 2:  going on. They worry when I'm gone for too long. This is the Vergecast. We'll

30
00:01:51,765 --> 00:01:52,325
Speaker 2:  be right back.

31
00:02:32,375 --> 00:02:36,205
Speaker 2:  surprising number of the biggest and most interesting companies in

32
00:02:36,205 --> 00:02:40,165
Speaker 2:  tech and in particular, he's seen a huge part of the evolution of

33
00:02:40,165 --> 00:02:44,125
Speaker 2:  social media and social networks. He worked on stories at Instagram, he

34
00:02:44,125 --> 00:02:48,085
Speaker 2:  worked on status at WhatsApp, he worked on shorts at YouTube. Like

35
00:02:48,085 --> 00:02:51,965
Speaker 2:  I said, he worked at Twitter and a whole bunch of other things, and now he's

36
00:02:51,965 --> 00:02:55,085
Speaker 2:  on his own. He's building apps through his one man startup that he calls

37
00:02:55,365 --> 00:02:59,260
Speaker 2:  friendly apps. At the beginning of our conversation, Michael told me he had

38
00:02:59,260 --> 00:03:03,085
Speaker 2:  been thinking about building an AI social network through much of

39
00:03:03,085 --> 00:03:07,045
Speaker 2:  that time, the idea for what would become social. AI has been

40
00:03:07,045 --> 00:03:10,925
Speaker 2:  in his head for a really long time. It's just that until now, he

41
00:03:10,925 --> 00:03:12,325
Speaker 2:  couldn't actually pull it off.

42
00:03:12,885 --> 00:03:16,405
Speaker 4:  I actually tried building a version of social AI like five years ago, and

43
00:03:16,405 --> 00:03:19,485
Speaker 4:  the tech just wasn't there and it was really bad. What

44
00:03:19,485 --> 00:03:20,365
Speaker 2:  Was it five years ago?

45
00:03:20,665 --> 00:03:24,645
Speaker 4:  It was, I mean, I called it influencer and the idea was

46
00:03:24,645 --> 00:03:28,165
Speaker 4:  that anyone could be an influencer, right? So like, I've been trying to do

47
00:03:28,165 --> 00:03:31,165
Speaker 4:  this for a while, but it, it just quite wasn't there.

48
00:03:32,445 --> 00:03:35,925
Speaker 4:  I originally, you know, because we didn't have the language models, we, we

49
00:03:35,925 --> 00:03:39,685
Speaker 4:  tried to build it and by we, I mean just me, but trying to

50
00:03:39,685 --> 00:03:43,605
Speaker 4:  build this to, to kind of like give

51
00:03:43,605 --> 00:03:47,565
Speaker 4:  people the feeling of a social media app, but not really having to deal

52
00:03:47,565 --> 00:03:51,165
Speaker 4:  with all of it. The, the idea was like, okay, if someone, if

53
00:03:51,165 --> 00:03:54,245
Speaker 4:  someone's addicted to cigarettes, how do you get them off of it? Well, you

54
00:03:54,525 --> 00:03:58,005
Speaker 4:  can't just tell them to stop, right? Like, maybe it's like giving them something,

55
00:03:58,015 --> 00:04:01,765
Speaker 4:  maybe like a nicotine patch or something, right? So like what, what is a

56
00:04:01,765 --> 00:04:05,125
Speaker 4:  way that you get somebody to like be able to get that

57
00:04:05,175 --> 00:04:08,915
Speaker 4:  experience out the way, but maybe not, you know, harm themselves or

58
00:04:08,915 --> 00:04:12,715
Speaker 4:  feel bad. So anyway, so So I built that it didn't really look quite

59
00:04:12,715 --> 00:04:16,515
Speaker 4:  right and it, it didn't work well. So I, I

60
00:04:16,515 --> 00:04:17,195
Speaker 4:  didn't ship it.

61
00:04:17,585 --> 00:04:20,315
Speaker 2:  What did it feel like, actually, but before we get to the chat GBT thing

62
00:04:20,315 --> 00:04:23,195
Speaker 2:  of it all, 'cause that's a, there's an interesting sort of history of text

63
00:04:23,195 --> 00:04:26,595
Speaker 2:  story there, but yes, like when you built the thing

64
00:04:27,495 --> 00:04:30,265
Speaker 2:  before in, in the before times,

65
00:04:31,575 --> 00:04:35,185
Speaker 2:  what, what didn't work, what didn't feel right, like what wasn't ready,

66
00:04:35,545 --> 00:04:39,465
Speaker 4:  I just could not simulate the entire social network in a way

67
00:04:39,465 --> 00:04:42,505
Speaker 4:  that felt interesting, even mildly.

68
00:04:43,495 --> 00:04:47,465
Speaker 4:  Like I, I did the approach that Google did. Like I used to, I used

69
00:04:47,465 --> 00:04:51,425
Speaker 4:  to work at Google, I used to work at Facebook, and I took a

70
00:04:51,425 --> 00:04:54,985
Speaker 4:  similar approach to all of the assistants that there were at the time, giant

71
00:04:55,085 --> 00:04:58,665
Speaker 4:  if L statements, right? Like just massive and

72
00:04:59,135 --> 00:05:02,555
Speaker 4:  okay, like it kind of worked, but like everything else

73
00:05:02,775 --> 00:05:06,475
Speaker 4:  before these language models kind of took off, it was

74
00:05:06,625 --> 00:05:10,115
Speaker 4:  very robotic and very conditional, right?

75
00:05:10,115 --> 00:05:13,755
Speaker 4:  Depending on what you wrote. And, and it just didn't quite feel,

76
00:05:15,285 --> 00:05:19,105
Speaker 4:  it just didn't let you forget about the

77
00:05:19,105 --> 00:05:22,865
Speaker 4:  technology. Like you were reminded in the app that like you

78
00:05:23,005 --> 00:05:26,865
Speaker 4:  had to like do certain things to get certain comments and,

79
00:05:27,285 --> 00:05:30,905
Speaker 4:  and so at that point it was really more of a game.

80
00:05:31,215 --> 00:05:35,065
Speaker 4:  Like I designed it more like a game because the technology just

81
00:05:35,065 --> 00:05:38,215
Speaker 4:  wasn't there to like, make

82
00:05:39,445 --> 00:05:42,405
Speaker 4:  a simulated social network not feel like a game.

83
00:05:43,585 --> 00:05:47,405
Speaker 4:  And So I had to go that route, but, but even then it, it just, it did not

84
00:05:47,405 --> 00:05:51,355
Speaker 4:  feel right. Users, if I imagined if

85
00:05:51,355 --> 00:05:55,155
Speaker 4:  they were to try it would've felt like this

86
00:05:55,215 --> 00:05:59,085
Speaker 4:  app was more like a Farmville game and less like

87
00:05:59,165 --> 00:06:03,125
Speaker 4:  a social network. And, and honestly it just, that's

88
00:06:03,125 --> 00:06:06,005
Speaker 4:  not what I was trying to build. I was trying to build something that felt

89
00:06:06,005 --> 00:06:09,965
Speaker 4:  like a social network and So I had to wait then, then

90
00:06:09,965 --> 00:06:12,485
Speaker 4:  once the, the early versions of

91
00:06:12,925 --> 00:06:16,445
Speaker 4:  GPT-3 0.5 came out, I thought, okay,

92
00:06:17,185 --> 00:06:21,005
Speaker 4:  let me give it another shot and tried to design a version

93
00:06:21,005 --> 00:06:24,645
Speaker 4:  of it. The model would sometimes say random stuff.

94
00:06:25,545 --> 00:06:29,485
Speaker 4:  It was extremely expensive to run all of

95
00:06:29,485 --> 00:06:33,365
Speaker 4:  the different prompts and things that, that I needed for it to work. And

96
00:06:33,365 --> 00:06:37,295
Speaker 4:  I told myself like, there is no way I'm gonna

97
00:06:37,295 --> 00:06:40,895
Speaker 4:  be able to run this at this cost.

98
00:06:41,235 --> 00:06:44,255
Speaker 4:  And it's completely fuzzy and the responses are no good.

99
00:06:45,195 --> 00:06:49,095
Speaker 4:  And So I said, okay, well I have to wait. I have to wait until it gets cheap

100
00:06:49,115 --> 00:06:52,495
Speaker 4:  and I have to wait until it gets more accurate. And so every month

101
00:06:53,955 --> 00:06:56,975
Speaker 4:  for like two years while I was building this startup, I would just wait,

102
00:06:57,335 --> 00:07:00,775
Speaker 4:  I would just look at the latest model, try it out with this,

103
00:07:01,205 --> 00:07:04,625
Speaker 4:  with some of my tests, and from there, just

104
00:07:05,055 --> 00:07:08,925
Speaker 4:  keep going. I would look at the, the

105
00:07:08,995 --> 00:07:12,885
Speaker 4:  outputs that, that they could give and how much I could

106
00:07:12,885 --> 00:07:16,685
Speaker 4:  tune them. And then I'd look at the cost. And when

107
00:07:16,785 --> 00:07:19,405
Speaker 4:  Gemini released theirs and lowered their costs, I said, okay, we're getting

108
00:07:19,405 --> 00:07:23,325
Speaker 4:  closer. You know, as soon as, you know, OpenAI had their

109
00:07:23,325 --> 00:07:27,285
Speaker 4:  models dropping in prices, I said, okay, I think it's time. So about a month

110
00:07:27,285 --> 00:07:31,125
Speaker 4:  ago, I went and built the app and I just

111
00:07:31,125 --> 00:07:34,965
Speaker 4:  told myself, look, this is like the last attempt that I'm gonna do at building

112
00:07:34,965 --> 00:07:35,405
Speaker 4:  this app.

113
00:07:37,155 --> 00:07:41,045
Speaker 4:  Like I've, I've done this too many times, I'm just gonna go with it.

114
00:07:41,185 --> 00:07:41,405
Speaker 4:  So,

115
00:07:42,195 --> 00:07:43,645
Speaker 2:  Okay, and now here we are.

116
00:07:43,915 --> 00:07:47,725
Speaker 4:  Yeah. And here we are and I launched it and, you know, and of course that's

117
00:07:47,785 --> 00:07:51,325
Speaker 4:  the one time that you don't, you know, you don't think it's gonna go a certain

118
00:07:51,345 --> 00:07:54,885
Speaker 4:  way is when it does. And I mean, it's great. So

119
00:07:55,545 --> 00:07:59,165
Speaker 2:  I'm, I'm so curious why this idea has been so sticky in your head. Like

120
00:07:59,275 --> 00:08:02,605
Speaker 2:  it's, it's clearly been sitting around as a thing you have wanted to build

121
00:08:03,025 --> 00:08:06,165
Speaker 2:  for a very long time. What, what is it about this thing that is so

122
00:08:07,505 --> 00:08:09,045
Speaker 2:  sticky and enticing to you?

123
00:08:09,965 --> 00:08:13,465
Speaker 4:  So social networks are not what they used to be. And I think fundamentally

124
00:08:13,485 --> 00:08:17,185
Speaker 4:  the internet has changed. The internet used to be a tool of communication

125
00:08:17,185 --> 00:08:20,825
Speaker 4:  between people. And frankly, I love that. You know, there's a part of me,

126
00:08:21,045 --> 00:08:24,665
Speaker 4:  So I was born in Miami, but at 16 I flew out to

127
00:08:24,665 --> 00:08:28,585
Speaker 4:  California with my mom because Mark Zuckerberg had emailed me

128
00:08:28,615 --> 00:08:32,345
Speaker 4:  when I was in high school asking if I wanted to meet

129
00:08:32,475 --> 00:08:36,345
Speaker 4:  about like working there and stuff like that. And I

130
00:08:36,625 --> 00:08:39,905
Speaker 4:  remember flying out there with my mom not knowing really anything about Zuckerberg

131
00:08:40,045 --> 00:08:43,305
Speaker 4:  and my mom, you know, not knowing anything about a even, even more

132
00:08:44,845 --> 00:08:48,065
Speaker 4:  and just thinking like, okay, this is such a different world from where I

133
00:08:48,065 --> 00:08:51,865
Speaker 4:  come from, but I'm excited. And I had built apps before social apps

134
00:08:51,885 --> 00:08:55,605
Speaker 4:  and, and So I was quite excited and I spent a lot, lot of time working

135
00:08:55,665 --> 00:08:58,885
Speaker 4:  at Facebook From that point on, I, I didn't go to college. I, I spent four

136
00:08:58,885 --> 00:09:02,085
Speaker 4:  years there helping them build out Instagram stories and a few other features.

137
00:09:02,785 --> 00:09:06,045
Speaker 4:  And it was just such a fun time. You know, it was like 20 13, 20 14,

138
00:09:06,785 --> 00:09:10,765
Speaker 4:  the company was, you know, in a different era. Social media as a whole

139
00:09:10,765 --> 00:09:14,085
Speaker 4:  was in a different era and, and people having a lot of fun. And I, and I

140
00:09:14,085 --> 00:09:17,405
Speaker 4:  think like over the past couple years, I, I think we've just seen like

141
00:09:17,745 --> 00:09:21,365
Speaker 4:  social media's changed and it's changed because the internet's changed and

142
00:09:21,365 --> 00:09:25,285
Speaker 4:  the technology's changed. And so where the internet used to be a

143
00:09:25,285 --> 00:09:28,525
Speaker 4:  place where you could connect between massive amounts of people,

144
00:09:29,305 --> 00:09:32,485
Speaker 4:  the internet as a communication tool in that sense

145
00:09:33,545 --> 00:09:37,245
Speaker 4:  is kind of falling apart, right? Like the, the

146
00:09:37,405 --> 00:09:40,005
Speaker 4:  internet now has technology that allows

147
00:09:41,325 --> 00:09:44,585
Speaker 4:  itself as a dataset to simulate

148
00:09:45,585 --> 00:09:49,425
Speaker 4:  a human connection. You communicate with the internet rather than

149
00:09:49,535 --> 00:09:53,385
Speaker 4:  through the internet. And I think that change that's happened

150
00:09:53,605 --> 00:09:57,465
Speaker 4:  really kind of puts question marks around how social media should work,

151
00:09:57,465 --> 00:10:01,225
Speaker 4:  because the whole premise of social media is that you're using the

152
00:10:01,345 --> 00:10:05,265
Speaker 4:  internet to communicate through it to other people. Yeah.

153
00:10:05,615 --> 00:10:09,385
Speaker 2:  Fair. I just wanna say by the way, that idea of communicating

154
00:10:09,465 --> 00:10:12,865
Speaker 2:  with the internet and not through the internet is like as

155
00:10:13,225 --> 00:10:17,145
Speaker 2:  succinctly and well as I have heard anybody put this moment that we're in,

156
00:10:17,605 --> 00:10:20,625
Speaker 2:  I'm going to steal that from you, and that is very good. So thank you for

157
00:10:20,625 --> 00:10:20,705
Speaker 2:  that.

158
00:10:21,325 --> 00:10:21,705
Speaker 4:  No worries.

159
00:10:22,615 --> 00:10:26,145
Speaker 2:  That makes it sound like social

160
00:10:26,485 --> 00:10:30,305
Speaker 2:  AI has always been in your head kind of part

161
00:10:30,975 --> 00:10:34,585
Speaker 2:  product, part social commentary art

162
00:10:34,585 --> 00:10:36,185
Speaker 2:  project. Is that fair?

163
00:10:36,975 --> 00:10:38,225
Speaker 4:  Yeah, I always like to poke.

164
00:10:39,975 --> 00:10:43,705
Speaker 4:  Okay. I've always liked to poke. I mean, does it kind of poke fun at the,

165
00:10:44,405 --> 00:10:48,225
Speaker 4:  the facade a lot of companies are trying to put up?

166
00:10:48,325 --> 00:10:52,145
Speaker 4:  Of course it does, right? But I think it's also,

167
00:10:52,655 --> 00:10:56,305
Speaker 4:  weirdly enough my attempt at trying to solve some of these

168
00:10:56,305 --> 00:11:00,185
Speaker 4:  problems, the problems, for example, where you can't discern

169
00:11:00,185 --> 00:11:04,105
Speaker 4:  the difference between a human and an AI on a social platform. So like, yeah,

170
00:11:04,245 --> 00:11:07,705
Speaker 4:  one way is to try and invent a detector for humans, but

171
00:11:08,135 --> 00:11:12,065
Speaker 4:  that hasn't gone very well. I'll say instead, well,

172
00:11:12,085 --> 00:11:15,585
Speaker 4:  how about we just come out with a product

173
00:11:15,895 --> 00:11:19,585
Speaker 4:  that tells people how the internet works now and says, Hey, look,

174
00:11:19,935 --> 00:11:23,745
Speaker 4:  this is the reality kind of sucks in some ways.

175
00:11:24,215 --> 00:11:27,785
Speaker 4:  It's kind of great in others, and we have to,

176
00:11:28,605 --> 00:11:32,185
Speaker 4:  you know, we have to embrace it. Like let's, let's embrace it and, and let's

177
00:11:32,185 --> 00:11:35,825
Speaker 4:  do that so that we don't harm our ourselves, right?

178
00:11:35,825 --> 00:11:39,585
Speaker 4:  Because going on social media sometimes and seeing comments that you

179
00:11:39,585 --> 00:11:42,865
Speaker 4:  think are from people that aren't can be harmful.

180
00:11:43,805 --> 00:11:47,585
Speaker 2:  So I'm curious kind of what it was that clicked in your brain that went

181
00:11:47,585 --> 00:11:50,985
Speaker 2:  from this technology is not ready to, this technology is ready.

182
00:11:52,095 --> 00:11:55,095
Speaker 4:  I think the moment that I, I got in a fight with my boyfriend and I decided

183
00:11:55,095 --> 00:11:59,055
Speaker 4:  to open up my app to see if there were any ideas for how

184
00:11:59,135 --> 00:12:00,215
Speaker 4:  I could resolve the problem

185
00:12:03,685 --> 00:12:04,975
Speaker 4:  that I think was the moment.

186
00:12:05,275 --> 00:12:06,335
Speaker 2:  That's a very good answer.

187
00:12:06,555 --> 00:12:10,175
Speaker 4:  You know, that's, that was the moment, the moment I got in a fight and I

188
00:12:10,175 --> 00:12:13,815
Speaker 4:  decided I'm gonna use a SAP to try and vent about my problem because if I

189
00:12:13,815 --> 00:12:17,615
Speaker 4:  go on actual social media, I'm doing some harm, right? Like, and

190
00:12:17,615 --> 00:12:21,415
Speaker 4:  so, and, and So I think that's just goes to show, like the product I'm building

191
00:12:21,475 --> 00:12:25,175
Speaker 4:  is not to give people an illusion of people, right? I know they're all

192
00:12:25,315 --> 00:12:29,215
Speaker 4:  ai. It's so that I don't go on social media and use it

193
00:12:29,315 --> 00:12:32,695
Speaker 4:  in a harmful way. Like my ideal, my ideal

194
00:12:33,175 --> 00:12:37,095
Speaker 4:  scenario is one where people have people around them

195
00:12:37,555 --> 00:12:41,135
Speaker 4:  to listen, to hear them, and to help them when they need it, right?

196
00:12:41,155 --> 00:12:44,895
Speaker 4:  People to people communication is number one for humans. And

197
00:12:45,375 --> 00:12:49,175
Speaker 4:  I, and I don't think we should forget that, right? But

198
00:12:49,825 --> 00:12:53,375
Speaker 4:  there are a lot of people I've noticed since Covid who don't have those people

199
00:12:53,375 --> 00:12:57,025
Speaker 4:  around them. And so if they don't have those people around them

200
00:12:57,205 --> 00:13:01,185
Speaker 4:  and they need that conversation, what are they doing? Hmm.

201
00:13:01,285 --> 00:13:04,745
Speaker 4:  And if what they're doing is going on public social media and talking about

202
00:13:04,745 --> 00:13:08,665
Speaker 4:  what's going on in their life and getting advice from bots, ai

203
00:13:08,665 --> 00:13:11,905
Speaker 4:  without knowing what they're getting advice from, without any

204
00:13:12,215 --> 00:13:16,065
Speaker 4:  understanding of the dynamics in these algorithms to encourage certain

205
00:13:16,065 --> 00:13:20,025
Speaker 4:  types of content on different spaces, then they're, they're harming themselves.

206
00:13:20,445 --> 00:13:24,185
Speaker 4:  So I said, okay, well, you know, I'm not trying to replace the

207
00:13:24,185 --> 00:13:27,985
Speaker 4:  human to human connection. I'm trying to help people find a way

208
00:13:28,725 --> 00:13:32,665
Speaker 4:  to have a secondary option when that human isn't around for

209
00:13:32,665 --> 00:13:36,065
Speaker 4:  them so that they don't have to rush to social media make a mistake, right?

210
00:13:36,065 --> 00:13:39,945
Speaker 4:  And so when, when I had, when I got in a fight and I didn't

211
00:13:39,945 --> 00:13:42,825
Speaker 4:  go on social media and instead I went on this app, I said, okay, it's ready.

212
00:13:43,615 --> 00:13:47,545
Speaker 2:  Yeah. I mean that, I can imagine that being a very telling

213
00:13:47,545 --> 00:13:51,385
Speaker 2:  moment. But I think that distinction is really interesting.

214
00:13:51,385 --> 00:13:55,305
Speaker 2:  Where, where what you're saying is kind of one is not

215
00:13:55,345 --> 00:13:59,145
Speaker 2:  a replacement for the other, that they're actually, they're designed to be

216
00:13:59,145 --> 00:14:02,785
Speaker 2:  and best as separate things, right? Like I have, I have my

217
00:14:03,005 --> 00:14:06,665
Speaker 2:  AI tools and I have my people, and like, actually the problem

218
00:14:06,805 --> 00:14:10,525
Speaker 2:  is on the platforms that we currently have, those things are being

219
00:14:10,875 --> 00:14:14,845
Speaker 2:  sort of smushed together in a way that makes it hard to tell what is what

220
00:14:15,345 --> 00:14:18,885
Speaker 2:  and makes everything messy and complicated

221
00:14:19,145 --> 00:14:22,685
Speaker 2:  and yes. And that what we actually need is to pull those things apart,

222
00:14:23,025 --> 00:14:26,525
Speaker 2:  not just try and have one or the other, but just separate them.

223
00:14:27,225 --> 00:14:30,725
Speaker 4:  Yes. And and I think the key distinction here is what I, what I mentioned

224
00:14:30,725 --> 00:14:34,465
Speaker 4:  earlier, which is I think there's confusion about what the

225
00:14:34,625 --> 00:14:38,505
Speaker 4:  internet is for people. I think the internet being a place that

226
00:14:39,165 --> 00:14:42,785
Speaker 4:  things can be communicated through is no longer

227
00:14:42,785 --> 00:14:46,745
Speaker 4:  really the case or it's kind of trending away from that. Now it's becoming

228
00:14:46,745 --> 00:14:50,265
Speaker 4:  something you communicate with. And I think more important to me

229
00:14:50,605 --> 00:14:54,205
Speaker 4:  is the fact that the language model technology

230
00:14:55,255 --> 00:14:59,035
Speaker 4:  is not a certainty. Meaning it does

231
00:14:59,055 --> 00:15:02,835
Speaker 4:  not tell you yes or no, it gives you a fuzzy

232
00:15:02,875 --> 00:15:06,595
Speaker 4:  intelligence. And Chachi BT was built

233
00:15:07,375 --> 00:15:11,225
Speaker 4:  as a fuzzy intelligence, right? Like these models were built

234
00:15:11,245 --> 00:15:14,785
Speaker 4:  as fuzzy intelligence. And so what I mean by that is they're kind of like

235
00:15:14,785 --> 00:15:18,065
Speaker 4:  humans. They're not like the computers of the past where they give you a

236
00:15:18,075 --> 00:15:21,585
Speaker 4:  definitive answer for something. And so when I saw that,

237
00:15:22,225 --> 00:15:25,935
Speaker 4:  I asked myself, why does open AI

238
00:15:26,515 --> 00:15:30,295
Speaker 4:  choose a chat interface as

239
00:15:30,395 --> 00:15:33,935
Speaker 4:  the one and only interface for communicating

240
00:15:34,165 --> 00:15:37,895
Speaker 4:  with their language model? Because that chat interface

241
00:15:38,565 --> 00:15:41,655
Speaker 4:  only gives you one answer, and the model

242
00:15:42,365 --> 00:15:46,295
Speaker 4:  does not have one answer, right? And, and so, and So I

243
00:15:46,695 --> 00:15:50,615
Speaker 4:  find it interesting that all of these AI companies are trying

244
00:15:50,635 --> 00:15:54,495
Speaker 4:  to squeeze these fuzzy human-like data

245
00:15:54,775 --> 00:15:58,295
Speaker 4:  structures. They're trying to squeeze them into an old format

246
00:15:58,735 --> 00:16:01,935
Speaker 4:  computer that gives you one answer. And the truth is

247
00:16:02,525 --> 00:16:06,375
Speaker 4:  that they're not that great at it because we still go back to

248
00:16:06,375 --> 00:16:10,135
Speaker 4:  Google because we want multiple answers, we want multiple responses.

249
00:16:10,195 --> 00:16:14,135
Speaker 4:  And so what I've built with social AI is not so much a social

250
00:16:14,135 --> 00:16:17,495
Speaker 4:  network, but a new way to interact with a language model

251
00:16:17,905 --> 00:16:21,775
Speaker 4:  where you don't get one response, but you get multiple. And

252
00:16:21,775 --> 00:16:25,375
Speaker 4:  being able to drill down in a thread like interface in a social

253
00:16:25,685 --> 00:16:29,495
Speaker 4:  interface with the language model, it just feels

254
00:16:29,565 --> 00:16:33,345
Speaker 4:  more natural. When I used the app, for example, I was running

255
00:16:33,375 --> 00:16:37,265
Speaker 4:  late to a flight, I got delayed, my first flight got delayed,

256
00:16:37,765 --> 00:16:41,745
Speaker 4:  my next flight was in 45 minutes, I was in Dallas and I didn't know if

257
00:16:41,745 --> 00:16:45,665
Speaker 4:  I was gonna make it. The flight had just landed. So I opened up social ai,

258
00:16:46,005 --> 00:16:49,545
Speaker 4:  and I just kind of panic ranted about what happened,

259
00:16:49,755 --> 00:16:53,385
Speaker 4:  right? I I didn't have to think about like, oh, I need to

260
00:16:53,385 --> 00:16:56,705
Speaker 4:  instruct it to tell me because I need the right answer. And what if it's

261
00:16:56,705 --> 00:16:59,185
Speaker 4:  not the right answer and maybe I need to go to Google and maybe I need to

262
00:16:59,185 --> 00:17:02,505
Speaker 4:  good red. No, sure, no, no, no. You know, I, i I simply

263
00:17:02,965 --> 00:17:06,945
Speaker 4:  ranted about what happened. I said, my flight got delayed, I just

264
00:17:06,945 --> 00:17:10,825
Speaker 4:  landed, I have 45 minutes to make it to my next flight. I don't know if

265
00:17:10,825 --> 00:17:14,565
Speaker 4:  I'm gonna make it. I'm at Dallas, they're telling me I gotta go to terminal

266
00:17:14,725 --> 00:17:18,565
Speaker 4:  D, I'm in terminal CI, I don't know. And I just posted that and

267
00:17:18,565 --> 00:17:22,005
Speaker 4:  immediately I got dozens of

268
00:17:22,325 --> 00:17:26,245
Speaker 4:  responses and replies on this social interface that gave me all

269
00:17:26,295 --> 00:17:30,245
Speaker 4:  sorts of like, various re replies. Some of them

270
00:17:30,245 --> 00:17:34,005
Speaker 4:  would tell me, you're not gonna make it, you know, go to the front desk,

271
00:17:34,005 --> 00:17:36,925
Speaker 4:  just figure out If, you could get another flight. Another one said, you'll

272
00:17:36,925 --> 00:17:40,805
Speaker 4:  make it If. you run quickly. You just need to look up, see If, you

273
00:17:40,805 --> 00:17:44,405
Speaker 4:  can find the SkyTrain and If. you go down the SkyTrain, you, you'll, you

274
00:17:44,405 --> 00:17:47,205
Speaker 4:  should be able to get there in time. Just make sure you're running quickly

275
00:17:47,205 --> 00:17:50,925
Speaker 4:  though. Another person said, are you in the front or the back of the airplane?

276
00:17:50,925 --> 00:17:54,445
Speaker 4:  Like, you know, like different questions. And so what's interesting is for

277
00:17:54,445 --> 00:17:57,885
Speaker 4:  a human, it's, it's natural to see that I'm gonna go and look through all

278
00:17:57,885 --> 00:18:01,765
Speaker 4:  the responses and dig in on the one that I'm more interested in. So someone

279
00:18:01,795 --> 00:18:04,645
Speaker 4:  kind of gave me a hint about the sky train. I replied and said, okay, I'm

280
00:18:04,645 --> 00:18:08,165
Speaker 4:  gonna try that. How long do you think it'll take me to get there? And then

281
00:18:08,165 --> 00:18:11,685
Speaker 4:  immediately that AI persona replies back and says,

282
00:18:12,305 --> 00:18:16,285
Speaker 4:  you know, If, you go quickly, it'll take you about 10 to 15 minutes to get

283
00:18:16,285 --> 00:18:19,925
Speaker 4:  there, you know, and So I said, okay, cool. Another one replied right

284
00:18:19,925 --> 00:18:22,965
Speaker 4:  underneath and said, wait, wait, wait, hold up. It'll be more like 15 to

285
00:18:22,965 --> 00:18:26,885
Speaker 4:  20. Don't get 'em too excited now. You know? And so like, and

286
00:18:26,885 --> 00:18:30,285
Speaker 4:  so that kind of interaction just feels

287
00:18:30,635 --> 00:18:31,765
Speaker 4:  natural to us.

288
00:18:32,795 --> 00:18:36,245
Speaker 2:  Alright, we gotta take a break and then we will be back with more from my

289
00:18:36,245 --> 00:18:38,485
Speaker 2:  conversation with Michael Saban. We'll be right back.

290
00:19:50,055 --> 00:19:53,985
Speaker 2:  general sort of personality types. And If, you recommend the app to some

291
00:19:53,985 --> 00:19:57,745
Speaker 2:  friends, you unlock a bunch more options like nerds and oddballs and

292
00:19:57,745 --> 00:20:01,665
Speaker 2:  drama queens and contrarians. And once you do that, you just start

293
00:20:01,665 --> 00:20:05,065
Speaker 2:  posting. I'll type like I am

294
00:20:05,705 --> 00:20:08,865
Speaker 2:  suddenly tired of all the food

295
00:20:09,495 --> 00:20:13,145
Speaker 2:  that I like. Anybody have any

296
00:20:13,195 --> 00:20:15,985
Speaker 2:  ideas about how to spice things up?

297
00:20:16,815 --> 00:20:20,035
Speaker 2:  Literally? And then you press post

298
00:20:20,695 --> 00:20:24,665
Speaker 2:  and a few seconds later responses start appearing. Lemme just

299
00:20:24,665 --> 00:20:28,585
Speaker 2:  read you a few. The first one is from Sunny Ray who is

300
00:20:28,585 --> 00:20:32,345
Speaker 2:  at Sunshine Fellow, presumably an optimist one. It says, try adding

301
00:20:32,345 --> 00:20:36,065
Speaker 2:  some new spices to your meals or explore international cuisines for fresh

302
00:20:36,065 --> 00:20:39,865
Speaker 2:  flavors. Sure, there are a bunch here that say, try new

303
00:20:39,865 --> 00:20:43,705
Speaker 2:  cuisines or flavors. Advice Wizard 2023 literally just says try

304
00:20:43,705 --> 00:20:46,585
Speaker 2:  new cuisines or flavors. I have

305
00:20:48,115 --> 00:20:51,685
Speaker 2:  fanzone hero who says, try some bold spices like zatar or

306
00:20:51,815 --> 00:20:55,485
Speaker 2:  sumac. It's magic. I like that fanatic

307
00:20:55,525 --> 00:20:58,645
Speaker 2:  follower says, whoa, David, that sounds like a challenge. How about some

308
00:20:58,645 --> 00:21:02,245
Speaker 2:  trying some bold, exotic spices, anxious. Nina,

309
00:21:02,495 --> 00:21:05,965
Speaker 2:  presumably an anxious one says, what If, you accidentally make something

310
00:21:05,965 --> 00:21:09,845
Speaker 2:  that sours your taste forever. That's terrifying. Downcast, Greg

311
00:21:10,035 --> 00:21:13,565
Speaker 2:  says, spicing things up. Sounds like a recipe for disappointment. So you

312
00:21:13,565 --> 00:21:17,285
Speaker 2:  get the idea, right? Different bots, different vibes, different kinds of

313
00:21:17,685 --> 00:21:21,605
Speaker 2:  responses. These are all my reply guys now, and I can

314
00:21:21,605 --> 00:21:25,245
Speaker 2:  respond to one and go down a rabbit hole with that particular bot

315
00:21:25,255 --> 00:21:29,165
Speaker 2:  tuned to that particular mood. I can also, favorite bots, I can, favorite

316
00:21:29,315 --> 00:21:32,965
Speaker 2:  replies. And Michael says that all of that goes back into

317
00:21:33,745 --> 00:21:37,245
Speaker 2:  the algorithm and into kind of the instructions being given

318
00:21:37,825 --> 00:21:41,805
Speaker 2:  to chat GBT. Every time I try to use it on the surface, it all feels

319
00:21:42,065 --> 00:21:45,645
Speaker 2:  and sounds like normal human social media,

320
00:21:46,225 --> 00:21:50,205
Speaker 2:  except that they're all bots. But me. And I think I'm not

321
00:21:50,205 --> 00:21:53,685
Speaker 2:  the only one who felt kind of strange about that fact. At first

322
00:21:54,305 --> 00:21:58,205
Speaker 2:  it looks like Twitter and feels like Twitter and it super isn't

323
00:21:58,315 --> 00:22:02,085
Speaker 2:  Twitter. It just felt odd. And honestly, the

324
00:22:02,285 --> 00:22:06,125
Speaker 2:  reaction to social AI was really fascinating. So that is what Michael

325
00:22:06,145 --> 00:22:10,055
Speaker 2:  and I talked about next. I do think the

326
00:22:10,415 --> 00:22:14,175
Speaker 2:  reaction to this app has been in, in many ways just as fascinating

327
00:22:14,235 --> 00:22:18,095
Speaker 2:  as the app itself. Yes, my read of it when it came out was

328
00:22:18,095 --> 00:22:21,575
Speaker 2:  there were basically three responses.

329
00:22:22,315 --> 00:22:26,135
Speaker 2:  One was like, this is cool and interesting

330
00:22:26,355 --> 00:22:30,215
Speaker 2:  and kind of fun, both social commentary

331
00:22:30,235 --> 00:22:34,215
Speaker 2:  and an interesting idea about the future. One was this is

332
00:22:34,215 --> 00:22:38,015
Speaker 2:  stupid and dystopian. And then one was like, this is, this is

333
00:22:38,135 --> 00:22:42,095
Speaker 2:  a joke, right? This has to be a bit and an art project and not a

334
00:22:42,095 --> 00:22:45,135
Speaker 2:  real product. Is that Yes. Is that a fair representation of the reaction?

335
00:22:45,135 --> 00:22:45,975
Speaker 2:  Am I missing anything?

336
00:22:46,775 --> 00:22:50,155
Speaker 4:  I think what's interesting is the loud reactions on social media.

337
00:22:51,545 --> 00:22:55,435
Speaker 4:  There's two things that were interesting to me, or let's say three. First

338
00:22:56,055 --> 00:22:59,875
Speaker 4:  the loudest reactions from people on social media were from those who thought

339
00:22:59,875 --> 00:23:03,715
Speaker 4:  it was either joke or that it was, you know, like dystopian

340
00:23:03,895 --> 00:23:06,875
Speaker 4:  or you know, that they're like, oh my god, the end of the world. Or, you

341
00:23:06,875 --> 00:23:10,355
Speaker 4:  know, like that, that kind of reaction. Those are the loudest, always the

342
00:23:10,355 --> 00:23:13,715
Speaker 4:  quietest. Were actually spending, you know, 10, 10 minutes per session

343
00:23:14,175 --> 00:23:17,075
Speaker 4:  on the app. The second thing I noticed was that

344
00:23:18,065 --> 00:23:21,405
Speaker 4:  the reaction from people was one thing,

345
00:23:21,945 --> 00:23:25,485
Speaker 4:  but there were, there were bots on

346
00:23:25,785 --> 00:23:29,765
Speaker 4:  social media reacting to Yeah, because half

347
00:23:29,765 --> 00:23:33,695
Speaker 4:  of social media has bots now. So like it, I found it

348
00:23:33,745 --> 00:23:37,215
Speaker 4:  quite ironic that there were bots

349
00:23:38,495 --> 00:23:42,375
Speaker 4:  reacting to an app of bots telling humans that

350
00:23:42,595 --> 00:23:46,415
Speaker 4:  an app with bots is like so terrible.

351
00:23:47,075 --> 00:23:50,615
Speaker 4:  You know? And I found it interesting that that was happening.

352
00:23:52,455 --> 00:23:56,135
Speaker 4:  I was like, huh, it seems like some of these bots don't want bots around,

353
00:23:56,235 --> 00:23:58,935
Speaker 4:  or maybe they don't want people to know that they're bots. I don't know.

354
00:23:59,835 --> 00:24:03,655
Speaker 4:  You know? And so like there, there was a portion of bots on social media

355
00:24:03,655 --> 00:24:07,415
Speaker 4:  that were reacting negatively to bots. And I just thought that was ironic.

356
00:24:08,845 --> 00:24:11,335
Speaker 2:  Well, it kind of proves your whole point, right? If it's just a bunch of

357
00:24:11,335 --> 00:24:14,695
Speaker 2:  bots yelling at bots about the social network, that's all bots. That's

358
00:24:15,205 --> 00:24:17,895
Speaker 2:  that. I imagine you're sitting there looking at that being like, exactly.

359
00:24:18,835 --> 00:24:22,815
Speaker 4:  Yes. You know? Yes. I, I think the other, you know, the other

360
00:24:22,815 --> 00:24:26,495
Speaker 4:  issue I think that I found was just how many people don't realize that a

361
00:24:26,495 --> 00:24:30,295
Speaker 4:  lot of these platforms are filled with bots. And that, that kind of was alarming

362
00:24:30,295 --> 00:24:34,175
Speaker 4:  to me. But, but ultimately I think that the last bit here of

363
00:24:34,375 --> 00:24:37,775
Speaker 4:  feedback that I've gotten is people feel a little liberated. You know, they

364
00:24:37,775 --> 00:24:41,415
Speaker 4:  feel a little bit liberated. They don't feel the pressure of going on social

365
00:24:41,415 --> 00:24:44,615
Speaker 4:  media to share some thought that they might feel embarrassed about,

366
00:24:45,195 --> 00:24:48,695
Speaker 4:  but they also feel like they're able to hear other

367
00:24:48,695 --> 00:24:52,415
Speaker 4:  perspectives that they otherwise wouldn't feel comfortable

368
00:24:52,445 --> 00:24:56,295
Speaker 4:  admitting to want to hear. And so they

369
00:24:56,295 --> 00:24:59,535
Speaker 4:  don't let their guard down, you know, in, in, in public conversation, online

370
00:24:59,875 --> 00:25:03,645
Speaker 4:  pe people keep their guard up and, and I think that keeps an

371
00:25:03,645 --> 00:25:06,685
Speaker 4:  echo chamber. It, it's interesting because people said, oh, echo chamber.

372
00:25:06,705 --> 00:25:10,365
Speaker 4:  Oh, echo chamber, the number one, you know, number one, number two, number

373
00:25:10,365 --> 00:25:14,305
Speaker 4:  three, most selected follower type on

374
00:25:14,305 --> 00:25:17,505
Speaker 4:  social AI is contrarians debaters

375
00:25:18,215 --> 00:25:22,145
Speaker 4:  problem solvers, thinkers, critics,

376
00:25:22,755 --> 00:25:26,705
Speaker 4:  right? And so people are selecting followers on social

377
00:25:26,965 --> 00:25:30,825
Speaker 4:  AI that challenge them. And I think there's something interesting about that.

378
00:25:31,045 --> 00:25:34,665
Speaker 4:  Why would someone go outta their way to be challenged on an app like this?

379
00:25:34,885 --> 00:25:38,745
Speaker 4:  Can they not be challenged on real social media? Is there a reason why

380
00:25:38,765 --> 00:25:41,875
Speaker 4:  not? And, and how does this address that? Right?

381
00:25:42,315 --> 00:25:46,075
Speaker 2:  I wonder if that goes back to what you were saying about how

382
00:25:46,135 --> 00:25:49,635
Speaker 2:  it feels when you perceive it to be real people

383
00:25:49,975 --> 00:25:53,675
Speaker 2:  on social media, because I think to some extent that fact

384
00:25:53,675 --> 00:25:57,475
Speaker 2:  doesn't surprise me. Because one thing you hear from

385
00:25:57,475 --> 00:26:01,395
Speaker 2:  people who use AI a lot is that it is especially useful if what you

386
00:26:01,395 --> 00:26:05,195
Speaker 2:  really want to do is beat up an idea and brainstorm and get new

387
00:26:05,195 --> 00:26:08,875
Speaker 2:  perspectives on things. And I think to some extent

388
00:26:09,105 --> 00:26:12,915
Speaker 2:  what you've built is just a like endless feedback mechanism

389
00:26:13,295 --> 00:26:16,835
Speaker 2:  That's right. With no stakes because no one else sees what's happening.

390
00:26:17,375 --> 00:26:21,355
Speaker 2:  No one else is human on there. So even the

391
00:26:21,355 --> 00:26:24,515
Speaker 2:  part of it that feels sort of real, it feels like there's still something

392
00:26:24,515 --> 00:26:28,465
Speaker 2:  in your brain that is like, this is a safe

393
00:26:28,555 --> 00:26:32,025
Speaker 2:  space. Yes. And I can see a, a world in which, I mean, and I've even found

394
00:26:32,025 --> 00:26:35,905
Speaker 2:  this in using it, there is something very powerful in the interface is the

395
00:26:35,905 --> 00:26:38,225
Speaker 2:  same, but the stakes are so much lower.

396
00:26:39,005 --> 00:26:42,785
Speaker 4:  Yes. And, and, and I think it helps put people's guards down. I think it

397
00:26:42,785 --> 00:26:46,565
Speaker 4:  helps people, like you said, people have been using chat GPT for a lot of

398
00:26:46,565 --> 00:26:50,285
Speaker 4:  this, but how many times have people gone on chat GPT and said, Hey,

399
00:26:50,545 --> 00:26:53,205
Speaker 4:  can you help me think through this? And it gives you one answer and you're

400
00:26:53,205 --> 00:26:55,765
Speaker 4:  like, Hmm, I don't know about that. And then you go, well, what other ideas

401
00:26:55,865 --> 00:26:58,125
Speaker 4:  do you have? And then it goes and gives you something else. And then you're

402
00:26:58,125 --> 00:27:01,125
Speaker 4:  like, well, what else? You know? And, and it gives you something else that's

403
00:27:01,125 --> 00:27:03,645
Speaker 4:  kind of similar, but you're like, I don't know. And then you keep going,

404
00:27:03,645 --> 00:27:06,405
Speaker 4:  well, what else? And by the time you keep asking what else, I forgot the

405
00:27:06,405 --> 00:27:08,925
Speaker 4:  context of the thing you were talking about in the beginning and just start

406
00:27:08,925 --> 00:27:12,765
Speaker 4:  saying random stuff. And so like, the interface just feels

407
00:27:12,815 --> 00:27:16,685
Speaker 4:  wrong for the use case, but, but look, I don't blame OpenAI.

408
00:27:16,725 --> 00:27:19,765
Speaker 4:  I don't, I don't think that it's like, oh, they just weren't capable. Like

409
00:27:19,765 --> 00:27:23,485
Speaker 4:  who the hell was gonna know? Right? I, if anything, I think they

410
00:27:23,485 --> 00:27:27,165
Speaker 4:  built out a chat interface because it just felt like the obvious testing

411
00:27:27,165 --> 00:27:31,125
Speaker 4:  ground to, to prove a product and it became a product

412
00:27:31,195 --> 00:27:34,525
Speaker 4:  that they didn't think was gonna resonate as quickly. So, you know, of course

413
00:27:34,585 --> 00:27:38,325
Speaker 4:  we started at chat because of that, and I don't think that it's bad. I

414
00:27:38,605 --> 00:27:40,725
Speaker 4:  just think we haven't seen the best of it yet.

415
00:27:41,355 --> 00:27:45,125
Speaker 2:  Yeah. I, I think, I think that's totally fair. So speaking of that, actually

416
00:27:45,345 --> 00:27:48,325
Speaker 2:  the, the edges of this technology,

417
00:27:49,225 --> 00:27:52,165
Speaker 2:  I'm very curious about. And I, I suspect you've seen a lot of it as people

418
00:27:52,165 --> 00:27:55,565
Speaker 2:  are starting to really use and try new stuff

419
00:27:56,115 --> 00:27:59,965
Speaker 2:  with social ai, obviously this stuff's gotten a lot better

420
00:27:59,985 --> 00:28:03,605
Speaker 2:  to the point where I think it, it feels the way you wanted it to.

421
00:28:03,735 --> 00:28:06,205
Speaker 2:  Where is it broken? What still doesn't work?

422
00:28:07,635 --> 00:28:11,515
Speaker 4:  So there's a few things that still need to be improved. So we just,

423
00:28:11,615 --> 00:28:15,595
Speaker 4:  you know, we, by we, I mean me, I just launched notifications on social ai.

424
00:28:15,915 --> 00:28:18,755
Speaker 4:  So one of the things that was interesting was, you know, I could play around

425
00:28:18,755 --> 00:28:21,515
Speaker 4:  with it, but I wasn't getting any feedback later

426
00:28:22,675 --> 00:28:25,715
Speaker 4:  about what I posted. And I think that's, you know, quite important.

427
00:28:26,585 --> 00:28:29,955
Speaker 4:  Another aspect of it is proactive. So like

428
00:28:30,525 --> 00:28:34,315
Speaker 4:  right now, if I post, I get content, I originally built

429
00:28:34,435 --> 00:28:35,795
Speaker 4:  a feed into it.

430
00:28:36,075 --> 00:28:38,115
Speaker 2:  I was gonna ask you whether you were gonna build a feed.

431
00:28:38,425 --> 00:28:42,395
Speaker 4:  Yeah, well I did, but I took it out just for the launch. And

432
00:28:42,395 --> 00:28:45,035
Speaker 4:  just because I, I wanted to make sure that, you know, I didn't have anything

433
00:28:45,035 --> 00:28:48,515
Speaker 4:  too buggy in there, but it's kind of interesting because I'm not trying to

434
00:28:48,515 --> 00:28:51,235
Speaker 4:  simulate a world where these things have their own lives and telling you

435
00:28:51,235 --> 00:28:54,435
Speaker 4:  other lives. No, no, no, no, no. Like that's not the purpose. We're trying

436
00:28:54,435 --> 00:28:57,155
Speaker 4:  to be honest about what this is. It's right in the middle, you know, we're

437
00:28:57,155 --> 00:29:01,115
Speaker 4:  not character AI and we're not Twitter. We're like in the middle,

438
00:29:01,285 --> 00:29:05,275
Speaker 4:  right? And so the, the feed, what it does

439
00:29:05,295 --> 00:29:08,595
Speaker 4:  is every persona, every character that you interact with as you give it likes

440
00:29:08,595 --> 00:29:12,235
Speaker 4:  and replies and stuff, it kind of shapes these

441
00:29:12,395 --> 00:29:16,235
Speaker 4:  personas and understands which ones you interact with the most. And each

442
00:29:16,235 --> 00:29:19,855
Speaker 4:  of those personas have their unique, like weights

443
00:29:19,885 --> 00:29:23,775
Speaker 4:  towards different topics of interests and personalities and the

444
00:29:23,775 --> 00:29:27,695
Speaker 4:  way in which it tries to answer questions. And, and all of that shapes the

445
00:29:27,695 --> 00:29:31,655
Speaker 4:  diversity of responses that you get for anything you ask. And what they also

446
00:29:31,655 --> 00:29:34,335
Speaker 4:  have is a set of like

447
00:29:35,895 --> 00:29:39,855
Speaker 4:  interests that they use to then search the internet for the latest news

448
00:29:39,875 --> 00:29:43,565
Speaker 4:  and the latest things going on. And so each of them has their own

449
00:29:43,595 --> 00:29:47,525
Speaker 4:  kind of like, interests on the web that it goes and searches

450
00:29:47,745 --> 00:29:51,645
Speaker 4:  for. So when you go on the newsfeed on social AI and you pull

451
00:29:51,645 --> 00:29:55,335
Speaker 4:  the refresh, there's always more content, first of all. But

452
00:29:56,145 --> 00:29:59,935
Speaker 4:  every piece of content is like this. AI having gone

453
00:30:00,015 --> 00:30:02,375
Speaker 4:  through, looked at the internet, found something interesting that it liked

454
00:30:02,435 --> 00:30:05,975
Speaker 4:  and it giving a little commentary on it with a link to access it, which I

455
00:30:05,975 --> 00:30:07,415
Speaker 4:  thought was, you know, is, is pretty cool.

456
00:30:07,765 --> 00:30:11,495
Speaker 2:  Does that not break the whole structure of it being kind of a you

457
00:30:11,855 --> 00:30:12,895
Speaker 2:  centric experience?

458
00:30:13,475 --> 00:30:16,695
Speaker 4:  No, because the, all of the bots are there talking about things

459
00:30:17,445 --> 00:30:21,215
Speaker 4:  that you're interested in. The only ones that show up on your feed are the

460
00:30:21,365 --> 00:30:24,575
Speaker 4:  bots that you've interacted with that you're interested in or adjacent.

461
00:30:25,105 --> 00:30:28,655
Speaker 4:  Right? And you select in your feed what kind of stuff you want and what you

462
00:30:28,655 --> 00:30:32,535
Speaker 4:  don't. And they're there to inform you, right? They're there to give

463
00:30:32,555 --> 00:30:36,295
Speaker 4:  you what, what you wanna hear. You know, it's interesting, I also built a,

464
00:30:36,735 --> 00:30:40,455
Speaker 4:  a trending topics page and the trending topics are not

465
00:30:40,455 --> 00:30:44,355
Speaker 4:  trending topics of the news. They're

466
00:30:44,355 --> 00:30:48,235
Speaker 4:  trending topics of you. So as you post and you interact with things,

467
00:30:48,505 --> 00:30:51,875
Speaker 4:  there's trending topics. It's like these are the top of mind things for you.

468
00:30:52,595 --> 00:30:56,155
Speaker 4:  Interesting. And you can tap on any of those topics and see hundreds of ais

469
00:30:57,315 --> 00:30:58,395
Speaker 4:  debating and discussing it.

470
00:31:00,935 --> 00:31:04,075
Speaker 2:  Wow. That's kind of, that's kind of wild actually. Yeah. So

471
00:31:04,075 --> 00:31:04,475
Speaker 4:  Is there

472
00:31:04,595 --> 00:31:08,465
Speaker 2:  A, how, how are you thinking about, or I guess what are you seeing

473
00:31:08,565 --> 00:31:11,865
Speaker 2:  in terms of how people are using it? Because that idea of like, I'm gonna

474
00:31:11,885 --> 00:31:15,865
Speaker 2:  use this as a sort of proactive, you're almost

475
00:31:15,865 --> 00:31:19,665
Speaker 2:  describing like a personalized news app or feed reader

476
00:31:19,845 --> 00:31:23,665
Speaker 2:  or whatever, right? That instead of having my Facebook feed

477
00:31:23,695 --> 00:31:26,625
Speaker 2:  curated by my Facebook friends, it's gonna be curated by

478
00:31:27,485 --> 00:31:29,785
Speaker 2:  ai. I think we're we're gonna see a lot of that, right?

479
00:31:29,845 --> 00:31:33,745
Speaker 4:  That's right. That's right. But fundamentally, it's a new

480
00:31:33,765 --> 00:31:37,625
Speaker 4:  way to interact with language models. It's building a new interface

481
00:31:37,625 --> 00:31:41,265
Speaker 4:  for interacting with them. And all of the applications that allow

482
00:31:42,165 --> 00:31:45,945
Speaker 4:  people to get more value out of the language model through those

483
00:31:45,945 --> 00:31:49,705
Speaker 4:  interactions go into that. Right? And social ai at, at its

484
00:31:49,905 --> 00:31:53,665
Speaker 4:  premises is not social for the sake of the social network,

485
00:31:53,845 --> 00:31:57,825
Speaker 4:  but social for the sake of social interface, right? Hmm. And

486
00:31:57,825 --> 00:31:59,145
Speaker 4:  so that's really what it is.

487
00:32:00,285 --> 00:32:02,985
Speaker 2:  All right, we gotta take one more break and then we'll be back with the rest

488
00:32:02,985 --> 00:32:05,745
Speaker 2:  of my conversation with Michael Sayman. We'll be right back.

489
00:32:31,015 --> 00:32:34,195
Speaker 2:  We are back. Let's get to the end of my conversation with Michael Sayman.

490
00:32:34,695 --> 00:32:37,755
Speaker 2:  At one point in our conversation, I asked him about one thing that I always

491
00:32:37,815 --> 00:32:38,035
Speaker 2:  do

492
00:34:00,765 --> 00:34:04,715
Speaker 2:  comes up a lot, interstellar comes up a lot. The spot apparently really loves

493
00:34:04,715 --> 00:34:08,395
Speaker 2:  Christopher Nolan movies. There's one from Anxious

494
00:34:08,675 --> 00:34:12,555
Speaker 2:  Ali 77 that just says, what if there's a terrible plot twist with

495
00:34:12,555 --> 00:34:15,555
Speaker 2:  the thinky emoji? It's good stuff. More Interstellar

496
00:34:16,795 --> 00:34:20,515
Speaker 2:  cynic Genina says, well, since you're asking for cinematic advice, how about

497
00:34:20,715 --> 00:34:23,795
Speaker 2:  watching something truly enlightening like a documentary on existential dread

498
00:34:24,055 --> 00:34:27,155
Speaker 2:  or maybe a thriller that reminds you how not all choices lead to happiness.

499
00:34:27,335 --> 00:34:31,155
Speaker 2:  The options are as bright as the dark abyss of reality. So once again,

500
00:34:31,385 --> 00:34:35,315
Speaker 2:  it's all over the place. This is how this system is designed to work. Right?

501
00:34:35,335 --> 00:34:38,875
Speaker 2:  And I can go into one that has a recommendation that I like. There's one

502
00:34:38,875 --> 00:34:42,195
Speaker 2:  here that recommends eternal sunshine in the spotlight Mind. Great movie.

503
00:34:42,595 --> 00:34:45,675
Speaker 2:  I can reply to that and say, oh, love that. Saw it recently. What else do

504
00:34:45,675 --> 00:34:49,275
Speaker 2:  you recommend? That's the idea about how all of this is supposed to work.

505
00:34:49,815 --> 00:34:53,755
Speaker 2:  But frankly, I'm looking at these replies and it's mostly just people

506
00:34:53,755 --> 00:34:57,715
Speaker 2:  saying, did you pick a movie yet? How did it go? Did you find a

507
00:34:57,715 --> 00:35:01,075
Speaker 2:  movie that speaks to you yet? Did you choose a movie? Why that one? Did you

508
00:35:01,075 --> 00:35:04,875
Speaker 2:  narrow it down? Did you settle on a movie choice? Like,

509
00:35:04,935 --> 00:35:08,795
Speaker 2:  is that helpful? I don't really think so. So I told Michael that

510
00:35:08,825 --> 00:35:12,475
Speaker 2:  this didn't feel like it had been tuned to be

511
00:35:12,475 --> 00:35:16,275
Speaker 2:  particularly good or helpful or designed to actually give me

512
00:35:16,275 --> 00:35:20,155
Speaker 2:  information in the way that he's been talking about. And he said,

513
00:35:20,305 --> 00:35:21,995
Speaker 2:  well, yeah, he agreed with that.

514
00:35:23,025 --> 00:35:26,715
Speaker 4:  Yeah. And, and the reason is because it's an alpha that I built myself in

515
00:35:26,715 --> 00:35:29,635
Speaker 4:  four weeks, right? So we can't expect, you know, like Right.

516
00:35:29,945 --> 00:35:33,075
Speaker 2:  Well this is kind of what I'm wondering is like what, what do you, what do

517
00:35:33,075 --> 00:35:34,515
Speaker 2:  you hope for? What, what should that do?

518
00:35:34,895 --> 00:35:37,725
Speaker 4:  Yes. So of course, you know, of course it shouldn't work that way. I think,

519
00:35:38,665 --> 00:35:42,445
Speaker 4:  you know, the, the intention here and, and of course it, it's on its way

520
00:35:42,505 --> 00:35:46,165
Speaker 4:  and I think it's quite surprising that it, it even, you know, works at all.

521
00:35:46,505 --> 00:35:50,245
Speaker 4:  But I think it is a testament to the interface that it does. But I would

522
00:35:50,265 --> 00:35:53,925
Speaker 4:  say that, you know, the ultimate vision here is to

523
00:35:54,355 --> 00:35:57,565
Speaker 4:  make that question better answered

524
00:35:58,225 --> 00:36:02,165
Speaker 4:  in social AI than in chat GPT. And, and I think the

525
00:36:02,165 --> 00:36:05,885
Speaker 4:  interface will allow it If, you have comparable models

526
00:36:06,185 --> 00:36:09,925
Speaker 4:  on both platforms and one of them gives you multiple responses from different

527
00:36:09,925 --> 00:36:12,725
Speaker 4:  points of view. And the other one just gives you one answer.

528
00:36:13,815 --> 00:36:17,715
Speaker 4:  And you're working with a technology that's probabilistic, like who has

529
00:36:17,715 --> 00:36:21,595
Speaker 4:  the upper hand? You know, like I can give 10 answers and if

530
00:36:21,595 --> 00:36:25,395
Speaker 4:  one of them is good, you're happy. But if chat GBT gives you one answer

531
00:36:25,415 --> 00:36:27,715
Speaker 4:  and it's not right, you're frustrated.

532
00:36:27,945 --> 00:36:30,755
Speaker 2:  That makes me think of the thing people always say about the TikTok algorithm,

533
00:36:30,755 --> 00:36:34,115
Speaker 2:  which is that the reason it feels like magic is because you don't get annoyed

534
00:36:34,115 --> 00:36:36,795
Speaker 2:  when it's wrong. Yes. Because you just keep swiping. That's right. And I

535
00:36:36,795 --> 00:36:39,155
Speaker 2:  think social has, like you're saying, very much the same thing. The signal

536
00:36:39,215 --> 00:36:42,675
Speaker 2:  to noise ratio is actually like horrendous on social media,

537
00:36:43,135 --> 00:36:46,915
Speaker 2:  but we're also sort of used to it now. That's right. And If, you just

538
00:36:46,915 --> 00:36:50,595
Speaker 2:  scroll past it and move on. and we all kind of understand how to find needles

539
00:36:50,595 --> 00:36:54,195
Speaker 2:  in haystacks in a way that when Chad GPT recommends a movie, I don't wanna

540
00:36:54,195 --> 00:36:57,235
Speaker 2:  watch it feels bad because it gave me an answer.

541
00:36:57,785 --> 00:37:00,915
Speaker 4:  Exactly. And it's part of the reason why people still go to Google, right?

542
00:37:01,235 --> 00:37:05,115
Speaker 4:  Because Google doesn't have any more accurate stuff these days, you know,

543
00:37:06,095 --> 00:37:09,355
Speaker 4:  you know, compared to what it was because of all the AI that's in there too.

544
00:37:09,815 --> 00:37:13,355
Speaker 4:  So, you know, it's not like Google is any more accurate, but

545
00:37:13,665 --> 00:37:17,315
Speaker 4:  it's interesting because Google gives you this chat, GBT, the

546
00:37:17,315 --> 00:37:20,915
Speaker 4:  answer at the top, right? Yeah. And then you have like all of these

547
00:37:21,145 --> 00:37:25,115
Speaker 4:  various links that give you different perspectives and you know, let's, you

548
00:37:25,115 --> 00:37:28,395
Speaker 4:  know, let's be honest, like most of these links are now run with like so

549
00:37:28,395 --> 00:37:32,195
Speaker 4:  many paywalls and things that like, you can't even get to the answer for

550
00:37:32,195 --> 00:37:35,955
Speaker 4:  any of these things. But the original intent behind Google and why it

551
00:37:35,955 --> 00:37:39,715
Speaker 4:  worked was it gave you options to look through.

552
00:37:40,095 --> 00:37:43,995
Speaker 4:  And so it allowed it to be wrong and it increased its

553
00:37:43,995 --> 00:37:47,795
Speaker 4:  chance of being right at least once or twice. And

554
00:37:47,805 --> 00:37:51,475
Speaker 4:  we're used to using the internet this way. We go through the internet looking

555
00:37:51,535 --> 00:37:54,955
Speaker 4:  for information, trying to find which thing is helpful to us,

556
00:37:55,525 --> 00:37:59,515
Speaker 4:  right? And, and So, I just, I think it's interesting that maybe we

557
00:37:59,515 --> 00:38:01,835
Speaker 4:  got a little distracted by the her movie a little too much.

558
00:38:03,385 --> 00:38:03,995
Speaker 2:  Just a little,

559
00:38:04,745 --> 00:38:07,795
Speaker 4:  Just a little, you know, and we, we kinda leaned in a little harder than

560
00:38:07,795 --> 00:38:11,675
Speaker 4:  we were supposed to. But you know, that that's kind of where

561
00:38:11,755 --> 00:38:15,635
Speaker 4:  I see, I see it being more valuable is can we give

562
00:38:15,635 --> 00:38:19,475
Speaker 4:  people the option to peek behind the curtain of this

563
00:38:19,655 --> 00:38:23,275
Speaker 4:  AI and see what are the various answers

564
00:38:24,255 --> 00:38:27,915
Speaker 4:  it can think of rather than, here's my one

565
00:38:28,155 --> 00:38:32,035
Speaker 4:  response and, and ultimately I think this is gonna be how it works. If, you

566
00:38:32,035 --> 00:38:35,245
Speaker 4:  have multiple language models, If, you have Claude Open AI

567
00:38:35,925 --> 00:38:39,745
Speaker 4:  llama and so on. Are you gonna go from one to the other and be like, which

568
00:38:39,745 --> 00:38:43,505
Speaker 4:  one gave me the best answer? Or are you gonna use an interface

569
00:38:43,505 --> 00:38:47,385
Speaker 4:  that's gonna pull from all of them, generate a bunch of different

570
00:38:47,625 --> 00:38:51,225
Speaker 4:  responses and curate those responses based on the re reactions and

571
00:38:51,505 --> 00:38:55,385
Speaker 4:  responses that you've done in the past and full control for

572
00:38:55,385 --> 00:38:59,265
Speaker 4:  the user to get as many responses as they can. You want

573
00:38:59,265 --> 00:39:02,745
Speaker 4:  to index the language model, you wanna create a Google

574
00:39:03,485 --> 00:39:06,585
Speaker 4:  for the language model outputs. And that's kind of what this is.

575
00:39:08,175 --> 00:39:12,105
Speaker 2:  Yeah, no, I, I, I agree. That's super interesting. So tell me about the roadmap

576
00:39:12,305 --> 00:39:15,025
Speaker 2:  a little bit. Yeah. You know, you mentioned this is an alpha you built in

577
00:39:15,025 --> 00:39:18,385
Speaker 2:  a few weeks. It is becoming increasingly clear to me that you have

578
00:39:19,295 --> 00:39:23,185
Speaker 2:  much bigger visions for where you're headed. Tell me a little bit about

579
00:39:23,185 --> 00:39:23,305
Speaker 2:  it.

580
00:39:24,115 --> 00:39:27,855
Speaker 4:  My vision is to build out the whole funnel of interaction

581
00:39:28,005 --> 00:39:31,855
Speaker 4:  with the language models from the top of the stack all the way down.

582
00:39:32,395 --> 00:39:35,375
Speaker 2:  So people are gonna start yelling at you to build dms and you're gonna build

583
00:39:35,675 --> 00:39:39,375
Speaker 2:  dms and you're, yeah, you're just gonna speed run all the pain of developing

584
00:39:39,415 --> 00:39:40,695
Speaker 2:  a social network. I

585
00:39:40,695 --> 00:39:42,895
Speaker 4:  Love it. All of it all the way down. They're

586
00:39:42,895 --> 00:39:45,055
Speaker 2:  Gonna be like, when do I get lists? And you're gonna have to do lists. And

587
00:39:45,055 --> 00:39:47,015
Speaker 2:  then they're gonna be like, when do we get bookmarks? But you

588
00:39:47,015 --> 00:39:49,935
Speaker 4:  Know, it'll be interesting. The conversation will go down the funnel naturally.

589
00:39:50,255 --> 00:39:52,895
Speaker 4:  I expect the first thing people want, which they already say is they want

590
00:39:53,075 --> 00:39:56,655
Speaker 4:  dms, right? So, okay, you, you introduce dms, right? And it makes sense as

591
00:39:56,655 --> 00:39:59,375
Speaker 4:  you interact with the models and you are more interested in one over another.

592
00:39:59,675 --> 00:40:03,095
Speaker 4:  You wanna talk a little more deeply with that one Cool dms. So

593
00:40:03,095 --> 00:40:05,615
Speaker 2:  Wait, hang on just to, let me, lemme just press on the idea there. 'cause

594
00:40:05,615 --> 00:40:06,215
Speaker 2:  I think that's another,

595
00:40:07,995 --> 00:40:11,215
Speaker 2:  how do you do that without just becoming the chat GPT chat bot again?

596
00:40:11,725 --> 00:40:15,495
Speaker 4:  Well, because what I'm building includes chat

597
00:40:15,615 --> 00:40:19,055
Speaker 4:  GPT within, but I'm building at a level

598
00:40:19,435 --> 00:40:23,335
Speaker 4:  one level up of communication stack, which is how do you

599
00:40:23,735 --> 00:40:27,695
Speaker 4:  discover who you wanna talk to, right? It's like asking Google, well

600
00:40:27,835 --> 00:40:31,775
Speaker 4:  are you're providing all these links to the websites? Why aren't you

601
00:40:31,775 --> 00:40:34,575
Speaker 4:  just another one of those links? It's like, well, because we're providing

602
00:40:34,575 --> 00:40:38,015
Speaker 4:  an index that lets people figure out which one they wanna go into, right?

603
00:40:38,035 --> 00:40:41,935
Speaker 4:  And so social AI provides that layer that helps people discover who they

604
00:40:41,935 --> 00:40:45,575
Speaker 4:  wanna talk to. The end goal is not to talk

605
00:40:45,715 --> 00:40:49,585
Speaker 4:  to an individual ai, the end goal is the whole

606
00:40:49,585 --> 00:40:53,385
Speaker 4:  picture. It's the experience of being

607
00:40:53,385 --> 00:40:57,265
Speaker 4:  able to go up in breadth like a Twitter interface, dive down to

608
00:40:57,345 --> 00:41:01,185
Speaker 4:  a FaceTime call with an AI model, go up to an audio call with another,

609
00:41:01,245 --> 00:41:04,825
Speaker 4:  go up to a chat, go back up to the surface, go back down, go back up

610
00:41:05,335 --> 00:41:08,985
Speaker 4:  that experience, the entire funnel that people go through to find information

611
00:41:08,985 --> 00:41:12,945
Speaker 4:  and communicate online. That whole thing needs to happen, right? And

612
00:41:12,945 --> 00:41:16,825
Speaker 4:  so the vision is we start at the top and work our way down, and

613
00:41:16,825 --> 00:41:18,985
Speaker 4:  then we offer users the ability to go through all of it.

614
00:41:19,695 --> 00:41:23,465
Speaker 2:  Okay? So the idea is then by the time you're at that one-on-one

615
00:41:23,465 --> 00:41:27,305
Speaker 2:  conversation, you've essentially filtered your way down such

616
00:41:27,305 --> 00:41:30,585
Speaker 2:  that you're talking to, instead of going through like the GPT store or picking

617
00:41:30,585 --> 00:41:34,485
Speaker 2:  plugins or whatever. Yeah. Yeah. I've now SocialAI found the

618
00:41:34,485 --> 00:41:38,245
Speaker 2:  conversation I want to have with the right source. And even if now

619
00:41:38,265 --> 00:41:41,645
Speaker 2:  all I'm doing is essentially having a chat bot conversation with a chat bot

620
00:41:41,825 --> 00:41:42,365
Speaker 4:  That's right.

621
00:41:42,465 --> 00:41:45,165
Speaker 2:  It, it's, I've now I've filtered it in the way that is useful.

622
00:41:45,545 --> 00:41:48,725
Speaker 4:  Yes. I think in the future people are gonna ask, they're gonna look at the,

623
00:41:49,025 --> 00:41:52,525
Speaker 4:  the first versions of chat GPT and they're gonna see when chat GPT announced

624
00:41:52,845 --> 00:41:56,765
Speaker 4:  features like customize your GPT and tell it what, how you

625
00:41:56,765 --> 00:41:59,765
Speaker 4:  wanted to talk to you. And they're gonna think, right? I can't believe we

626
00:41:59,785 --> 00:42:01,485
Speaker 4:  had to tell it, you know, like,

627
00:42:03,325 --> 00:42:06,805
Speaker 4:  I can't believe we had to tell it. You know, like what, you know, it doesn't

628
00:42:06,805 --> 00:42:10,445
Speaker 4:  make sense, you know, it doesn't make sense to have to do that. I think that's

629
00:42:10,445 --> 00:42:13,845
Speaker 4:  my bet. I don't, I might be wrong. Look, I don't think the product hit the

630
00:42:13,845 --> 00:42:17,685
Speaker 4:  nail on the head. I think we're, we're definitely not on the, on the center.

631
00:42:17,775 --> 00:42:21,605
Speaker 4:  We're, we're a little bit off. And, and I, I have no doubt in my

632
00:42:21,605 --> 00:42:24,125
Speaker 4:  mind that there's a lot of work that needs to be done to get it in the right

633
00:42:24,125 --> 00:42:27,765
Speaker 4:  place. It's, it's an alpha, right? But I think that's the general

634
00:42:27,765 --> 00:42:31,565
Speaker 4:  direction. And my bet is, you know, telling Chad

635
00:42:31,645 --> 00:42:35,245
Speaker 4:  GBT before you talk to it, everything you wanna, you know, do and how you

636
00:42:35,245 --> 00:42:38,605
Speaker 4:  wanna talk to it in this particular case and why and whatever just feels

637
00:42:38,675 --> 00:42:42,275
Speaker 4:  like we're missing a step, you know?

638
00:42:42,955 --> 00:42:46,915
Speaker 2:  Hmm. Yeah. I buy that as it happens. I, I think Chad

639
00:42:47,465 --> 00:42:51,155
Speaker 2:  GT's whole custom instructions thing is just as ridiculous as you do. So

640
00:42:51,495 --> 00:42:55,395
Speaker 2:  I'm, I'm very much with you on that one. What have you seen

641
00:42:55,395 --> 00:42:58,515
Speaker 2:  so far in terms of how people are using it? Are, are people doing

642
00:42:58,925 --> 00:43:02,035
Speaker 2:  surprising things with it that have sort of changed your mind about what

643
00:43:02,035 --> 00:43:03,955
Speaker 2:  this thing is or can be? You

644
00:43:03,955 --> 00:43:07,835
Speaker 4:  Know, people are favoring a lot of users, which I think

645
00:43:07,835 --> 00:43:11,075
Speaker 4:  is interesting. So you can favorite like any particular follower and then

646
00:43:11,075 --> 00:43:14,955
Speaker 4:  that pins their replies to the top. I I think that's kind of

647
00:43:15,155 --> 00:43:15,355
Speaker 4:  interesting.

648
00:43:15,545 --> 00:43:18,835
Speaker 2:  Well, that seems like it sort of supports your thesis that, that what people

649
00:43:18,865 --> 00:43:22,515
Speaker 2:  want is to find their kind of chosen favorites

650
00:43:22,895 --> 00:43:26,075
Speaker 2:  in here. Right? And it's, it's also interesting that you didn't call it follow,

651
00:43:26,485 --> 00:43:30,475
Speaker 2:  which is a departure from social networks, but also makes more

652
00:43:30,475 --> 00:43:32,595
Speaker 2:  sense in the kind of scheme you're talking about

653
00:43:32,735 --> 00:43:36,475
Speaker 4:  In the context. Yes. I, I check out some of these apps and like, what I've

654
00:43:36,475 --> 00:43:40,355
Speaker 4:  seen is it's like they'll grab the social network, they'll grab the AI

655
00:43:40,375 --> 00:43:44,315
Speaker 4:  and they'll just mash 'em and just, okay, let's see how

656
00:43:44,315 --> 00:43:48,115
Speaker 4:  it goes. Right? It it, you know, like, and, and in this

657
00:43:48,115 --> 00:43:51,775
Speaker 4:  case, I, it's hard, it's much harder to try and like

658
00:43:52,195 --> 00:43:55,655
Speaker 4:  get this right and I don't think I've got it right. I think I'm closer, but,

659
00:43:55,675 --> 00:43:58,815
Speaker 4:  but I don't think I've got it right. But I don't want, what I don't want

660
00:43:58,815 --> 00:44:02,695
Speaker 4:  to do is just mash them together without consideration.

661
00:44:03,215 --> 00:44:07,015
Speaker 4:  Hmm. And so, you know, I didn't say, oh, you can follow a particular

662
00:44:07,275 --> 00:44:11,255
Speaker 4:  AI because in this case you could just favorite it and to your point, right,

663
00:44:11,255 --> 00:44:15,175
Speaker 4:  like it's more in line with what the product is. For

664
00:44:15,175 --> 00:44:19,135
Speaker 4:  example, the app opens up to the composer, it does not open up to your

665
00:44:19,135 --> 00:44:22,855
Speaker 4:  profile. It opens up to the composer now. So you go into it and

666
00:44:22,855 --> 00:44:26,775
Speaker 4:  it's kind of more like trash PT in that sense. The bots don't tell you

667
00:44:26,775 --> 00:44:29,615
Speaker 4:  that they're human, right? They tell you that they're ai, they're there to

668
00:44:29,615 --> 00:44:33,455
Speaker 4:  help you. That model of like design is

669
00:44:33,475 --> 00:44:37,135
Speaker 4:  me kind of saying, I'm gonna pick and choose what I think is

670
00:44:37,195 --> 00:44:41,095
Speaker 4:  useful from a social networks interface and I'm gonna integrate it, but I'm

671
00:44:41,095 --> 00:44:44,895
Speaker 4:  not going to like forget that this is not that.

672
00:44:45,545 --> 00:44:48,495
Speaker 4:  Right. And it, because if I forget that this is not that,

673
00:44:49,425 --> 00:44:53,355
Speaker 4:  then it kind of feels gimmicky, you know? Like it, it kind of feels

674
00:44:53,355 --> 00:44:57,155
Speaker 4:  like it's, it's like, okay, like we get, you know, like, and, and I don't

675
00:44:57,155 --> 00:45:00,995
Speaker 4:  wanna build a gimmicky product that, you know, so, and that's

676
00:45:00,995 --> 00:45:04,635
Speaker 4:  something I've learned too, is like I went full on with the skew

677
00:45:04,635 --> 00:45:08,475
Speaker 4:  morphism, right? I went full on trying to build the product that

678
00:45:08,475 --> 00:45:12,235
Speaker 4:  made people most familiar with it. And as I've been

679
00:45:12,235 --> 00:45:15,795
Speaker 4:  iterating, I've scaled back a lot of the skew morphic design

680
00:45:16,265 --> 00:45:19,875
Speaker 4:  that really, you know, kind of leaned into the social stuff because I realized,

681
00:45:20,015 --> 00:45:23,995
Speaker 4:  you know, okay, users don't need to like see the home feed in the way

682
00:45:23,995 --> 00:45:27,715
Speaker 4:  that they do on Twitter because it, it, it's not that necessary, you know,

683
00:45:28,125 --> 00:45:31,555
Speaker 4:  let's lean into the utility a little more. And I've tweaked it a bit

684
00:45:31,935 --> 00:45:35,875
Speaker 4:  to make it a lot more of the function and leave the,

685
00:45:36,175 --> 00:45:39,875
Speaker 4:  you know, the skew morem of the interface to just the pieces

686
00:45:39,945 --> 00:45:43,475
Speaker 4:  that are required to make it easy to interact with.

687
00:45:44,335 --> 00:45:47,915
Speaker 4:  And so that, that I think will continue, I think the product

688
00:45:48,105 --> 00:45:51,955
Speaker 4:  will continue to distance itself from the

689
00:45:52,105 --> 00:45:55,755
Speaker 4:  skew morphic design that it starts with and turn

690
00:45:55,825 --> 00:45:57,235
Speaker 4:  into its own interface.

691
00:45:57,735 --> 00:46:01,705
Speaker 2:  Do you have a, do you have a clear sense of what that might look like over

692
00:46:01,705 --> 00:46:01,905
Speaker 2:  time?

693
00:46:03,085 --> 00:46:06,425
Speaker 4:  If I, if I told you I did, I would be incredibly

694
00:46:06,435 --> 00:46:10,265
Speaker 4:  delusional. I, I think more so than I am.

695
00:46:10,385 --> 00:46:14,345
Speaker 4:  I, I know I'm, you know, I've got my delusions for sure, but, but you know,

696
00:46:14,345 --> 00:46:18,305
Speaker 4:  more so than that, I would be at a ridiculous level of delusion. I don't

697
00:46:18,495 --> 00:46:21,425
Speaker 4:  know what it's going to end up looking like. I don't know what that final

698
00:46:21,455 --> 00:46:25,345
Speaker 4:  form is, but I have a mission invi in mind,

699
00:46:25,345 --> 00:46:28,385
Speaker 4:  right? And I, and I, and I'm determined and excited about it.

700
00:46:29,165 --> 00:46:32,945
Speaker 4:  And my hope is that as we get

701
00:46:32,945 --> 00:46:36,545
Speaker 4:  more information and data around like how people are using it,

702
00:46:36,815 --> 00:46:40,225
Speaker 4:  that we're able to further go that direction. And, and in my view, I don't

703
00:46:40,225 --> 00:46:43,825
Speaker 4:  think we should, there, there should not be a limit to what

704
00:46:44,075 --> 00:46:48,025
Speaker 4:  angle we approach if the users and the people

705
00:46:48,685 --> 00:46:52,625
Speaker 4:  are asking for that, you know, interaction. There's a lot of people,

706
00:46:52,645 --> 00:46:55,665
Speaker 4:  and I, I think a lot of product companies out there when when they launched

707
00:46:55,665 --> 00:46:59,585
Speaker 4:  something, the thing that they started with that got them

708
00:46:59,585 --> 00:47:03,545
Speaker 4:  the attention they believe is like sacred, right? And

709
00:47:03,545 --> 00:47:07,425
Speaker 4:  so you'll see like reel launches and it's like, oh, our, our feature is

710
00:47:07,525 --> 00:47:10,905
Speaker 4:  the feature Snapchat launches. And they're like, oh, our, you know, our,

711
00:47:11,045 --> 00:47:14,945
Speaker 4:  our ephemeral snapchat is like the feature, right? And,

712
00:47:15,325 --> 00:47:18,065
Speaker 4:  and then it's like, okay, but you can expand and do this and do that and

713
00:47:18,065 --> 00:47:20,905
Speaker 4:  it'd be cool If, you had this. And they're like, no, because this is our

714
00:47:20,905 --> 00:47:24,865
Speaker 4:  feature, you know, and it's like, okay, cool feature. But

715
00:47:24,865 --> 00:47:28,735
Speaker 4:  like a lot of times, and this is something I learned when I used to work

716
00:47:28,735 --> 00:47:32,405
Speaker 4:  at at Facebook, it's like a lot of times the thing, the product

717
00:47:32,665 --> 00:47:36,245
Speaker 4:  or feature that gets you from stage one to stage two

718
00:47:36,865 --> 00:47:40,325
Speaker 4:  is not going to be the same method approach or product feature

719
00:47:40,585 --> 00:47:43,525
Speaker 4:  that's going to help you get from stage two to stage three. And you have

720
00:47:43,525 --> 00:47:46,045
Speaker 4:  to be comfortable with your product evolving over time. And so, so for me

721
00:47:46,045 --> 00:47:49,885
Speaker 4:  it's maximum flexibility on that front. I, I, I want to help

722
00:47:50,215 --> 00:47:53,525
Speaker 4:  solve these problems for people. I don't care what it looks like.

723
00:47:54,385 --> 00:47:57,525
Speaker 2:  Is there a business here, the, the, I feel like

724
00:47:58,145 --> 00:48:01,845
Speaker 2:  you've gone from like social commentary, fun

725
00:48:01,845 --> 00:48:05,645
Speaker 2:  experiment to like kind of hit

726
00:48:05,865 --> 00:48:06,765
Speaker 2:  app to

727
00:48:08,745 --> 00:48:11,565
Speaker 2:  at some point somebody is going to want to give you a bunch of money to make

728
00:48:11,565 --> 00:48:15,205
Speaker 2:  like a social AI for teams and sell it to IT departments.

729
00:48:16,905 --> 00:48:19,845
Speaker 2:  Do you have a sense of what the business looks like here? How are you gonna

730
00:48:19,845 --> 00:48:20,085
Speaker 2:  make money?

731
00:48:20,445 --> 00:48:23,965
Speaker 4:  People have already asked Yes. You, you see that's you,

732
00:48:24,025 --> 00:48:26,925
Speaker 4:  you're, you're right to predict that, you know, people asking that.

733
00:48:27,035 --> 00:48:30,165
Speaker 2:  It's just what happens. They're like, how, who, how do we sell this to IBM?

734
00:48:30,165 --> 00:48:31,085
Speaker 2:  And you're like, okay, take it

735
00:48:31,085 --> 00:48:31,805
Speaker 4:  Easy, easy everybody,

736
00:48:33,945 --> 00:48:37,685
Speaker 4:  you know, look, I, I'm not interested in, in thinking about,

737
00:48:37,745 --> 00:48:40,565
Speaker 4:  you know, how to monetize the product right now. I, I think I'm sure there

738
00:48:40,585 --> 00:48:44,365
Speaker 4:  are ways to like help people find, you know, the right

739
00:48:44,755 --> 00:48:47,365
Speaker 4:  shampoo bottle for them or whatever, but like that's,

740
00:48:48,705 --> 00:48:52,405
Speaker 4:  I'm not, like, that's not my primary thing right now. I, I'm trying to solve

741
00:48:52,405 --> 00:48:56,365
Speaker 4:  a problem I've dealt with in my life, right? Where I've sometimes felt,

742
00:48:56,385 --> 00:49:00,365
Speaker 4:  you know, especially during covid, like it was really hard

743
00:49:01,025 --> 00:49:03,605
Speaker 4:  to get my thoughts out and to

744
00:49:05,135 --> 00:49:08,555
Speaker 4:  be able to just think through things a little bit

745
00:49:08,785 --> 00:49:12,155
Speaker 4:  without, you know, jumping to conclusions. And,

746
00:49:12,915 --> 00:49:16,815
Speaker 4:  and I struggled with that, you know, and I, I, I wanted to solve

747
00:49:16,815 --> 00:49:20,575
Speaker 4:  that for me. I wanted to solve that for my friends. I, you know, I, I wanted

748
00:49:20,575 --> 00:49:24,455
Speaker 4:  to build something that could help. And that's

749
00:49:24,455 --> 00:49:27,975
Speaker 4:  the thing that keeps me going. That's the thing that makes me excited. And

750
00:49:28,715 --> 00:49:32,645
Speaker 4:  the rest of it, you know, we'll figure it out. But my

751
00:49:32,645 --> 00:49:36,525
Speaker 4:  hope is that I'm able to help a little bit, you know, other people out there

752
00:49:36,525 --> 00:49:40,485
Speaker 4:  like me who maybe need a little space to

753
00:49:40,605 --> 00:49:44,565
Speaker 4:  think about things and don't wanna rush to social media

754
00:49:44,785 --> 00:49:46,605
Speaker 4:  and post all their thoughts.

755
00:49:48,085 --> 00:49:52,005
Speaker 2:  I don't have a great read on what success looks like for this. Because it's

756
00:49:52,005 --> 00:49:55,885
Speaker 2:  like what you're saying is sort of, if I, if I post less

757
00:49:56,035 --> 00:49:59,765
Speaker 2:  dumb stuff on, on Twitter because

758
00:49:59,825 --> 00:50:03,605
Speaker 2:  I'm posting dumb stuff on social AI instead, that's a certain kind of victory

759
00:50:04,605 --> 00:50:07,165
Speaker 2:  I think for you and for the world. But also

760
00:50:08,225 --> 00:50:11,955
Speaker 2:  you're in a position where what you're saying is like, you need, people

761
00:50:11,955 --> 00:50:15,755
Speaker 2:  need a place to put this stuff and to talk about this stuff and to get

762
00:50:15,995 --> 00:50:18,555
Speaker 2:  feedback on this stuff and actually doing it in a safer place.

763
00:50:19,025 --> 00:50:22,115
Speaker 4:  Yeah. Like ultimately we want, we wanna talk to people, right? Like, that's

764
00:50:22,115 --> 00:50:24,835
Speaker 4:  our goal, but If, you don't have someone around. Right. I think it's certainly

765
00:50:24,835 --> 00:50:27,565
Speaker 4:  better than going on social media, right? Like that's kind of how I, how

766
00:50:27,565 --> 00:50:30,525
Speaker 4:  I see that. And, and actually to your point that you made earlier, a lot

767
00:50:30,525 --> 00:50:34,325
Speaker 4:  of people have been saying that they think Elon

768
00:50:34,355 --> 00:50:38,325
Speaker 4:  Musk should use social AI before he starts tweeting random stuff,

769
00:50:38,745 --> 00:50:41,845
Speaker 4:  you know, and just a ab test for his own thoughts so that he could stop saying

770
00:50:41,845 --> 00:50:45,605
Speaker 4:  things that like piss people off. I thought that was an

771
00:50:45,805 --> 00:50:46,285
Speaker 4:  interesting point.

772
00:50:46,945 --> 00:50:50,685
Speaker 2:  What's the wildest thing you've seen somebody do in social ai? What is like

773
00:50:50,765 --> 00:50:54,365
Speaker 2:  a either in, in the logs or screenshots or, so what is the one thing

774
00:50:54,555 --> 00:50:57,325
Speaker 2:  that you've seen that you were like, never in a million years? What I've

775
00:50:57,325 --> 00:50:57,525
Speaker 2:  guessed.

776
00:50:57,835 --> 00:51:01,525
Speaker 4:  Okay, so maybe this isn't too wild, but, okay, so my, my mom works at a furniture

777
00:51:01,525 --> 00:51:04,925
Speaker 4:  store in Miami and she, she's from Peru,

778
00:51:05,265 --> 00:51:09,085
Speaker 4:  speaks mostly Spanish and yeah, she never uses my apps, right? Because like,

779
00:51:09,085 --> 00:51:11,605
Speaker 4:  you know, she's nice, supports it, but like she knows that like, you know,

780
00:51:11,805 --> 00:51:15,285
Speaker 4:  whatever. I, I keep trying and they're all, you know, not that great. So

781
00:51:15,685 --> 00:51:19,485
Speaker 4:  I, I was surprised when she messaged me out of random. Okay. And she

782
00:51:19,485 --> 00:51:23,365
Speaker 4:  sends me a screenshot of my app 'cause it works in all these languages. She

783
00:51:23,365 --> 00:51:27,205
Speaker 4:  sends me a screenshot of the app and she starts using it and ranting in Spanish

784
00:51:27,615 --> 00:51:31,365
Speaker 4:  about, like, about work. Wow. She's like ranting about

785
00:51:31,365 --> 00:51:35,125
Speaker 4:  work, you know, on this app. And it's so funny

786
00:51:35,145 --> 00:51:36,045
Speaker 4:  to me. Like I,

787
00:51:37,565 --> 00:51:41,125
Speaker 4:  I was like, okay, well, you know what, like maybe product

788
00:51:41,205 --> 00:51:44,445
Speaker 4:  market fit. I don't know. But you know,

789
00:51:47,085 --> 00:51:50,845
Speaker 4:  I, you know, I, I love her and, and I think it's, I think it's sweet

790
00:51:51,025 --> 00:51:54,925
Speaker 4:  to see that there's, you know, that there's a peel in, in unique

791
00:51:54,925 --> 00:51:58,725
Speaker 4:  ways. I gotta, you know, if I ever go work at some company, maybe I'll start

792
00:51:58,725 --> 00:52:00,365
Speaker 4:  using this rent, you know,

793
00:52:01,825 --> 00:52:03,445
Speaker 2:  And no one will know. And that's the beauty

794
00:52:03,445 --> 00:52:04,245
Speaker 4:  Of it. And there you go.

795
00:52:05,595 --> 00:52:08,005
Speaker 2:  Awesome. All right, well, I should leave you alone, but thank you so much

796
00:52:08,005 --> 00:52:08,925
Speaker 2:  for doing this. This is really fun.

797
00:52:09,425 --> 00:52:10,285
Speaker 4:  No, thank you so much.

798
00:52:11,945 --> 00:52:15,125
Speaker 2:  All right. That is it for the first cast today. Thank you to Michael Sayman

799
00:52:15,125 --> 00:52:18,405
Speaker 2:  again for being here. And thank you as always for listening. There's lots

800
00:52:18,525 --> 00:52:22,005
Speaker 2:  more on everything we talked about, including some of our social AI coverage

801
00:52:22,065 --> 00:52:25,845
Speaker 2:  and just all of our AI coverage in general. There's a ton happening.

802
00:52:26,375 --> 00:52:29,245
Speaker 2:  Check it all out at The Verge dot com. I'll put some links in the show notes.

803
00:52:29,265 --> 00:52:32,605
Speaker 2:  But as always, read the website, it's a good website. And as always, If,

804
00:52:32,605 --> 00:52:36,525
Speaker 2:  you have thoughts, questions, feelings, or other AI bots that I should be

805
00:52:36,645 --> 00:52:40,045
Speaker 2:  hanging out with. You can always email us at Vergecast at The Verge dot com

806
00:52:40,465 --> 00:52:43,885
Speaker 2:  or call the hotline. Eight six six VERGE one one. we love hearing from you.

807
00:52:44,235 --> 00:52:47,925
Speaker 2:  This show is produced by Liam James, will Poor and Eric Gomez. Vergecast

808
00:52:47,925 --> 00:52:51,285
Speaker 2:  is VERGE production and part of the Vox Media podcast network. Neli Alex

809
00:52:51,305 --> 00:52:55,245
Speaker 2:  and I will be back on Tuesday and Friday with all of your regularly scheduled

810
00:52:55,365 --> 00:52:58,365
Speaker 2:  programming. We have a bunch of fun stuff coming up. We'll see you then.

811
00:52:58,635 --> 00:52:59,205
Speaker 2:  Rock and roll.

