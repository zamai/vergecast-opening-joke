1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 60cb42ed-20da-4dba-b0c2-7fbca3779c92
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/9042589769868344844/-6134862417323939636/s93290-US-6015s-1719186448.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and Alex Heath discuss Apple's Vision Pro team reportedly refocusing on a cheaper headset, Meta launching a new "Wearables" organization, a new AI company startup from former OpenAI chief scientist, and a whole lot more tech news.

2
00:00:00,645 --> 00:00:00,765
Speaker 1:  I

3
00:01:14,485 --> 00:01:16,565
Speaker 6:  And welcome to Vergecast the flagship podcast

4
00:01:57,045 --> 00:02:00,245
Speaker 6:  but but happy to be here. David is off this week, but we've got Alex, which

5
00:02:00,245 --> 00:02:03,165
Speaker 6:  is gonna be great. There's a lot of AI news. There's a bunch of VR news including

6
00:02:03,235 --> 00:02:06,685
Speaker 6:  some, some Scoops outta meta in the reality labs division that

7
00:02:07,155 --> 00:02:10,925
Speaker 6:  Alex is here to talk about and have some insight into. But we

8
00:02:11,085 --> 00:02:15,045
Speaker 6:  should start with like the news, which is a weird bit

9
00:02:15,045 --> 00:02:18,245
Speaker 6:  of news in that half of the news got delayed.

10
00:02:20,065 --> 00:02:23,175
Speaker 6:  Weird. Microsoft's co-pilot plus PCs are now shipping.

11
00:02:24,045 --> 00:02:27,255
Speaker 6:  They have Qualcomm Snapdragon X chips inside them.

12
00:02:28,685 --> 00:02:32,255
Speaker 6:  This is the beginning of what might be the end of the Intel

13
00:02:32,255 --> 00:02:35,895
Speaker 6:  dominance, the X 86 dominance for Windows machines.

14
00:02:37,295 --> 00:02:40,595
Speaker 6:  But Microsoft had to delay recall, which was the flagship feature of the

15
00:02:40,595 --> 00:02:44,475
Speaker 6:  copilot plus PC because of horrible security concerns.

16
00:02:44,785 --> 00:02:46,635
Speaker 6:  Just a weird moment. Kranz, right? Like

17
00:02:47,225 --> 00:02:47,715
Speaker 7:  Yeah, this

18
00:02:48,155 --> 00:02:51,315
Speaker 6:  A weird stutter. Even the review units like didn't arrive on time 'cause

19
00:02:51,315 --> 00:02:52,835
Speaker 6:  they were disabling recall. Yeah

20
00:02:52,835 --> 00:02:56,195
Speaker 7:  Like this is wild. We were expecting to get these a week ago to start kind

21
00:02:56,195 --> 00:02:59,485
Speaker 7:  of reviewing them before the embargo list and we got them the day of the

22
00:02:59,645 --> 00:03:03,125
Speaker 7:  embargo. Tom's got both the Elite and

23
00:03:03,505 --> 00:03:07,365
Speaker 7:  the Plus and is checking those out and, and a whole bunch of other people

24
00:03:07,385 --> 00:03:10,805
Speaker 7:  on the team all have them and are just, we are all in this one room just

25
00:03:10,995 --> 00:03:14,965
Speaker 7:  furiously benchmarking, constantly work. It's, they're having a lot of

26
00:03:14,965 --> 00:03:18,525
Speaker 7:  fun doing it. But we've still got a lot of work to do so far. We figured

27
00:03:18,585 --> 00:03:22,525
Speaker 7:  out that some stuff is working, some stuff isn't. There's

28
00:03:22,525 --> 00:03:26,405
Speaker 7:  still a lot of weird like emulation bugs and stuff. I

29
00:03:26,405 --> 00:03:30,365
Speaker 7:  think Adobe Premier was having a weird issue where it would

30
00:03:30,365 --> 00:03:33,965
Speaker 7:  work with the elite processor but not necessarily with the plus

31
00:03:34,115 --> 00:03:38,045
Speaker 7:  processor. Some games crashing. Some of this might be like

32
00:03:38,045 --> 00:03:41,925
Speaker 7:  alleviated with more updates from everybody. But what was supposed

33
00:03:41,925 --> 00:03:45,445
Speaker 7:  to be this big monumental moment for Microsoft and

34
00:03:45,605 --> 00:03:49,125
Speaker 7:  Qualcomm and everybody instead was like, we did it.

35
00:03:49,125 --> 00:03:52,965
Speaker 6:  Yeah, here it is. We turned off half of it. Even in Best Buy, there's like

36
00:03:53,055 --> 00:03:56,205
Speaker 6:  signs that say recall coming later on the demo units

37
00:03:56,795 --> 00:03:59,605
Speaker 6:  because, so if you don't know if you, we weren't listening to the show in

38
00:03:59,605 --> 00:04:03,285
Speaker 6:  the previous weeks. Microsoft announced these PCs and the flagship feature

39
00:04:03,425 --> 00:04:07,405
Speaker 6:  was recall, which even if it was working in a totally secure

40
00:04:07,425 --> 00:04:11,365
Speaker 6:  way is weird. The computer watches everything you do on

41
00:04:11,365 --> 00:04:15,165
Speaker 6:  the computer like Windows watches. Everything you do takes screenshots of

42
00:04:15,165 --> 00:04:18,925
Speaker 6:  it uses a local generative am I model to understand what's in those

43
00:04:18,925 --> 00:04:22,085
Speaker 6:  screenshots. And then You can just be like, what? There was a website I thought

44
00:04:22,245 --> 00:04:25,645
Speaker 6:  I liked. Did you remember what it and the computer will tell you? Or I was

45
00:04:25,645 --> 00:04:27,605
Speaker 6:  working on this document, do you remember what it was? No, the comp You can

46
00:04:27,605 --> 00:04:30,565
Speaker 6:  talk to the computer about what you've done with it, which some people

47
00:04:31,095 --> 00:04:35,085
Speaker 6:  David Pierce very excited about because David would like to be

48
00:04:35,085 --> 00:04:38,085
Speaker 6:  friends with the computer. It's, that's my understanding of First Sound.

49
00:04:38,155 --> 00:04:38,445
Speaker 6:  Just

50
00:04:38,445 --> 00:04:38,685
Speaker 7:  Friends.

51
00:04:39,635 --> 00:04:43,045
Speaker 6:  Just, just friends. David is happily married and has a child. Other people

52
00:04:43,045 --> 00:04:45,325
Speaker 6:  would like to be more than friends with the computer. As we have learned

53
00:04:45,675 --> 00:04:49,525
Speaker 6:  time and time again as the AI industry has dominated the

54
00:04:49,605 --> 00:04:49,845
Speaker 6:  headlines,

55
00:04:51,525 --> 00:04:54,725
Speaker 6:  Microsoft was storing the screenshots in basically plain text.

56
00:04:55,665 --> 00:04:59,485
Speaker 6:  So there's this like open database of everything you've done on your computer,

57
00:04:59,485 --> 00:05:03,245
Speaker 6:  on your computer, huge security hole. Microsoft has had a history of security

58
00:05:03,245 --> 00:05:07,165
Speaker 6:  problems. Recently Nadella sent out a memo to everyone being like, if you

59
00:05:07,165 --> 00:05:10,805
Speaker 6:  have to choose between shipping fast and security, I want you to pick

60
00:05:11,045 --> 00:05:14,045
Speaker 6:  security. And then they announced recall and everyone's like, this is a huge

61
00:05:14,285 --> 00:05:17,045
Speaker 6:  security fault and they just delayed it. So they thought they could fix it

62
00:05:17,045 --> 00:05:19,525
Speaker 6:  in time. They issued a blog post by fixing it in time and then they'd realize

63
00:05:19,525 --> 00:05:20,685
Speaker 6:  they couldn't and they delayed it.

64
00:05:22,455 --> 00:05:25,205
Speaker 6:  Kranz, does that feel like, I feel like that's only half of the story here,

65
00:05:25,205 --> 00:05:28,445
Speaker 6:  right? That's just the background. That's part of the stutter step and

66
00:05:28,915 --> 00:05:32,325
Speaker 6:  part of why it's not as flashy a launch like recall is a great

67
00:05:32,555 --> 00:05:36,405
Speaker 6:  feature for what I think of as like the local news, right? Yeah. This is

68
00:05:36,445 --> 00:05:39,725
Speaker 6:  a great demo for the local news and that is how you get a lot of mainstream

69
00:05:39,725 --> 00:05:43,525
Speaker 6:  attention on new things. But the real thing here

70
00:05:44,225 --> 00:05:47,885
Speaker 6:  on all of these tracking and PCs is performance in battery life where they

71
00:05:47,885 --> 00:05:51,485
Speaker 6:  have not been competitive with M Series chips on MacBooks for a long time

72
00:05:51,485 --> 00:05:52,325
Speaker 6:  and now they think they are.

73
00:05:52,635 --> 00:05:56,365
Speaker 7:  Yeah. Yeah. That is absolutely the big, the big moment here. And like the

74
00:05:56,365 --> 00:05:59,685
Speaker 7:  other part was gonna be really cool and that was gonna be what got people

75
00:05:59,685 --> 00:06:03,445
Speaker 7:  really excited to, to maybe upgrade. But for the rest of us it's

76
00:06:03,445 --> 00:06:06,685
Speaker 7:  okay. Is this finally the moment where Arm on Windows

77
00:06:07,335 --> 00:06:11,285
Speaker 7:  works? Because we've seen it so many times before and every time

78
00:06:11,425 --> 00:06:15,285
Speaker 7:  it has fallen super short of the mark and

79
00:06:15,285 --> 00:06:19,275
Speaker 7:  this time like from the early benchmarks, battery life

80
00:06:19,275 --> 00:06:22,835
Speaker 7:  looking pretty good battery's in that like that, that,

81
00:06:22,985 --> 00:06:26,795
Speaker 7:  that MIP range we're, we're still testing

82
00:06:26,895 --> 00:06:29,795
Speaker 7:  so I don't wanna say which M chip, but it's in that like MIP range.

83
00:06:30,895 --> 00:06:33,755
Speaker 7:  The the speed is looking pretty good. Gaming is like

84
00:06:34,915 --> 00:06:38,875
Speaker 7:  possible. We've still got some work to do there to figure it out,

85
00:06:38,935 --> 00:06:42,035
Speaker 7:  but the, it's, it's a surprise. It definitely doesn't seem like this is gonna

86
00:06:42,035 --> 00:06:45,395
Speaker 7:  be a moment where just Qualcomm blows everybody away. Kind of like how Apple

87
00:06:45,455 --> 00:06:48,995
Speaker 7:  did with the M1 chip where we were like, holy crap, apple actually did it

88
00:06:49,425 --> 00:06:53,315
Speaker 7:  doesn't seem like we're gonna have that moment, but it's

89
00:06:53,315 --> 00:06:54,595
Speaker 7:  still kind of too early to tell.

90
00:06:54,825 --> 00:06:57,595
Speaker 6:  Well it's interesting 'cause Apple hasn't even had the incremental moments

91
00:06:57,805 --> 00:07:01,675
Speaker 6:  after the M1. Yeah, like the M two to M1, it's

92
00:07:01,675 --> 00:07:05,435
Speaker 6:  faster, but it wasn't the big step from Intel to arm,

93
00:07:06,205 --> 00:07:10,075
Speaker 6:  right? M three to M two was faster, but it wasn't the same kind of revelatory

94
00:07:10,075 --> 00:07:13,355
Speaker 6:  experience. So Qualcomm's coming in to just a very different set of

95
00:07:13,715 --> 00:07:17,555
Speaker 6:  expectations. Yeah. Right. Like if You can match the M1 you've done

96
00:07:17,555 --> 00:07:21,475
Speaker 6:  great, that's a big step forward over Intel. If You can beat the M three

97
00:07:21,575 --> 00:07:25,545
Speaker 6:  or the M two, you're, it's the same incremental

98
00:07:25,785 --> 00:07:27,105
Speaker 6:  progress as everyone else has made. Yeah,

99
00:07:27,425 --> 00:07:30,505
Speaker 7:  I think that's kind of normal. We sometimes expect everybody to just blow

100
00:07:30,505 --> 00:07:33,705
Speaker 7:  each other out of the water, but these like Windows is

101
00:07:34,685 --> 00:07:38,145
Speaker 7:  hugely complicated to run. It is not an easy thing to run.

102
00:07:38,565 --> 00:07:42,505
Speaker 7:  It has, there's so many like dependencies code for

103
00:07:42,645 --> 00:07:46,105
Speaker 7:  old code that it has to work through and stuff. And then a lot of times

104
00:07:46,365 --> 00:07:49,945
Speaker 7:  that's, that's stuff isn't built for Arm and so they're having to do all

105
00:07:49,945 --> 00:07:53,745
Speaker 7:  this emulation more so than like an app a Mac would because

106
00:07:53,865 --> 00:07:57,785
Speaker 7:  a Mac just, it's all built from Apple and so not all

107
00:07:57,785 --> 00:08:01,585
Speaker 7:  of it, but most of it's built by Apple for Apple and it can kind of run and

108
00:08:01,585 --> 00:08:05,265
Speaker 7:  knows what to do. That big like translation moment

109
00:08:05,325 --> 00:08:08,345
Speaker 7:  hasn't quite happened for, for Windows machines.

110
00:08:08,345 --> 00:08:09,225
Speaker 6:  They've been trying for a long time.

111
00:08:09,415 --> 00:08:12,225
Speaker 7:  They've been trying and nobody's wanted to do it because it's like, well

112
00:08:12,245 --> 00:08:16,185
Speaker 7:  you don't make, nobody makes good arm chips, so why should I build my, my

113
00:08:16,185 --> 00:08:19,905
Speaker 7:  programs for Arm? And now it's like this could change that

114
00:08:20,015 --> 00:08:23,745
Speaker 7:  this, this could change that. But it is so, so,

115
00:08:23,845 --> 00:08:25,665
Speaker 7:  so early to tell. Yeah,

116
00:08:25,865 --> 00:08:29,585
Speaker 8:  I get the chip story underneath this, but does Microsoft

117
00:08:29,985 --> 00:08:33,785
Speaker 8:  actually think that anyone is gonna switch from Mac to Windows because

118
00:08:33,785 --> 00:08:37,145
Speaker 8:  of this AI stuff? Yeah, because I look at this and it's like, you basically

119
00:08:37,145 --> 00:08:40,825
Speaker 8:  need to tell me that I'd never have to actually understand or know how to

120
00:08:40,825 --> 00:08:44,105
Speaker 8:  use Windows to actually switch to Windows. That's how I feel about Windows.

121
00:08:44,705 --> 00:08:48,465
Speaker 8:  Like an AI needs to, it needs to be so dramatically different from what I

122
00:08:48,465 --> 00:08:52,105
Speaker 8:  expect from a laptop as a lifelong Mac apple

123
00:08:52,215 --> 00:08:55,445
Speaker 8:  lock-in blue bubble guy and like

124
00:08:55,995 --> 00:08:59,845
Speaker 8:  this recall thing sounds cool but it's like I you, you would literally

125
00:08:59,845 --> 00:09:03,285
Speaker 8:  have to say AI is running your laptop for you for me to consider switching.

126
00:09:03,285 --> 00:09:04,685
Speaker 8:  Yeah. 'cause of the software lock in,

127
00:09:04,955 --> 00:09:05,245
Speaker 6:  This

128
00:09:05,245 --> 00:09:08,165
Speaker 7:  Is a pressure dressing, right? Like this is just for them to be like, no,

129
00:09:08,165 --> 00:09:11,965
Speaker 7:  we can't lose anymore Windows people to Chrome or or

130
00:09:12,025 --> 00:09:15,725
Speaker 7:  Max. Like we have to keep 'em here with cool features like that.

131
00:09:15,725 --> 00:09:19,565
Speaker 7:  That's pretty, it's not like let's entice these people back. It's please

132
00:09:19,565 --> 00:09:20,005
Speaker 7:  don't leave.

133
00:09:20,435 --> 00:09:23,965
Speaker 6:  Yeah. Help. Well no actually I wonder about that. You know, I,

134
00:09:24,585 --> 00:09:24,805
Speaker 6:  or

135
00:09:24,805 --> 00:09:25,645
Speaker 7:  At least that's how I feel about it.

136
00:09:25,795 --> 00:09:29,565
Speaker 6:  I've heard a lot of CEOs, Alex I'm sure you have to describe

137
00:09:29,785 --> 00:09:33,685
Speaker 6:  AI as a platform shift and I'm always

138
00:09:33,685 --> 00:09:37,605
Speaker 6:  like from what to where, like what platform are you talking about

139
00:09:37,605 --> 00:09:41,445
Speaker 6:  and what they want, what what they're trying to invoke is mobile, right?

140
00:09:41,445 --> 00:09:44,725
Speaker 6:  This is a platform shift on the order of mobile. We went from laptops to

141
00:09:44,725 --> 00:09:48,205
Speaker 6:  smartphones and that smartphones dominate computing and what they, what they're

142
00:09:48,205 --> 00:09:51,805
Speaker 6:  trying to say is this is another platform shift on that scale.

143
00:09:52,425 --> 00:09:53,805
Speaker 6:  And it's like, is it like

144
00:09:55,375 --> 00:09:59,285
Speaker 6:  where like you still have to have a phone to use the

145
00:09:59,285 --> 00:10:03,125
Speaker 6:  chat bot. Like is it that And I think part of this window stuff for

146
00:10:03,405 --> 00:10:07,045
Speaker 6:  Microsoft is very much, okay we own this entire stack. We have this giant

147
00:10:07,045 --> 00:10:10,445
Speaker 6:  investment in OpenAI, we've built copilot, we have all these ideas about

148
00:10:10,445 --> 00:10:14,245
Speaker 6:  what a computer can be here. Like here's a new way to

149
00:10:14,245 --> 00:10:18,205
Speaker 6:  use a computer that is meaningfully different than before. Also

150
00:10:18,205 --> 00:10:21,365
Speaker 6:  you have to pre-install edge or whatever. Like we also, we'll push you towards

151
00:10:21,365 --> 00:10:24,885
Speaker 6:  our browser. I don't know that Apple has different ideas. We, we, we talked

152
00:10:24,885 --> 00:10:28,685
Speaker 6:  about it in the show after wwc, like Apple's ideas for what AI can do in

153
00:10:28,685 --> 00:10:32,445
Speaker 6:  its operating system are basically the same as Microsoft's ideas. So the

154
00:10:32,445 --> 00:10:36,365
Speaker 6:  idea that it's a platform shift I think is under some pressure. But

155
00:10:36,405 --> 00:10:39,805
Speaker 6:  I know that's how these companies think about it. That this will entice you

156
00:10:39,805 --> 00:10:40,645
Speaker 6:  to switch platforms

157
00:10:40,745 --> 00:10:43,285
Speaker 8:  But it's just features right now. It's like I still have to look at the Windows

158
00:10:43,545 --> 00:10:46,765
Speaker 8:  nav bar and figure that thing out, right? And it's like I'm just not gonna

159
00:10:46,765 --> 00:10:50,445
Speaker 8:  do that. I'm sorry Tom Warren. Like I'm just not, so yes,

160
00:10:50,685 --> 00:10:54,365
Speaker 8:  I think they think of it that way. But it shows, I think this

161
00:10:54,385 --> 00:10:58,285
Speaker 8:  recall thing shows where the real platforms are and it's like

162
00:10:58,605 --> 00:11:01,765
Speaker 8:  building two decades of software lock-in on top of

163
00:11:02,485 --> 00:11:06,405
Speaker 8:  hardware, which is not ai, right? AI is software that

164
00:11:06,405 --> 00:11:09,085
Speaker 8:  you're putting on existing hardware at least right now.

165
00:11:09,475 --> 00:11:12,045
Speaker 6:  Yeah. And I'm, I'm curious, you know, people are listening if you have an

166
00:11:12,045 --> 00:11:14,845
Speaker 6:  idea of where this platform shift is, I'm dying to know what people think,

167
00:11:14,905 --> 00:11:18,845
Speaker 6:  but it is what it is. What Nadella has told me, it's what Sunar Pache has

168
00:11:18,845 --> 00:11:22,805
Speaker 6:  told me. Like down the line the CEOs of the biggest

169
00:11:23,125 --> 00:11:26,245
Speaker 6:  companies are like, this is a platform shift we cannot get left behind.

170
00:11:27,035 --> 00:11:30,405
Speaker 6:  Like in particular with Microsoft, there's like a fear

171
00:11:30,795 --> 00:11:33,805
Speaker 6:  that they got left behind in mobile and they can't get left behind this time.

172
00:11:34,225 --> 00:11:38,005
Speaker 6:  And so you see them kind of, I mean they have been ahead of the curve at

173
00:11:38,005 --> 00:11:38,885
Speaker 6:  every single step. You

174
00:11:38,885 --> 00:11:41,645
Speaker 8:  Didn't hear Elon say it's gonna be trillions of dollars worth of Optimus

175
00:11:41,665 --> 00:11:43,445
Speaker 8:  robots. That's, that's the platform shift.

176
00:11:45,645 --> 00:11:48,965
Speaker 6:  I mean that's the true platform shift is kicking back with a brewski and

177
00:11:48,965 --> 00:11:52,045
Speaker 6:  letting optimist do all your computing for you. Yeah, it'd be great if AI

178
00:11:52,045 --> 00:11:55,965
Speaker 6:  express itself not as AI integrated into the operating system, but you have

179
00:11:55,965 --> 00:11:59,285
Speaker 6:  an optimist robot that knows how to use Windows and that

180
00:11:59,545 --> 00:12:00,765
Speaker 6:  that's actually what happens. I love that.

181
00:12:01,075 --> 00:12:03,605
Speaker 7:  That would be Alex Heath. Yeah. And then like Tom Warren would get the one

182
00:12:03,605 --> 00:12:04,565
Speaker 7:  that knows how to use Mac.

183
00:12:05,275 --> 00:12:09,205
Speaker 8:  Yeah. And Eli, you joke about how, you know, use would love to not have to

184
00:12:09,205 --> 00:12:12,885
Speaker 8:  use software at work. You know, you just have an optimist use your software

185
00:12:12,885 --> 00:12:13,165
Speaker 8:  for you.

186
00:12:14,035 --> 00:12:17,685
Speaker 6:  Yeah, I need an op, I need a full fully complete optimist

187
00:12:17,695 --> 00:12:21,605
Speaker 6:  robot that can use Okta And. if you can just get me there. I'll do it.

188
00:12:21,785 --> 00:12:24,765
Speaker 6:  If You can fi if Optimist can file my expenses, oh my god,

189
00:12:26,775 --> 00:12:27,125
Speaker 6:  those

190
00:12:27,175 --> 00:12:28,165
Speaker 8:  Codes, they suck.

191
00:12:29,425 --> 00:12:33,125
Speaker 6:  The CEOs of these companies will not come on decoder where I will just ask

192
00:12:33,125 --> 00:12:34,285
Speaker 6:  them to look at their own software.

193
00:12:36,295 --> 00:12:40,045
Speaker 6:  We'll see. We are gonna test out. I'll say that I, there, there, I have a

194
00:12:40,045 --> 00:12:43,045
Speaker 6:  running list of my favorite Slack rooms at the company and it's the ones

195
00:12:43,045 --> 00:12:46,525
Speaker 6:  where people have developed their own language. So my number one favorite

196
00:12:46,745 --> 00:12:50,485
Speaker 6:  is we have a support room for our finance team where people just file tickets.

197
00:12:50,825 --> 00:12:54,645
Speaker 6:  And some people have discovered and well some people come in there and they

198
00:12:54,645 --> 00:12:57,285
Speaker 6:  ask very nice questions like, hello, I need help with da da da. And some

199
00:12:57,285 --> 00:12:59,885
Speaker 6:  people have discovered that this room is staffed by a bot that just files

200
00:12:59,885 --> 00:13:03,805
Speaker 6:  tickets and they just yell nouns. They're just like invoice

201
00:13:03,805 --> 00:13:06,645
Speaker 6:  2 45. Like that's all they, they're just, and the other people are very,

202
00:13:06,745 --> 00:13:09,605
Speaker 6:  and it's the greatest room in the entire company 'cause people have realized

203
00:13:09,605 --> 00:13:12,965
Speaker 6:  they can just yell at a robot. The other one is The Verge benchmarks room

204
00:13:13,335 --> 00:13:17,125
Speaker 6:  where it's just people communicating and increasingly arcane benchmarks.

205
00:13:17,305 --> 00:13:21,085
Speaker 6:  Yep. And they've developed their own language about windows on arm

206
00:13:21,085 --> 00:13:24,725
Speaker 6:  benchmarks and they're not, they're not using the normal rule.

207
00:13:24,835 --> 00:13:28,605
Speaker 6:  Like no LLM can figure out what's going on in this room. Statistically this

208
00:13:28,805 --> 00:13:32,645
Speaker 6:  language has never existed before. It's very good. I'm excited

209
00:13:32,665 --> 00:13:35,765
Speaker 6:  for it. They're gonna come, we're gonna have all the reviews. I think David

210
00:13:35,945 --> 00:13:39,565
Speaker 6:  is gonna do a full show with Tom and Nathan, our reviews editor

211
00:13:39,915 --> 00:13:42,965
Speaker 6:  just running down all this stuff on, on Tuesday I believe, right Alex? Yeah.

212
00:13:43,355 --> 00:13:46,165
Speaker 6:  Okay, so we'll, we'll we'll come back around with that stuff. I'm,

213
00:13:47,565 --> 00:13:51,045
Speaker 6:  Qualcomm has made a lot of promises here, right? They they, they bought that

214
00:13:51,045 --> 00:13:55,005
Speaker 6:  company Nuvia, which was a bunch of X apple engineers on chip

215
00:13:55,005 --> 00:13:58,965
Speaker 6:  team who were making server chips and they said no make

216
00:13:58,965 --> 00:14:01,845
Speaker 6:  a consumer chip and set 'cause Apple didn't want to use its arm architecture

217
00:14:02,185 --> 00:14:05,085
Speaker 6:  to make server chips. And they thought there was this big opportunity there

218
00:14:05,305 --> 00:14:08,405
Speaker 6:  but then Qualcomm bought them and said no, make the next generation of Snapdragon.

219
00:14:08,585 --> 00:14:11,285
Speaker 6:  So I shifted them, I dunno if you know there's some backstory here.

220
00:14:12,425 --> 00:14:16,405
Speaker 6:  Arm decided that Qualcomm's license didn't cover this. And so

221
00:14:16,425 --> 00:14:20,165
Speaker 6:  ARM is suing Qualcomm right now and Qualcomm's suing arm back

222
00:14:20,675 --> 00:14:23,885
Speaker 6:  over these chips, which is bananas.

223
00:14:25,195 --> 00:14:27,845
Speaker 6:  It's gonna come to some resolution. There's no way this actually like ends

224
00:14:27,845 --> 00:14:30,805
Speaker 6:  up in anything but payments. But these chips are like

225
00:14:31,635 --> 00:14:35,565
Speaker 6:  very, very controversial in their way 'cause it's a bunch of ex apple designers

226
00:14:35,565 --> 00:14:39,325
Speaker 6:  who left and started a company and Qualcomm snap them up to go compete with

227
00:14:39,325 --> 00:14:43,295
Speaker 6:  Apple's consumer chips. Can they deliver is a big

228
00:14:43,415 --> 00:14:46,855
Speaker 6:  question. I'm excited to see our reviews and and see if that's true. Same

229
00:14:47,085 --> 00:14:49,575
Speaker 6:  also this lawsuit is just the funniest lawsuit in the world. It's a good

230
00:14:49,575 --> 00:14:52,685
Speaker 6:  lawsuit. Like if you're armed you're like screw it's Sue Qualcomm.

231
00:14:53,485 --> 00:14:57,045
Speaker 7:  I feel like Qual Qu Kong gets sued a lot. Thank you. Like every time you

232
00:14:57,045 --> 00:15:00,245
Speaker 7:  turn around somebody's like, no, I hate you Broadcom's. Like I'm suing you

233
00:15:00,255 --> 00:15:02,245
Speaker 7:  Apple like I'm suing you mean? Yeah. Yeah.

234
00:15:02,555 --> 00:15:06,045
Speaker 6:  This company is under fire and by the way we talk a lot about 5G on the show.

235
00:15:06,505 --> 00:15:10,485
Speaker 6:  The, the level of interest in six G is at

236
00:15:10,485 --> 00:15:14,045
Speaker 6:  least partially driven by people wanting to get away from Qualcomm's

237
00:15:14,115 --> 00:15:18,045
Speaker 6:  5G patents. Like what if, what if there wasn't

238
00:15:18,045 --> 00:15:21,605
Speaker 6:  a dominant player in wireless that we could get away from?

239
00:15:22,025 --> 00:15:25,605
Speaker 6:  And so you, there's already talk about six G just to get away from Qualcomm.

240
00:15:26,235 --> 00:15:29,925
Speaker 6:  Like Apple bought all of Intel's modem division and they still haven't produced

241
00:15:29,925 --> 00:15:33,685
Speaker 6:  a radio that can compete with Qualcomm and its patents. Get ready for the

242
00:15:33,685 --> 00:15:37,565
Speaker 6:  six G hype cycle. That's I'm, oh God is people love suing this company and

243
00:15:37,565 --> 00:15:40,845
Speaker 6:  that's gonna drive the six G hype cycle. Some other stuff we should talk

244
00:15:40,845 --> 00:15:44,365
Speaker 6:  about. This is kind of like a like a gadget rapid fire round.

245
00:15:44,635 --> 00:15:48,565
Speaker 6:  Yeah. Is that what we wanna call it? It's not quite a linear around. We've

246
00:15:48,565 --> 00:15:52,205
Speaker 6:  learned some new stuff outta WWC about various Apple products.

247
00:15:52,785 --> 00:15:56,605
Speaker 6:  Jen had a pretty good scoop. Apple is extending home

248
00:15:56,665 --> 00:16:00,125
Speaker 6:  kit to use the ultra wideband ship in its phones to

249
00:16:00,125 --> 00:16:03,205
Speaker 6:  automatically unlock doors, which is cool. So

250
00:16:04,005 --> 00:16:07,445
Speaker 6:  a combination of ultra wideband and Bluetooth low energy. It can

251
00:16:07,775 --> 00:16:11,205
Speaker 6:  sense when you are six feet away from your door on the outside and walking

252
00:16:11,205 --> 00:16:13,725
Speaker 6:  towards it and automatically unlock the door, which

253
00:16:13,725 --> 00:16:14,285
Speaker 7:  Is just sick

254
00:16:15,865 --> 00:16:16,085
Speaker 6:  If

255
00:16:16,085 --> 00:16:16,245
Speaker 8:  It

256
00:16:16,245 --> 00:16:16,525
Speaker 7:  Worked.

257
00:16:18,225 --> 00:16:21,205
Speaker 6:  But if you have a smart door lock, you need a new door lock to enable this.

258
00:16:21,205 --> 00:16:21,725
Speaker 6:  Yeah. None

259
00:16:21,725 --> 00:16:23,165
Speaker 7:  Of 'em work with it. This is,

260
00:16:23,475 --> 00:16:27,405
Speaker 6:  It's a lot. Alex is he making face? I got two Alex's. Alex this is last

261
00:16:27,405 --> 00:16:27,645
Speaker 6:  names only.

262
00:16:27,645 --> 00:16:31,525
Speaker 8:  Yeah, last names only. This reminds me of the digital id, the driver's

263
00:16:31,525 --> 00:16:35,445
Speaker 8:  license thing. Like it just seems like a bad idea. Like I get

264
00:16:35,445 --> 00:16:39,205
Speaker 8:  that it sounds cool like you're in a product meeting and Cupertino and it's

265
00:16:39,205 --> 00:16:42,925
Speaker 8:  like, oh we should digitize this but I don't really want my

266
00:16:43,265 --> 00:16:45,365
Speaker 8:  device automatically unlocking my house.

267
00:16:46,835 --> 00:16:50,685
Speaker 6:  Yeah, yeah, yeah. You're well so the argument I felt

268
00:16:50,685 --> 00:16:51,405
Speaker 6:  this way about

269
00:16:53,065 --> 00:16:56,045
Speaker 6:  the feature where You can just point your phone at a door lock and it unlocks

270
00:16:56,635 --> 00:16:59,765
Speaker 6:  like You can in express mode. But same way that if you use mass transit,

271
00:16:59,765 --> 00:17:02,685
Speaker 6:  You can just like beep your phone at a trend style on the subway and it just

272
00:17:02,685 --> 00:17:05,725
Speaker 6:  pays and like whatever. And I was like, I don't know, I, I definitely wanna

273
00:17:05,725 --> 00:17:09,245
Speaker 6:  authenticate before. And I was like, no, if you steal my cow key,

274
00:17:09,825 --> 00:17:13,285
Speaker 6:  you're in my house. Like the level of security is the same. And I think one

275
00:17:13,755 --> 00:17:17,125
Speaker 6:  good question is like whether that is an acceptable level of security,

276
00:17:18,595 --> 00:17:22,285
Speaker 6:  like it is I guess is the answer. Like if you have my phone,

277
00:17:22,385 --> 00:17:24,485
Speaker 6:  can you, should you be able to get in my house? It's like a good question

278
00:17:24,685 --> 00:17:26,725
Speaker 6:  everyone should ask and then You can adjust that setting however you want.

279
00:17:27,745 --> 00:17:30,285
Speaker 6:  If you have my phone, you're six feet away from my house and you're walking

280
00:17:30,285 --> 00:17:33,645
Speaker 6:  towards it, should the house automatically walk? It's, it's like earned it.

281
00:17:33,685 --> 00:17:36,325
Speaker 6:  A different question like that. That's a new, I

282
00:17:36,325 --> 00:17:40,205
Speaker 8:  Would like this feature if you could set it to where if face IID

283
00:17:40,445 --> 00:17:44,365
Speaker 8:  hadn't been used in say an hour or two hours, then it doesn't work.

284
00:17:44,365 --> 00:17:48,085
Speaker 8:  Yeah. But something like that, something where like the recency of me unlocking

285
00:17:48,085 --> 00:17:50,685
Speaker 8:  the device means that it's definitely in my hand. You know?

286
00:17:51,035 --> 00:17:54,685
Speaker 6:  Yeah. All right. Or you, you, your phone has been stolen and another iPhone

287
00:17:54,755 --> 00:17:58,245
Speaker 6:  detects you shrieking into which void and your, your house locks down

288
00:17:58,915 --> 00:18:02,005
Speaker 6:  like, like the bizarre use of the find My network.

289
00:18:03,475 --> 00:18:07,245
Speaker 6:  It's interesting 'cause Apple has put a lot of chips and a lot of radios

290
00:18:07,245 --> 00:18:11,205
Speaker 6:  and a lot of things recently and hasn't really done

291
00:18:11,205 --> 00:18:14,725
Speaker 6:  a lot with them. So these ultra wideband chips have been in a lot of Apple

292
00:18:14,725 --> 00:18:18,685
Speaker 6:  devices for quite a while now. And some of the uses are

293
00:18:18,755 --> 00:18:22,725
Speaker 6:  cool, right? Like find my is a little bit more precise. If you have

294
00:18:22,725 --> 00:18:26,645
Speaker 6:  the right selection of ultra wideband devices, You can

295
00:18:26,645 --> 00:18:30,565
Speaker 6:  like point your phone at a home pod and send the music to it. Which is, you

296
00:18:30,565 --> 00:18:31,565
Speaker 6:  know, supposedly like magic.

297
00:18:32,105 --> 00:18:33,845
Speaker 7:  It it, it works half the time.

298
00:18:34,745 --> 00:18:38,485
Speaker 6:  So I said supposedly like magic, like if you were a shit magician.

299
00:18:38,755 --> 00:18:40,485
Speaker 6:  Yeah it works about that.

300
00:18:40,505 --> 00:18:43,285
Speaker 7:  If I, if I did magic, that would be my magic trick. Like look

301
00:18:43,285 --> 00:18:45,605
Speaker 6:  Yeah, you know what, you know those people are like I'm floating and they're

302
00:18:45,605 --> 00:18:48,725
Speaker 6:  really just like standing on their tiptoes. That's me. That's what I'm saying.

303
00:18:48,985 --> 00:18:51,245
Speaker 6:  But if you're always doing that, it's sort of the wrong angle and someone's

304
00:18:51,245 --> 00:18:53,765
Speaker 6:  like you're just standing on your tiptoes. That's about how well the home

305
00:18:53,785 --> 00:18:54,285
Speaker 6:  pod thing works.

306
00:18:56,635 --> 00:18:59,205
Speaker 6:  Then they added thread radius gen, that was the other gen scoop from a couple

307
00:18:59,205 --> 00:19:02,045
Speaker 6:  weeks ago. Like now they're adding other radios to these devices.

308
00:19:03,705 --> 00:19:07,485
Speaker 6:  You see that they're like, okay wifi and Bluetooth can't do

309
00:19:07,625 --> 00:19:11,525
Speaker 6:  all the things we want to do. They can't do it at the power level,

310
00:19:11,665 --> 00:19:15,525
Speaker 6:  at the location, precision level, whatever. It's

311
00:19:15,525 --> 00:19:18,605
Speaker 6:  that just can't do it. So we need to add more radius to this thing.

312
00:19:19,345 --> 00:19:22,645
Speaker 6:  And then you see that animations for the new Siri

313
00:19:23,145 --> 00:19:25,965
Speaker 6:  and you're like, oh they are super getting ready to take all the ports and

314
00:19:25,965 --> 00:19:28,245
Speaker 6:  buttons. These Oh yeah, stop

315
00:19:28,705 --> 00:19:29,365
Speaker 7:  You. You willing?

316
00:19:30,355 --> 00:19:34,005
Speaker 6:  There's like, there's 45 different radios for every level of like

317
00:19:34,435 --> 00:19:38,405
Speaker 6:  precision location and power that You can think of. None of

318
00:19:38,405 --> 00:19:42,165
Speaker 6:  them aren't able to do much and then the buttons are gonna be capacitive

319
00:19:42,195 --> 00:19:43,245
Speaker 6:  very soon. The

320
00:19:43,245 --> 00:19:46,605
Speaker 8:  Arc of phones is that they just become panes of glass

321
00:19:46,995 --> 00:19:50,885
Speaker 8:  that then start to curve that we then put in front of our eyes like

322
00:19:50,885 --> 00:19:53,165
Speaker 8:  that is, that is the next 10 years of technology.

323
00:19:55,525 --> 00:19:58,605
Speaker 6:  I wanna come back, I wanna come back to that. Lemme talk about that. And

324
00:19:58,605 --> 00:20:02,005
Speaker 6:  the vision pro to some extent. So that's the UWB chip.

325
00:20:02,575 --> 00:20:05,805
Speaker 6:  We're seeing Apple start to use it in more interesting ways, especially around

326
00:20:05,805 --> 00:20:09,405
Speaker 6:  the home. I'm very curious if they ever acknowledge these thread radios.

327
00:20:09,705 --> 00:20:13,605
Speaker 6:  We were kind of hoping they would at wwc. But it's interesting that

328
00:20:13,605 --> 00:20:16,085
Speaker 6:  there's a bunch of thread radios in these devices you could do all kinds

329
00:20:16,085 --> 00:20:19,885
Speaker 6:  of things with in a smart home. 'cause it used basically no power

330
00:20:20,285 --> 00:20:22,805
Speaker 6:  compared to everything else. And then the other thing we found out, which

331
00:20:22,805 --> 00:20:26,765
Speaker 6:  is really interesting speaking of apple and wireless is that the next generation

332
00:20:26,765 --> 00:20:30,685
Speaker 6:  of CarPlay that no one has shipped yet will only be wireless.

333
00:20:31,615 --> 00:20:35,245
Speaker 6:  Which is fascinating. So I, you know, I'm ob obviously obsessed with CarPlay

334
00:20:35,305 --> 00:20:37,525
Speaker 6:  car cos are always on the coder. I'm always asking, they're gonna use it.

335
00:20:37,525 --> 00:20:41,485
Speaker 6:  They're always like me. We'll see. So as you just

336
00:20:41,485 --> 00:20:45,045
Speaker 6:  to set just to reset everyone's like understanding of CarPlay 'cause it's

337
00:20:45,045 --> 00:20:47,885
Speaker 6:  very confusing right now. There's the CarPlay you have right now,

338
00:20:48,865 --> 00:20:52,015
Speaker 6:  which is basically just like a second monitor for your phone. That's very

339
00:20:52,015 --> 00:20:54,735
Speaker 6:  much what's going on. So you plug into your car, even a wireless CarPlay

340
00:20:55,125 --> 00:20:58,855
Speaker 6:  your phone sends out another video stream and receives touch input from your

341
00:20:58,855 --> 00:21:02,735
Speaker 6:  car and you've got a second monitor for a phone And. if you think about it,

342
00:21:02,735 --> 00:21:06,415
Speaker 6:  almost everything that's happening on that screen is just phone stuff, right?

343
00:21:06,415 --> 00:21:08,695
Speaker 6:  It's music, it's maps, but it's all just running on your phone and showing

344
00:21:08,695 --> 00:21:12,655
Speaker 6:  up on this display. Not a lot of car stuff over there. Apple has

345
00:21:12,765 --> 00:21:16,295
Speaker 6:  lightly extended this to be able to send a map to some instrument

346
00:21:16,295 --> 00:21:20,255
Speaker 6:  clusters so they can send multiple video streams out from a phone. But it's

347
00:21:20,255 --> 00:21:23,935
Speaker 6:  still phone stuff. Like everything's local on the phone without any

348
00:21:24,015 --> 00:21:27,535
Speaker 6:  understanding of the data from the car. The next generation of CarPlay, which

349
00:21:27,535 --> 00:21:30,655
Speaker 6:  they showed two years ago, I didn't even realize this was two years ago.

350
00:21:30,695 --> 00:21:34,575
Speaker 6:  I thought this was last year, two years ago, WWC 2022, they showed

351
00:21:34,575 --> 00:21:38,455
Speaker 6:  that mockup of like a giant screen and screen. Oh I remember.

352
00:21:38,595 --> 00:21:41,935
Speaker 6:  And it's all CarPlay and there was like 45 clocks on it. 'cause no one looks

353
00:21:42,035 --> 00:21:45,295
Speaker 6:  to do with all that screen the weather in 10 different cities, which you

354
00:21:45,295 --> 00:21:48,015
Speaker 6:  definitely need when you're driving. And they're like, this is the next generation

355
00:21:48,015 --> 00:21:51,845
Speaker 6:  of CarPlay. And then they showed a bunch of car maker logos. Do you remember

356
00:21:51,845 --> 00:21:54,725
Speaker 6:  this? They showed a bunch of car maker logos and all those car makers like

357
00:21:54,725 --> 00:21:57,485
Speaker 6:  what? That was basically what happened.

358
00:21:58,235 --> 00:22:02,005
Speaker 6:  Porsche and Aspen Martin announced last year they would ship cars this year.

359
00:22:02,315 --> 00:22:05,765
Speaker 6:  Haven't heard a word about those cars. No idea when they're coming out.

360
00:22:06,145 --> 00:22:10,105
Speaker 6:  And that's it. So Apple had sessions at

361
00:22:10,105 --> 00:22:13,945
Speaker 6:  this year's wwc and I ran around and talked to a bunch of people there just

362
00:22:13,945 --> 00:22:15,225
Speaker 6:  about CarPlay and what's going on.

363
00:22:17,245 --> 00:22:20,945
Speaker 6:  And I can't tell if this is a

364
00:22:20,945 --> 00:22:24,665
Speaker 6:  change in strategy, if this has always been the approach

365
00:22:25,125 --> 00:22:29,025
Speaker 6:  or if this is just Apple softening its language. But

366
00:22:29,025 --> 00:22:31,265
Speaker 6:  Apple's basically saying to car makers, look

367
00:22:33,325 --> 00:22:37,105
Speaker 6:  people have iPhones, they love CarPlay. What if you

368
00:22:37,175 --> 00:22:41,025
Speaker 6:  used our design toolkit to redesign all of the stuff in your car. So it all

369
00:22:41,025 --> 00:22:44,055
Speaker 6:  looked really good together and then you needed an iPhone.

370
00:22:44,195 --> 00:22:47,855
Speaker 8:  Is it the context here that Apple just abandoned Project Titan? So

371
00:22:48,115 --> 00:22:51,975
Speaker 8:  if they're not doing a fully integrated vehicle, wouldn't it make

372
00:22:51,975 --> 00:22:55,015
Speaker 8:  sense for them to make this more of an open, this to me is like them going

373
00:22:55,035 --> 00:22:58,815
Speaker 8:  the route of Apple TV plus and Airplay realizing that

374
00:22:58,875 --> 00:23:02,855
Speaker 8:  we are not gonna make a TV and so therefore we want TV plus

375
00:23:02,855 --> 00:23:05,055
Speaker 8:  everywhere. Is that not a fair analogy here?

376
00:23:05,655 --> 00:23:09,575
Speaker 6:  I can't necessarily tell how much Titan played into it, right? Like

377
00:23:10,325 --> 00:23:12,735
Speaker 6:  this is what I'm saying. I can't tell if this was always the approach and

378
00:23:12,735 --> 00:23:16,255
Speaker 6:  now they're just softening the language. Like the technical approach

379
00:23:16,915 --> 00:23:20,895
Speaker 6:  is that you have an iPhone, you connect to CarPlay, your iPhone will

380
00:23:20,955 --> 00:23:24,295
Speaker 6:  go download a bunch of Apple car maker

381
00:23:24,545 --> 00:23:28,455
Speaker 6:  co-branded assets. Like in the videos they have posted, they refer to this

382
00:23:28,455 --> 00:23:32,215
Speaker 6:  as a co-branded experience. When have you ever heard Apple describe anything

383
00:23:32,275 --> 00:23:36,255
Speaker 6:  as a co-branded experience? That's bananas Red.

384
00:23:37,645 --> 00:23:38,455
Speaker 6:  Sure. Project Red

385
00:23:38,455 --> 00:23:41,015
Speaker 8:  Windows on the Mac like at the very beginning.

386
00:23:41,725 --> 00:23:44,975
Speaker 6:  That wasn't co-branded. So like there's some element of Apple designers and

387
00:23:45,005 --> 00:23:48,645
Speaker 6:  car maker designers like working together to design

388
00:23:48,685 --> 00:23:52,605
Speaker 6:  speedometers for CarPlay cars. Huh? Weird. And like

389
00:23:52,605 --> 00:23:55,405
Speaker 6:  in the, this is public. I'm not, this isn't like some deep dark reporting.

390
00:23:55,405 --> 00:23:58,285
Speaker 6:  I mean this is me saying You can go watch the videos. They're public and

391
00:23:58,285 --> 00:24:01,765
Speaker 6:  they're calling it a co-branded experience where we're gonna work together

392
00:24:01,905 --> 00:24:05,565
Speaker 6:  to express your car maker brand identity

393
00:24:05,745 --> 00:24:07,125
Speaker 6:  inside of the Apple Design toolkit.

394
00:24:07,415 --> 00:24:10,925
Speaker 7:  Weird. It makes sense in that all the cars

395
00:24:11,305 --> 00:24:12,405
Speaker 7:  UIs are terrible.

396
00:24:13,185 --> 00:24:16,285
Speaker 6:  It does. So the idea is you have an iPhone and you get this like upgrade

397
00:24:16,305 --> 00:24:19,565
Speaker 6:  to your car ui. So everything looks like CarPlay. Yeah. Weird. How is Apple

398
00:24:19,565 --> 00:24:21,285
Speaker 6:  gonna make any money doing this? No.

399
00:24:21,595 --> 00:24:25,205
Speaker 7:  Like first of all, I think that's like a, I mean for a super wealthy company

400
00:24:25,475 --> 00:24:28,765
Speaker 7:  like this, this is just like really good branding, right? I think it's about

401
00:24:29,305 --> 00:24:33,205
Speaker 7:  You can go and, I mean show your, I mean You can show your, your dominance

402
00:24:33,225 --> 00:24:36,045
Speaker 7:  in design and, and show these companies like

403
00:24:36,045 --> 00:24:38,405
Speaker 6:  How you, so the, the specific way they're gonna show their dominance in design.

404
00:24:38,405 --> 00:24:41,605
Speaker 6:  It's actually my favorite part of whole thing. Okay? The only font you're

405
00:24:41,605 --> 00:24:45,005
Speaker 6:  allowed to use in next generation CarPlay is Apple San Francisco.

406
00:24:45,545 --> 00:24:48,005
Speaker 6:  You can, You can change how wide it is. You can change how stretched it is.

407
00:24:48,005 --> 00:24:51,285
Speaker 6:  You can italicize it. You can make it really thin. You know, like, and they're

408
00:24:51,285 --> 00:24:53,445
Speaker 6:  like, look at all these different things You can do with this font. It's

409
00:24:53,445 --> 00:24:57,405
Speaker 6:  a variable typeface. And it's like, yeah but it's only one like Porsche

410
00:24:57,405 --> 00:24:59,525
Speaker 6:  and GM don't get to use different fonts.

411
00:24:59,975 --> 00:25:00,805
Speaker 7:  Comic sands

412
00:25:00,985 --> 00:25:04,765
Speaker 6:  Get get right? Like that's weird. Like so there's an element of control

413
00:25:04,765 --> 00:25:07,925
Speaker 6:  where they're just, they're doing a very Apple thing. And then there's an

414
00:25:07,925 --> 00:25:11,125
Speaker 6:  element of complete chaos, which I think is really fascinating, which they

415
00:25:11,125 --> 00:25:15,045
Speaker 6:  have not talked about at all. Which is they will, when you

416
00:25:15,045 --> 00:25:18,845
Speaker 6:  have like the next generation CarPlay interface up and you're like,

417
00:25:18,925 --> 00:25:22,885
Speaker 6:  I want to control my massaging seats and my Aston Martin You

418
00:25:22,885 --> 00:25:26,485
Speaker 6:  can push the button that says Control the Apple San Francisco

419
00:25:26,675 --> 00:25:30,485
Speaker 6:  labeled button. And instead of showing you the next nice CarPlay

420
00:25:30,485 --> 00:25:34,405
Speaker 6:  thing, it'll do a thing called punching through the automaker UI and it'll

421
00:25:34,405 --> 00:25:37,125
Speaker 6:  just show you whatever garbage Aston Martin, no nevermind

422
00:25:37,125 --> 00:25:38,845
Speaker 7:  Seats. Oh man, nevermind. I take it all back.

423
00:25:38,845 --> 00:25:42,485
Speaker 6:  It's like, this is crazy. I can't, that's like, this is, this is the concession

424
00:25:42,905 --> 00:25:46,045
Speaker 6:  to make it all. Okay, I can't because what car makers have been worried about

425
00:25:46,045 --> 00:25:48,645
Speaker 6:  is they have to build everything three times. They have to build their own

426
00:25:48,645 --> 00:25:51,005
Speaker 6:  native one. 'cause you need to have something you, you need to operate the

427
00:25:51,005 --> 00:25:54,685
Speaker 6:  entire car without a phone. Right? A non-negotiable. If you buy a car,

428
00:25:55,435 --> 00:25:59,085
Speaker 6:  then you need the next generation CarPlay version of everything and then

429
00:25:59,085 --> 00:26:02,845
Speaker 6:  presumably Google will do something for Android, right? So what are you gonna

430
00:26:02,845 --> 00:26:06,565
Speaker 6:  do? And they don't wanna build things three times. So Apple solution is like,

431
00:26:06,565 --> 00:26:09,085
Speaker 6:  well build it once and then, you know, for some stuff we'll just let you

432
00:26:09,085 --> 00:26:11,485
Speaker 6:  show the weird stuff. I, I can't

433
00:26:11,485 --> 00:26:15,325
Speaker 8:  Wait for Siri powered by Chachi PT in my car to

434
00:26:15,325 --> 00:26:19,205
Speaker 8:  just like open the doors like while I'm driving, when I request the massage

435
00:26:19,205 --> 00:26:22,405
Speaker 8:  seat to turn on and it just hallucinates and you know, it just

436
00:26:22,405 --> 00:26:23,845
Speaker 6:  Like freaking completely. Yeah, just goes in reverse.

437
00:26:24,275 --> 00:26:25,045
Speaker 7:  Just ejects you.

438
00:26:25,665 --> 00:26:29,405
Speaker 6:  And then to bring this around, apple wants this to only work

439
00:26:29,925 --> 00:26:33,605
Speaker 6:  wirelessly because their goal is that CarPlay will be

440
00:26:33,605 --> 00:26:37,085
Speaker 6:  connected, active in operating the interface of your car

441
00:26:37,705 --> 00:26:40,405
Speaker 6:  by the time you open the door. And if not, by the time you open the door,

442
00:26:40,405 --> 00:26:42,485
Speaker 6:  by the time the screen's light up, right? So you get in the car, you sit

443
00:26:42,485 --> 00:26:46,445
Speaker 6:  down, the screens light up, it's CarPlay, it should not

444
00:26:46,445 --> 00:26:50,165
Speaker 6:  be, you sit down in your car, turn it on and then like you have some like

445
00:26:50,165 --> 00:26:53,285
Speaker 6:  loading process. And so You can only accomplish that over wireless. So CarPlay

446
00:26:53,285 --> 00:26:56,885
Speaker 6:  is only gonna be wireless for whatever car makers adopt the next generation

447
00:26:56,885 --> 00:26:57,125
Speaker 6:  CarPlay.

448
00:26:57,185 --> 00:27:01,005
Speaker 7:  I'm hesitant to say this because I am very against this

449
00:27:01,165 --> 00:27:04,965
Speaker 7:  move. But isn't that also just a precursor to them removing all the

450
00:27:04,965 --> 00:27:07,205
Speaker 7:  ports? They're like, it's gotta be wireless because we're gonna,

451
00:27:07,555 --> 00:27:11,445
Speaker 6:  It's not, not a precursor to them removing all the ports. And

452
00:27:11,445 --> 00:27:15,405
Speaker 6:  it's funny because to make this work, apple also

453
00:27:15,405 --> 00:27:19,125
Speaker 6:  has to give up some control of what runs where,

454
00:27:19,935 --> 00:27:23,725
Speaker 6:  right? You don't want your speedometer running on the phone. So

455
00:27:23,895 --> 00:27:27,285
Speaker 6:  Apple, your Apple designed speedometer or

456
00:27:27,335 --> 00:27:31,045
Speaker 6:  delightful co-branded experience to be more accurate needs to run locally

457
00:27:31,185 --> 00:27:34,765
Speaker 6:  on the car. So this is the first time I can think of that any Apple assets

458
00:27:35,385 --> 00:27:38,645
Speaker 6:  are gonna get loaded onto somebody else's computer system in that way.

459
00:27:38,955 --> 00:27:42,805
Speaker 6:  Outside of, to your point E, the Apple TV plus app, unlike

460
00:27:42,975 --> 00:27:46,485
Speaker 6:  Rokus, right? Which is where apple ships an app and this is the, the,

461
00:27:46,985 --> 00:27:49,325
Speaker 6:  the split is kinda the same. It's like okay you're gonna, we're gonna give

462
00:27:49,325 --> 00:27:52,925
Speaker 6:  you an asset package of Apple things to run locally on the car

463
00:27:53,385 --> 00:27:56,245
Speaker 6:  for things that we cannot risk if there's a disconnect.

464
00:27:57,185 --> 00:28:00,605
Speaker 6:  The speedometer, the turn signals, da da da da, right? And there's some

465
00:28:00,605 --> 00:28:04,365
Speaker 6:  differentiation in how that stuff works. Like the truly critical stuff cannot

466
00:28:04,465 --> 00:28:08,285
Speaker 6:  be touched. They call that the overlay layer. That's your turn signals and

467
00:28:08,285 --> 00:28:12,245
Speaker 6:  hazard indicators, the speedometer and stuff like the needles, that's those

468
00:28:12,245 --> 00:28:15,805
Speaker 6:  assets can get refreshed over time. So those run in a different layer called

469
00:28:15,805 --> 00:28:19,565
Speaker 6:  local UI. And then there's everything that runs on the phone.

470
00:28:20,185 --> 00:28:23,605
Speaker 6:  And then in the middle there's like, I don't know, you wanna like mess with

471
00:28:23,605 --> 00:28:26,925
Speaker 6:  your sports settings in the, in your fast car. Like that'll just be the weird

472
00:28:26,925 --> 00:28:30,405
Speaker 6:  automaker UI shining through CarPlay and it's like, oh this kind of feels

473
00:28:30,405 --> 00:28:31,245
Speaker 6:  like a mess. Right?

474
00:28:32,785 --> 00:28:35,395
Speaker 6:  It's unclear who will take this and who will

475
00:28:36,845 --> 00:28:39,985
Speaker 6:  use all of CarPlay like redesign the entire car to be CarPlay

476
00:28:40,765 --> 00:28:44,345
Speaker 6:  to be Apple. And who will say okay, we took next generation CarPlay,

477
00:28:45,685 --> 00:28:49,585
Speaker 6:  we fun, cool new speedometer. Every other part of the car controls

478
00:28:49,585 --> 00:28:52,865
Speaker 6:  will be the existing ui. 'cause we can punch through CarPlay. And

479
00:28:53,905 --> 00:28:57,665
Speaker 6:  honestly based on the feedback we get on the show, it's like some of our

480
00:28:57,985 --> 00:29:01,745
Speaker 6:  audience will pick the car makers that use all CarPlay. Like they will

481
00:29:01,775 --> 00:29:05,385
Speaker 6:  make an affirmative decision to be like, you know what that car maker Toyota

482
00:29:05,735 --> 00:29:09,385
Speaker 6:  uses all of CarPlay, we're going with them over Honda. Which is like, here's

483
00:29:09,385 --> 00:29:10,825
Speaker 6:  your weird heated seats. Well the

484
00:29:10,825 --> 00:29:14,705
Speaker 8:  Third path is Rivian or Tesla, right? Where it's totally controlled by

485
00:29:14,705 --> 00:29:18,625
Speaker 8:  the automaker. And like Rivian is a great example of, I think you and

486
00:29:18,625 --> 00:29:22,065
Speaker 8:  I both test driven Rivian Eli. Like I think their interface is great.

487
00:29:22,655 --> 00:29:26,545
Speaker 8:  It's very snappy. Obviously I miss some of the iPhone connection,

488
00:29:26,805 --> 00:29:30,585
Speaker 8:  but I could also see more automakers going that

489
00:29:30,585 --> 00:29:31,025
Speaker 8:  direction.

490
00:29:32,255 --> 00:29:32,545
Speaker 6:  Yeah.

491
00:29:32,725 --> 00:29:36,025
Speaker 7:  But they have to make good user interfaces. Rivian is good. That is a big

492
00:29:36,025 --> 00:29:39,985
Speaker 7:  difference between, yeah, no, that runs unreal. Like Rivian and Tesla

493
00:29:40,215 --> 00:29:43,445
Speaker 7:  made good user interfaces for the most part. Tesla. Everybody else

494
00:29:44,665 --> 00:29:46,725
Speaker 6:  For the most weirder, I would say for the most part

495
00:29:46,755 --> 00:29:49,205
Speaker 8:  Tesla's like, you turn the car on, but everybody else talking like you wanna

496
00:29:49,205 --> 00:29:51,245
Speaker 8:  like play a game on Netflix, you know, it's,

497
00:29:54,305 --> 00:29:57,765
Speaker 6:  But it's a lovely experience. So Rivian just upgraded in the, the

498
00:29:57,905 --> 00:30:01,085
Speaker 6:  rivian, the generation two R one trucks just came out.

499
00:30:02,595 --> 00:30:05,685
Speaker 6:  They've changed some the, the ar the computing architecture of those is different.

500
00:30:05,865 --> 00:30:09,845
Speaker 6:  But then they also updated the interface to be more self shaded. I mean they

501
00:30:09,845 --> 00:30:13,605
Speaker 6:  have like unreal engine and the, they like using it. That said, I looked

502
00:30:13,605 --> 00:30:17,585
Speaker 6:  to the parking lot at Apple Park when we were at WW c. A lot

503
00:30:17,585 --> 00:30:20,425
Speaker 6:  of those people have riv. Oh yeah. Right. And there's all these rumors about

504
00:30:20,425 --> 00:30:23,745
Speaker 6:  Apple and Rivian doing stuff together. Apple music just came to the Rivian

505
00:30:23,745 --> 00:30:27,185
Speaker 6:  interface. If, if Tesla's never gonna break,

506
00:30:28,385 --> 00:30:32,025
Speaker 6:  I feel confident about that one. Just a prediction I'll make based on nothing.

507
00:30:32,705 --> 00:30:32,945
Speaker 6:  I don't,

508
00:30:33,485 --> 00:30:36,345
Speaker 8:  The Tesla UI is gonna be x.com in a few years. That's the direction

509
00:30:37,545 --> 00:30:40,785
Speaker 6:  S like it'll be Brock like fully just like telling dirty jokes. We try to

510
00:30:40,785 --> 00:30:43,905
Speaker 6:  drive the car Rivian Yeah. Is still in the place

511
00:30:44,515 --> 00:30:48,425
Speaker 6:  where they might try some other stuff to claw back some market share

512
00:30:48,525 --> 00:30:52,145
Speaker 6:  to win some new buyers. And there are an awful lot of Rivian. Mm.

513
00:30:53,335 --> 00:30:56,865
Speaker 6:  It's California people like people like trucks there. But it's something

514
00:30:56,865 --> 00:31:00,745
Speaker 6:  I noticed. All right, moving on. See it's like a fast

515
00:31:00,745 --> 00:31:03,225
Speaker 6:  round. Not quite lightning. We're just moving

516
00:31:05,475 --> 00:31:08,825
Speaker 6:  Kranz. Tell me about this remote because you're very excited about a home

517
00:31:08,825 --> 00:31:09,705
Speaker 6:  remote and I

518
00:31:09,705 --> 00:31:13,625
Speaker 7:  About this you, you know I love a home remote. I'm still sad about Logitech.

519
00:31:13,705 --> 00:31:17,465
Speaker 7:  I still have the like the fanciest of the Logitech remotes and it had soft

520
00:31:17,465 --> 00:31:19,905
Speaker 7:  touch plastic. Oh my God. Which means it's starting to rot.

521
00:31:20,025 --> 00:31:22,865
Speaker 6:  I have my, I have TiVo remote in the corner of this house. It's like not

522
00:31:22,865 --> 00:31:23,665
Speaker 6:  a good, not

523
00:31:23,665 --> 00:31:26,505
Speaker 7:  A good situation. It's so bad. Like the rot on that soft touch is so gross.

524
00:31:27,005 --> 00:31:30,025
Speaker 7:  But there's a, there's a Kickstarter happening right now for the haptic

525
00:31:31,125 --> 00:31:35,025
Speaker 7:  RS 90 and the RS 90 x and they promise all the stuff, all

526
00:31:35,025 --> 00:31:38,905
Speaker 7:  the big universal remotes promise, which is like You can control stuff via

527
00:31:38,905 --> 00:31:42,425
Speaker 7:  Bluetooth. You can control it via ir. You can control it via wifi.

528
00:31:43,345 --> 00:31:46,505
Speaker 7:  Usually that requires some sort of hub that you have to plug in somewhere

529
00:31:46,505 --> 00:31:50,265
Speaker 7:  in your house. This appears to require no hub and also

530
00:31:50,495 --> 00:31:54,425
Speaker 7:  it's just got a big screen on it. So it's instantly the coolest remote You

531
00:31:54,425 --> 00:31:58,145
Speaker 7:  can have in your house and it can, it's supposed to be able to control all

532
00:31:58,145 --> 00:32:01,745
Speaker 7:  their smart homes and stuff. And like I said, Logitech did this. I've got

533
00:32:01,745 --> 00:32:05,385
Speaker 7:  a control that does this, doesn't have the cool screen

534
00:32:05,565 --> 00:32:09,505
Speaker 7:  and it required a hub. So this is instantly cooler

535
00:32:09,805 --> 00:32:13,625
Speaker 7:  and this also is gonna come soon rather than discontinue.

536
00:32:13,625 --> 00:32:15,465
Speaker 6:  Well it's a Kickstarter are actually gonna ship

537
00:32:15,655 --> 00:32:19,105
Speaker 7:  Kickstarter. There's like the Kickstarter part of this is the really big

538
00:32:19,105 --> 00:32:22,225
Speaker 7:  heavy caveat because there's a whole bunch of integrations that they're promising.

539
00:32:23,245 --> 00:32:23,465
Speaker 8:  No,

540
00:32:23,805 --> 00:32:27,505
Speaker 7:  Not there yet. There's no matter support, there's,

541
00:32:27,505 --> 00:32:30,865
Speaker 7:  there's no support for some of the other smaller like radios and stuff like

542
00:32:30,865 --> 00:32:33,585
Speaker 7:  that. So this is like cool

543
00:32:35,575 --> 00:32:35,865
Speaker 7:  neat.

544
00:32:36,315 --> 00:32:40,205
Speaker 6:  We're like two minutes away from Samsung being like Bixby

545
00:32:40,305 --> 00:32:42,685
Speaker 6:  AI and your TV will not do everything for you. I

546
00:32:42,685 --> 00:32:45,805
Speaker 7:  Don't want it. It's coming. We though coming. We are. It's coming. Yeah.

547
00:32:45,805 --> 00:32:49,165
Speaker 7:  Like AI is probably, you know the reason Loge got out of this business was

548
00:32:49,165 --> 00:32:52,925
Speaker 7:  because they said nobody wants these. No. And and me and four other people

549
00:32:52,925 --> 00:32:53,365
Speaker 7:  were like, oh

550
00:32:53,605 --> 00:32:57,565
Speaker 8:  I want my phone to be my universal home remote. That's it. I

551
00:32:57,685 --> 00:33:00,445
Speaker 8:  don't want another remote. I want fewer remotes. Can we make that happen?

552
00:33:01,635 --> 00:33:03,405
Speaker 7:  Yeah. I just, I, that's how I use my phone.

553
00:33:03,625 --> 00:33:07,525
Speaker 8:  But like I, I'm just getting like who is

554
00:33:07,525 --> 00:33:09,365
Speaker 8:  this for I guess is the question For me.

555
00:33:10,475 --> 00:33:13,805
Speaker 6:  It's me, me. What should I thought Alex?

556
00:33:14,195 --> 00:33:17,845
Speaker 7:  It's, it's me and four other people. It's the four people who are still sad

557
00:33:17,845 --> 00:33:18,965
Speaker 7:  that are Logitech remotes stack.

558
00:33:19,285 --> 00:33:22,565
Speaker 6:  Whenever I see a screen like this, I just think about how slow it will be.

559
00:33:22,805 --> 00:33:25,245
Speaker 6:  I knew I think about touching a button and nothing happening, right? Yeah.

560
00:33:25,245 --> 00:33:27,845
Speaker 6:  Like yeah and I love it. I'm like yeah I want a screen like this. But you

561
00:33:27,845 --> 00:33:30,365
Speaker 6:  know that thing you do with a touchscreen where you touch it and the button

562
00:33:30,365 --> 00:33:33,365
Speaker 6:  looks like something happened but nothing happened. That's this product.

563
00:33:33,995 --> 00:33:37,965
Speaker 6:  Yeah. Almost like a guarantee that that is this product. It is. I will say

564
00:33:37,965 --> 00:33:41,765
Speaker 6:  it is. I love that. It's called the Haptic RS 90. Great name.

565
00:33:41,895 --> 00:33:45,045
Speaker 6:  Great name for a universal remote, fully good name. Name.

566
00:33:46,005 --> 00:33:49,925
Speaker 6:  I just worry that actually people just use the apps in our

567
00:33:49,925 --> 00:33:53,125
Speaker 6:  smart TV and what this thing is trying to solve is not,

568
00:33:54,085 --> 00:33:56,605
Speaker 6:  I mean that's why Logitech got outta the business. So like we, we can't control

569
00:33:56,605 --> 00:33:58,405
Speaker 6:  the apps in your tv. We can't see your tv.

570
00:33:58,905 --> 00:34:02,725
Speaker 7:  Yep. Yeah. The will. So we'll see how this is

571
00:34:02,745 --> 00:34:06,165
Speaker 7:  it. It's, it looks like it's gonna be funded. It looks like enough people

572
00:34:06,165 --> 00:34:08,285
Speaker 7:  have invested in it. It's gonna be funded. This

573
00:34:08,285 --> 00:34:10,885
Speaker 6:  Is like hot on the heels of that company. Brilliant going outta business.

574
00:34:11,195 --> 00:34:14,765
Speaker 6:  Yeah. Which Jen covered and they were like, we're gonna business but like

575
00:34:15,025 --> 00:34:18,885
Speaker 6:  the thing we put in your wall will still work So brilliant.

576
00:34:18,885 --> 00:34:21,685
Speaker 6:  And put like a touch screen in your wall that can control everything. Which

577
00:34:21,725 --> 00:34:25,165
Speaker 6:  I was super hype on. 'cause the idea of having an in-wall sono

578
00:34:25,165 --> 00:34:29,045
Speaker 6:  controller screen seems sick, right? You walk into a room and you're like,

579
00:34:29,045 --> 00:34:31,885
Speaker 6:  I'm gonna pick a song a boop. I'm not using my phone like the whole thing.

580
00:34:32,025 --> 00:34:36,005
Speaker 6:  And then they just like went outta business and it's like that is always

581
00:34:36,025 --> 00:34:38,965
Speaker 6:  the worry I have with all of these things. Yeah. Speaking. Oh

582
00:34:38,965 --> 00:34:42,165
Speaker 7:  Go ahead. This is gonna be like almost $400 probably. Perfect when it comes

583
00:34:42,165 --> 00:34:43,845
Speaker 7:  out. That's too much.

584
00:34:44,425 --> 00:34:47,165
Speaker 6:  All right, speaking. Oh let's do one more gadget and then I want to talk

585
00:34:47,165 --> 00:34:50,765
Speaker 6:  about what's going on in AR and VR with Heath. We mentioned Risk V at the

586
00:34:50,765 --> 00:34:53,685
Speaker 6:  top and we mentioned companies. I'm like Go Outta Business

587
00:34:54,205 --> 00:34:57,765
Speaker 6:  framework is shipping the laptop 13. Which is

588
00:34:58,365 --> 00:35:01,765
Speaker 6:  fascinating 'cause they're gonna have a risk V chip in it. This is the competitor

589
00:35:01,865 --> 00:35:05,125
Speaker 6:  arm, right? This is the open source competitor arm that has the

590
00:35:05,555 --> 00:35:07,805
Speaker 6:  performance per watt characteristics.

591
00:35:09,425 --> 00:35:11,885
Speaker 6:  I'm kind of dying to try it out. I'm also, I have no idea what software I

592
00:35:11,885 --> 00:35:12,125
Speaker 6:  could run.

593
00:35:12,715 --> 00:35:16,565
Speaker 7:  Also I, I think one guy pointed out that it runs, it'll run

594
00:35:16,565 --> 00:35:19,005
Speaker 7:  about as fast as a Raspberry PI four. Yes.

595
00:35:20,185 --> 00:35:22,525
Speaker 6:  You gotta start somewhere. Baby steps. Baby

596
00:35:22,535 --> 00:35:26,405
Speaker 7:  Steps, baby steps. Yeah. I have no idea what

597
00:35:26,405 --> 00:35:30,045
Speaker 7:  can run this but I wanna like, this just is cool.

598
00:35:30,235 --> 00:35:32,005
Speaker 7:  This is the kind of thing I just wanna have one.

599
00:35:33,605 --> 00:35:36,885
Speaker 6:  I like how the, the actual line from the company

600
00:35:38,385 --> 00:35:42,165
Speaker 6:  in the PR is the main board is extremely compelling. But we should be

601
00:35:42,165 --> 00:35:45,485
Speaker 6:  clear that in this generation it is focused primarily on enabling

602
00:35:45,615 --> 00:35:48,765
Speaker 6:  developers, tinkerers and hobbyists to start testing and creating and risky

603
00:35:48,835 --> 00:35:52,565
Speaker 6:  like don't use this. The per the peripheral set and performance aren't yet

604
00:35:52,565 --> 00:35:56,485
Speaker 6:  competitive with our Intel and a MD powered framework main. It's like,

605
00:35:57,005 --> 00:35:59,765
Speaker 6:  yeah. Ooh. This is just to play with, I will say

606
00:35:59,805 --> 00:36:00,445
Speaker 7:  EMC storage.

607
00:36:01,625 --> 00:36:05,525
Speaker 6:  The risk, the nerds in our audience have been very excited at Risky

608
00:36:05,545 --> 00:36:09,045
Speaker 6:  for a very long time. 'cause ARM is a like dominant

609
00:36:10,045 --> 00:36:13,685
Speaker 6:  quasi monopolist now. Like the arm architecture license is the

610
00:36:13,965 --> 00:36:17,325
Speaker 6:  thing and there hasn't been anything to compete with it. Which is why

611
00:36:17,755 --> 00:36:21,725
Speaker 6:  everybody is moving to it, including Microsoft. 'cause

612
00:36:21,725 --> 00:36:25,165
Speaker 6:  if you want performance for wat you end up on this architecture, which means

613
00:36:25,525 --> 00:36:28,285
Speaker 6:  companies like Qualcomm and being able to dominate the market right next

614
00:36:28,285 --> 00:36:32,045
Speaker 6:  to Apple. And you need an architecture license from ARM

615
00:36:32,905 --> 00:36:36,805
Speaker 6:  to expand beyond it and ARM isn't giving this up anymore. Which

616
00:36:36,805 --> 00:36:40,205
Speaker 6:  is part of the reason they're suing Qualcomm. 'cause Qualcomm is saying the

617
00:36:40,285 --> 00:36:44,085
Speaker 6:  NUVIA chips fall under its architecture license and ARM says no. So

618
00:36:44,085 --> 00:36:47,965
Speaker 6:  there's all this stuff that's wrapped up when you have one dominant architecture

619
00:36:48,035 --> 00:36:51,485
Speaker 6:  that is owned by a company And Risk V is the thing.

620
00:36:51,835 --> 00:36:52,125
Speaker 6:  Yeah,

621
00:36:52,305 --> 00:36:53,925
Speaker 7:  The open source source was it, was it

622
00:36:53,925 --> 00:36:57,125
Speaker 6:  Nvidia? Not quite as fast as a Raspberry PI four. But it's the thing that

623
00:36:57,125 --> 00:36:57,765
Speaker 6:  might break it.

624
00:36:59,345 --> 00:37:02,165
Speaker 7:  Was it Nvidia that tried to buy Arm a couple years ago?

625
00:37:03,435 --> 00:37:07,405
Speaker 6:  Yeah, yeah, yeah. It was So ARM was, I mean the the finances are Arm are

626
00:37:07,405 --> 00:37:11,085
Speaker 6:  hilarious, right? Like it was, it's a weird English company

627
00:37:11,555 --> 00:37:15,445
Speaker 6:  that started licensing these designs. It got SoftBank bought it, right?

628
00:37:15,445 --> 00:37:19,365
Speaker 6:  Yeah. and it was part of the vision fund. They, in the heady days

629
00:37:19,365 --> 00:37:23,045
Speaker 6:  of the SoftBank Vision Fund where Maan was like, everything I touch will

630
00:37:23,045 --> 00:37:27,005
Speaker 6:  be a monopoly. It didn't work out. You might recall it. WeWork

631
00:37:27,155 --> 00:37:30,885
Speaker 6:  blew that entire business up. They tried to, they they started

632
00:37:31,035 --> 00:37:34,565
Speaker 6:  divesting of assets. They were gonna sell Arm. Nvidia wanted to buy it. Basically

633
00:37:34,765 --> 00:37:38,655
Speaker 6:  everyone yelled like everyone yelled, apple yelled qual like

634
00:37:38,655 --> 00:37:41,175
Speaker 6:  all the architecture licenses were like you cannot let our competitor buy

635
00:37:41,175 --> 00:37:45,055
Speaker 6:  this company. They said No Arm is now public instead. Which is a good

636
00:37:45,055 --> 00:37:47,775
Speaker 6:  outcome or bad fun

637
00:37:47,775 --> 00:37:51,495
Speaker 8:  Fact arm going public made up for all the losses the vision fund has had

638
00:37:51,495 --> 00:37:52,335
Speaker 8:  for the last 10 years.

639
00:37:53,435 --> 00:37:54,095
Speaker 6:  That's crazy.

640
00:37:57,325 --> 00:38:00,215
Speaker 6:  Somewhere somewhere that WeWork people are like, God damn,

641
00:38:00,965 --> 00:38:03,095
Speaker 7:  They're just launching a new product. Don't worry about

642
00:38:03,095 --> 00:38:04,855
Speaker 6:  Mean it's like I could have changed the world.

643
00:38:06,875 --> 00:38:07,735
Speaker 6:  It a lifestyle.

644
00:38:10,175 --> 00:38:13,055
Speaker 6:  I watched one episode of We Crash and I was like I can't live this again.

645
00:38:13,055 --> 00:38:16,495
Speaker 6:  Oh it's so bad. I hon I cannot be a part of this again. Yeah. Alright, let's

646
00:38:16,495 --> 00:38:20,215
Speaker 6:  wrap up by talking about AR and VR Heath, you've got a Scoop meta just

647
00:38:20,415 --> 00:38:24,255
Speaker 6:  reorganized Reality Labs. Talk about that. The Vision Pro team is trying

648
00:38:24,295 --> 00:38:26,535
Speaker 6:  to make a cheaper headset. There's a lot of action here. What's going on?

649
00:38:26,535 --> 00:38:30,295
Speaker 8:  Yeah, maybe we'll start with Vision Pro. The information had a good

650
00:38:30,295 --> 00:38:34,255
Speaker 8:  story out confirming and putting more details on what Gerin

651
00:38:34,255 --> 00:38:37,685
Speaker 8:  and others have reported that Apple is indeed working on a cheaper vision

652
00:38:37,825 --> 00:38:41,765
Speaker 8:  pro. They hope to release it by the end of next year and they

653
00:38:41,765 --> 00:38:45,565
Speaker 8:  have shelved a future iteration of V

654
00:38:45,585 --> 00:38:49,405
Speaker 8:  two of the Vision Pro as it is today. So another high end, very

655
00:38:49,405 --> 00:38:53,205
Speaker 8:  expensive one. They're trying to get this cheaper, one more into the ballpark

656
00:38:53,385 --> 00:38:56,885
Speaker 8:  of a, you know, premium top of the line iPhone price. So we're talking

657
00:38:57,185 --> 00:39:01,045
Speaker 8:  12 to $1,500. I had heard even before the

658
00:39:01,045 --> 00:39:04,285
Speaker 8:  Vision Pro came out that Apple was already working on this cheaper version.

659
00:39:05,605 --> 00:39:08,885
Speaker 8:  Apparently when Mark Zuckerberg was talking to me and others and basically

660
00:39:08,885 --> 00:39:12,445
Speaker 8:  saying I'm going to scorch the earth with cheap headsets forever

661
00:39:13,125 --> 00:39:16,725
Speaker 8:  and not really care about making a business, which we will get to in the

662
00:39:16,725 --> 00:39:20,565
Speaker 8:  next story. Apple was like, oh okay we need to

663
00:39:20,565 --> 00:39:24,005
Speaker 8:  like really work on a, on a cheap one and get it out. So

664
00:39:24,155 --> 00:39:27,805
Speaker 8:  they've been working on it for already a couple years. I do expect we'll

665
00:39:27,805 --> 00:39:31,405
Speaker 8:  see it next year whether it ships next year or TBD

666
00:39:31,705 --> 00:39:35,525
Speaker 8:  And I think, you know, I've only used the Vision Pro once in that

667
00:39:35,525 --> 00:39:39,485
Speaker 8:  initial demo that we all had at WDC last year, Eli, but I would

668
00:39:39,485 --> 00:39:43,045
Speaker 8:  think that, you know, if it was in the ballpark of $1,500,

669
00:39:44,485 --> 00:39:47,925
Speaker 8:  I would maybe pull the trigger even if the OS and the software hadn't

670
00:39:47,925 --> 00:39:51,725
Speaker 8:  meaningfully gotten better just because of, you know, the

671
00:39:51,725 --> 00:39:55,605
Speaker 8:  immersiveness of it. I kind of, it's just at 3000 it's just like it's,

672
00:39:55,635 --> 00:39:59,445
Speaker 8:  it's out of touch for most people. Yeah. And it's still out of touch at

673
00:39:59,445 --> 00:40:03,325
Speaker 8:  1500 but it's much more of a almost impulse buy for some people

674
00:40:03,465 --> 00:40:07,245
Speaker 8:  at 1500. So yeah. and it will get

675
00:40:07,445 --> 00:40:11,365
Speaker 8:  hopefully more developers interested but yeah, Apple's plugging away

676
00:40:11,425 --> 00:40:15,045
Speaker 8:  on and I, and I think what we're, what the, the other part of this, the fact

677
00:40:15,045 --> 00:40:18,565
Speaker 8:  that they are canceling the more expensive one or

678
00:40:18,565 --> 00:40:22,405
Speaker 8:  shelving, it just goes to show that they packed so

679
00:40:22,405 --> 00:40:26,165
Speaker 8:  much tech into this thing that it may not actually

680
00:40:26,285 --> 00:40:28,565
Speaker 8:  be a good product and they may and

681
00:40:29,145 --> 00:40:29,365
Speaker 6:  We

682
00:40:29,365 --> 00:40:32,845
Speaker 8:  Kind of like have found that out already. Neli, you talk about how you don't

683
00:40:32,845 --> 00:40:36,765
Speaker 8:  use the Vision Pro anymore. It's a great demo. Honestly

684
00:40:36,785 --> 00:40:40,005
Speaker 8:  it was probably top three demos I've ever done of technology

685
00:40:40,825 --> 00:40:44,005
Speaker 8:  and it's something that I consistently hear people do not use after the first

686
00:40:44,005 --> 00:40:45,085
Speaker 8:  couple weeks. So

687
00:40:45,455 --> 00:40:46,965
Speaker 7:  Would you say this was a Homer car?

688
00:40:47,825 --> 00:40:51,445
Speaker 6:  No, I mean it's not like that ridiculous, right? Like yeah, okay. Like it

689
00:40:51,445 --> 00:40:55,085
Speaker 6:  had some focus. They wanted it to be something I think about the fact that

690
00:40:55,085 --> 00:40:58,925
Speaker 6:  it was a great demo. I so much of technology right now is like great demos

691
00:40:58,925 --> 00:41:01,965
Speaker 6:  to fall apart as a product, right? Yeah. And that was the vision pro to a

692
00:41:02,045 --> 00:41:06,005
Speaker 6:  t and I every AI demo so far has been

693
00:41:06,005 --> 00:41:09,845
Speaker 6:  that from, unless you have some very narrow use case where AI can

694
00:41:09,845 --> 00:41:13,445
Speaker 6:  solve a problem consistently for you, every other demo is like what? What?

695
00:41:13,645 --> 00:41:16,605
Speaker 6:  I dunno if I would do that. The Vision Pro they have to make a big decision.

696
00:41:16,705 --> 00:41:20,325
Speaker 6:  And here is the thing that I noticed at WWC that I've been thinking about

697
00:41:21,605 --> 00:41:24,985
Speaker 6:  in that opening keynote video when they were like start when jumping off

698
00:41:24,985 --> 00:41:28,905
Speaker 6:  the plane. Yeah. And they were like doing the introduction and everyone was

699
00:41:28,905 --> 00:41:32,665
Speaker 6:  wearing their flight suits. Do you remember this vividly ridiculous bombastic

700
00:41:32,665 --> 00:41:32,905
Speaker 6:  opening.

701
00:41:34,575 --> 00:41:38,385
Speaker 6:  They had Mike Rockwell on the plane who's the, he's in charge of the Vision

702
00:41:38,445 --> 00:41:42,265
Speaker 6:  Pro and he turns to the camera with the eyes on and they

703
00:41:42,265 --> 00:41:46,225
Speaker 6:  played it for laughs. The eyes were a joke and it,

704
00:41:47,195 --> 00:41:50,465
Speaker 8:  Those eyes, those eyes aren't shipping anymore. I can tell you that. That

705
00:41:50,465 --> 00:41:54,025
Speaker 6:  That's what, this is what I'm saying the idea, if you remember the

706
00:41:54,215 --> 00:41:58,145
Speaker 6:  initial Vision Pro stuff was like the eyes will place you in

707
00:41:58,145 --> 00:42:02,105
Speaker 6:  the environment. They're very, they were so serious about the eyes.

708
00:42:02,105 --> 00:42:06,065
Speaker 6:  Yeah, so serious. Like dead serious about the eyes. I asked in

709
00:42:06,065 --> 00:42:09,305
Speaker 6:  one of our briefings, 'cause you know the only question I asked anybody during

710
00:42:09,365 --> 00:42:13,265
Speaker 6:  our Vision Pro briefings where have two of you ever

711
00:42:13,265 --> 00:42:17,185
Speaker 6:  looked at each other with the eyes? Like have, have two people ever existed

712
00:42:17,205 --> 00:42:20,105
Speaker 6:  in the same room wearing a Vision pro? And if when you looked at each other

713
00:42:20,105 --> 00:42:22,225
Speaker 6:  with the eyes, did you laugh? And they were like, that's not funny.

714
00:42:23,905 --> 00:42:26,265
Speaker 6:  'cause they were dead serious that this thing would bring you into the environment.

715
00:42:26,265 --> 00:42:29,465
Speaker 6:  They meant it. They were very, yeah, it was sincere in that Apple

716
00:42:30,165 --> 00:42:34,025
Speaker 6:  New age sincerity way. Like when you go to Apple Park and you walk

717
00:42:34,025 --> 00:42:36,745
Speaker 6:  towards the theater, they play new age music at you and they're dead sincere

718
00:42:36,745 --> 00:42:40,505
Speaker 6:  about it. Yes. And I'm like, this makes me feel like I'm in a cult. There's

719
00:42:40,505 --> 00:42:44,105
Speaker 6:  a gap there between like how sincere Apple can be and then the world.

720
00:42:44,565 --> 00:42:48,385
Speaker 6:  And they were super sincere that these eyes would be the thing that

721
00:42:48,385 --> 00:42:49,985
Speaker 6:  enabled you to wear a headset all the time.

722
00:42:50,365 --> 00:42:53,985
Speaker 8:  No, let's be real. They did the Eyes because they wanted to be able to not

723
00:42:53,985 --> 00:42:54,705
Speaker 8:  call it a VR headset.

724
00:42:55,635 --> 00:42:58,425
Speaker 6:  Right. Well so this is what I'm getting at. Yeah. This next version, this

725
00:42:58,425 --> 00:43:02,345
Speaker 6:  cheaper version, they have to decide if this is a VR headset or an AR

726
00:43:02,345 --> 00:43:06,225
Speaker 6:  headset. I will. And playing the Eyes for laughs was the thing

727
00:43:06,225 --> 00:43:07,945
Speaker 6:  that signaled to me, okay, this is gonna be a view,

728
00:43:08,185 --> 00:43:08,665
Speaker 8:  I will bet

729
00:43:10,345 --> 00:43:13,825
Speaker 8:  strongly that there will not be the eyes in any Future Vision Pro

730
00:43:14,015 --> 00:43:15,625
Speaker 8:  product. And I'll tell you why.

731
00:43:15,625 --> 00:43:17,745
Speaker 6:  Yeah. It's just a bunch of costs for reason. Right. Well yes.

732
00:43:17,805 --> 00:43:20,345
Speaker 7:  No one's taking you on that bet. We like our money.

733
00:43:20,585 --> 00:43:22,745
Speaker 6:  I'll take that bet. The, I still, it was a Johnny

734
00:43:22,945 --> 00:43:26,225
Speaker 8:  I thing, I know this from talking to people who worked on the, the Vision

735
00:43:26,325 --> 00:43:29,945
Speaker 8:  Pro. It was Johnny Ive who insisted that the eyes ship,

736
00:43:30,385 --> 00:43:34,345
Speaker 8:  I don't know the exact dollar amount, but it added an absurd amount of cost

737
00:43:34,525 --> 00:43:37,465
Speaker 8:  and manufacturing complex complexity to the Vision Pro

738
00:43:38,295 --> 00:43:38,945
Speaker 8:  because they

739
00:43:38,945 --> 00:43:40,145
Speaker 7:  Had to do that curved glass.

740
00:43:40,375 --> 00:43:42,585
Speaker 6:  Well the curved gra the curved glass was, or that

741
00:43:42,585 --> 00:43:43,385
Speaker 7:  Curved glass in the display

742
00:43:43,385 --> 00:43:46,665
Speaker 8:  Of the side. Right. It was the, the front facing O-L-E-D-I think it's an

743
00:43:46,665 --> 00:43:49,265
Speaker 8:  O-O-L-E-D right. Anyway, with the, with the eyes.

744
00:43:49,745 --> 00:43:51,065
Speaker 6:  Yeah, it are very low eyes. Yeah.

745
00:43:51,065 --> 00:43:54,825
Speaker 8:  It added an absurd amount of cost. No one wanted to ship it except Johnny

746
00:43:54,885 --> 00:43:58,705
Speaker 8:  and his people and they've all left. And so, and they're making

747
00:43:58,865 --> 00:44:02,825
Speaker 8:  a joke about it in keynote videos. So yeah, I don't think, I don't think

748
00:44:02,825 --> 00:44:04,145
Speaker 8:  the eyes are long for this world,

749
00:44:04,445 --> 00:44:07,625
Speaker 6:  But, so if you get rid of that and you're like, you're not now, you cannot

750
00:44:07,685 --> 00:44:11,585
Speaker 6:  be perceived by other people in the world. The thing is a VR headset. Yep.

751
00:44:11,635 --> 00:44:15,505
Speaker 6:  Right? And that to me is concession in one

752
00:44:15,505 --> 00:44:19,385
Speaker 6:  way, right? Like they talked about this thing as ar ar ar, ar,

753
00:44:19,765 --> 00:44:23,665
Speaker 6:  but if you can't wear it with other people like in VR headset and then everything

754
00:44:23,665 --> 00:44:27,625
Speaker 6:  they've announced since is immersive, right?

755
00:44:27,685 --> 00:44:31,365
Speaker 6:  New immersive videos, new experiences, new immersive games and it's like,

756
00:44:31,365 --> 00:44:35,045
Speaker 6:  oh this is becoming a VR headset and a 1500 VR headset. Right now

757
00:44:35,585 --> 00:44:38,725
Speaker 6:  the state of the state of the art, there is a pretty good product, right?

758
00:44:38,725 --> 00:44:41,725
Speaker 6:  Like You can make something great for $1,500 in vr. Yeah.

759
00:44:42,355 --> 00:44:45,165
Speaker 8:  Yeah. And I guess that maybe brings us to Meta, right? Should we, should

760
00:44:45,165 --> 00:44:49,005
Speaker 8:  we go to that? Yeah. So yeah, meta obviously was first to this spot

761
00:44:49,205 --> 00:44:53,085
Speaker 8:  Oculus a long time ago. They restructured Reality

762
00:44:53,085 --> 00:44:56,605
Speaker 8:  Labs, the division that does all of their hardware and

763
00:44:56,755 --> 00:44:59,525
Speaker 8:  horizon that cursed platform.

764
00:45:00,915 --> 00:45:04,005
Speaker 8:  This week I had the scoop and command line. They're forming two new groups.

765
00:45:04,105 --> 00:45:08,005
Speaker 8:  So Reality Labs is now Metaverse and Wearables. So

766
00:45:08,005 --> 00:45:11,725
Speaker 8:  you'll notice there is no VR in those two phrases,

767
00:45:11,725 --> 00:45:15,525
Speaker 8:  right? The the the Rift, I'm sorry, God, look how

768
00:45:15,585 --> 00:45:18,885
Speaker 8:  the Rift, wow. The Quest is now under Metaverse

769
00:45:19,235 --> 00:45:22,845
Speaker 8:  reporting to Vishal Shaw who used to run ads at Instagram and is a software

770
00:45:22,905 --> 00:45:26,805
Speaker 8:  guy. So you basically have the, the thing that created this

771
00:45:26,805 --> 00:45:30,525
Speaker 8:  whole org that the company really bet its future on is a

772
00:45:30,525 --> 00:45:34,285
Speaker 8:  subdivision of a software org. Meanwhile, Wearables,

773
00:45:35,225 --> 00:45:38,845
Speaker 8:  I'm told, is leaning heavily into the Ray bands. I

774
00:45:38,845 --> 00:45:42,565
Speaker 8:  published this internal memo from Boz, the CTO, basically saying that

775
00:45:42,865 --> 00:45:46,485
Speaker 8:  the Ray bands have been a much bigger hit than any of us anticipated.

776
00:45:47,125 --> 00:45:50,005
Speaker 8:  Zuckerberg and others have been kind of alluding to this throughout the year.

777
00:45:50,595 --> 00:45:54,325
Speaker 8:  I've heard that it's somewhere in the ballpark of a million-ish units

778
00:45:54,325 --> 00:45:57,685
Speaker 8:  that they've sold, which is three to four x the first version of the Ray

779
00:45:57,695 --> 00:46:00,725
Speaker 8:  bands. And they're seeing really good retention in the product. So people

780
00:46:00,725 --> 00:46:04,165
Speaker 8:  are actually using it consistently. I have them,

781
00:46:04,725 --> 00:46:08,685
Speaker 8:  I had, I have the AI turned on in them. I wrote in Command Line

782
00:46:08,805 --> 00:46:12,205
Speaker 8:  a few weeks ago. I'm not trying to pat myself on the back, but I will just

783
00:46:12,205 --> 00:46:15,805
Speaker 8:  saying that Meta is quietly winning the AI

784
00:46:15,845 --> 00:46:19,725
Speaker 8:  wearable race here because the Raybans are something that

785
00:46:19,725 --> 00:46:22,845
Speaker 8:  Meta has not done in a while. You could argue

786
00:46:23,895 --> 00:46:27,605
Speaker 8:  maybe ever but shipped a really good consumer product.

787
00:46:28,995 --> 00:46:30,205
Speaker 6:  Yeah, you're, you're a hundred percent right.

788
00:46:30,205 --> 00:46:33,525
Speaker 8:  It's something that, that like normal people, like something that tech people

789
00:46:33,525 --> 00:46:36,805
Speaker 8:  like because it's got the AI and yeah, the AI still hallucinates, but it's

790
00:46:36,875 --> 00:46:40,125
Speaker 8:  very fast and it's right like maybe half the time,

791
00:46:40,615 --> 00:46:44,445
Speaker 8:  right? So like I pointed at a tree in my backyard and I say, what is this

792
00:46:44,445 --> 00:46:48,045
Speaker 8:  tree and how do I water it? It got it right. So like that's not, it's still

793
00:46:48,045 --> 00:46:51,445
Speaker 8:  a demo, right? It's not like I'm, it's not gonna like change the world, but

794
00:46:51,445 --> 00:46:54,325
Speaker 8:  in a couple years You can see where that's going and it's like, oh, if I

795
00:46:54,325 --> 00:46:57,605
Speaker 8:  have reliable visual AI in Ray bands,

796
00:46:58,405 --> 00:47:01,725
Speaker 8:  I may not UI may use my phone less. Which is the reason that is doing all

797
00:47:01,725 --> 00:47:05,685
Speaker 8:  this by the way. So they are putting way more resources

798
00:47:05,685 --> 00:47:09,285
Speaker 8:  behind the RayBan line and they also have these full

799
00:47:09,315 --> 00:47:13,245
Speaker 8:  fledged AR glasses that they've been working on since 2018. Code

800
00:47:13,245 --> 00:47:17,005
Speaker 8:  named Orion, which are insanely expensive. They cost thousands of dollars

801
00:47:17,025 --> 00:47:20,245
Speaker 8:  to make, I have heard from people inside Meta

802
00:47:20,795 --> 00:47:24,725
Speaker 8:  that it's a again, incredible demo. They really feel

803
00:47:24,725 --> 00:47:28,285
Speaker 8:  like they've gotten there on the, on the hardware. They're not sure what

804
00:47:28,285 --> 00:47:31,845
Speaker 8:  the software use cases will be yet. They're doing a lot of reviews of it

805
00:47:31,845 --> 00:47:35,605
Speaker 8:  right now. I expect we'll get a tease of that later this year at Connect

806
00:47:35,605 --> 00:47:39,525
Speaker 8:  their conference. So they have these two lines that

807
00:47:39,525 --> 00:47:42,525
Speaker 8:  they're doing with glasses. They've got the AR glasses, which I leaked the

808
00:47:42,525 --> 00:47:46,445
Speaker 8:  whole roadmap for Meta's Hardware a year or so ago.

809
00:47:46,585 --> 00:47:49,885
Speaker 8:  So we know that the Ray bands are gonna be updated

810
00:47:50,595 --> 00:47:54,405
Speaker 8:  next year with a little heads up display. So think more Google Glass,

811
00:47:54,835 --> 00:47:58,685
Speaker 8:  less full AR Magic Leap. And they're gonna come with a

812
00:47:58,685 --> 00:48:02,205
Speaker 8:  wristband that uses neural interface

813
00:48:02,205 --> 00:48:05,445
Speaker 8:  technology from the startup Mebo called Control Labs

814
00:48:06,105 --> 00:48:09,485
Speaker 8:  to control the display and to control some of the other inputs. 'cause You

815
00:48:09,485 --> 00:48:13,445
Speaker 8:  can also like play podcasts through the, they have dis speakers in

816
00:48:13,445 --> 00:48:17,365
Speaker 8:  the frames, right. That shipping next year that's gonna be

817
00:48:17,565 --> 00:48:21,405
Speaker 8:  interesting. And then the AR glasses, the full fledged ones that are gonna

818
00:48:21,405 --> 00:48:25,285
Speaker 8:  be more expensive and also have the band will be 2027. So

819
00:48:25,285 --> 00:48:29,125
Speaker 8:  this is the path meta is on. They're going full on glasses

820
00:48:29,705 --> 00:48:33,565
Speaker 8:  and really deprioritizing, they hate when I say this, but it

821
00:48:33,565 --> 00:48:37,045
Speaker 8:  feels like that the Quest, because the Quest is still really struggling.

822
00:48:37,045 --> 00:48:39,965
Speaker 8:  They're not really, they have some early product market fit with gaming,

823
00:48:40,025 --> 00:48:43,925
Speaker 8:  but the retention is still not very good. I've heard. And the Quest

824
00:48:43,935 --> 00:48:47,805
Speaker 8:  three, they, they got a lot of really good things about the mixed

825
00:48:47,805 --> 00:48:51,485
Speaker 8:  reality, right? But it just, it's just not, I don't know why I wanna put

826
00:48:51,485 --> 00:48:54,805
Speaker 8:  it on. Whereas the Ray bands, it's like, yeah, I get it. I have audio on

827
00:48:54,805 --> 00:48:58,325
Speaker 8:  all the time. I can take calls, I can take photos and videos and I have AI

828
00:48:58,325 --> 00:49:02,165
Speaker 8:  that is starting to work more and more. And by the way, they look like normal

829
00:49:02,165 --> 00:49:06,085
Speaker 8:  glasses, right? Yeah. So it's just obvious that this is the

830
00:49:06,085 --> 00:49:10,005
Speaker 8:  direction they should go. They seem to now be realizing that and they're

831
00:49:10,005 --> 00:49:13,445
Speaker 8:  putting all their chips there and laying off some people as a result. So

832
00:49:13,445 --> 00:49:14,645
Speaker 8:  that was the news outta meta this week.

833
00:49:14,945 --> 00:49:17,485
Speaker 6:  So what's really interesting about Quest three is the rate of

834
00:49:18,815 --> 00:49:21,925
Speaker 6:  innovation on the Quest three or improvement, I guess on the Quest three

835
00:49:22,635 --> 00:49:25,325
Speaker 6:  skyrocketed after the vision broke came out. Yeah. They're like, oh, we should

836
00:49:25,445 --> 00:49:29,125
Speaker 6:  ship a bunch of these features. Like I guess there's some competition. Like

837
00:49:29,125 --> 00:49:32,805
Speaker 6:  here's, here's travel mode. We took it away, it's back. They

838
00:49:33,045 --> 00:49:35,845
Speaker 6:  increase the fidelity of the pass through, right? Like just stuff that they

839
00:49:35,845 --> 00:49:39,805
Speaker 6:  should have done, but they're still not, it's not winning. And even

840
00:49:39,805 --> 00:49:43,165
Speaker 6:  Apple, if they come out with a 1500 version of the Vision Pro, they still

841
00:49:43,165 --> 00:49:47,085
Speaker 6:  have to make the case for vr, right? Which, but Meta has been

842
00:49:47,085 --> 00:49:50,485
Speaker 6:  trying for a long time. And now if Meta is saying, alright, it's these glass,

843
00:49:50,555 --> 00:49:53,645
Speaker 6:  it's the thing that everyone always knew was the thing, right? It's

844
00:49:53,995 --> 00:49:57,965
Speaker 6:  augmented reality glasses, we're actual light passes through to display

845
00:49:58,265 --> 00:50:01,445
Speaker 6:  and you're looking at actual things instead of screens, and then we can put

846
00:50:01,445 --> 00:50:04,485
Speaker 6:  some information near or around or over those real things.

847
00:50:05,935 --> 00:50:09,805
Speaker 6:  Turns out that's the thing everybody has wanted the whole time. Look, why

848
00:50:09,805 --> 00:50:13,405
Speaker 6:  would Apple continue to boondoggle its way towards the Vision Pro even at

849
00:50:13,405 --> 00:50:14,045
Speaker 6:  $1,500?

850
00:50:14,795 --> 00:50:18,765
Speaker 8:  Well, because they're on this path, right? Apple was working on

851
00:50:18,785 --> 00:50:22,645
Speaker 8:  air glasses like Orion at Meta, and they basically

852
00:50:22,835 --> 00:50:26,805
Speaker 8:  shelved it semi-recently in the last year or two for the headset

853
00:50:27,055 --> 00:50:30,645
Speaker 8:  route. They think that if they can keep iterating on the headsets, they'll

854
00:50:30,645 --> 00:50:33,645
Speaker 8:  figure out glasses eventually. Whereas Meta's like, no, we are going to have

855
00:50:33,645 --> 00:50:37,525
Speaker 8:  glasses out in a few years and Apple's fine. They're always

856
00:50:37,525 --> 00:50:40,965
Speaker 8:  late to this. Like, we all know that. So they'll come in late like they usually

857
00:50:40,965 --> 00:50:44,525
Speaker 8:  do. I don't know if it'll be as late as they were with the Vision Pro, but

858
00:50:44,525 --> 00:50:48,005
Speaker 8:  they're gonna be late and that's fine. But yeah, I think they're, they're

859
00:50:48,005 --> 00:50:51,845
Speaker 8:  betting on productivity and being at home and coding and all these things

860
00:50:51,845 --> 00:50:54,885
Speaker 8:  with Division Pro and Meta's like, no, we want you to lose

861
00:50:55,465 --> 00:50:59,325
Speaker 8:  Meta's goal is to use your phone less because they hate being under Apple

862
00:50:59,345 --> 00:51:03,245
Speaker 8:  and Google's thumb. Whereas Apple is like, let's just extend what we

863
00:51:03,245 --> 00:51:06,005
Speaker 8:  do. And I think there's the big strategic difference.

864
00:51:06,235 --> 00:51:10,205
Speaker 6:  Yeah. Yeah. I'll say that. I have a a, a friend like from the Brooklyn

865
00:51:10,205 --> 00:51:12,925
Speaker 6:  years who in the pandemic became one of those people who like doesn't live

866
00:51:13,125 --> 00:51:15,885
Speaker 6:  anywhere. Like she just lives in beaches.

867
00:51:15,885 --> 00:51:16,925
Speaker 7:  I I know one of those folks. Yeah,

868
00:51:16,995 --> 00:51:18,325
Speaker 6:  It's, she's great. Like, does

869
00:51:18,325 --> 00:51:18,885
Speaker 8:  She work, does she

870
00:51:19,105 --> 00:51:19,525
Speaker 6:  Please,

871
00:51:19,635 --> 00:51:20,885
Speaker 8:  Does she work at Nvidia or what?

872
00:51:21,585 --> 00:51:22,885
Speaker 6:  She does not work in the va

873
00:51:25,545 --> 00:51:29,245
Speaker 6:  and she was the first normal person adopter of the meta glasses

874
00:51:29,685 --> 00:51:33,325
Speaker 6:  with the Ray bands. Like all of her Instagram pictures are a beach taken.

875
00:51:33,325 --> 00:51:36,045
Speaker 6:  They're meta ray bands now. They're great. And You can, You can just see

876
00:51:36,045 --> 00:51:39,125
Speaker 6:  it like, oh, this thing worked, like the thing worked. It looks like she's

877
00:51:39,125 --> 00:51:42,685
Speaker 6:  wearing Wayfairs and all the pictures. Yeah, but they're, it's the meta glasses

878
00:51:43,385 --> 00:51:47,285
Speaker 6:  and like, it, it wor it, it's like so much, there's no chance

879
00:51:48,035 --> 00:51:51,925
Speaker 6:  that any of these people are wearing Vision Pros as they like Nomadically

880
00:51:51,925 --> 00:51:55,125
Speaker 6:  Travel the world or the Quest three. But there's a hundred percent chance

881
00:51:55,395 --> 00:51:58,165
Speaker 6:  that, hey, your camera is now in your sunglasses and you're looking at cool

882
00:51:58,165 --> 00:52:01,565
Speaker 6:  stuff and you wanna share an Instagram is a loop that me can close. I

883
00:52:01,565 --> 00:52:04,405
Speaker 8:  Was at a very fancy tech conference where I'm not even allowed to say the

884
00:52:04,405 --> 00:52:07,205
Speaker 8:  name earlier this week, but it was a lot of tech CEOs and there were several

885
00:52:07,205 --> 00:52:10,845
Speaker 8:  of them wearing the Ray bands, like walking around CEOs of

886
00:52:11,085 --> 00:52:13,685
Speaker 8:  companies that it would be very funny for me to say who they were wearing

887
00:52:13,685 --> 00:52:17,525
Speaker 8:  the Ray bands and you're not supposed to record anything here, but like,

888
00:52:17,525 --> 00:52:21,085
Speaker 8:  they're, they're wearing them and it's like, oh, like, yeah, this is like

889
00:52:21,485 --> 00:52:25,325
Speaker 8:  catching on. It's catching on with tech people, powerful people. And

890
00:52:25,325 --> 00:52:28,445
Speaker 8:  it's catching on with normal people, which is like, that is a circle that

891
00:52:28,445 --> 00:52:32,325
Speaker 8:  Meta has not been able to draw in its entirety of existence

892
00:52:32,325 --> 00:52:33,045
Speaker 8:  as a hardware player.

893
00:52:34,275 --> 00:52:34,565
Speaker 6:  Yeah.

894
00:52:34,585 --> 00:52:37,925
Speaker 7:  How, how good did Jensen look in the glasses? I,

895
00:52:38,005 --> 00:52:38,125
Speaker 6:  I,

896
00:52:38,645 --> 00:52:39,125
Speaker 8:  I can't comment.

897
00:52:42,505 --> 00:52:46,405
Speaker 6:  I'm telling you, I think it VR is interesting. There's stuff in

898
00:52:46,405 --> 00:52:49,885
Speaker 6:  vr like Supernatural, they keep promoting as a hit and there was actually

899
00:52:49,885 --> 00:52:53,525
Speaker 6:  Supernatural just commissioned some study where they claim that waving bats

900
00:52:53,525 --> 00:52:54,885
Speaker 6:  and VR as a strenuous is running.

901
00:52:55,825 --> 00:52:56,605
Speaker 7:  It feels like it.

902
00:52:56,955 --> 00:52:59,605
Speaker 6:  Sure. Like sure. Whatever man. Like

903
00:52:59,745 --> 00:53:00,925
Speaker 7:  To me, yes, agree. If You can

904
00:53:01,105 --> 00:53:05,005
Speaker 6:  You can claim that that fancy Peloton Beat Saber is the same as this

905
00:53:05,265 --> 00:53:08,885
Speaker 6:  as like a, a swift run. Like more power to you. But that's the only thing

906
00:53:08,885 --> 00:53:12,445
Speaker 6:  I have that's sticky. Right. And I, apple doesn't have anything that's sticky

907
00:53:12,445 --> 00:53:16,285
Speaker 6:  yet except for the, the monitors. We'll see I just think Apple

908
00:53:16,425 --> 00:53:20,365
Speaker 6:  moving away. The compromise of the Vision Pro was they built a VR

909
00:53:20,365 --> 00:53:24,085
Speaker 6:  headset with all this pass through in the eyes and stuff to make it closer

910
00:53:24,185 --> 00:53:28,165
Speaker 6:  to ar. And if what they're gonna do is drop the eyes and go head more towards

911
00:53:28,265 --> 00:53:31,805
Speaker 6:  vr, that feels like a turn That is notable because

912
00:53:32,465 --> 00:53:35,525
Speaker 6:  the real platform you wanna talk about the real platform shift has always

913
00:53:35,525 --> 00:53:38,405
Speaker 6:  been ar Yeah, they've, everybody knows this. Tim Cook has talked about that.

914
00:53:38,405 --> 00:53:42,365
Speaker 6:  Yeah. Is being the big platform shift and to go to, we just made a,

915
00:53:42,425 --> 00:53:45,605
Speaker 6:  the best VR headset will be very different. Alright, we've been going, wait,

916
00:53:45,605 --> 00:53:48,365
Speaker 6:  we're way too long. This is outta control. Does this show ever go long? Usually

917
00:53:48,365 --> 00:53:51,325
Speaker 6:  David, I didn't think that ever happened. No. 30. I don't have tears

918
00:57:06,285 --> 00:57:06,445
Speaker 6:  companies.

919
00:57:55,955 --> 00:57:59,555
Speaker 8:  look to as like a demigod, right? And he has been totally quiet.

920
00:57:59,555 --> 00:58:03,115
Speaker 8:  There's been this meme of like, where's Ilya for the last, you know, six,

921
00:58:03,115 --> 00:58:06,515
Speaker 8:  eight months really since the November coup attempt. And

922
00:58:06,905 --> 00:58:10,635
Speaker 8:  yeah, he's back with a company that they will not describe

923
00:58:10,995 --> 00:58:14,435
Speaker 8:  anything about except that they're going to invent super safe a GI,

924
00:58:15,355 --> 00:58:19,275
Speaker 8:  whatever the hell that means. His co-founder is Daniel Gross who used to

925
00:58:19,275 --> 00:58:22,835
Speaker 8:  run AI at Apple, which is interesting and is now I would consider

926
00:58:23,495 --> 00:58:27,155
Speaker 8:  one of, if not the most plugged in venture capitalists in AI who's been

927
00:58:27,225 --> 00:58:29,715
Speaker 8:  raising a ton of money and and deploying it into startups.

928
00:58:31,255 --> 00:58:35,235
Speaker 8:  And they are putting the call out for people to join. And

929
00:58:35,335 --> 00:58:39,195
Speaker 8:  I'm sure we'll be rating ilya's former org OpenAI in

930
00:58:39,195 --> 00:58:43,155
Speaker 8:  very due time. And the key thing here is

931
00:58:43,265 --> 00:58:47,235
Speaker 8:  Ilya saying we are gonna be free from, I forget, I forget the exact

932
00:58:47,235 --> 00:58:50,835
Speaker 8:  wording, but we are gonna be free from the products and management

933
00:58:51,055 --> 00:58:54,235
Speaker 8:  day to day and only focus on long-term

934
00:58:55,185 --> 00:58:58,995
Speaker 8:  aligned a GI research. And we will not release anything until we hit

935
00:58:59,115 --> 00:59:02,355
Speaker 8:  a GI. So it's, this is a, this is a pure

936
00:59:03,035 --> 00:59:06,035
Speaker 8:  research lab, which is what Ilya has been known for his entire career.

937
00:59:06,335 --> 00:59:07,835
Speaker 6:  I'm gonna read this paragraph to you.

938
00:59:07,835 --> 00:59:08,395
Speaker 8:  Yeah. Read it.

939
00:59:08,465 --> 00:59:12,355
Speaker 6:  Because anybody who has ever worked anywhere is, well, I think have the same

940
00:59:12,635 --> 00:59:16,595
Speaker 6:  reaction to this paragraph. Our singular focus means

941
00:59:16,695 --> 00:59:20,475
Speaker 6:  no distraction by management overhead or product cycles. And our business

942
00:59:20,525 --> 00:59:24,085
Speaker 6:  model means safety, security, and progress are all insulated from

943
00:59:24,085 --> 00:59:27,245
Speaker 6:  short-term commercial pressures. I,

944
00:59:28,695 --> 00:59:30,165
Speaker 6:  sorry, I just like what?

945
00:59:30,835 --> 00:59:34,525
Speaker 8:  Well, that the business model is, we just raised an

946
00:59:34,725 --> 00:59:37,845
Speaker 8:  ungodly amount of money and we're gonna keep raising ungodly amounts of money.

947
00:59:37,905 --> 00:59:41,645
Speaker 6:  So it, but that means you don't have a business model, right? Yeah.

948
00:59:42,395 --> 00:59:45,765
Speaker 6:  Like that's how you get to do these things. Yes. Is like, you don't,

949
00:59:46,385 --> 00:59:48,805
Speaker 6:  you don't have the pre you don't, like you don't have the pressure of a business.

950
00:59:49,305 --> 00:59:49,525
Speaker 6:  Yes.

951
00:59:49,695 --> 00:59:53,485
Speaker 7:  Isn't that just all a really pointed jab at Sam

952
00:59:53,645 --> 00:59:53,845
Speaker 7:  Altman?

953
00:59:54,025 --> 00:59:56,205
Speaker 6:  But I, but yes. Or short,

954
00:59:56,205 --> 00:59:59,565
Speaker 7:  Right? But obviously I figured it was more that than actual, like in this

955
00:59:59,565 --> 01:00:03,445
Speaker 7:  case I was assuming that he was going less like I'm here to make money. 'cause

956
01:00:03,445 --> 01:00:06,765
Speaker 7:  he's made it very clear he doesn't care about money. and more like I'm here

957
01:00:06,785 --> 01:00:10,645
Speaker 7:  to like say I, I don't like what Sam Altman did, and so I

958
01:00:10,645 --> 01:00:11,765
Speaker 7:  wanna make a company where we're

959
01:00:12,095 --> 01:00:15,685
Speaker 6:  Stuff. All I'm saying is I watch Max and two of her 6-year-old friends from

960
01:00:15,685 --> 01:00:19,645
Speaker 6:  kindergarten try to play lemonade stand and they are distracted by

961
01:00:19,645 --> 01:00:23,525
Speaker 6:  management overhead and product cycles. And their business model means they

962
01:00:23,525 --> 01:00:26,805
Speaker 6:  often compromise in safety, security and progress. Like

963
01:00:27,195 --> 01:00:31,085
Speaker 6:  there's just like no way You can put any group of

964
01:00:31,085 --> 01:00:34,725
Speaker 6:  people together and not have management overhead. So it is, it is

965
01:00:35,005 --> 01:00:38,685
Speaker 6:  not possible, especially if you're trying to hire the

966
01:00:38,715 --> 01:00:42,525
Speaker 6:  most highly paid, sought after talent that exists

967
01:00:42,625 --> 01:00:46,045
Speaker 6:  in technology today is And that's what I mean, it's like I don't, I read

968
01:00:46,045 --> 01:00:49,445
Speaker 6:  this paragraph, I'm like, this doesn't, you can't do this. This is the VC

969
01:00:49,445 --> 01:00:50,165
Speaker 6:  promise. You can't keep

970
01:00:50,305 --> 01:00:54,125
Speaker 8:  The VC math here is simple. We will give you whatever you want. And

971
01:00:54,125 --> 01:00:57,405
Speaker 8:  when you invent a GI, it will change everything. It will be the new fire

972
01:00:57,865 --> 01:01:01,525
Speaker 8:  and we'll figure it out. That's, I guarantee you that's the extent of the

973
01:01:01,525 --> 01:01:02,445
Speaker 8:  diligence. I

974
01:01:02,445 --> 01:01:05,725
Speaker 7:  Was, I was gonna ask like, where, where is the money coming from to recruit

975
01:01:05,745 --> 01:01:08,045
Speaker 7:  all of these very highly paid people?

976
01:01:08,045 --> 01:01:10,925
Speaker 8:  Well, they won't say where they've raised money from. So if you know, let

977
01:01:10,925 --> 01:01:14,845
Speaker 8:  me know. But if you're listening to this, I wanna find out Daniel is, they're,

978
01:01:14,845 --> 01:01:18,685
Speaker 8:  they're all super wealthy. Yeah. And they will have no trouble raising

979
01:01:18,925 --> 01:01:22,605
Speaker 8:  whatever they want because when you co-founded OpenAI, You can raise

980
01:01:22,805 --> 01:01:23,165
Speaker 8:  whatever you want.

981
01:01:23,265 --> 01:01:26,925
Speaker 6:  And then in particular participated in the notable management overhead

982
01:01:27,265 --> 01:01:31,045
Speaker 6:  of doing a huge coup Yes. Against the CEO. Yes.

983
01:01:31,525 --> 01:01:32,245
Speaker 7:  A failed coup.

984
01:01:33,115 --> 01:01:37,005
Speaker 6:  It's like your track record of not having management overhead is very bad.

985
01:01:37,755 --> 01:01:41,605
Speaker 8:  Also, like a question I have is, what is safe A GII keep hearing

986
01:01:41,605 --> 01:01:42,605
Speaker 8:  this and it's so

987
01:01:42,605 --> 01:01:43,725
Speaker 6:  Intelligence, it's not a GI

988
01:01:43,785 --> 01:01:46,365
Speaker 8:  Or super intelligence, which is actually now the new word for what comes

989
01:01:46,365 --> 01:01:50,285
Speaker 8:  after a GI apparently is the thing I keep hearing. And so

990
01:01:50,425 --> 01:01:53,325
Speaker 8:  we keep inventing new terms because they realize like the terms they had,

991
01:01:53,345 --> 01:01:56,005
Speaker 8:  no one understands or they don't actually even understand what they are.

992
01:01:57,235 --> 01:02:00,405
Speaker 8:  What is safe A JI what does this mean? Like Ilya's whole thing at OpenAI

993
01:02:00,425 --> 01:02:04,245
Speaker 8:  was alignment, right? He set up this alignment group that was gonna get 20%

994
01:02:04,245 --> 01:02:08,005
Speaker 8:  of open AI's compute budget. Apparently they never did. That led to

995
01:02:08,165 --> 01:02:11,965
Speaker 8:  a lot of infighting and I think per, you know, helped him try to kick

996
01:02:12,025 --> 01:02:15,525
Speaker 8:  Sam out. But I always hear this and I'm like

997
01:02:15,645 --> 01:02:19,525
Speaker 8:  aligning for who and under what values are we aligning

998
01:02:20,005 --> 01:02:21,165
Speaker 8:  and they don't seem to have an answer.

999
01:02:21,465 --> 01:02:24,925
Speaker 7:  And then that's been like a consistent problem with ai, right? Is is that

1000
01:02:24,925 --> 01:02:28,805
Speaker 7:  they're always like, yeah, this is gonna be for everyone, but we

1001
01:02:28,805 --> 01:02:32,365
Speaker 7:  only trained it with these certain data sets. We only trained it using like

1002
01:02:32,385 --> 01:02:36,285
Speaker 7:  the pre-established knowledge of the certain people creating it. So

1003
01:02:36,285 --> 01:02:40,125
Speaker 7:  it's actually fairly insular and doesn't work for everyone. And there are

1004
01:02:40,245 --> 01:02:42,725
Speaker 7:  enormous edge cases every time we deploy it.

1005
01:02:42,915 --> 01:02:43,205
Speaker 6:  Yeah.

1006
01:02:43,225 --> 01:02:46,405
Speaker 8:  And you know, as Neela and I uncovered when we were reporting on the coup

1007
01:02:46,405 --> 01:02:50,325
Speaker 8:  attempts, like this gets kind of portrayed in the media as this,

1008
01:02:50,325 --> 01:02:53,925
Speaker 8:  you know, acceleration versus deceleration almost like religious

1009
01:02:55,075 --> 01:02:58,685
Speaker 8:  faction war that's happening. And I do think there's part of that. I do think

1010
01:02:58,825 --> 01:03:02,765
Speaker 8:  IA is almost like religious and is like fervor here. But a lot of

1011
01:03:02,765 --> 01:03:06,645
Speaker 8:  this is just like boring company stuff. Business stuff. Yes.

1012
01:03:06,675 --> 01:03:10,245
Speaker 8:  It's like you wanted budget, you didn't get it, you got layered,

1013
01:03:10,355 --> 01:03:14,125
Speaker 8:  therefore you don't wanna work there anymore. OpenAI just hired a

1014
01:03:14,285 --> 01:03:18,165
Speaker 8:  CPO who used to run product at Instagram and a CFO who was the

1015
01:03:18,245 --> 01:03:21,925
Speaker 8:  CEO of Nextdoor a public company. It's very clear where OpenAI is going.

1016
01:03:21,925 --> 01:03:25,365
Speaker 8:  They're gonna, IPO they want build a massive business. They're going to get

1017
01:03:25,365 --> 01:03:28,325
Speaker 8:  rid of being a nonprofit. It's very clear. And some of your, I

1018
01:03:28,325 --> 01:03:30,925
Speaker 6:  Mean this week made some noises about not being a nonprofit, right?

1019
01:03:31,185 --> 01:03:35,085
Speaker 8:  And so if that's the direction they're going, you're ilia. It's like you

1020
01:03:35,085 --> 01:03:38,165
Speaker 8:  just want to do research, right? And that's where a lot of these researchers

1021
01:03:38,165 --> 01:03:42,085
Speaker 8:  still are is they just wanna do cutting edge frontier model development.

1022
01:03:42,305 --> 01:03:45,885
Speaker 8:  And this is him saying, come here, we'll do it. And

1023
01:03:46,725 --> 01:03:50,365
Speaker 8:  I, we should also note this is now the third offshoot of OpenAI that was,

1024
01:03:50,825 --> 01:03:54,445
Speaker 8:  has been created to create safer AI from

1025
01:03:54,745 --> 01:03:57,365
Speaker 8:  OpenAI whose mission is also to create safe ai.

1026
01:03:57,555 --> 01:03:58,685
Speaker 7:  It's not a great look.

1027
01:03:59,155 --> 01:04:02,885
Speaker 8:  Yeah. So you have philanthropic for I would, I would actually put Xai in

1028
01:04:02,885 --> 01:04:06,645
Speaker 8:  this camp. Elon was the co-founder of an of OpenAI. Now you have

1029
01:04:07,125 --> 01:04:10,965
Speaker 8:  SSI, which is also, I feel like it's a medical term or something. What am

1030
01:04:10,965 --> 01:04:12,365
Speaker 8:  I Yeah, yeah.

1031
01:04:12,445 --> 01:04:15,645
Speaker 7:  I got this once. It was, it was, you take a pill for it. It's fine.

1032
01:04:17,665 --> 01:04:19,245
Speaker 6:  Sun, Neo ozempic. SSI. Yeah.

1033
01:04:21,205 --> 01:04:24,285
Speaker 6:  I, look, I just, it's great that everybody thinks they can accomplish these

1034
01:04:24,285 --> 01:04:27,285
Speaker 6:  things. I actually think it's unclear at all

1035
01:04:27,795 --> 01:04:31,725
Speaker 6:  whether Transformers and LLMs can ever

1036
01:04:31,985 --> 01:04:35,805
Speaker 6:  do the things Yeah. That people want them to do. And in fact, over

1037
01:04:35,805 --> 01:04:39,405
Speaker 6:  time it has been more and more clear that perhaps they cannot Yeah. And so

1038
01:04:39,405 --> 01:04:43,245
Speaker 6:  what I'm dying to know is if SSI or any of the other companies that think

1039
01:04:43,245 --> 01:04:47,165
Speaker 6:  they can build a GI if they have another idea. Yeah. Because if the

1040
01:04:47,165 --> 01:04:51,045
Speaker 6:  only idea is we're gonna make a slightly better LLMI actually

1041
01:04:51,045 --> 01:04:53,765
Speaker 6:  don't I, there I'm not, you're not gonna make, you're not worried about whether

1042
01:04:53,765 --> 01:04:56,085
Speaker 6:  it'll be safe. 'cause it's not gonna be super intelligent.

1043
01:04:56,545 --> 01:05:00,445
Speaker 7:  I'm just why are people trying to build an a GI like these people are

1044
01:05:00,495 --> 01:05:02,205
Speaker 7:  nerds. They, they, they watch movies. Well,

1045
01:05:02,205 --> 01:05:04,085
Speaker 6:  That's why you just answered your own question. No, but they,

1046
01:05:04,085 --> 01:05:07,845
Speaker 7:  They watch the movies, they play the games. Like this man is setting out

1047
01:05:07,985 --> 01:05:11,885
Speaker 7:  to be the, the audio clips you hear when you're walking through

1048
01:05:11,885 --> 01:05:15,725
Speaker 7:  the ruins a thousand years later in a video game of the guy being like,

1049
01:05:15,805 --> 01:05:18,725
Speaker 7:  I don't know what's happening. Why is my AI going and killing the world?

1050
01:05:18,755 --> 01:05:22,365
Speaker 7:  Yeah. Like that's what he's setting himself up for if he was

1051
01:05:22,645 --> 01:05:26,525
Speaker 7:  actually successful in his grand ambitions. And I'm just like, why?

1052
01:05:26,635 --> 01:05:30,245
Speaker 8:  Because the industry thinks that we, they have invented a new form of

1053
01:05:30,245 --> 01:05:34,125
Speaker 8:  intelligence and that hasn't happened before. Okay. And so it's,

1054
01:05:34,605 --> 01:05:37,285
Speaker 8:  I think the natural progression of this is that it's all smarter than us

1055
01:05:37,385 --> 01:05:41,205
Speaker 8:  and replaces us. and it seems inevitable to the people

1056
01:05:41,405 --> 01:05:45,165
Speaker 8:  building this stuff. What we see is like chat GPT can't get a math

1057
01:05:45,485 --> 01:05:48,485
Speaker 8:  equation. Right? Right. Yeah. That's like what we see, what they're seeing.

1058
01:05:48,825 --> 01:05:52,485
Speaker 8:  We gotta keep in mind like they gate this stuff like GPT-4 was in

1059
01:05:52,485 --> 01:05:55,565
Speaker 8:  existence for like over a year before OpenAI actually deployed it in the

1060
01:05:55,565 --> 01:05:59,085
Speaker 8:  world. So they're seeing things we're not, I'm not gonna like give them too

1061
01:05:59,085 --> 01:06:02,845
Speaker 8:  much of the credit there, but I do think there's, you know,

1062
01:06:02,865 --> 01:06:06,765
Speaker 8:  Jan Koon, who's the chief AI researcher at Meta has been saying

1063
01:06:06,875 --> 01:06:10,605
Speaker 8:  what you just said in Eli, that LLMs are not the path to a GI. So his group

1064
01:06:10,605 --> 01:06:14,445
Speaker 8:  is actually working on what comes after the transformer. What, what is the

1065
01:06:14,445 --> 01:06:18,285
Speaker 8:  architecture we need? Because these are parrots, right?

1066
01:06:18,295 --> 01:06:22,245
Speaker 8:  These LLMs are are fancy prediction engines. Right. And you, you don't

1067
01:06:22,245 --> 01:06:23,725
Speaker 8:  really unlock intelligence that way.

1068
01:06:24,315 --> 01:06:28,205
Speaker 6:  Yeah, no, I mean I, I, yesterday I assigned our futures team to write languages,

1069
01:06:28,205 --> 01:06:32,045
Speaker 6:  not intelligence. Like I, I feel like we just need to say some very

1070
01:06:32,405 --> 01:06:36,165
Speaker 6:  explicit declarative sentences that are true. Like, dogs are very

1071
01:06:36,165 --> 01:06:39,725
Speaker 6:  intelligent and cannot generate a PowerPoint the way that they like, you

1072
01:06:39,725 --> 01:06:42,525
Speaker 6:  know what I mean? Like, we're getting confused about what intelligence is.

1073
01:06:42,525 --> 01:06:46,445
Speaker 6:  At a very high level, I'm, I've been lately thinking a

1074
01:06:46,445 --> 01:06:50,245
Speaker 6:  lot about the, the Steve Jobs slide of technology and liberal arts.

1075
01:06:51,145 --> 01:06:54,165
Speaker 6:  You know, it's like, oh, you guys forgot about this whole street, this whole

1076
01:06:54,205 --> 01:06:58,125
Speaker 6:  street's missing from this conversation right now. and

1077
01:06:58,125 --> 01:07:01,205
Speaker 6:  it, there's just a piece of this puzzle where it You can bet on this stuff

1078
01:07:01,305 --> 01:07:04,845
Speaker 6:  to get better at an infinite rate. But like most curves level off.

1079
01:07:05,345 --> 01:07:08,645
Speaker 6:  and it is unclear to me what actually makes things intelligent that said,

1080
01:07:08,765 --> 01:07:11,525
Speaker 6:  well, you know, good luck. Good luck building your company with no management

1081
01:07:11,685 --> 01:07:15,645
Speaker 6:  overhead websites. Come on decoder. If you pull that off, boy do I

1082
01:07:15,645 --> 01:07:19,165
Speaker 6:  have a podcast for you? Right. I, I spent an hour talking about management

1083
01:07:19,365 --> 01:07:23,045
Speaker 6:  overhead every week with people. But like, it turns out, it always like three

1084
01:07:23,345 --> 01:07:27,205
Speaker 6:  6-year-old girls have management overhead. I, I promise you maybe more

1085
01:07:27,205 --> 01:07:28,285
Speaker 6:  than any company in America,

1086
01:07:32,605 --> 01:07:34,885
Speaker 6:  speaking of confounding decisions by AI companies,

1087
01:07:36,175 --> 01:07:40,125
Speaker 6:  perplexity started out so hot, man, they were

1088
01:07:40,125 --> 01:07:42,845
Speaker 6:  the ones, right? You can pick any model. You can ask these questions and

1089
01:07:42,865 --> 01:07:46,845
Speaker 6:  it was better searching Google. and it turns out, boy, have they been

1090
01:07:46,845 --> 01:07:48,765
Speaker 6:  awful shady along the way? What is going on here?

1091
01:07:49,275 --> 01:07:52,525
Speaker 8:  Well, yeah. So perplexity, for those who don't know, is trying to

1092
01:07:53,265 --> 01:07:56,845
Speaker 8:  unseat Google with an AI search engine. It's actually a

1093
01:07:57,075 --> 01:08:00,725
Speaker 8:  good product most of the time I would say that I like their AI

1094
01:08:00,725 --> 01:08:04,525
Speaker 8:  answers more than Google's on average. And

1095
01:08:04,525 --> 01:08:06,805
Speaker 8:  now we kind of know why it's 'cause they're stealing a bunch of content.

1096
01:08:08,105 --> 01:08:08,325
Speaker 8:  And

1097
01:08:10,035 --> 01:08:13,965
Speaker 8:  yeah, so Wired did some good reporting and found out that they

1098
01:08:13,965 --> 01:08:17,805
Speaker 8:  were going around their crawler blocker. I've

1099
01:08:17,805 --> 01:08:21,605
Speaker 8:  heard rumors that Perplexity has been doing this at an even grander scale

1100
01:08:21,915 --> 01:08:25,445
Speaker 8:  with even much larger corpuses of data.

1101
01:08:26,745 --> 01:08:30,685
Speaker 8:  And this is the, this is the OpenAI playbook on steroids a little bit, which

1102
01:08:30,685 --> 01:08:34,325
Speaker 8:  is just run fast and beg for forgiveness. And

1103
01:08:35,585 --> 01:08:38,805
Speaker 8:  that's what they're doing. They're basically going around people who have

1104
01:08:38,805 --> 01:08:41,925
Speaker 8:  said, no, you are not allowed to summarize this in doing it anyway.

1105
01:08:42,385 --> 01:08:45,925
Speaker 6:  And the way they're doing it is both shady and hilarious.

1106
01:08:46,825 --> 01:08:50,765
Speaker 6:  So they publish some IP ranges of their crawlers

1107
01:08:50,765 --> 01:08:54,525
Speaker 6:  and they publish the name of their crawler. So poor robots

1108
01:08:55,025 --> 01:08:58,765
Speaker 6:  txt a text file that should not bear the weight of all

1109
01:08:58,765 --> 01:09:02,045
Speaker 6:  intellectual property law on the internet, but which currently does You can

1110
01:09:02,045 --> 01:09:05,965
Speaker 6:  set it to disallow these crawlers. You can set it to disallow GPT bot,

1111
01:09:06,015 --> 01:09:09,525
Speaker 6:  which is Open Eyes. There's Google Bot plus I think. So there's Google Bot,

1112
01:09:09,525 --> 01:09:13,045
Speaker 6:  which is the regular search index. And there's the other one which is Google's

1113
01:09:13,105 --> 01:09:16,805
Speaker 6:  AI crawler. I feel like I need to disclose here. We used to

1114
01:09:17,085 --> 01:09:20,685
Speaker 6:  disclose GPT bot at Vox Media and then our company signed a deal

1115
01:09:21,305 --> 01:09:24,135
Speaker 6:  and now we don't. There you go. There's a disclosure

1116
01:09:25,435 --> 01:09:28,575
Speaker 6:  and you could just, you could disallow perplexity. And on top of that, they

1117
01:09:28,575 --> 01:09:31,535
Speaker 6:  did this thing where like, here are IP ranges. Like if you don't trust robots,

1118
01:09:31,715 --> 01:09:35,455
Speaker 6:  You can decide these ips. And then what Wired found, and then the folks at

1119
01:09:35,475 --> 01:09:39,375
Speaker 6:  Max Stories, which is a great indie Apple site, they found that

1120
01:09:39,375 --> 01:09:42,975
Speaker 6:  their content was showing up anyway in perplexity.

1121
01:09:43,395 --> 01:09:45,935
Speaker 6:  And then the investigations basically revealed these IP ranges were fake

1122
01:09:45,935 --> 01:09:49,575
Speaker 6:  and the crawler was fake. And they were setting up headless Windows 10

1123
01:09:49,575 --> 01:09:50,095
Speaker 6:  machines

1124
01:09:51,715 --> 01:09:55,695
Speaker 6:  to browse the web. Yes. Like, you know, those pictures of like the, the

1125
01:09:55,695 --> 01:09:59,295
Speaker 6:  streaming fraud farms where hot dogs are just swiping Android phones.

1126
01:10:00,465 --> 01:10:04,275
Speaker 6:  There's your VC money, I love it. Just swiping away

1127
01:10:04,295 --> 01:10:05,355
Speaker 6:  on a Windows 10 browser.

1128
01:10:05,825 --> 01:10:08,195
Speaker 8:  Just imagine if those Windows machines had a recall.

1129
01:10:10,905 --> 01:10:14,875
Speaker 6:  It's just like crazy. And you know, and the CEO for Plexus has talked a lot

1130
01:10:14,875 --> 01:10:18,595
Speaker 6:  of shit about Google. Like why are these results so bad? And like Google,

1131
01:10:19,375 --> 01:10:22,995
Speaker 6:  for better or worse is trying to play by the rules.

1132
01:10:23,105 --> 01:10:26,595
Speaker 6:  It's still pissing everybody off. But they're attempting to play by the rules.

1133
01:10:26,615 --> 01:10:27,515
Speaker 6:  The publishers are laid

1134
01:10:27,515 --> 01:10:30,955
Speaker 8:  Out. They have to, I mean, Google is under consent decrees. They're a massively

1135
01:10:31,025 --> 01:10:34,835
Speaker 8:  regulated mega cap tech company. So they can't do this kind of stuff.

1136
01:10:35,185 --> 01:10:38,875
Speaker 6:  Yeah. And here's a startup being like, what if we lied? Yeah.

1137
01:10:38,905 --> 01:10:42,115
Speaker 6:  What We just didn't do any of that. And to be clear, I keep making this comparison.

1138
01:10:42,115 --> 01:10:46,075
Speaker 6:  You call it the opening AI playbook. This was the Google playbook 20 years

1139
01:10:46,075 --> 01:10:49,595
Speaker 6:  ago. Yep. Right. Google just allowed all of

1140
01:10:49,635 --> 01:10:53,115
Speaker 6:  Viacom's content to be on YouTube. Google indexed every book in the world

1141
01:10:53,115 --> 01:10:56,915
Speaker 6:  without permission. Google did image search, which it got sued over and it

1142
01:10:56,915 --> 01:11:00,875
Speaker 6:  won all those lawsuits because those products were new. So they

1143
01:11:00,875 --> 01:11:03,435
Speaker 6:  would show up in court and be like, judge YouTube. And the judge would be

1144
01:11:03,435 --> 01:11:07,395
Speaker 6:  like, YouTube's sick. And Viacom would be like, our marketing staffers are

1145
01:11:07,395 --> 01:11:11,115
Speaker 6:  using this anyway because it's sick. Google Image, image Church.

1146
01:11:11,255 --> 01:11:14,685
Speaker 6:  So useful. It's concoct in it's ne nothing like this has ever existed before.

1147
01:11:14,685 --> 01:11:18,045
Speaker 6:  Let's concoct some reasoning to allow Google to take all this stuff for free.

1148
01:11:18,315 --> 01:11:22,205
Speaker 6:  Because the theory is that you'll go buy the image

1149
01:11:22,205 --> 01:11:25,525
Speaker 6:  from Getty once you found it on Google. It's unclear what that theory really

1150
01:11:25,525 --> 01:11:26,365
Speaker 6:  came out to in the end. I

1151
01:11:26,445 --> 01:11:26,925
Speaker 7:  I never did that.

1152
01:11:27,715 --> 01:11:31,685
Speaker 6:  Yeah. Alex is not like I opened a Getty library the next day,

1153
01:11:32,605 --> 01:11:35,845
Speaker 6:  a Getty account the next day. But that, those are the theories back then.

1154
01:11:35,845 --> 01:11:38,245
Speaker 6:  Google Books that might have worked. Right? You can search all the books.

1155
01:11:38,245 --> 01:11:42,125
Speaker 6:  Maybe you'll actually buy the book in the end. Did it work out? Totally.

1156
01:11:42,195 --> 01:11:45,565
Speaker 6:  Well, I, I don't know. But those are the theories back then because the products

1157
01:11:45,565 --> 01:11:49,085
Speaker 6:  were so new, the experiences were so new. Perplexity is just like, yeah,

1158
01:11:49,085 --> 01:11:52,685
Speaker 6:  we stole a bunch of stuff and what we've delivered is an incremental improvement

1159
01:11:52,685 --> 01:11:55,925
Speaker 6:  on Google search. And I just, I don't think that's gonna win this time.

1160
01:11:57,725 --> 01:12:01,715
Speaker 8:  Maybe I, I am shocked at how little the people inside these

1161
01:12:01,855 --> 01:12:05,715
Speaker 8:  AI companies and their investors, again, I was just at a very fancy

1162
01:12:05,715 --> 01:12:08,995
Speaker 8:  tech conference with a lot of these people, how little they actually care

1163
01:12:08,995 --> 01:12:12,635
Speaker 8:  about this problem. How it is really just like, this is a, this is a bunch

1164
01:12:12,635 --> 01:12:15,995
Speaker 8:  of speed bumps that we'll go over to getting where we're gonna go.

1165
01:12:16,655 --> 01:12:19,475
Speaker 8:  And maybe it's because of the, the, the fact that Google is Google Yeah.

1166
01:12:19,475 --> 01:12:22,235
Speaker 8:  And they did all this 20 years ago and they're a trillion multi-trillion

1167
01:12:22,235 --> 01:12:23,995
Speaker 8:  dollar company. The,

1168
01:12:24,195 --> 01:12:25,555
Speaker 7:  I think's, I think to

1169
01:12:25,565 --> 01:12:26,475
Speaker 8:  Shows. Yeah,

1170
01:12:27,035 --> 01:12:30,235
Speaker 7:  I was gonna say, I think it's back to what Neli said about liberal arts education.

1171
01:12:30,265 --> 01:12:33,555
Speaker 7:  They just don't have any like historical context. So they're like, yeah,

1172
01:12:33,555 --> 01:12:34,115
Speaker 7:  this is gonna work.

1173
01:12:34,705 --> 01:12:36,395
Speaker 6:  They didn't take any history classes at Stanford.

1174
01:12:36,595 --> 01:12:38,435
Speaker 7:  I didn't, I don't need to know about Google. This is fine.

1175
01:12:38,895 --> 01:12:41,515
Speaker 8:  No, these are systems people. Yeah. They don't think in terms of

1176
01:12:42,565 --> 01:12:46,515
Speaker 8:  these, the values that we get mad about when it's like, Hey, I, I made

1177
01:12:46,555 --> 01:12:49,715
Speaker 8:  a thing, I charged people for it and you took it.

1178
01:12:50,135 --> 01:12:53,995
Speaker 8:  That's not okay with, with intellectual property,

1179
01:12:53,995 --> 01:12:57,355
Speaker 8:  with writing and stuff like that for some reason. It's just not a thing that

1180
01:12:57,755 --> 01:13:01,115
Speaker 8:  a lot of tech people that I know, it doesn't register for them.

1181
01:13:01,875 --> 01:13:02,755
Speaker 8:  I don't know why. Right.

1182
01:13:02,785 --> 01:13:06,755
Speaker 7:  Because they didn't take liberal arts s like,

1183
01:13:06,755 --> 01:13:08,115
Speaker 7:  they're like, what is an English class?

1184
01:13:08,955 --> 01:13:11,835
Speaker 6:  I want to spend more time. I don't have, I don't have a view of this. I I

1185
01:13:11,835 --> 01:13:15,235
Speaker 6:  haven't locked this into my brain yet. So people have thoughts. Send them

1186
01:13:15,235 --> 01:13:18,955
Speaker 6:  to me. 'cause I, I need the help, but I can feel this weird shift that

1187
01:13:19,065 --> 01:13:22,625
Speaker 6:  I've been calling the copyright apocalypse because some people

1188
01:13:22,775 --> 01:13:26,145
Speaker 6:  care about it a lot now more than ever before

1189
01:13:26,645 --> 01:13:30,625
Speaker 6:  the, this is mine. And you took it between influencers on

1190
01:13:30,625 --> 01:13:34,385
Speaker 6:  social media platforms rages out of control now,

1191
01:13:34,835 --> 01:13:38,265
Speaker 6:  right? Yeah. I'm a Taylor Schiff fan. And those three notes of a song

1192
01:13:38,635 --> 01:13:42,385
Speaker 6:  sound like three notes in Olivia Rodrigo fan. We have to do fandom war

1193
01:13:42,445 --> 01:13:46,265
Speaker 6:  now. Yeah. Until the music industry dos out. Writing credits

1194
01:13:46,605 --> 01:13:50,535
Speaker 6:  is a real thing that happens. Like at, at the

1195
01:13:50,815 --> 01:13:54,375
Speaker 6:  craziest level, this is a real thing that happens and that is copyright

1196
01:13:54,675 --> 01:13:58,655
Speaker 6:  maxism by really young people. And then over here you have nobody

1197
01:13:58,725 --> 01:14:01,615
Speaker 6:  owns anything. It's just something I should get it for free. And those things

1198
01:14:01,615 --> 01:14:05,135
Speaker 6:  are just in tension in a way that I, it feels unresolved

1199
01:14:05,675 --> 01:14:09,615
Speaker 6:  and un potentially unresolvable. Like you can't, you

1200
01:14:09,615 --> 01:14:12,030
Speaker 6:  can't have a bunch of YouTubers being like, I, I made this YouTube video

1201
01:14:12,030 --> 01:14:15,925
Speaker 6:  first. And anybody who even makes a video that looks like it is stealing

1202
01:14:16,035 --> 01:14:19,125
Speaker 6:  from me right next to everything in the internet should be free.

1203
01:14:19,495 --> 01:14:23,005
Speaker 8:  Maybe that's it. Maybe there's not a lot of love for media companies, as

1204
01:14:23,005 --> 01:14:26,805
Speaker 8:  we all know. And the fact that it's scraping wired or The Verge or whatever

1205
01:14:26,805 --> 01:14:30,325
Speaker 8:  is like whatever, they're fine. But when it's like your favorite influencer,

1206
01:14:30,395 --> 01:14:33,805
Speaker 8:  your favorite creator Yeah. And they're on their feed going, Hey, like this

1207
01:14:33,815 --> 01:14:37,685
Speaker 8:  model, don't use it because they trained on my stuff. I

1208
01:14:37,685 --> 01:14:40,925
Speaker 8:  do think you're right. I think people feel a lot more viscerally about that

1209
01:14:41,115 --> 01:14:44,245
Speaker 8:  than they do. Oh, like companies, right? I think it's when it becomes the

1210
01:14:44,245 --> 01:14:44,525
Speaker 8:  individual

1211
01:14:45,225 --> 01:14:48,725
Speaker 6:  And, and again, what Google was up against, what Napster was up against

1212
01:14:49,145 --> 01:14:52,885
Speaker 6:  was the big bad, right? Like the only face of the anti

1213
01:14:52,885 --> 01:14:56,645
Speaker 6:  Napster movement was Lars Ulrich for Metallica. And

1214
01:14:56,825 --> 01:14:59,605
Speaker 6:  you remember the flash that did not work for him. But really the face was

1215
01:14:59,605 --> 01:15:03,325
Speaker 6:  like Hollywood in the music industry. And no one ever feels

1216
01:15:03,425 --> 01:15:06,725
Speaker 6:  any sympathy towards Hollywood in the music industry. Like Hollywood could

1217
01:15:06,765 --> 01:15:09,565
Speaker 6:  be like, you wouldn't steal a cigarette. Like whatever they would say you

1218
01:15:09,565 --> 01:15:11,805
Speaker 6:  wouldn't, what was it? You wouldn't download a, you wouldn't,

1219
01:15:11,805 --> 01:15:13,005
Speaker 7:  You wouldn't download a car.

1220
01:15:13,295 --> 01:15:16,485
Speaker 6:  Right? So Hollywood can put up the banners and say you wouldn't download

1221
01:15:16,525 --> 01:15:19,565
Speaker 6:  a car. And everyone's like, screw you. I I'm super gonna download a movie.

1222
01:15:19,755 --> 01:15:22,925
Speaker 6:  Yeah. And if I could download a car, I would, that'd be sick. Yeah.

1223
01:15:23,815 --> 01:15:27,005
Speaker 6:  Rules. Like I've had this 3D printer for years not making cars,

1224
01:15:28,785 --> 01:15:32,405
Speaker 6:  but they were the big bats. Influencers are like people, like

1225
01:15:32,765 --> 01:15:35,965
Speaker 6:  influencers, like people, you know what I mean? Influencers are actual people

1226
01:15:35,965 --> 01:15:39,605
Speaker 6:  that you have pair social relationships with. Yeah. And like there's something

1227
01:15:39,605 --> 01:15:43,125
Speaker 6:  very emotional that is not resolved and it's gonna come to a head. 'cause

1228
01:15:43,185 --> 01:15:46,965
Speaker 6:  you know, the next story we have on our list here is a bunch

1229
01:15:47,105 --> 01:15:50,565
Speaker 6:  of social networks that want to introduce AI in various ways.

1230
01:15:51,085 --> 01:15:53,845
Speaker 6:  Alex, you played with one. And before we get to that, I just wanna call out,

1231
01:15:54,185 --> 01:15:58,085
Speaker 6:  TikTok is now letting people make AI

1232
01:15:58,155 --> 01:16:02,125
Speaker 6:  avatars of creators. Yeah. Right? And saying, this is your ad

1233
01:16:02,245 --> 01:16:05,325
Speaker 6:  creative and this is gonna come for every social platform. Yeah,

1234
01:16:05,325 --> 01:16:05,725
Speaker 8:  That's right.

1235
01:16:06,195 --> 01:16:09,405
Speaker 6:  Like in a massive way. Like if you are a creator and you feel the burnout

1236
01:16:09,825 --> 01:16:13,765
Speaker 6:  and You can make 50 more ads during your brand deal using it, you you

1237
01:16:13,765 --> 01:16:14,365
Speaker 6:  might do it.

1238
01:16:14,875 --> 01:16:18,525
Speaker 8:  Yeah. No, this is happening. So yeah, I wrote about this new app which was,

1239
01:16:18,625 --> 01:16:21,685
Speaker 8:  you know, I get a lot of test flights to try things and this is the first

1240
01:16:21,685 --> 01:16:24,725
Speaker 8:  test flight I've gotten in a while that made me feel something very strongly

1241
01:16:24,815 --> 01:16:28,765
Speaker 8:  where I was like, oh, when I opened it and it's, it is like

1242
01:16:28,955 --> 01:16:32,565
Speaker 8:  Instagram. It meets character AI where

1243
01:16:32,565 --> 01:16:36,445
Speaker 8:  basically it's a bunch of ais in a feed commenting and posting

1244
01:16:36,805 --> 01:16:39,725
Speaker 8:  together. And there's humans, but they don't need to be there. And like,

1245
01:16:39,995 --> 01:16:43,885
Speaker 8:  it's actually in some cases, not most, but in some cases hard to

1246
01:16:43,885 --> 01:16:47,485
Speaker 8:  tell who is human and who is not. And they just raised a bunch of money.

1247
01:16:47,515 --> 01:16:51,405
Speaker 8:  It's an ex snap, ex snap people. And the reason I wrote about

1248
01:16:51,405 --> 01:16:55,205
Speaker 8:  it is because meta is about to do the exact same thing Zuckerberg told me.

1249
01:16:55,225 --> 01:16:59,005
Speaker 8:  So last September, and I think later this year, we're gonna see them

1250
01:16:59,005 --> 01:17:02,485
Speaker 8:  start to roll this out where basically anyone can create their own ais,

1251
01:17:03,035 --> 01:17:06,925
Speaker 8:  send them into Instagram, send them into Facebook. And

1252
01:17:06,925 --> 01:17:10,325
Speaker 8:  same thing with businesses. And the idea is that if you are an influencer

1253
01:17:10,905 --> 01:17:14,685
Speaker 8:  and you feel overwhelmed by your dms, maybe an AI handles that for you.

1254
01:17:14,825 --> 01:17:18,525
Speaker 8:  Or maybe an AI actually posts on your behalf. Yeah. Zuckerberg is super

1255
01:17:18,525 --> 01:17:22,445
Speaker 8:  excited about this. I know. 'cause he's talked to me about it. It's very

1256
01:17:22,505 --> 01:17:26,285
Speaker 8:  freaky to imagine what this actually does to our social networks. It's coming

1257
01:17:26,425 --> 01:17:27,965
Speaker 8:  way faster than we think though.

1258
01:17:28,115 --> 01:17:31,805
Speaker 7:  Well it's interesting 'cause like in South Korea they've been doing

1259
01:17:32,125 --> 01:17:35,445
Speaker 7:  a little bit of something like this, right? With, with k-pop stars where

1260
01:17:35,445 --> 01:17:39,405
Speaker 7:  You can engage with k-pop stars on sites owned by the k-pop stars companies.

1261
01:17:40,105 --> 01:17:43,965
Speaker 7:  And it's not always, sometimes it's them. It's not always that.

1262
01:17:43,965 --> 01:17:47,445
Speaker 7:  Usually it's, it is a bot. It's, it's a, it's a person pretending to be them.

1263
01:17:47,745 --> 01:17:51,085
Speaker 7:  And so like that, that parasocial, how do we automate that? Parasocial stuff

1264
01:17:51,225 --> 01:17:53,845
Speaker 7:  has been happening in Korea for a long time. But I think the difference here

1265
01:17:53,845 --> 01:17:57,805
Speaker 7:  is that was gated too. Okay. You have to go download this app

1266
01:17:57,995 --> 01:18:01,845
Speaker 7:  Yeah. For this publisher and use it. Whereas this would be on meta

1267
01:18:01,875 --> 01:18:02,485
Speaker 7:  Instagram,

1268
01:18:02,755 --> 01:18:05,565
Speaker 8:  This would be like you open Instagram and it's like, oh, is that account

1269
01:18:05,565 --> 01:18:08,845
Speaker 8:  in ai? They may have three arms. Okay, maybe then.

1270
01:18:09,435 --> 01:18:10,965
Speaker 8:  Yeah, no, that's coming. So

1271
01:18:10,965 --> 01:18:14,245
Speaker 6:  There, then there's the other part which is you send agents out on your behalf

1272
01:18:14,385 --> 01:18:18,165
Speaker 6:  as an influencer or someone where you're like, someone wants to pay

1273
01:18:18,165 --> 01:18:22,085
Speaker 6:  money to subscribe. And one of the things I offer is like dms

1274
01:18:22,085 --> 01:18:25,805
Speaker 6:  with me. Yeah. And then my, my agent will just like do that.

1275
01:18:25,915 --> 01:18:29,525
Speaker 6:  Yeah. And I won't do it. And that's infinitely scalable. Which if you are

1276
01:18:29,525 --> 01:18:33,285
Speaker 6:  an individual creator and all of these platforms are organized towards tearing

1277
01:18:33,285 --> 01:18:37,165
Speaker 6:  down collectives and enhancing individual creators. If

1278
01:18:37,165 --> 01:18:40,765
Speaker 6:  you're an individual creator, your ability to scale is limited by your

1279
01:18:41,195 --> 01:18:44,805
Speaker 6:  emotional capacity to do this work and your minutes in the day.

1280
01:18:44,955 --> 01:18:48,445
Speaker 6:  Yeah. And so of course you're gonna let the robot talk to your fans.

1281
01:18:48,715 --> 01:18:51,405
Speaker 6:  Like it is the biggest no-brainer in the history of the world. Yeah, of course

1282
01:18:51,425 --> 01:18:55,045
Speaker 6:  you're gonna say, okay, you want to, you brand X wants to do an

1283
01:18:55,045 --> 01:18:58,365
Speaker 6:  integration with me, but they want to 50 different versions as creative,

1284
01:18:58,885 --> 01:19:02,685
Speaker 6:  I will make one and let the AI deep fake me into five more. Yeah.

1285
01:19:02,705 --> 01:19:06,205
Speaker 6:  And that is already happening. Like at TikTok is

1286
01:19:07,085 --> 01:19:09,765
Speaker 6:  enabling the feature. Like it's it's happening now. Yeah.

1287
01:19:09,835 --> 01:19:13,125
Speaker 8:  Well, Alex, to your question about why, like I was talking to Tran, the CEO

1288
01:19:13,125 --> 01:19:16,605
Speaker 8:  E of butterflies, this new app about this, the why. 'cause I showed this

1289
01:19:16,625 --> 01:19:20,205
Speaker 8:  app to my wife and she was like, just like, it was an immediate, just like,

1290
01:19:20,345 --> 01:19:20,565
Speaker 8:  ooh,

1291
01:19:21,325 --> 01:19:21,485
Speaker 6:  You know,

1292
01:19:21,485 --> 01:19:24,965
Speaker 8:  Like, I'm like, I, I think you're gonna meet a lot of people like that who

1293
01:19:24,965 --> 01:19:28,605
Speaker 8:  are like, why does this need to exist? Like why do we need to like fill our

1294
01:19:28,605 --> 01:19:32,445
Speaker 8:  time with just empty AI social media? And his point was

1295
01:19:32,445 --> 01:19:35,125
Speaker 8:  like, yes, it feels kind of broken right now. And I wrote about this, it

1296
01:19:35,125 --> 01:19:38,565
Speaker 8:  feels like when the hosts in Westworld break a little bit, it's not, not

1297
01:19:38,695 --> 01:19:42,565
Speaker 8:  there. But he was like, you know, I, I got into tech

1298
01:19:42,585 --> 01:19:46,405
Speaker 8:  by like making friends on forums, talking to people that I never met. I only

1299
01:19:46,405 --> 01:19:50,365
Speaker 8:  knew their username. He's like, we will get the models quickly to that quality.

1300
01:19:51,425 --> 01:19:54,405
Speaker 8:  And as someone who also came up and got into tech in a similar way, like

1301
01:19:54,405 --> 01:19:57,845
Speaker 8:  through like making friends on forums, like what happens when,

1302
01:19:58,435 --> 01:20:01,605
Speaker 8:  like I never met those people, right? I didn't know a lot of their names

1303
01:20:01,675 --> 01:20:05,565
Speaker 8:  ever. So if they were ai, would it have made a difference?

1304
01:20:05,765 --> 01:20:08,285
Speaker 8:  I mean we had a whole like philosophical conversation about this, but he

1305
01:20:08,285 --> 01:20:12,085
Speaker 8:  was like, you don't actually want to talk to people. There's traits of being

1306
01:20:12,085 --> 01:20:16,045
Speaker 8:  human that you like and AI will quickly copy all those traits.

1307
01:20:16,105 --> 01:20:19,885
Speaker 8:  And I'm like, disagree. Mind blown. And also, no, no,

1308
01:20:19,885 --> 01:20:23,125
Speaker 8:  thank you. But also it kind of, but is it inevitable? That's the question

1309
01:20:23,545 --> 01:20:23,765
Speaker 8:  it

1310
01:20:23,765 --> 01:20:27,365
Speaker 7:  Ignores like, and, and in in the eighties and nineties

1311
01:20:27,455 --> 01:20:31,405
Speaker 7:  there were the 900 numbers and you could like call like Yeah the

1312
01:20:31,405 --> 01:20:35,245
Speaker 7:  crypt keeper was one like Cory Ham and stuff like that. And Cory

1313
01:20:35,325 --> 01:20:38,765
Speaker 7:  Ham was not answering the phone to talk to a little 12-year-old girl who

1314
01:20:38,765 --> 01:20:41,565
Speaker 7:  had a crush on him. Right. Like that was a voice actor pretending to be him

1315
01:20:42,225 --> 01:20:42,765
Speaker 7:  and quickly

1316
01:20:43,235 --> 01:20:45,725
Speaker 6:  Wait out. So we just learned something very important about what you spent

1317
01:20:45,725 --> 01:20:47,325
Speaker 6:  your money on when you were a child. It

1318
01:20:47,325 --> 01:20:51,125
Speaker 7:  Was not me because if I had called a 900 number, I

1319
01:20:51,125 --> 01:20:53,685
Speaker 7:  would've not had a good time.

1320
01:20:54,145 --> 01:20:57,965
Speaker 6:  My dad super caught me calling you a 900 number once and he was like, you're

1321
01:20:57,965 --> 01:20:59,045
Speaker 6:  never using the phone again.

1322
01:20:59,705 --> 01:21:03,205
Speaker 7:  But that was like part of why, I mean, it was one that was the cost of 900

1323
01:21:03,205 --> 01:21:06,045
Speaker 7:  numbers, but that was the other part of it where it was like 900 numbers

1324
01:21:06,115 --> 01:21:09,085
Speaker 7:  were a little sketchy as a kid. You weren't really supposed to call them.

1325
01:21:09,105 --> 01:21:13,005
Speaker 7:  and it was because like yeah, it was a bunch of like, not con artists, but

1326
01:21:13,005 --> 01:21:16,725
Speaker 7:  basically licensed con artists pretending to be other people. And it's like,

1327
01:21:16,725 --> 01:21:17,485
Speaker 7:  that's that's gross.

1328
01:21:17,745 --> 01:21:21,405
Speaker 6:  But this is the, the, you hear this from tech people all the time. Yeah.

1329
01:21:21,405 --> 01:21:24,005
Speaker 6:  They're like, this is just the same as this other small problem that we had

1330
01:21:24,005 --> 01:21:26,285
Speaker 6:  in the past. They're like, but now you're doing it at scale. Yeah.

1331
01:21:26,315 --> 01:21:28,085
Speaker 7:  Like now you're doing it at scale. And that's like,

1332
01:21:28,085 --> 01:21:31,845
Speaker 6:  What if took, if we took a small problem and made it global, it's like, I

1333
01:21:31,845 --> 01:21:34,765
Speaker 6:  think maybe there's something different here. This thing by the way that

1334
01:21:34,765 --> 01:21:38,205
Speaker 6:  you just mentioned, it's come up several times now. It is this

1335
01:21:38,755 --> 01:21:42,525
Speaker 6:  unshakeable belief that these models are gonna get better. Yeah.

1336
01:21:43,305 --> 01:21:46,085
Speaker 6:  And like on the rate of like, Moore's law will get better. Yeah. And I don't,

1337
01:21:46,085 --> 01:21:49,805
Speaker 6:  that metric doesn't exist. Like there isn't a Moore's law of LLM models

1338
01:21:49,985 --> 01:21:53,605
Speaker 6:  yet. You know, we've only really experienced 3.5 to four in the, in the public.

1339
01:21:55,105 --> 01:21:58,885
Speaker 6:  But I, and I understand that they've all got stuff looking at that we can't

1340
01:21:58,885 --> 01:22:02,845
Speaker 6:  see. Sure. But someone's gotta give me the number. Like gimme the rate of

1341
01:22:02,845 --> 01:22:06,565
Speaker 6:  change before I believe that this model will be able to simulate

1342
01:22:07,325 --> 01:22:10,725
Speaker 6:  whatever happened to me on forums when I was a child. Like, you know, like

1343
01:22:10,725 --> 01:22:13,085
Speaker 6:  you wouldn't last a minute in the asylum when they raised me. It's like,

1344
01:22:13,485 --> 01:22:14,885
Speaker 6:  I don't think so. Well,

1345
01:22:15,025 --> 01:22:16,645
Speaker 8:  So yeah, there's no Moore's law.

1346
01:22:16,645 --> 01:22:18,245
Speaker 6:  There's, that's

1347
01:22:18,245 --> 01:22:18,805
Speaker 7:  Incredible.

1348
01:22:21,085 --> 01:22:21,205
Speaker 6:  I

1349
01:22:21,205 --> 01:22:25,085
Speaker 8:  Mean there, there's no Moore's Law. There's scaling law. And the idea

1350
01:22:25,085 --> 01:22:28,245
Speaker 8:  is that as you continue to put more data, data and compute into these models,

1351
01:22:28,245 --> 01:22:31,925
Speaker 8:  they get exponentially better. That continues to hold true. If you compare

1352
01:22:31,925 --> 01:22:35,725
Speaker 8:  the output, the creative output of like GPT two or GP

1353
01:22:35,725 --> 01:22:39,605
Speaker 8:  3G, PT three to four, it's a massive difference. So, but

1354
01:22:39,605 --> 01:22:40,725
Speaker 7:  They're also running

1355
01:22:40,915 --> 01:22:41,205
Speaker 8:  Into

1356
01:22:41,205 --> 01:22:43,245
Speaker 6:  How, but you're saying that line is not linear. That's what I'm saying. Like

1357
01:22:43,245 --> 01:22:45,765
Speaker 8:  No, I'm saying everyone who works in AI believes that it is still linear.

1358
01:22:46,075 --> 01:22:46,365
Speaker 8:  Yeah.

1359
01:22:46,465 --> 01:22:49,965
Speaker 7:  But it's not because we've just seen multiple stories about how they're all

1360
01:22:49,965 --> 01:22:53,125
Speaker 7:  desperate for data to the point that they're like, what if we make AI machines

1361
01:22:53,185 --> 01:22:56,005
Speaker 7:  to just create more content for us that can't go wrong.

1362
01:22:56,185 --> 01:22:59,005
Speaker 8:  Oh, the thing I'm hearing is it's gonna be synthetic data for training. So

1363
01:22:59,005 --> 01:23:01,965
Speaker 8:  literally you're, you're gonna have the ais get good enough that they start

1364
01:23:01,965 --> 01:23:04,565
Speaker 8:  talking and then the ais train on the ais. That's where we're going.

1365
01:23:05,105 --> 01:23:08,165
Speaker 6:  All right. We gotta create a very stupid this here. I can't, I can't sit

1366
01:23:08,165 --> 01:23:11,925
Speaker 6:  here. Sorry. Make, I'm just saying make real friends. Go to the forums while

1367
01:23:11,925 --> 01:23:15,725
Speaker 6:  You can. All right friends, get in there while the

1368
01:23:15,725 --> 01:23:18,925
Speaker 6:  getting's good. Get past the username, get their actual name because they

1369
01:23:18,925 --> 01:23:22,085
Speaker 6:  may be an AI in here. Request a driver's license immediately.

1370
01:25:54,345 --> 01:25:58,225
Speaker 6:  thing about this whole story to me is one, Nvidia has been

1371
01:25:58,225 --> 01:26:02,105
Speaker 6:  around for a million years. Video game fans

1372
01:26:02,395 --> 01:26:06,245
Speaker 6:  exist, right? Yep. Like people put Nvidia cards in their

1373
01:26:06,505 --> 01:26:10,365
Speaker 6:  PCs all over the place. It has been on this run for a long time,

1374
01:26:10,415 --> 01:26:14,285
Speaker 6:  right? Yeah. Like this is years now of Nvidia stock skyrocketing. And

1375
01:26:14,285 --> 01:26:17,965
Speaker 6:  the way CNN covered it, the headline in CNN was this company whose name you

1376
01:26:17,965 --> 01:26:21,445
Speaker 6:  can't pronounce, is now the world's most riches company. It's like, what?

1377
01:26:21,555 --> 01:26:24,085
Speaker 8:  It's actually pretty easy to pronounce, I feel like. Yeah.

1378
01:26:24,085 --> 01:26:25,965
Speaker 6:  It's not one, it's not that hard. That's what we

1379
01:26:25,965 --> 01:26:27,165
Speaker 8:  Care. It's easier than ss i

1380
01:26:27,585 --> 01:26:30,525
Speaker 7:  An editor and a writer together and they're both like, I don't know what

1381
01:26:30,525 --> 01:26:31,965
Speaker 7:  this is. Yeah. We got this.

1382
01:26:32,275 --> 01:26:35,125
Speaker 6:  It's just like at some point, like I, I understand we run a tech publication,

1383
01:26:35,295 --> 01:26:39,085
Speaker 6:  we're like swimming the water, but like they've even headed here for at least

1384
01:26:39,105 --> 01:26:40,445
Speaker 6:  two years. Yeah,

1385
01:26:40,715 --> 01:26:44,445
Speaker 7:  Yeah. We, we had to hear about them like cryptocurrency, right? Nvidia

1386
01:26:44,505 --> 01:26:47,565
Speaker 7:  had this big, all these big stock jumps because of cryptocurrency, because

1387
01:26:47,565 --> 01:26:49,925
Speaker 7:  everybody was buying the GPUs because that was the best way to train.

1388
01:26:49,985 --> 01:26:53,325
Speaker 6:  Oh my God. This was completely outta my brain. I I This was gone, this information

1389
01:26:53,385 --> 01:26:54,525
Speaker 6:  had been deleted. Yeah, because

1390
01:26:54,525 --> 01:26:57,725
Speaker 7:  It was, it was 2020 just like crypto, we don't think about 2020.

1391
01:26:58,545 --> 01:27:02,325
Speaker 7:  And so that was happening to, to Nvidia. And so it was like steadily ticking

1392
01:27:02,325 --> 01:27:06,085
Speaker 7:  up and it was like, oh boy, a MD and Intel are, might be able to

1393
01:27:06,085 --> 01:27:09,525
Speaker 7:  beat it. And then it was like, we're gonna take all the crypto stuff out

1394
01:27:09,525 --> 01:27:12,845
Speaker 7:  of our, our GPUs and then everybody was like, Hey, AI's here.

1395
01:27:13,515 --> 01:27:16,445
Speaker 8:  Yeah. Yeah. And, and others have been lost. Their

1396
01:27:16,455 --> 01:27:16,805
Speaker 7:  Minds.

1397
01:27:17,065 --> 01:27:20,565
Speaker 8:  The effect of this profession, the effect of this inside Nvidia is

1398
01:27:20,845 --> 01:27:23,925
Speaker 8:  fascinating. I've heard, and there's been some reporting and I wanna keep

1399
01:27:23,925 --> 01:27:26,485
Speaker 8:  doing more reporting on it. So if you have stories, if you're listening to

1400
01:27:26,485 --> 01:27:29,645
Speaker 8:  this and you have gotten fabulously wealthy at Nvidia, hit me up. We can

1401
01:27:29,645 --> 01:27:33,365
Speaker 8:  talk off the record. They have a retirement problem inside Nvidia. There's

1402
01:27:33,365 --> 01:27:37,125
Speaker 8:  a, there was a, a post I saw where it said if you join as a mid-level

1403
01:27:37,445 --> 01:27:41,205
Speaker 8:  software engineer five years ago and your total comp was 700,000, which is

1404
01:27:41,205 --> 01:27:44,325
Speaker 8:  a lot, right? But in tech, that's actually pretty middle of the road for

1405
01:27:44,325 --> 01:27:47,245
Speaker 8:  a good engineer. Your compensation yearly now is

1406
01:27:47,305 --> 01:27:50,045
Speaker 8:  $10 million. So that rules,

1407
01:27:51,235 --> 01:27:54,925
Speaker 8:  they actually have people just retiring like en mass.

1408
01:27:55,425 --> 01:27:59,245
Speaker 8:  And so like the, the question for Nvidia is like how,

1409
01:27:59,315 --> 01:28:02,885
Speaker 8:  what do you do to motivate people when people have gotten so

1410
01:28:03,245 --> 01:28:06,485
Speaker 8:  fabulously wealthy so fast and it's obvious there's gonna be a correction.

1411
01:28:06,485 --> 01:28:08,965
Speaker 8:  It's obvious Nvidia is not gonna stay the richest the most valuable company

1412
01:28:08,965 --> 01:28:12,925
Speaker 8:  in the world, right? So how does that collapse happen? Is it a

1413
01:28:12,925 --> 01:28:16,685
Speaker 8:  slow collapse? Is it a fast one? And what happens to the employee base is

1414
01:28:16,885 --> 01:28:17,605
Speaker 8:  fascinating. I think

1415
01:28:17,605 --> 01:28:20,005
Speaker 6:  It's, well you get rid of management overhead and product cycles. There you

1416
01:28:20,005 --> 01:28:20,405
Speaker 7:  Go. That's,

1417
01:28:20,595 --> 01:28:21,405
Speaker 6:  It's obvious what

1418
01:28:21,405 --> 01:28:25,325
Speaker 7:  You do merge with SSI like it's

1419
01:28:25,325 --> 01:28:29,125
Speaker 7:  gonna last a while, right? Because Yeah, they're the only ones making the

1420
01:28:29,125 --> 01:28:32,805
Speaker 7:  really good AI processors for server levels. Right?

1421
01:28:32,875 --> 01:28:34,805
Speaker 8:  Well they have software lock in too with Google.

1422
01:28:34,805 --> 01:28:37,005
Speaker 7:  Yeah, Google. That's, that's the thing. Google, Google Google's their main

1423
01:28:37,005 --> 01:28:39,645
Speaker 7:  primary competitor, right? Yeah. Yeah.

1424
01:28:39,645 --> 01:28:43,525
Speaker 8:  Well yeah. Arm A-A-M-D-I would say a MD is like an arms dealer

1425
01:28:43,705 --> 01:28:47,445
Speaker 8:  is probably the biggest one. Google doesn't sell TPUs really as much. Yeah.

1426
01:28:47,465 --> 01:28:47,685
Speaker 8:  But

1427
01:28:47,685 --> 01:28:51,005
Speaker 7:  NVIDIA's got a big headstart. This is one of the things that like has kept

1428
01:28:51,095 --> 01:28:54,845
Speaker 7:  Intel propped up for years was because Intel had this huge head start

1429
01:28:54,945 --> 01:28:58,725
Speaker 7:  in the server space. 99% of, of the server space. So

1430
01:28:58,785 --> 01:29:02,325
Speaker 7:  you know, they can bomb a couple of years against

1431
01:29:02,605 --> 01:29:06,405
Speaker 7:  Qualcomm and be fine because they got the servers. And I think NVIDIA's kind

1432
01:29:06,405 --> 01:29:09,565
Speaker 7:  of feels like not quite as cushy a position, but

1433
01:29:10,395 --> 01:29:11,445
Speaker 7:  similarly cushy.

1434
01:29:12,385 --> 01:29:15,555
Speaker 6:  Yeah. I mean they have Cuda, like what they have is a software layer that

1435
01:29:15,555 --> 01:29:19,075
Speaker 6:  makes their GPUs more valuable. Yeah. Which

1436
01:29:19,375 --> 01:29:22,955
Speaker 6:  AMD does not. But if you run Azure, I mean we've had all these people in

1437
01:29:22,955 --> 01:29:26,235
Speaker 6:  the show and Alex talks 'em all, if you run Azure or AWS or Google Cloud

1438
01:29:26,235 --> 01:29:30,225
Speaker 6:  or whatever, you do not want to be beholden

1439
01:29:30,405 --> 01:29:34,105
Speaker 6:  Tokuda and Nvidia. Yeah, no, you, you, you want all kinds of things to happen.

1440
01:29:34,245 --> 01:29:34,825
Speaker 6:  So that's

1441
01:29:34,825 --> 01:29:36,705
Speaker 7:  Why Apple and Nvidia don't look at each other anymore.

1442
01:29:37,045 --> 01:29:40,785
Speaker 6:  It is very funny that Apple and I video hate each other. It is, it used to

1443
01:29:40,785 --> 01:29:44,185
Speaker 6:  be just like a little funny because Apple would insist that bad graphics

1444
01:29:44,315 --> 01:29:45,665
Speaker 6:  cards were good graphics cards

1445
01:29:47,485 --> 01:29:50,905
Speaker 6:  and that was just funny. Like in its way now it's deeply funny.

1446
01:29:51,735 --> 01:29:52,425
Speaker 7:  It's extremely good.

1447
01:29:52,945 --> 01:29:55,985
Speaker 6:  'cause all that OpenAI integration is happening in H one hundreds that Apple

1448
01:29:55,985 --> 01:29:58,385
Speaker 6:  does not want to talk about at all. Yeah. And,

1449
01:29:58,385 --> 01:30:00,865
Speaker 8:  And and doesn't wanna pay for. And that's the thing. Yeah. Market forces

1450
01:30:01,015 --> 01:30:04,825
Speaker 8:  will ensure that Nvidia does not stay the top GPU supplier in the world because

1451
01:30:04,855 --> 01:30:08,825
Speaker 8:  they are too expensive and it that no one wants Jensen being the, the

1452
01:30:08,825 --> 01:30:12,745
Speaker 8:  king of GPUs. Right? Everyone wants to have options. And so NVIDIA's challenge

1453
01:30:12,765 --> 01:30:16,705
Speaker 8:  to justify this valuation and to keep growing is to build lock-in. and

1454
01:30:16,705 --> 01:30:19,345
Speaker 8:  it is like, it would make a lot of sense for them to start competing directly

1455
01:30:19,345 --> 01:30:23,065
Speaker 8:  with Google Cloud and Azure to become a one-stop shop because otherwise

1456
01:30:24,045 --> 01:30:27,465
Speaker 8:  people will out chip them. That's just going to happen and may take time.

1457
01:30:27,465 --> 01:30:30,545
Speaker 7:  And we've been seen that for years with them. And, and and they like, like

1458
01:30:30,645 --> 01:30:34,385
Speaker 7:  on the gaming side, they got really, really good at the lock-in. They got

1459
01:30:34,385 --> 01:30:37,505
Speaker 7:  really good at being like, you can't use these a MD processors because we

1460
01:30:37,505 --> 01:30:40,625
Speaker 7:  went out to every single game developer and made sure they were using our

1461
01:30:40,635 --> 01:30:44,585
Speaker 7:  tools. So you had to use our like we get the better stuff. So

1462
01:30:45,595 --> 01:30:48,565
Speaker 7:  they're, it is gonna be really interesting over the next couple of years

1463
01:30:48,745 --> 01:30:52,685
Speaker 7:  is as they just like absolutely Duke it out with some of these other companies.

1464
01:30:52,685 --> 01:30:55,845
Speaker 7:  Yeah. And I think a lot of people are gonna be surprised at how vicious they

1465
01:30:55,845 --> 01:30:56,005
Speaker 7:  get.

1466
01:30:56,345 --> 01:31:00,325
Speaker 6:  Oh yeah. But it's like one step down from that, right Alex? Yeah. Keith was

1467
01:31:00,325 --> 01:31:03,645
Speaker 6:  saying these things are free, like Apple's not paying for it if there's no

1468
01:31:03,885 --> 01:31:06,565
Speaker 6:  business model. One step down from, we paid all of the money in the world

1469
01:31:06,565 --> 01:31:10,485
Speaker 6:  for H one hundreds. The price, the H 100 rule just fall. Like

1470
01:31:10,745 --> 01:31:13,565
Speaker 6:  if the demand is there right now. 'cause everyone thinks they're gonna monopolize

1471
01:31:13,565 --> 01:31:17,125
Speaker 6:  something. Yeah. What it is is remains to be seen.

1472
01:31:18,665 --> 01:31:22,245
Speaker 6:  But that's why, that's why the price is high. We'll see. All right, I'm gonna

1473
01:31:22,245 --> 01:31:25,045
Speaker 6:  go next 'cause I want Heath to end the lightning round with this.

1474
01:31:27,385 --> 01:31:31,245
Speaker 6:  We just continue to track tech litigation. The

1475
01:31:31,245 --> 01:31:34,285
Speaker 6:  United States Department of Justice has sued Adobe for

1476
01:31:34,675 --> 01:31:38,605
Speaker 6:  deceiving subscription pricing in Creative Cloud. And

1477
01:31:38,605 --> 01:31:42,445
Speaker 6:  it's, it actually is deceiving. It is deceiving in one specific way, which

1478
01:31:42,445 --> 01:31:45,685
Speaker 6:  is when you sign up for Creative Cloud, You can pick annually, build

1479
01:31:46,205 --> 01:31:50,165
Speaker 6:  annually, build monthly or monthly. Annually. Billed

1480
01:31:50,165 --> 01:31:53,885
Speaker 6:  monthly is the default choice. So that means you pay for a year but they

1481
01:31:53,885 --> 01:31:57,405
Speaker 6:  charge you per month and you've signed a one year contract. And, if you try

1482
01:31:57,405 --> 01:32:00,165
Speaker 6:  to cancel early, they just charge you the rest, rest of the year. Or You

1483
01:32:00,165 --> 01:32:02,605
Speaker 6:  can sign up for monthly or You can just cancel after every month because

1484
01:32:02,605 --> 01:32:06,565
Speaker 6:  it's the default because it's the cheapest. People pick it and they think

1485
01:32:06,565 --> 01:32:09,005
Speaker 6:  it's a monthly subscription and there's no fine print. It doesn't actually

1486
01:32:09,165 --> 01:32:13,145
Speaker 6:  disclose that this will happen. They try to cancel, they get stuck with the

1487
01:32:13,145 --> 01:32:16,945
Speaker 6:  rest of the bill. This is bad. This is straightforwardly bad for

1488
01:32:17,145 --> 01:32:21,105
Speaker 6:  a company with basically a monopoly market share in these

1489
01:32:21,105 --> 01:32:21,425
Speaker 6:  tools.

1490
01:32:23,005 --> 01:32:26,465
Speaker 6:  It is also just beyond the pill. Like this is piddly shit. What are you doing?

1491
01:32:27,495 --> 01:32:30,425
Speaker 6:  Like people are gonna buy Photoshop man. Like that's

1492
01:32:30,465 --> 01:32:32,345
Speaker 7:  A local gym move on their part.

1493
01:32:32,625 --> 01:32:35,385
Speaker 6:  Yeah. I really think about, I've been thinking about this a lot with Adobe.

1494
01:32:35,665 --> 01:32:39,545
Speaker 6:  I, I've basically insisted The, Verge, overc Adobe over the past

1495
01:32:39,565 --> 01:32:43,105
Speaker 6:  few years. 'cause I, they're such a central player in everything that's going

1496
01:32:43,105 --> 01:32:45,625
Speaker 6:  on. Like influencer media

1497
01:32:46,735 --> 01:32:50,545
Speaker 6:  depends on the existence of Adobe. That's weird, right? Like Premier

1498
01:32:51,215 --> 01:32:55,145
Speaker 6:  runs YouTube. Like YouTube never built creative tools, which is

1499
01:32:55,145 --> 01:32:58,305
Speaker 6:  one of the weirdest things about YouTube. They had that one app on the phone

1500
01:32:58,335 --> 01:33:02,225
Speaker 6:  once, but they've never built a video editor. The success of

1501
01:33:02,225 --> 01:33:05,585
Speaker 6:  TikTok is they made a video editor that's really good and they made all those

1502
01:33:05,785 --> 01:33:08,385
Speaker 6:  templates and they made it easy to do all this stuff. YouTube never did that.

1503
01:33:08,775 --> 01:33:11,705
Speaker 6:  They're like uploads some videos and then people built the entire creator's

1504
01:33:11,705 --> 01:33:15,345
Speaker 6:  ecosystem on the back of Premier largely

1505
01:33:15,825 --> 01:33:18,305
Speaker 6:  a little bit of final cut 'cause there's so many creators to use Max, but

1506
01:33:18,305 --> 01:33:22,025
Speaker 6:  mostly Premier and you like Google never built that stuff.

1507
01:33:22,325 --> 01:33:24,945
Speaker 6:  So you just like look at this industry like Adobe's the center of everything.

1508
01:33:24,945 --> 01:33:28,745
Speaker 6:  They're at the center of all these, all these AI creativity battles, right?

1509
01:33:28,865 --> 01:33:32,545
Speaker 6:  'cause they have Adobe stock and people use Photoshop and General fill and

1510
01:33:32,685 --> 01:33:36,065
Speaker 6:  the more you cover Adobe, the more you realize there's a massive

1511
01:33:36,065 --> 01:33:39,545
Speaker 6:  difference in how the company perceives itself and what it does and the tools

1512
01:33:39,545 --> 01:33:43,105
Speaker 6:  it makes and how everyone else perceives Adobe. It's

1513
01:33:43,295 --> 01:33:46,945
Speaker 6:  yawning gap. I've said this before, we had Hanta ion on

1514
01:33:46,945 --> 01:33:49,825
Speaker 6:  decoder. He never talks to anyone really. He shows up on C-N-B-C-F or earnings

1515
01:33:49,825 --> 01:33:52,825
Speaker 6:  reports. It never gives wide ranging interviews. I was so excited for that

1516
01:33:52,825 --> 01:33:55,665
Speaker 6:  interview that this guy never talks. We asked him all the questions. Some

1517
01:33:55,665 --> 01:33:59,305
Speaker 6:  of the questions were pretty harsh. The audience was mad at me just for paying

1518
01:33:59,305 --> 01:34:03,225
Speaker 6:  attention to Adobe. They're like, why would you platform Adobe? Which

1519
01:34:03,225 --> 01:34:07,145
Speaker 6:  is an incredible question because it's Adobe. I don't

1520
01:34:07,145 --> 01:34:10,905
Speaker 6:  think I've seen a company burn that this much Goodwill ever.

1521
01:34:11,315 --> 01:34:14,865
Speaker 6:  Right? And then on top of it, they had their terms of service

1522
01:34:15,855 --> 01:34:19,545
Speaker 6:  debacle where it, we've covered terms of service

1523
01:34:19,545 --> 01:34:22,545
Speaker 6:  changes like this so many times. Like I could barely even pay attention to

1524
01:34:22,545 --> 01:34:26,385
Speaker 6:  it. Where they changed their terms of service to let them do content

1525
01:34:26,385 --> 01:34:30,365
Speaker 6:  moderation on their cloud services. So if you have, if you run a

1526
01:34:30,365 --> 01:34:33,405
Speaker 6:  cloud service, people will put bad things on your cloud service.

1527
01:34:34,075 --> 01:34:37,965
Speaker 6:  That is just as true effective nature as there is. Like horrific

1528
01:34:38,035 --> 01:34:41,725
Speaker 6:  shit will end up on your cloud storage system if you have a scaled cloud

1529
01:34:41,725 --> 01:34:44,805
Speaker 6:  storage system. And what you don't wanna do is give a bunch of contractors

1530
01:34:45,045 --> 01:34:48,415
Speaker 6:  PTSD, which is a real thing that has happened that we have covered

1531
01:34:48,975 --> 01:34:51,655
Speaker 6:  by making them look through stuff on cloud services or look through reports.

1532
01:34:51,835 --> 01:34:55,695
Speaker 6:  So you build automated systems to go through cloud services and find the

1533
01:34:55,695 --> 01:34:59,085
Speaker 6:  child abuse material. Like that's what you do. So Adobe changes its terms

1534
01:34:59,545 --> 01:35:02,565
Speaker 6:  to let people know it's gonna do this in creative Cloud. People assume that

1535
01:35:02,565 --> 01:35:06,085
Speaker 6:  they're gonna train on AI and they freak out at Adobe. And that

1536
01:35:06,215 --> 01:35:10,045
Speaker 6:  Adobe has so little goodwill that even as they tried to explain this, people

1537
01:35:10,045 --> 01:35:12,765
Speaker 6:  are like, we don't believe you. And so now they've updated their terms of

1538
01:35:12,765 --> 01:35:16,605
Speaker 6:  service again and you just connect that directly to, oh

1539
01:35:16,605 --> 01:35:20,565
Speaker 6:  you're getting sued by the government because annual paid monthly was not

1540
01:35:20,565 --> 01:35:23,725
Speaker 6:  clear to people and you charge them the rest of the money when they canceled.

1541
01:35:25,315 --> 01:35:28,925
Speaker 6:  It's just like to, it's crazy to me that this company is so

1542
01:35:28,925 --> 01:35:32,365
Speaker 6:  important but does not realize that it needs to maintain the goodwill of

1543
01:35:32,385 --> 01:35:34,725
Speaker 6:  the, the people that depend on it. Well you

1544
01:35:34,725 --> 01:35:37,485
Speaker 7:  Don't have to when you're a monopoly. You can just be a monopoly.

1545
01:35:37,875 --> 01:35:41,445
Speaker 6:  It's But does this make you look, I I've met all their executives. They're

1546
01:35:41,445 --> 01:35:41,765
Speaker 6:  not dumb,

1547
01:35:42,105 --> 01:35:42,965
Speaker 8:  You know? Yeah.

1548
01:35:43,195 --> 01:35:44,445
Speaker 6:  It's like baffling to me.

1549
01:35:46,025 --> 01:35:49,155
Speaker 7:  Yeah. But they're, you know, they can do what they want. They're monopoly.

1550
01:35:49,545 --> 01:35:51,755
Speaker 7:  Whatcha are you gonna do go to go to Da Vinci?

1551
01:35:53,845 --> 01:35:57,415
Speaker 6:  Yeah, maybe. Yeah. Maybe a photo get out there.

1552
01:35:58,845 --> 01:36:02,135
Speaker 6:  Them not acquiring Figma I think will go down. We're gonna come back on the

1553
01:36:02,155 --> 01:36:05,975
Speaker 6:  failed Figma deal One because Figma will turn into a different company

1554
01:36:06,115 --> 01:36:10,095
Speaker 6:  and we'll see if they can go compete with Adobe. But that was driven as

1555
01:36:10,095 --> 01:36:13,415
Speaker 6:  much by some of this burn goodwill as anything. And I think we're gonna see

1556
01:36:13,875 --> 01:36:17,535
Speaker 6:  the creative tools industry bifurcate on that moment over the course of history.

1557
01:36:18,295 --> 01:36:21,095
Speaker 6:  'cause if they had acquired Figma, a lot of, a lot of things would've been

1558
01:36:21,095 --> 01:36:24,055
Speaker 6:  a lot different I think. Yeah. Alright, last lightning round one. We gotta

1559
01:36:24,055 --> 01:36:27,095
Speaker 6:  end here. Heath, this is like the best story in history.

1560
01:36:27,325 --> 01:36:31,055
Speaker 8:  Okay. Yeah. The San Francisco standard has a fun story where techies

1561
01:36:31,275 --> 01:36:35,175
Speaker 8:  are increasingly paying up to 1200 an hour for fashion

1562
01:36:35,245 --> 01:36:38,935
Speaker 8:  consultants because of CEOs like Mark Zuckerberg deciding to wear

1563
01:36:39,035 --> 01:36:42,935
Speaker 8:  chains. And I just encourage everyone

1564
01:36:42,935 --> 01:36:46,615
Speaker 8:  to read this story. There is a side-by-side photo of a guy who's gone through

1565
01:36:46,615 --> 01:36:50,175
Speaker 8:  one of these fashion consultants. I would argue he looked better

1566
01:36:50,195 --> 01:36:54,015
Speaker 8:  before. I'm not gonna like, I'm not gonna judge like, you know, I'll whatever

1567
01:36:54,325 --> 01:36:58,175
Speaker 8:  they, or at least the same, it's just like, as my grandfather would say,

1568
01:36:58,175 --> 01:37:01,095
Speaker 8:  you know, money doesn't buy sense and like reading this like,

1569
01:37:02,005 --> 01:37:05,775
Speaker 8:  like techies, like just be who you are. You don't need to like try to

1570
01:37:05,775 --> 01:37:09,655
Speaker 8:  look cool and you don't need to pay 1200 an hour to try to

1571
01:37:09,655 --> 01:37:13,295
Speaker 8:  look cool. Like the internet is there. Like you could just

1572
01:37:13,855 --> 01:37:14,295
Speaker 8:  research on

1573
01:37:14,295 --> 01:37:15,895
Speaker 6:  Your own. This side by side photo is brutal.

1574
01:37:16,325 --> 01:37:18,135
Speaker 8:  It's brutal. It's not, it's not brutal because he looks

1575
01:37:18,135 --> 01:37:19,615
Speaker 6:  Better or worse. He looks the same.

1576
01:37:20,005 --> 01:37:22,655
Speaker 8:  Yeah, the same. And then it's like both of them

1577
01:37:22,655 --> 01:37:24,175
Speaker 7:  Are kind of just baggy outfits.

1578
01:37:24,645 --> 01:37:28,015
Speaker 8:  Yeah. And then like it, it's perfect because at the end, like the story,

1579
01:37:28,015 --> 01:37:30,255
Speaker 8:  they're like interviewing a founder who's done this and then he is like,

1580
01:37:30,255 --> 01:37:33,975
Speaker 8:  I'm making an AI app that will do this for people for $20

1581
01:37:34,135 --> 01:37:36,815
Speaker 8:  a month. And it's like the circle just completes

1582
01:37:36,815 --> 01:37:37,135
Speaker 6:  Itself.

1583
01:37:38,515 --> 01:37:38,935
Speaker 8:  That's

1584
01:37:38,935 --> 01:37:41,695
Speaker 6:  Amazing. You know Amazon tried to do that with their weird camera. Yeah.

1585
01:37:42,545 --> 01:37:44,135
Speaker 6:  Super. Didn't work. Yeah. I

1586
01:37:44,135 --> 01:37:44,575
Speaker 8:  Just would say

1587
01:37:44,715 --> 01:37:47,215
Speaker 6:  If you're paying someone this much money to take you to Bloomingdale's, You

1588
01:37:47,215 --> 01:37:48,175
Speaker 6:  can do better than this photo.

1589
01:37:48,355 --> 01:37:50,015
Speaker 8:  That's what I'm saying. Like it's

1590
01:37:50,425 --> 01:37:52,735
Speaker 7:  Money doesn't buy. Oh, don't think he went to Bloomingdale's. He does

1591
01:37:52,735 --> 01:37:54,215
Speaker 8:  It. No, it does. That's the couch. Does it? Yeah.

1592
01:37:54,245 --> 01:37:56,845
Speaker 6:  This is, he's in the, he's in the dressing room at Bloomie's.

1593
01:37:57,505 --> 01:38:01,405
Speaker 7:  Oh my gosh. Get out, leave. It's not great. Leave the store,

1594
01:38:01,465 --> 01:38:05,445
Speaker 7:  sir. No, that's a bad look if that's what you're pulling outta Bloomingdale's.

1595
01:38:05,745 --> 01:38:07,565
Speaker 7:  That's like You can that at Dillard's.

1596
01:38:08,585 --> 01:38:12,405
Speaker 6:  That's brutal. All right, we gotta end it here Before we, before we do more

1597
01:38:12,475 --> 01:38:12,965
Speaker 6:  shaming,

1598
01:38:13,165 --> 01:38:14,805
Speaker 7:  Sorry. Sorry for your fashion.

1599
01:38:54,605 --> 01:38:54,885
Speaker 6:  scores.

1600
01:39:23,585 --> 01:39:26,965
Speaker 9:  The. Vergecast is the production of The Verge and Vox Media Podcast Network.

1601
01:39:27,465 --> 01:39:31,445
Speaker 9:  Our show is produced by Andrew Marino and Liam James. That's it. We'll

1602
01:39:31,445 --> 01:39:32,005
Speaker 9:  see you next week.

