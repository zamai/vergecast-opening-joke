1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 0908029c-1082-43fb-9188-a4dd2f462d58
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-5598332316944854281/-6787370208146472682/s93290-US-4949s-1748337362.mp3
Description: After more than a month of testimony, the Meta antirust trial is beginning to slow down. The Google search remedies trial, meanwhile, is about to heat up again, with closing arguments coming soon. The Verge’s Lauren Feiner has been in the DC courthouse for all of it, and has finally emerged to tell us about what she’s seen, and learned, from two all-important monopoly trials. After that, The Verge’s Victoria Song tells us about her latest experience with Google’s smart glasses prototypes, what Google is doing differently from Meta and Apple, and what she thinks Jony Ive and OpenAI might be building. Finally, we answer a question on the Vergecast Hotline about what to do now that Mozilla is shutting down Pocket.


  FTC v. Meta: The antitrust battle over WhatsApp and Instagram

  Did WhatsApp really need Meta?

  Why the FTC argues Meta is a closer rival to MeWe than TikTok

  Instagram CEO testifies about competing with TikTok: ‘You’re either growing, or you’re slowly dying’

  Android XR is getting stylish partners in Warby Parker, Gentle Monster

  Xreal teases Project Aura smart glasses for Android XR

  We tried on Google’s prototype AI smart glasses

  Android XR and Project Moohan hands-on: Gemini is the killer app

  Mozilla is shutting down Pocket

  Raindrop.io

  Instapaper

  Matter

  Wallabag

  Readwise Reader


Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (23 ads detected)

2
00:00:03,145 --> 00:00:06,955
Speaker 1:  Welcome to The Vergecast, the flagship podcast of endless Litigation.

3
00:00:07,295 --> 00:00:10,875
Speaker 1:  I'm m your friend David Pierce, and I am sitting here down one of the

4
00:00:10,875 --> 00:00:14,635
Speaker 1:  deepest rabbit holes. I have been down in a long time, So I started

5
00:00:14,635 --> 00:00:18,075
Speaker 1:  reading this book called Riptide, which I bought just sort of on a whim because

6
00:00:18,075 --> 00:00:21,435
Speaker 1:  it was like on sale for a dollar and it had good reviews. It's about treasure

7
00:00:21,435 --> 00:00:24,675
Speaker 1:  hunters. So I started reading this book, got very into this book. It's very

8
00:00:24,675 --> 00:00:28,195
Speaker 1:  good. Highly recommend it. And this has sent me just

9
00:00:28,305 --> 00:00:31,995
Speaker 1:  spiraling in the weirdest, most fun way on all things

10
00:00:32,395 --> 00:00:36,195
Speaker 1:  treasure hunters and shipwrecks. So there's this great New Yorker

11
00:00:36,195 --> 00:00:39,275
Speaker 1:  story about treasure hunters from a bunch of years ago. There's a great story

12
00:00:39,275 --> 00:00:42,675
Speaker 1:  from much more recently about shipwreck detectives. I've

13
00:00:42,745 --> 00:00:46,595
Speaker 1:  literally, I bought the game uncharted to go play it now. I watched

14
00:00:46,615 --> 00:00:49,955
Speaker 1:  the movie Uncharted the other night. I'm reading about treasure hunters.

15
00:00:49,975 --> 00:00:53,875
Speaker 1:  I'm watching documentaries about treasure hunters. This is like a whole universe

16
00:00:54,135 --> 00:00:57,795
Speaker 1:  of basically puzzle solvers that is

17
00:00:57,865 --> 00:01:01,595
Speaker 1:  full of technology and full of interesting ways that people are doing

18
00:01:01,915 --> 00:01:05,475
Speaker 1:  research. It's like all kinds of just my

19
00:01:05,715 --> 00:01:09,675
Speaker 1:  interests colliding all at once. And I make fun of my wife

20
00:01:09,695 --> 00:01:12,875
Speaker 1:  all the time because she loves every true crime show you can possibly imagine.

21
00:01:13,575 --> 00:01:17,395
Speaker 1:  But I'm realizing that I am falling deep into like the treasure hunter

22
00:01:17,445 --> 00:01:20,955
Speaker 1:  world for exactly the same reason, and It is

23
00:01:21,395 --> 00:01:24,275
Speaker 1:  blowing my mind and it's also the only thing I care about. So if I bring

24
00:01:24,275 --> 00:01:28,115
Speaker 1:  up treasure hunting 65 times, or if three VERGE casts from

25
00:01:28,115 --> 00:01:32,075
Speaker 1:  now, I'm just coming to you from the bow of a ship attempting

26
00:01:32,075 --> 00:01:35,835
Speaker 1:  to find several hundred years worth of treasure, you'll know this is how

27
00:01:35,835 --> 00:01:39,795
Speaker 1:  it all started. Anyway, as much as I can, that is not what we are here

28
00:01:39,795 --> 00:01:42,875
Speaker 1:  to do today. Today on the show, we're gonna do two things. First, I'm gonna

29
00:01:42,875 --> 00:01:46,435
Speaker 1:  talk to Lauren Finer, who finally emerged from a courtroom

30
00:01:46,655 --> 00:01:50,395
Speaker 1:  and is going to talk to me about the Google trial and the Meta trial and

31
00:01:50,395 --> 00:01:54,155
Speaker 1:  everything that we've learned about the antitrust case for and against two

32
00:01:54,155 --> 00:01:57,715
Speaker 1:  of the biggest companies in tech. Then V Song is gonna come on and we're

33
00:01:57,715 --> 00:02:01,395
Speaker 1:  gonna talk about Smart Glasses. She was at Google IO last week. She saw the

34
00:02:01,395 --> 00:02:05,235
Speaker 1:  Android XR glasses. She's seen Project Mohan, Samsung, and

35
00:02:05,235 --> 00:02:09,035
Speaker 1:  Google's project before. We have thoughts on what Johnny I

36
00:02:09,055 --> 00:02:12,955
Speaker 1:  and Open AI are up to. We have a lot to talk about. It's a very fun show

37
00:02:13,015 --> 00:02:16,355
Speaker 1:  and V thinks more deeply about smart glasses than just about anybody I know.

38
00:02:16,695 --> 00:02:19,715
Speaker 1:  So we're gonna have some fun. We also have a hotline question about Pocket

39
00:02:19,765 --> 00:02:23,035
Speaker 1:  three later app, which is shutting down. People wanted some alternatives.

40
00:02:23,075 --> 00:02:26,515
Speaker 1:  I have some ideas for you. We're gonna get into all of that, just a second.

41
00:02:27,015 --> 00:02:30,235
Speaker 1:  But first I'm, I'm gonna go play Uncharted. It just finished downloading

42
00:02:30,235 --> 00:02:33,195
Speaker 1:  and I'm gonna go play it and then we'll be right back. This is The Vergecast.

43
00:02:33,415 --> 00:02:33,995
Speaker 1:  See you in a sec.

44
00:04:15,005 --> 00:04:18,665
Speaker 1:  on what, how you're feeling after this much time sitting in

45
00:04:19,365 --> 00:04:22,825
Speaker 1:  mostly the Meta trial, but also some of the Google trial down the hall.

46
00:04:23,285 --> 00:04:23,985
Speaker 1:  How are you holding up?

47
00:04:25,375 --> 00:04:29,225
Speaker 5:  It's good. You know, I've gotten a lot of my steps in going between the

48
00:04:29,225 --> 00:04:32,785
Speaker 5:  courtrooms and media rooms running up and down the stairs.

49
00:04:34,135 --> 00:04:38,105
Speaker 5:  Yeah, a little bit of Stockholm syndrome as we were just talking about.

50
00:04:39,125 --> 00:04:42,665
Speaker 5:  But you know, overall it's, you know, it's good to be on the other side.

51
00:04:42,695 --> 00:04:46,465
Speaker 5:  It's a little bittersweet 'cause this is such a historic time. I think

52
00:04:46,465 --> 00:04:50,065
Speaker 5:  that like so much has converged in this one courthouse

53
00:04:50,885 --> 00:04:54,865
Speaker 5:  and I've been covering these cases since they were filed. So

54
00:04:55,005 --> 00:04:58,185
Speaker 5:  It is kind of crazy that it's now somewhat in the rear view,

55
00:04:59,125 --> 00:05:02,665
Speaker 5:  but you know, there's still a long way to go, so don't, don't worry about

56
00:05:02,665 --> 00:05:02,825
Speaker 5:  that.

57
00:05:03,485 --> 00:05:07,105
Speaker 1:  It, It is a weird time in that this moment feels both

58
00:05:07,765 --> 00:05:11,745
Speaker 1:  hugely consequential and also just kind of procedural. And I've been trying

59
00:05:11,745 --> 00:05:15,025
Speaker 1:  to figure out this out about the Google trial in particular. It's like we're,

60
00:05:15,025 --> 00:05:18,945
Speaker 1:  we're in a phase where a judge has to decide whether to break up Google,

61
00:05:19,115 --> 00:05:19,705
Speaker 1:  which is

62
00:05:20,015 --> 00:05:20,705
Speaker 5:  Huge. Yeah.

63
00:05:20,965 --> 00:05:23,465
Speaker 1:  But then Google is like, we don't care what happens, we're just gonna appeal

64
00:05:23,465 --> 00:05:26,785
Speaker 1:  and we're gonna do this all over again several four times. And so part of

65
00:05:26,785 --> 00:05:28,825
Speaker 1:  me is like, what are we even, what are we even doing here?

66
00:05:29,695 --> 00:05:30,345
Speaker 5:  Yeah, I But

67
00:05:30,345 --> 00:05:32,065
Speaker 1:  It's also so important what's happening here.

68
00:05:32,935 --> 00:05:36,785
Speaker 5:  Exactly. And I think it's so clear to me with the Facebook

69
00:05:36,785 --> 00:05:40,625
Speaker 5:  case in particular, like how long it took for them to get to trial

70
00:05:41,185 --> 00:05:44,545
Speaker 5:  and you know, there were, it, part of that was because the initial complaint

71
00:05:44,605 --> 00:05:48,545
Speaker 5:  was thrown out and then the judge gave the FTC an

72
00:05:48,545 --> 00:05:52,385
Speaker 5:  opportunity to refile, but the original case was filed like in

73
00:05:52,385 --> 00:05:56,185
Speaker 5:  2020 and you know, the fact that

74
00:05:56,185 --> 00:06:00,005
Speaker 5:  it took this long to get to trial and in that time TikTok

75
00:06:00,025 --> 00:06:03,765
Speaker 5:  became a huge player in the space. It,

76
00:06:03,875 --> 00:06:07,725
Speaker 5:  it's just, you know, really underscores how quickly tech moves and how

77
00:06:07,725 --> 00:06:09,925
Speaker 5:  slowly the legal system moves.

78
00:06:10,315 --> 00:06:13,405
Speaker 1:  Yeah, I think I, I mostly wanna talk about the Meta trial because I think

79
00:06:13,805 --> 00:06:17,765
Speaker 1:  a, we've, we've talked a bunch about the Google trial and BI think in a

80
00:06:17,765 --> 00:06:20,165
Speaker 1:  certain way it's a little more straightforward just because of where It is

81
00:06:20,165 --> 00:06:23,125
Speaker 1:  in the process. We kind of know what everybody wants. We know what's going

82
00:06:23,125 --> 00:06:27,085
Speaker 1:  on. The Meta trial, I think last we talked

83
00:06:27,865 --> 00:06:31,605
Speaker 1:  was at the very beginning and there was this sense that, okay, the FTC is

84
00:06:31,865 --> 00:06:35,845
Speaker 1:  trying to prove a thing that is going to be very complicated to prove, which

85
00:06:35,845 --> 00:06:39,765
Speaker 1:  is essentially that me we is very important, right? And like there

86
00:06:39,865 --> 00:06:43,765
Speaker 1:  we were having this big debate about whether this is the service

87
00:06:43,795 --> 00:06:47,685
Speaker 1:  that people go to that has monopolized talking to friends and

88
00:06:47,685 --> 00:06:50,165
Speaker 1:  family, that that is like, that's the market. And that we were gonna spend

89
00:06:50,205 --> 00:06:53,645
Speaker 1:  a lot of time over the next six weeks arguing about the market

90
00:06:53,645 --> 00:06:57,365
Speaker 1:  definition of this thing. And I think to your point, I

91
00:06:57,445 --> 00:06:59,525
Speaker 1:  hadn't really thought about this until you just said it, but I think this

92
00:06:59,525 --> 00:07:03,365
Speaker 1:  would've been a really different trial five years ago than it was. So

93
00:07:03,365 --> 00:07:07,165
Speaker 1:  much has changed in the market since this

94
00:07:07,165 --> 00:07:11,045
Speaker 1:  thing got filed originally. But I'm curious now, we, we at

95
00:07:11,045 --> 00:07:13,965
Speaker 1:  the very beginning we were like, okay, this is going to ultimately be about

96
00:07:13,965 --> 00:07:17,885
Speaker 1:  market definition and it seems like the FTC has a pretty steep hill to climb

97
00:07:18,385 --> 00:07:22,105
Speaker 1:  to prove its case here. How do you feel about how that's

98
00:07:22,105 --> 00:07:23,305
Speaker 1:  gone on the other side of this?

99
00:07:24,185 --> 00:07:27,955
Speaker 5:  Yeah, I mean I think market definition is still going to be

100
00:07:28,495 --> 00:07:32,435
Speaker 5:  at the crux of the judge's decision here. And you know,

101
00:07:32,515 --> 00:07:35,875
Speaker 5:  I think as part of that, it's how important

102
00:07:36,375 --> 00:07:40,115
Speaker 5:  is, you know, this kind of core use case that the

103
00:07:40,495 --> 00:07:44,275
Speaker 5:  FTC is pointing to of, you know, being able to go on Facebook and Instagram

104
00:07:44,455 --> 00:07:48,195
Speaker 5:  to share things with your friends and family and see what they're

105
00:07:48,395 --> 00:07:51,475
Speaker 5:  doing. How important is that still to

106
00:07:52,325 --> 00:07:56,075
Speaker 5:  these apps and you know, this market that they've laid out that they

107
00:07:56,255 --> 00:08:00,195
Speaker 5:  say that Meta dominates. And you know, I

108
00:08:00,195 --> 00:08:03,675
Speaker 5:  think what is interesting here is it doesn't have to be

109
00:08:04,185 --> 00:08:08,155
Speaker 5:  necessarily that it's the main thing people do on these apps

110
00:08:08,155 --> 00:08:11,995
Speaker 5:  anymore, but the judge has to feel

111
00:08:11,995 --> 00:08:15,595
Speaker 5:  like it's important enough still that it's, you know,

112
00:08:15,595 --> 00:08:19,475
Speaker 5:  something that Meta can monopolize, that it matters to consumers,

113
00:08:19,475 --> 00:08:23,315
Speaker 5:  that it's not something that's like, you know, gonna die in a year

114
00:08:23,335 --> 00:08:27,075
Speaker 5:  or so. So I think, you know, when I read comments

115
00:08:27,335 --> 00:08:31,275
Speaker 5:  on stories, we write about this, I see people saying, yeah, I

116
00:08:31,365 --> 00:08:34,315
Speaker 5:  don't feel like I can connect with my friends and family as well on these

117
00:08:34,315 --> 00:08:37,715
Speaker 5:  apps anymore. And I think that's something that we hear a lot. So, you know,

118
00:08:37,715 --> 00:08:41,315
Speaker 5:  obviously that's very anecdotal, but you know, the judge is gonna have to

119
00:08:41,315 --> 00:08:45,115
Speaker 5:  decide, is that a few users who just

120
00:08:45,115 --> 00:08:48,995
Speaker 5:  really want to see what their grandchildren are doing on

121
00:08:49,275 --> 00:08:53,115
Speaker 5:  Facebook? Or is this really an important use

122
00:08:53,115 --> 00:08:54,115
Speaker 5:  case for consumers?

123
00:08:54,365 --> 00:08:57,765
Speaker 1:  Right. And it, it seems to me like, I think you're right, there's a lot going

124
00:08:57,765 --> 00:09:01,605
Speaker 1:  on here, but it does seem like a lot of the rest of It is downstream of that

125
00:09:02,005 --> 00:09:05,925
Speaker 1:  question, right? Because if, if, if the thing we're litigating here was,

126
00:09:06,345 --> 00:09:10,285
Speaker 1:  was it illegal for Meta then Facebook to

127
00:09:10,505 --> 00:09:14,445
Speaker 1:  buy Instagram and WhatsApp? The way you get there is

128
00:09:14,465 --> 00:09:17,885
Speaker 1:  by saying Facebook was where people went to hang out with their friends.

129
00:09:18,765 --> 00:09:22,245
Speaker 1:  And then Instagram and WhatsApp showed up providing competitive ways to hang

130
00:09:22,245 --> 00:09:25,885
Speaker 1:  out with their friends. And so Facebook bought them to

131
00:09:25,955 --> 00:09:29,645
Speaker 1:  essentially quash a competitor, right? Like, 'cause it, it, that line actually

132
00:09:29,645 --> 00:09:33,245
Speaker 1:  to me has gotten sort of straighter and straighter over the course of this

133
00:09:33,245 --> 00:09:36,805
Speaker 1:  case that you, you don't have to believe that logic. And I think the question

134
00:09:36,805 --> 00:09:39,485
Speaker 1:  will come down to whether the judge believes that logic, but increasingly

135
00:09:39,485 --> 00:09:42,685
Speaker 1:  to me it feels like It is that few steps, if you start from

136
00:09:43,365 --> 00:09:45,885
Speaker 1:  Facebook is a place where people hang out with friends and family. That's

137
00:09:45,885 --> 00:09:48,605
Speaker 1:  how you get where the FTC is trying to go. And it seems like that has actually

138
00:09:48,675 --> 00:09:51,285
Speaker 1:  sort of crystallized over the last several weeks.

139
00:09:52,035 --> 00:09:55,805
Speaker 5:  Exactly. Yeah. And I think, you know, it's, it's a less straightforward

140
00:09:55,915 --> 00:09:59,605
Speaker 5:  case than the one Meta wants to make, which is that of course we

141
00:09:59,635 --> 00:10:03,445
Speaker 5:  compete with TikTok and YouTube because we're all, you know,

142
00:10:03,725 --> 00:10:07,125
Speaker 5:  everyone has, many people have all these apps on their phone and we're competing

143
00:10:07,185 --> 00:10:10,365
Speaker 5:  for people's time and attention to make sure they click on ours.

144
00:10:11,745 --> 00:10:15,325
Speaker 5:  But, you know, I think what the FTC has been arguing

145
00:10:15,705 --> 00:10:19,205
Speaker 5:  did become more clear throughout trial that they're saying, you know,

146
00:10:20,085 --> 00:10:24,045
Speaker 5:  Facebook was worried that Instagram was going to compete

147
00:10:24,045 --> 00:10:27,965
Speaker 5:  with it at this time when it was struggling to

148
00:10:28,825 --> 00:10:32,645
Speaker 5:  do the shift from desktop to mobile and Instagram was

149
00:10:32,775 --> 00:10:36,445
Speaker 5:  built on mobile and it worried that it would

150
00:10:36,445 --> 00:10:40,325
Speaker 5:  become a real threat to its power. And with WhatsApp, I

151
00:10:40,325 --> 00:10:43,965
Speaker 5:  think maybe the argument is a, a little bit less clear because

152
00:10:44,155 --> 00:10:47,965
Speaker 5:  it's so evident that the founders really were not

153
00:10:47,965 --> 00:10:51,565
Speaker 5:  interested in building a social network. Although they've tried to

154
00:10:51,565 --> 00:10:55,245
Speaker 5:  undermine that argument a little bit. But I think ultimately

155
00:10:55,345 --> 00:10:59,005
Speaker 5:  the fear was also, well maybe they don't wanna do it, but maybe Google

156
00:10:59,155 --> 00:11:02,165
Speaker 5:  buys them and Google is able to make, you know, this major

157
00:11:03,005 --> 00:11:06,645
Speaker 5:  social platform by leveraging this power

158
00:11:06,715 --> 00:11:07,765
Speaker 5:  that WhatsApp has.

159
00:11:08,275 --> 00:11:11,325
Speaker 1:  Well, the other part of the WhatsApp one that is so interesting to me, and

160
00:11:11,325 --> 00:11:14,525
Speaker 1:  I, I confess that throughout this, I have found the WhatsApp piece of this

161
00:11:14,525 --> 00:11:17,765
Speaker 1:  to be vastly more interesting than the Instagram side of things. I think

162
00:11:18,225 --> 00:11:20,725
Speaker 1:  in a certain way the Instagram thing is more sort of straightforward, which

163
00:11:20,725 --> 00:11:24,525
Speaker 1:  is maybe why it's less interesting to me. It's like it was a competitor

164
00:11:25,145 --> 00:11:28,725
Speaker 1:  and did Meta buy it and help it or buy it and hurt it? It is like,

165
00:11:29,285 --> 00:11:31,325
Speaker 1:  that's the question. You can do that, you can do it that which you will.

166
00:11:31,585 --> 00:11:35,245
Speaker 1:  But there's like, it's so funny to rewind back to like 2013

167
00:11:35,385 --> 00:11:38,845
Speaker 1:  before all of this is happening, WhatsApp was a completely different

168
00:11:39,625 --> 00:11:43,445
Speaker 1:  animal. It was like they were charging people a dollar a year to like

169
00:11:43,445 --> 00:11:46,525
Speaker 1:  message their friends around the world and it was growing like crazy and

170
00:11:46,525 --> 00:11:49,245
Speaker 1:  it was a whole different thing and you never heard about them. And like 11

171
00:11:49,245 --> 00:11:53,085
Speaker 1:  people worked at the company and they had no ambitions or so it seemed of

172
00:11:53,575 --> 00:11:56,365
Speaker 1:  being anything more than just that. And yet

173
00:11:57,435 --> 00:12:00,695
Speaker 1:  it was taking over the world. Right. And I think this is the thing that I

174
00:12:00,725 --> 00:12:04,535
Speaker 1:  keep just sort of spinning around in my head is like, at least

175
00:12:04,535 --> 00:12:07,095
Speaker 1:  from what I can tell and from what you've been reporting, there's been a

176
00:12:07,215 --> 00:12:10,535
Speaker 1:  ton of evidence to suggest that Facebook was terrified of how popular WhatsApp

177
00:12:10,535 --> 00:12:10,975
Speaker 1:  was becoming,

178
00:12:12,485 --> 00:12:15,815
Speaker 1:  both because in theory it could become that popular and then add a bunch

179
00:12:15,815 --> 00:12:19,655
Speaker 1:  of other social networky stuff. But also because even back then it seems

180
00:12:19,655 --> 00:12:23,615
Speaker 1:  like Facebook understood that group chat was gonna win and that

181
00:12:23,615 --> 00:12:27,495
Speaker 1:  like pretty fundamentally messaging among friends was going

182
00:12:27,495 --> 00:12:31,295
Speaker 1:  to be the actual next phase of social networking. And that

183
00:12:31,315 --> 00:12:35,255
Speaker 1:  if WhatsApp did that, even if they didn't make any money off of it, it

184
00:12:35,255 --> 00:12:38,815
Speaker 1:  was a huge threat to Facebook's business if people went and used this

185
00:12:39,045 --> 00:12:42,855
Speaker 1:  like adorable mostly free service that had no ambitions of ever

186
00:12:42,855 --> 00:12:46,775
Speaker 1:  trying to be a trillion dollar company. And I just, it's such a weird thing.

187
00:12:46,885 --> 00:12:50,375
Speaker 1:  Like it both was and was not a competitor and I have not, I cannot figure

188
00:12:50,375 --> 00:12:51,655
Speaker 1:  out how to weigh those things in my head.

189
00:12:52,285 --> 00:12:55,615
Speaker 5:  Yeah, it, it's so fascinating 'cause I think one of the core

190
00:12:55,975 --> 00:12:59,535
Speaker 5:  arguments that kept coming up when it came to WhatsApp was like, no,

191
00:12:59,725 --> 00:13:03,495
Speaker 5:  seriously, the founders of WhatsApp do not wanna build a social network.

192
00:13:03,845 --> 00:13:07,255
Speaker 5:  They have absolutely no interest. They don't wanna have an ad model.

193
00:13:07,645 --> 00:13:11,495
Speaker 5:  Like no one is gonna strong arm them into it. And you know,

194
00:13:11,495 --> 00:13:15,135
Speaker 5:  even when Brian Acton, the co-founder came and testified

195
00:13:15,485 --> 00:13:19,295
Speaker 5:  this past week, you know, he kind of said that too, I was

196
00:13:19,295 --> 00:13:22,775
Speaker 5:  not interested in an ad model. I did not wanna build a feed into

197
00:13:23,215 --> 00:13:26,895
Speaker 5:  WhatsApp and my investors were not gonna make me.

198
00:13:27,455 --> 00:13:31,295
Speaker 5:  Interestingly, on cross examination, the FTC was kind of able to pull out,

199
00:13:31,405 --> 00:13:35,135
Speaker 5:  well, you know, when you signed this deal with

200
00:13:35,165 --> 00:13:38,775
Speaker 5:  Meta, you understood that the valuation they were giving you

201
00:13:39,245 --> 00:13:43,175
Speaker 5:  came in part from, you know, them thinking about how this would work in an

202
00:13:43,395 --> 00:13:47,175
Speaker 5:  ad supported business. Mm. And he wasn't able to get them

203
00:13:47,275 --> 00:13:51,255
Speaker 5:  to completely rule out ever running ads on WhatsApp. I think there

204
00:13:51,255 --> 00:13:54,935
Speaker 5:  was some language about monetization in their contract, but

205
00:13:55,395 --> 00:13:59,175
Speaker 5:  you know, it, it seemed like he at least went into it knowing

206
00:13:59,175 --> 00:14:02,455
Speaker 5:  that this was a possibility. And despite all the resistance,

207
00:14:02,945 --> 00:14:06,895
Speaker 5:  maybe if push came to shove one day, WhatsApp would've

208
00:14:06,995 --> 00:14:10,735
Speaker 5:  had an ad supported model or, you know, would've allowed some social

209
00:14:11,135 --> 00:14:15,055
Speaker 5:  features or sold to another company that would've made it into leveraged

210
00:14:15,055 --> 00:14:16,175
Speaker 5:  it into a social play.

211
00:14:16,765 --> 00:14:19,895
Speaker 1:  Yeah. I thought that was so interesting and, and I, I really liked the thing

212
00:14:19,895 --> 00:14:22,935
Speaker 1:  you said at one point that it came up in testimony that

213
00:14:23,775 --> 00:14:27,655
Speaker 1:  Facebook overpaid by somewhere between six and $10 billion for

214
00:14:27,675 --> 00:14:30,855
Speaker 1:  its actual evaluation of WhatsApp. And it's like, oh yeah, okay. Everyone

215
00:14:30,855 --> 00:14:34,815
Speaker 1:  understood that this was a massive overpay for the

216
00:14:34,815 --> 00:14:38,335
Speaker 1:  thing that you were getting. And the inclination was there's, there's, you

217
00:14:38,335 --> 00:14:41,775
Speaker 1:  know, there's, there's money in the couch cushions here. And, and Facebook

218
00:14:41,795 --> 00:14:44,535
Speaker 1:  was actually probably better equipped to get it than almost anybody. And

219
00:14:44,535 --> 00:14:48,375
Speaker 1:  ironically never really figured it out. Like, we're still here in

220
00:14:48,375 --> 00:14:52,325
Speaker 1:  2025 and WhatsApp is like, you know, Titan popular all over

221
00:14:52,325 --> 00:14:56,085
Speaker 1:  the world and it's still not like the earth shattering the

222
00:14:56,245 --> 00:14:59,645
Speaker 1:  humongous business that it needs to be to be worth that valuation. So like

223
00:14:59,645 --> 00:15:03,125
Speaker 1:  history has proven all of that to just be more and more complicated over

224
00:15:03,125 --> 00:15:03,325
Speaker 1:  time.

225
00:15:05,025 --> 00:15:08,245
Speaker 1:  But what, what do you make of the, the WhatsApp side of it in particular?

226
00:15:08,345 --> 00:15:12,205
Speaker 1:  Do you think the, the story that is being told

227
00:15:12,625 --> 00:15:16,605
Speaker 1:  by Meta about what WhatsApp was going to be or might have

228
00:15:16,605 --> 00:15:19,245
Speaker 1:  been planning to be, or why it could have been a threat or whatever, is that,

229
00:15:19,705 --> 00:15:22,805
Speaker 1:  is that working? Who, who is winning this side of the argument here, do you

230
00:15:22,805 --> 00:15:22,885
Speaker 1:  think?

231
00:15:24,085 --> 00:15:27,725
Speaker 5:  I think it's hard to tell. It's kind of complicated because there's some

232
00:15:27,725 --> 00:15:31,205
Speaker 5:  things that seem like they could cut both ways. For example,

233
00:15:31,865 --> 00:15:35,405
Speaker 5:  you know, they show that there were several of these messaging apps that

234
00:15:35,405 --> 00:15:39,005
Speaker 5:  were really exploding at the time and WhatsApp was particularly

235
00:15:39,005 --> 00:15:42,405
Speaker 5:  exploding and exploding in a lot of different markets where some of these

236
00:15:42,405 --> 00:15:46,205
Speaker 5:  other apps like Cacao Talk or Line or WeChat were really more

237
00:15:46,325 --> 00:15:50,205
Speaker 5:  like localized to a certain region. But WhatsApp

238
00:15:50,205 --> 00:15:54,045
Speaker 5:  was actually the only one of those that wasn't really expanding into social

239
00:15:54,405 --> 00:15:58,285
Speaker 5:  features. So arguably it's one that was, you know, the least

240
00:15:58,285 --> 00:16:01,725
Speaker 5:  competitive with Facebook's business and that's the one that Facebook decided

241
00:16:01,725 --> 00:16:05,245
Speaker 5:  to go after. But on the other hand, you know, it was

242
00:16:05,245 --> 00:16:09,045
Speaker 5:  exploding in growth and you know, they saw potential for it maybe in different

243
00:16:09,045 --> 00:16:12,725
Speaker 5:  ways than they saw for these other apps. And so, you know,

244
00:16:12,895 --> 00:16:16,765
Speaker 5:  maybe that's a reason they did end up going after it. So I,

245
00:16:16,845 --> 00:16:20,165
Speaker 5:  I think there's a lot of things like that where it's hard to fully tell which

246
00:16:20,225 --> 00:16:23,565
Speaker 5:  way the judge is going to ultimately take it.

247
00:16:24,065 --> 00:16:27,445
Speaker 5:  And I think it's also tough when you hear over and over from the founders

248
00:16:27,445 --> 00:16:31,405
Speaker 5:  themselves, from everyone who's testified, who had insight into the founders

249
00:16:31,515 --> 00:16:35,245
Speaker 5:  that they did not wanna sell ads, they were not going to be

250
00:16:35,505 --> 00:16:39,165
Speaker 5:  pushed over by their investors. Mark Zuckerberg said he was surprised

251
00:16:39,395 --> 00:16:43,245
Speaker 5:  that, you know, they basically just wanted to run a lifestyle

252
00:16:43,245 --> 00:16:47,005
Speaker 5:  company and compared it to Craigslist. You know, like

253
00:16:47,005 --> 00:16:50,685
Speaker 5:  these were painted as like extremely unambitious people

254
00:16:51,025 --> 00:16:52,045
Speaker 5:  in a certain way

255
00:16:52,425 --> 00:16:56,245
Speaker 1:  As if that's like an outrageously horrible thing to want is to just

256
00:16:56,465 --> 00:16:58,005
Speaker 1:  be Craigslist, right?

257
00:16:58,145 --> 00:17:02,085
Speaker 5:  But it's also, it's somewhat hard to believe when then you're seeing, well

258
00:17:02,085 --> 00:17:06,005
Speaker 5:  these are founders who got billions of dollars for this app

259
00:17:06,005 --> 00:17:09,525
Speaker 5:  that they created. Like were they really that unambitious and

260
00:17:09,795 --> 00:17:13,685
Speaker 5:  were they really going to, you know, leave their investors and their employees

261
00:17:13,685 --> 00:17:14,765
Speaker 5:  hanging at the end of the day

262
00:17:15,155 --> 00:17:18,405
Speaker 1:  They did go buy yachts, you know what I mean? Like at, at some point you

263
00:17:18,405 --> 00:17:22,285
Speaker 1:  did buy a yacht. Yeah, Yeah, yeah. So

264
00:17:22,285 --> 00:17:25,765
Speaker 1:  I, I think I, I I think that's right and it, it, one of the things that has

265
00:17:25,765 --> 00:17:29,405
Speaker 1:  struck me just in, in reading and reporting is that it, it seems so much

266
00:17:29,515 --> 00:17:33,005
Speaker 1:  like the, the vibe in the courtroom has just sort of

267
00:17:33,155 --> 00:17:36,645
Speaker 1:  whipsawed back and forth in a way that I think I'm not used to in covering

268
00:17:36,645 --> 00:17:40,325
Speaker 1:  a lot of these trials. You sort of see momentum kind of builds

269
00:17:40,505 --> 00:17:44,325
Speaker 1:  and, and waxes and wanes a little bit, but it feels like even

270
00:17:44,325 --> 00:17:47,725
Speaker 1:  like witness to witness in this, you've had these like

271
00:17:47,985 --> 00:17:50,525
Speaker 1:  wildly different stories about what's going on. And I just think about like

272
00:17:50,795 --> 00:17:54,645
Speaker 1:  from from Kevin's Strom to Adam er talking about what's going on at

273
00:17:54,645 --> 00:17:58,365
Speaker 1:  Instagram, even with the same people asking them the same

274
00:17:58,375 --> 00:18:02,045
Speaker 1:  kinds of questions, just completely different perspectives on how all of

275
00:18:02,045 --> 00:18:04,525
Speaker 1:  this has worked. And on the one hand, like of course that's true, but on

276
00:18:04,525 --> 00:18:08,365
Speaker 1:  the other hand, this whole thing just strikes me as like, it has gotten sort

277
00:18:08,365 --> 00:18:11,765
Speaker 1:  of more unknowable than ever over time. And

278
00:18:12,315 --> 00:18:15,405
Speaker 1:  like on the Instagram side, it seems like the, the big question is

279
00:18:16,145 --> 00:18:19,885
Speaker 1:  did Meta help or hurt Instagram? Right? Like, is that, is that a, am I

280
00:18:20,065 --> 00:18:23,165
Speaker 1:  overgeneralizing to say that's kind of what it boils down to? Yeah,

281
00:18:23,285 --> 00:18:26,765
Speaker 5:  I, I think for sure, and I think we saw that, especially in

282
00:18:27,255 --> 00:18:31,085
Speaker 5:  Kevin RA's testimony, the co-founder of Instagram

283
00:18:31,135 --> 00:18:34,685
Speaker 5:  where, you know, it was really about, okay, you know,

284
00:18:35,385 --> 00:18:39,245
Speaker 5:  he took this offer from Meta thinking that they're really going to

285
00:18:39,245 --> 00:18:43,205
Speaker 5:  help grow the company. And in a lot of ways it's really hard to

286
00:18:43,355 --> 00:18:46,925
Speaker 5:  look at Instagram and see, you know, billions of users

287
00:18:47,345 --> 00:18:50,765
Speaker 5:  and you know, this huge platform for

288
00:18:51,205 --> 00:18:54,845
Speaker 5:  advertising and think that it's could have been more

289
00:18:54,845 --> 00:18:58,565
Speaker 5:  successful than It is today. But on the other hand, Kevin

290
00:18:58,925 --> 00:19:02,845
Speaker 5:  Strom seemed pretty confident that it could be, and he could have

291
00:19:02,885 --> 00:19:06,645
Speaker 5:  done it himself, and that, you know, Meta withheld resources for

292
00:19:06,665 --> 00:19:10,445
Speaker 5:  growth, for integrity that, you know, they could have

293
00:19:10,445 --> 00:19:14,165
Speaker 5:  used to make Instagram better in various ways. So

294
00:19:14,795 --> 00:19:18,365
Speaker 5:  it's really hard to say, you know, is this just another Silicon

295
00:19:18,545 --> 00:19:22,085
Speaker 5:  Valley executive just like blowing steam

296
00:19:22,505 --> 00:19:26,165
Speaker 5:  or is this, you know, really what could have happened. And

297
00:19:26,485 --> 00:19:30,445
Speaker 5:  I think that's always the challenge in antitrust cases is what would've happened

298
00:19:30,545 --> 00:19:33,205
Speaker 5:  if, you know, this major event didn't occur. Yeah.

299
00:19:33,205 --> 00:19:36,005
Speaker 1:  Have they used the phrase the but for world a lot? Oh yes.

300
00:19:37,265 --> 00:19:41,245
Speaker 1:  The Google case has also been full of the but for world. Yes. Which I

301
00:19:41,245 --> 00:19:44,285
Speaker 1:  enjoy very much. And It is like a phrase I say to myself all the time now,

302
00:19:44,525 --> 00:19:48,045
Speaker 1:  and I'm like, well, in a but for world I wouldn't be having goldfish right

303
00:19:48,045 --> 00:19:51,885
Speaker 1:  now, but today I'm eating goldfish. It's, it's very good.

304
00:19:53,035 --> 00:19:56,685
Speaker 1:  Have you found yourself as an observer being in the courtroom all day,

305
00:19:57,395 --> 00:20:00,645
Speaker 1:  leaning towards one side or the other of the Instagram argument

306
00:20:01,225 --> 00:20:04,525
Speaker 1:  has, is, is, is, are you compelled by one version of the argument or the

307
00:20:04,525 --> 00:20:04,645
Speaker 1:  other?

308
00:20:06,155 --> 00:20:10,115
Speaker 5:  I mean, I'd say it's not necessarily that I'm compelled one way

309
00:20:10,115 --> 00:20:13,715
Speaker 5:  or the other, but I find that the FTC has a pretty

310
00:20:14,165 --> 00:20:17,555
Speaker 5:  tough case here. 'cause I think it's just, it's

311
00:20:18,455 --> 00:20:21,875
Speaker 5:  not that they can't win it and not that, you know, they haven't presented

312
00:20:21,875 --> 00:20:25,635
Speaker 5:  strong arguments. I just think it's obviously they have the burden to

313
00:20:25,635 --> 00:20:29,395
Speaker 5:  begin with. And it's really difficult looking at

314
00:20:29,395 --> 00:20:33,275
Speaker 5:  Instagram and seeing, you know, at one point Meta's chief

315
00:20:33,275 --> 00:20:36,235
Speaker 5:  marketing officer Alex Schultz testified about how

316
00:20:36,665 --> 00:20:40,635
Speaker 5:  Instagram has seen such growth that, you know, they have reached almost

317
00:20:40,705 --> 00:20:44,635
Speaker 5:  like all of the something like 250 million potential

318
00:20:45,235 --> 00:20:48,605
Speaker 5:  eligible smartphone users in the US who could

319
00:20:49,325 --> 00:20:53,245
Speaker 5:  possibly be on this app. And to grow any more than

320
00:20:53,245 --> 00:20:57,125
Speaker 5:  that is, that's really gonna happen at a much slower

321
00:20:57,155 --> 00:21:00,925
Speaker 5:  rate than it used to. And so later on, judge

322
00:21:00,985 --> 00:21:04,965
Speaker 5:  Boberg had brought that up to the FTCs expert and said, you know, could

323
00:21:04,965 --> 00:21:08,725
Speaker 5:  this really have been more successful than where It is now? They

324
00:21:08,725 --> 00:21:12,565
Speaker 5:  have like almost all the users they could get. And I think that

325
00:21:12,615 --> 00:21:16,605
Speaker 5:  makes this really challenging. But at the same time, I, I

326
00:21:16,605 --> 00:21:19,565
Speaker 5:  guess the argument from the FTC is, well yeah, they have all these users

327
00:21:19,705 --> 00:21:23,565
Speaker 5:  but they have nowhere else to go. So, you know, that's kind of

328
00:21:23,905 --> 00:21:27,605
Speaker 5:  the point there. Yeah, I, I think it's just, it's difficult because

329
00:21:28,495 --> 00:21:32,485
Speaker 5:  we're in a world now with TikTok and I think friends and family

330
00:21:32,995 --> 00:21:36,845
Speaker 5:  does seem like it's still important. The question is just how important and

331
00:21:36,845 --> 00:21:37,445
Speaker 5:  for how long.

332
00:21:38,075 --> 00:21:41,405
Speaker 1:  Talk me through that actually. 'cause I think it was, it was Tom Allison,

333
00:21:41,665 --> 00:21:44,325
Speaker 1:  the head of Facebook who was on the stand just the other day, right? Talking

334
00:21:44,325 --> 00:21:48,245
Speaker 1:  about this. And he, like some other Meta

335
00:21:48,565 --> 00:21:51,965
Speaker 1:  employees who have testified during this trial kind of

336
00:21:52,145 --> 00:21:56,085
Speaker 1:  seemed to argue out of both sides of their mouth about like, yes, these are

337
00:21:56,285 --> 00:21:59,485
Speaker 1:  platforms where people connect with friends and family and that is important

338
00:21:59,505 --> 00:22:02,405
Speaker 1:  and It is core to what we're doing. And Mark Zuckerberg is out here in, in

339
00:22:02,405 --> 00:22:04,885
Speaker 1:  the meanwhile like talking about OG Facebook and wanting to bring all that

340
00:22:04,885 --> 00:22:08,365
Speaker 1:  back. And yet over and over these people seem very happy

341
00:22:09,185 --> 00:22:12,125
Speaker 1:  to talk about the fact that actually no, this isn't for friends and family

342
00:22:12,425 --> 00:22:16,005
Speaker 1:  at all. Help me figure out what's really going on there.

343
00:22:16,475 --> 00:22:20,205
Speaker 5:  Yeah, I, I mean I think this is a really interesting

344
00:22:20,205 --> 00:22:23,805
Speaker 5:  element of this trial and I think something where I think the

345
00:22:24,225 --> 00:22:27,605
Speaker 5:  FTC does have something to kind of sink their teeth in here. And

346
00:22:28,125 --> 00:22:32,085
Speaker 5:  I think with Tom Allison's testimony, one thing that was interesting

347
00:22:32,085 --> 00:22:35,965
Speaker 5:  to me was when they brought up this new kind of, yeah, the

348
00:22:36,285 --> 00:22:39,845
Speaker 5:  OG Facebook where they're focusing back in on

349
00:22:39,845 --> 00:22:43,485
Speaker 5:  connecting with your friends, like bringing you back to your roots. And to

350
00:22:43,485 --> 00:22:47,085
Speaker 5:  me that felt like some kind of internal recognition from

351
00:22:47,435 --> 00:22:50,565
Speaker 5:  Meta that this is still something people want.

352
00:22:51,305 --> 00:22:54,965
Speaker 5:  And you know, I think the company was kind of arguing, well

353
00:22:55,155 --> 00:22:58,925
Speaker 5:  yeah, like we know this is something that some people want. And

354
00:22:59,185 --> 00:23:03,165
Speaker 5:  the fact is that now, because most people really, you

355
00:23:03,165 --> 00:23:06,485
Speaker 5:  know, they might tell us they wanna see friends and family, but if we only

356
00:23:06,485 --> 00:23:10,085
Speaker 5:  show them friends and family posts, they're gonna leave. And so that their

357
00:23:10,085 --> 00:23:13,765
Speaker 5:  actions kind of tell us they want more engaging content than whatever, you

358
00:23:13,765 --> 00:23:17,565
Speaker 5:  know, your uncle is posting on Facebook. Sure. So they

359
00:23:17,705 --> 00:23:21,605
Speaker 5:  end up putting more, you know, unconnected content, content from people you

360
00:23:21,605 --> 00:23:25,405
Speaker 5:  don't follow in your feed. So, you know, one thing that was interesting

361
00:23:25,425 --> 00:23:29,245
Speaker 5:  was Tom Allison said, if you did really wanna see only your friend's content,

362
00:23:29,655 --> 00:23:33,325
Speaker 5:  you'd have to scroll like all day to see all of it. Yeah. Because

363
00:23:34,295 --> 00:23:37,685
Speaker 5:  you'd have so much of this unconnected content in between.

364
00:23:38,225 --> 00:23:41,725
Speaker 5:  So, you know, they figure for the people who really wanna see that, let's

365
00:23:41,725 --> 00:23:45,005
Speaker 5:  stick it in a tab and they can go to it when they want. So,

366
00:23:45,505 --> 00:23:49,485
Speaker 5:  you know, on the one hand maybe that says that this is really

367
00:23:49,485 --> 00:23:53,405
Speaker 5:  something that's a novelty or you know, just a small

368
00:23:53,405 --> 00:23:56,965
Speaker 5:  part of what we do. And now a bigger part is all of this like

369
00:23:57,145 --> 00:24:01,125
Speaker 5:  algorithmically recommended content. But on the other hand you could say

370
00:24:01,615 --> 00:24:05,005
Speaker 5:  while they're recognizing that It is something that's still important, even

371
00:24:05,005 --> 00:24:08,525
Speaker 5:  if it's less important than, you know,

372
00:24:08,585 --> 00:24:12,245
Speaker 5:  things like reels and things of that nature. And I think

373
00:24:12,345 --> 00:24:16,125
Speaker 5:  that's why we heard throughout trial Meadow would say something

374
00:24:16,125 --> 00:24:19,525
Speaker 5:  like, well, you know, it's becoming a much smaller part of our business.

375
00:24:20,305 --> 00:24:24,245
Speaker 5:  And then the FTC would ask, well, but in absolute

376
00:24:24,375 --> 00:24:28,285
Speaker 5:  terms, is it still growing? And the answer would usually be

377
00:24:28,305 --> 00:24:32,165
Speaker 5:  yes. So, you know, yes it's small, but small on,

378
00:24:32,265 --> 00:24:33,645
Speaker 5:  you know, a really large base.

379
00:24:34,215 --> 00:24:36,845
Speaker 1:  Right. Well, I mean to me it makes me think of like, have you ever heard

380
00:24:36,845 --> 00:24:39,805
Speaker 1:  the saying that there are, there are three u's there's the, you think you

381
00:24:39,805 --> 00:24:42,605
Speaker 1:  are the, you other people think you are, and then the, you actually are like,

382
00:24:42,605 --> 00:24:45,525
Speaker 1:  yeah, I, I've been thinking about this so much with Facebook in the context

383
00:24:45,585 --> 00:24:49,445
Speaker 1:  of this whole trial that like the, the you that Facebook thinks It is

384
00:24:50,345 --> 00:24:53,245
Speaker 1:  has always been about friends and family, right? Like It is, It is a place

385
00:24:53,245 --> 00:24:57,205
Speaker 1:  for real people to connect to real people. And I think that is like the reason

386
00:24:57,695 --> 00:25:01,405
Speaker 1:  Zuckerberg is so into this idea of the OG Facebook is partly about people

387
00:25:01,405 --> 00:25:04,845
Speaker 1:  use it, but it's also about, like that is, that is his conception of what

388
00:25:04,845 --> 00:25:08,485
Speaker 1:  this thing does and why It is important and why It is valuable and why it

389
00:25:08,485 --> 00:25:11,725
Speaker 1:  became what It is now. I don't think Mark Zuckerberg's

390
00:25:11,935 --> 00:25:15,645
Speaker 1:  conception of why Facebook is great is because it shows you all the

391
00:25:15,865 --> 00:25:19,805
Speaker 1:  AI nonsense, right? But I think, I think that's what people see on Facebook

392
00:25:19,805 --> 00:25:22,605
Speaker 1:  now that is, that is our conception of Facebook and that is what Facebook

393
00:25:22,605 --> 00:25:26,365
Speaker 1:  has become for so many of us. Like I don't, I have probably

394
00:25:26,435 --> 00:25:30,325
Speaker 1:  been on Facebook to see what my friends are up to one time in

395
00:25:30,325 --> 00:25:33,285
Speaker 1:  the last decade, otherwise it's like marketplace and

396
00:25:33,905 --> 00:25:37,445
Speaker 1:  groups. That's it. That's what Facebook is for me. And I think that's true

397
00:25:37,445 --> 00:25:41,165
Speaker 1:  for a lot of people. And so this question of like, I, I just keep seeing

398
00:25:41,165 --> 00:25:45,045
Speaker 1:  people on the stand be like, okay, yes, I said in an interview that it's

399
00:25:45,045 --> 00:25:47,965
Speaker 1:  for friends and family. Yes, it's on our website that it's for friends and

400
00:25:47,965 --> 00:25:50,845
Speaker 1:  family. Yes, we tell people all the time that it's for friends and family,

401
00:25:51,185 --> 00:25:54,045
Speaker 1:  but when you look at how people actually use it, it's not remotely for friends

402
00:25:54,065 --> 00:25:57,965
Speaker 1:  and family and Yeah, like there was the, the thing Tom,

403
00:25:58,045 --> 00:26:01,525
Speaker 1:  I think it was Tom Allison who said a lot of people now make Facebook accounts

404
00:26:01,525 --> 00:26:05,125
Speaker 1:  and have no friends. Yeah, yeah. And that to me is, so that was such an eyeopening

405
00:26:05,125 --> 00:26:09,045
Speaker 1:  moment of like, oh, that is a complete inversion of the whole idea

406
00:26:09,045 --> 00:26:11,485
Speaker 1:  of what Facebook was supposed to be when we get to that point.

407
00:26:12,275 --> 00:26:16,005
Speaker 5:  Exactly. Yeah. That was really interesting. And you know, I think he's basically

408
00:26:16,005 --> 00:26:19,605
Speaker 5:  saying those people maybe are going on marketplace groups, whatever. But

409
00:26:19,605 --> 00:26:22,805
Speaker 1:  I also think that I bet, and this would be an interesting experiment to run.

410
00:26:22,805 --> 00:26:26,525
Speaker 1:  You could show up on Facebook, make zero friends and still have an infinite

411
00:26:26,525 --> 00:26:28,565
Speaker 1:  feed of content immediately available to you.

412
00:26:28,835 --> 00:26:32,085
Speaker 5:  Yeah. And that's what makes the experience so different than it used to be.

413
00:26:32,085 --> 00:26:35,885
Speaker 5:  Right. But I think at the same time, like if I just think as like a very

414
00:26:35,885 --> 00:26:39,205
Speaker 5:  casual social media user, if I'm going to

415
00:26:39,665 --> 00:26:43,445
Speaker 5:  go try to see what a friend's up to or you know, message

416
00:26:43,445 --> 00:26:47,405
Speaker 5:  them or something, I'm not really going to TikTok and that's kind of their

417
00:26:47,405 --> 00:26:51,285
Speaker 5:  point here. I'm going to Instagram or Facebook. Right. And you know, maybe

418
00:26:51,345 --> 00:26:55,165
Speaker 5:  I'm doing those things less on Instagram and Facebook in general than I used

419
00:26:55,165 --> 00:26:58,805
Speaker 5:  to. And maybe I'm doing more of it in just regular old messaging, but

420
00:26:59,505 --> 00:27:03,245
Speaker 5:  I'm not really going to TikTok for that. And maybe some people

421
00:27:03,385 --> 00:27:07,165
Speaker 5:  are, but one of tiktoks executives even said that their

422
00:27:07,165 --> 00:27:11,005
Speaker 5:  friends tab about like 1% of users are watching videos

423
00:27:11,145 --> 00:27:11,845
Speaker 5:  in there. So

424
00:27:12,205 --> 00:27:15,205
Speaker 1:  I honestly didn't know the friends tab existed until Exactly they said that

425
00:27:15,205 --> 00:27:17,565
Speaker 1:  on the stand. Like straight up had no idea. Yeah.

426
00:27:19,115 --> 00:27:22,565
Speaker 1:  Yeah. I think to me that has actually been the most compelling part of the

427
00:27:22,835 --> 00:27:26,805
Speaker 1:  FTCs argument here is that it, it's not necessarily that

428
00:27:26,805 --> 00:27:30,725
Speaker 1:  this is the main thing people do, it's that if this is

429
00:27:31,515 --> 00:27:34,725
Speaker 1:  what you want to do, there is nowhere else to go. And the reason there is

430
00:27:34,725 --> 00:27:38,125
Speaker 1:  nowhere else to go is because of Facebook. Like that's just true. Whether

431
00:27:38,185 --> 00:27:42,045
Speaker 1:  you can prove that in, in court, I don't know. But like

432
00:27:42,555 --> 00:27:46,405
Speaker 1:  I've, I, you and I have been covering the space to know long enough

433
00:27:46,405 --> 00:27:49,565
Speaker 1:  to know that like Facebook ate that market, it, it just did.

434
00:27:50,625 --> 00:27:53,045
Speaker 1:  And then we get back to the question of like, if Instagram had been left

435
00:27:53,045 --> 00:27:55,405
Speaker 1:  alone, could it have done? And so that's a whole separate thing. But, but

436
00:27:55,405 --> 00:27:59,245
Speaker 1:  that piece of it I find very compelling that, that FTC is like, this is

437
00:27:59,285 --> 00:28:02,805
Speaker 1:  a thing people wanna do, and you have made sure that there is nowhere else

438
00:28:02,825 --> 00:28:06,485
Speaker 1:  for them to do it. And I think every time that they've done a successful

439
00:28:06,485 --> 00:28:10,445
Speaker 1:  job of coming back to that is when I end up like back on the FTC side of

440
00:28:10,445 --> 00:28:13,805
Speaker 1:  this case because I'm like, no, you're right. This may not be, we don't have

441
00:28:13,805 --> 00:28:17,565
Speaker 1:  to litigate everything that Facebook is, we just have to litigate that part.

442
00:28:17,825 --> 00:28:20,605
Speaker 1:  And I think whether the judge buys that sort of,

443
00:28:21,625 --> 00:28:25,205
Speaker 1:  you know, slicing of the pie of Facebook remains to be seen. But I do find

444
00:28:25,205 --> 00:28:27,365
Speaker 1:  that part of the argument pretty convincing.

445
00:28:28,165 --> 00:28:31,245
Speaker 5:  I agree with that. And I think that's where I wonder, you know, judge Boberg

446
00:28:31,245 --> 00:28:34,645
Speaker 5:  has kind of admitted he is not really a social media user. So,

447
00:28:35,425 --> 00:28:39,245
Speaker 5:  you know, Adam, er, before the trial started did a tutorial where he

448
00:28:39,245 --> 00:28:43,165
Speaker 5:  walked him through these apps No. Where it just, you know, felt like a,

449
00:28:43,425 --> 00:28:45,605
Speaker 5:  you know, teenager explaining to their dad how

450
00:28:45,605 --> 00:28:48,645
Speaker 1:  What I would give to be a fly on the wall for that. Yeah. Like in the judges'

451
00:28:48,645 --> 00:28:52,245
Speaker 1:  chambers, just Oh, scrolling. I was in the program. Oh, you were there. Yeah.

452
00:28:52,465 --> 00:28:56,165
Speaker 1:  Was it, was it, was it the most awkward thing in history or was it amazing?

453
00:28:56,865 --> 00:29:00,045
Speaker 5:  It was. So, I mean, throughout the trial, one thing that's happened is Judge

454
00:29:00,145 --> 00:29:03,565
Speaker 5:  Boberg played basketball at Yale, I believe.

455
00:29:03,955 --> 00:29:07,485
Speaker 5:  Cool. And so everyone is working basketball references

456
00:29:07,995 --> 00:29:11,765
Speaker 5:  into their exhibits. So we had some like NBA videos

457
00:29:12,245 --> 00:29:13,805
Speaker 5:  referenced in the Instagram demo.

458
00:29:14,465 --> 00:29:17,605
Speaker 1:  That's very good. Yeah. Let, let's talk about TikTok for a minute because

459
00:29:17,605 --> 00:29:20,405
Speaker 1:  you, you brought this up and it does feel like, you know, back to the, what

460
00:29:20,405 --> 00:29:24,005
Speaker 1:  would this case have been like if it was tried five years ago? TikTok is

461
00:29:24,005 --> 00:29:27,685
Speaker 1:  sort of the big wrench here, right? And, and it has come up a lot kind of

462
00:29:27,685 --> 00:29:31,365
Speaker 1:  on both sides of the argument. How would you explain sort of

463
00:29:31,435 --> 00:29:35,205
Speaker 1:  tiktoks role in what this case has been? Because it does seem like It is

464
00:29:35,205 --> 00:29:39,045
Speaker 1:  everybody's favorite comparison in all directions and

465
00:29:39,045 --> 00:29:39,605
Speaker 1:  it's very strange.

466
00:29:40,035 --> 00:29:43,295
Speaker 5:  Yeah, I mean I think TikTok is just kind of the elephant in the room because

467
00:29:43,805 --> 00:29:47,175
Speaker 5:  yeah, it existed when the FTC filed this case, but

468
00:29:48,315 --> 00:29:52,255
Speaker 5:  it really was nothing like It is today. That has become such a huge

469
00:29:52,255 --> 00:29:56,135
Speaker 5:  competitor for Meta in a general sense of the term. You know,

470
00:29:56,135 --> 00:29:59,815
Speaker 5:  if you told anyone that Meta doesn't compete with

471
00:29:59,815 --> 00:30:03,735
Speaker 5:  TikTok, they'd be like, what are you talking about? Right? But you

472
00:30:03,735 --> 00:30:06,335
Speaker 5:  know, we're talking about this very like legal

473
00:30:07,485 --> 00:30:11,215
Speaker 5:  case where it has a very specific definition about

474
00:30:11,445 --> 00:30:15,175
Speaker 5:  what is really a competitor. And so in this case, the

475
00:30:15,475 --> 00:30:19,415
Speaker 5:  FTC says that we're talking about personal social networking services, which

476
00:30:19,435 --> 00:30:23,375
Speaker 5:  are used to connect with friends and family. They have a social

477
00:30:23,735 --> 00:30:26,935
Speaker 5:  graph, which means, you know, they connect, they, you know, kind of map out

478
00:30:26,935 --> 00:30:30,695
Speaker 5:  like all of your connections based on you know, who you know

479
00:30:30,795 --> 00:30:34,495
Speaker 5:  versus like what you're interested in, which is more of how TikTok does it.

480
00:30:35,435 --> 00:30:39,255
Speaker 5:  And it, they say in that market it's really just Facebook,

481
00:30:39,255 --> 00:30:42,895
Speaker 5:  Instagram, Snapchat, and then some of these other apps like me, we and BeReal.

482
00:30:43,635 --> 00:30:47,455
Speaker 5:  And So I think with TikTok, you know, they're saying that's more of a passive

483
00:30:47,465 --> 00:30:50,655
Speaker 5:  experience. You're, you know, scrolling through, you're getting served

484
00:30:51,295 --> 00:30:54,375
Speaker 5:  different videos that you might be interested in, but it's not really about

485
00:30:54,515 --> 00:30:58,135
Speaker 5:  who you know, or, you know, you're not really gonna like message a friend

486
00:30:58,155 --> 00:31:01,295
Speaker 5:  on there, maybe you will, but like, that's not really what you're going to

487
00:31:01,295 --> 00:31:04,815
Speaker 5:  TikTok for. And So I think that's really what it comes down to.

488
00:31:05,595 --> 00:31:09,415
Speaker 5:  And what Meta says is that, well, it doesn't really matter

489
00:31:09,755 --> 00:31:13,655
Speaker 5:  why users specifically come to the app because first of all,

490
00:31:13,735 --> 00:31:17,655
Speaker 5:  a lot of users come to Instagram to do the same thing that they do

491
00:31:17,755 --> 00:31:21,735
Speaker 5:  on TikTok, which is scroll reels, right? And second of all, they say

492
00:31:21,735 --> 00:31:25,175
Speaker 5:  it's really about time and attention because that's what really

493
00:31:25,175 --> 00:31:29,135
Speaker 5:  constrain our power is we're thinking about how do we make sure that

494
00:31:29,135 --> 00:31:32,935
Speaker 5:  people spend more time on our apps versus on TikTok or

495
00:31:32,935 --> 00:31:36,615
Speaker 5:  something else. And you know, that's what we should be focused on. Because

496
00:31:37,345 --> 00:31:41,295
Speaker 5:  internally you can see that that's what we look at. We look at, you know,

497
00:31:41,295 --> 00:31:45,215
Speaker 5:  how do we win users' time? We're not saying like, oh, let's like try

498
00:31:45,215 --> 00:31:47,175
Speaker 5:  to get more of the friends and family sharing group.

499
00:31:47,865 --> 00:31:51,135
Speaker 1:  Can't both of those things be true? At the same time, this is what I, I'm

500
00:31:51,295 --> 00:31:55,255
Speaker 1:  so hung up on this, like, the, the, every time anybody makes an

501
00:31:55,375 --> 00:31:58,015
Speaker 1:  argument about TikTok, I'm like, of course you're right. Everybody's like,

502
00:31:58,165 --> 00:32:02,135
Speaker 1:  everybody's like, no, people don't do the same things sort

503
00:32:02,135 --> 00:32:06,095
Speaker 1:  of in relation to each other that they do on Facebook. And even

504
00:32:06,095 --> 00:32:09,815
Speaker 1:  Instagram, like my Instagram experience in terms of like how I

505
00:32:10,015 --> 00:32:13,685
Speaker 1:  interact with my friends is completely different from TikTok, but most of

506
00:32:13,685 --> 00:32:16,845
Speaker 1:  the time I open Instagram or TikTok, it's, it's for the same purpose like

507
00:32:16,845 --> 00:32:20,485
Speaker 1:  you're saying. And so everybody keeps bringing up this like, was it an

508
00:32:20,485 --> 00:32:21,365
Speaker 1:  Instagram outage?

509
00:32:22,265 --> 00:32:24,285
Speaker 5:  It was the Meta outage. It was the Meta 21

510
00:32:24,285 --> 00:32:28,045
Speaker 1:  Meta, yeah. So everybody brings up this outage and, and points out

511
00:32:28,125 --> 00:32:32,045
Speaker 1:  where people went as, as a way of saying, okay, who are our actual

512
00:32:32,045 --> 00:32:34,605
Speaker 1:  competitors? Like, where do they go when they're not here?

513
00:32:36,265 --> 00:32:39,525
Speaker 1:  And like, it's the obvious place, like everybody went to YouTube and TikTok,

514
00:32:39,525 --> 00:32:42,565
Speaker 1:  right? And so yeah, Meta is like, of course that's where they went. They're,

515
00:32:42,565 --> 00:32:46,445
Speaker 1:  those are our competitors and that's also obviously true. And

516
00:32:46,445 --> 00:32:49,885
Speaker 1:  so like I, I just don't know how to piece those two things apart or if I

517
00:32:49,885 --> 00:32:52,965
Speaker 1:  even need to that like these two sides just sort of seem like they're saying

518
00:32:52,965 --> 00:32:54,565
Speaker 1:  true things that have nothing to do with each other.

519
00:32:55,455 --> 00:32:59,325
Speaker 5:  Right. Well then I think the other part of that is that the FTC then points

520
00:32:59,385 --> 00:33:03,045
Speaker 5:  out that, yeah, okay, there's like some percentage that

521
00:33:03,465 --> 00:33:06,885
Speaker 5:  was diverted to these other apps, but this doesn't add up to a hundred percent

522
00:33:07,235 --> 00:33:11,205
Speaker 5:  some of these diverted users just like logged off. Like we

523
00:33:11,205 --> 00:33:13,685
Speaker 5:  dunno, maybe they went to, maybe they went to other apps, but maybe they

524
00:33:13,685 --> 00:33:17,285
Speaker 5:  also like touched grass or went to sleep sleep. So I think

525
00:33:17,285 --> 00:33:19,045
Speaker 1:  We're suing grass it's time. Yeah.

526
00:33:19,865 --> 00:33:23,605
Speaker 5:  And you know, actually Judge Boberg has kind of touched on this

527
00:33:23,605 --> 00:33:27,565
Speaker 5:  before because he, when he was ruling on the motion for

528
00:33:27,565 --> 00:33:31,485
Speaker 5:  summary judgment late last year, he said that, you know, the fact

529
00:33:31,485 --> 00:33:35,165
Speaker 5:  that a Meta competes with TikTok for time and attention

530
00:33:35,385 --> 00:33:39,045
Speaker 5:  is true, but beside the point. And

531
00:33:39,185 --> 00:33:43,045
Speaker 5:  he also said quote, Meta competes not just with YouTube, TikTok and

532
00:33:43,125 --> 00:33:46,405
Speaker 5:  X, but also with watching a movie at a friend's house, reading a book at

533
00:33:46,405 --> 00:33:50,125
Speaker 5:  the library and playing online poker. Antitrust law does not require

534
00:33:50,605 --> 00:33:53,685
Speaker 5:  consideration of such an infinite range of possible substitutes.

535
00:33:54,585 --> 00:33:58,125
Speaker 5:  So I think Okay, you know, that's gonna be somewhat of a difficult argument

536
00:33:58,145 --> 00:34:02,045
Speaker 5:  for Meta to make that, you know, it really does matter that we compete for

537
00:34:02,045 --> 00:34:05,845
Speaker 5:  time and attention. And I think they would say it's because, you know, people

538
00:34:05,875 --> 00:34:09,205
Speaker 5:  have multiple apps on their phones already. And Yeah, we've like almost reached

539
00:34:09,255 --> 00:34:13,165
Speaker 5:  saturation maybe with how many more people could download it. So at this

540
00:34:13,165 --> 00:34:16,405
Speaker 5:  point we're competing on, you know, who spends more time on each app.

541
00:34:16,905 --> 00:34:19,845
Speaker 1:  So this comes back yet again to market definition. Yeah. Because like if,

542
00:34:19,865 --> 00:34:23,845
Speaker 1:  if you can convince Judge Roseberg to buy the idea that there is

543
00:34:23,925 --> 00:34:27,285
Speaker 1:  a market for I want to find my friends and family on the internet,

544
00:34:27,875 --> 00:34:31,085
Speaker 1:  then we're somewhere. But if you can't, I think

545
00:34:31,785 --> 00:34:35,365
Speaker 1:  all like rational evidence suggests that you can't

546
00:34:36,075 --> 00:34:39,685
Speaker 1:  come at Meta for, it's a place where people watch videos

547
00:34:40,225 --> 00:34:44,085
Speaker 1:  and, and So I, this this also now kind of makes the FTC ass

548
00:34:44,085 --> 00:34:47,925
Speaker 1:  whole plan here makes sense. Like we, we even talked about at the beginning,

549
00:34:47,925 --> 00:34:51,125
Speaker 1:  this is a pretty tricky needle to thread,

550
00:34:51,905 --> 00:34:54,525
Speaker 1:  but by the, the way you're talking about it and the way the judge sees it,

551
00:34:54,525 --> 00:34:55,645
Speaker 1:  that's kind of, its only move.

552
00:34:56,195 --> 00:34:59,965
Speaker 5:  Yeah, I I think that's right. I think, you know, they have to, they have

553
00:34:59,965 --> 00:35:03,725
Speaker 5:  to show that there is this market, however small It is, that

554
00:35:03,725 --> 00:35:07,605
Speaker 5:  it's not so small that it doesn't matter. It's still matters and

555
00:35:07,605 --> 00:35:11,485
Speaker 5:  it's still something that Meta can leverage even over the

556
00:35:11,485 --> 00:35:14,885
Speaker 5:  users who maybe really are just coming there for reels and

557
00:35:15,195 --> 00:35:19,165
Speaker 5:  that that really separates them out. And I think one way

558
00:35:19,165 --> 00:35:22,965
Speaker 5:  that we've seen this throughout trial is Meta keeps bringing up these three

559
00:35:23,115 --> 00:35:26,605
Speaker 5:  screenshots of reels, TikTok and

560
00:35:26,635 --> 00:35:30,405
Speaker 5:  YouTube shorts and it's like on the same video and they look exactly

561
00:35:30,545 --> 00:35:34,405
Speaker 5:  the same. And you know, interestingly the FTCs

562
00:35:34,405 --> 00:35:38,125
Speaker 5:  expert actually couldn't distinguish which was witch, which was not a great

563
00:35:38,125 --> 00:35:38,365
Speaker 5:  look,

564
00:35:39,625 --> 00:35:42,805
Speaker 1:  That's a bad look for everyone. Like no one comes out of that looking like

565
00:35:42,805 --> 00:35:43,725
Speaker 1:  they're doing a good job.

566
00:35:44,235 --> 00:35:48,005
Speaker 5:  Yeah. But you know, I think it was tiktoks executive who

567
00:35:48,505 --> 00:35:52,485
Speaker 5:  had looked at the same kind of screenshot side by side and was

568
00:35:52,485 --> 00:35:56,325
Speaker 5:  like, yeah, you know, they do seem very similar. But when you

569
00:35:56,325 --> 00:35:59,925
Speaker 5:  click out of that experience, you get to what is core to the app.

570
00:36:00,465 --> 00:36:04,245
Speaker 5:  And for TikTok, this is the core like that. That's it. It's the

571
00:36:04,245 --> 00:36:08,085
Speaker 5:  short form video player. And for YouTube it's the long form

572
00:36:08,085 --> 00:36:11,725
Speaker 5:  videos. And for Instagram and Facebook, it's the

573
00:36:12,035 --> 00:36:15,885
Speaker 5:  feed and stories and you know, those are elements that the

574
00:36:16,065 --> 00:36:20,045
Speaker 5:  FTC says are really about sharing with your friends and family, even if they

575
00:36:20,045 --> 00:36:21,525
Speaker 5:  include some other kinds of content.

576
00:36:22,275 --> 00:36:25,495
Speaker 1:  Okay. So as everybody's hearing this on Tuesday,

577
00:36:26,075 --> 00:36:30,055
Speaker 1:  you will be back in court for what we think is the last day of

578
00:36:30,055 --> 00:36:31,575
Speaker 1:  this, right? For now.

579
00:36:31,575 --> 00:36:31,935
Speaker 5:  That's

580
00:36:31,935 --> 00:36:34,855
Speaker 1:  Right. Yes. The last day until the next first day. And then we do this all

581
00:36:34,855 --> 00:36:36,695
Speaker 1:  over again, presumably several more times.

582
00:36:36,965 --> 00:36:38,015
Speaker 5:  Exactly. Okay.

583
00:36:38,335 --> 00:36:41,575
Speaker 1:  I like it. You and I both live in DC and I've seen you more in the DC District

584
00:36:41,575 --> 00:36:44,455
Speaker 1:  courthouse than anywhere else since we've been working together. It's been

585
00:36:44,455 --> 00:36:44,615
Speaker 1:  great.

586
00:36:46,975 --> 00:36:50,655
Speaker 1:  I I will not hold you to this and neither will all of the many people listening

587
00:36:50,655 --> 00:36:53,775
Speaker 1:  to this, people to this. If you had to call it right now, where do you think

588
00:36:53,775 --> 00:36:54,175
Speaker 1:  it lands?

589
00:36:54,935 --> 00:36:55,055
Speaker 5:  O

590
00:36:56,785 --> 00:36:59,845
Speaker 5:  if I had to call it right now, I would say

591
00:37:01,285 --> 00:37:02,205
Speaker 5:  I think Meadow wins.

592
00:37:02,885 --> 00:37:03,645
Speaker 1:  I think that's what I meant

593
00:37:03,645 --> 00:37:07,605
Speaker 5:  Too. But yeah, I mean I just think it's, it's a tough case

594
00:37:07,605 --> 00:37:11,165
Speaker 5:  and it, you know, the burden is always on the plaintiff that said,

595
00:37:11,435 --> 00:37:15,125
Speaker 5:  will I be shocked if the FTC wins? No, but

596
00:37:15,365 --> 00:37:19,285
Speaker 5:  I, I think it's just, we had Google go first. I think that was a

597
00:37:19,285 --> 00:37:23,205
Speaker 5:  very strong case and it, that was tough because it was the first

598
00:37:23,205 --> 00:37:26,245
Speaker 5:  one out of the gate. It's hard to be the first judge to make a ruling like

599
00:37:26,245 --> 00:37:30,205
Speaker 5:  that. Totally. But this is a very different kind of case, you

600
00:37:30,205 --> 00:37:33,445
Speaker 5:  know, that was really about contracts. A lot of it actually lined up really

601
00:37:33,465 --> 00:37:36,685
Speaker 5:  neatly with the Microsoft case. So

602
00:37:37,365 --> 00:37:41,245
Speaker 5:  I think it was very different. I think this is a different kind of case.

603
00:37:41,505 --> 00:37:45,285
Speaker 5:  And the judge, in that same order on the motion for summary judgment

604
00:37:45,545 --> 00:37:49,485
Speaker 5:  in November, he said that the government's claims quote

605
00:37:49,545 --> 00:37:52,925
Speaker 5:  at times straightened this country's creaking, antitrust precedents to their

606
00:37:52,925 --> 00:37:56,605
Speaker 5:  limits. Oh boy. So I, it's just, you know, it's a hard case to bring

607
00:37:56,605 --> 00:38:00,525
Speaker 5:  given the state of antitrust law and where

608
00:38:00,525 --> 00:38:03,765
Speaker 5:  we're at today. But you know, it, it's not out of the question. And I think

609
00:38:04,335 --> 00:38:08,045
Speaker 5:  Judge Boberg seemed like he was asking really astute

610
00:38:08,325 --> 00:38:11,565
Speaker 5:  questions by the end. He didn't ask a ton of questions, but the ones he did

611
00:38:11,585 --> 00:38:15,485
Speaker 5:  ask seemed like, okay, he's really following this. Mm. So I will be

612
00:38:15,485 --> 00:38:17,805
Speaker 5:  very interested to see what he ends up saying.

613
00:38:18,545 --> 00:38:21,165
Speaker 1:  You and me both. Yeah. There's, there's a lot of these all happening at once.

614
00:38:21,305 --> 00:38:24,605
Speaker 1:  And this one, this one feels like maybe the hardest to call.

615
00:38:25,195 --> 00:38:29,165
Speaker 1:  Yeah. But it's gonna be fascinating. Alright, for now I will, I

616
00:38:29,165 --> 00:38:31,685
Speaker 1:  will leave you alone. But thank you for doing this. It's great to see you

617
00:38:31,715 --> 00:38:34,205
Speaker 1:  come. Yeah. Leave the courtroom occasionally. We're gonna, we're gonna do

618
00:38:34,205 --> 00:38:35,605
Speaker 1:  this more. It's gonna be great. Great. Would

619
00:38:35,605 --> 00:38:35,965
Speaker 7:  Love that.

620
00:38:36,425 --> 00:38:38,525
Speaker 1:  All right, we gotta take a break and then we're gonna come back and we're

621
00:38:38,525 --> 00:38:40,245
Speaker 1:  gonna talk smart glasses. We'll be right back.

622
00:40:27,295 --> 00:40:29,195
Speaker 10:  post-conference high.

623
00:41:01,955 --> 00:41:05,915
Speaker 10:  comparatively about AI instead of just going ai. Though there

624
00:41:05,915 --> 00:41:09,595
Speaker 10:  was a lot of that too with many charts, albeit with Y

625
00:41:09,835 --> 00:41:13,355
Speaker 10:  axes, unlike other conferences that we shall not name. And

626
00:41:14,175 --> 00:41:17,955
Speaker 10:  you know, I think this is one of the first years where we felt where I felt

627
00:41:17,955 --> 00:41:21,795
Speaker 10:  at least that hardware was present in some way in, in the

628
00:41:21,795 --> 00:41:24,435
Speaker 10:  keynote. Not in a large way, but in a way.

629
00:41:25,275 --> 00:41:27,755
Speaker 1:  Interesting. Well, and one of the reasons I wanna talk to you is it felt

630
00:41:27,755 --> 00:41:31,315
Speaker 1:  to me like Google makes this like long series of product

631
00:41:31,595 --> 00:41:35,445
Speaker 1:  announcements about Mariner and Astra and AI mode and all this

632
00:41:35,445 --> 00:41:38,805
Speaker 1:  stuff where they're like, okay, what we want to do is make AI

633
00:41:39,195 --> 00:41:42,605
Speaker 1:  that is like interactive with you all the time and it can see everything

634
00:41:42,605 --> 00:41:46,325
Speaker 1:  and it's always listening and proactive and all this stuff. And all I kept

635
00:41:46,325 --> 00:41:50,005
Speaker 1:  hearing all the time was it's s smart glasses that it's like this is, this

636
00:41:50,005 --> 00:41:53,525
Speaker 1:  is a world you build if you believe that smart glasses

637
00:41:54,065 --> 00:41:57,645
Speaker 1:  are going to take over the world. Was that your sense too that like

638
00:41:57,865 --> 00:42:01,505
Speaker 1:  all of this is like kind of phony, but it's like built

639
00:42:01,645 --> 00:42:02,345
Speaker 1:  for smart glasses.

640
00:42:03,505 --> 00:42:07,465
Speaker 10:  I 100% feel that way because what did they

641
00:42:07,465 --> 00:42:10,945
Speaker 10:  end on? Smart Glasses. Right? Right. It was a very interesting

642
00:42:11,055 --> 00:42:14,945
Speaker 10:  concept because they went like, search, search, search. Here's a bunch

643
00:42:14,945 --> 00:42:18,745
Speaker 10:  of projects whose names you absolutely won't remember the difference between

644
00:42:18,925 --> 00:42:22,545
Speaker 10:  by the end of this conference and smart glasses at the end.

645
00:42:23,685 --> 00:42:27,505
Speaker 10:  So I, I feel like they were just kind of handholding everyone to go to this

646
00:42:27,505 --> 00:42:31,225
Speaker 10:  thesis that Android XR is how they're going to

647
00:42:31,905 --> 00:42:35,705
Speaker 10:  embody all of this ai. Because AI is not really

648
00:42:35,975 --> 00:42:39,905
Speaker 10:  something that is tangible at the moment. Like it's software, right?

649
00:42:40,175 --> 00:42:43,545
Speaker 10:  It's on your devices but it's not necessarily

650
00:42:44,065 --> 00:42:47,985
Speaker 10:  integral to your devices. And you know, the thesis that Meta

651
00:42:48,085 --> 00:42:52,065
Speaker 10:  and now Google have kind of come up with in the last two years

652
00:42:52,065 --> 00:42:55,985
Speaker 10:  or so is that Smart glasses are the hardware vehicle for AI

653
00:42:56,405 --> 00:42:59,745
Speaker 10:  for Normies. We got the headsets for you, you early

654
00:42:59,865 --> 00:43:03,425
Speaker 10:  adopters, tech enthusiasts, you know, gadget nerds,

655
00:43:03,925 --> 00:43:07,805
Speaker 10:  but for the Normies weird pairing up with a

656
00:43:07,805 --> 00:43:11,485
Speaker 10:  Gentle Monster and Warby Parker and you'll have smart glasses and you'll

657
00:43:11,485 --> 00:43:14,645
Speaker 10:  like it. Right. That that's kind of the, that's kind of the vibe.

658
00:43:14,955 --> 00:43:18,885
Speaker 1:  Yeah. So you got a couple of new demos this time. What did you actually get

659
00:43:18,885 --> 00:43:20,085
Speaker 1:  to see and try at ia?

660
00:43:20,585 --> 00:43:24,445
Speaker 10:  No new demos for me. I think this is the, the public unveiling

661
00:43:25,025 --> 00:43:28,965
Speaker 10:  of what demos they're comfortable letting out into the

662
00:43:28,965 --> 00:43:32,725
Speaker 10:  world with minimal guardrails. Okay. It was actually

663
00:43:32,725 --> 00:43:36,605
Speaker 10:  really funny because I came in my sneakers and running shorts

664
00:43:36,605 --> 00:43:40,245
Speaker 10:  under a skirt as soon as the keynote was done, I used all of my training

665
00:43:40,265 --> 00:43:44,205
Speaker 10:  to run to the demo. I got there and they

666
00:43:44,205 --> 00:43:47,245
Speaker 10:  were like, you've got five minutes with everything. And I was like, oh my

667
00:43:47,245 --> 00:43:50,725
Speaker 10:  God, that's not a lot of time. Right? And I think that's on purpose, not

668
00:43:50,725 --> 00:43:54,485
Speaker 10:  only because this was the thing that everyone was gonna wanna try, but because

669
00:43:55,345 --> 00:43:58,045
Speaker 10:  you know, as they said on the keynote on stage when they did the live AI

670
00:43:58,045 --> 00:44:01,565
Speaker 10:  translations, it stuck out to me 'cause they're like, this is a risky demo

671
00:44:01,595 --> 00:44:05,565
Speaker 10:  because these are risky demos. Live AI demos are always going to be risky,

672
00:44:05,985 --> 00:44:09,605
Speaker 10:  but inherently so in a device that Google is known for failing

673
00:44:09,945 --> 00:44:13,805
Speaker 10:  at. So the demos I got in Wuhan,

674
00:44:13,925 --> 00:44:17,205
Speaker 10:  I was like, oh, here we go again. This is like what I saw in December. This

675
00:44:17,205 --> 00:44:20,325
Speaker 10:  is a Android flavored Apple Vision pro

676
00:44:21,115 --> 00:44:24,605
Speaker 10:  that is much, much, much, much, much, much like 10 times more comfortable

677
00:44:24,605 --> 00:44:24,925
Speaker 10:  to wear.

678
00:44:25,405 --> 00:44:26,045
Speaker 1:  Interesting. Okay.

679
00:44:26,045 --> 00:44:29,805
Speaker 10:  But the demo I got in there was not particularly special as

680
00:44:30,315 --> 00:44:34,135
Speaker 10:  like, I have a problem. 'cause I saw all of it. I saw

681
00:44:34,155 --> 00:44:38,015
Speaker 10:  all of it. I think what's important is what they chose to let people

682
00:44:38,115 --> 00:44:42,095
Speaker 10:  try. And so what they're choosing to let people try is talking to

683
00:44:42,235 --> 00:44:46,175
Speaker 10:  Gemini in some very guided ways. Okay. And

684
00:44:46,175 --> 00:44:49,695
Speaker 10:  then for the headset it was like,

685
00:44:49,995 --> 00:44:53,855
Speaker 10:  here's a ability for you to ask Gemini to take you

686
00:44:53,855 --> 00:44:57,135
Speaker 10:  to a place without opening the Google Maps app. But it's gonna take you

687
00:44:57,135 --> 00:45:00,655
Speaker 10:  there contextually and then you can look around. Okay. And then

688
00:45:01,195 --> 00:45:04,415
Speaker 10:  the other thing is like, also we know people wanna use headsets for immersive

689
00:45:04,415 --> 00:45:08,135
Speaker 10:  content. What if we took a video that was filmed on a drone

690
00:45:08,275 --> 00:45:12,015
Speaker 10:  and made it 3D eh, eh, eh, spatializing, eh,

691
00:45:12,155 --> 00:45:15,695
Speaker 10:  eh, that that, that was the, the Wuhan demo I got. Which

692
00:45:16,415 --> 00:45:19,615
Speaker 10:  I don't think is gonna convince people of Android xr. I really don't think

693
00:45:19,635 --> 00:45:23,255
Speaker 10:  that's what's gonna, I actually think the lighter form factor is what is

694
00:45:23,255 --> 00:45:26,855
Speaker 10:  gonna make people go like, oh, okay, vision Pro. Very cool,

695
00:45:27,315 --> 00:45:31,215
Speaker 10:  but wearable. So I think that's kind of the takeaway I had from

696
00:45:31,445 --> 00:45:35,015
Speaker 10:  Moan. Again, solidified And the glasses. The glasses

697
00:45:35,235 --> 00:45:39,015
Speaker 10:  is what everyone was just like, move aside project moan.

698
00:45:39,015 --> 00:45:42,895
Speaker 10:  I'll get to you later. The glasses. Let me have the glasses. And

699
00:45:42,965 --> 00:45:46,455
Speaker 10:  that was interesting because the three

700
00:45:47,505 --> 00:45:51,415
Speaker 10:  demos that they had, everyone try, I think were very intentional and

701
00:45:51,415 --> 00:45:55,255
Speaker 10:  purposeful. So one of the demos was, Hey, there's two paintings in

702
00:45:55,255 --> 00:45:59,095
Speaker 10:  here. Ask Gemini facts about those paintings. And you

703
00:45:59,095 --> 00:46:02,895
Speaker 10:  can ask it questions contextually, like compare these two paintings.

704
00:46:03,825 --> 00:46:07,655
Speaker 10:  There was one that was very brightly colored and pointless. And another

705
00:46:07,655 --> 00:46:11,575
Speaker 10:  that was just like, you know, like in TV shows where they wanna

706
00:46:11,575 --> 00:46:14,655
Speaker 10:  show you that the person is sad now. So everything is in like cool lighting.

707
00:46:14,845 --> 00:46:18,735
Speaker 10:  Sure. That was that, that was that picture. And you know, you, you go

708
00:46:18,735 --> 00:46:22,175
Speaker 10:  like, hey, you tap the side, the, the side arm and it brings up

709
00:46:22,375 --> 00:46:26,095
Speaker 10:  Gemini Live and I think it was Gemini Live at least. And you know, you can

710
00:46:26,095 --> 00:46:29,695
Speaker 10:  see the little display in one of the lenses and it,

711
00:46:29,885 --> 00:46:33,575
Speaker 10:  it's like, Hey, yeah, I, you have the little animation that shows that it's

712
00:46:33,575 --> 00:46:37,055
Speaker 10:  listening and then it gives you captain obvious

713
00:46:37,055 --> 00:46:40,735
Speaker 10:  observations. Like Right. You know, if you're a seeing person, if you're

714
00:46:40,735 --> 00:46:44,375
Speaker 10:  not a seeing person, then that might be very helpful what it's describing

715
00:46:44,375 --> 00:46:48,175
Speaker 10:  to you. But for me they were just like, oh, the, the picture on the right

716
00:46:48,175 --> 00:46:51,735
Speaker 10:  is brightly colored. It has pointalism in it and the picture on the left

717
00:46:52,035 --> 00:46:54,935
Speaker 10:  has moodier subdued hues. And I was like, yes.

718
00:46:55,445 --> 00:46:59,295
Speaker 1:  It's on the level of like, you point the camera at a bag of nuts

719
00:46:59,295 --> 00:47:02,815
Speaker 1:  that says nuts in large letters and it tells you it's a bag of nuts. It's

720
00:47:02,815 --> 00:47:06,695
Speaker 1:  like, yes, it's not unimpressive technology, but it's not

721
00:47:07,365 --> 00:47:11,295
Speaker 1:  hard work. Right. Like this, this is stuff we know we have solved. And I

722
00:47:11,295 --> 00:47:13,375
Speaker 1:  think it, it sounds like that's a lot of what you're saying here. It's like

723
00:47:13,725 --> 00:47:16,695
Speaker 1:  what you've described is a bunch of things that Google has known it's very

724
00:47:16,725 --> 00:47:19,575
Speaker 1:  good at for a while and is actually

725
00:47:20,595 --> 00:47:24,575
Speaker 1:  now just trying to do inside of glasses, which does present new kinds of

726
00:47:24,575 --> 00:47:28,175
Speaker 1:  problems. Right. The cameras and the processing, all that stuff that's new

727
00:47:28,175 --> 00:47:31,965
Speaker 1:  and different stuff. So the fact that even the easy stuff works is not nothing,

728
00:47:32,025 --> 00:47:35,565
Speaker 1:  but it's also not, you have not invented a new thing

729
00:47:36,635 --> 00:47:39,135
Speaker 1:  if you just give me a new way to do my camera.

730
00:47:39,915 --> 00:47:43,855
Speaker 10:  Yes. So these are very, very tightly controlled and like short,

731
00:47:44,095 --> 00:47:47,975
Speaker 10:  purposefully, short demos that we were getting at this event.

732
00:47:48,115 --> 00:47:52,015
Speaker 10:  The other one in December were much longer you were like able to ask

733
00:47:52,115 --> 00:47:55,975
Speaker 10:  Gemini to do more impressive things. Like one was you could look at a

734
00:47:55,975 --> 00:47:59,935
Speaker 10:  bookshelf and the glasses would remember the books that you saw. So

735
00:47:59,935 --> 00:48:03,855
Speaker 10:  if you left and you went, oh hey, what was that book that I

736
00:48:03,855 --> 00:48:07,535
Speaker 10:  saw in yellow? Like the yellow cover, what was the title of that? It would

737
00:48:07,535 --> 00:48:11,295
Speaker 10:  be able to remember that for you. And that is something if you're forgetful,

738
00:48:11,295 --> 00:48:14,855
Speaker 10:  you'd be like, oh, maybe I would use that if I could remember to use that.

739
00:48:14,955 --> 00:48:18,055
Speaker 10:  But you know, that that is something that feels a little more magical and

740
00:48:18,075 --> 00:48:21,615
Speaker 10:  not something that we were able to try. Right. In this like setting where

741
00:48:21,615 --> 00:48:25,295
Speaker 10:  people are finally allowed to take photos of the prototype where they're

742
00:48:25,295 --> 00:48:28,415
Speaker 10:  finally allowed to try the prototypes and they're finally allowed to take

743
00:48:28,415 --> 00:48:32,255
Speaker 10:  video of people trying these prototypes. Another thing like

744
00:48:32,375 --> 00:48:35,815
Speaker 10:  a slightly modified version of that demo that I got where they had all these

745
00:48:35,815 --> 00:48:39,575
Speaker 10:  travel books on a shelf and they were like, why don't you try

746
00:48:39,595 --> 00:48:43,055
Speaker 10:  asking Gemini which book you should read based on like

747
00:48:43,485 --> 00:48:47,015
Speaker 10:  your preferences. So, you know, I come up with a persona and I go, Hey,

748
00:48:47,215 --> 00:48:51,135
Speaker 10:  I am planning a trip to Japan. I'm not an outdoorsy person. Which of these

749
00:48:51,135 --> 00:48:54,935
Speaker 10:  travel guides is gonna be most helpful for me? And I picked one. Okay. And

750
00:48:54,935 --> 00:48:58,685
Speaker 10:  I'm like, cool. I guess that's helpful. If you're

751
00:48:58,745 --> 00:49:02,245
Speaker 10:  in a bookstore and you're like, I don't know which one of these books to

752
00:49:02,245 --> 00:49:04,965
Speaker 10:  choose, why don't you choose that one for me? So

753
00:49:05,695 --> 00:49:07,445
Speaker 10:  ostensibly you could do that, but

754
00:49:07,795 --> 00:49:10,965
Speaker 1:  Well, but again, it's, it's the, the, the little bits of rails there I think

755
00:49:10,965 --> 00:49:13,965
Speaker 1:  are what's so interesting. 'cause it's like, It is one thing to do that from

756
00:49:14,005 --> 00:49:17,765
Speaker 1:  a series of travel guides. It is another thing entirely to

757
00:49:17,795 --> 00:49:21,565
Speaker 1:  just be in a room or a bookstore or whatever and be like,

758
00:49:21,625 --> 00:49:25,245
Speaker 1:  I'm going to Japan. What in this Barnes and Noble should I read? Which is

759
00:49:25,245 --> 00:49:28,885
Speaker 1:  like that that's the thing. Yeah. Right. If you're already standing in front

760
00:49:28,885 --> 00:49:32,725
Speaker 1:  of the shelf of travel guides, the the question of do I look at the

761
00:49:32,725 --> 00:49:36,485
Speaker 1:  Japan travel guide for my Japan trip is not interesting user

762
00:49:36,485 --> 00:49:40,365
Speaker 1:  interface stuff. But I think like, this is

763
00:49:40,365 --> 00:49:43,125
Speaker 1:  not to like diminish what's happening here. All of this is like a stepping

764
00:49:43,135 --> 00:49:47,125
Speaker 1:  stone to that. And I think this is where I go back to the, the sort

765
00:49:47,125 --> 00:49:50,765
Speaker 1:  of point counterpoint between all of the stuff Google was announcing at

766
00:49:50,905 --> 00:49:54,165
Speaker 1:  IO and these glasses. 'cause like all of the Project ASRA stuff, and you

767
00:49:54,165 --> 00:49:56,845
Speaker 1:  and I have talked about this before, all of that is smart glasses, right?

768
00:49:56,865 --> 00:50:00,245
Speaker 1:  The idea that this thing can be on all the time and seeing everything that

769
00:50:00,245 --> 00:50:04,165
Speaker 1:  I'm seeing and listening is like, and and sort of proactively piping

770
00:50:04,165 --> 00:50:06,885
Speaker 1:  up when it has something to tell me or remembering what all the books are

771
00:50:06,945 --> 00:50:10,285
Speaker 1:  and then telling me later, like that's all the stuff they're working on with

772
00:50:10,455 --> 00:50:14,205
Speaker 1:  Astra. This like universal AI assistant. All of that is for smart glasses.

773
00:50:14,425 --> 00:50:16,925
Speaker 1:  And they've said as much like, I talked to the team that's building this

774
00:50:16,925 --> 00:50:19,565
Speaker 1:  and they were like, we're doing it for phones and smart glasses. But it's

775
00:50:19,565 --> 00:50:23,325
Speaker 1:  pretty clear to us that the best version of this thing works

776
00:50:23,465 --> 00:50:26,125
Speaker 1:  on smart glasses. Because you can see all the things you see and hear all

777
00:50:26,125 --> 00:50:29,885
Speaker 1:  the things you hear. It makes perfect sense. But It is so painfully clear

778
00:50:29,885 --> 00:50:33,325
Speaker 1:  that putting those two things together, the glasses and

779
00:50:33,785 --> 00:50:36,965
Speaker 1:  the AI and the underlying like sensing technology,

780
00:50:38,025 --> 00:50:40,645
Speaker 1:  all the pieces of that are happening. But the, the shoving them together

781
00:50:40,765 --> 00:50:44,325
Speaker 1:  I think is gonna be way harder than most people are reckoning with. It's

782
00:50:44,495 --> 00:50:48,405
Speaker 10:  Gonna be much harder, the parts exist, but the problem is that the

783
00:50:48,405 --> 00:50:51,805
Speaker 10:  parts have existed for a really long time. It's just putting together that's

784
00:50:51,875 --> 00:50:55,165
Speaker 10:  very difficult. And then on top of that, you know,

785
00:50:55,745 --> 00:50:59,645
Speaker 10:  the problem with all of this is that I've seen it. Have you seen it?

786
00:50:59,865 --> 00:51:03,285
Speaker 10:  Do you get it? Do you get why these people are so gung-ho about it? Because

787
00:51:03,365 --> 00:51:06,365
Speaker 10:  I can describe it to you, I can describe it to you at length, I can describe

788
00:51:06,365 --> 00:51:10,005
Speaker 10:  it to you in detail. You just don't fully

789
00:51:10,225 --> 00:51:12,925
Speaker 10:  get it. You know? And Yep. You know the readers are just like, well isn't

790
00:51:12,925 --> 00:51:16,885
Speaker 10:  your job to describe that to us? And I'm like, yes I am. But what I'm

791
00:51:16,885 --> 00:51:20,645
Speaker 10:  trying to convey to you is that I can be

792
00:51:20,665 --> 00:51:24,125
Speaker 10:  the most descriptive person in the world. I can hype it up

793
00:51:24,585 --> 00:51:28,525
Speaker 10:  so much. You're still not going to get it until you put

794
00:51:28,525 --> 00:51:31,885
Speaker 10:  a pair of glasses on. And then when you put the pair of glasses on, you

795
00:51:31,885 --> 00:51:35,605
Speaker 10:  still may not be convinced because the tech is so nascent at this point

796
00:51:35,625 --> 00:51:39,565
Speaker 10:  in time, our brains and our cultural cues have not evolved to the point

797
00:51:39,565 --> 00:51:43,485
Speaker 10:  where we naturally think, oh yeah, this is what I'm gonna ask it

798
00:51:43,485 --> 00:51:46,925
Speaker 10:  to do. Like within the whole keynote and the whole demo, the thing that

799
00:51:46,965 --> 00:51:50,325
Speaker 10:  I was like, aha, this is what's gonna get people to think

800
00:51:51,125 --> 00:51:55,085
Speaker 10:  about the possibilities, was that demo of the guy trying to fix

801
00:51:55,085 --> 00:51:58,765
Speaker 10:  his bike and just going like, pull this up. Hey, I'm looking at this.

802
00:51:59,035 --> 00:52:02,045
Speaker 10:  What do I need to do right there? Because like if you're trying to solve

803
00:52:02,045 --> 00:52:05,045
Speaker 10:  that, now what are you doing? You're pulling up YouTube videos, you're trying

804
00:52:05,045 --> 00:52:08,245
Speaker 10:  to sort between the five different YouTube videos doing the same thing all

805
00:52:08,245 --> 00:52:11,525
Speaker 10:  with the same tedious two minute intro where you're just like, just get

806
00:52:11,525 --> 00:52:13,925
Speaker 10:  me to the part that I need to know. And then you're having to physically

807
00:52:14,165 --> 00:52:16,965
Speaker 10:  reference other stuff, you know? And it's saying that it's gonna do that

808
00:52:17,485 --> 00:52:21,045
Speaker 10:  seamlessly for you based on what you're seeing. So that part,

809
00:52:21,725 --> 00:52:24,765
Speaker 10:  I think everyone can look at that and go, Hey, I think there's a time where

810
00:52:24,845 --> 00:52:28,245
Speaker 10:  I would've loved that to be easier with less friction.

811
00:52:28,745 --> 00:52:32,685
Speaker 10:  So that yes, people will get that. And that's why they did the low

812
00:52:32,685 --> 00:52:36,405
Speaker 10:  vision and blind use cases scenario there too. Because that is

813
00:52:36,645 --> 00:52:40,405
Speaker 10:  another clear cut thing where you go, oh, now

814
00:52:40,565 --> 00:52:44,365
Speaker 10:  I get why we might want smart glasses for this very specific subset of

815
00:52:44,365 --> 00:52:47,965
Speaker 10:  people who it could very much help. And those are the kinds of things that

816
00:52:47,965 --> 00:52:51,765
Speaker 10:  you need to get normal people on board. Whereas like

817
00:52:52,185 --> 00:52:56,005
Speaker 10:  the, so the final third demo that I got that they were ready

818
00:52:56,005 --> 00:52:59,965
Speaker 10:  to share publicly and they think is going to convince people that this

819
00:52:59,965 --> 00:53:03,685
Speaker 10:  is, this is it was the fact that you can take a picture with the glasses,

820
00:53:03,815 --> 00:53:07,405
Speaker 10:  which no bigs snap glasses have been doing that. Metal ray band glasses

821
00:53:07,435 --> 00:53:11,085
Speaker 10:  have been doing that. The difference now is there's a display. So you snap

822
00:53:11,085 --> 00:53:15,045
Speaker 10:  the photo and you can see what it looks like. You get a preview. And the

823
00:53:15,045 --> 00:53:18,365
Speaker 10:  problem with the Meta ray bands for a lot of content creators is that there's

824
00:53:18,365 --> 00:53:22,285
Speaker 10:  no framing, right? So like when I tested the Ray bands, I noticed that

825
00:53:22,285 --> 00:53:25,405
Speaker 10:  I tilt my head a lot because all of my photos had an unintentional Dutch

826
00:53:25,405 --> 00:53:26,445
Speaker 10:  angle. I was like, Ooh,

827
00:53:26,865 --> 00:53:30,245
Speaker 1:  I'm so glad that's not just me. That's exactly been my experience too, is

828
00:53:30,245 --> 00:53:33,605
Speaker 1:  like every time I think my head is straight, it turns out my head is actually

829
00:53:33,605 --> 00:53:35,005
Speaker 1:  at like a 20 degree angle.

830
00:53:35,345 --> 00:53:38,565
Speaker 10:  You know, it's at, it's at an angle. I, I was like, oh, I must be such a

831
00:53:38,565 --> 00:53:41,765
Speaker 10:  thoughtful person 'cause I'm at an angle and I'm thinking so hard about

832
00:53:41,765 --> 00:53:45,645
Speaker 10:  what I'm looking at or you know, I realized that if you're

833
00:53:45,645 --> 00:53:49,165
Speaker 10:  gonna wear glass smart glasses where the camera's usually in the hinges

834
00:53:49,265 --> 00:53:53,165
Speaker 10:  so far, you really shouldn't have bangs or you should tie your

835
00:53:53,165 --> 00:53:56,165
Speaker 10:  hair back. Sure, yeah. Because there were so many pictures where just pieces

836
00:53:56,185 --> 00:54:00,165
Speaker 10:  of my hair were ruining the framing in the shot. So if you have a display,

837
00:54:01,105 --> 00:54:04,965
Speaker 10:  you take the shot, you see what the picture's gonna look like and you can

838
00:54:04,965 --> 00:54:08,485
Speaker 10:  go, oh, I've tilted my head. Let me just straighten my head and get this

839
00:54:08,485 --> 00:54:12,205
Speaker 10:  photo again. So, and it happens in a blink of a second. It's like really

840
00:54:12,235 --> 00:54:15,405
Speaker 10:  cool. It's a small thing, but it's also a thing that

841
00:54:15,755 --> 00:54:19,085
Speaker 10:  they're very subtly like, Hey, you know who can't do that just yet?

842
00:54:19,595 --> 00:54:23,005
Speaker 10:  Meta Meta, yeah. We got the display. Yeah, I,

843
00:54:23,385 --> 00:54:27,365
Speaker 1:  I'm fascinated by how all of these companies

844
00:54:27,365 --> 00:54:30,725
Speaker 1:  are gonna try to use those tiny screens because like the screens aren't very

845
00:54:30,725 --> 00:54:33,485
Speaker 1:  good, at least in the, the demos that I've gotten in the past. Maybe these

846
00:54:33,485 --> 00:54:37,045
Speaker 1:  are better, but in general the screens mostly suck, but they're there

847
00:54:37,305 --> 00:54:39,725
Speaker 1:  and so you, you have to figure out what you can do. But even the thing you

848
00:54:39,725 --> 00:54:43,285
Speaker 1:  described where like it shows the animation that Gemini is listening to you.

849
00:54:43,475 --> 00:54:46,925
Speaker 1:  Like I instinctively hate that because I'm like, actually your job is to

850
00:54:46,985 --> 00:54:50,805
Speaker 1:  not put interface in my way while something

851
00:54:50,805 --> 00:54:54,285
Speaker 1:  is not happening. Right. That that's the equivalent of like putting a loading

852
00:54:54,285 --> 00:54:57,805
Speaker 1:  screen in front of my face 24 hours a day. Like that's bad. I don't want

853
00:54:57,805 --> 00:55:01,365
Speaker 1:  that. But the like one second to frame your shot

854
00:55:01,785 --> 00:55:05,565
Speaker 1:  as you're taking a photo makes a ton of sense to me. And it's like, this

855
00:55:05,565 --> 00:55:08,005
Speaker 1:  is the thing I think is so interesting about smart glasses in general is

856
00:55:08,095 --> 00:55:11,885
Speaker 1:  we're just gonna have to make a million of those decisions about how

857
00:55:11,885 --> 00:55:15,565
Speaker 1:  everything is supposed to work all the time because it should be as little

858
00:55:15,805 --> 00:55:19,615
Speaker 1:  screen as possible, but you also need the screen for lots of things.

859
00:55:19,755 --> 00:55:23,055
Speaker 1:  And I think like that getting that balance right I think is like kind of

860
00:55:23,055 --> 00:55:25,775
Speaker 1:  the whole product ballgame with a lot of this stuff.

861
00:55:25,965 --> 00:55:29,015
Speaker 10:  Yeah. We're at the spaghetti phase 'cause I don't think we know what we

862
00:55:29,015 --> 00:55:31,695
Speaker 10:  want fully. The other thing that they're trying to do is heads up display.

863
00:55:31,755 --> 00:55:35,295
Speaker 10:  So they're talking about Google maps and this isn't exactly

864
00:55:36,235 --> 00:55:40,175
Speaker 10:  the scenario you're talking about, David, where you know your, your head's

865
00:55:40,175 --> 00:55:43,055
Speaker 10:  up. They're only gonna give you the little kind of, if you have a heads

866
00:55:43,055 --> 00:55:46,215
Speaker 10:  up display in a car, you probably know what I'm talking about. Where it's

867
00:55:46,215 --> 00:55:49,855
Speaker 10:  just like 500 feet in a, in an arrow in a tiny little display in the

868
00:55:50,425 --> 00:55:53,935
Speaker 10:  peripheral of your vision. And then when you look down it morphs

869
00:55:54,245 --> 00:55:58,215
Speaker 10:  into a little map. Oh see? So you, you can kind of get

870
00:55:58,735 --> 00:56:01,615
Speaker 10:  a little, a little sense. Some people I talked to were like, I don't, I

871
00:56:01,615 --> 00:56:04,655
Speaker 10:  don't know if I like that 'cause I wanna see the map, I wanna see the blue

872
00:56:05,265 --> 00:56:09,175
Speaker 10:  arrow floating in the sky. And I was like, I don't wanna see that. I want

873
00:56:09,175 --> 00:56:13,135
Speaker 10:  something less obtrusive. I wanna, if I'm, if I'm trying to get

874
00:56:13,135 --> 00:56:17,015
Speaker 10:  out of a New York subway, I never know what is up down north south once

875
00:56:17,055 --> 00:56:20,175
Speaker 10:  I get out of the subway. 'cause I've emerged from the, the infrastructural

876
00:56:20,235 --> 00:56:24,205
Speaker 10:  depths of hell and I, I wanna know what direction I go in if

877
00:56:24,205 --> 00:56:26,765
Speaker 10:  I could just look down and go like, oh I'm supposed to go that way. Cool.

878
00:56:27,225 --> 00:56:30,645
Speaker 10:  And then have like a guide of where I'm supposed to go So I can stay

879
00:56:30,655 --> 00:56:34,565
Speaker 10:  interacted with my surroundings. That, that feels like a good use

880
00:56:34,565 --> 00:56:38,445
Speaker 10:  to me. But I, I met so many people who are like, I want the big blue

881
00:56:38,445 --> 00:56:41,845
Speaker 10:  arrow floating in space. I wanna see that. And you know,

882
00:56:42,265 --> 00:56:46,245
Speaker 10:  the prototypes that we saw, they were only one type

883
00:56:46,245 --> 00:56:50,045
Speaker 10:  of prototype that I know Google has. Because when I went from my

884
00:56:50,485 --> 00:56:54,045
Speaker 10:  December hands-on where we were not allowed to have photos, we were not

885
00:56:54,045 --> 00:56:57,885
Speaker 10:  allowed to have cameras. We could just only see and I, I almost feel like

886
00:56:57,885 --> 00:57:01,605
Speaker 10:  I was gaslit into thinking did this exist? Did I see that? Because

887
00:57:01,765 --> 00:57:05,565
Speaker 10:  I wasn't able to take anything but my own written experience and recording

888
00:57:05,565 --> 00:57:09,365
Speaker 10:  and my notes out of that. I saw binocular versions.

889
00:57:09,445 --> 00:57:13,325
Speaker 10:  I saw versions that were sunglasses, you know, so it's clear

890
00:57:13,325 --> 00:57:17,245
Speaker 10:  that they're trying to refine what this display is gonna look like because

891
00:57:17,265 --> 00:57:20,805
Speaker 10:  the ones that we all got shown in public was the

892
00:57:20,835 --> 00:57:24,125
Speaker 10:  monocular single lens display in one of the lenses.

893
00:57:24,595 --> 00:57:28,445
Speaker 10:  Okay. But I saw versions where there were two one in each and

894
00:57:28,515 --> 00:57:32,285
Speaker 10:  that allows a much bigger screen to appear that gives you

895
00:57:32,355 --> 00:57:36,245
Speaker 10:  more options. Like I saw a wild demo back in December where

896
00:57:36,245 --> 00:57:39,285
Speaker 10:  you could just like really kind of watch a movie

897
00:57:40,145 --> 00:57:43,845
Speaker 10:  inside of your smart glasses. Was it good? No, it has the same all the,

898
00:57:43,985 --> 00:57:47,525
Speaker 10:  it was like the same problems that maybe watching a movie on a

899
00:57:47,525 --> 00:57:50,965
Speaker 10:  projector in your living room versus on an actual TV has a lot of that same

900
00:57:50,965 --> 00:57:54,885
Speaker 10:  issue, you know? And I asked them, Hey how did you solve the ambient

901
00:57:54,885 --> 00:57:58,445
Speaker 10:  light question? That's my gotcha question. And more people have better answers

902
00:57:58,465 --> 00:58:02,445
Speaker 10:  to that in the last few years than they have ever with regard to smart glasses.

903
00:58:02,745 --> 00:58:06,645
Speaker 10:  And they say, oh you know it has ambient light sensors, it'll be

904
00:58:06,645 --> 00:58:09,205
Speaker 10:  able to adjust the brightness based on where you're at. And I was like,

905
00:58:09,205 --> 00:58:11,925
Speaker 10:  cool, but we are in a tiny little room right now. We're not doing these

906
00:58:11,975 --> 00:58:15,925
Speaker 10:  demos outside in direct sunlight. Right. This is what I mean by guardrails

907
00:58:16,175 --> 00:58:20,005
Speaker 10:  controlled, trying to bottle the magic. I heard

908
00:58:20,245 --> 00:58:23,605
Speaker 10:  references to Iron Man and Jarvis because

909
00:58:23,855 --> 00:58:27,525
Speaker 10:  ironically I think Tony Stark looms large over our

910
00:58:27,525 --> 00:58:30,725
Speaker 10:  imagination of what smart glasses ought to. Very much so. Yeah. So like

911
00:58:31,385 --> 00:58:34,405
Speaker 10:  you should be having Jarvis in your glasses, it should be able to

912
00:58:35,075 --> 00:58:38,405
Speaker 10:  project all this information. And then the reality is, is that

913
00:58:39,265 --> 00:58:42,245
Speaker 10:  that's not where we're at. We're probably more in the, I'm gonna say the

914
00:58:42,245 --> 00:58:46,205
Speaker 10:  Kingsman exy scenario where you have a pair of smart glasses that look very

915
00:58:46,205 --> 00:58:50,045
Speaker 10:  discreet and you can see green holograms is like one

916
00:58:50,045 --> 00:58:53,005
Speaker 10:  that these are actual glasses that people at CES brought where you can see

917
00:58:53,005 --> 00:58:56,845
Speaker 10:  like green light. Totally. 'cause that's their answer to

918
00:58:56,995 --> 00:59:00,325
Speaker 10:  ambient light. And I'm just gonna say it, it's the spaghetti phase.

919
00:59:00,765 --> 00:59:04,655
Speaker 10:  Everyone does not know what people actually want or

920
00:59:04,655 --> 00:59:08,535
Speaker 10:  what they will actually buy or what their reservations are. So they're like,

921
00:59:08,535 --> 00:59:11,375
Speaker 10:  here, here's a version with a display. Here's a version that's just audio.

922
00:59:11,435 --> 00:59:14,455
Speaker 10:  Here's a version with camera, here's a version with a, where the camera

923
00:59:14,675 --> 00:59:18,655
Speaker 10:  is in the nose bridge, what are we gonna call smart glasses? Is

924
00:59:18,655 --> 00:59:22,285
Speaker 10:  everything going to be called a smart glass? Because here's another interesting

925
00:59:22,485 --> 00:59:25,685
Speaker 10:  tidbit in the developer keynote. So not the main keynote, the developer

926
00:59:25,685 --> 00:59:29,565
Speaker 10:  keynote X reel was announced to have the first official

927
00:59:29,635 --> 00:59:33,605
Speaker 10:  Android XR pair of, I'm gonna call them smart

928
00:59:33,605 --> 00:59:36,965
Speaker 10:  glasses, but were they allowed to call it smart glasses? I don't know because

929
00:59:36,965 --> 00:59:40,685
Speaker 10:  their press release said optical see-through XR

930
00:59:40,685 --> 00:59:44,525
Speaker 10:  device. Huh. So not glasses but if you look at them they're glasses

931
00:59:45,105 --> 00:59:47,845
Speaker 1:  XR device. I don't even know what to make of that sentence.

932
00:59:48,675 --> 00:59:52,325
Speaker 10:  Well what's an optical see-through XR device that you wear on your face?

933
00:59:52,435 --> 00:59:56,405
Speaker 10:  It's glasses, right? Like that's it's it's optical. It's, you can see through

934
00:59:56,405 --> 01:00:00,205
Speaker 10:  it but you know XR X reels made a a name for

935
01:00:00,205 --> 01:00:03,925
Speaker 10:  itself making like little portable screens that you carry around that happen

936
01:00:03,945 --> 01:00:07,205
Speaker 10:  to look like glasses that you have to plug into another device and it's

937
01:00:07,205 --> 01:00:10,525
Speaker 10:  just like a portable monitor that you can bring around. Yeah, this version

938
01:00:10,745 --> 01:00:14,245
Speaker 10:  we have very few details. They didn't have it on hand. They were there but

939
01:00:14,245 --> 01:00:18,005
Speaker 10:  they didn't have it on hand. And the render shows cameras

940
01:00:18,065 --> 01:00:21,125
Speaker 10:  in the nose bridge, which we haven't seen yet. We haven't seen cameras in

941
01:00:21,125 --> 01:00:23,525
Speaker 10:  the nose bridge just yet. So I'm like ooh that's interesting.

942
01:00:23,865 --> 01:00:26,725
Speaker 1:  So it's like if you're imagining you're cross-eyed and that's how you take

943
01:00:26,725 --> 01:00:27,565
Speaker 1:  pictures, I love that.

944
01:00:27,905 --> 01:00:31,725
Speaker 10:  Or just trying to get beyond the bangs problem. Fair to be quite frank.

945
01:00:32,905 --> 01:00:36,165
Speaker 10:  So it's interesting, it's an interesting

946
01:00:36,795 --> 01:00:40,605
Speaker 10:  reclamation of Google's history with this particular form

947
01:00:40,605 --> 01:00:44,365
Speaker 10:  factor as well because I Google, you ain't fooling me. That

948
01:00:44,365 --> 01:00:48,125
Speaker 10:  keynote comes out there, you have Azadi out on

949
01:00:48,125 --> 01:00:51,405
Speaker 10:  stage and he is like, we've been in this space for 10 years. I was like,

950
01:00:51,425 --> 01:00:55,405
Speaker 10:  really? 'cause I really do feel like you tried to bury that under the

951
01:00:55,405 --> 01:00:58,045
Speaker 10:  enterprise tech carpet for a good number of years.

952
01:00:58,275 --> 01:01:01,125
Speaker 1:  It's so funny you bring this up. I was thinking about that as they were saying

953
01:01:01,125 --> 01:01:04,725
Speaker 1:  this. So I was like is when was the last time Google willingly acknowledged

954
01:01:04,745 --> 01:01:08,645
Speaker 1:  the existence of Google Glass? And I think it a, it's actually a

955
01:01:08,645 --> 01:01:12,205
Speaker 1:  lot longer than 10 years Friend OS and B,

956
01:01:12,705 --> 01:01:15,885
Speaker 1:  you spent like eight of those 10 years trying to get everybody to forget

957
01:01:15,915 --> 01:01:18,285
Speaker 1:  that you had ever done any of this in the first place.

958
01:01:18,915 --> 01:01:21,845
Speaker 10:  Yeah, it it like from the outside it looks like, oh,

959
01:01:23,145 --> 01:01:27,075
Speaker 10:  oh the Meadow Ray bands are people actually want

960
01:01:27,075 --> 01:01:30,915
Speaker 10:  that. Oh well you know we got like these 20 years of notes

961
01:01:30,975 --> 01:01:34,235
Speaker 10:  and research and partnerships like let's get back on that, that train,

962
01:01:34,235 --> 01:01:36,555
Speaker 1:  It's actually the most Google thing in the world, right? Yeah. This is the

963
01:01:36,555 --> 01:01:39,955
Speaker 1:  same company that like invented all of the important technology behind

964
01:01:40,345 --> 01:01:43,795
Speaker 1:  chat GPT and then chat GPT launched and Google was like, you want that?

965
01:01:44,135 --> 01:01:47,915
Speaker 1:  Oh we can do that. People like this. Sure we'll make a product

966
01:01:48,015 --> 01:01:51,635
Speaker 1:  out of that. But I think on the glasses front, the the other piece of news

967
01:01:52,065 --> 01:01:55,715
Speaker 1:  that I think I I found even more interesting than the RA stuff is some of

968
01:01:55,715 --> 01:01:58,835
Speaker 1:  the partnership stuff that yes Google announced about who they're making

969
01:01:58,835 --> 01:02:02,555
Speaker 1:  glasses with and these are companies you know a lot more about than I do.

970
01:02:02,655 --> 01:02:06,355
Speaker 1:  Can you just like set in context who these partners are for Google?

971
01:02:06,775 --> 01:02:10,475
Speaker 10:  So the big two that they announced on stage are Gentle Monster

972
01:02:10,895 --> 01:02:14,875
Speaker 10:  and Warby Parker. I'm gonna guess most people Forge

973
01:02:14,905 --> 01:02:15,515
Speaker 10:  listeners

974
01:02:17,155 --> 01:02:20,765
Speaker 10:  probably may not know about Gentle Monster. You know, Gentle Monster

975
01:02:20,945 --> 01:02:24,325
Speaker 10:  if one you are Korean because every Korean person,

976
01:02:25,375 --> 01:02:28,685
Speaker 10:  their mother turns to them and goes, did you know that company was Korean?

977
01:02:28,825 --> 01:02:31,845
Speaker 10:  And so that's how I actually know interesting about Gentle Monster because

978
01:02:32,035 --> 01:02:35,285
Speaker 10:  it's a big brand in Korea and it's super

979
01:02:35,955 --> 01:02:38,805
Speaker 10:  popular with Luxury Ho Couture.

980
01:02:39,825 --> 01:02:42,725
Speaker 10:  You have 100% seen Gentle Monster glasses

981
01:02:43,355 --> 01:02:46,965
Speaker 10:  because Kendrick wears them. Kendrick Lamar wears them, Beyonce wears them,

982
01:02:47,325 --> 01:02:49,965
Speaker 10:  Rihanna wears them. Billie Eilish wears them.

983
01:02:50,715 --> 01:02:53,885
Speaker 10:  Everybody famous who has worn kind of a weird

984
01:02:54,365 --> 01:02:57,885
Speaker 10:  silhouette, cool pair of sunglasses or glasses

985
01:02:58,185 --> 01:03:01,965
Speaker 10:  in the last, I wanna say 10 years guarantee you one of them is

986
01:03:02,165 --> 01:03:02,845
Speaker 10:  probably Gentle Monster.

987
01:03:03,185 --> 01:03:05,725
Speaker 1:  I'm looking at this collection now and Gentle Monster, they have a bunch

988
01:03:05,745 --> 01:03:09,285
Speaker 1:  of the like very skinny sort of like

989
01:03:09,515 --> 01:03:13,325
Speaker 1:  vaguely like Blade Runner futuristic looking ones that

990
01:03:13,325 --> 01:03:17,045
Speaker 1:  you're right, I've seen on like you, you you see red carpets everywhere filled

991
01:03:17,045 --> 01:03:20,845
Speaker 1:  with this like there's like slightly wide, slightly skinny

992
01:03:21,355 --> 01:03:24,445
Speaker 1:  glasses that, and now I'm thinking a lot of these are probably Gentle Monster

993
01:03:25,265 --> 01:03:29,125
Speaker 10:  Or or if you see someone with extremely large

994
01:03:29,125 --> 01:03:32,885
Speaker 10:  glasses, like extremely like don't talk to me,

995
01:03:33,285 --> 01:03:37,005
Speaker 10:  these are obscenely large S sunglasses, those are also Gentle Monster.

996
01:03:37,395 --> 01:03:39,965
Speaker 10:  Love it. Okay. They're very artsy-fartsy,

997
01:03:41,275 --> 01:03:44,715
Speaker 10:  edgy, bold, that kind of vibe. So that's, that's an

998
01:03:45,275 --> 01:03:45,915
Speaker 10:  interesting pick.

999
01:03:46,455 --> 01:03:50,355
Speaker 1:  So like less mainstream than Ray Band but maybe like

1000
01:03:50,355 --> 01:03:52,035
Speaker 1:  cooler than RayBan at the moment.

1001
01:03:52,175 --> 01:03:55,955
Speaker 10:  Yes. Okay. Interesting. Busier than RayBan. Yeah. Much bigger

1002
01:03:55,955 --> 01:03:56,875
Speaker 10:  with Gen Z.

1003
01:03:57,745 --> 01:04:01,395
Speaker 1:  That seems like a big win for Google. If you can't be like Meta and get

1004
01:04:01,755 --> 01:04:05,195
Speaker 1:  s sort Luxottica and have all of the glasses that exist in the universe as

1005
01:04:05,195 --> 01:04:09,115
Speaker 1:  your partners, this is what you do. Right? Instead

1006
01:04:09,115 --> 01:04:12,835
Speaker 1:  of like try to go and Warby Parker is the other partner. And I think this,

1007
01:04:13,235 --> 01:04:16,395
Speaker 1:  it's like that's more along the lines of what I would've expected Google

1008
01:04:16,495 --> 01:04:19,515
Speaker 1:  to do, which is not a ding, but Warby Parker is like very straightforward,

1009
01:04:20,065 --> 01:04:23,915
Speaker 1:  very kind of like earnest regular people glasses and

1010
01:04:23,915 --> 01:04:27,275
Speaker 1:  that's fine and like Warby Parker is like a very

1011
01:04:27,715 --> 01:04:31,555
Speaker 1:  millennial company for millennials and that's fine, but what, what

1012
01:04:31,555 --> 01:04:35,195
Speaker 1:  you need if you're Google is something that is the like absolute cultural

1013
01:04:35,475 --> 01:04:39,235
Speaker 1:  opposite of Google Glass and Gentle Monster from the way you described it

1014
01:04:39,235 --> 01:04:42,075
Speaker 1:  sound like about as close to that as there might be right now.

1015
01:04:42,545 --> 01:04:46,435
Speaker 10:  Yeah and my conspiracy theory is that they got it because they're partnering

1016
01:04:46,435 --> 01:04:50,155
Speaker 10:  with Samsung. So you know, Samsung is an

1017
01:04:50,155 --> 01:04:53,875
Speaker 10:  institution in Korea, It is responsible for a huge

1018
01:04:53,875 --> 01:04:57,395
Speaker 10:  chunk of the country's GDP. There's a saying in Korea that if

1019
01:04:57,425 --> 01:05:01,235
Speaker 10:  Samsung's knees the whole country shakes, like that's the cultural cachet

1020
01:05:01,235 --> 01:05:02,315
Speaker 10:  that Samsung has in Korea.

1021
01:05:02,775 --> 01:05:06,635
Speaker 1:  So we're gonna start seeing like K-pop s bands wearing moan pretty

1022
01:05:06,635 --> 01:05:08,155
Speaker 1:  soon is what you're saying. Like that's coming,

1023
01:05:08,535 --> 01:05:12,515
Speaker 10:  You know, honest to God probably yes. You'll see BT S is

1024
01:05:12,515 --> 01:05:16,435
Speaker 10:  back from the Army. You're gonna see John Cook in, in Moan probably Stray

1025
01:05:16,435 --> 01:05:20,245
Speaker 10:  Kids as well. Grag, these are, if you know K-pop, these are huge

1026
01:05:20,245 --> 01:05:23,725
Speaker 10:  names that I'm, I'm listing right now. But you know, Gentle Monster has

1027
01:05:23,725 --> 01:05:27,565
Speaker 10:  done these huge celeb collabs. They know how to be like, we are the

1028
01:05:27,615 --> 01:05:31,165
Speaker 10:  artsy one, we are the cool one. And I think from a strategic

1029
01:05:31,165 --> 01:05:34,485
Speaker 10:  perspective, what you're seeing here is they're going accessibility,

1030
01:05:34,955 --> 01:05:38,885
Speaker 10:  affordability, and people not wanting to look crazy

1031
01:05:39,395 --> 01:05:43,245
Speaker 10:  with Warby Parker. That's a great play. Especially

1032
01:05:43,245 --> 01:05:46,965
Speaker 10:  because Warby Parker is direct to consumer, right? Right. So

1033
01:05:46,995 --> 01:05:50,165
Speaker 10:  it's easier than having to go to your LensCrafter, which is what I have

1034
01:05:50,165 --> 01:05:53,885
Speaker 10:  to do with a lot of Cel Luxottica stuff because that's

1035
01:05:54,005 --> 01:05:57,845
Speaker 10:  their thing. And RayBan's, the Ray band to Warby Parker is probably more

1036
01:05:57,845 --> 01:06:01,525
Speaker 10:  of a one-to-one in terms of classic frames. Yes, that's right. Things that

1037
01:06:01,635 --> 01:06:05,485
Speaker 10:  that are, you know, people aren't gonna be afraid to wear, but

1038
01:06:05,485 --> 01:06:08,725
Speaker 10:  then they're trying to get this cool, this cachet of cool with

1039
01:06:09,105 --> 01:06:12,565
Speaker 10:  Gentle Monster that I don't know if RayBan

1040
01:06:12,905 --> 01:06:16,685
Speaker 10:  and Meta fully have that because the rumor is that they're gonna go

1041
01:06:16,685 --> 01:06:20,565
Speaker 10:  with Oakley's for their next thing and that's just targeting

1042
01:06:20,705 --> 01:06:24,245
Speaker 10:  the athletes. Right. And, and, and I guess dads in Texas, which

1043
01:06:24,245 --> 01:06:26,245
Speaker 1:  Makes sense, but it's a very different play than this one.

1044
01:06:26,475 --> 01:06:29,565
Speaker 10:  Yeah, it's a very different play. I think it's a very interesting one. I

1045
01:06:29,565 --> 01:06:33,245
Speaker 10:  think Maui Jim was another brand that was kind of mentioned

1046
01:06:33,295 --> 01:06:37,005
Speaker 10:  under the, I forget the name of the, the company that they're working with

1047
01:06:37,005 --> 01:06:40,845
Speaker 10:  in their umbrella of eyewear, but Maui gym are, they're,

1048
01:06:40,845 --> 01:06:44,285
Speaker 10:  they're also sunglasses that are au wears. So you know, they're kind of

1049
01:06:44,285 --> 01:06:47,645
Speaker 10:  going for that Yeah. Vibe as well, which is smart because

1050
01:06:48,545 --> 01:06:52,245
Speaker 10:  the, the biggest eyewear brand in the world is Essilor Luxottica.

1051
01:06:52,445 --> 01:06:56,365
Speaker 1:  I agree. All right, I'm gonna let you go here, but before we do, I

1052
01:06:56,365 --> 01:06:59,805
Speaker 1:  need to know what you think about this Johnny, ive Sam Altman thing I'm losing

1053
01:07:00,195 --> 01:07:00,485
Speaker 10:  Last

1054
01:07:00,485 --> 01:07:04,445
Speaker 1:  Week. We think it's not smart glasses. The the the, the winds seem to be

1055
01:07:04,445 --> 01:07:08,285
Speaker 1:  saying it's not smart glasses. People tell lies all the time. It might

1056
01:07:08,285 --> 01:07:11,445
Speaker 1:  still be smart glasses, you never know, but what, what is your like

1057
01:07:12,125 --> 01:07:13,685
Speaker 1:  earliest read on what you think this thing is?

1058
01:07:14,305 --> 01:07:18,285
Speaker 10:  So here's what we know, right? It's not glasses 'cause they've kind of outright

1059
01:07:18,285 --> 01:07:19,565
Speaker 10:  said it's not glasses sort

1060
01:07:19,565 --> 01:07:23,325
Speaker 1:  Of, again, people tell lies but I people, people tell lies it does all

1061
01:07:24,005 --> 01:07:25,645
Speaker 1:  existing evidence points to it's not glasses.

1062
01:07:26,515 --> 01:07:30,485
Speaker 10:  Yeah. Joni, I've kind of shaded humane and

1063
01:07:30,485 --> 01:07:34,285
Speaker 10:  rabbit. If he's gonna go that far to shade them and say those are bad products,

1064
01:07:35,245 --> 01:07:37,045
Speaker 10:  probably not in that vein. Right.

1065
01:07:37,475 --> 01:07:40,205
Speaker 1:  Okay. Can I, can I just very quickly make the opposite case of that?

1066
01:07:40,435 --> 01:07:41,005
Speaker 10:  Okay, yeah, sure.

1067
01:07:41,345 --> 01:07:44,845
Speaker 1:  That's Apple's thing, right? Like what, what Johnny I did is the equivalent

1068
01:07:44,845 --> 01:07:48,205
Speaker 1:  of putting up the slide of all the ugly smartphones before you announce your

1069
01:07:48,205 --> 01:07:52,045
Speaker 1:  own smartphone. I, I'm just, I'm not ruling it out. I think the funniest

1070
01:07:52,485 --> 01:07:55,325
Speaker 1:  possible outcome would be if they just made it a nicer looking humane pin,

1071
01:07:55,335 --> 01:07:57,085
Speaker 1:  which I still think is possible.

1072
01:07:57,905 --> 01:08:00,925
Speaker 10:  So I'm cor I sort of was just like, oh, they're not gonna do a pin because

1073
01:08:00,925 --> 01:08:04,725
Speaker 10:  he, he shot on the pin and then the more details that

1074
01:08:04,905 --> 01:08:08,045
Speaker 10:  slowly trickle out, I'm just sort of like, oh, he's not doing humane pin,

1075
01:08:08,075 --> 01:08:10,685
Speaker 10:  he's doing plot, he's doing the plot notin. Hmm.

1076
01:08:10,785 --> 01:08:11,205
Speaker 1:  That's

1077
01:08:11,205 --> 01:08:14,885
Speaker 10:  A good tip. 'cause that's, that's kind of where I'm landing

1078
01:08:15,105 --> 01:08:18,685
Speaker 10:  at now because let's think of the components that go into AI

1079
01:08:18,965 --> 01:08:22,245
Speaker 10:  hardware as we know it now that companies seem to be

1080
01:08:22,375 --> 01:08:26,205
Speaker 10:  converging on, they kind of want it to be screenless

1081
01:08:26,205 --> 01:08:29,405
Speaker 10:  because as we've just spent a great amount of time talking about, no one

1082
01:08:29,405 --> 01:08:32,965
Speaker 10:  can, no one can agree on what the displays should be or how it should look.

1083
01:08:33,425 --> 01:08:37,045
Speaker 10:  So cut that out of the equation. No display. Yep. And also

1084
01:08:37,305 --> 01:08:40,685
Speaker 10:  you probably don't wanna do the humane thing where you make people

1085
01:08:41,505 --> 01:08:45,325
Speaker 10:  pay for a separate phone LTE connection thing. No one wants that. So

1086
01:08:45,325 --> 01:08:49,165
Speaker 10:  correct. Let's have it connect through your phone or another device. They're

1087
01:08:49,165 --> 01:08:52,525
Speaker 10:  saying it's the third device you buy because the first is gonna be a computer,

1088
01:08:52,625 --> 01:08:55,685
Speaker 10:  the second is gonna be your phone or you could reverse that order. So this

1089
01:08:55,685 --> 01:08:58,965
Speaker 10:  is the third device we want you to have. We want it to fit in your pocket

1090
01:08:59,025 --> 01:09:01,965
Speaker 10:  but also on your desk. But also maybe you could wear it around your neck

1091
01:09:03,425 --> 01:09:06,685
Speaker 10:  and I'm landing on the PLA note pin or B, that

1092
01:09:07,755 --> 01:09:11,325
Speaker 10:  vein of device, but make it Joni. Ive make it beautiful,

1093
01:09:11,795 --> 01:09:15,645
Speaker 10:  make it less stupid compared to Humane under

1094
01:09:15,645 --> 01:09:17,005
Speaker 10:  Promise what it can do.

1095
01:09:18,515 --> 01:09:22,295
Speaker 10:  But flaunt the fact that it's with OpenAI officially.

1096
01:09:23,315 --> 01:09:26,415
Speaker 10:  You get the subscription to OpenAI and they send you this hardware, it could

1097
01:09:26,415 --> 01:09:29,895
Speaker 10:  sit on your desk, it that can fit in your pocket and you can wear it, but

1098
01:09:29,895 --> 01:09:33,695
Speaker 10:  it doesn't need to be wearable. That's I'm landing on

1099
01:09:33,695 --> 01:09:37,655
Speaker 10:  something that looks and feels like the Claude note pin,

1100
01:09:38,345 --> 01:09:42,295
Speaker 10:  which let's face it, that's kind of the category that's gaining buzz

1101
01:09:42,365 --> 01:09:43,255
Speaker 10:  outside of glasses.

1102
01:09:43,605 --> 01:09:46,655
Speaker 1:  Fair enough. All right v thank you as always. We gotta take a break and then

1103
01:09:46,655 --> 01:09:49,575
Speaker 1:  we're gonna come back and take a collection from The Vergecast hotline. V

1104
01:09:49,595 --> 01:09:50,735
Speaker 1:  Go Sleep. I'll see you soon.

1105
01:09:51,075 --> 01:09:52,735
Speaker 10:  Ah, thank you for letting me ramble.

1106
01:09:53,345 --> 01:09:54,055
Speaker 1:  We'll be right back.

1107
01:12:56,575 --> 01:12:59,475
Speaker 1:  All right, we're back. Let's do a question from The Vergecast hotline. As

1108
01:12:59,475 --> 01:13:02,955
Speaker 1:  always, the number is 8 6 6 VERGE one one. The email is vergecast at The

1109
01:13:03,035 --> 01:13:06,315
Speaker 1:  Verge dot com. We love hearing from you. Thank you by the way, to everybody

1110
01:13:06,315 --> 01:13:09,595
Speaker 1:  who's reached out with responses to Allison's question about phone therapy.

1111
01:13:09,775 --> 01:13:13,475
Speaker 1:  If you're going through a phone conundrum, we want to hear from you. We've

1112
01:13:13,475 --> 01:13:16,155
Speaker 1:  gotten some really fun questions. That's gonna be a super fun episode this

1113
01:13:16,155 --> 01:13:20,075
Speaker 1:  summer, so keep 'em coming, keep calling, keep emailing this time.

1114
01:13:20,475 --> 01:13:24,075
Speaker 1:  I have one that I actually got a bunch of emails and calls about just in

1115
01:13:24,075 --> 01:13:27,875
Speaker 1:  the last like couple of days since this news dropped. Lemme just read you

1116
01:13:27,875 --> 01:13:30,915
Speaker 1:  one email. This comes from Blake. It says, I've been using pockets since

1117
01:13:30,915 --> 01:13:33,515
Speaker 1:  it was called Read It Later. The news that Mozilla is shutting it down is

1118
01:13:33,515 --> 01:13:37,435
Speaker 1:  sad but not surprising. Agree and agree. By the way, other alternatives

1119
01:13:37,435 --> 01:13:40,275
Speaker 1:  I've tried like Matter or read Wise, I've bounced off of. Is that the main

1120
01:13:40,275 --> 01:13:43,515
Speaker 1:  recommendation now or what should I be trying to migrate my read Readit later

1121
01:13:43,795 --> 01:13:47,275
Speaker 1:  workflow into additional context? I do desktop reading in a chromium based

1122
01:13:47,275 --> 01:13:51,235
Speaker 1:  browser. Arc Edge Chrome Most people do and mobile and Safari

1123
01:13:51,535 --> 01:13:55,355
Speaker 1:  and ideally want a nice native app. I use Reader Classic for Feedly and

1124
01:13:55,355 --> 01:13:58,195
Speaker 1:  Pocket. So I guess if I wanna keep using that then my options are limited

1125
01:13:58,195 --> 01:14:01,595
Speaker 1:  to iCloud or Instapaper and the latter of which definitely won't be around

1126
01:14:01,595 --> 01:14:05,355
Speaker 1:  forever either. Thanks Blake. Okay, I have

1127
01:14:05,465 --> 01:14:09,155
Speaker 1:  five answers to the what should you replace

1128
01:14:09,155 --> 01:14:12,945
Speaker 1:  Pocket with question and I will say there are, there are two different things

1129
01:14:12,945 --> 01:14:16,545
Speaker 1:  people use an app like this for and I'm a big believer in both of them, but

1130
01:14:16,545 --> 01:14:20,385
Speaker 1:  they have different answers. So the one version of

1131
01:14:20,665 --> 01:14:24,385
Speaker 1:  a pocket user is somebody who just like uses it to dump links into. It's

1132
01:14:24,385 --> 01:14:27,785
Speaker 1:  like a searchable bookmarking system for that

1133
01:14:28,325 --> 01:14:32,305
Speaker 1:  pocket was never really the best option. And I think there are many better

1134
01:14:32,305 --> 01:14:35,985
Speaker 1:  options out there if you're an Apple user. There are apps like Any

1135
01:14:36,085 --> 01:14:40,025
Speaker 1:  Box and good links that do that more specifically, there's also one called

1136
01:14:40,235 --> 01:14:43,705
Speaker 1:  Pliny that I like, but the one that I recommend to most people

1137
01:14:44,085 --> 01:14:47,905
Speaker 1:  is called Raindrop dot io. It's basically a bookmarking service

1138
01:14:48,205 --> 01:14:51,265
Speaker 1:  but it works on every platform you can think of. It's really easy to share

1139
01:14:51,265 --> 01:14:54,465
Speaker 1:  stuff to. It is very good search. It has desktop apps, it's really good for

1140
01:14:54,465 --> 01:14:56,785
Speaker 1:  exporting, which I think is really important. You can get stuff out of it

1141
01:14:56,785 --> 01:15:00,545
Speaker 1:  really easily. It does cost money for some of the

1142
01:15:00,615 --> 01:15:03,665
Speaker 1:  main features, but the free tier I think is actually plenty for most people.

1143
01:15:04,245 --> 01:15:07,025
Speaker 1:  So that's where I would start you if you're just like, where do I put all

1144
01:15:07,025 --> 01:15:10,745
Speaker 1:  of my links? I still think browser bookmarks are a terrible

1145
01:15:10,965 --> 01:15:14,825
Speaker 1:  and underused system. I don't recommend that I would start you with

1146
01:15:14,825 --> 01:15:18,585
Speaker 1:  Raindrop and see where you get. But if you want a place to like read

1147
01:15:18,985 --> 01:15:22,745
Speaker 1:  articles, there are a bunch of possibilities. One is like

1148
01:15:22,845 --> 01:15:26,665
Speaker 1:  use the reading mode in your browser and that's fine. But I

1149
01:15:26,665 --> 01:15:29,625
Speaker 1:  don't think that's the answer. Like if that's all you need, terrific. But

1150
01:15:29,625 --> 01:15:32,545
Speaker 1:  I I, I don't think that's all people who are looking for a way to replace

1151
01:15:32,545 --> 01:15:36,305
Speaker 1:  Pocket actually need. So lemme just go in order of

1152
01:15:36,475 --> 01:15:40,185
Speaker 1:  chaos. I guess the simplest version is Instapaper

1153
01:15:40,665 --> 01:15:44,545
Speaker 1:  and Blake mentioned it but I think Blake and I disagree on

1154
01:15:44,545 --> 01:15:47,905
Speaker 1:  whether Instapaper is gonna be around for a while. So Instapaper has been

1155
01:15:47,905 --> 01:15:51,425
Speaker 1:  through kind of a weird history. It was one of the earliest apps in this

1156
01:15:51,425 --> 01:15:55,055
Speaker 1:  space. It eventually sold to Pinterest.

1157
01:15:55,475 --> 01:15:59,375
Speaker 1:  It has like morphed over the years through a couple

1158
01:15:59,375 --> 01:16:03,215
Speaker 1:  of different owners, but now the guy who owns it and runs It is

1159
01:16:03,215 --> 01:16:06,415
Speaker 1:  this guy named Brian Donahue who has been doing it for a long time. He's

1160
01:16:06,415 --> 01:16:09,295
Speaker 1:  very serious about It, is doing a really good job. The app kind of languished

1161
01:16:09,295 --> 01:16:13,015
Speaker 1:  for a while but has really come up recently. And I think in terms of just

1162
01:16:13,425 --> 01:16:17,095
Speaker 1:  basic, I would like a nice reading experience offline on my phone to read

1163
01:16:17,095 --> 01:16:20,695
Speaker 1:  articles. Instagram, Francisco as it gets, again, there's a premium tier

1164
01:16:20,695 --> 01:16:23,735
Speaker 1:  that you have to pay for if you want, but a lot of the stuff that you need

1165
01:16:23,735 --> 01:16:27,055
Speaker 1:  is in the free tier. So I would say actually most people who are looking

1166
01:16:27,055 --> 01:16:31,015
Speaker 1:  for somewhere to go from Pocket, I would send you to Instapaper.

1167
01:16:31,755 --> 01:16:35,495
Speaker 1:  Option number two is an app called Matter and Matter

1168
01:16:35,995 --> 01:16:39,735
Speaker 1:  is amazing and beautiful and really clever and has some really

1169
01:16:40,015 --> 01:16:43,415
Speaker 1:  interesting AI features, but it has one big downside, which is that It is

1170
01:16:43,415 --> 01:16:47,055
Speaker 1:  expensive and It is Apple products only. So if you're on your

1171
01:16:47,115 --> 01:16:50,135
Speaker 1:  iPhone and your iPad and the web and that's where you wanna do all of your

1172
01:16:50,135 --> 01:16:53,895
Speaker 1:  stuff, fine. But if you are cross-platform or want desktop stuff,

1173
01:16:54,675 --> 01:16:58,455
Speaker 1:  you're just sort of stuck going elsewhere. So that's option number two. Option

1174
01:16:58,455 --> 01:17:02,305
Speaker 1:  number three is Readwise Reader, which I

1175
01:17:02,305 --> 01:17:06,185
Speaker 1:  think is the best and most powerful reading app out there. It's the

1176
01:17:06,185 --> 01:17:10,165
Speaker 1:  one that I use but also like I read and research for a living. So

1177
01:17:10,245 --> 01:17:13,445
Speaker 1:  I need much more than a lot of people. I really like the way that it does

1178
01:17:13,445 --> 01:17:15,885
Speaker 1:  highlighting. I like the way that it integrates with lots of other apps So

1179
01:17:15,885 --> 01:17:19,685
Speaker 1:  I can get my highlights out into my note taking app. I like all the organizational

1180
01:17:19,685 --> 01:17:23,405
Speaker 1:  systems that it has. I like the way that you can also add PDFs and you can

1181
01:17:23,405 --> 01:17:26,045
Speaker 1:  add books and you can search through all the text of everything and It is

1182
01:17:26,045 --> 01:17:29,965
Speaker 1:  like the app is not attractive and it feels

1183
01:17:30,075 --> 01:17:33,965
Speaker 1:  like a work app if that makes sense. It's much closer to like a

1184
01:17:34,085 --> 01:17:37,885
Speaker 1:  B2B SaaS piece of software than to like a

1185
01:17:37,885 --> 01:17:41,485
Speaker 1:  lovely little reading app. But for my purposes

1186
01:17:41,825 --> 01:17:45,565
Speaker 1:  it does the job. It has a great speech thing that

1187
01:17:45,635 --> 01:17:49,565
Speaker 1:  lets you read articles out loud. It's expensive like some of these other

1188
01:17:49,565 --> 01:17:53,125
Speaker 1:  ones, but It is, I think for my money, the best one on the market.

1189
01:17:53,945 --> 01:17:57,445
Speaker 1:  That's option number three. Option number four is an app called

1190
01:17:57,615 --> 01:18:01,165
Speaker 1:  Walla Bag. And Walla Bag is different than the other ones in the sense that

1191
01:18:01,165 --> 01:18:04,885
Speaker 1:  It is an open source project that is kind of meant to be

1192
01:18:05,195 --> 01:18:08,845
Speaker 1:  self-hosted. And the upside of Walla Bag is that means no one can take it

1193
01:18:08,845 --> 01:18:11,605
Speaker 1:  away from you. You can run it pretty easily. It's really easy to spin up

1194
01:18:11,605 --> 01:18:15,405
Speaker 1:  on your own hardware. It's a, I don know, it's like a fine

1195
01:18:15,505 --> 01:18:18,325
Speaker 1:  app. It's not great looking, but it's fine. It does the job

1196
01:18:19,785 --> 01:18:23,645
Speaker 1:  and if you are willing to do a little bit of work, It is a much more future

1197
01:18:23,695 --> 01:18:27,645
Speaker 1:  proof way to roll something like this for yourself. I think that's

1198
01:18:27,645 --> 01:18:30,085
Speaker 1:  way more work than most people want to do. But if you wanna do the work,

1199
01:18:30,495 --> 01:18:34,405
Speaker 1:  Walla Bag is a great place to start. Then the last

1200
01:18:34,465 --> 01:18:38,325
Speaker 1:  option I would offer you is I think to try

1201
01:18:38,345 --> 01:18:41,965
Speaker 1:  and combine a Relator app with an RSS reader.

1202
01:18:42,185 --> 01:18:45,765
Speaker 1:  My guess is if you're a person who uses something like Pocket, you're also

1203
01:18:45,845 --> 01:18:49,485
Speaker 1:  a person who might use something like Feedly or Feed Bin

1204
01:18:49,825 --> 01:18:53,725
Speaker 1:  or one of these other apps that lets you sort of put all of the things

1205
01:18:53,725 --> 01:18:56,965
Speaker 1:  that you care about into one place. I really recommend doing that by the

1206
01:18:56,965 --> 01:19:00,885
Speaker 1:  way, I, I have been using Feed bin for years and love it to pieces and recommend

1207
01:19:00,885 --> 01:19:04,605
Speaker 1:  it to everybody. But many of these things have a place that you can

1208
01:19:05,045 --> 01:19:09,005
Speaker 1:  manually send an article into your RSS reader and it just treats it like

1209
01:19:09,005 --> 01:19:12,525
Speaker 1:  another feed of articles that you've saved. That's really useful in the sense

1210
01:19:12,525 --> 01:19:16,205
Speaker 1:  that it, It is yet more stuff all in one place. The problem with a lot of

1211
01:19:16,205 --> 01:19:19,285
Speaker 1:  these read later apps is it just turns into another inbox to check and as

1212
01:19:19,285 --> 01:19:22,365
Speaker 1:  the list of stuff to read gets longer, you wanna check it less. And so it

1213
01:19:22,365 --> 01:19:25,765
Speaker 1:  just becomes this like thousands of things. Long list of stuff you're never

1214
01:19:25,765 --> 01:19:29,205
Speaker 1:  gonna read, not bad. But if you put it in the place where you're actually

1215
01:19:29,205 --> 01:19:33,165
Speaker 1:  reading the news all the day and you have this other feed of like, here's

1216
01:19:33,165 --> 01:19:36,245
Speaker 1:  the stuff I wanna get back to, I actually have found that that balance works

1217
01:19:36,245 --> 01:19:40,205
Speaker 1:  really well and there are a bunch of really beautiful read apps for

1218
01:19:40,365 --> 01:19:44,005
Speaker 1:  RSS, like Reader, R-E-E-D-E-R, and

1219
01:19:44,225 --> 01:19:47,965
Speaker 1:  you can kind of make that whole experience work. So I would tell

1220
01:19:47,995 --> 01:19:50,365
Speaker 1:  most people, if you're looking for somewhere to go from Pocket and you don't

1221
01:19:50,365 --> 01:19:54,005
Speaker 1:  have like a giant long list of additional feature requests, try instant paper

1222
01:19:54,175 --> 01:19:58,045
Speaker 1:  first and see how far that gets you. But for my money, if

1223
01:19:58,045 --> 01:20:00,485
Speaker 1:  you're willing to do the work and willing to spend the money, Readwise Reader

1224
01:20:00,625 --> 01:20:03,845
Speaker 1:  is the best one out there and I actually don't think it's super close.

1225
01:20:04,645 --> 01:20:07,685
Speaker 1:  I hope that helps. I'm very sad. Pocket is gone. Pocket did a lot of good

1226
01:20:07,685 --> 01:20:10,725
Speaker 1:  for a lot of people for a long time and it's a bummer to see it go,

1227
01:20:11,545 --> 01:20:12,365
Speaker 1:  but this is what happens.

1228
01:20:14,665 --> 01:20:17,765
Speaker 1:  Anyway, that's it for the show today. Thank you to everybody who came on

1229
01:20:17,765 --> 01:20:21,725
Speaker 1:  and thank you as always for listening. As ever, if you have thoughts, questions,

1230
01:20:22,005 --> 01:20:25,805
Speaker 1:  feelings, if you have smart glasses you want to tell us about. If

1231
01:20:25,805 --> 01:20:29,325
Speaker 1:  you are thinking about what's gonna happen at the end of all of these trials

1232
01:20:29,325 --> 01:20:32,485
Speaker 1:  and you have a theory that we haven't talked about yet, get at us. 8 6 6

1233
01:20:32,685 --> 01:20:35,445
Speaker 1:  VERGE one. One is the hotline. Vergecast at The Verge dot com is the email

1234
01:20:35,445 --> 01:20:39,285
Speaker 1:  address we love hearing from you. This show is produced by Will Poor Eric Gomez

1235
01:20:39,285 --> 01:20:41,805
Speaker 1:  and Brandon Keefer. The Vergecast is The Verge production and part of the

1236
01:20:41,805 --> 01:20:45,525
Speaker 1:  Vox Media podcast network. Neli and I'll be back on Friday to talk about

1237
01:20:45,915 --> 01:20:49,605
Speaker 1:  more stuff with Johnny Ivan OpenAI, all of the stuff we're hearing

1238
01:20:49,605 --> 01:20:53,565
Speaker 1:  leading up to WWDC and lots more. We'll see you then rock and

1239
01:20:53,565 --> 01:20:53,725
Speaker 1:  roll

