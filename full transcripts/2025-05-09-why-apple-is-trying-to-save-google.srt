1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 45055ed4-da06-47d1-a47e-5e57648087b6
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/6259229236207102074/-1352064839180583531/s93290-US-6239s-1746784984.mp3
Description: Where will Meta, Apple, and Google be three years from now? It's starting to look like they might all be very different. Nilay, David, and The Verge's Richard Lawler start the show with Eddy Cue's testimony in the Google search trial, in which Cue argued that AI is taking over â€” and that Google should be allowed to keep paying Apple gobs of money. The hosts also chat about the latest in the Meta trials, and how the recent Apple ruling is already changing the App Store. Then, there are some gadgets to talk about: the panopticon-slash-killer-app coming for Meta's smart glasses, the new Surface Pro and Surface Laptop, and a lot of new iPhones. In the lightning round, we do another round of Brendan Carr is a Dummy, then talk about some new Netflix designs and the latest in our worldwide hunt for party speakers.


2
00:01:27,095 --> 00:01:27,695
Speaker 5:  I had this week.

3
00:01:27,765 --> 00:01:29,015
Speaker 6:  This is your new claim to fame.

4
00:01:29,265 --> 00:01:32,295
Speaker 5:  We're gonna talk about it vaguely remembered.

5
00:01:32,995 --> 00:01:36,335
Speaker 6:  Is it better to be like very remembered

6
00:01:36,915 --> 00:01:39,055
Speaker 6:  or not remembered? Like if you had to choose between

7
00:01:40,815 --> 00:01:44,015
Speaker 6:  slightly, vaguely, barely remembered and not remembered at all. If he was

8
00:01:44,015 --> 00:01:47,255
Speaker 6:  like, I have no memory of doing that podcast with Eli four years ago, is

9
00:01:47,255 --> 00:01:49,535
Speaker 6:  that better or worse? Well, if you'd said Nelly

10
00:01:49,545 --> 00:01:53,455
Speaker 5:  Patel, he'd know who you're talking about. Yeah, exactly. Eli, who's

11
00:01:53,455 --> 00:01:57,375
Speaker 5:  that guy? You know, as a podcast host, I think what you want is, I remember

12
00:01:57,375 --> 00:02:00,775
Speaker 5:  every minute of that experience and changed. I send it to everyone that I've

13
00:02:00,775 --> 00:02:04,725
Speaker 5:  ever met and I'm like, this is my org chart. We're gonna get to it, we're

14
00:02:04,725 --> 00:02:08,645
Speaker 5:  gonna explain what we're talking about. But It is true that literally today,

15
00:02:08,645 --> 00:02:12,605
Speaker 5:  just hours before we are speaking adversary under

16
00:02:12,675 --> 00:02:16,325
Speaker 5:  oath in the United States Justice system said he

17
00:02:16,325 --> 00:02:17,725
Speaker 5:  vaguely remembered talking to me.

18
00:02:18,185 --> 00:02:19,165
Speaker 6:  Congratulations.

19
00:02:19,685 --> 00:02:23,365
Speaker 5:  It's, it's a, it's a weird life. I live, I'm your friend Eli. That's

20
00:02:23,365 --> 00:02:23,765
Speaker 5:  David Pierce.

21
00:02:23,895 --> 00:02:24,245
Speaker 6:  Hello,

22
00:02:24,475 --> 00:02:27,715
Speaker 5:  Richard Lawlers here. Hey, buddy, I'm back. He's back

23
00:02:28,835 --> 00:02:32,315
Speaker 5:  A lot going on this week. A lot of, a lot of courthouse activity.

24
00:02:33,705 --> 00:02:37,155
Speaker 5:  There's a rumor, by the way, from CBC's, David Faber, that Warner Brothers

25
00:02:37,155 --> 00:02:40,955
Speaker 5:  in Discovery will split up again, which is just the, the circle of life.

26
00:02:41,585 --> 00:02:43,715
Speaker 5:  Like we're gonna, we're gonna play the Lion King song here.

27
00:02:44,145 --> 00:02:47,235
Speaker 6:  They should call the companies Warner Discovery and just, bro,

28
00:02:47,955 --> 00:02:49,115
Speaker 6:  those should be the two companies

29
00:02:49,875 --> 00:02:53,355
Speaker 5:  I would join, bros. Yeah. Obviously I work for Bros Bro is the one. Yeah,

30
00:02:53,665 --> 00:02:57,355
Speaker 5:  there's there's some facial recognition news in the AR world.

31
00:02:58,185 --> 00:03:00,045
Speaker 5:  Brendan talked a lot this week.

32
00:03:00,855 --> 00:03:02,145
Speaker 6:  Yeah, he was back with a vengeance.

33
00:03:02,285 --> 00:03:05,945
Speaker 5:  Our boy, Brendan just did not shut up this week. We got, he

34
00:03:05,945 --> 00:03:08,905
Speaker 6:  Didn't like that the segment was short last week, so he, he had to do some

35
00:03:08,905 --> 00:03:09,105
Speaker 6:  stuff.

36
00:03:09,325 --> 00:03:13,065
Speaker 5:  He didn't, he was like, well, I gotta, I gotta start running my mouth. We

37
00:03:13,065 --> 00:03:16,985
Speaker 5:  have Gen Tui battling robot lawnmowers to the death. And

38
00:03:17,225 --> 00:03:21,185
Speaker 5:  then finally Netflix took my advice. Just be, let's be direct

39
00:03:21,275 --> 00:03:24,225
Speaker 5:  about what happened here. I told 'em to do something and they finally did

40
00:03:24,225 --> 00:03:28,105
Speaker 5:  it. So a lot, a lot going on. Let's start, let's start

41
00:03:28,105 --> 00:03:32,025
Speaker 5:  in court. Where else? My hot prediction is that 36 months from now,

42
00:03:32,445 --> 00:03:35,655
Speaker 5:  we will not recognize Apple and Google.

43
00:03:36,495 --> 00:03:39,275
Speaker 5:  That's, that's just my hottest prediction I've got going right now. Whatever

44
00:03:39,275 --> 00:03:42,355
Speaker 5:  you think of as Google now, and whatever you think of It is Apple. Now, 36

45
00:03:42,355 --> 00:03:44,835
Speaker 5:  months from now, these companies will be radically different in some way.

46
00:03:45,195 --> 00:03:47,915
Speaker 6:  Interesting that you're not lumping Meta into that. Do you not think it also

47
00:03:48,155 --> 00:03:48,395
Speaker 6:  qualifies?

48
00:03:49,235 --> 00:03:52,315
Speaker 5:  I don't know. Okay. We'll, we'll come to that meta's also in our trial, but

49
00:03:52,315 --> 00:03:55,955
Speaker 5:  like, if you peel Instagram in WhatsApp off of

50
00:03:56,345 --> 00:03:59,955
Speaker 5:  Meta, like I, I think the essential character of Meta remains,

51
00:04:00,635 --> 00:04:04,445
Speaker 5:  whereas, you know, what we're talking about with Apple is you're gonna, in

52
00:04:04,465 --> 00:04:08,445
Speaker 5:  one trial, they're gonna take $20 billion of search revenue away from Apple.

53
00:04:08,945 --> 00:04:12,205
Speaker 5:  In the other case, they can't make money in the app store the way they were.

54
00:04:12,205 --> 00:04:16,205
Speaker 5:  They've appealed that decision. That just changes the company. Yeah.

55
00:04:16,215 --> 00:04:20,165
Speaker 5:  Right. And then in the Google case, the, just in 50 different ways, the

56
00:04:20,165 --> 00:04:24,085
Speaker 5:  government might break up Google fundamental changes of the company. So

57
00:04:24,175 --> 00:04:27,005
Speaker 5:  those are my hot predictions. I maybe they'll come true, maybe they won't,

58
00:04:27,005 --> 00:04:30,325
Speaker 5:  that's why they're they're hot. But you really see that pressure

59
00:04:30,755 --> 00:04:34,565
Speaker 5:  that these companies are feeling right now in court where their

60
00:04:34,565 --> 00:04:38,405
Speaker 5:  various executives are on the stand talking about how their businesses work.

61
00:04:38,905 --> 00:04:42,685
Speaker 5:  In particular in the Google search remedies trial. Eddie

62
00:04:42,925 --> 00:04:46,845
Speaker 5:  Q is on the stand this week and he, I mean literally he, with just

63
00:04:46,965 --> 00:04:50,405
Speaker 5:  a sideways comment, he dropped Google stock price by

64
00:04:50,425 --> 00:04:54,405
Speaker 5:  10% or almost 10%. David, you've been paying

65
00:04:54,445 --> 00:04:56,525
Speaker 5:  a lot of attention to the Google search case. You were in that courtroom

66
00:04:56,525 --> 00:04:58,565
Speaker 5:  during the, the other part of the trial. What's going on here?

67
00:04:58,795 --> 00:05:01,645
Speaker 6:  Yeah, so Apple's been a really important part of this trial the whole time

68
00:05:01,645 --> 00:05:05,525
Speaker 6:  because like, it, it, the, the simplest version of this trial was about

69
00:05:06,145 --> 00:05:09,485
Speaker 6:  Google became too powerful because it could afford to be the default search

70
00:05:09,725 --> 00:05:13,645
Speaker 6:  engine essentially everywhere. And the most important and most

71
00:05:13,645 --> 00:05:17,365
Speaker 6:  expensive version of that is Safari on iOS. So Apple was,

72
00:05:17,525 --> 00:05:21,485
Speaker 6:  I mean like all over the, the liabilities part of this

73
00:05:21,485 --> 00:05:24,925
Speaker 6:  trial. And a huge piece of this was essentially Google is the only company

74
00:05:25,075 --> 00:05:28,965
Speaker 6:  that was able to give Apple $20 billion a year to be

75
00:05:29,265 --> 00:05:32,925
Speaker 6:  the Safari search engine. Thus we have a problem. Like you, you can kind

76
00:05:32,925 --> 00:05:36,285
Speaker 6:  of boil it all the way down to that. And that is, that is where we are. So

77
00:05:36,745 --> 00:05:40,045
Speaker 6:  it was meaningful again, to see Eddie Q come back at all to talk about this

78
00:05:40,045 --> 00:05:43,325
Speaker 6:  stuff. He talked to, he testified in the first version of the trial and now

79
00:05:43,325 --> 00:05:46,765
Speaker 6:  he's back doing it again. And he made a surprising amount of

80
00:05:47,035 --> 00:05:50,805
Speaker 6:  news for somebody who is like, very well media trained.

81
00:05:50,825 --> 00:05:53,845
Speaker 6:  And I think the thing I've come to believe is the people like this who come

82
00:05:53,845 --> 00:05:57,750
Speaker 6:  to testify largely come to do it on purpose. Right? And I think

83
00:05:58,155 --> 00:06:01,125
Speaker 6:  Nila, you're, you're a lawyer, so you, you know this better than I do, but

84
00:06:01,165 --> 00:06:04,925
Speaker 6:  I, there's like two ways it goes. There's either somebody asked them a

85
00:06:04,925 --> 00:06:07,525
Speaker 6:  relatively open-ended question and they get to say the thing that they've

86
00:06:07,525 --> 00:06:10,205
Speaker 6:  been preparing to say. And there was a lot of that. Like I was there for

87
00:06:10,365 --> 00:06:13,285
Speaker 6:  Sundar Phai last time and he really wanted to talk about not letting people

88
00:06:13,285 --> 00:06:16,725
Speaker 6:  reverse engineer Google. And he said that over and over whenever he was able

89
00:06:16,725 --> 00:06:20,205
Speaker 6:  to. And then there are just times where you ask them yes or no questions

90
00:06:20,205 --> 00:06:22,685
Speaker 6:  to get them to actually reveal things that they don't wanna reveal because

91
00:06:22,685 --> 00:06:26,625
Speaker 6:  they're, and they have to, those are the two sides of how

92
00:06:26,625 --> 00:06:29,265
Speaker 6:  you learn information in these trials is essentially what I've discovered

93
00:06:29,265 --> 00:06:33,105
Speaker 6:  sitting in them. Yeah, this one, Eddie Cube just unprompted starts dropping

94
00:06:33,355 --> 00:06:36,825
Speaker 6:  bombs about what's going on with Google search. So he's like,

95
00:06:37,165 --> 00:06:40,465
Speaker 6:  he says, the biggest news, the one you're referring to is that Eddie Q said

96
00:06:41,725 --> 00:06:45,385
Speaker 6:  by way of sort of explaining how transformative AI is

97
00:06:45,685 --> 00:06:48,985
Speaker 6:  and why AI is so important and why it's becoming competitive to search. And

98
00:06:49,335 --> 00:06:52,745
Speaker 6:  it's very clear obviously, that he would like to continue making $20 billion

99
00:06:52,865 --> 00:06:56,705
Speaker 6:  a year from Google. So all of this under the guise of like Apple would

100
00:06:56,705 --> 00:06:59,785
Speaker 6:  really like Google to win this trial so that it can keep giving Apple lots

101
00:06:59,785 --> 00:07:03,585
Speaker 6:  of money. He says that for the first time

102
00:07:03,685 --> 00:07:07,625
Speaker 6:  in, I believe 22 years was the number he gave ever. But in 22 years

103
00:07:07,625 --> 00:07:11,025
Speaker 6:  of having this deal, Google searches are going down in

104
00:07:11,425 --> 00:07:11,625
Speaker 6:  Safari

105
00:07:13,615 --> 00:07:17,275
Speaker 6:  by some meaningful amount. And that's

106
00:07:17,855 --> 00:07:19,355
Speaker 6:  the thing that tanked Google stock

107
00:07:19,545 --> 00:07:23,515
Speaker 5:  Like instantly, like yeah, the line went down CNBC

108
00:07:23,515 --> 00:07:27,315
Speaker 5:  like broke into coverage. Like Google stock is down because Eddie Q

109
00:07:27,315 --> 00:07:29,875
Speaker 5:  said search volume is going down because of ai. Right.

110
00:07:29,875 --> 00:07:33,835
Speaker 6:  And again, this, this is a search surface that is worth $20 billion

111
00:07:33,995 --> 00:07:37,955
Speaker 6:  a year to Google. That's like that. It is such a core part of what makes

112
00:07:37,955 --> 00:07:41,875
Speaker 6:  search work that any meaningful downtick is, is

113
00:07:41,875 --> 00:07:45,795
Speaker 6:  a huge deal for Google. Eddie Q's thesis is this has to

114
00:07:45,795 --> 00:07:49,595
Speaker 6:  do with ai. People are turning to the, the built in

115
00:07:49,615 --> 00:07:52,995
Speaker 6:  Syrian chat GPT stuff to replace it. People are using perplexity to replace

116
00:07:52,995 --> 00:07:56,355
Speaker 6:  it. People are using chat GPT search to replace it. Like

117
00:07:57,095 --> 00:08:00,675
Speaker 6:  his theory is that instead of going to Safari and typing in the search box,

118
00:08:00,815 --> 00:08:04,475
Speaker 6:  people are doing lots of different things. Google even actually I think in

119
00:08:04,475 --> 00:08:08,275
Speaker 6:  a really interesting way, confirmed that. So Google immediately

120
00:08:08,475 --> 00:08:12,315
Speaker 6:  comes out with a statement, lemme just read you the statement from

121
00:08:12,315 --> 00:08:16,075
Speaker 6:  Google. It says, we continue to see overall query growth in search. That

122
00:08:16,075 --> 00:08:19,195
Speaker 6:  includes an increase in total queries coming from Apple's devices and platforms

123
00:08:19,345 --> 00:08:21,755
Speaker 6:  more generally. As we enhance search with new features, people are seeing

124
00:08:21,755 --> 00:08:24,675
Speaker 6:  that Google search is more useful for more of their queries and they're accessing

125
00:08:24,675 --> 00:08:27,635
Speaker 6:  it for new things in, in new ways. Whether from browsers or the Google app

126
00:08:27,635 --> 00:08:31,115
Speaker 6:  using their voice or Google lens, all none of that

127
00:08:31,735 --> 00:08:35,155
Speaker 6:  is a refutation of anything that Eddie Q said. Right? Like both of these

128
00:08:35,155 --> 00:08:38,795
Speaker 6:  things can be true at the same time. But what is, what this

129
00:08:38,905 --> 00:08:42,835
Speaker 6:  implies is that a lot of people are leaving Safari and

130
00:08:42,845 --> 00:08:46,835
Speaker 6:  doing Google stuff elsewhere. And

131
00:08:47,635 --> 00:08:51,075
Speaker 6:  I certainly believe that like the Google app is hugely popular and Google

132
00:08:51,135 --> 00:08:53,835
Speaker 6:  has started pushing really, really, really, really hard to get you

133
00:08:55,905 --> 00:08:59,005
Speaker 6:  in part because it's desperately afraid it's gonna lose this trial and not

134
00:08:59,005 --> 00:09:02,925
Speaker 6:  be able to be the default in Safari anymore. And one thing that Edq said

135
00:09:02,945 --> 00:09:06,565
Speaker 6:  in the first version of this trial was that one of the things Apple was afraid

136
00:09:06,565 --> 00:09:10,325
Speaker 6:  of was that if they stopped doing this deal with Google, Google would use

137
00:09:10,345 --> 00:09:14,245
Speaker 6:  all the other surfaces that it has to get people to do

138
00:09:14,245 --> 00:09:17,485
Speaker 6:  things like download Chrome and download the Google app. They were afraid

139
00:09:17,485 --> 00:09:21,285
Speaker 6:  that like every time you open YouTube, which is like an unassailably powerful

140
00:09:21,305 --> 00:09:25,005
Speaker 6:  app on iOS, there would just be a big ass bar at the top that's like, wanna

141
00:09:25,005 --> 00:09:26,125
Speaker 6:  use Google download Chrome?

142
00:09:26,785 --> 00:09:27,485
Speaker 5:  Why use Google?

143
00:09:27,865 --> 00:09:30,485
Speaker 6:  Or like whatever. Right? And that's, that's stuff is really important. Those

144
00:09:30,485 --> 00:09:34,405
Speaker 6:  are like Google can do that kind of work and move

145
00:09:34,485 --> 00:09:38,445
Speaker 6:  a lot of people wherever it wants and it doesn't do that because of

146
00:09:38,445 --> 00:09:42,325
Speaker 6:  this deal. And so Eddie Q is sitting here being like, maybe

147
00:09:42,325 --> 00:09:45,845
Speaker 6:  this is already shifting away from me and he wants to do everything he can

148
00:09:45,845 --> 00:09:49,245
Speaker 6:  to protect this thing that is hugely lucrative

149
00:09:49,665 --> 00:09:50,685
Speaker 6:  for Apple. Yeah.

150
00:09:50,795 --> 00:09:54,725
Speaker 5:  There's a thing here that you, you mentioned it briefly at

151
00:09:54,725 --> 00:09:58,165
Speaker 5:  the top, a Apple and EQ want to protect the

152
00:09:58,165 --> 00:10:01,205
Speaker 5:  $20 billion, maybe even $25 billion

153
00:10:01,815 --> 00:10:05,005
Speaker 5:  worth of revenue that they get for doing nothing,

154
00:10:05,295 --> 00:10:06,325
Speaker 6:  Right? It's free money,

155
00:10:06,735 --> 00:10:10,125
Speaker 5:  Right? They do literally nothing except set

156
00:10:10,525 --> 00:10:14,505
Speaker 5:  Google search to be the default in Safari, which is the thing that

157
00:10:14,505 --> 00:10:17,825
Speaker 5:  everybody wants to be the default in Safari and for doing

158
00:10:18,095 --> 00:10:21,745
Speaker 5:  nothing except what everyone wants them to do for

159
00:10:22,015 --> 00:10:25,985
Speaker 5:  zero effort. They received 20 to $25 billion a year

160
00:10:25,985 --> 00:10:26,865
Speaker 5:  in revenue. And a DQ

161
00:10:26,865 --> 00:10:29,465
Speaker 6:  Even kind of alluded to that where he was like, even if they didn't pay us,

162
00:10:29,575 --> 00:10:32,465
Speaker 6:  we'd probably still have to do it because it's the thing that everybody wants.

163
00:10:32,745 --> 00:10:36,545
Speaker 5:  And so if you're, you're Apple, you're like, well, that would go away. That's

164
00:10:36,545 --> 00:10:40,105
Speaker 5:  just the freest money that's ever existed. And then the other big pot of

165
00:10:40,105 --> 00:10:44,025
Speaker 5:  free money is in-app purchases and the Apple tax and all that stuff.

166
00:10:44,405 --> 00:10:48,185
Speaker 5:  And they're under a court order, which they're appealing, which has

167
00:10:48,185 --> 00:10:51,545
Speaker 5:  allowed Spotify and Amazon and other companies to say, you can just buy your

168
00:10:51,545 --> 00:10:54,625
Speaker 5:  stuff on the web and you don't have to pay the tax. This is a big deal. Like

169
00:10:54,775 --> 00:10:58,625
Speaker 5:  Apple's looking at the free money going away. And so I, yeah, I, I,

170
00:10:58,905 --> 00:11:02,625
Speaker 5:  I see what Eddie is doing on the stand. He's, he's trying to

171
00:11:02,655 --> 00:11:06,465
Speaker 5:  make this thing seem like it's already dying so the government

172
00:11:06,465 --> 00:11:10,225
Speaker 5:  doesn't take it away. Right? And he's doing it in a framework that,

173
00:11:10,685 --> 00:11:14,385
Speaker 5:  you know, antitrust regulators like will roll their eyes and be like, yeah,

174
00:11:14,385 --> 00:11:18,265
Speaker 5:  yeah, we know the monopoly lifecycle, right? The companies get too big

175
00:11:18,265 --> 00:11:22,065
Speaker 5:  and they get too lazy. And then the disruptors come up and then the companies

176
00:11:22,065 --> 00:11:25,865
Speaker 5:  go away. And on the stand, Eddie Q was like, look, when we

177
00:11:25,865 --> 00:11:29,745
Speaker 5:  started Apple, you know, IBM was the big dog and now look at them, right?

178
00:11:29,765 --> 00:11:33,105
Speaker 5:  And he's like, maybe one day we'll go away. Just leave us alone so we can

179
00:11:33,105 --> 00:11:36,865
Speaker 5:  collect the $25 billion in free money until like, there's just a thing

180
00:11:36,895 --> 00:11:40,505
Speaker 5:  like he's, he's doing like, you know, it's like what? Like the

181
00:11:40,505 --> 00:11:43,625
Speaker 5:  archetypes and storytelling, like the hero's journey. But it's the monopoly

182
00:11:43,625 --> 00:11:47,025
Speaker 5:  lifecycle, right? Like, we're gonna learn your lesson and come back stronger.

183
00:11:47,095 --> 00:11:49,385
Speaker 5:  It's like we're all gonna die one day, leave us alone,

184
00:11:49,595 --> 00:11:52,745
Speaker 6:  Right? Just let us die in peace with our $25 billion.

185
00:11:53,155 --> 00:11:56,965
Speaker 5:  Just let us die rich man. I, I beg of you judge,

186
00:11:57,505 --> 00:12:01,365
Speaker 5:  let me die a rich man. That's all I've ever wanted. I'm on the board of

187
00:12:01,525 --> 00:12:05,125
Speaker 5:  Ferrari, Eddie Q's on the board of Ferrari. Why do you think Ferrari got

188
00:12:05,125 --> 00:12:08,165
Speaker 5:  CarPlay? You know what I mean? Like, it's just very obvious why Ferrari was

189
00:12:08,165 --> 00:12:11,765
Speaker 5:  one of the first, what's like Ferrari portion, Aston Martin got CarPlay first.

190
00:12:11,765 --> 00:12:14,125
Speaker 5:  Like, you know, where Apple executives are spending their time.

191
00:12:15,665 --> 00:12:19,315
Speaker 5:  But there's just a piece here where you're like, the other

192
00:12:19,385 --> 00:12:22,035
Speaker 5:  side of that Apple wants to preserve this money, they're gonna make this

193
00:12:22,275 --> 00:12:25,755
Speaker 5:  argument that they're already seeing the change. Like, you don't, don't worry

194
00:12:25,755 --> 00:12:28,475
Speaker 5:  about this. The market's taking care of it. The thing that's really interesting

195
00:12:28,475 --> 00:12:32,395
Speaker 5:  to me is how fast and furiously the stock

196
00:12:32,395 --> 00:12:36,055
Speaker 5:  dropped. And there's a part of me that says Google

197
00:12:36,065 --> 00:12:39,055
Speaker 5:  stock dropped. And the thing that's really, the thing that's really interesting

198
00:12:39,055 --> 00:12:42,975
Speaker 5:  to me is just how fast and furiously Google stock dropped on a

199
00:12:42,975 --> 00:12:46,615
Speaker 5:  pretty innocuous statement, right? Like what q fundamentally said

200
00:12:47,195 --> 00:12:50,855
Speaker 5:  was the market's already shifting and like all of these big

201
00:12:50,855 --> 00:12:53,255
Speaker 5:  search remedies you wanna do, they, they're not gonna matter. 'cause the

202
00:12:53,255 --> 00:12:55,375
Speaker 5:  markets already taking care of this monopoly that you're so worried about

203
00:12:55,375 --> 00:12:58,705
Speaker 5:  government. It is pretty innocuous, but the market dropped.

204
00:12:59,445 --> 00:13:03,425
Speaker 5:  But the but the stock dropped because I think everyone is waiting for that

205
00:13:03,425 --> 00:13:06,865
Speaker 5:  first piece of evidence that Google search is under threat.

206
00:13:07,165 --> 00:13:08,945
Speaker 6:  Yes, that's exactly what I was gonna say,

207
00:13:09,155 --> 00:13:13,105
Speaker 5:  Right? The first sliver of evidence that AI

208
00:13:13,105 --> 00:13:16,915
Speaker 5:  will actually take search is like a

209
00:13:16,915 --> 00:13:18,395
Speaker 5:  kind of death nail for Google. And

210
00:13:18,395 --> 00:13:20,075
Speaker 6:  Until now it really has been all vibes.

211
00:13:20,225 --> 00:13:24,195
Speaker 5:  Yeah, all vibes. Like at the launch of Bing, Sasha Nadela told me

212
00:13:24,905 --> 00:13:28,795
Speaker 5:  he's like, every point of market share I can take from Google is billions

213
00:13:28,795 --> 00:13:31,955
Speaker 5:  of dollars. Like, I don't have to win. I just to peel off like two points

214
00:13:31,955 --> 00:13:35,715
Speaker 5:  of market share. And this is a huge business for us and everyone's just been

215
00:13:35,715 --> 00:13:38,075
Speaker 5:  waiting and it's like, well, Bing didn't do it.

216
00:13:39,485 --> 00:13:43,235
Speaker 5:  Sorry guys. But like everyone was waiting for like, oh well Google

217
00:13:43,335 --> 00:13:47,075
Speaker 5:  get dethroned. And even if you believe

218
00:13:47,075 --> 00:13:50,035
Speaker 5:  Google's statement here, which I think is reason reasonable interpretation

219
00:13:50,035 --> 00:13:53,475
Speaker 5:  of this is search volumes are dropping in Safari. 'cause Google's getting

220
00:13:53,475 --> 00:13:56,875
Speaker 5:  people to use the Google search app, right? That's more or less what they're

221
00:13:56,875 --> 00:13:59,715
Speaker 5:  saying, but maybe search volumes are dropping in Sari. 'cause everyone's

222
00:13:59,715 --> 00:14:03,675
Speaker 5:  using Chacha PT instead who like, who truly knows Apple knows, but they

223
00:14:03,675 --> 00:14:04,835
Speaker 5:  said Safari. Yeah.

224
00:14:04,835 --> 00:14:08,075
Speaker 6:  I mean, and realistically I think the answer is, is pretty clearly some of

225
00:14:08,075 --> 00:14:08,315
Speaker 6:  each,

226
00:14:09,855 --> 00:14:13,715
Speaker 6:  but it It is, It is certainly true that we

227
00:14:13,715 --> 00:14:16,795
Speaker 6:  are finally slowly getting

228
00:14:17,465 --> 00:14:20,435
Speaker 6:  real information that suggests that Google is fragile, right? And I think

229
00:14:20,435 --> 00:14:24,315
Speaker 6:  part of that is also as this trial goes on, the people I talk to who are

230
00:14:24,515 --> 00:14:27,915
Speaker 6:  watching the trial seem to be like day by day becoming

231
00:14:28,265 --> 00:14:31,835
Speaker 6:  more and more convinced that something catastrophic is gonna happen to Google

232
00:14:32,575 --> 00:14:36,555
Speaker 6:  at the end of this. That I think like going in the, the, the sort of overwhelming

233
00:14:36,555 --> 00:14:40,395
Speaker 6:  betting favorite for the outcome was Google is going to

234
00:14:40,625 --> 00:14:44,555
Speaker 6:  stop having to do these deals exclusively and is just gonna win on its merits,

235
00:14:44,555 --> 00:14:47,755
Speaker 6:  right? That either it was just gonna get what it wanted, which is to just

236
00:14:47,755 --> 00:14:50,515
Speaker 6:  remove the word exclusive from all of its deals and nothing else changed.

237
00:14:51,185 --> 00:14:55,125
Speaker 6:  Or Judge Meadow is gonna be like, you can't pay money or you can't pay

238
00:14:55,125 --> 00:14:59,005
Speaker 6:  money above a certain amount to be part of these deals. And that would, that

239
00:14:59,005 --> 00:15:02,285
Speaker 6:  would have real ramifications, but it would still leave Google with a giant

240
00:15:02,285 --> 00:15:06,245
Speaker 6:  advantage because it's Google. But now more and more there is this

241
00:15:06,245 --> 00:15:10,165
Speaker 6:  sense that like, okay, something, some Google is going

242
00:15:10,165 --> 00:15:13,925
Speaker 6:  to get punched in the face in one, at least one like seriously

243
00:15:14,475 --> 00:15:18,445
Speaker 6:  lasting way. And then you pair that with, oh, and maybe

244
00:15:18,505 --> 00:15:21,925
Speaker 6:  it doesn't even matter because maybe this thing is already starting to crack

245
00:15:21,945 --> 00:15:25,605
Speaker 6:  in front of our eyes and that there's like, you just set off this incredible

246
00:15:25,615 --> 00:15:26,445
Speaker 6:  panic about Google.

247
00:15:26,875 --> 00:15:29,885
Speaker 5:  Yeah. And again, the, the market reacted to what is a pretty

248
00:15:30,515 --> 00:15:34,205
Speaker 5:  fundamentally innocuous comment. Yeah. Right. Searches

249
00:15:34,305 --> 00:15:38,165
Speaker 5:  in Safari are falling for the first time ever. Like Apple is

250
00:15:38,165 --> 00:15:42,045
Speaker 5:  obviously trying to protect the free money. So of course

251
00:15:42,115 --> 00:15:44,365
Speaker 5:  it's saying that this thing you think is a problem is not a problem. Here's

252
00:15:44,445 --> 00:15:47,125
Speaker 5:  a a tiny slice of evidence, right?

253
00:15:47,125 --> 00:15:49,445
Speaker 6:  This thing you think is good actually sucks. Don't even worry about it.

254
00:15:49,445 --> 00:15:52,685
Speaker 5:  It's fine. Yeah. This is like, whenever, whenever regulators go to big cable

255
00:15:53,005 --> 00:15:55,685
Speaker 5:  companies, they're like, but we compete with everyone. Like how can, how

256
00:15:55,685 --> 00:15:58,725
Speaker 5:  we, you can't possibly regulate us. Like there's a guy down the street with

257
00:15:58,725 --> 00:16:01,485
Speaker 5:  a tin can and a dream and he's obviously our biggest competitor. And it's

258
00:16:01,485 --> 00:16:03,365
Speaker 5:  like, what are you talking about? Like, that's this again,

259
00:16:05,145 --> 00:16:07,445
Speaker 5:  but, you know, app Eddie said some other stuff. He said, they're looking

260
00:16:07,445 --> 00:16:11,405
Speaker 5:  at adding perplexity in other search engines, AI powered search engines to

261
00:16:11,605 --> 00:16:15,125
Speaker 5:  Safari. He, he made a lot of noise about

262
00:16:15,985 --> 00:16:19,365
Speaker 5:  AI being a new user interface paradigm that might replace the iPhone entirely

263
00:16:19,465 --> 00:16:23,165
Speaker 5:  in 10 years. I, I think that was, you know, to your point David,

264
00:16:23,195 --> 00:16:27,175
Speaker 5:  like that was one of his canned lines. Like, there's

265
00:16:27,175 --> 00:16:30,615
Speaker 5:  so much change happening. Why would you bother the market? Like e 10 years

266
00:16:30,615 --> 00:16:34,055
Speaker 5:  from now, we'll all be wearing AI glasses and you won't even need an iPhone.

267
00:16:34,055 --> 00:16:37,915
Speaker 5:  Like, right. Just let, let me die rich. And you

268
00:16:37,915 --> 00:16:39,795
Speaker 5:  just look at all that, you're like, oh, there, he's trying to protect it.

269
00:16:40,095 --> 00:16:43,375
Speaker 5:  But I think inadvertently he

270
00:16:43,565 --> 00:16:47,095
Speaker 5:  validated some of people's biggest ideas about the change to come.

271
00:16:47,365 --> 00:16:51,315
Speaker 6:  Yeah. And I think It is important to remember his

272
00:16:52,135 --> 00:16:55,995
Speaker 6:  upside and his incentive for what he should believe. Like It is good for

273
00:16:55,995 --> 00:16:59,475
Speaker 6:  Apple to convince everyone for lots of reasons that AI is about to change

274
00:16:59,475 --> 00:17:02,315
Speaker 6:  everything. And that suddenly this is all hugely competitive. And actually

275
00:17:02,315 --> 00:17:06,275
Speaker 6:  maybe you should be more afraid of open AI than anybody. But the the other

276
00:17:06,275 --> 00:17:09,555
Speaker 6:  thing that I really liked is that Eddie Q said all this stuff. He's like,

277
00:17:09,555 --> 00:17:13,515
Speaker 6:  we're we're building it all. And then pressed, he said, and I quote to date,

278
00:17:13,545 --> 00:17:16,515
Speaker 6:  they're just not good enough, which is the answer to why they're not doing

279
00:17:16,515 --> 00:17:20,115
Speaker 6:  more AI stuff. And so he is like, AI is gonna change everything, but also

280
00:17:20,295 --> 00:17:24,275
Speaker 6:  AI sucks. And It is like, welcome to the discourse about ai,

281
00:17:24,275 --> 00:17:27,195
Speaker 6:  right? And it's like, I, you know what I'm tired of, and Richard, you and

282
00:17:27,195 --> 00:17:30,515
Speaker 6:  I have talked about this before, the people who were like, well, this is

283
00:17:30,515 --> 00:17:34,395
Speaker 6:  the worst it's ever gonna be, so just wait and AI is

284
00:17:34,395 --> 00:17:37,115
Speaker 6:  gonna change. Like maybe it's sucks now, but this is the worst it's ever

285
00:17:37,115 --> 00:17:40,555
Speaker 6:  gonna be. And I'm like, but that it's still so we agree. It's bad

286
00:17:41,745 --> 00:17:45,595
Speaker 7:  That it's, it's a weird thing that you can get people to say that at first.

287
00:17:45,595 --> 00:17:48,395
Speaker 7:  They'll, they'll talk only about how great It is, but then once they can't

288
00:17:48,395 --> 00:17:51,645
Speaker 7:  talk about that, suddenly it changes and it's, oh well, what it will be and

289
00:17:51,645 --> 00:17:55,565
Speaker 7:  what it could be. Right? And it's just like, what is going on? What is the

290
00:17:55,565 --> 00:17:59,445
Speaker 7:  real discussion here? What is it right now? What, what can you say? And I

291
00:17:59,445 --> 00:18:02,725
Speaker 7:  think it's notable, particularly because Apple is at least perceived as being

292
00:18:02,725 --> 00:18:06,565
Speaker 7:  so far behind in ai. So there's a lot of questions about like, what

293
00:18:06,595 --> 00:18:10,405
Speaker 7:  they can do there. And I, I feel like if they were really that

294
00:18:10,405 --> 00:18:14,085
Speaker 7:  worried about how fast AI is changing, they would, his reaction would be

295
00:18:14,085 --> 00:18:16,845
Speaker 7:  different. The things he would say on the stand would be different than what

296
00:18:16,845 --> 00:18:18,085
Speaker 7:  he actually said. Well,

297
00:18:18,085 --> 00:18:21,885
Speaker 5:  He, wait, he did say that the Google money prevented

298
00:18:21,885 --> 00:18:25,365
Speaker 5:  Apple from building its own search engine. Like he straightforward, he was

299
00:18:25,365 --> 00:18:27,765
Speaker 5:  like, yeah, that's, that's, we probably didn't do it 'cause we were just

300
00:18:27,765 --> 00:18:30,165
Speaker 5:  getting this free money. But then he was like, but it would've been hard.

301
00:18:30,185 --> 00:18:33,285
Speaker 5:  And like they're good at it. And it's like, yeah dude, you just wanted 20,

302
00:18:33,285 --> 00:18:36,405
Speaker 5:  20 billion, billion dollars. Like, you shut up my house. And you're like,

303
00:18:36,405 --> 00:18:38,725
Speaker 5:  if you would just keep eating the peanut butter you're already eating, we'll

304
00:18:38,725 --> 00:18:41,765
Speaker 5:  give you $20 billion. I'd be like, all right. Yeah. I'm super not gonna start

305
00:18:41,765 --> 00:18:44,285
Speaker 5:  a peanut butter factory at this point in time. Like, yep.

306
00:18:45,225 --> 00:18:47,805
Speaker 7:  How much money did they, did they spend on the car that they never made?

307
00:18:48,865 --> 00:18:51,185
Speaker 6:  I think 10 billion was the number we heard, wasn't it? So

308
00:18:51,375 --> 00:18:53,665
Speaker 5:  Half, half a year worth of free Google money.

309
00:18:54,025 --> 00:18:54,265
Speaker 6:  Yeah.

310
00:18:56,205 --> 00:18:56,425
Speaker 5:  You

311
00:18:56,425 --> 00:18:57,985
Speaker 7:  Know, but that, that is the world of ai.

312
00:18:58,495 --> 00:18:58,785
Speaker 6:  Yeah.

313
00:18:59,135 --> 00:19:03,105
Speaker 5:  Well the actually, the, the funny thing about the AI not being

314
00:19:03,345 --> 00:19:07,225
Speaker 5:  good enough yet, sure this is as bad as

315
00:19:07,225 --> 00:19:11,065
Speaker 5:  it will ever be. We can all fine. But like there was a study this

316
00:19:11,065 --> 00:19:14,865
Speaker 5:  week from IBM, which is essentially a consulting company

317
00:19:14,885 --> 00:19:18,225
Speaker 5:  now trying to sell AI services branded as Watson and other people's AI services.

318
00:19:18,565 --> 00:19:21,505
Speaker 5:  And in their own study about how much AI you need and you should buy from

319
00:19:21,545 --> 00:19:24,865
Speaker 5:  IBM. They're like, in our survey of CEOs, only

320
00:19:24,865 --> 00:19:28,825
Speaker 5:  25% of AI projects had returned on the investment.

321
00:19:30,165 --> 00:19:34,105
Speaker 5:  And so one out of four, and then the rest of the answer is like, and the

322
00:19:34,135 --> 00:19:36,865
Speaker 5:  CEOs were like, we're using too much stuff and it's not all well grade. And

323
00:19:36,865 --> 00:19:39,825
Speaker 5:  it's like, yeah, this kind of sucks. It's like if you just, if in

324
00:19:39,825 --> 00:19:43,585
Speaker 5:  2004 you just walked up to everyone, you're like, Bluetooth, now

325
00:19:44,515 --> 00:19:47,335
Speaker 5:  you're just gonna use Bluetooth. And everyone's like, but these are giant,

326
00:19:48,005 --> 00:19:50,895
Speaker 5:  like not all of us are real estate agents. We can't wear these headsets all

327
00:19:50,895 --> 00:19:54,295
Speaker 5:  day. You know, I, I leased a three series and I've got this headset, but

328
00:19:54,295 --> 00:19:57,775
Speaker 5:  that doesn't make me a real estate agent. You understand what I'm like, this

329
00:19:57,775 --> 00:20:01,655
Speaker 5:  isn't right yet and you gotta wait 10 years for AirPods and

330
00:20:01,655 --> 00:20:04,575
Speaker 5:  like that's still where we are. It's like, it'll be great next year.

331
00:20:04,835 --> 00:20:07,815
Speaker 6:  No, but there, but it's, it's not even like that's the best case scenario.

332
00:20:07,835 --> 00:20:11,455
Speaker 6:  The worst case scenario is like, we go and we tore a house

333
00:20:11,715 --> 00:20:14,295
Speaker 6:  and it's not as big as I want it to be, but I'm standing in the kitchen.

334
00:20:14,435 --> 00:20:17,015
Speaker 6:  I'm like, but think about it. This is the smallest this kitchen will ever

335
00:20:17,015 --> 00:20:20,975
Speaker 6:  be. Like that doesn't, that's nothing that, that

336
00:20:21,535 --> 00:20:25,335
Speaker 6:  I have, I have implied that something is going to get bigger or better and

337
00:20:25,645 --> 00:20:28,775
Speaker 6:  just, and it doesn't matter that there's no evidence that it's gonna,

338
00:20:29,515 --> 00:20:32,855
Speaker 7:  And I I think the way, the place that it's the most obvious to me is in the

339
00:20:32,955 --> 00:20:36,655
Speaker 7:  AI trends, the AI mees that develop, like we saw what the action figures

340
00:20:36,655 --> 00:20:39,945
Speaker 7:  and other things like that. You have these people who are paying for access

341
00:20:39,945 --> 00:20:43,305
Speaker 7:  to AI tools. They have no idea what to do with them. And so they're like,

342
00:20:43,585 --> 00:20:46,705
Speaker 7:  I can make an action figure of myself. Would, would that be evidence that

343
00:20:46,705 --> 00:20:50,335
Speaker 7:  I'm paying for something that's useful? Anything that will work, okay, I'll,

344
00:20:50,335 --> 00:20:54,135
Speaker 7:  I'll turn myself into a cartoon. Whatever I, I need to do something to

345
00:20:54,135 --> 00:20:55,775
Speaker 7:  prove that I this investment was worth it.

346
00:20:56,045 --> 00:20:59,175
Speaker 5:  Well I wanna make a distinction here. 'cause there's an entire class of

347
00:21:00,045 --> 00:21:03,615
Speaker 5:  professionals that are using AI in different ways. A Figma this week

348
00:21:04,375 --> 00:21:08,175
Speaker 5:  launched Figma sites and you can just make like beautiful

349
00:21:08,695 --> 00:21:12,295
Speaker 5:  designed websites with ai. It's basically a site builder. Like you just tell

350
00:21:12,295 --> 00:21:14,455
Speaker 5:  her what you want. It just like makes you a website with all kinds of animations.

351
00:21:14,455 --> 00:21:17,535
Speaker 5:  And so yes, it's, it's cool. They have an a co-generation tool that they're

352
00:21:17,535 --> 00:21:21,455
Speaker 5:  gonna launch soon. Obviously Cursor exists. We see the AI

353
00:21:21,455 --> 00:21:24,855
Speaker 5:  usage in, in apps like Photoshop, which is basically it like a hundred percent

354
00:21:25,325 --> 00:21:28,975
Speaker 5:  like usage rate. It's the same as layers, which is essentially a hundred

355
00:21:28,975 --> 00:21:32,855
Speaker 5:  percent Yes. So there's like, and we have like lots of listeners

356
00:21:32,855 --> 00:21:36,095
Speaker 5:  who are software developers who are like using this stuff all day long. There's

357
00:21:36,095 --> 00:21:39,735
Speaker 5:  some value there. I think what we are focused on in this

358
00:21:39,735 --> 00:21:43,565
Speaker 5:  conversation is the idea that AI search is

359
00:21:43,565 --> 00:21:47,325
Speaker 5:  already a useful great product that can do everything You want that ag

360
00:21:47,325 --> 00:21:51,205
Speaker 5:  agentic AI where you just tell Siri to get you a sandwich and it

361
00:21:51,465 --> 00:21:55,445
Speaker 5:  can do it, or you can tell Alexa plus to exist

362
00:21:56,065 --> 00:21:59,915
Speaker 5:  and it will be in your house. None of that stuff is real yet. Like

363
00:21:59,915 --> 00:22:03,675
Speaker 5:  that is all pipe dreams, like truly pipe dreams.

364
00:22:04,175 --> 00:22:07,955
Speaker 5:  And I think that's the gap. I I I think that gap is getting bigger

365
00:22:08,425 --> 00:22:11,955
Speaker 5:  between what people say it can do or will do or one day will do.

366
00:22:12,255 --> 00:22:15,475
Speaker 5:  And then the real things people are using it for now. And

367
00:22:15,515 --> 00:22:19,235
Speaker 5:  IIII think it's important to validate like, yes, people use cursor

368
00:22:19,325 --> 00:22:22,555
Speaker 5:  every day to great effect. Yeah. I use

369
00:22:22,685 --> 00:22:26,595
Speaker 5:  generative noise reduction in Lightroom almost every time I

370
00:22:26,595 --> 00:22:30,035
Speaker 5:  take a photo now. It, it's not even it, I think it should just be built into

371
00:22:30,035 --> 00:22:33,195
Speaker 5:  the import workflow for me. It's really good. Like I'm, I'm like, why am

372
00:22:33,195 --> 00:22:37,115
Speaker 5:  I pressing a second button? Like just put it, just do it here. There's all

373
00:22:37,115 --> 00:22:40,995
Speaker 5:  this stuff that is good and that works. There's this other

374
00:22:41,005 --> 00:22:44,875
Speaker 5:  piece. And even some of the generative AI tech stuff, like if you

375
00:22:44,875 --> 00:22:47,155
Speaker 5:  sell something in eBay and you're just like, make me an item description,

376
00:22:47,425 --> 00:22:51,355
Speaker 5:  fine, right? Like, it doesn't matter. But then there's this other

377
00:22:51,355 --> 00:22:54,395
Speaker 5:  piece where you're like, 10 years from now you won't need an iPhone

378
00:22:54,865 --> 00:22:58,155
Speaker 5:  because AI will be so good that you're just gonna be talking to your glasses.

379
00:22:58,335 --> 00:23:01,895
Speaker 5:  And it's like, well, a lot of stuff has to go. Right, right.

380
00:23:02,435 --> 00:23:06,375
Speaker 6:  My, my new shortcut for that is to just, every time you

381
00:23:06,375 --> 00:23:10,335
Speaker 6:  see ai, just auto replace it with machine learning. And, and

382
00:23:10,335 --> 00:23:12,375
Speaker 6:  in some places that makes perfect sense, right? Like a lot of the stuff you

383
00:23:12,375 --> 00:23:16,215
Speaker 6:  just described, like Figma is doing this stuff and it, it's using

384
00:23:16,215 --> 00:23:18,695
Speaker 6:  machine learning to do all kinds of interesting stuff for you. And this stuff

385
00:23:18,695 --> 00:23:21,975
Speaker 6:  is like useful and valuable and it's a feature of the software. But then

386
00:23:21,975 --> 00:23:25,455
Speaker 6:  anytime I hear somebody be like, yeah, I asked my machine learning what I

387
00:23:25,455 --> 00:23:28,695
Speaker 6:  should and I'm like, oh, right, we're out. Like

388
00:23:29,525 --> 00:23:33,375
Speaker 7:  Yeah, we used, we used to just call this algorithms and I think we all

389
00:23:33,375 --> 00:23:36,455
Speaker 7:  understood what they were and it's still really good for those things when

390
00:23:36,455 --> 00:23:38,775
Speaker 7:  they were like, oh yeah, you'll use the algorithms and it'll figure it out

391
00:23:39,075 --> 00:23:42,615
Speaker 7:  if you know how to program those AI coding tools are really useful because

392
00:23:42,835 --> 00:23:44,815
Speaker 7:  you know what they're doing and you know how to work around it, you know

393
00:23:44,815 --> 00:23:47,925
Speaker 7:  how to, to fix it and whatever it's not good at, you can fill in for me.

394
00:23:48,045 --> 00:23:50,605
Speaker 7:  I don't know how to program, if I tried to use an AI coding tool, you will

395
00:23:50,605 --> 00:23:52,885
Speaker 7:  get something that does not work and I do not know how to fix it.

396
00:23:53,225 --> 00:23:56,765
Speaker 5:  We should do Richard Lawler hackathon. We should, where we just demand that

397
00:23:56,765 --> 00:23:58,805
Speaker 5:  you vibe code different kinds of apps and see what,

398
00:23:59,305 --> 00:24:00,085
Speaker 7:  And they will, that's

399
00:24:00,665 --> 00:24:04,085
Speaker 5:  10 Richard bring down NORAD by himself. Like let's find out today.

400
00:24:05,945 --> 00:24:09,125
Speaker 6:  But just to, just to bring this back to Eddie QI think a, a thing that I'm

401
00:24:09,125 --> 00:24:12,365
Speaker 6:  just realizing now as we're talking is that Apple is actually massively

402
00:24:12,365 --> 00:24:15,965
Speaker 6:  incentivized to bring all of these things to

403
00:24:16,225 --> 00:24:20,125
Speaker 6:  iOS as fast as it possibly can. Like Apple's case four, we

404
00:24:20,125 --> 00:24:24,045
Speaker 6:  should keep getting Google's money gets stronger and easier once it

405
00:24:24,045 --> 00:24:27,485
Speaker 6:  integrates perplexity. Like it's because they can be like, look, this stuff

406
00:24:27,485 --> 00:24:30,605
Speaker 6:  is already being cannibalized. The change is already happening. So the fact

407
00:24:30,835 --> 00:24:34,325
Speaker 6:  that Apple is saying this stuff is not good enough and is actively not doing

408
00:24:34,345 --> 00:24:38,245
Speaker 6:  it, strikes me as an even more damning critique of this whole

409
00:24:38,245 --> 00:24:41,485
Speaker 6:  thing. Because it would be much easier for EQ to get up and be like, the

410
00:24:41,485 --> 00:24:45,205
Speaker 6:  Google deal doesn't matter that much because we, we already have a perplexity

411
00:24:45,205 --> 00:24:48,885
Speaker 6:  deal and people are using that too. Like it could introduce the competition

412
00:24:48,885 --> 00:24:52,845
Speaker 6:  if it wanted to and it doesn't because I think at some, somewhere in that

413
00:24:52,845 --> 00:24:56,085
Speaker 6:  company are people who continue to make like making good products. Well

414
00:24:56,085 --> 00:24:59,845
Speaker 5:  There's also a, a very interesting sort of like business flywheel

415
00:25:00,115 --> 00:25:03,805
Speaker 5:  that I don't think the AI search companies have really solved yet. So

416
00:25:03,805 --> 00:25:07,245
Speaker 5:  Google can spend $20 billion a year on Apple

417
00:25:07,825 --> 00:25:10,965
Speaker 5:  to get the default placement. Then they collect all of the searches.

418
00:25:11,805 --> 00:25:14,895
Speaker 5:  They show ads on those search pages, which are very good at making money

419
00:25:15,355 --> 00:25:17,655
Speaker 5:  to the point where like the search pages are getting more useless. 'cause

420
00:25:17,855 --> 00:25:21,215
Speaker 5:  they're just ads now. Yeah. So they collect all the data from the searches,

421
00:25:21,445 --> 00:25:25,415
Speaker 5:  they monetize each individual search at varying rates and with increasing

422
00:25:25,415 --> 00:25:26,935
Speaker 5:  amounts of like revenue aggression.

423
00:25:28,475 --> 00:25:31,175
Speaker 5:  And then they've got all the search data, which helps make the search product

424
00:25:31,175 --> 00:25:34,895
Speaker 5:  better and feeds all their other training efforts. And so they've just got

425
00:25:35,535 --> 00:25:39,015
Speaker 5:  a lot of different ways to collect value for the $20 billion they spent.

426
00:25:39,015 --> 00:25:42,735
Speaker 5:  Right? Perplexity is like, you do a search on

427
00:25:42,735 --> 00:25:46,575
Speaker 5:  perplexity, it costs them a bunch of money, right? Like there's just a

428
00:25:46,575 --> 00:25:50,375
Speaker 5:  bunch of GPUs that have to do some work and use a bunch of power and then

429
00:25:50,375 --> 00:25:54,175
Speaker 5:  they're like, they don't make any money. I'm

430
00:25:54,175 --> 00:25:55,015
Speaker 6:  Now mentioning Perplexities,

431
00:25:55,015 --> 00:25:57,415
Speaker 5:  Right? They have a subscription fee and they make some revenue, but it's

432
00:25:57,415 --> 00:26:00,015
Speaker 5:  not a lot of revenue and they're certainly not profitable. And it's like

433
00:26:00,155 --> 00:26:03,935
Speaker 5:  if they paid Apple money to collect a bunch of searches which

434
00:26:03,935 --> 00:26:07,895
Speaker 5:  cost them money, that's just a, that's just a black hole of money.

435
00:26:07,925 --> 00:26:11,375
Speaker 5:  Like it, you're just spending money to lose money. And I, I don't know that

436
00:26:11,375 --> 00:26:15,295
Speaker 5:  any of these companies can actually make that deal yet in a way that doesn't

437
00:26:15,295 --> 00:26:19,135
Speaker 5:  just accelerate their own doom loop. Right? They'll, they'll just lose

438
00:26:19,135 --> 00:26:19,415
Speaker 5:  money.

439
00:26:19,625 --> 00:26:23,015
Speaker 6:  Right. Except for open ai, which has an infinite capability to keep raising

440
00:26:23,015 --> 00:26:23,255
Speaker 6:  money.

441
00:26:23,255 --> 00:26:26,335
Speaker 5:  Yeah. Then Sam Altman just has to go tour the capitals of the world. Again,

442
00:26:26,335 --> 00:26:28,535
Speaker 5:  he was literally the United States Senate today I be like, what I need is

443
00:26:28,695 --> 00:26:32,415
Speaker 5:  infinite money for nuclear power. And you know, Ted Cruz is

444
00:26:32,575 --> 00:26:35,695
Speaker 5:  like you or tell me more. And it's like, how do you do this?

445
00:26:37,425 --> 00:26:41,015
Speaker 5:  Teach me your ways, Sam. Seriously, but I, but that's a real problem for

446
00:26:41,015 --> 00:26:43,765
Speaker 5:  Apple and I don't think it can get those deals yet from these companies.

447
00:26:43,765 --> 00:26:46,605
Speaker 6:  Certainly not in the way that it, it has from Google,

448
00:26:47,865 --> 00:26:51,325
Speaker 6:  Google capable of making those deals for

449
00:26:51,545 --> 00:26:54,325
Speaker 6:  Gemini. And we heard Sundar Phai say that that

450
00:26:55,215 --> 00:26:57,925
Speaker 6:  talks for a deal like that are underway and they were hoping to get it done

451
00:26:58,465 --> 00:27:02,245
Speaker 6:  by the middle of this year. So like that's, there's a deal there somewhere

452
00:27:02,245 --> 00:27:06,085
Speaker 6:  that makes more sense for Google just because it's Google and like you,

453
00:27:06,085 --> 00:27:09,005
Speaker 6:  you see Google on its own side working towards that, right? Like there's,

454
00:27:09,005 --> 00:27:12,725
Speaker 6:  there's AI mode in search. Now Gemini is starting to do search that it's

455
00:27:12,725 --> 00:27:16,605
Speaker 6:  like you can start to see how they just pull piece by piece out of search

456
00:27:16,625 --> 00:27:20,605
Speaker 6:  and into ai, integrate Gemini, move on with your life.

457
00:27:21,445 --> 00:27:25,375
Speaker 6:  But I think you're right that like perplexity just can't pull that off, I

458
00:27:25,375 --> 00:27:25,655
Speaker 6:  don't think.

459
00:27:25,795 --> 00:27:27,895
Speaker 5:  Not yet until they figure out how to make money doing this stuff. I don't

460
00:27:27,895 --> 00:27:31,455
Speaker 5:  think they can. Okay, so my hot prediction is 36 months from now

461
00:27:31,825 --> 00:27:35,335
Speaker 5:  these companies do not look recognizably with the companies they're today.

462
00:27:35,335 --> 00:27:37,655
Speaker 5:  What do you think Google, especially Richard,

463
00:27:37,655 --> 00:27:38,215
Speaker 6:  What do you think?

464
00:27:40,875 --> 00:27:44,745
Speaker 7:  I mean, what is left of Google? If if they, what? So

465
00:27:45,125 --> 00:27:48,145
Speaker 7:  are we thinking that they lose Chrome or that they lose Android? Or both?

466
00:27:49,315 --> 00:27:53,105
Speaker 5:  Maybe both. If they, you know, they've lost the ad tech case,

467
00:27:53,125 --> 00:27:56,785
Speaker 5:  now they're onto the remedies phase, which starts in September. So that'll

468
00:27:56,785 --> 00:28:00,425
Speaker 5:  wrap up. And for all we know, they're gonna have to sell their ad exchange

469
00:28:00,565 --> 00:28:02,945
Speaker 5:  or they're gonna have to split off DFP, like something is gonna happen.

470
00:28:02,945 --> 00:28:05,825
Speaker 6:  There was news there this week. Yeah. The the, the government once again

471
00:28:06,275 --> 00:28:09,345
Speaker 6:  going for it, like we, we talked about there are three parts of the Google

472
00:28:09,645 --> 00:28:13,345
Speaker 6:  ad tech stack. The government would like them to get rid of two of them.

473
00:28:13,695 --> 00:28:13,985
Speaker 6:  Yeah.

474
00:28:14,125 --> 00:28:17,425
Speaker 5:  And Google's counter proposal by the way, is what if we do nothing? Yeah.

475
00:28:17,425 --> 00:28:18,945
Speaker 6:  Like actually it's fine. I

476
00:28:18,945 --> 00:28:21,665
Speaker 5:  Dunno, just kidding. I can go home just lit up.

477
00:28:22,225 --> 00:28:25,385
Speaker 7:  I can't even imagine Google without the infinite money spout that is their

478
00:28:25,385 --> 00:28:26,625
Speaker 7:  ad tech sec. Like,

479
00:28:26,775 --> 00:28:29,905
Speaker 5:  This is what I mean, 36 months from now, they lose this trial. They'll find

480
00:28:29,905 --> 00:28:32,425
Speaker 5:  they'll appeal. They have already lost that trial. They're gonna go to Remedy,

481
00:28:32,425 --> 00:28:35,625
Speaker 5:  something will happen and won't be nothing, which is what Google has once

482
00:28:35,625 --> 00:28:39,425
Speaker 5:  again asked for. It'll be something they'll appeal that

483
00:28:39,995 --> 00:28:43,825
Speaker 5:  it'll go up and down the chain. Something will result. That's why I'm, that's

484
00:28:43,825 --> 00:28:46,845
Speaker 5:  why I'm giving myself three years, right? It's not the end of next year.

485
00:28:46,875 --> 00:28:48,165
Speaker 5:  It's at the end of three years,

486
00:28:49,135 --> 00:28:52,315
Speaker 6:  10 days ago I would've said even three years feels too short. But then

487
00:28:53,595 --> 00:28:57,315
Speaker 6:  watching what has happened to Apple and how quickly these

488
00:28:57,675 --> 00:29:01,635
Speaker 6:  policies have changed and how quickly the app store has changed and how hard

489
00:29:01,655 --> 00:29:05,395
Speaker 6:  it will be to wind this back. Even if Apple does appeal and win on appeal,

490
00:29:06,725 --> 00:29:10,135
Speaker 6:  like maybe three years is not that long a time. Like once, once this stuff

491
00:29:10,135 --> 00:29:11,095
Speaker 6:  shifts, it shifts.

492
00:29:11,515 --> 00:29:15,135
Speaker 5:  And again, this is part of the monopoly lifestyle like story.

493
00:29:15,725 --> 00:29:19,615
Speaker 5:  Like these things are big and they seem invincible and then tomorrow they're

494
00:29:19,615 --> 00:29:19,735
Speaker 5:  over.

495
00:29:20,545 --> 00:29:20,835
Speaker 6:  Yeah.

496
00:29:21,625 --> 00:29:25,445
Speaker 5:  And there's a lot to be said for that, but the pressure right now is from

497
00:29:25,445 --> 00:29:29,285
Speaker 5:  the government and a underrated part of the monopoly lifecycle

498
00:29:29,385 --> 00:29:32,725
Speaker 5:  is the monopoly gets himself in trouble with the government, gets all distracted

499
00:29:32,745 --> 00:29:36,525
Speaker 5:  and the then the challenger comes along and it has like free space. This

500
00:29:36,525 --> 00:29:37,805
Speaker 5:  is very much what happened to Microsoft.

501
00:29:38,035 --> 00:29:38,325
Speaker 6:  Yeah.

502
00:29:38,575 --> 00:29:41,685
Speaker 5:  Right. This is why Google and Apple exists the way they do because Microsoft

503
00:29:41,705 --> 00:29:45,645
Speaker 5:  got distracted. So you have opportunities for like OpenAI and Perplexity

504
00:29:45,645 --> 00:29:49,325
Speaker 5:  and whoever else. I'm just saying three years in three years,

505
00:29:49,525 --> 00:29:52,605
Speaker 5:  I don't think these companies look recognizably like the companies we have

506
00:29:52,605 --> 00:29:55,165
Speaker 5:  today, particularly Google. I'm not sure what's gonna happen with Apple,

507
00:29:55,185 --> 00:29:58,165
Speaker 5:  but particularly Google. It just feels to me like

508
00:29:58,995 --> 00:30:00,565
Speaker 5:  it's gonna look a lot different.

509
00:30:01,025 --> 00:30:03,565
Speaker 6:  See, I would actually put Apple at the top of the rankings as I'm thinking

510
00:30:03,565 --> 00:30:06,765
Speaker 6:  about this. Yeah. Because I think there is an overwhelming chance that

511
00:30:06,765 --> 00:30:10,725
Speaker 6:  $20 billion goes away one way or another. It's not gonna

512
00:30:10,725 --> 00:30:14,205
Speaker 6:  get out of this trial still being $20 billion. I just, I just don't, I can't

513
00:30:14,265 --> 00:30:17,925
Speaker 6:  see a way knowing what we know now that that deal

514
00:30:18,105 --> 00:30:21,885
Speaker 6:  at that number gets preserved. And then like you said, stick that

515
00:30:22,235 --> 00:30:25,645
Speaker 6:  next to all of the stuff that's happening with the app store. Like the,

516
00:30:26,185 --> 00:30:30,085
Speaker 6:  the, my Victory lap this week was I absolutely called that Amazon had a version

517
00:30:30,085 --> 00:30:33,005
Speaker 6:  of the Kindle app sitting around waiting with links in it to the web store.

518
00:30:33,065 --> 00:30:36,325
Speaker 6:  So you could buy a book in the Kindle app that's already out the

519
00:30:36,715 --> 00:30:40,645
Speaker 6:  Patreon change things and now you can subscribe to Patreons through the web.

520
00:30:40,645 --> 00:30:44,445
Speaker 6:  Like every app that has another way to bill you is starting to

521
00:30:44,445 --> 00:30:48,405
Speaker 6:  bill you quickly outside of the, and like all that is just money that

522
00:30:48,405 --> 00:30:51,165
Speaker 6:  Apple used to get for free that now it doesn't get for free. Well,

523
00:30:51,275 --> 00:30:55,165
Speaker 5:  Wait, no, hold on. So John Gruber will be on to code on Monday,

524
00:30:55,225 --> 00:30:58,765
Speaker 5:  so look forward to that. But I I, I made a similar point to him in that conversation

525
00:30:59,265 --> 00:31:03,205
Speaker 5:  and what he reminded me of was you couldn't buy a book in the

526
00:31:03,205 --> 00:31:06,865
Speaker 5:  Kindle app before. So it was not, it was money that Apple wasn't getting

527
00:31:06,965 --> 00:31:10,385
Speaker 5:  anyway. Right. And so that's fair. I guess the first players are seeing are

528
00:31:11,165 --> 00:31:14,705
Speaker 5:  Amazon, Spotify, Patreon, which

529
00:31:15,835 --> 00:31:17,165
Speaker 5:  weren't paying the taxes.

530
00:31:17,955 --> 00:31:21,885
Speaker 6:  Sure. But there's a, there's a massively long, like Candy Crush is

531
00:31:21,885 --> 00:31:25,565
Speaker 6:  coming if it hasn't yet, I don't Yeah. Pay terrible attention to Candy Crush,

532
00:31:25,665 --> 00:31:29,205
Speaker 6:  but like this is the move and now everybody has it. And

533
00:31:29,475 --> 00:31:33,045
Speaker 6:  like you said, apple has both appealed the overall ruling and

534
00:31:33,045 --> 00:31:36,805
Speaker 6:  appealed having to make this policy change ahead of the

535
00:31:36,805 --> 00:31:40,645
Speaker 6:  appeal. So it's like the, the way It is now, apple has to had to

536
00:31:40,645 --> 00:31:44,485
Speaker 6:  comply immediately and then appeal. And it's saying, no,

537
00:31:44,835 --> 00:31:47,925
Speaker 6:  this thing you're asking us to do is so horrible and unconscionable that

538
00:31:48,145 --> 00:31:51,815
Speaker 6:  we can't be asked to do it until the appeal is over again,

539
00:31:51,955 --> 00:31:55,815
Speaker 6:  not a lawyer seems like they're probably gonna lose that one based on

540
00:31:55,845 --> 00:31:58,655
Speaker 6:  what we know of how everybody feels about this case. But

541
00:31:59,915 --> 00:32:03,695
Speaker 6:  it, it's just happening so fast that at some point, like Apple

542
00:32:03,715 --> 00:32:06,655
Speaker 6:  is going to have to decide to individually pick a fight with every important

543
00:32:06,655 --> 00:32:07,615
Speaker 6:  developer on its store

544
00:32:09,155 --> 00:32:12,795
Speaker 6:  or the toothpaste is just out of the tube here and,

545
00:32:12,935 --> 00:32:16,475
Speaker 6:  and once it's out, it's out. Right? Like the people aren't gonna wind back

546
00:32:16,495 --> 00:32:18,995
Speaker 6:  to happily paying Apple 30% if they don't have to.

547
00:32:19,385 --> 00:32:22,835
Speaker 5:  Yeah. There, there's a bill, it's a, it's a kind of a fake bill

548
00:32:23,815 --> 00:32:26,555
Speaker 5:  in the way that some of these bills are just fake. Like they're opportunistic

549
00:32:26,555 --> 00:32:30,435
Speaker 5:  bills from Congress, people who just wanna like be a part of the good fight.

550
00:32:30,435 --> 00:32:33,355
Speaker 5:  But there's a bill this week in the Congress that would

551
00:32:34,025 --> 00:32:37,035
Speaker 5:  require Apple to have alternative app stores require Google to have alternative

552
00:32:37,035 --> 00:32:40,045
Speaker 5:  app stores on their platforms. And so you just see, oh, the pressure's mounting

553
00:32:40,045 --> 00:32:43,805
Speaker 5:  the political will, especially once people can see it, oh, I was able to

554
00:32:43,805 --> 00:32:47,565
Speaker 5:  buy a book in the Kindle app and then Apple took it away. It's a

555
00:32:47,565 --> 00:32:50,725
Speaker 5:  very hard to convince people of something they can't see and feel. It is

556
00:32:50,875 --> 00:32:53,285
Speaker 5:  very easy to convince people that taking something away is bad.

557
00:32:53,765 --> 00:32:56,085
Speaker 6:  Speaking of John Gruber, he, he wrote a good piece about that this week that

558
00:32:56,085 --> 00:32:59,205
Speaker 6:  you, It is one of those things that you open up the Kindle app, you tap the

559
00:32:59,205 --> 00:33:02,685
Speaker 6:  link to go get a book and immediately your brain goes, oh my God, it's insane

560
00:33:02,685 --> 00:33:06,645
Speaker 6:  that it was not like this the whole time. Like It is so unbelievably

561
00:33:06,755 --> 00:33:10,645
Speaker 6:  user hostile that you, you couldn't even, there was no way

562
00:33:10,645 --> 00:33:14,285
Speaker 6:  to even go get the book. You had to go to a different app and type the name

563
00:33:14,285 --> 00:33:18,165
Speaker 6:  of the book it like insanity. And the minute you see how it's

564
00:33:18,245 --> 00:33:22,205
Speaker 6:  supposed to work, the minute you realize this is just because Apple hates

565
00:33:22,205 --> 00:33:25,285
Speaker 6:  you. That's the only like, and and I think you're right that there, it's,

566
00:33:25,285 --> 00:33:26,365
Speaker 6:  there's no going back from that.

567
00:33:27,365 --> 00:33:31,335
Speaker 5:  Okay. So answer my question, Richard, 36 months from now, do

568
00:33:31,805 --> 00:33:34,625
Speaker 5:  you think it looks the same or do you think it looks unrecognizably different

569
00:33:34,625 --> 00:33:35,945
Speaker 5:  or some, I guess somewhere in between.

570
00:33:36,285 --> 00:33:39,865
Speaker 7:  I'm gonna take the field. I think that it still looks the same,

571
00:33:40,085 --> 00:33:43,665
Speaker 7:  but the e like the ecosystem has changed and now Apple and Google are throwing

572
00:33:43,665 --> 00:33:44,465
Speaker 7:  haymakers at each other.

573
00:33:45,675 --> 00:33:46,535
Speaker 5:  Oh, interesting.

574
00:33:46,715 --> 00:33:47,495
Speaker 6:  That's a good take.

575
00:33:48,385 --> 00:33:48,885
Speaker 5:  All right, David,

576
00:33:50,295 --> 00:33:53,475
Speaker 6:  I'm gonna, I'm gonna sort of throw in with Richard, which is, I think, I

577
00:33:53,475 --> 00:33:57,315
Speaker 6:  think these companies stay recognizable but

578
00:33:57,335 --> 00:34:01,205
Speaker 6:  are so vastly less powerful that other stuff

579
00:34:01,315 --> 00:34:05,205
Speaker 6:  bubbles up. Like I thing I'm starting to hear is everyone on Earth now is

580
00:34:05,325 --> 00:34:07,685
Speaker 6:  building a browser because they understand the browser is very important

581
00:34:07,945 --> 00:34:11,845
Speaker 6:  in new ways, in part because of this trial and ev and

582
00:34:11,845 --> 00:34:14,845
Speaker 6:  like, they also see that maybe whatever's gonna happen to Chrome is going

583
00:34:14,845 --> 00:34:17,645
Speaker 6:  to open up the browser market in a meaningful way. Again, we've been hearing

584
00:34:17,665 --> 00:34:20,925
Speaker 6:  all these AI companies are building a browser like that

585
00:34:21,545 --> 00:34:25,165
Speaker 6:  is going to march into every single one of these markets, right? Like the,

586
00:34:25,185 --> 00:34:28,805
Speaker 6:  the market for being a payment processor on the internet is more interesting

587
00:34:28,805 --> 00:34:31,365
Speaker 6:  than it's been in an extremely long time because now suddenly these things

588
00:34:31,365 --> 00:34:35,245
Speaker 6:  are open to all of those things. So like, I think it's

589
00:34:35,275 --> 00:34:39,005
Speaker 6:  less that like to some extent the big companies are gonna be like pulled

590
00:34:39,005 --> 00:34:42,485
Speaker 6:  down a little bit, but also like the tide under them is gonna come up much

591
00:34:42,485 --> 00:34:44,045
Speaker 6:  higher than it has been able to in a long time.

592
00:34:44,475 --> 00:34:47,205
Speaker 5:  Yeah. You know, there's other pieces of the search remedies case this week

593
00:34:47,205 --> 00:34:50,685
Speaker 5:  beyond Eddie Q right? The, the head of search said

594
00:34:50,855 --> 00:34:54,805
Speaker 5:  these proposed changes would totally undermine trust in Google and search

595
00:34:55,425 --> 00:34:59,405
Speaker 5:  if you let everyone have access to the Google search graph, right? And basically

596
00:35:00,165 --> 00:35:02,765
Speaker 5:  everyone can read all the searches that are happening, everyone can get access

597
00:35:02,765 --> 00:35:06,735
Speaker 5:  to our search results and repackage them. Like maybe people will just stop

598
00:35:06,735 --> 00:35:10,615
Speaker 5:  searching tru using search. And you know, the quote Google has used for years

599
00:35:10,675 --> 00:35:13,295
Speaker 5:  is people ask Google questions that are so intimate they won't even say them

600
00:35:13,295 --> 00:35:16,555
Speaker 5:  out loud. Right? And so you break that, you're gonna break search altogether.

601
00:35:16,695 --> 00:35:20,315
Speaker 6:  And what the DOJ keeps saying to that, by the way, is do you think you're

602
00:35:20,315 --> 00:35:23,875
Speaker 6:  the only company on earth that can protect user privacy? And Google

603
00:35:24,195 --> 00:35:28,115
Speaker 6:  continues to say no. Like again, on its face, that's not a

604
00:35:28,285 --> 00:35:31,475
Speaker 6:  crazy thing to say, right? Like we, we, we make a lot of fun of companies

605
00:35:31,475 --> 00:35:35,235
Speaker 6:  who don't have a better defense who just scream privacy and security that,

606
00:35:35,235 --> 00:35:38,885
Speaker 6:  that that's a shortcut for I don't actually have a good reason, but,

607
00:35:39,105 --> 00:35:41,725
Speaker 6:  but in this case, like there, there is some truth to it, right? Like if you

608
00:35:41,725 --> 00:35:45,085
Speaker 6:  think about the stuff you have put into the Google search box, you, you can,

609
00:35:45,145 --> 00:35:48,445
Speaker 6:  you can tell everything about a person from what they Google in, in ways

610
00:35:48,445 --> 00:35:51,725
Speaker 6:  that are potentially very scary. But It is also true that Google is not the

611
00:35:51,725 --> 00:35:54,765
Speaker 6:  only company on earth capable of being a good steward of that information.

612
00:35:55,005 --> 00:35:58,725
Speaker 5:  I also think It is now increasingly clear that people are

613
00:35:58,725 --> 00:36:02,005
Speaker 5:  asking wildly intimate questions to chat GPT.

614
00:36:02,505 --> 00:36:04,285
Speaker 6:  Oh yeah. Right. Even more so,

615
00:36:04,595 --> 00:36:08,205
Speaker 5:  Yeah. Even more like incredible. I mean, it's trying to get Kevin Ru to break

616
00:36:08,205 --> 00:36:10,845
Speaker 5:  up with his wife, man, and he's, he's talking about the front page of the

617
00:36:10,845 --> 00:36:11,885
Speaker 5:  New York Times. People

618
00:36:11,885 --> 00:36:15,365
Speaker 7:  Weren't just uploading their entire medical history into Google

619
00:36:15,765 --> 00:36:17,805
Speaker 7:  in the little search box. Like that wasn't happening

620
00:36:17,805 --> 00:36:18,565
Speaker 5:  Before you make this a podcast.

621
00:36:21,235 --> 00:36:24,885
Speaker 5:  Like there's, it's obvious that something else is happening, but Right. But

622
00:36:24,885 --> 00:36:28,005
Speaker 5:  that's, that would be something that really changes the dynamic of the web.

623
00:36:28,105 --> 00:36:31,765
Speaker 5:  Yes. Firefox basically said we'd be doomed if Google didn't pay us the money.

624
00:36:32,255 --> 00:36:36,205
Speaker 5:  Which our friend Casey Newton keeps pointing out like, that's your

625
00:36:36,205 --> 00:36:39,925
Speaker 5:  fault, bro. Right? Like, you've built your entire business

626
00:36:40,425 --> 00:36:43,765
Speaker 5:  is your competitor to Chrome on the back of a Google search deal. Like

627
00:36:44,645 --> 00:36:46,815
Speaker 5:  have you thought about, okay, but that Firefox has,

628
00:36:47,215 --> 00:36:51,135
Speaker 6:  Mozilla has been through like several CEOs and a decade knowing

629
00:36:51,405 --> 00:36:55,255
Speaker 6:  this is the problem and utterly failing to figure out what to do about it.

630
00:36:55,325 --> 00:36:56,895
Speaker 5:  Yeah. They're like, we're gonna do NFTs.

631
00:36:56,925 --> 00:36:59,095
Speaker 6:  It's hard to have that much sympathy at some point. Yeah,

632
00:36:59,245 --> 00:37:02,815
Speaker 5:  It's it's tough. I I'm, I'm always rooting for them, you know, like they're

633
00:37:02,815 --> 00:37:05,935
Speaker 5:  be open source champions. Yeah. But you know, they took the, the side

634
00:37:06,895 --> 00:37:10,575
Speaker 5:  quest and the NFTs and it was like, oh, you, we are totally out of ideas.

635
00:37:11,755 --> 00:37:15,015
Speaker 5:  And then right, then there's the ad tech stuff. So I just like, I look at

636
00:37:15,015 --> 00:37:18,255
Speaker 5:  this and I'm like, part I, I would re-rank them, David, I would say Google.

637
00:37:19,615 --> 00:37:23,175
Speaker 5:  Absolutely. 36 months from now, it's revenue looks different.

638
00:37:23,475 --> 00:37:26,775
Speaker 5:  It it's gonna have to build other businesses.

639
00:37:27,385 --> 00:37:30,335
Speaker 6:  Maybe it's gonna go up 'cause it doesn't have to give Apple $20 billion.

640
00:37:30,655 --> 00:37:34,535
Speaker 5:  Maybe there's that, but maybe, maybe. But like right now, and you have

641
00:37:34,535 --> 00:37:38,475
Speaker 5:  written about this many, many times, every business at Google

642
00:37:38,525 --> 00:37:41,435
Speaker 5:  looks tiny next to search. Yes. So they don't invest in them.

643
00:37:42,355 --> 00:37:44,865
Speaker 5:  Right. You're like, what? We'll move the needle at Google and you're like

644
00:37:44,905 --> 00:37:48,885
Speaker 5:  a business as big as search and everyone's like, well, we don't

645
00:37:48,885 --> 00:37:50,365
Speaker 5:  have any of those ideas. Right. It's just

646
00:37:50,365 --> 00:37:52,965
Speaker 6:  Like, right. And then eventually if you get big enough, you build a business

647
00:37:52,965 --> 00:37:56,885
Speaker 6:  that by accident runs into search and then you die. Yep. Because search will

648
00:37:56,885 --> 00:37:57,125
Speaker 6:  kill you.

649
00:37:57,585 --> 00:38:00,965
Speaker 5:  And the only things that have survived are things that like help search in

650
00:38:00,965 --> 00:38:04,365
Speaker 5:  like meaningful ways. Android survives because It is a surface for search.

651
00:38:04,545 --> 00:38:07,685
Speaker 5:  Yep. And that's why they did it YouTube, you did not want to concede that

652
00:38:07,685 --> 00:38:11,285
Speaker 5:  user base to Microsoft at that time. Right. Okay. But it means that

653
00:38:11,435 --> 00:38:14,125
Speaker 5:  like how many times have we talked about the pixel and are they gonna truly

654
00:38:14,125 --> 00:38:17,725
Speaker 5:  invest in the Pixel and like the pixel is there to keep Samsung honest and

655
00:38:17,725 --> 00:38:20,975
Speaker 5:  that is about it. Yeah. It's, it's not there to be a big winner of a product

656
00:38:21,085 --> 00:38:24,095
Speaker 5:  because they don't care as long as people are searching Google on Samsung

657
00:38:24,335 --> 00:38:28,215
Speaker 5:  phones. Right. Well, maybe if the search business starts to

658
00:38:28,395 --> 00:38:32,275
Speaker 5:  wither, like those priorities shift and Google has to invest

659
00:38:32,275 --> 00:38:35,205
Speaker 5:  in its other businesses and grow that maybe be more diversified. This is

660
00:38:35,205 --> 00:38:38,685
Speaker 5:  what happened to Microsoft by the way. Windows came under significant pressure.

661
00:38:39,205 --> 00:38:41,565
Speaker 5:  Sachi Nadel was like, all right, we're just gonna do Azure until I figure

662
00:38:41,565 --> 00:38:44,485
Speaker 5:  something else out. And then he started rebuilding each of those businesses

663
00:38:44,485 --> 00:38:47,365
Speaker 5:  in turn and he pulled it off. He's very, he was very good at what he did.

664
00:38:47,365 --> 00:38:50,525
Speaker 5:  He pulled, he pulled it off at Microsoft. Vastly more boring now than it

665
00:38:50,525 --> 00:38:54,335
Speaker 5:  was before. Sure. But like, he pulled it off. I

666
00:38:54,775 --> 00:38:57,815
Speaker 5:  I think Google is gonna undergo that kind of transformation. I think Apple

667
00:38:57,815 --> 00:39:01,655
Speaker 5:  looks different because the sense that something is gonna come

668
00:39:01,655 --> 00:39:03,815
Speaker 5:  after the iPhone will continue to panic that company.

669
00:39:04,635 --> 00:39:04,855
Speaker 6:  Yes.

670
00:39:05,525 --> 00:39:09,195
Speaker 5:  Right now they're saying it out loud. Something is gonna come after the iPhone

671
00:39:09,575 --> 00:39:12,635
Speaker 5:  and they think the user interface will probably be voice in ai.

672
00:39:13,625 --> 00:39:16,995
Speaker 5:  Well, that means we're just in, we're in for it. Right. Right. That is a,

673
00:39:16,995 --> 00:39:19,995
Speaker 5:  that is a company in a defensive posture, not an offensive posture. And I,

674
00:39:19,995 --> 00:39:22,715
Speaker 6:  Well, and a company that is for whatever reason,

675
00:39:23,485 --> 00:39:27,405
Speaker 6:  completely unable to compete in a meaningful way on those

676
00:39:27,405 --> 00:39:31,325
Speaker 6:  things. Like if you're, if you're sitting around being like, okay, if, if

677
00:39:31,325 --> 00:39:35,295
Speaker 6:  we agree that AI is the thing, who's gonna win apple's? Not

678
00:39:35,295 --> 00:39:39,135
Speaker 6:  in the first five companies, you would name It is the biggest company on

679
00:39:39,135 --> 00:39:42,895
Speaker 6:  earth. And it wouldn't, you wouldn't bet on Apple as a, as a powerhouse

680
00:39:42,915 --> 00:39:46,415
Speaker 6:  to solve ai. As AI gets really good, which is where I think Google has a

681
00:39:46,415 --> 00:39:49,095
Speaker 6:  real shot. Right. And this is another thing that keeps coming up in the remedies

682
00:39:49,095 --> 00:39:52,975
Speaker 6:  cases. Google is out there trying to do, again in AI what it

683
00:39:52,975 --> 00:39:56,335
Speaker 6:  already did in search based on the premise that Google

684
00:39:56,795 --> 00:40:00,575
Speaker 6:  has the resources to be a winner in ai. And I think that's true. I don't

685
00:40:00,575 --> 00:40:03,815
Speaker 6:  think it'll be the only winner. I think there are going to be lots of winners.

686
00:40:03,815 --> 00:40:07,615
Speaker 6:  As we saw Sam Waltman open AI acknowledged this week. They went, they like

687
00:40:07,635 --> 00:40:11,255
Speaker 6:  bailed on the whole for-profit idea and are just back to being like, nevermind,

688
00:40:11,395 --> 00:40:15,095
Speaker 6:  we thought we were gonna win so spectacularly, but now we're not. And just

689
00:40:15,095 --> 00:40:15,335
Speaker 6:  kidding.

690
00:40:16,955 --> 00:40:19,975
Speaker 6:  But you look at all of that and it's like, okay, Google has a real shot at

691
00:40:19,975 --> 00:40:23,135
Speaker 6:  this, but Apple is gonna have to go undergo like a complete

692
00:40:23,765 --> 00:40:27,655
Speaker 6:  culture and organizational shift in order to

693
00:40:27,655 --> 00:40:31,215
Speaker 6:  be the kind of AI company that It is going to need to be if AI is going to

694
00:40:31,215 --> 00:40:31,375
Speaker 6:  win.

695
00:40:31,725 --> 00:40:35,415
Speaker 5:  Yeah. I feel like I'm obligated by law right now to tell everyone

696
00:40:36,465 --> 00:40:40,235
Speaker 5:  that the t in chat, BT stands for transformer, which is invented at Google

697
00:40:40,295 --> 00:40:43,475
Speaker 5:  if you don't mention this every so often, a Google PR person like leaps out

698
00:40:43,475 --> 00:40:47,315
Speaker 5:  of a bush. That's true. And brandishes that fact at you, right? They,

699
00:40:47,315 --> 00:40:49,995
Speaker 5:  they invented a bunch of the core technology and they know how to deploy

700
00:40:49,995 --> 00:40:53,635
Speaker 5:  it. Sundar has said many times, like, they know how to make it cheaper and

701
00:40:53,665 --> 00:40:57,015
Speaker 5:  more energy efficient. He's like, that's what we're good at. So like

702
00:40:57,475 --> 00:41:01,255
Speaker 5:  you can, you can believe that Google will just do its core competency. They've

703
00:41:01,255 --> 00:41:05,105
Speaker 5:  also, to their credit, done the hard restructure. Yeah.

704
00:41:05,105 --> 00:41:08,305
Speaker 5:  Right. They, they got rid of Google Brain, they exited some

705
00:41:08,515 --> 00:41:11,545
Speaker 5:  executives, they put Demis in charge of everything.

706
00:41:12,495 --> 00:41:15,305
Speaker 5:  Okay. You can, you can see that they've undergone some changes to make the

707
00:41:15,305 --> 00:41:19,115
Speaker 5:  move. But now the, the big thing is

708
00:41:19,115 --> 00:41:23,035
Speaker 5:  under the pressure, and I don't know, apple hasn't undergone

709
00:41:23,095 --> 00:41:26,515
Speaker 5:  any changes, right? They're like, this guy's gone, but here's another executive

710
00:41:26,515 --> 00:41:28,275
Speaker 5:  who's been here for 40 years. You're in charge now. Right?

711
00:41:28,615 --> 00:41:31,755
Speaker 6:  And for a company like Eddie DQ gets on stage and or gets on stage. Eddie

712
00:41:31,775 --> 00:41:34,605
Speaker 6:  DQ gets on the stand and is talking about, like,

713
00:41:34,865 --> 00:41:37,725
Speaker 5:  It really is a few good men. Like he just starts vamping, you know, like

714
00:41:37,725 --> 00:41:41,645
Speaker 5:  Jack Nicholson, like just improving the lines in poor Tom Cruise,

715
00:41:41,875 --> 00:41:43,965
Speaker 5:  just sitting there smiling, be like, I don't know what's going on.

716
00:41:44,975 --> 00:41:48,405
Speaker 6:  Eddie, Eddie Q would really like that comparison. But Eddie Q gets on the

717
00:41:48,405 --> 00:41:52,325
Speaker 6:  stand and talks about Apple's thing being cannibalizing its projects

718
00:41:52,385 --> 00:41:56,325
Speaker 6:  before they're dead. Right? Like the iPhone exists because they cannibalize

719
00:41:56,325 --> 00:41:59,325
Speaker 6:  the I or the iPod and everybody told them they were nuts to do so, but then

720
00:41:59,325 --> 00:42:03,285
Speaker 6:  they got the iPhone. Apple's not doing that now. Like, there is none

721
00:42:03,285 --> 00:42:06,845
Speaker 6:  of that left in what we see from Apple anymore. And it, it,

722
00:42:07,235 --> 00:42:11,005
Speaker 6:  that posture just does not exist. It is kicking and screaming at

723
00:42:11,005 --> 00:42:11,925
Speaker 6:  every one of these changes.

724
00:42:12,465 --> 00:42:15,565
Speaker 5:  By the way, I just searched it on Google. I use Google for what it's intended

725
00:42:15,585 --> 00:42:18,685
Speaker 5:  for. And It is in fact true that Jack Nicholson improvises the line. You

726
00:42:18,685 --> 00:42:19,965
Speaker 5:  can't handle the truth in, if you go, Ben,

727
00:42:20,065 --> 00:42:20,645
Speaker 6:  That's cool.

728
00:42:21,385 --> 00:42:23,925
Speaker 5:  And, and Tom Cruise just had to, he just had to be like,

729
00:42:25,455 --> 00:42:29,345
Speaker 5:  okay, nice. Let's keep this baby going. And I'm

730
00:42:29,345 --> 00:42:33,165
Speaker 5:  Eddie, I dunno if you're listening, if you end up on the stand again

731
00:42:33,185 --> 00:42:36,845
Speaker 5:  in this case, I encourage you to scream at the United States

732
00:42:36,845 --> 00:42:38,685
Speaker 5:  Department of Justice. You can't handle the truth.

733
00:42:40,445 --> 00:42:40,965
Speaker 6:  Absolutely, yes.

734
00:42:41,265 --> 00:42:45,125
Speaker 5:  All right. We gotta take a break. Let's all breathe. You, you write

735
00:42:45,125 --> 00:42:48,765
Speaker 5:  us, tell us what you think is gonna happen 36 months from now. Do these companies

736
00:42:48,765 --> 00:42:52,405
Speaker 5:  look the same or different? I'm, I'm, you know, you know what I think, but

737
00:42:52,405 --> 00:42:53,165
Speaker 5:  I'm curious what you all think.

738
00:42:53,425 --> 00:42:57,045
Speaker 6:  May, 2028 predict what the world's gonna look like. Good luck everybody.

739
00:42:59,285 --> 00:43:01,685
Speaker 5:  We're gonna come back and we're gonna spend a little more time in a courtroom

740
00:43:01,685 --> 00:43:02,685
Speaker 5:  with Meta. We'll be right back.

741
00:46:31,745 --> 00:46:35,725
Speaker 5:  All right, we're back. Look, I wanna leave the policy

742
00:46:35,755 --> 00:46:39,005
Speaker 5:  talk and the legal talk by always doing a pallet conser with gadgets. And

743
00:46:39,005 --> 00:46:42,485
Speaker 5:  we're gonna do that, I promise we're gonna do that. But we can't leave court

744
00:46:42,515 --> 00:46:46,125
Speaker 5:  just yet because Richard Adam ary,

745
00:46:47,265 --> 00:46:51,135
Speaker 5:  he was on fire today just, just chopping it up

746
00:46:51,195 --> 00:46:53,295
Speaker 5:  in our nation's capitol. What, what, what happened in this case?

747
00:46:53,795 --> 00:46:56,775
Speaker 7:  He had a lot to say when he came on the stand today. We've heard a lot about

748
00:46:56,775 --> 00:47:00,255
Speaker 7:  Instagram. We've heard about reels, we've heard about threads. I mean,

749
00:47:00,935 --> 00:47:03,175
Speaker 7:  I don't know, I don't know if he's explained why they haven't launched an

750
00:47:03,175 --> 00:47:05,815
Speaker 7:  iPad app yet, but I'm sure he'll get to it at some point. You know, we're,

751
00:47:05,815 --> 00:47:07,975
Speaker 7:  we're still recording this as, as he's talking, he

752
00:47:07,975 --> 00:47:10,535
Speaker 5:  Said they're going to, like a couple weeks ago he said Instagram's gonna

753
00:47:10,535 --> 00:47:13,015
Speaker 5:  do an iPad app. Yeah, I don't, I don't believe it until he says it under

754
00:47:13,015 --> 00:47:13,215
Speaker 5:  oath.

755
00:47:13,815 --> 00:47:16,975
Speaker 7:  Probably because he knew this testimony was coming and how he would be today.

756
00:47:17,445 --> 00:47:20,335
Speaker 7:  That is what it was. 'cause I've been wondering why would, why now, why,

757
00:47:20,335 --> 00:47:23,815
Speaker 7:  why is that happening? It's this because he talked about everything. He talked

758
00:47:23,815 --> 00:47:27,015
Speaker 7:  about Facebook deciding to buy Instagram in 2012, which he called one of

759
00:47:27,015 --> 00:47:30,765
Speaker 7:  the best acquisitions of all time. He said that Threads was originally supposed

760
00:47:30,765 --> 00:47:34,445
Speaker 7:  to live within the Instagram app, which is kind of weird to think about.

761
00:47:34,545 --> 00:47:37,525
Speaker 7:  But the way that they're linked, I can certainly see how they would've had

762
00:47:37,525 --> 00:47:41,245
Speaker 7:  that idea. And also when you look at, for example, how YouTube

763
00:47:41,305 --> 00:47:44,805
Speaker 7:  has done the posts in YouTube that these apps don't necessarily think about

764
00:47:45,405 --> 00:47:48,285
Speaker 7:  bringing things out. But, but then they kind of realized that if you're gonna

765
00:47:48,285 --> 00:47:51,645
Speaker 7:  take on Twitter, you need to have a dedicated app. So, so that makes a lot

766
00:47:51,645 --> 00:47:55,485
Speaker 7:  of sense. I think also he, he said that one of the biggest mistakes he made

767
00:47:55,485 --> 00:47:59,165
Speaker 7:  was the first version of reels that, that to me was just,

768
00:48:00,615 --> 00:48:03,355
Speaker 7:  I'm not sure if the current version of reels is not still one of the big,

769
00:48:03,355 --> 00:48:05,875
Speaker 7:  biggest mistakes. Yeah. But I did think it was interesting that he said

770
00:48:05,895 --> 00:48:06,675
Speaker 6:  All the subsequent ones

771
00:48:06,705 --> 00:48:10,035
Speaker 5:  Also. Well, the first version of Reels was built on stories, right? Yeah.

772
00:48:10,055 --> 00:48:12,475
Speaker 5:  And so he was like, you're gonna open a story and you're gonna swipe through

773
00:48:12,475 --> 00:48:16,275
Speaker 5:  stories the way that you're swiping through reels now. And he is like, no,

774
00:48:16,275 --> 00:48:19,635
Speaker 5:  that's not what people wanna do. They just want TikTok. So they copied TikTok,

775
00:48:19,785 --> 00:48:23,515
Speaker 7:  Copy Snapchat, copy TikTok, move on. And that is the metal way.

776
00:48:23,655 --> 00:48:27,635
Speaker 7:  As, as we've seen, they said that they, Instagram has spent up to $700 million

777
00:48:27,755 --> 00:48:30,755
Speaker 7:  a year to lure creators. So they're bringing in a lot of money. They're spending

778
00:48:30,795 --> 00:48:31,875
Speaker 7:  a lot of money on Instagram.

779
00:48:33,385 --> 00:48:36,355
Speaker 7:  Just, just so, so many different things. I I, I was

780
00:48:37,435 --> 00:48:40,445
Speaker 7:  surprised that he said so much. But also the way that er has been lately

781
00:48:41,545 --> 00:48:44,885
Speaker 7:  as It is open, as he is been as frequently posting, is constantly talking

782
00:48:45,265 --> 00:48:48,685
Speaker 7:  to the people about algorithms and Instagram and, and what they think. I'm

783
00:48:48,745 --> 00:48:52,685
Speaker 7:  not really that surprised to see that, that he just has so much to say.

784
00:48:52,705 --> 00:48:52,925
Speaker 7:  Now,

785
00:48:53,305 --> 00:48:57,005
Speaker 5:  You know, I've briefly talked to Adam about his

786
00:48:57,055 --> 00:48:59,965
Speaker 5:  posts and like what he does, and he, he always, he, he makes two points in

787
00:48:59,965 --> 00:49:03,565
Speaker 5:  general. He, I think he's even said this publicly, he's like, one,

788
00:49:03,985 --> 00:49:07,885
Speaker 5:  my audience for this is so small. Like, I just want creators. Like they have

789
00:49:07,885 --> 00:49:10,685
Speaker 5:  a lot of questions they should hear from me. And then his point is, I want

790
00:49:10,685 --> 00:49:13,935
Speaker 5:  to use the tools, right? Like he, he's like, I wanna feel what it's like

791
00:49:13,935 --> 00:49:16,895
Speaker 5:  on that side of the screen, not just as a consumer Instagram. So I, I always

792
00:49:16,895 --> 00:49:20,295
Speaker 5:  think it's like people overread why he's doing it. Like

793
00:49:20,355 --> 00:49:24,335
Speaker 5:  90% of Myer's Instagram posts are like, no, we're not listening

794
00:49:24,335 --> 00:49:28,095
Speaker 5:  to you. You know, very basic stuff. Like the algorithm is not punishing you

795
00:49:28,095 --> 00:49:31,535
Speaker 5:  if you use the letter Q in your caption. Like it's just him

796
00:49:31,565 --> 00:49:34,975
Speaker 5:  debunking conspiracy theories and then almost all of them boil down to, and

797
00:49:34,975 --> 00:49:38,575
Speaker 5:  you should post more, like the most basic stuff. But I, the part where he

798
00:49:38,575 --> 00:49:42,215
Speaker 5:  is like, I wanna use the tools. I think It is really interesting, but he

799
00:49:42,215 --> 00:49:46,045
Speaker 5:  doesn't get off. You can't knock. He's as good as it gets, right? You

800
00:49:46,045 --> 00:49:49,885
Speaker 5:  can't knock him off his game. And it felt to me, I mean, this is

801
00:49:49,885 --> 00:49:53,845
Speaker 5:  the case where the government is trying to prove that Meta bought

802
00:49:53,845 --> 00:49:57,645
Speaker 5:  Instagram to foreclose a competitor. And Adam's point was

803
00:49:57,705 --> 00:50:01,525
Speaker 5:  to push Meta's argument that Instagram would've never been

804
00:50:01,525 --> 00:50:05,415
Speaker 5:  successful without Meta. Right. That all by itself, it would've

805
00:50:05,415 --> 00:50:07,415
Speaker 5:  never developed the way it was. That Meta really turbocharged it.

806
00:50:08,975 --> 00:50:12,895
Speaker 5:  I think he made some of those points. And then I

807
00:50:12,895 --> 00:50:15,695
Speaker 5:  think, you know, as all these go, like I think he oversold the idea that

808
00:50:15,695 --> 00:50:18,735
Speaker 5:  they're under existential threat from small competitors all the time. David,

809
00:50:18,735 --> 00:50:19,695
Speaker 5:  what do you think? I,

810
00:50:19,775 --> 00:50:23,175
Speaker 6:  I think that's right. He went way out of his way to talk about how

811
00:50:23,835 --> 00:50:27,645
Speaker 6:  big a competitor TikTok is, which is, we, we've talked about this before,

812
00:50:27,665 --> 00:50:31,365
Speaker 6:  but this has become meta's sort of hobby horse in this trial is

813
00:50:32,225 --> 00:50:36,125
Speaker 6:  the very existence of TikTok suggests Meta cannot possibly be a

814
00:50:36,245 --> 00:50:39,525
Speaker 6:  monopoly, which in a certain way is actually a very compelling argument.

815
00:50:40,385 --> 00:50:44,325
Speaker 6:  But what it has done and did, again to Miss Air is just make a

816
00:50:44,325 --> 00:50:47,845
Speaker 6:  bunch of Instagram executives admit that they're not doing a very good job,

817
00:50:48,685 --> 00:50:52,205
Speaker 6:  which I really enjoy. And so like over and over and over again, they just

818
00:50:52,205 --> 00:50:55,925
Speaker 6:  keep apologizing for all of the ways that they're losing to TikTok. And I

819
00:50:55,925 --> 00:50:59,765
Speaker 6:  just find, I find that to be so very funny. And, and

820
00:50:59,765 --> 00:51:02,845
Speaker 6:  again, like the all this money, like you're talking about Richard, the $700 million

821
00:51:02,865 --> 00:51:04,285
Speaker 6:  to lure creators, like

822
00:51:06,265 --> 00:51:10,245
Speaker 6:  how, how, where is that money going? Like YouTube

823
00:51:10,395 --> 00:51:14,205
Speaker 6:  just pays ad revenue. There are creator funds

824
00:51:14,225 --> 00:51:17,205
Speaker 6:  and there are this stuff, but like Instagram is spending a huge amount of

825
00:51:17,205 --> 00:51:21,165
Speaker 6:  money in these sort of unknowable ways trying to get creators

826
00:51:21,425 --> 00:51:25,045
Speaker 6:  off of those other platforms and onto Instagram. I find that really fascinating.

827
00:51:25,105 --> 00:51:25,325
Speaker 6:  But

828
00:51:27,635 --> 00:51:31,405
Speaker 6:  even just listening to Myer talk about it, like I, I find

829
00:51:31,405 --> 00:51:35,285
Speaker 6:  myself wondering a lot if Instagram knows what Instagram

830
00:51:35,585 --> 00:51:39,445
Speaker 6:  is, and this kind of goes back neli to your point about wanting

831
00:51:39,445 --> 00:51:42,885
Speaker 6:  to use the tools. Like Adam Terry also made a video

832
00:51:43,405 --> 00:51:47,085
Speaker 6:  I think today about, oh no, sorry, this was

833
00:51:47,085 --> 00:51:50,685
Speaker 6:  months ago now that, but it came resurfaced today about how

834
00:51:50,945 --> 00:51:54,605
Speaker 6:  he wanted Instagram to not be a lean back experience.

835
00:51:54,985 --> 00:51:58,885
Speaker 6:  And it's like this thing is supposed to be an interactive and social and

836
00:51:58,885 --> 00:52:02,005
Speaker 6:  like a place you hang out. And that was like, one of their big things with

837
00:52:02,005 --> 00:52:04,725
Speaker 6:  threads was the reason they pulled it out into its own app was because they

838
00:52:04,725 --> 00:52:08,085
Speaker 6:  wanted replies and posts to be on the same level with each other. And so,

839
00:52:08,085 --> 00:52:11,285
Speaker 6:  like they have this idea that it's a social network and like, I don't know

840
00:52:11,285 --> 00:52:13,485
Speaker 6:  anyone who experiences Instagram that way.

841
00:52:13,485 --> 00:52:16,405
Speaker 5:  Yeah. I mean that's why they're spending the money in creators, right? Right.

842
00:52:16,995 --> 00:52:20,405
Speaker 5:  Instagram sees creators as a good source of content after many rank and file

843
00:52:20,405 --> 00:52:23,325
Speaker 5:  users began posting fewer of their own updates. That's why they spend the

844
00:52:23,325 --> 00:52:25,605
Speaker 5:  money. That's from the, that's from the testimony today.

845
00:52:25,875 --> 00:52:29,525
Speaker 6:  Yeah. And just, just come out and be like, Hey, we're a, we're a streaming

846
00:52:29,525 --> 00:52:33,045
Speaker 6:  service. Like, it's fine. It's, I I just, I don't know. We, I, we've gotten

847
00:52:33,045 --> 00:52:36,845
Speaker 6:  to this point now where I think Meta as a company, it

848
00:52:37,285 --> 00:52:40,565
Speaker 6:  continues to be obsessed with this idea that like, we can get back to friends,

849
00:52:40,565 --> 00:52:44,485
Speaker 6:  sharing with friends. And it hasn't been that in a very, very, very

850
00:52:44,485 --> 00:52:44,965
Speaker 6:  long time.

851
00:52:45,325 --> 00:52:49,045
Speaker 5:  I, I think that's what they say. I think internally

852
00:52:49,075 --> 00:52:53,005
Speaker 5:  they all know this is why they spend $700 million a year on me. I fair

853
00:52:53,685 --> 00:52:56,685
Speaker 5:  creators from TikTok. Like, this is why they're fine with spaghetti Jesus

854
00:52:56,745 --> 00:53:00,685
Speaker 5:  in the feeds. It just doesn't matter to them. Right. And I, I, I

855
00:53:00,685 --> 00:53:04,445
Speaker 5:  think, you know, mark posting on Facebook or Adam posting on Instagram, like

856
00:53:04,555 --> 00:53:07,165
Speaker 5:  they're using the tools. I actually really appreciate they use the tools.

857
00:53:07,485 --> 00:53:10,845
Speaker 5:  I think not enough executives use the tools except for Tim Cook claiming,

858
00:53:10,925 --> 00:53:14,245
Speaker 5:  I believe the GQ that he uses every single Apple product every day,

859
00:53:14,615 --> 00:53:17,285
Speaker 5:  which I don't, there are not enough minutes in the day. But

860
00:53:17,545 --> 00:53:20,765
Speaker 6:  Do you remember all those, all the times during like the, the Twitter years

861
00:53:20,765 --> 00:53:24,005
Speaker 6:  where people would keep track of how many tweets the various like executives

862
00:53:24,005 --> 00:53:27,325
Speaker 6:  and board members had? Yeah, I think, I think like, I think you're right.

863
00:53:27,405 --> 00:53:31,045
Speaker 6:  I think it, it's, it's useful and correct to see

864
00:53:31,455 --> 00:53:32,645
Speaker 6:  executives be part of this.

865
00:53:33,125 --> 00:53:36,485
Speaker 5:  I think Mark and Adam are warped because they have, it doesn't matter,

866
00:53:36,985 --> 00:53:39,495
Speaker 5:  right? There's just like millions of people will just respond to everything

867
00:53:39,495 --> 00:53:43,295
Speaker 5:  they do. I Adam want really wanted to experience Instagram. He would start

868
00:53:43,455 --> 00:53:47,135
Speaker 5:  a meme page from scratch po as a college student and try to go viral

869
00:53:47,135 --> 00:53:50,495
Speaker 5:  every day because that is actually the experience the creators of Instagram

870
00:53:50,495 --> 00:53:54,415
Speaker 5:  are having, right? This like desperate need for algorithmic validation,

871
00:53:55,265 --> 00:53:58,655
Speaker 5:  right? If Adam really wanted to experience Instagram, he would

872
00:53:59,175 --> 00:54:03,095
Speaker 5:  download a Netflix video, downloader speed everything up 15%, add

873
00:54:03,095 --> 00:54:06,975
Speaker 5:  that weird vertical line and begin posting copies clips of a few Good men

874
00:54:07,155 --> 00:54:08,255
Speaker 5:  to Instagram every day.

875
00:54:08,955 --> 00:54:10,015
Speaker 6:  You just broke Richard A. Little.

876
00:54:10,255 --> 00:54:13,935
Speaker 5:  Do you see what I'm saying? That's the real experience of Instagram, right?

877
00:54:14,085 --> 00:54:17,855
Speaker 5:  Free booting memes from X and being like, here's the difference between

878
00:54:17,875 --> 00:54:18,655
Speaker 5:  men and women. Like

879
00:54:20,935 --> 00:54:24,595
Speaker 5:  that's the experience of Instagram and that's what

880
00:54:24,705 --> 00:54:28,595
Speaker 5:  many, many creators experience like being an Instagram creator

881
00:54:28,615 --> 00:54:32,035
Speaker 5:  is choosing between the dark path of

882
00:54:32,505 --> 00:54:36,115
Speaker 5:  just free booting memes or doing those ai baby podcasts for like Theo v's

883
00:54:36,115 --> 00:54:39,155
Speaker 5:  a baby that's all over the place, right? And it's outta control like every

884
00:54:39,155 --> 00:54:41,755
Speaker 5:  other clip of my feed. And it's just like, can I just say I love this force

885
00:54:41,775 --> 00:54:42,275
Speaker 5:  fed to me,

886
00:54:42,755 --> 00:54:46,555
Speaker 6:  I am like I am, I am sort of constitutionally against AI generated

887
00:54:46,555 --> 00:54:49,955
Speaker 6:  content in my feed and baby podcasts are very much for me.

888
00:54:50,115 --> 00:54:53,875
Speaker 5:  That's very funny. Please don't do it to us or do it to us and don't share

889
00:54:53,875 --> 00:54:57,155
Speaker 5:  with anyone but us. You see what I'm saying? That's like one path, which

890
00:54:57,155 --> 00:55:01,075
Speaker 5:  is the dark, dark side. There's the slightly less dark path

891
00:55:01,415 --> 00:55:05,395
Speaker 5:  of like be a Gary Vayner, Chuck Hussle bro, where he

892
00:55:05,395 --> 00:55:08,835
Speaker 5:  just screams at you about your content not being good enough. And the algorithm

893
00:55:08,895 --> 00:55:12,655
Speaker 5:  is just people and just make great shit. And like, that's the path I've chosen

894
00:55:12,655 --> 00:55:15,535
Speaker 5:  personally. And then there's like everybody else being like, I just want

895
00:55:15,535 --> 00:55:19,285
Speaker 5:  more views. And it's like, well, you're not, you're not hustling hard enough.

896
00:55:19,295 --> 00:55:23,165
Speaker 5:  Right? And none of that is, I made a nice video update

897
00:55:23,165 --> 00:55:25,525
Speaker 5:  about changes to the algorithm. We got a million views just 'cause I'm Madary,

898
00:55:26,925 --> 00:55:29,965
Speaker 5:  Right? Like you're, you're you're still having a synthetic experience on

899
00:55:29,965 --> 00:55:33,395
Speaker 5:  their own platforms. And I, I look at all this and it's like

900
00:55:33,895 --> 00:55:37,875
Speaker 5:  the, the thesis of this case is they

901
00:55:37,895 --> 00:55:41,355
Speaker 5:  saw a competitor and they bought it because they were afraid,

902
00:55:42,455 --> 00:55:45,245
Speaker 5:  right? And you can argue at the markets and me we and all this stuff, but

903
00:55:45,245 --> 00:55:48,765
Speaker 5:  like that's the real case is look at how bad these emails were.

904
00:55:49,455 --> 00:55:53,345
Speaker 5:  They saw a competitor and they wanted to crush it. And I, I don't know if

905
00:55:53,345 --> 00:55:57,265
Speaker 5:  they realize that what they're doing right now is making

906
00:55:57,285 --> 00:56:00,665
Speaker 5:  the case very strongly that if they could have bought TikTok and crushed

907
00:56:00,665 --> 00:56:04,625
Speaker 5:  it, they would've, 'cause they obviously saw this competitor and they

908
00:56:04,625 --> 00:56:08,545
Speaker 5:  weren't allowed to buy it. So they had to compete and like, look at

909
00:56:08,545 --> 00:56:12,385
Speaker 5:  all this evidence of all this stuff they're doing to compete with TikTok

910
00:56:12,695 --> 00:56:15,585
Speaker 5:  that they didn't have to do 'cause they bought Instagram.

911
00:56:16,515 --> 00:56:20,455
Speaker 5:  And like, would, wouldn't that have been better? Right? And I, I

912
00:56:20,455 --> 00:56:24,285
Speaker 5:  don't know if that has been tied together correctly. I know the government

913
00:56:24,285 --> 00:56:27,745
Speaker 5:  will get there, but it, it's just like we're kind of,

914
00:56:28,345 --> 00:56:31,955
Speaker 5:  you're kind of proving the point a little bit. Like look at how hard we're

915
00:56:31,955 --> 00:56:35,275
Speaker 5:  competing with TikTok. It is like, yeah, look at how hard Meta did not have

916
00:56:35,275 --> 00:56:38,275
Speaker 5:  to compete. Look at how hard Facebook did not have to compete with Instagram.

917
00:56:38,825 --> 00:56:42,545
Speaker 5:  Look at how hard Facebook didn't have to compete with WhatsApp. That's

918
00:56:42,825 --> 00:56:43,825
Speaker 5:  the, that's the case.

919
00:56:44,135 --> 00:56:47,905
Speaker 6:  Yeah. And the messaging stuff I think has been in some ways even more

920
00:56:47,905 --> 00:56:50,225
Speaker 6:  damning. Like you talk about just what's in their emails.

921
00:56:52,105 --> 00:56:55,665
Speaker 6:  Facebook was petrified of what was happening as people were moving to messaging

922
00:56:55,685 --> 00:56:59,265
Speaker 6:  and understood that that was the next place people were gonna go to actually

923
00:56:59,265 --> 00:57:03,185
Speaker 6:  talk to friends and family. And it has just come up over and over. Like they

924
00:57:03,185 --> 00:57:07,065
Speaker 6:  just keep showing emails where one Facebook executive or another is like,

925
00:57:07,065 --> 00:57:11,025
Speaker 6:  if we don't buy a messaging service, everyone will die because all

926
00:57:11,025 --> 00:57:14,265
Speaker 6:  of our, everyone's friends are leaving. And then they're like, ah, we didn't

927
00:57:14,265 --> 00:57:17,545
Speaker 6:  really think WhatsApp was gonna make any money. And it's like, well is that,

928
00:57:18,165 --> 00:57:19,625
Speaker 6:  are those not the same thing?

929
00:57:20,695 --> 00:57:20,985
Speaker 5:  Yeah.

930
00:57:21,335 --> 00:57:25,155
Speaker 6:  It's a weird thing. And it does, it really does feel like everyone is

931
00:57:25,355 --> 00:57:29,075
Speaker 6:  actually kind of saying the same thing with really different words. And it's,

932
00:57:29,215 --> 00:57:33,015
Speaker 6:  to your point, not super clear who is going to be the one to

933
00:57:33,015 --> 00:57:35,015
Speaker 6:  try to like make something cogent out of all of this.

934
00:57:35,275 --> 00:57:39,135
Speaker 5:  So there's another turn of this Harry's testimony today where he

935
00:57:39,135 --> 00:57:43,125
Speaker 5:  talked about trying to integrate the teams and it eventually

936
00:57:43,125 --> 00:57:46,965
Speaker 5:  got to the point where he was like, the Instagram team was too

937
00:57:47,165 --> 00:57:50,125
Speaker 5:  spiky and too weird and I had to reel them in and I didn't do that first.

938
00:57:50,545 --> 00:57:54,335
Speaker 5:  And then he, he just dunked on me a little bit, which was super great. The,

939
00:57:54,595 --> 00:57:58,255
Speaker 5:  the Federal Trade Commission in court, their lawyers played

940
00:57:58,605 --> 00:58:02,595
Speaker 5:  erry, his own interview on decoder. They played him a clip

941
00:58:02,595 --> 00:58:06,395
Speaker 5:  from Decoder and they prefaced it by saying, do you remember this conversation

942
00:58:06,395 --> 00:58:09,755
Speaker 5:  with Eli? And Adam said, I vaguely remember speaking to Eli.

943
00:58:10,585 --> 00:58:14,505
Speaker 5:  Congrats. Super great. Yeah, I love you too Adam. I vaguely remember our

944
00:58:14,505 --> 00:58:18,345
Speaker 5:  conversations as well, but the clip is really interesting. They,

945
00:58:18,345 --> 00:58:20,745
Speaker 5:  they played in the, well should we run the clip? We don't have it, but let's

946
00:58:20,745 --> 00:58:22,425
Speaker 5:  pretend I'll read it to you and we can actually run the clips.

947
00:58:22,425 --> 00:58:23,065
Speaker 6:  Yeah, we can play the clip.

948
00:58:24,675 --> 00:58:27,615
Speaker 5:  So Clip is really interesting because, you know, decoder is an org chart

949
00:58:27,645 --> 00:58:30,335
Speaker 5:  show, right? Where asked like, well how do you structure the company? And

950
00:58:30,335 --> 00:58:34,215
Speaker 5:  so I asked him how Instagram was structured and how he'd integrated more

951
00:58:34,215 --> 00:58:36,855
Speaker 5:  into Facebook over time. And this is, if you'll recall,

952
00:58:37,715 --> 00:58:41,655
Speaker 5:  2021 is this conversation. It is, Facebook is trying to tie

953
00:58:41,655 --> 00:58:45,255
Speaker 5:  everything more tightly together because it sees this antitrust stuff coming.

954
00:58:46,075 --> 00:58:49,815
Speaker 5:  And if you'll recall, 2021, this is when Donald Trump had been

955
00:58:49,835 --> 00:58:53,735
Speaker 5:  banned after from all the platforms after

956
00:58:53,735 --> 00:58:57,575
Speaker 5:  January 6th, after all the elections. Like just bad, just

957
00:58:57,575 --> 00:58:58,455
Speaker 5:  bad stuff all the way around.

958
00:58:58,555 --> 00:59:00,895
Speaker 6:  Didn't this interview run with like a super spicy headline?

959
00:59:01,035 --> 00:59:04,335
Speaker 5:  The, the headline of the interview on our site was Adversary says Banning

960
00:59:04,335 --> 00:59:07,895
Speaker 5:  Donald Trump was the right thing to do. It's, and the Donald Trump FTC just

961
00:59:07,895 --> 00:59:11,695
Speaker 5:  played a clip of that interview to Sary in the courthouse. But they

962
00:59:11,775 --> 00:59:14,575
Speaker 5:  used, I think the Apple Podcast headline is like much softer.

963
00:59:15,795 --> 00:59:19,615
Speaker 5:  So they showed that screenshot instead of the one on our site, which is

964
00:59:19,615 --> 00:59:21,095
Speaker 5:  very funny. So he said,

965
00:59:23,005 --> 00:59:26,045
Speaker 5:  I told every, when I took over Instagram, I told everyone I was gonna be

966
00:59:26,045 --> 00:59:29,285
Speaker 5:  a sponge. I wouldn't change for any push, for any change, I would try to

967
00:59:29,285 --> 00:59:31,845
Speaker 5:  understand Instagram, the product, the employee advice and the val values.

968
00:59:32,225 --> 00:59:35,725
Speaker 5:  The one place where I almost immediately broke my promise was safety and

969
00:59:35,725 --> 00:59:38,725
Speaker 5:  integrity. I was very interested in details 'cause I'd run blah blah blah.

970
00:59:38,745 --> 00:59:42,005
Speaker 5:  I'm just skipping ahead 'cause we're gonna run the audio. And he said, I

971
00:59:42,005 --> 00:59:44,245
Speaker 5:  found for the most part, Instagram was running around stuff and our team

972
00:59:44,245 --> 00:59:47,285
Speaker 5:  was tiny and I made them integrate with the bigger Meta team so it could

973
00:59:47,285 --> 00:59:51,125
Speaker 5:  grow. And this is the best argument that exists.

974
00:59:51,345 --> 00:59:54,205
Speaker 5:  I'm actually really curious why the government played this to Adam, right?

975
00:59:54,595 --> 00:59:58,145
Speaker 5:  Because the best argument is trust and safety is really hard.

976
00:59:58,405 --> 01:00:02,145
Speaker 5:  The scale is really hard doing it on photography and video is even

977
01:00:02,145 --> 01:00:04,985
Speaker 5:  harder, right? 'cause you just need more compute to scan all that content.

978
01:00:05,525 --> 01:00:08,345
Speaker 5:  And you need unified rules and you need to manage contractors around the

979
01:00:08,345 --> 01:00:11,725
Speaker 5:  world and multiple language, all this stuff. That is the best argument they

980
01:00:11,725 --> 01:00:14,685
Speaker 5:  have that Instagram was able to scale because

981
01:00:16,055 --> 01:00:20,015
Speaker 5:  Meta had that particular infrastructure. And maybe

982
01:00:20,015 --> 01:00:22,495
Speaker 5:  you could argue that they had ad sales infrastructure all also, but it was

983
01:00:22,495 --> 01:00:25,775
Speaker 5:  that particular infrastructure. I thought it was really interesting. And

984
01:00:25,775 --> 01:00:29,455
Speaker 5:  then he gets to the Instagram team was chafing against

985
01:00:29,485 --> 01:00:33,335
Speaker 5:  Meta, right? And he, he, he thinks a bunch of stuff was overblown. And eventually,

986
01:00:33,335 --> 01:00:36,775
Speaker 5:  of course Kevin's system and Mike Krieger leave Meta.

987
01:00:36,845 --> 01:00:39,855
Speaker 5:  They're, they were on the stand last week. They're very spiky. They said

988
01:00:39,855 --> 01:00:42,375
Speaker 5:  they could have made it on their own. But I just think it's really interesting

989
01:00:42,375 --> 01:00:46,295
Speaker 5:  that so much of the story is here are the capabilities

990
01:00:46,295 --> 01:00:49,655
Speaker 5:  that Meta actually provide Instagram versus having compete with Instagram.

991
01:00:49,845 --> 01:00:50,135
Speaker 5:  Yeah.

992
01:00:50,135 --> 01:00:53,975
Speaker 6:  I mean, and over the years Meta has made the case for itself

993
01:00:53,995 --> 01:00:57,335
Speaker 6:  as a trust and safety organization like many, many times. It is one of the

994
01:00:57,335 --> 01:01:01,215
Speaker 6:  things that that company is invested in at a level beyond almost

995
01:01:01,215 --> 01:01:04,975
Speaker 6:  anybody else, which is a wild given all the stuff that still happens on its

996
01:01:05,055 --> 01:01:08,975
Speaker 6:  platforms. But yeah, It is kind of weird to basically play that.

997
01:01:09,095 --> 01:01:11,335
Speaker 6:  'cause it, I mean both of those things I suppose can be true at the same

998
01:01:11,335 --> 01:01:15,295
Speaker 6:  time where the Instagram team can be chafing against it, but also

999
01:01:15,295 --> 01:01:18,695
Speaker 6:  really needed the trust and safety help. But I don't know that, like at best

1000
01:01:18,695 --> 01:01:20,815
Speaker 6:  that feels like a wash of an argument to me from

1001
01:01:21,205 --> 01:01:21,495
Speaker 5:  Yeah,

1002
01:01:22,115 --> 01:01:25,725
Speaker 7:  Well I think that the two things that can be true at the same time are that

1003
01:01:26,325 --> 01:01:29,415
Speaker 7:  even if in in a world where Instagram had remained independent, that it still

1004
01:01:29,505 --> 01:01:33,295
Speaker 7:  could have been weird and bad in the ways that It is weird and bad now

1005
01:01:33,615 --> 01:01:36,615
Speaker 7:  and just a, a cluster of things that don't really work together, even if

1006
01:01:36,615 --> 01:01:39,895
Speaker 7:  Meta had never purchased it. We've seen what's happened with Twitter

1007
01:01:41,885 --> 01:01:45,855
Speaker 7:  like prior, prior to Elon Musk. And also that the reason

1008
01:01:45,855 --> 01:01:47,695
Speaker 7:  why Meta purchased it was to take out a competitor

1009
01:01:48,285 --> 01:01:48,895
Speaker 6:  Fair. Yeah.

1010
01:01:49,235 --> 01:01:52,375
Speaker 5:  And also if you are the sort of person who believes all these companies censor

1011
01:01:52,475 --> 01:01:56,295
Speaker 5:  too much and you're worried about social big platform free

1012
01:01:56,295 --> 01:02:00,055
Speaker 5:  speech abuse, sure you can worry about that. The solution is you have

1013
01:02:00,055 --> 01:02:03,935
Speaker 5:  more big companies moderating different ways and competing to have the best

1014
01:02:03,935 --> 01:02:07,575
Speaker 5:  moderation policies. Not one ever bigger company imposing the same rules,

1015
01:02:07,595 --> 01:02:10,855
Speaker 5:  in the same processes, in the same Mark Zuckerberg whims

1016
01:02:11,755 --> 01:02:14,375
Speaker 5:  not, meta's not doing content moderation anymore. We're doing community ed.

1017
01:02:14,375 --> 01:02:18,255
Speaker 5:  So like that is the problem, right? Is the scale. I don't know

1018
01:02:18,255 --> 01:02:21,485
Speaker 5:  that the, the Trump administration, they were smart would be making this

1019
01:02:21,645 --> 01:02:25,005
Speaker 5:  argument. 'cause it's such a big argument they make all the time. But what

1020
01:02:25,005 --> 01:02:28,725
Speaker 5:  they actually want is control over Monopoly speech. So I don't think they're

1021
01:02:28,725 --> 01:02:29,525
Speaker 5:  gonna make this argument,

1022
01:02:31,105 --> 01:02:33,845
Speaker 5:  But it's really like that's a thing that you would want, you would want multiple

1023
01:02:34,205 --> 01:02:37,965
Speaker 5:  platform companies operating independently so that if you hated the moderation

1024
01:02:38,065 --> 01:02:41,245
Speaker 5:  on Facebook, you would leave and go to an independent Instagram Right. And

1025
01:02:41,245 --> 01:02:44,445
Speaker 5:  experience a different moderation system. And that just has never played

1026
01:02:44,445 --> 01:02:48,005
Speaker 5:  out. We've never really seen that play out. No. Or you would have an Instagram

1027
01:02:48,005 --> 01:02:51,655
Speaker 5:  that was better at protecting teens, right? Because you would,

1028
01:02:52,445 --> 01:02:55,375
Speaker 5:  they would feel pressure and people would leave their platform to go to some

1029
01:02:55,375 --> 01:02:57,735
Speaker 5:  other platform. And we certainly have not seen that play out.

1030
01:02:58,075 --> 01:02:58,295
Speaker 6:  No.

1031
01:02:58,965 --> 01:03:02,225
Speaker 5:  All right. So same question as before, 36 months from now, what does Meta

1032
01:03:02,225 --> 01:03:05,105
Speaker 5:  look like? Is it recognizable? Is it very different?

1033
01:03:05,625 --> 01:03:08,705
Speaker 6:  I think Meta gets out of this one. Yeah. Yeah, because

1034
01:03:08,705 --> 01:03:10,505
Speaker 5:  Mark Zuckerberg's just floating around the Oval Office

1035
01:03:11,095 --> 01:03:13,945
Speaker 6:  Kind of. Yeah. I think Mark Zuckerberg

1036
01:03:15,265 --> 01:03:18,625
Speaker 6:  probably thinks he can convince Donald Trump to just throw out this case

1037
01:03:18,805 --> 01:03:22,785
Speaker 6:  in one way or another. And I think the, the,

1038
01:03:23,055 --> 01:03:26,705
Speaker 6:  like, again, the technicalities of this continue to be so complicated and

1039
01:03:26,705 --> 01:03:30,545
Speaker 6:  it's, the market definitions are weird and like friends and family

1040
01:03:30,545 --> 01:03:34,345
Speaker 6:  sharing services is a strange thing. And so I feel like it might

1041
01:03:34,895 --> 01:03:38,385
Speaker 6:  sort of lose this case in the court of public opinion and win it on

1042
01:03:38,385 --> 01:03:39,145
Speaker 6:  technicalities.

1043
01:03:40,015 --> 01:03:40,475
Speaker 5:  All right, Richard,

1044
01:03:41,565 --> 01:03:44,815
Speaker 7:  They get out of this, but they do some weird thing where Facebook becomes

1045
01:03:44,895 --> 01:03:48,335
Speaker 7:  a spin code that they spin off into its own thing and Mark just takes

1046
01:03:48,655 --> 01:03:52,455
Speaker 7:  Instagram and, and Meta and, and all the VR stuff

1047
01:03:52,615 --> 01:03:54,775
Speaker 7:  and turns into Horizon and goes and lives in the Metaverse.

1048
01:03:55,495 --> 01:03:58,905
Speaker 6:  Okay, that's an unbelievably good idea. Mark. Just fully,

1049
01:03:59,885 --> 01:04:02,465
Speaker 6:  he just zags on him, spin off Facebook.

1050
01:04:04,535 --> 01:04:07,345
Speaker 6:  He's like, the company's not even called that anymore. This is for old people.

1051
01:04:07,565 --> 01:04:11,425
Speaker 6:  I'm gonna sell Facebook to the A A RP and I'm gonna go

1052
01:04:11,425 --> 01:04:15,305
Speaker 6:  do Instagram and WhatsApp. The metaverse unbelievable idea. I have no

1053
01:04:15,305 --> 01:04:15,505
Speaker 6:  notes.

1054
01:04:16,865 --> 01:04:20,675
Speaker 5:  I, my answer here all depends on what happens with TikTok.

1055
01:04:21,695 --> 01:04:25,325
Speaker 5:  I think if TikTok survives Meta does not look

1056
01:04:25,525 --> 01:04:27,645
Speaker 5:  recognized in three years, I, I actually think there's a, a better chance

1057
01:04:27,645 --> 01:04:31,485
Speaker 5:  they lose this case than anyone thinks. Okay. Because the market

1058
01:04:31,485 --> 01:04:34,245
Speaker 5:  definition of they're, they pinned a lot on that. The judge gets to pick,

1059
01:04:35,035 --> 01:04:37,375
Speaker 5:  the judge is allowed to just invent their own market, right? They're like

1060
01:04:37,375 --> 01:04:41,215
Speaker 5:  the market for Scrolly videos, like done and done, I'm calling it. Right?

1061
01:04:42,195 --> 01:04:45,615
Speaker 5:  So we'll see what happens there. But if there's no

1062
01:04:45,875 --> 01:04:49,425
Speaker 5:  TikTok, right, TikTok actually gets banned. I think Meta

1063
01:04:49,615 --> 01:04:53,305
Speaker 5:  just collects all those users and it can withstand whatever changes comes

1064
01:04:53,365 --> 01:04:56,865
Speaker 5:  its way if TikTok does not get banned, I think the competitive threat is

1065
01:04:56,865 --> 01:05:00,425
Speaker 5:  still so high. And the relevance of these, these things are

1066
01:05:00,485 --> 01:05:04,345
Speaker 5:  fading. Like TikTok is just vastly more culturally relevant than Instagram

1067
01:05:04,345 --> 01:05:07,825
Speaker 5:  is today. Oh yeah. It's vastly more culturally relevant than

1068
01:05:08,425 --> 01:05:12,345
Speaker 5:  Facebook is today. Aside from your child's little

1069
01:05:12,345 --> 01:05:16,065
Speaker 5:  league softball schedule. It is like posted in a, an uns scannable

1070
01:05:16,145 --> 01:05:20,045
Speaker 5:  PDF in a Facebook group. Like it's fine, but like

1071
01:05:20,045 --> 01:05:23,725
Speaker 5:  the cultural relevance of TikTok is so high and Meta doesn't have moves.

1072
01:05:24,045 --> 01:05:26,605
Speaker 5:  I I don't think they can figure it out. And so you get the pressure, the

1073
01:05:26,605 --> 01:05:29,045
Speaker 5:  court case and whatever remedies in a court case, all the distraction of

1074
01:05:29,045 --> 01:05:32,125
Speaker 5:  compliance, blah, blah, blah, blah. And then a, a continued ascendance of

1075
01:05:32,125 --> 01:05:35,525
Speaker 5:  TikTok 36 months from now. These companies look totally different. I dunno

1076
01:05:36,055 --> 01:05:37,795
Speaker 5:  it that's, that's my conditional

1077
01:05:38,055 --> 01:05:41,675
Speaker 6:  It is certainly true that there doesn't seem to be any way in the next

1078
01:05:41,675 --> 01:05:45,475
Speaker 6:  36 months for Meta to buy its way out of its problems, which is a thing he's

1079
01:05:45,545 --> 01:05:46,315
Speaker 5:  Done. I mean, they've been trying

1080
01:05:46,455 --> 01:05:47,115
Speaker 6:  That's Yeah.

1081
01:05:48,065 --> 01:05:50,085
Speaker 5:  How many meme coins has, has Zuck bought?

1082
01:05:51,695 --> 01:05:55,665
Speaker 5:  Zuck is like, here's what I'm gonna do. I'm gonna punch Ukraine. Like I'm

1083
01:05:55,665 --> 01:05:59,185
Speaker 5:  gonna do some MMA on in the Middle East. Like

1084
01:05:59,625 --> 01:06:02,745
Speaker 5:  I He's trying Yeah. Dana White's running it now. There you go.

1085
01:06:03,485 --> 01:06:07,345
Speaker 5:  All right. That's the baby podcast right there. If you want the clip, that's

1086
01:06:07,345 --> 01:06:11,185
Speaker 5:  the one. All right, let's, let's breathe. Let

1087
01:06:11,405 --> 01:06:15,335
Speaker 5:  one second and then actually p cleanse this with some gadget news and it's

1088
01:06:15,335 --> 01:06:17,535
Speaker 5:  Meta Gadget News, but it's still Gadget News. It's

1089
01:06:17,805 --> 01:06:20,975
Speaker 6:  Neli specific Meta Gadget. It is, It is

1090
01:06:21,665 --> 01:06:24,135
Speaker 6:  Eli's number one dream gadget.

1091
01:06:24,845 --> 01:06:28,695
Speaker 5:  Okay. So here the, the news is, there's a rumor that Meta will have new

1092
01:06:28,755 --> 01:06:32,495
Speaker 5:  AI glasses and they will have a quote, super sensing mode with facial recognition.

1093
01:06:33,405 --> 01:06:37,175
Speaker 5:  I've been saying for years now that the killer app

1094
01:06:37,955 --> 01:06:41,935
Speaker 5:  for glasses, for smart glasses is names and faces. I'm horrible at names

1095
01:06:41,935 --> 01:06:45,695
Speaker 5:  and faces. If I could just look at people and be told their name, I would

1096
01:06:45,855 --> 01:06:49,295
Speaker 5:  probably be the most po po powerful politician in America. I'm say I, I'm

1097
01:06:49,295 --> 01:06:49,575
Speaker 5:  not, you know,

1098
01:06:50,495 --> 01:06:52,715
Speaker 6:  You think that's what, that's the only thing that's stopping you.

1099
01:06:52,865 --> 01:06:55,875
Speaker 5:  Yeah. 'cause I walk up to people, I'm like, who are you? And that you can't

1100
01:06:55,875 --> 01:06:59,835
Speaker 5:  get votes that way. That's not normal. Like Bill Clinton famously would

1101
01:06:59,835 --> 01:07:03,275
Speaker 5:  like go to a rope line in Vermont like five years after he'd last been there,

1102
01:07:03,275 --> 01:07:06,715
Speaker 5:  and he'd remember every single person and, and their children. And everyone's

1103
01:07:06,715 --> 01:07:09,555
Speaker 5:  like, well, we love that Billy Clinton. And then, you know, like, now he

1104
01:07:09,635 --> 01:07:12,635
Speaker 5:  became president. I show up at a rope line in Vermont, I'm like, I don't

1105
01:07:12,655 --> 01:07:16,315
Speaker 5:  all of you look exactly the same to me. Like total face blindness. If I could

1106
01:07:16,315 --> 01:07:17,195
Speaker 5:  just do names and faces.

1107
01:07:17,975 --> 01:07:19,115
Speaker 6:  Is it like a white people thing?

1108
01:07:19,995 --> 01:07:23,755
Speaker 5:  Yeah, that's another baby podcast moment right there. You're the theo.

1109
01:07:23,895 --> 01:07:24,915
Speaker 5:  I'm trying not to say you did it.

1110
01:07:27,895 --> 01:07:31,435
Speaker 5:  No, it's like literally everywhere. I just, I can't do it. If I could do

1111
01:07:31,435 --> 01:07:34,995
Speaker 5:  it, the problem is to do it, you need to build a worldwide facial

1112
01:07:34,995 --> 01:07:38,835
Speaker 5:  recognition database, which is deeply problematic. Like,

1113
01:07:38,835 --> 01:07:42,755
Speaker 5:  that is the panopticon surveillance state. And as

1114
01:07:42,755 --> 01:07:44,915
Speaker 5:  much as I want this product, I'm like, I think that trade off's pretty bad.

1115
01:07:45,955 --> 01:07:49,725
Speaker 5:  Facebook has a long been able to build this product because they have

1116
01:07:49,725 --> 01:07:50,565
Speaker 5:  everybody's names and faces.

1117
01:07:50,755 --> 01:07:51,045
Speaker 6:  Yeah.

1118
01:07:52,275 --> 01:07:56,025
Speaker 5:  There have been versions of this product hacked together using Meta AI

1119
01:07:56,345 --> 01:08:00,025
Speaker 5:  glasses, live streaming to Instagram live captured in a, in a web browser

1120
01:08:00,085 --> 01:08:03,065
Speaker 5:  and sent to Clearview. You've, we've run the demos, we've talked about it.

1121
01:08:03,245 --> 01:08:06,945
Speaker 5:  Yep. Like the proof of concept has been there. And so I think Meta knows

1122
01:08:06,945 --> 01:08:09,465
Speaker 5:  like, oh, this is the killer app. Like we should show people the killer app.

1123
01:08:09,465 --> 01:08:12,585
Speaker 5:  Like all bets are off right now with privacy law in the United States. All

1124
01:08:12,585 --> 01:08:15,765
Speaker 5:  bets are off with AI regulation in the United States. We're just doing it.

1125
01:08:16,565 --> 01:08:19,585
Speaker 5:  And I think people are gonna be so creeped out so fast that the backlash

1126
01:08:19,585 --> 01:08:20,785
Speaker 5:  will be intense. Oh,

1127
01:08:20,825 --> 01:08:24,265
Speaker 6:  I completely agree. I mean, I think it, it's, it's interesting to see

1128
01:08:25,015 --> 01:08:28,665
Speaker 6:  this start to percolate up next to real time

1129
01:08:28,665 --> 01:08:31,825
Speaker 6:  translation as like everyone's holy grail feature for this stuff, right?

1130
01:08:31,825 --> 01:08:35,705
Speaker 6:  Because it's, that's the thing everyone is working on. Anyone who has

1131
01:08:35,705 --> 01:08:39,265
Speaker 6:  ever done any AI anything would love to tell you how it's gonna help

1132
01:08:39,265 --> 01:08:41,985
Speaker 6:  facilitate real time translation so all the people in the world could talk

1133
01:08:41,985 --> 01:08:45,385
Speaker 6:  to each other. And this is like, it's, it's getting right up there with it.

1134
01:08:45,385 --> 01:08:48,025
Speaker 6:  And I think I would define it slightly broader than the way you're thinking

1135
01:08:48,025 --> 01:08:51,825
Speaker 6:  about it, which is just like sort of latent world awareness,

1136
01:08:52,075 --> 01:08:55,745
Speaker 6:  right? Because like, think about the, the Project Astra demo we saw from

1137
01:08:55,745 --> 01:08:58,745
Speaker 6:  Google last year where you can like ask it where you left your keys. Like

1138
01:08:59,175 --> 01:09:03,105
Speaker 6:  it's, it's relatively similar technology that makes my glasses

1139
01:09:03,255 --> 01:09:06,625
Speaker 6:  know your neli and my glasses know that my keys are over there, right? So

1140
01:09:06,625 --> 01:09:10,505
Speaker 6:  I think that is the thing now that everybody understands is

1141
01:09:11,125 --> 01:09:14,985
Speaker 6:  or at least believes is like a killer app for all of this stuff. And

1142
01:09:14,985 --> 01:09:18,905
Speaker 6:  it's like, where is the, you know, where are the groceries in the

1143
01:09:18,905 --> 01:09:21,665
Speaker 6:  car? And it'll like remind you that you left them in the car because it's

1144
01:09:21,665 --> 01:09:25,345
Speaker 6:  all, you put them in the car and all that kind of stuff is, is

1145
01:09:25,745 --> 01:09:29,345
Speaker 6:  starting to bubble up from all of these different companies. But It is, it's

1146
01:09:29,345 --> 01:09:33,185
Speaker 6:  the same thing like, oh, I have to give you 100%

1147
01:09:33,585 --> 01:09:37,425
Speaker 6:  constant access to everything happening around me all the time. And in exchange

1148
01:09:37,425 --> 01:09:40,505
Speaker 6:  you'll tell me where I left my keys and what Eli's name is,

1149
01:09:40,645 --> 01:09:41,705
Speaker 5:  You're gonna label stuff.

1150
01:09:41,845 --> 01:09:45,705
Speaker 6:  And I don't think most people wanna make that trade. I just don't. Look,

1151
01:09:45,765 --> 01:09:48,865
Speaker 5:  Tim Cook is heavily invested in air glasses as a rumor this week that Apple

1152
01:09:48,965 --> 01:09:52,905
Speaker 5:  is, is building chips for its own ar glasses. Now that

1153
01:09:52,905 --> 01:09:56,705
Speaker 5:  stuff all seems still p pretty pipe dream to me, right? Like Meta's Orion

1154
01:09:56,705 --> 01:10:00,005
Speaker 5:  glasses are too expensive to build because the, the,

1155
01:10:00,835 --> 01:10:04,815
Speaker 5:  the, the blocker is the optics, right? Right. You gotta make

1156
01:10:04,855 --> 01:10:08,695
Speaker 5:  a lens that can accept images. Meta insists on calling them holograms.

1157
01:10:08,955 --> 01:10:12,935
Speaker 5:  If you see a Meta executive refer to their glasses as doing projecting

1158
01:10:12,935 --> 01:10:16,255
Speaker 5:  holograms, like I literally want you to stop them in their tracks and be

1159
01:10:16,255 --> 01:10:19,695
Speaker 5:  like, use words correctly. They're not holograms, they're just image overlays.

1160
01:10:19,695 --> 01:10:22,695
Speaker 5:  They're just projecting onto wave guides and glasses like everyone else is

1161
01:10:22,695 --> 01:10:25,375
Speaker 5:  doing. Holograms are, you know, three dimensional the existence.

1162
01:10:26,515 --> 01:10:29,815
Speaker 5:  It, we we're Vergecast people. You understand what I'm saying? Use the words

1163
01:10:29,815 --> 01:10:30,095
Speaker 5:  right.

1164
01:10:31,865 --> 01:10:35,505
Speaker 5:  I mean it Okay. But like the optics are the problem.

1165
01:10:35,695 --> 01:10:39,105
Speaker 5:  Meta invested in one set of optical technologies that thought would scale.

1166
01:10:39,105 --> 01:10:41,865
Speaker 5:  They didn't, they didn't get cheaper. Apple hasn't solved this problem. That's

1167
01:10:41,865 --> 01:10:45,625
Speaker 5:  why they built the Vision pro the way they did. There's a lot of steps between

1168
01:10:45,625 --> 01:10:49,065
Speaker 5:  here and now. And then you look at what the promise is. And again, I would

1169
01:10:49,065 --> 01:10:52,945
Speaker 5:  buy names and faces, glasses in one RV given all, even with

1170
01:10:52,945 --> 01:10:56,345
Speaker 5:  all their problems, I would buy them tomorrow. I think it'd be great for

1171
01:10:56,345 --> 01:10:58,665
Speaker 5:  conferences where everyone opted in. Like there's a lot of places you could

1172
01:10:58,665 --> 01:10:59,465
Speaker 5:  make it work, right?

1173
01:11:01,325 --> 01:11:04,845
Speaker 5:  But at the end of the day, the is we're gonna label stuff.

1174
01:11:05,485 --> 01:11:07,925
Speaker 5:  You're gonna look at a painting and we're gonna be like, here's the name

1175
01:11:07,925 --> 01:11:10,845
Speaker 5:  of the painting. And it's like, you gotta do a little bit better than that,

1176
01:11:10,845 --> 01:11:14,565
Speaker 5:  right? You gotta make a better promise than everything will be labeled.

1177
01:11:15,285 --> 01:11:17,645
Speaker 7:  I think something interesting that happened here, when you talk about that

1178
01:11:17,885 --> 01:11:20,685
Speaker 7:  backlash and, and what might happen that they're not anticipating, remember

1179
01:11:20,685 --> 01:11:24,365
Speaker 7:  when Microsoft talked about bringing out out their recall

1180
01:11:24,785 --> 01:11:27,485
Speaker 7:  for, for Windows where it Yeah. Remember every single thing that has been

1181
01:11:27,485 --> 01:11:30,765
Speaker 7:  on your screen and then people said, wait, every single thing that has been

1182
01:11:30,765 --> 01:11:31,405
Speaker 7:  on my screen,

1183
01:11:32,735 --> 01:11:35,805
Speaker 5:  David wrote a piece where he was like, I love this. He's like, screenshots

1184
01:11:35,805 --> 01:11:38,005
Speaker 5:  are the future of ai. That was like your headline. Yeah.

1185
01:11:38,145 --> 01:11:41,965
Speaker 6:  For precisely the opposite reason of recall. Like it's, it's

1186
01:11:41,965 --> 01:11:45,845
Speaker 6:  because the thing a screenshot lets me do is say, I care about this, save

1187
01:11:45,845 --> 01:11:49,645
Speaker 6:  this for me and then make something out of it. Like the, I

1188
01:11:49,725 --> 01:11:53,045
Speaker 6:  I, we've talked about this v song was on the show just

1189
01:11:53,685 --> 01:11:57,445
Speaker 6:  recently talking about the, these ai recording wearables and there is, there's

1190
01:11:57,445 --> 01:12:00,765
Speaker 6:  this thing you realize where you're like, oh my God, I do so much stuff that

1191
01:12:00,765 --> 01:12:04,565
Speaker 6:  I don't wanna have any record of and everybody goes to like porn and

1192
01:12:04,765 --> 01:12:08,685
Speaker 6:  whatever. But like, I don't, it's weird that it has a, it

1193
01:12:08,685 --> 01:12:12,165
Speaker 6:  it has an audio record of every TikTok I watched today. Like that's weird.

1194
01:12:12,275 --> 01:12:12,565
Speaker 6:  Like,

1195
01:12:12,565 --> 01:12:15,445
Speaker 7:  Like it shouldn't have a recording of what someone was saying when I walked

1196
01:12:15,445 --> 01:12:18,605
Speaker 7:  past them in the store and like, it it's pulling that up when I search because

1197
01:12:19,455 --> 01:12:21,295
Speaker 7:  I don't even remember that I didn't even hear it at the time.

1198
01:12:21,845 --> 01:12:25,255
Speaker 6:  Yeah. It's, it's not my information to have, and yet I have it. I

1199
01:12:25,255 --> 01:12:27,775
Speaker 5:  Mean, I, I think we should summarize this podcast immediately.

1200
01:12:29,825 --> 01:12:33,015
Speaker 5:  We'll see, like I'm exci if medic Meta of all companies

1201
01:12:33,895 --> 01:12:37,295
Speaker 5:  announces a facial, a set of facial recognition glasses, like

1202
01:12:38,335 --> 01:12:41,745
Speaker 5:  it's, they don't have a great privacy rep to begin with, right?

1203
01:12:42,785 --> 01:12:46,725
Speaker 5:  It is the killer app for these glasses. I I'm being a little snarky about

1204
01:12:46,965 --> 01:12:50,885
Speaker 5:  labels. Like that is the promise labeling people is the killer app for

1205
01:12:50,885 --> 01:12:54,525
Speaker 5:  these glasses, right? If again, if you're at a conference, just imagine a

1206
01:12:54,525 --> 01:12:58,405
Speaker 5:  conference and, and you know, there's big signs that say in this,

1207
01:12:58,825 --> 01:13:01,845
Speaker 5:  in this convention hall, everyone's gonna be wearing these glasses. Like

1208
01:13:01,845 --> 01:13:05,725
Speaker 5:  we, we were already name tags before we're enabling the glasses in in this

1209
01:13:05,785 --> 01:13:09,685
Speaker 5:  one place. Yeah. Okay. Still a little gray area. You should be able to opt

1210
01:13:09,685 --> 01:13:13,245
Speaker 5:  out. But like, let's say that exists, right? Everyone has agreed to

1211
01:13:13,245 --> 01:13:17,125
Speaker 5:  participate in this social experiment and you're walking around and you

1212
01:13:17,125 --> 01:13:20,925
Speaker 5:  see the person that, you saw the conference last year and you had a conversation

1213
01:13:20,925 --> 01:13:23,285
Speaker 5:  and you totally forgot their name and you gave them the card and it's like

1214
01:13:23,345 --> 01:13:27,285
Speaker 5:  got lost the backpack and that's gone. And what you get is here's

1215
01:13:27,285 --> 01:13:31,045
Speaker 5:  this person, here's their name, here's like the company

1216
01:13:31,045 --> 01:13:34,285
Speaker 5:  they work at, like the name tag, but you don't have to like look down at

1217
01:13:34,285 --> 01:13:37,885
Speaker 5:  the name tag. That is actually a, a trip. Like it would ease social

1218
01:13:37,885 --> 01:13:40,445
Speaker 5:  interaction in that environment in a very specific way that would be worth

1219
01:13:40,445 --> 01:13:41,125
Speaker 5:  wearing glasses

1220
01:13:43,325 --> 01:13:46,505
Speaker 5:  in almost every case. Like having to charge the glasses and put them on your

1221
01:13:46,505 --> 01:13:49,465
Speaker 5:  face is my theory of wearable bullshit. Like that's a lot of care for not

1222
01:13:49,465 --> 01:13:49,945
Speaker 5:  a lot of value,

1223
01:13:50,435 --> 01:13:50,785
Speaker 6:  Right?

1224
01:13:52,755 --> 01:13:55,555
Speaker 5:  I don't think Meta can convincingly make the case,

1225
01:13:56,795 --> 01:14:00,255
Speaker 5:  but I also think they know that the only thing people are wearing the ray

1226
01:14:00,255 --> 01:14:04,135
Speaker 5:  bands for is to have a very convenient camera and no one is using

1227
01:14:04,135 --> 01:14:07,715
Speaker 5:  any of the AI features of the Meta glasses and they have to come up with

1228
01:14:07,715 --> 01:14:08,035
Speaker 5:  something else.

1229
01:14:08,505 --> 01:14:12,485
Speaker 6:  Yeah, I mean, I, I agree, but I'm not, I'm just not convinced

1230
01:14:12,485 --> 01:14:13,245
Speaker 6:  this is that thing

1231
01:14:13,915 --> 01:14:17,785
Speaker 5:  We'll see. Yes. More gat news. Just to run through it quickly,

1232
01:14:18,385 --> 01:14:21,185
Speaker 5:  there's a rumor that Apple's gonna do the iPhone Air with a bigger screen

1233
01:14:21,185 --> 01:14:24,945
Speaker 5:  in 2027. There's also a rumor that Apple's gonna do a foldable next year

1234
01:14:24,945 --> 01:14:28,505
Speaker 5:  and they're gonna move the iPhone release date around.

1235
01:14:28,655 --> 01:14:29,625
Speaker 5:  What do you think the

1236
01:14:29,785 --> 01:14:33,025
Speaker 6:  Idea is that there are now going to be, I believe the end, I believe it would

1237
01:14:33,025 --> 01:14:36,825
Speaker 6:  be six different iPhones and that that's too many to

1238
01:14:36,825 --> 01:14:40,585
Speaker 6:  launch at one event. So there are going to be two

1239
01:14:40,585 --> 01:14:44,225
Speaker 6:  different iPhone launch events going forward because they're gonna do, I

1240
01:14:44,225 --> 01:14:47,745
Speaker 6:  think it would be Pro Air and foldable

1241
01:14:48,485 --> 01:14:52,105
Speaker 6:  at one and then base plus

1242
01:14:53,045 --> 01:14:56,705
Speaker 6:  se at another. And I would argue that Apple has learned

1243
01:14:56,705 --> 01:15:00,385
Speaker 6:  precisely the wrong lesson here. And the correct answer is don't have six

1244
01:15:00,405 --> 01:15:03,995
Speaker 6:  phones, but this is where we are. So that's apparently

1245
01:15:04,585 --> 01:15:07,755
Speaker 6:  like the, the eight, the iPhone 18 cycle

1246
01:15:09,225 --> 01:15:11,155
Speaker 6:  appears. It is going to be a huge one.

1247
01:15:12,225 --> 01:15:15,645
Speaker 5:  The iPhone, this iPhone 16 cycle is supposed be huge 'cause of Apple intelligence.

1248
01:15:15,795 --> 01:15:19,405
Speaker 6:  Well, right, I was just gonna say the, this is all based on like some

1249
01:15:20,385 --> 01:15:24,195
Speaker 6:  perennially sketchy supply chain stuff and we never

1250
01:15:24,195 --> 01:15:27,915
Speaker 6:  really know and it's all very messy, but I think

1251
01:15:28,005 --> 01:15:31,955
Speaker 6:  there has been some smoke about a foldable iPhone coming in around in

1252
01:15:31,955 --> 01:15:35,555
Speaker 6:  and around 2027. That leads me to believe that's probably still

1253
01:15:35,555 --> 01:15:39,385
Speaker 6:  relatively right. But I just, I don't know. I'm,

1254
01:15:39,485 --> 01:15:43,145
Speaker 6:  I'm having so much hard, I'm having such a hard time caring about a

1255
01:15:43,255 --> 01:15:46,865
Speaker 6:  slim iPhone and it appears that is going to be the thing this year

1256
01:15:47,325 --> 01:15:50,505
Speaker 6:  and I, I don't care. I just don't care.

1257
01:15:51,625 --> 01:15:54,265
Speaker 5:  I think if they make a foldable iPhone, it, it folds out to like iPhone,

1258
01:15:54,415 --> 01:15:58,265
Speaker 5:  iPad mini size, and then the iPad gets like windowing, which they've rumored

1259
01:15:59,165 --> 01:16:02,255
Speaker 5:  it's gonna get real weird. It's gonna get real weird. And

1260
01:16:02,255 --> 01:16:06,215
Speaker 6:  Then they will still refuse to make a touchscreen MacBook for reasons that

1261
01:16:06,535 --> 01:16:07,535
Speaker 6:  continue to not make any sense forever

1262
01:16:07,715 --> 01:16:09,175
Speaker 7:  Or multi-user on the iPad.

1263
01:16:09,725 --> 01:16:10,495
Speaker 6:  Yeah. Also

1264
01:16:10,495 --> 01:16:13,735
Speaker 5:  That it's the only thing anybody actually wants. Richard, there's a bunch

1265
01:16:13,735 --> 01:16:15,055
Speaker 5:  of new surface stuff. What's going on here?

1266
01:16:15,705 --> 01:16:19,515
Speaker 7:  This is what Tom, Tom Warren had had all of the information this week

1267
01:16:19,515 --> 01:16:23,115
Speaker 7:  about my Microsoft launching some. First of all, they have a smaller Surface

1268
01:16:23,255 --> 01:16:27,235
Speaker 7:  Pro laptop, so they've got a 12 inch. Finally, we, we'd heard

1269
01:16:27,235 --> 01:16:31,115
Speaker 7:  some rumors about this. It starts at $799. It has arm inside,

1270
01:16:31,115 --> 01:16:34,915
Speaker 7:  so it doesn't have a fan. I think you have the iPad competitor that

1271
01:16:34,985 --> 01:16:38,915
Speaker 7:  this smaller, that the Surface Pro is kind of always supposed to be. Now

1272
01:16:38,915 --> 01:16:42,795
Speaker 7:  the Windows on arm is pretty good with the Qualcomm chips

1273
01:16:43,055 --> 01:16:45,915
Speaker 7:  and with that, all the things that Windows have done to be able to run more

1274
01:16:45,915 --> 01:16:49,755
Speaker 7:  and more programs, it's the time the, the Surface Pro

1275
01:16:50,025 --> 01:16:53,395
Speaker 7:  that people were looking for that it was kind of supposed to always be

1276
01:16:53,865 --> 01:16:57,835
Speaker 7:  This is a lot more it, the the price Yeah. Is a lot lighter. It's, it's

1277
01:16:57,835 --> 01:17:01,675
Speaker 7:  a good size. It's, it seems like his first impressions of it. I think he

1278
01:17:01,675 --> 01:17:05,195
Speaker 7:  said that the 12 inch has the eight core chip, 16 gigs Ram

1279
01:17:05,495 --> 01:17:07,715
Speaker 7:  256 gigs of storage for 7 99.

1280
01:17:08,385 --> 01:17:10,325
Speaker 5:  This fully looks like my new airplane computer.

1281
01:17:10,835 --> 01:17:11,125
Speaker 7:  Yeah.

1282
01:17:11,345 --> 01:17:12,445
Speaker 5:  One second. Perfect. Right?

1283
01:17:12,905 --> 01:17:15,805
Speaker 7:  It seems like that is the one that you could just take with you and you'll

1284
01:17:15,805 --> 01:17:19,445
Speaker 7:  have your computer in the kind of the sa the same size and shape of an iPad,

1285
01:17:19,545 --> 01:17:21,885
Speaker 7:  but you'll be able to have multiple accounts and computer stuff.

1286
01:17:22,955 --> 01:17:23,245
Speaker 5:  Yeah.

1287
01:17:23,245 --> 01:17:27,205
Speaker 6:  This does seem like, at least in theory, Microsoft has

1288
01:17:27,205 --> 01:17:31,045
Speaker 6:  been sort of dancing around the right set of specs for a long time. Like

1289
01:17:31,045 --> 01:17:34,085
Speaker 6:  it was, it was too small and then, and then it was too big and then there

1290
01:17:34,085 --> 01:17:36,765
Speaker 6:  was the surface go, which was, and then some of it was too slow and some

1291
01:17:36,765 --> 01:17:39,405
Speaker 6:  of it, like it always had a couple of things right? And a couple of things

1292
01:17:39,405 --> 01:17:42,125
Speaker 6:  wrong. And at least on paper, I think you're right that this is the one that

1293
01:17:42,125 --> 01:17:45,565
Speaker 6:  is like, okay, you, you made the list correctly this time

1294
01:17:46,675 --> 01:17:49,245
Speaker 6:  it's still, it's a thousand bucks by the time you buy the keyboard, which

1295
01:17:49,245 --> 01:17:53,205
Speaker 6:  everybody's gonna do. So whatever. But still, like as a, as a

1296
01:17:53,875 --> 01:17:57,565
Speaker 6:  what is the sort of easy computer to recommend to most

1297
01:17:57,565 --> 01:18:01,525
Speaker 6:  people, this feels plausible to me in a way that I'm

1298
01:18:01,525 --> 01:18:01,965
Speaker 6:  very excited about.

1299
01:18:02,225 --> 01:18:05,485
Speaker 7:  And the idea, the keyboard case doesn't have tower on it anymore.

1300
01:18:05,785 --> 01:18:08,285
Speaker 5:  Oh, good. So look, get all matted and nasty. Yeah,

1301
01:18:09,395 --> 01:18:12,925
Speaker 7:  They don't have that. They also launched the Surface laptop 13 inch with,

1302
01:18:12,925 --> 01:18:14,485
Speaker 7:  with an arm processor in it. So,

1303
01:18:15,265 --> 01:18:17,765
Speaker 5:  And that one oddly covered in Berber carpet.

1304
01:18:20,485 --> 01:18:24,345
Speaker 5:  That'd be amazing. Microsoft made like a full seventies basement

1305
01:18:24,645 --> 01:18:27,185
Speaker 5:  laptop. No, that one's just made of, it's made of normal

1306
01:18:27,465 --> 01:18:29,945
Speaker 6:  Materials. This one has a fan though, which I think is weird. That's the

1307
01:18:29,945 --> 01:18:32,665
Speaker 6:  only thing about the Surface laptop, which is a machine I have loved for

1308
01:18:32,665 --> 01:18:36,545
Speaker 6:  many years. They, they went ahead and gave this thing a fan, which strikes

1309
01:18:36,545 --> 01:18:40,185
Speaker 6:  me as maybe a good idea, but not a great sign of confidence

1310
01:18:40,365 --> 01:18:42,985
Speaker 6:  in the whole Qualcomm situation at the moment.

1311
01:18:44,225 --> 01:18:48,185
Speaker 5:  Yeah, I, you know, the goal for these devices is to

1312
01:18:48,185 --> 01:18:50,785
Speaker 5:  set the tone for the Windows ecosystem. I think they're, they do that job

1313
01:18:50,785 --> 01:18:51,185
Speaker 5:  just fine.

1314
01:18:51,455 --> 01:18:53,025
Speaker 6:  Yeah, they're great looking. Yeah,

1315
01:18:53,025 --> 01:18:56,785
Speaker 5:  They look good. Those videos are on YouTube. Go watch 'em.

1316
01:18:56,785 --> 01:18:58,785
Speaker 5:  They're great. Let us know what you think. Also

1317
01:18:58,785 --> 01:19:02,505
Speaker 7:  The testing, what's the smoke blowing around inside of the surface

1318
01:19:02,605 --> 01:19:05,345
Speaker 7:  laptop to show how the air air flows. I thought that was really cool. You

1319
01:19:05,345 --> 01:19:07,185
Speaker 7:  can see that on the social media and the article.

1320
01:19:07,215 --> 01:19:10,705
Speaker 6:  Dude, every time I watch one of these like stress testing gadgets, videos,

1321
01:19:10,885 --> 01:19:13,305
Speaker 6:  I'm always like, yeah, I've seen like a hundred of these before and then

1322
01:19:13,305 --> 01:19:16,185
Speaker 6:  11 minutes later I'm like, riveted. Yeah, just watching a thing, press the

1323
01:19:16,185 --> 01:19:19,665
Speaker 6:  button over and over and over again. I will watch an infinite number

1324
01:19:20,005 --> 01:19:21,825
Speaker 6:  of something beating up a gadget that

1325
01:19:22,085 --> 01:19:25,825
Speaker 5:  It turns out like four, five years into The Verge. I actually made a rule

1326
01:19:25,825 --> 01:19:29,665
Speaker 5:  that we were never gonna look at phone prototypes again. Like this. Just

1327
01:19:29,665 --> 01:19:33,105
Speaker 5:  like fundamentally stopped. We were like, I was like, great. Like, once again,

1328
01:19:33,105 --> 01:19:35,785
Speaker 5:  you've brought me to the quietest room in Northern California to look at

1329
01:19:35,785 --> 01:19:38,145
Speaker 5:  your headphones, like, we're just not doing this anymore. And then you're,

1330
01:19:38,335 --> 01:19:41,305
Speaker 5:  it's like kind of, can I go to the quiet room again?

1331
01:19:42,165 --> 01:19:45,785
Speaker 6:  You just have to acknowledge it for what It is like is this is, I'm, I'm

1332
01:19:45,785 --> 01:19:46,585
Speaker 6:  here once again

1333
01:19:46,765 --> 01:19:48,545
Speaker 5:  In the quietest room in California

1334
01:19:48,885 --> 01:19:51,865
Speaker 6:  To watch you punch a gadget in the face over and over again. I'm like, I'm

1335
01:19:51,865 --> 01:19:52,185
Speaker 6:  ready for it.

1336
01:19:52,185 --> 01:19:54,345
Speaker 5:  It's very good. Watch the video, it's great. All right, we gotta take a break.

1337
01:19:54,345 --> 01:19:57,385
Speaker 5:  We'll right back with lighting around unsponsored for flavor.

1338
01:21:27,195 --> 01:21:30,975
Speaker 5:  Alright, we're back to lightning round unsponsored

1339
01:21:31,155 --> 01:21:34,965
Speaker 5:  for extra flavor. See, the reason I keep saying that a joke,

1340
01:21:34,965 --> 01:21:38,405
Speaker 5:  which is I I agree with many of our audience is getting less funny over time,

1341
01:21:39,325 --> 01:21:42,085
Speaker 5:  although people do want the shirts. And so every time you hear someone say

1342
01:21:42,085 --> 01:21:45,685
Speaker 5:  who their sponsor is, you realize they have less flavor because we don't

1343
01:21:45,685 --> 01:21:49,465
Speaker 5:  take the money. You can't pay us to say anything. You can show up

1344
01:21:50,505 --> 01:21:54,265
Speaker 5:  suitcases full of money, they have Ford Fiesta's full of money, then

1345
01:21:54,415 --> 01:21:56,185
Speaker 5:  it's not a lot of money. You see? See what I'm getting? Well

1346
01:21:56,185 --> 01:21:58,825
Speaker 6:  That was good because if there's one thing that resuscitates a joke, it's

1347
01:21:59,105 --> 01:22:01,265
Speaker 6:  explaining it in great detail to the audience.

1348
01:22:01,505 --> 01:22:03,385
Speaker 5:  It's a small, it's a small car.

1349
01:22:05,285 --> 01:22:06,705
Speaker 5:  That's, that's the joke.

1350
01:22:08,335 --> 01:22:09,385
Speaker 5:  Frank's full of money.

1351
01:22:11,045 --> 01:22:14,105
Speaker 5:  You, we won't take it. Everyone else takes the money. If you wanna support

1352
01:22:14,105 --> 01:22:17,225
Speaker 5:  us, you, you can subscribe on The Verge dot com. You, you, you can pay us

1353
01:22:17,225 --> 01:22:20,185
Speaker 5:  money, but then you still can't tell us what to do. That's true. It's a great

1354
01:22:20,185 --> 01:22:20,385
Speaker 5:  deal.

1355
01:22:20,845 --> 01:22:23,945
Speaker 6:  But you get, you get a badge next to it when you yell at us about it.

1356
01:22:24,045 --> 01:22:27,105
Speaker 5:  That's true. That's good. And that's good. It's very good. All right, David,

1357
01:22:27,135 --> 01:22:27,505
Speaker 5:  it's time.

1358
01:22:28,125 --> 01:22:30,785
Speaker 6:  Oh, we're so back. Last week it was quiet.

1359
01:22:32,035 --> 01:22:35,095
Speaker 6:  Too quiet. And so this week, according to the notes that I'm seeing, we are

1360
01:22:35,155 --> 01:22:38,975
Speaker 6:  so, so, so very back on America's favorite podcast.

1361
01:22:38,975 --> 01:22:42,655
Speaker 6:  Within a podcast. Brendan Carr is a dummy neela, what do we have?

1362
01:22:42,965 --> 01:22:46,775
Speaker 5:  Brendan Carr was a real dummy this week and

1363
01:22:46,965 --> 01:22:50,845
Speaker 5:  like a shotgun blast of Brendan Carr this week. He, he was all

1364
01:22:50,845 --> 01:22:53,765
Speaker 5:  over the place. He was at a Milken conference. Alex Heath is actually at

1365
01:22:53,765 --> 01:22:56,245
Speaker 5:  this conference, but at this conference he gave a bunch of interviews to

1366
01:22:56,245 --> 01:22:59,935
Speaker 5:  a bunch of media outlets. Not us, Brendan Rude,

1367
01:23:00,435 --> 01:23:03,415
Speaker 5:  I'm just saying if, if you had reel stones you'd show up on our show, but

1368
01:23:03,415 --> 01:23:07,285
Speaker 5:  he gave a bunch of, you know, interviews to your cable news outlets. And

1369
01:23:07,505 --> 01:23:11,325
Speaker 5:  boy did he say a lot of things. So I wanna start with some classic

1370
01:23:11,355 --> 01:23:15,045
Speaker 5:  Brendan, like classic Republican FCC chairman

1371
01:23:15,095 --> 01:23:18,485
Speaker 5:  stuff because, you know, for all of his, many, many faults,

1372
01:23:18,835 --> 01:23:22,205
Speaker 5:  he's still a deregulatory telecom friendly

1373
01:23:23,285 --> 01:23:27,045
Speaker 5:  FCC chairman. And so he just says the things now

1374
01:23:27,275 --> 01:23:31,165
Speaker 5:  that you would expect people to say, but after all this time, you

1375
01:23:31,165 --> 01:23:34,965
Speaker 5:  just know that there are industry plants and they're dumb. So Brendan

1376
01:23:35,075 --> 01:23:38,645
Speaker 5:  said, during the Biden administration, the United States fell behind

1377
01:23:39,225 --> 01:23:42,925
Speaker 5:  on opening the airwaves up to private sector. And China now has an edge,

1378
01:23:44,055 --> 01:23:44,795
Speaker 5:  if you'll recall,

1379
01:23:45,135 --> 01:23:47,795
Speaker 6:  In American airwaves. I don't know. Okay,

1380
01:23:49,175 --> 01:23:53,075
Speaker 5:  But we, we might be losing the race to 5G again, you guys. Oh no.

1381
01:23:53,295 --> 01:23:57,155
Speaker 5:  And then to put a gloss on it, a 2025

1382
01:23:57,525 --> 01:24:01,355
Speaker 5:  gloss, he said AI is gonna be the use case that will drive

1383
01:24:01,425 --> 01:24:05,195
Speaker 5:  some of these telecom networks. That's why we have to get more spectrum out

1384
01:24:05,195 --> 01:24:08,435
Speaker 5:  there. We have to make it easier to invest in these high speed networks.

1385
01:24:08,735 --> 01:24:10,995
Speaker 5:  And it's like, bro, they're chatbots

1386
01:24:11,545 --> 01:24:15,395
Speaker 6:  Also. The, that, that quote is going to stick in my head

1387
01:24:15,395 --> 01:24:19,355
Speaker 6:  for a long time as the moment AI officially lost all meaning as

1388
01:24:19,395 --> 01:24:23,355
Speaker 6:  a thing that anyone agrees on. Yeah. Like what, what does that even mean?

1389
01:24:23,985 --> 01:24:27,675
Speaker 6:  It's AI is going to drive some of these telecom. Does he just mean it uses

1390
01:24:27,675 --> 01:24:28,155
Speaker 6:  bandwidth?

1391
01:24:28,575 --> 01:24:32,115
Speaker 5:  But again, the, It is currently expressed as

1392
01:24:32,275 --> 01:24:32,835
Speaker 5:  chatbots.

1393
01:24:34,435 --> 01:24:38,405
Speaker 5:  Like we're we're pumping vast amounts of UTF text

1394
01:24:38,405 --> 01:24:40,725
Speaker 5:  around the internet. Like what are you talking about?

1395
01:24:42,215 --> 01:24:45,825
Speaker 5:  He's a dummy, but he wants to have more spectrum auctions and sell more spectrum

1396
01:24:45,885 --> 01:24:49,505
Speaker 5:  to the big companies and provide more government subsidies to the biggest

1397
01:24:49,535 --> 01:24:53,395
Speaker 5:  telecom companies on in the country because we

1398
01:24:53,395 --> 01:24:54,275
Speaker 5:  have to win a fake race.

1399
01:24:56,455 --> 01:25:00,335
Speaker 5:  I don't know, man, if you, Brendan, if you can explain yourself,

1400
01:25:00,335 --> 01:25:03,615
Speaker 5:  you can show up, but you tell like the race to six G is coming

1401
01:25:04,745 --> 01:25:08,685
Speaker 5:  where you sit today, think just be ready. Take a

1402
01:25:08,685 --> 01:25:12,485
Speaker 5:  beat, take a moment, pull your car over and get ready to

1403
01:25:12,535 --> 01:25:15,645
Speaker 5:  shout down the people who say there is a race to six G because of ai.

1404
01:25:16,495 --> 01:25:20,325
Speaker 5:  Steal yourself for this battle because it's coming. And it won't be robot

1405
01:25:20,325 --> 01:25:23,205
Speaker 5:  surgery on a grape, which never happened.

1406
01:25:24,795 --> 01:25:28,565
Speaker 5:  It's gonna be ai. If we don't beat ai, China will win the race to six

1407
01:25:28,645 --> 01:25:32,085
Speaker 6:  G eight K, six G AI is a thing you're gonna start to hear, it's

1408
01:25:32,085 --> 01:25:34,965
Speaker 5:  Gonna be nuts. So that's just like, that's like your little amused boosh

1409
01:25:35,065 --> 01:25:38,525
Speaker 5:  of Brendan being a dummy. Right? Just saying the same dumb things.

1410
01:25:38,995 --> 01:25:42,365
Speaker 5:  Dumb that FCC chairman who want to give more spectrum and handouts to telecom

1411
01:25:42,565 --> 01:25:45,485
Speaker 5:  companies always say dumb, not

1412
01:25:46,075 --> 01:25:50,045
Speaker 5:  illegally dumb. Sure. Right. Not you're a traitor to America. Dumb.

1413
01:25:50,045 --> 01:25:50,325
Speaker 5:  Which

1414
01:25:50,325 --> 01:25:52,325
Speaker 6:  For Brendan is, you know, great job. Good job, Brendan.

1415
01:25:52,525 --> 01:25:56,325
Speaker 5:  Yeah. He made, he made he whatever, you know, he yeah, he, he stayed in his

1416
01:25:56,325 --> 01:25:56,525
Speaker 5:  lane

1417
01:25:56,525 --> 01:25:57,365
Speaker 6:  Regular dumb.

1418
01:25:57,555 --> 01:26:01,485
Speaker 5:  Then he started to get, well, I think traitor dumb. He

1419
01:26:01,485 --> 01:26:05,085
Speaker 5:  talked about the fccs investigation into Disney's DEI

1420
01:26:05,405 --> 01:26:09,045
Speaker 5:  practices. And he said the preliminary data from our investigation

1421
01:26:09,045 --> 01:26:12,405
Speaker 5:  indicates they're doing intentional discrimination potentially among race

1422
01:26:12,405 --> 01:26:15,845
Speaker 5:  and gender. That's a really big deal. It's a good thing by the way. 'cause

1423
01:26:15,845 --> 01:26:18,125
Speaker 5:  you know, Disney's slightly caved here. They've gotten rid of some of their

1424
01:26:18,125 --> 01:26:21,485
Speaker 5:  rules. You know, they're, they're, they're trying to appease the Trump administration.

1425
01:26:21,745 --> 01:26:25,005
Speaker 5:  He said it's a good thing they're headed in the right direction, but we wanna

1426
01:26:25,005 --> 01:26:28,685
Speaker 5:  make sure that it's in substance and not in name only. We have to like

1427
01:26:28,715 --> 01:26:32,365
Speaker 5:  take a look at the conduct that took place in the past that is a

1428
01:26:32,365 --> 01:26:35,925
Speaker 5:  potential problem under the fccs own equal opportunity

1429
01:26:35,925 --> 01:26:39,685
Speaker 5:  employment rules. Which by the way, they got rid of because Brendan

1430
01:26:39,865 --> 01:26:43,325
Speaker 5:  and he got rid of all the DI rules, right? So he's basically saying, I'm

1431
01:26:43,385 --> 01:26:46,605
Speaker 5:  I'm gonna continue to try to punish Disney for nothing.

1432
01:26:47,515 --> 01:26:48,425
Speaker 5:  Right? Like

1433
01:26:49,995 --> 01:26:53,885
Speaker 5:  they, Ariel was black one time and now Bob Iger has to go to jail and

1434
01:26:53,885 --> 01:26:55,685
Speaker 5:  Brendan Carr's gonna be the one to do it. Yeah.

1435
01:26:55,685 --> 01:26:58,605
Speaker 6:  They're doing intentional discrimination potentially among race and gender

1436
01:26:58,705 --> 01:27:02,645
Speaker 6:  is the whole problem that existed forever that we tried to

1437
01:27:02,645 --> 01:27:05,925
Speaker 6:  solve with DEI like that's very good was the problem.

1438
01:27:06,715 --> 01:27:10,445
Speaker 5:  It's very good. You dous. So he's still right. He's still

1439
01:27:10,445 --> 01:27:14,285
Speaker 5:  trying to punish companies for their speech and how they run their

1440
01:27:14,565 --> 01:27:18,365
Speaker 5:  companies. He is very interested in punishing companies for their speech.

1441
01:27:18,915 --> 01:27:21,885
Speaker 5:  This is the thing he cares about the most. And so

1442
01:27:22,705 --> 01:27:26,685
Speaker 5:  pushed on this, he tried to claim, and this is the

1443
01:27:26,685 --> 01:27:29,925
Speaker 5:  one that just really, really got me. He tried to claim

1444
01:27:30,915 --> 01:27:34,245
Speaker 5:  that the fccs investigation into CCBs

1445
01:27:34,745 --> 01:27:38,165
Speaker 5:  and the Kamala Harris interview on 60 Minutes and the

1446
01:27:38,275 --> 01:27:41,965
Speaker 5:  Paramount merger with Skydance Paramount owned CBS was

1447
01:27:41,995 --> 01:27:45,205
Speaker 5:  totally unrelated to Donald Trump's own

1448
01:27:45,665 --> 01:27:49,365
Speaker 5:  $10 billion lawsuit against CBS for the Kamala Harris interview.

1449
01:27:49,785 --> 01:27:53,125
Speaker 5:  He said, I haven't even read the president's complaint. We're just doing

1450
01:27:53,265 --> 01:27:56,965
Speaker 5:  law enforcement. These aren't threats. That's just a penalty where I can

1451
01:27:57,115 --> 01:28:00,605
Speaker 5:  take away the broadcast licenses. Which by the way, no FCC has done for decades

1452
01:28:00,675 --> 01:28:03,525
Speaker 5:  upon decades. And even the last time they did it required multiple and multiple

1453
01:28:03,525 --> 01:28:07,245
Speaker 5:  court cases and they still didn't get to do it. He said, no, no, no, no,

1454
01:28:07,245 --> 01:28:11,215
Speaker 5:  no. There are three separate things that are going on. President Trump has

1455
01:28:11,215 --> 01:28:14,935
Speaker 5:  his lawsuit against CBS that's in state court. I haven't read that complaint.

1456
01:28:15,055 --> 01:28:18,335
Speaker 5:  I don't know what all the complaints are. There's the transaction before

1457
01:28:18,335 --> 01:28:21,815
Speaker 5:  US Paramount and Skydance, and then there's a 60 minutes complaint. Those

1458
01:28:21,815 --> 01:28:24,975
Speaker 5:  last two have to do with The FCC. The first one doesn't. We're staying in

1459
01:28:24,975 --> 01:28:27,895
Speaker 5:  our lie and we're just reviewing it. It's just the normal course of review

1460
01:28:28,145 --> 01:28:32,015
Speaker 5:  where we take a normal interview with a politician during an election

1461
01:28:32,315 --> 01:28:35,695
Speaker 5:  and we threaten to punish a company as hard as we can because they want get

1462
01:28:35,695 --> 01:28:39,665
Speaker 5:  a merger approved. And what we want from them is a settlement

1463
01:28:39,965 --> 01:28:43,745
Speaker 5:  saying that they will not do quote unquote news distortion, which means they

1464
01:28:43,745 --> 01:28:47,395
Speaker 5:  will tilt their coverage towards us. But that has nothing to do with Donald Trump,

1465
01:28:47,845 --> 01:28:51,655
Speaker 5:  even though no, of course not Two weeks ago, Brendan is proudly wearing

1466
01:28:51,655 --> 01:28:55,455
Speaker 5:  the literal gold bust of Donald Trump's head as a pin on his chest.

1467
01:28:56,275 --> 01:29:00,215
Speaker 5:  You have no credibility, Brendan, like none at all, because you love

1468
01:29:00,215 --> 01:29:03,845
Speaker 5:  to spout out on social media and be a MAGA warrior. And then when you're

1469
01:29:03,845 --> 01:29:07,605
Speaker 5:  pressed on it, you pretend that you are a normal gray suit

1470
01:29:07,605 --> 01:29:11,045
Speaker 5:  government bureaucrat just doing the normal process of review. And you can't

1471
01:29:11,045 --> 01:29:14,735
Speaker 5:  have both, you can't, you can't be a totally corrupt

1472
01:29:15,325 --> 01:29:19,215
Speaker 5:  like Censorious bureaucrat and then on the other

1473
01:29:19,215 --> 01:29:22,655
Speaker 5:  hand be like, I'm just a lawyer doing my job. By the way, you know who agrees

1474
01:29:22,655 --> 01:29:26,315
Speaker 5:  with me about this? It's not a bunch of weirdo progressive libs.

1475
01:29:26,545 --> 01:29:30,155
Speaker 5:  It's the American Enterprise Institute. One of the most conservative think

1476
01:29:30,205 --> 01:29:34,155
Speaker 5:  tanks in America. Published an article this week called Policing News.

1477
01:29:34,435 --> 01:29:38,315
Speaker 5:  Policing, DEI The FCC s Shifting Priorities Erode Its Credibility.

1478
01:29:38,425 --> 01:29:42,275
Speaker 5:  It's an entire article about Brendan destroying the credibility

1479
01:29:42,275 --> 01:29:46,075
Speaker 5:  of this agency by doing censorship from

1480
01:29:46,595 --> 01:29:49,995
Speaker 5:  a conservative think tank. Here's just a quote in short, the

1481
01:29:50,425 --> 01:29:53,875
Speaker 5:  fccs mission cre panders to the president eroding the commission's stability

1482
01:29:53,875 --> 01:29:57,755
Speaker 5:  because it's hard to predict the regulatory direction which it will head

1483
01:29:57,755 --> 01:30:01,525
Speaker 5:  next other than a pro-Trump one. That's not

1484
01:30:01,585 --> 01:30:03,725
Speaker 5:  me. It's not, you know, uncle woke over here

1485
01:30:05,865 --> 01:30:09,765
Speaker 5:  that's one of the most conservative think tanks in America, agreeing with

1486
01:30:09,775 --> 01:30:13,645
Speaker 5:  bipartisan groups of former FCC commissioners agreeing with

1487
01:30:13,645 --> 01:30:16,205
Speaker 5:  everybody who's ever looked at Stephen Levy wrote a piece in Wired about

1488
01:30:16,205 --> 01:30:20,065
Speaker 5:  this a couple weeks ago. Down the line, everyone looks at this and

1489
01:30:20,065 --> 01:30:23,625
Speaker 5:  says, this man who is in charge of our nation's communications infrastructure

1490
01:30:23,955 --> 01:30:27,785
Speaker 5:  keeps issuing threats about content to broadcasters, to cable

1491
01:30:27,785 --> 01:30:30,785
Speaker 5:  networks, to internet companies, has no power to regulate. And it's because

1492
01:30:31,125 --> 01:30:34,995
Speaker 5:  he wants to censor speech and these little claims that he's just doing the

1493
01:30:35,235 --> 01:30:38,755
Speaker 5:  normal review with a penalty that has essentially never

1494
01:30:39,145 --> 01:30:42,715
Speaker 5:  been used in the course of a merger review to take away broadcast

1495
01:30:42,715 --> 01:30:46,705
Speaker 5:  licenses for news distortion. You don't get to do that when you

1496
01:30:46,705 --> 01:30:49,825
Speaker 5:  also wear the gold head of Donald Trump on your chest. Where when you're

1497
01:30:49,825 --> 01:30:53,425
Speaker 5:  also a MAGA warrior, when you're also on Twitter all day every day,

1498
01:30:53,555 --> 01:30:56,785
Speaker 5:  responding to reply guy saying you're gonna punish companies for their speech.

1499
01:30:57,525 --> 01:31:01,505
Speaker 5:  As always, Brendan, you're welcome to come on this show, which is much harder

1500
01:31:01,695 --> 01:31:05,065
Speaker 5:  than the softball cable networks that you go on and try to answer some of

1501
01:31:05,065 --> 01:31:08,785
Speaker 5:  these questions and see if you can respond with rigor and care to the

1502
01:31:08,785 --> 01:31:12,305
Speaker 5:  accusations that your desire to censor speech

1503
01:31:12,715 --> 01:31:16,385
Speaker 5:  rises to the level of not just first Amendment violation, but

1504
01:31:16,385 --> 01:31:20,145
Speaker 5:  outright hostility to our constitution. Anytime you want this show decoder,

1505
01:31:20,525 --> 01:31:23,265
Speaker 5:  you can just hang out with David in a coffee shop and I'll film it from around

1506
01:31:23,265 --> 01:31:24,065
Speaker 5:  the corner with an iPhone.

1507
01:31:24,265 --> 01:31:24,625
Speaker 6:  Sounds great.

1508
01:31:24,855 --> 01:31:27,505
Speaker 5:  However you wanna do it, but I don't think you can. I think you're a coward

1509
01:31:28,205 --> 01:31:31,375
Speaker 5:  that has been Brendan Carr's dummy America's favorite podcast with our podcast.

1510
01:31:31,675 --> 01:31:35,655
Speaker 6:  Oh, we're so back. Felt good. I didn't like taking a weed. We

1511
01:31:35,965 --> 01:31:39,015
Speaker 6:  last week we were just like, yeah, Brendan kind of still seems like he sucks

1512
01:31:39,015 --> 01:31:41,815
Speaker 5:  Better. Don't worry, he'll talk again better. Yeah. He, he can't show, he

1513
01:31:41,815 --> 01:31:45,255
Speaker 5:  can't help himself. He's saying he's loving his job. So being America's chief

1514
01:31:45,255 --> 01:31:47,895
Speaker 5:  sensor, all right, we need a pallet cleanser. David, what you got for me?

1515
01:31:47,895 --> 01:31:51,215
Speaker 6:  Let's talk about Netflix just for a minute. Yeah. Because Netflix had

1516
01:31:51,845 --> 01:31:55,455
Speaker 6:  kind of a sneakily interesting week product wise. First

1517
01:31:56,195 --> 01:32:00,165
Speaker 6:  big new update for the, the TV app for Netflix, which I find

1518
01:32:00,585 --> 01:32:04,285
Speaker 6:  my like hobby horse of reporting is about streaming

1519
01:32:04,285 --> 01:32:08,005
Speaker 6:  recommendations, like how to tell you what you wanna watch. I think It is

1520
01:32:08,005 --> 01:32:11,125
Speaker 6:  just the most fascinating thing in the world. And what Netflix did in part

1521
01:32:11,545 --> 01:32:14,965
Speaker 6:  was that banner at the top of the Netflix app, it's now much bigger.

1522
01:32:15,745 --> 01:32:18,805
Speaker 6:  And the, the one thing that that communicates is that Netflix

1523
01:32:19,205 --> 01:32:23,165
Speaker 6:  believes more strongly than ever that it can just put something in

1524
01:32:23,165 --> 01:32:26,245
Speaker 6:  front of you and you'll probably click play. And so I think that's fascinating.

1525
01:32:26,955 --> 01:32:30,685
Speaker 6:  This is much less like a discovery thing that we're, we're

1526
01:32:30,685 --> 01:32:34,285
Speaker 6:  moving rapidly away from the like infinite, you know, side

1527
01:32:34,285 --> 01:32:38,165
Speaker 6:  scrolling tiles to just like a big ass thing that's

1528
01:32:38,165 --> 01:32:42,005
Speaker 6:  like, watch this you clown and Netflix is

1529
01:32:42,005 --> 01:32:45,525
Speaker 6:  betting that that's gonna work. And I find that really fascinating. But thing

1530
01:32:45,525 --> 01:32:49,365
Speaker 6:  number two, and Neli, I believe you are going to

1531
01:32:49,735 --> 01:32:53,085
Speaker 6:  maybe rightfully, maybe wrongfully take a victory lap on this one. Netflix

1532
01:32:53,145 --> 01:32:57,125
Speaker 6:  is just doing TikTok now. Yeah. Like imagine Netflix did TikTok. That's what

1533
01:32:57,125 --> 01:32:57,485
Speaker 6:  it's doing.

1534
01:32:58,465 --> 01:33:02,245
Speaker 5:  So it's interesting 'cause it, it's not TikTok it, it's in the mobile

1535
01:33:02,265 --> 01:33:06,085
Speaker 5:  app. They're gonna do a, a vertical swipey feed of stuff

1536
01:33:06,085 --> 01:33:09,775
Speaker 5:  you might wanna watch. We haven't seen it. Right. There's this quote

1537
01:33:09,925 --> 01:33:13,815
Speaker 5:  from Eunice Kim, the cheap product officer at Netflix. We know swiping through

1538
01:33:14,015 --> 01:33:16,415
Speaker 5:  vertical feed on social media apps is an easy way to browse video content.

1539
01:33:16,875 --> 01:33:19,775
Speaker 5:  We know our members love to browse clips and trailers by the next obsession.

1540
01:33:20,805 --> 01:33:24,055
Speaker 5:  Sort of w what people love to do is watch

1541
01:33:24,325 --> 01:33:27,975
Speaker 5:  pirated clips of movies from the middle on TikTok. And so

1542
01:33:28,355 --> 01:33:32,175
Speaker 5:  that's the experience you have to recreate, not you're gonna swipe

1543
01:33:32,275 --> 01:33:33,935
Speaker 5:  and then watch a trailer from the beginning.

1544
01:33:34,435 --> 01:33:34,655
Speaker 6:  Yes.

1545
01:33:35,105 --> 01:33:39,095
Speaker 5:  Right. Like that's not it. Like what It is. I'll just keep using a few

1546
01:33:39,095 --> 01:33:42,135
Speaker 5:  Good men 'cause it's very funny and TikTok just insists that I keep watching

1547
01:33:42,135 --> 01:33:45,735
Speaker 5:  this movie maybe 'cause I watch every clip to the end and it's like, do you

1548
01:33:45,735 --> 01:33:46,495
Speaker 5:  want more of this? It's like I do.

1549
01:33:48,155 --> 01:33:52,005
Speaker 5:  It's, we're gonna drop you in the middle of some of the most impactful scenes

1550
01:33:52,005 --> 01:33:55,845
Speaker 5:  from the movie Ice Cold. And there's comments next to it, most of which

1551
01:33:55,845 --> 01:33:58,245
Speaker 5:  are what movie is this? And then people lying, which is very entertaining.

1552
01:33:59,315 --> 01:34:03,245
Speaker 5:  It's that. Yeah. So I'm curious how they actually execute this

1553
01:34:03,245 --> 01:34:06,725
Speaker 5:  idea. I it, I've asked everybody. I've asked the CEO Tuby, I've asked Netflix

1554
01:34:06,725 --> 01:34:10,595
Speaker 5:  itself, the their executives have been on the show. I've asked them and

1555
01:34:10,595 --> 01:34:12,555
Speaker 5:  they're all like, yeah, it's hard 'cause we don't have the rights to all

1556
01:34:12,625 --> 01:34:16,295
Speaker 5:  this content. Right. And you know, TikTok, there's people are stealing it,

1557
01:34:16,295 --> 01:34:18,695
Speaker 5:  so of course you just put the best part. But like the studio is gonna say

1558
01:34:18,925 --> 01:34:22,295
Speaker 5:  Netflix owns a bunch of its content so it could do it. Right.

1559
01:34:23,205 --> 01:34:27,115
Speaker 5:  We'll see. But I, I think in many ways this is the new discovery

1560
01:34:27,115 --> 01:34:30,995
Speaker 5:  method and you know, and David is just like, oh, the laziest TV

1561
01:34:30,995 --> 01:34:34,195
Speaker 5:  watcher in the world. He is like, just play me whatever. Like this is, this

1562
01:34:34,195 --> 01:34:37,915
Speaker 5:  is the modern, just, I wanna start watching something so I'm just gonna

1563
01:34:37,915 --> 01:34:41,755
Speaker 5:  flick channels. It's that mechanism. But for like the social media

1564
01:34:41,755 --> 01:34:42,635
Speaker 5:  social video age.

1565
01:34:42,945 --> 01:34:45,635
Speaker 6:  Yeah. But it's, it's funny you mention the, the trailers thing. 'cause that,

1566
01:34:45,635 --> 01:34:49,475
Speaker 6:  that's been the thing I've been thinking about too. Like Spotify has

1567
01:34:49,475 --> 01:34:53,315
Speaker 6:  been desperately trying to do this with music and podcasts too. And

1568
01:34:53,865 --> 01:34:57,485
Speaker 6:  like you go to the podcasts page on Spotify and it just like

1569
01:34:57,485 --> 01:35:01,325
Speaker 6:  starts playing a podcast right there in line and it

1570
01:35:01,325 --> 01:35:05,165
Speaker 6:  starts at the beginning, which is a huge mistake. And, and like you're saying,

1571
01:35:05,195 --> 01:35:09,125
Speaker 6:  when you're in this browse thing, showing me the beginning of the movie

1572
01:35:09,145 --> 01:35:12,925
Speaker 6:  is the wrong thing to do, showing me like the, the

1573
01:35:12,925 --> 01:35:16,605
Speaker 6:  thing that is happening that is gonna catch my eye is the thing to do. And

1574
01:35:16,605 --> 01:35:19,645
Speaker 6:  we're already seeing this in trailers now. Like the, there's now the trailer

1575
01:35:19,705 --> 01:35:23,005
Speaker 6:  before the trailer where they, they show like eight seconds and then it's

1576
01:35:23,005 --> 01:35:26,165
Speaker 6:  like the trailer for Thunderbolts begins now. And it's like, I thought it

1577
01:35:26,165 --> 01:35:29,845
Speaker 6:  began when I clicked play on this YouTube video 19 seconds ago. So

1578
01:35:30,035 --> 01:35:33,565
Speaker 6:  already that has shifted to like grab you faster

1579
01:35:34,265 --> 01:35:38,205
Speaker 6:  and I think either a bunch of content makers are gonna

1580
01:35:38,205 --> 01:35:41,365
Speaker 6:  start to have to like be smarter about how they make these trailers for these

1581
01:35:41,365 --> 01:35:45,245
Speaker 6:  people or Netflix is gonna have to get really smart about saying, okay, this

1582
01:35:45,265 --> 01:35:49,045
Speaker 6:  is the grabs 30 seconds of this show, which is the thing Netflix

1583
01:35:49,375 --> 01:35:52,725
Speaker 6:  knows. Like, it it sees how people interact with this stuff. Oh, that's

1584
01:35:52,725 --> 01:35:53,885
Speaker 5:  Really interesting. I hadn't thought about that.

1585
01:35:53,885 --> 01:35:57,765
Speaker 6:  Netflix can theoretically figure out and it could just, it could play me

1586
01:35:57,785 --> 01:36:01,125
Speaker 6:  the thing that is likely to work for me in my own feed.

1587
01:36:01,275 --> 01:36:05,085
Speaker 5:  Does Netflix know that? I mean, Netflix watch data is like mostly linear,

1588
01:36:05,255 --> 01:36:08,605
Speaker 5:  right? What they know is when people stop watching, they don't know when

1589
01:36:08,605 --> 01:36:12,525
Speaker 5:  most people begin watching. 'cause for Netflix everybody begins

1590
01:36:12,525 --> 01:36:13,005
Speaker 5:  at the beginning,

1591
01:36:13,255 --> 01:36:13,605
Speaker 6:  Right?

1592
01:36:14,075 --> 01:36:17,765
Speaker 7:  Well they they know which ones you come back to. Sure. And obviously maybe

1593
01:36:17,835 --> 01:36:21,645
Speaker 7:  they, they maybe their, their, their producer here is actually the people

1594
01:36:21,665 --> 01:36:24,445
Speaker 7:  on TikTok who post the clips and they just go back and take those clips.

1595
01:36:24,795 --> 01:36:28,165
Speaker 5:  That would be amazing if Netflix was scraping TikTok to see which Netflix

1596
01:36:28,165 --> 01:36:30,885
Speaker 5:  shows got the most reactions as pirated TikTok videos.

1597
01:36:31,715 --> 01:36:34,565
Speaker 5:  There's something here that's really powerful. Yeah. It just, it all comes

1598
01:36:34,565 --> 01:36:37,925
Speaker 5:  down to the execution and the thing that they're competing against is infinity.

1599
01:36:37,925 --> 01:36:41,765
Speaker 5:  People trying infinity variations on infinity free content that

1600
01:36:41,765 --> 01:36:45,565
Speaker 5:  they're not paying for, which is like always the problem. But to me

1601
01:36:45,565 --> 01:36:48,765
Speaker 5:  this is, it's so the future of discovery, I'm, I'm kind of excited to try

1602
01:36:48,765 --> 01:36:48,885
Speaker 5:  that.

1603
01:36:49,275 --> 01:36:52,085
Speaker 6:  It's gonna be fascinating. And to Richard's point, there, there is this source

1604
01:36:52,185 --> 01:36:56,125
Speaker 6:  of tons of good data on what people actually care about.

1605
01:36:57,345 --> 01:37:00,005
Speaker 6:  It just, it's just all illegal and doesn't belong to Netflix.

1606
01:37:01,885 --> 01:37:05,685
Speaker 7:  I, I wonder if, what if the, the use of it isn't even

1607
01:37:05,745 --> 01:37:08,925
Speaker 7:  as much. Okay. So you watch this and then you watch the video, but just,

1608
01:37:09,345 --> 01:37:12,245
Speaker 7:  it means that you're using the Netflix app more and that you're less willing

1609
01:37:12,265 --> 01:37:15,725
Speaker 7:  to let your Netflix subscription die. Whether or not you watch the show

1610
01:37:16,285 --> 01:37:19,205
Speaker 7:  anything, but it's just something you can use to eat time, which is what

1611
01:37:19,205 --> 01:37:22,485
Speaker 7:  a lot of these apps really are. You don't know what to watch, but you just

1612
01:37:22,485 --> 01:37:25,525
Speaker 7:  don't want to scroll past some men talking on a podcast

1613
01:37:26,855 --> 01:37:27,315
Speaker 5:  Or babies.

1614
01:37:28,065 --> 01:37:31,755
Speaker 6:  Yeah. Like a what if step two of this thing is, it just becomes another

1615
01:37:31,895 --> 01:37:35,755
Speaker 6:  set of things to watch on Netflix that they're just like, oh, you're, you're

1616
01:37:35,955 --> 01:37:39,915
Speaker 6:  watching, instead of watching Netflix shows, you're watching things 18 seconds

1617
01:37:39,935 --> 01:37:40,395
Speaker 6:  at a time.

1618
01:37:40,975 --> 01:37:44,595
Speaker 5:  That's fine. And Netflix is right. We compete with death or whatever

1619
01:37:44,705 --> 01:37:48,315
Speaker 5:  they say, yeah, sleep not death. Netflix

1620
01:37:48,575 --> 01:37:51,195
Speaker 5:  is like-and is like, I will defeat death.

1621
01:37:53,975 --> 01:37:56,690
Speaker 5:  But they're like, we compete with like, they're fully in the attention economy.

1622
01:37:56,690 --> 01:38:00,165
Speaker 5:  Yeah. They've known it for a long time. Their competitor is YouTube and video

1623
01:38:00,165 --> 01:38:04,005
Speaker 5:  games. And so if you are in the Netflix app watching short

1624
01:38:04,005 --> 01:38:06,605
Speaker 5:  clips of Bridger and all that, I don't think they care. 'cause they already

1625
01:38:06,605 --> 01:38:09,405
Speaker 5:  own the content. That's the thing that makes this harder for every other

1626
01:38:09,405 --> 01:38:13,135
Speaker 5:  service. Right? Like even Apple TV can't do it.

1627
01:38:13,315 --> 01:38:16,735
Speaker 5:  You, you think Apple's paying for all that stuff, but like many other big

1628
01:38:16,735 --> 01:38:19,655
Speaker 5:  production companies and studios actually make the majority of their content

1629
01:38:19,665 --> 01:38:23,315
Speaker 5:  still. So these, they're all licensing deals and they, they don't have all

1630
01:38:23,315 --> 01:38:26,235
Speaker 5:  these moves. Netflix just outright owns a bunch of its stuff. Yeah.

1631
01:38:28,055 --> 01:38:30,355
Speaker 5:  So, well, let's see. I'm excited for this one. It, there's something here

1632
01:38:30,355 --> 01:38:31,715
Speaker 5:  that I think is very powerful. The TV

1633
01:38:31,855 --> 01:38:35,635
Speaker 7:  UI is weird because it's like three different versions of interfaces. They

1634
01:38:35,635 --> 01:38:38,755
Speaker 7:  already did. But I don't know. Back to the future, I guess.

1635
01:38:39,035 --> 01:38:42,235
Speaker 6:  I mean this is like the Netflix story in a nutshell. They're like, what if

1636
01:38:42,235 --> 01:38:45,675
Speaker 6:  we just, what if we just stretched some boxes and condensed some boxes?

1637
01:38:46,615 --> 01:38:47,225
Speaker 6:  Does that help?

1638
01:38:49,575 --> 01:38:51,945
Speaker 5:  What if we made the thumbnails bigger and the text smaller?

1639
01:38:52,665 --> 01:38:56,065
Speaker 7:  I just want them to never show me a summary that is like a clip of a review

1640
01:38:56,125 --> 01:38:58,585
Speaker 7:  of the show. That doesn't tell me anything about what It is in the show.

1641
01:38:59,145 --> 01:39:02,025
Speaker 7:  I don't, I don't care if XY, Z website said it was cool.

1642
01:39:02,055 --> 01:39:05,965
Speaker 6:  Yeah. Yeah. What is it about? Fascinating deadline. And you're like,

1643
01:39:05,965 --> 01:39:08,885
Speaker 6:  cool. Thanks Netflix. This is great. That's exactly what I needed to know.

1644
01:39:09,605 --> 01:39:12,645
Speaker 5:  You know, apple doesn't show you the Rotten Tomato scores of its own shows.

1645
01:39:14,015 --> 01:39:16,875
Speaker 5:  If you ever like, look and oh that's, you know, the Apple TV interface usually

1646
01:39:16,875 --> 01:39:19,355
Speaker 5:  has that little bit of metadata. But for Apple stuff it's like not there.

1647
01:39:19,745 --> 01:39:23,705
Speaker 5:  It's very good. All right. I wanna end on two more gadgets. Well,

1648
01:39:23,705 --> 01:39:27,425
Speaker 5:  one more gadget really, I just wanna call it the fact that gen two E reviewed

1649
01:39:27,455 --> 01:39:31,145
Speaker 5:  four robot lawnmowers and maybe the most gen two e

1650
01:39:31,685 --> 01:39:35,655
Speaker 5:  moment that can exist. You, you have to read it. It's very

1651
01:39:35,655 --> 01:39:39,055
Speaker 5:  good. It includes the line. They don't cut your lawn, they shave it.

1652
01:39:39,575 --> 01:39:41,415
Speaker 5:  Perfect. Per perfect.

1653
01:39:42,675 --> 01:39:46,055
Speaker 6:  Jen went through a lot to get that story done and it's very good. So everyone

1654
01:39:46,055 --> 01:39:49,855
Speaker 6:  should go read it and buy a lawnmower Just to support Jen, because this is

1655
01:39:49,855 --> 01:39:53,415
Speaker 6:  like a year of work to try and get these things to shave her lawn.

1656
01:39:53,695 --> 01:39:55,655
Speaker 5:  I don't know. You should buy the lawnmower. I think you should read the story

1657
01:39:55,715 --> 01:39:58,735
Speaker 5:  and subscribe to the Vich. You can wait, wait a couple years in the lawnmower.

1658
01:39:58,735 --> 01:40:01,575
Speaker 6:  That's fair. Read the story and subscribe to the Vich and then maybe don't

1659
01:40:01,575 --> 01:40:02,095
Speaker 6:  buy a lawnmower.

1660
01:40:02,095 --> 01:40:05,935
Speaker 5:  Okay. And then I wanna end with just a tiny we party speaker update.

1661
01:40:06,125 --> 01:40:09,015
Speaker 5:  It's very hard to top party speaker in front of a fighter jet, which is what

1662
01:40:09,015 --> 01:40:12,295
Speaker 5:  we had last week. Thank, thank you again for sending us that photo. We got

1663
01:40:12,295 --> 01:40:16,035
Speaker 5:  some more photos, many more photos of party

1664
01:40:16,035 --> 01:40:19,115
Speaker 5:  speakers in boxes, in stores. Again, I love you.

1665
01:40:20,035 --> 01:40:24,025
Speaker 5:  I'm happy that when you see a party speaker, you think of us. It's very validating.

1666
01:40:24,365 --> 01:40:28,315
Speaker 5:  I'm glad Where, I'm glad we're a community. That's not it. Like

1667
01:40:28,315 --> 01:40:32,215
Speaker 5:  you, it's in the wild Right Guys in

1668
01:40:32,215 --> 01:40:36,115
Speaker 5:  Europe on a scooter using it as a chair. We did get one photo

1669
01:40:36,115 --> 01:40:39,075
Speaker 5:  that I was really good. I don't wanna run it 'cause it was, it's an audible

1670
01:40:39,185 --> 01:40:42,535
Speaker 5:  marketing campaign, you know? Yeah.

1671
01:40:43,375 --> 01:40:47,195
Speaker 5:  But Audible apparently puts people in marathons wearing

1672
01:40:47,195 --> 01:40:49,635
Speaker 5:  party speakers as backpacks playing audio books.

1673
01:40:51,175 --> 01:40:54,875
Speaker 5:  And so you can like run along in the race. That's good.

1674
01:40:54,975 --> 01:40:58,795
Speaker 5:  Listen, and that's like multiple different books. Like there's a rom-com

1675
01:40:58,795 --> 01:41:02,555
Speaker 5:  book, there's like another book and you can follow it. Very good. Audible

1676
01:41:02,555 --> 01:41:02,835
Speaker 5:  stunt.

1677
01:41:03,175 --> 01:41:07,115
Speaker 6:  We got one like up on a shelf in a gym. A lot of people seeing party speakers

1678
01:41:07,115 --> 01:41:10,675
Speaker 6:  at their gym. One at a, at a, I think it was a

1679
01:41:11,035 --> 01:41:14,915
Speaker 6:  butcher shop in India. I think the, the thing that we've done that I'm very

1680
01:41:14,915 --> 01:41:18,235
Speaker 6:  proud of is we have, we have taught people to see

1681
01:41:19,155 --> 01:41:22,095
Speaker 6:  the party speakers in the wild. Yeah. It turns out they're everywhere and

1682
01:41:22,095 --> 01:41:25,375
Speaker 6:  we were all just trained to not notice them. Yeah. It's like a real fish

1683
01:41:25,695 --> 01:41:29,175
Speaker 6:  noticing the water situation that's happening right now. And we are discovering

1684
01:41:29,175 --> 01:41:33,015
Speaker 6:  the extent to which the world has been infiltrated by party speakers. And

1685
01:41:33,015 --> 01:41:35,655
Speaker 6:  so people who were like, I've been to this place every day for five years

1686
01:41:35,655 --> 01:41:39,575
Speaker 6:  and they never noticed this giant fifth grader sized speaker

1687
01:41:40,085 --> 01:41:43,855
Speaker 6:  blowing audio at me all day. Now we're seeing it.

1688
01:41:43,945 --> 01:41:45,055
Speaker 6:  We've, we've changed the world.

1689
01:41:45,475 --> 01:41:49,415
Speaker 5:  And then we received one person who was like, how do you not know about this

1690
01:41:49,655 --> 01:41:53,495
Speaker 5:  phenomenon called sound clashes that apparently take place in Jamaica and

1691
01:41:53,495 --> 01:41:57,295
Speaker 5:  India where they just build giant speaker systems

1692
01:41:57,395 --> 01:42:00,615
Speaker 5:  and play them at each other? And I was like, first of all, that's I, how

1693
01:42:00,615 --> 01:42:01,095
Speaker 5:  did I not know

1694
01:42:01,095 --> 01:42:02,575
Speaker 6:  This? Like battle bots, but it's party speakers.

1695
01:42:02,575 --> 01:42:06,135
Speaker 5:  But those aren't party speakers. Those are giant custom audio rigs. So oh,

1696
01:42:06,565 --> 01:42:09,775
Speaker 5:  honorable mention to sound clashes. I encourage you to go watch the videos

1697
01:42:09,875 --> 01:42:13,805
Speaker 5:  on YouTube. They're wild, different, different, right?

1698
01:42:13,805 --> 01:42:17,205
Speaker 5:  That's like the people who, if you get in the car audio, they, people are

1699
01:42:17,205 --> 01:42:19,525
Speaker 5:  like, oh, I really care about SPL and what they're just talking about, how

1700
01:42:19,525 --> 01:42:22,965
Speaker 5:  loud their sub buffers are. I love you, you know, that's great.

1701
01:42:23,075 --> 01:42:23,565
Speaker 5:  Forward

1702
01:42:23,565 --> 01:42:26,525
Speaker 7:  Facing or rear facing neli, are you stealing your trunk or, or are you not

1703
01:42:26,605 --> 01:42:27,885
Speaker 7:  stealing your trunk? We gotta talk about it.

1704
01:42:29,195 --> 01:42:32,485
Speaker 5:  This is a, this is a big issue. You know, what's up? Like

1705
01:42:32,785 --> 01:42:36,325
Speaker 5:  that's a different, we're we'll do a whole other series on that someday.

1706
01:42:37,345 --> 01:42:39,845
Speaker 5:  And I appreciate that. People were like, oh, you wanna talk about party speaker?

1707
01:42:39,905 --> 01:42:42,725
Speaker 5:  But I get it. I understand the connection that was made in your mind.

1708
01:42:44,075 --> 01:42:46,845
Speaker 5:  Different. We're we're, we're still on party speakers.

1709
01:42:47,215 --> 01:42:48,645
Speaker 6:  We'll get to that. Alright,

1710
01:42:48,895 --> 01:42:52,805
Speaker 5:  We're way over. Just so over. We love you all. That's it.

1711
01:42:52,805 --> 01:42:53,805
Speaker 5:  That's The Vergecast. Bye.

1712
01:42:59,385 --> 01:43:02,405
Speaker 9:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1713
01:43:02,405 --> 01:43:06,285
Speaker 9:  you. Give us a call at eight six six Verge one one.

1714
01:43:06,465 --> 01:43:09,965
Speaker 9:  The Vergecast is a production of The Verge and the Vox Media Podcast network.

1715
01:43:10,305 --> 01:43:14,205
Speaker 9:  Our show is produced by Will Poor Eric Gomez and Brandon Keefer. And

1716
01:43:14,205 --> 01:43:15,525
Speaker 9:  that's it. We'll see you next week.

