1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 21df4be7-3faf-43bb-86ef-ef5f4d608bdb
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-7201974140045927629/-1505402745094791194/s93290-US-3046s-1731842433.mp3
Description: For the third episode in our series about the future of music, we talk with Ge Wang. Ge is a professor at Stanford, a co-founder of Smule, the conductor of Stanfordâ€™s laptop orchestra, and has been at the center of technology and artistry for most of his life. We talk about how humans can use AI without giving in to it, what it means to truly play with technology, and the value of art and creativity and friction when it feels like all those things are being taken away.

2
00:00:03,185 --> 00:00:07,115
Speaker 2:  Welcome To The Vergecast, the flagship podcast of music apps

3
00:00:07,335 --> 00:00:11,195
Speaker 2:  you play on your phone with your mouth. I'm Iran David Pierce, and

4
00:00:11,235 --> 00:00:14,315
Speaker 2:  I am at Union Station in Washington, DC. It's like

5
00:00:14,315 --> 00:00:18,155
Speaker 2:  4 54 in the morning and I'm on my way to New York to go do some

6
00:00:18,595 --> 00:00:21,875
Speaker 2:  planning meetings with a bunch of other VERGE folks. I've been at The Verge

7
00:00:21,895 --> 00:00:24,955
Speaker 2:  for like two and a half years now, and there are a bunch of coworkers that

8
00:00:24,955 --> 00:00:28,515
Speaker 2:  I've just never met in person. It's weird post pandemic

9
00:00:28,835 --> 00:00:32,155
Speaker 2:  universe we live in, but I'm gonna go meet some of them in person. This is

10
00:00:32,295 --> 00:00:35,635
Speaker 2:  the third and last episode in our mini series about the future of music.

11
00:00:35,925 --> 00:00:39,875
Speaker 2:  We've talked about Track star and TikTok. We've talked about autotune, and

12
00:00:39,875 --> 00:00:43,635
Speaker 2:  today I'm talking with a Stanford professor named Ga Wong. GA

13
00:00:43,735 --> 00:00:47,635
Speaker 2:  is fascinating. He's been in the music world for a long time, but

14
00:00:47,635 --> 00:00:50,955
Speaker 2:  he's also an academic. He's also a former

15
00:00:51,155 --> 00:00:54,235
Speaker 2:  entrepreneur who built a company called SMU that you might have heard of.

16
00:00:54,665 --> 00:00:58,475
Speaker 2:  He's just been in the music world thinking about this stuff For a really

17
00:00:58,475 --> 00:01:02,315
Speaker 2:  long time, So I wanted to talk to him about AI and about

18
00:01:02,585 --> 00:01:06,555
Speaker 2:  virtual reality and about what it means to make music now

19
00:01:07,015 --> 00:01:10,635
Speaker 2:  in a world where increasingly technology mediates all parts

20
00:01:10,855 --> 00:01:14,235
Speaker 2:  of this process. We had a really fun conversation. It's

21
00:01:14,935 --> 00:01:18,195
Speaker 2:  not at all what I expected, but I really enjoyed it and I suspect you will

22
00:01:18,195 --> 00:01:21,835
Speaker 2:  too. All that is coming up in just a sec, but literally this train is best

23
00:01:21,835 --> 00:01:24,235
Speaker 2:  to leave without me. This is The Vergecast. We'll be right back.

24
00:02:09,545 --> 00:02:13,475
Speaker 4:  Welcome back. Let's get into my conversation with Ga Wong. I

25
00:02:13,475 --> 00:02:17,355
Speaker 4:  wanted to talk to GA because he sits at a really unusual place in the

26
00:02:17,485 --> 00:02:21,275
Speaker 4:  music slash technology universe. A bunch of years ago, he

27
00:02:21,275 --> 00:02:24,435
Speaker 4:  co-founded a company called smu, which is still around. You might have heard

28
00:02:24,435 --> 00:02:27,635
Speaker 4:  of it. They make a bunch of really popular music based apps.

29
00:02:28,215 --> 00:02:32,115
Speaker 4:  One is a karaoke app called, I think S Mule that's still really popular.

30
00:02:32,505 --> 00:02:36,315
Speaker 4:  There's also one called Magic Piano, which is kind of like Guitar Hero or

31
00:02:36,345 --> 00:02:40,155
Speaker 4:  Beat Saber, but it's for playing piano. GA also wrote

32
00:02:40,235 --> 00:02:44,115
Speaker 4:  a whole programming language called Chuck, which lets you write code that

33
00:02:44,115 --> 00:02:47,635
Speaker 4:  outputs as music. It's very cool and kind of

34
00:02:47,825 --> 00:02:51,675
Speaker 4:  wild to watch someone work with. He's now been at Stanford

35
00:02:51,815 --> 00:02:55,435
Speaker 4:  for a long time, also teaching in the Center for Computer Research in Music

36
00:02:55,455 --> 00:02:58,635
Speaker 4:  and Acoustics, which is a very cool thing that I just learned exists.

37
00:02:59,315 --> 00:03:03,085
Speaker 4:  He's also written a book about design. He's been teaching about music and

38
00:03:03,085 --> 00:03:06,925
Speaker 4:  technology for years, and in his free time, he's also the conductor

39
00:03:06,925 --> 00:03:10,845
Speaker 4:  of Stanford's laptop orchestra, which is exactly what it sounds

40
00:03:10,845 --> 00:03:14,725
Speaker 4:  like. It's a bunch of students on stage using their laptops and other

41
00:03:14,835 --> 00:03:18,765
Speaker 4:  gadgets to make music, frankly, that you've never heard before. Here

42
00:03:18,825 --> 00:03:20,765
Speaker 4:  is just a glimpse of what that sounds like,

43
00:03:28,675 --> 00:03:32,245
Speaker 4:  That sound If, you can imagine it is being made by GA himself who is

44
00:03:32,645 --> 00:03:36,445
Speaker 4:  standing on stage holding a glove connected to his

45
00:03:36,445 --> 00:03:40,165
Speaker 4:  laptop, and it's changing the sound as he moves his

46
00:03:40,165 --> 00:03:44,085
Speaker 4:  hand in space. It's wild. You should watch it. Their concerts are very cool.

47
00:03:44,645 --> 00:03:47,965
Speaker 4:  I wanted to talk to GA about the future of music because I figured he would

48
00:03:47,965 --> 00:03:51,765
Speaker 4:  just see it all from an unusual perspective. He makes music,

49
00:03:52,065 --> 00:03:55,965
Speaker 4:  he teaches about music. He's a programmer and an engineer, both for fun

50
00:03:55,985 --> 00:03:59,765
Speaker 4:  and by training. He's started companies before. He's seen kind of

51
00:03:59,825 --> 00:04:03,685
Speaker 4:  all sides of this, and I figured he'd have cool ideas about how we

52
00:04:03,685 --> 00:04:07,485
Speaker 4:  make music, how we can interact with computers to make music, and

53
00:04:08,005 --> 00:04:11,205
Speaker 4:  probably would have some like neat new AI apps that he was into that we might

54
00:04:11,205 --> 00:04:15,165
Speaker 4:  talk about. Our conversation went really differently from

55
00:04:15,165 --> 00:04:18,725
Speaker 4:  that, and it went really differently in a way that I really love. Actually,

56
00:04:19,115 --> 00:04:22,885
Speaker 4:  what we ended up talking about more than anything was what it means to be

57
00:04:23,485 --> 00:04:27,285
Speaker 4:  creative in a time when everything is being optimized and simplified

58
00:04:27,425 --> 00:04:30,565
Speaker 4:  and made more convenient and more efficient and more engaging.

59
00:04:31,325 --> 00:04:35,205
Speaker 4:  Absolutely. Everywhere you look, this is much more philosophical

60
00:04:35,395 --> 00:04:38,845
Speaker 4:  than a lot of the stuff we talk about in this show, but I really like the

61
00:04:38,845 --> 00:04:42,445
Speaker 4:  way he talks about what it means to be human and be

62
00:04:42,685 --> 00:04:46,325
Speaker 4:  yourself among all of this technology, and I hope you enjoy it too.

63
00:04:46,785 --> 00:04:50,685
Speaker 4:  So let's just dive in. To start with, I asked to try and frame

64
00:04:50,705 --> 00:04:54,325
Speaker 4:  the way he sees the world a little bit. One thing I've heard him say a few

65
00:04:54,325 --> 00:04:57,685
Speaker 4:  times in interviews and stuff is that he makes computer music,

66
00:04:58,305 --> 00:05:02,165
Speaker 4:  but it's 2024, right? Pretty much all music is made with

67
00:05:02,165 --> 00:05:05,925
Speaker 4:  and on computers. So what does GA's version of computer music

68
00:05:05,995 --> 00:05:09,365
Speaker 4:  look like now and how did he get so obsessed with it in the first place?

69
00:05:10,585 --> 00:05:13,975
Speaker 6:  Since I was a small child, I've had a fascination with computers, probably

70
00:05:13,975 --> 00:05:17,295
Speaker 6:  through video games. Actually, I still remember the first time I saw a video

71
00:05:17,295 --> 00:05:21,085
Speaker 6:  game. I think I was seven or

72
00:05:21,085 --> 00:05:24,875
Speaker 6:  eight in Beijing. You know, I had not seen a computer

73
00:05:24,935 --> 00:05:28,155
Speaker 6:  up to that point. I was thinking it was a big deal when we got a nine inch

74
00:05:28,705 --> 00:05:32,355
Speaker 6:  colored television. You know, this is in the, the early eighties

75
00:05:32,535 --> 00:05:36,395
Speaker 6:  in China, and the first time a, a win. My mom, I think, took me to

76
00:05:36,415 --> 00:05:39,995
Speaker 6:  an arcade, a video game arcade. I don't know if if

77
00:05:40,105 --> 00:05:42,435
Speaker 6:  listeners still know what those are,

78
00:05:43,025 --> 00:05:45,835
Speaker 4:  They've seen 'em in movies at the very least, you know? Yeah. Yes.

79
00:05:46,045 --> 00:05:49,595
Speaker 6:  Those are places where you go and you put like coins into and you can play

80
00:05:50,005 --> 00:05:53,915
Speaker 6:  games, and there's these giant machines, arcade machines, and that's the

81
00:05:53,915 --> 00:05:57,115
Speaker 6:  first time I saw like a video game. And something about the pixels

82
00:05:57,965 --> 00:06:01,885
Speaker 6:  I think really kind of drew me in, not unlike a

83
00:06:01,915 --> 00:06:05,725
Speaker 6:  moth to a flame. I guess that's, it's not entirely like a

84
00:06:05,725 --> 00:06:09,645
Speaker 6:  bad analogy, but I'm like, Ooh. And I think ever since then I've been

85
00:06:09,845 --> 00:06:13,805
Speaker 6:  fascinated with just computers. But I think over time, I think

86
00:06:13,805 --> 00:06:17,485
Speaker 6:  I've really come to love computers and computer science as a discipline,

87
00:06:17,825 --> 00:06:21,805
Speaker 6:  but perhaps in a strange way, and I'll try to explain that. So I'm all

88
00:06:21,805 --> 00:06:25,765
Speaker 6:  my degrees are in computer science. My, my, my, my bachelor of

89
00:06:25,765 --> 00:06:29,685
Speaker 6:  science is in cs. My PhD is in computer science,

90
00:06:30,505 --> 00:06:34,125
Speaker 6:  and in some ways I'm kind of a strange computer scientist,

91
00:06:34,405 --> 00:06:37,925
Speaker 6:  but I think I say strange because I, I tend to build things that

92
00:06:38,475 --> 00:06:41,765
Speaker 6:  with computers that nobody quite asks for.

93
00:06:43,125 --> 00:06:47,045
Speaker 6:  Interesting. And that solves no problems that seem to exist, for

94
00:06:47,045 --> 00:06:51,005
Speaker 6:  example, a arena, right? This is something that was

95
00:06:51,285 --> 00:06:55,205
Speaker 6:  designed back in 2008 for, for the iPhone. And

96
00:06:55,265 --> 00:06:59,205
Speaker 6:  you know, I, let me, So I can play a little here. So I can hear

97
00:06:59,205 --> 00:06:59,405
Speaker 6:  this.

98
00:07:04,625 --> 00:07:08,325
Speaker 6:  So I'm holding my phone as I'm at a sandwich and,

99
00:07:08,825 --> 00:07:12,765
Speaker 6:  and I'm blowing into the microphone located at the bottom of the phone, and

100
00:07:12,765 --> 00:07:16,205
Speaker 6:  I'm using Multitouch to control the pitch, and vibrato is controlled by the

101
00:07:16,205 --> 00:07:19,685
Speaker 6:  tilt of the phone. Oh, cool. So I'm barely even seeing the,

102
00:07:20,145 --> 00:07:24,125
Speaker 6:  the screen, but you know, it's a kind of a physical thing.

103
00:07:24,265 --> 00:07:26,165
Speaker 6:  So So, I'm gonna try to play a little Diddy with this,

104
00:07:42,065 --> 00:07:44,845
Speaker 6:  For example. So that's, that's me blowing literally into my, well, thank

105
00:07:44,845 --> 00:07:48,405
Speaker 6:  you very much. That's me literally blowing into my iPhone

106
00:07:48,945 --> 00:07:52,885
Speaker 6:  and into Aina. And this is an app that I would say is took a lot of,

107
00:07:53,545 --> 00:07:57,205
Speaker 6:  you know, it took software engineering, software design, interaction design,

108
00:07:57,205 --> 00:08:00,245
Speaker 6:  signal processing, all the sound is generated live on the phone

109
00:08:01,215 --> 00:08:05,205
Speaker 6:  using, you know, you know, the sound part is written in the programming

110
00:08:05,445 --> 00:08:08,565
Speaker 6:  language. Chuck, which is was my dissertation

111
00:08:09,265 --> 00:08:13,165
Speaker 6:  at Princeton. Oh. In which I'm still developing, right? Like

112
00:08:13,425 --> 00:08:13,845
Speaker 6:  20,

113
00:08:15,685 --> 00:08:19,565
Speaker 6:  actually 20 some years later. I'm still working on, and myself and a large

114
00:08:19,795 --> 00:08:23,405
Speaker 6:  team of people and a community of people working on Chuck. Right. Just explain

115
00:08:23,405 --> 00:08:23,565
Speaker 6:  what

116
00:08:23,565 --> 00:08:25,045
Speaker 7:  Chuck is, just real quick for people who don't

117
00:08:25,245 --> 00:08:28,125
Speaker 6:  Know. So Chuck is one of these tools that I've been talking about. So Chuck

118
00:08:28,185 --> 00:08:31,485
Speaker 6:  is a programming language for music synthesis,

119
00:08:32,545 --> 00:08:36,285
Speaker 6:  and you're writing code to generate sound, but also to write code, to

120
00:08:36,435 --> 00:08:40,365
Speaker 6:  kind of either to algorithmically figure out

121
00:08:40,365 --> 00:08:44,045
Speaker 6:  or generate like kind of the music that's coming next, or

122
00:08:44,355 --> 00:08:47,725
Speaker 6:  it's actually could be mapped from human interaction. Hmm. In this case,

123
00:08:47,725 --> 00:08:51,325
Speaker 6:  in Karina, it's human interaction, human computer interaction. And

124
00:08:51,645 --> 00:08:55,485
Speaker 6:  I am using my physical interactions to actually actually control

125
00:08:55,505 --> 00:08:59,205
Speaker 6:  the sound that's happening. And so, and a lot of times it's a mixture of

126
00:08:59,205 --> 00:09:02,485
Speaker 6:  the two where it's, there's some amount of automation,

127
00:09:03,025 --> 00:09:07,005
Speaker 6:  but also like, it's a, it's really trying to also figure out what is an

128
00:09:07,165 --> 00:09:11,125
Speaker 6:  interesting way to kind of put human interaction into the loop

129
00:09:11,305 --> 00:09:14,725
Speaker 6:  of, of thing that we're building. Again, nobody asked for,

130
00:09:15,325 --> 00:09:18,885
Speaker 6:  I solved no problems that exist. And I think in a way that's kind of

131
00:09:19,835 --> 00:09:23,805
Speaker 6:  been the, a running through line If, you will, for my

132
00:09:23,925 --> 00:09:27,765
Speaker 6:  students and I, and in my research group is that we, you know, we think

133
00:09:27,765 --> 00:09:30,445
Speaker 6:  about engineering, we think about technology, and we think about, well, what

134
00:09:30,445 --> 00:09:34,325
Speaker 6:  kind of problems can we solve with this? Right? And while

135
00:09:34,705 --> 00:09:38,605
Speaker 6:  yes, problem solving is something that we, we as humans need to do, and,

136
00:09:38,605 --> 00:09:42,485
Speaker 6:  and, and I'm, I'm interested in doing myself, I also feel like as an

137
00:09:42,645 --> 00:09:45,965
Speaker 6:  engineer, we can build things that isn't always motivated by

138
00:09:46,395 --> 00:09:48,965
Speaker 6:  kind of a practical use utility.

139
00:09:49,875 --> 00:09:53,405
Speaker 4:  Tell me how you go through that as kind of a creative process. I think

140
00:09:53,845 --> 00:09:57,685
Speaker 4:  I spend a lot of my time talking to engineers

141
00:09:57,705 --> 00:10:01,365
Speaker 4:  and developers and stuff who say, you, you know, some of the stuff you just

142
00:10:01,365 --> 00:10:05,245
Speaker 4:  said, and then they, they like small leap to, and now I

143
00:10:05,245 --> 00:10:09,085
Speaker 4:  make B2B database management software and or

144
00:10:09,325 --> 00:10:13,085
Speaker 4:  like, and So I made yet another AI chat bot,

145
00:10:13,085 --> 00:10:16,965
Speaker 4:  right? And I think there, there is this easy way to work backwards from like,

146
00:10:16,965 --> 00:10:20,925
Speaker 4:  okay, what is the problem I can solve that will help people? And I

147
00:10:20,925 --> 00:10:24,565
Speaker 4:  think using technology in service of

148
00:10:24,815 --> 00:10:28,685
Speaker 4:  going the other way and saying like, what, what can I just

149
00:10:28,955 --> 00:10:32,685
Speaker 4:  make is just such an interesting and different kind of creative process.

150
00:10:32,945 --> 00:10:36,085
Speaker 4:  I'm curious how you have sort of, especially you, you have to teach this

151
00:10:36,085 --> 00:10:39,685
Speaker 4:  stuff to students. Have you I do refined the process of how to do this stuff

152
00:10:39,685 --> 00:10:40,045
Speaker 4:  for people.

153
00:10:40,955 --> 00:10:44,885
Speaker 6:  Yeah. I mean, I, I teach this, I wrote a whole book about it, artful

154
00:10:44,885 --> 00:10:48,765
Speaker 6:  design technology in search of the sublime. And it's really this, this idea

155
00:10:48,765 --> 00:10:51,925
Speaker 6:  of looking at tool building as, as engineers, but also

156
00:10:52,545 --> 00:10:56,315
Speaker 6:  as critical engineers. Hmm. You know, critical tool builders.

157
00:10:56,335 --> 00:11:00,155
Speaker 6:  And the critical part is really has to do the question, why are you even

158
00:11:00,155 --> 00:11:04,075
Speaker 6:  doing this thing? Right. Why did I design aina? And it

159
00:11:04,355 --> 00:11:07,995
Speaker 6:  wasn't because like I went out and and figured out what do people

160
00:11:08,065 --> 00:11:11,435
Speaker 6:  need? You know, it's like, Hey David, what do you need? And you're like,

161
00:11:11,435 --> 00:11:12,635
Speaker 6:  well, I need to blow into my phone.

162
00:11:13,675 --> 00:11:14,755
Speaker 4:  I need a flute app. Yeah.

163
00:11:15,115 --> 00:11:17,675
Speaker 6:  I need a flute app and I need to all also in the same app, I need to be able

164
00:11:17,675 --> 00:11:20,075
Speaker 6:  to listen to other people around the world blown to their phones. Right.

165
00:11:20,075 --> 00:11:24,035
Speaker 6:  That's what missing. That's a, that's a deficit. No, that did not

166
00:11:24,035 --> 00:11:27,555
Speaker 6:  happen. It. And so it wasn't really kind of this traditional

167
00:11:27,745 --> 00:11:30,395
Speaker 6:  idea, at least traditional. Now when we think of engineering, we think of

168
00:11:30,395 --> 00:11:33,835
Speaker 6:  this as like, okay, that's a, what is a need? What's the problem?

169
00:11:34,285 --> 00:11:38,115
Speaker 6:  State the problem and so can find a solution. This is

170
00:11:38,115 --> 00:11:41,995
Speaker 6:  not need-based, designed, at least not in that sense. So why was this

171
00:11:42,235 --> 00:11:45,915
Speaker 6:  designed? This is what some people actually might call, you know, for, for

172
00:11:45,915 --> 00:11:49,715
Speaker 6:  perhaps lack of a better word, value space design. Hmm. And

173
00:11:50,005 --> 00:11:53,845
Speaker 6:  value based design is saying, well, you don't design out of a

174
00:11:53,875 --> 00:11:57,565
Speaker 6:  necessarily a clear and present practical need, but out of

175
00:11:57,565 --> 00:12:00,765
Speaker 6:  something you just, you really deeply as a person believe in.

176
00:12:01,625 --> 00:12:05,005
Speaker 6:  And so for something like ING and perhaps for a lot of the other tools that

177
00:12:05,005 --> 00:12:08,805
Speaker 6:  my students and I build, perhaps one of the values that core values that

178
00:12:08,805 --> 00:12:12,365
Speaker 6:  we are trying to speak to in these design is just simply that music making

179
00:12:13,305 --> 00:12:17,295
Speaker 6:  is good. And that's the belief music making does a person good.

180
00:12:18,035 --> 00:12:21,775
Speaker 6:  And it's not about getting to a product necessarily, but

181
00:12:21,785 --> 00:12:25,595
Speaker 6:  about actually the process of playing. And sometimes

182
00:12:25,615 --> 00:12:29,515
Speaker 6:  and often learning an instrument, not unlike learning

183
00:12:29,615 --> 00:12:33,435
Speaker 6:  to play like a, a well made, perhaps challenging video

184
00:12:33,465 --> 00:12:33,755
Speaker 6:  game.

185
00:12:35,385 --> 00:12:38,545
Speaker 6:  'cause there's a, there's a great satisfaction in that if the game is well

186
00:12:38,545 --> 00:12:41,985
Speaker 6:  made, it does not matter if it's difficult. In fact, you might appreciate

187
00:12:41,985 --> 00:12:45,705
Speaker 6:  the difficulty. 'cause then you, once you feel like you're learning

188
00:12:45,925 --> 00:12:49,745
Speaker 6:  the system, you really can then express yourself. You using the system

189
00:12:50,525 --> 00:12:53,905
Speaker 6:  and then the challenge and then, and able to overcome the challenges eventually

190
00:12:53,905 --> 00:12:56,545
Speaker 6:  of the game becomes hugely gratifying.

191
00:12:57,055 --> 00:13:00,845
Speaker 4:  Okay. Has that started to feel like

192
00:13:00,965 --> 00:13:04,805
Speaker 4:  a, a, I don't know, dying phenomenon

193
00:13:04,805 --> 00:13:08,725
Speaker 4:  to you, the idea that in, in this world of incredible convenience

194
00:13:08,725 --> 00:13:12,365
Speaker 4:  and efficiency where everything is available to you, you know, with all at,

195
00:13:12,715 --> 00:13:16,605
Speaker 4:  with a push of a button or like a plugin in an app, that doing

196
00:13:16,605 --> 00:13:20,165
Speaker 4:  something just for sort of the, the joy and

197
00:13:20,485 --> 00:13:24,165
Speaker 4:  pleasure and slog of doing it is worthwhile. Like

198
00:13:24,195 --> 00:13:27,645
Speaker 4:  that that almost feels like a, a sort of beautiful, deeply

199
00:13:27,715 --> 00:13:30,125
Speaker 4:  anachronistic way of looking at the world right now.

200
00:13:30,595 --> 00:13:34,125
Speaker 6:  Well, unfortunately it is. It feels

201
00:13:34,315 --> 00:13:38,045
Speaker 6:  anachronistic. It feels like it, it's out of time. It's not something that,

202
00:13:38,465 --> 00:13:41,565
Speaker 6:  you know, in this convenience driven,

203
00:13:42,525 --> 00:13:45,325
Speaker 6:  optimization driven, competition driven

204
00:13:46,145 --> 00:13:48,845
Speaker 6:  fabric of a society we live in,

205
00:13:50,185 --> 00:13:54,085
Speaker 6:  it is, it feels deeply anachronistic to build things that are playful,

206
00:13:55,035 --> 00:13:57,485
Speaker 6:  that are interesting for their own sake.

207
00:13:59,065 --> 00:14:02,645
Speaker 6:  But at the same time, I feel like it's what makes us,

208
00:14:03,145 --> 00:14:06,285
Speaker 6:  us, and I don't even mean, I don't mean just building things. So, I mean,

209
00:14:06,285 --> 00:14:09,245
Speaker 6:  just anything I think that you feel like makes you, you,

210
00:14:10,405 --> 00:14:14,125
Speaker 6:  whatever that may be, I like to at least offer the possibility that

211
00:14:14,185 --> 00:14:18,145
Speaker 6:  it actually is something that is, that you, you wanna do and

212
00:14:18,145 --> 00:14:22,065
Speaker 6:  you and you care to do. In fact, you would probably forego a lot

213
00:14:22,065 --> 00:14:25,865
Speaker 6:  of practical needs in order to do, it's like a passionate hobby.

214
00:14:26,645 --> 00:14:30,425
Speaker 6:  And the passionate hobby is usually not about getting to the

215
00:14:30,425 --> 00:14:34,305
Speaker 6:  result. It's not about optimization. Right. And So I think this gets to this

216
00:14:34,305 --> 00:14:37,985
Speaker 6:  idea that yeah, while this idea of building things for its own sake or doing

217
00:14:37,985 --> 00:14:41,385
Speaker 6:  things for its own sake is feels like it's out of fashion, but it's also

218
00:14:41,485 --> 00:14:45,265
Speaker 6:  not, in a sense, I think we actually still do these things.

219
00:14:45,265 --> 00:14:49,225
Speaker 6:  They're just usually not in engineering contexts. You

220
00:14:49,225 --> 00:14:52,805
Speaker 6:  know, for example, when people cook for themselves because they enjoy

221
00:14:53,035 --> 00:14:56,715
Speaker 6:  cooking, there's a deep joy going to the pantry, work up,

222
00:14:57,625 --> 00:15:01,555
Speaker 6:  find the raw ingredients and make a mess of things as you concoct a

223
00:15:01,555 --> 00:15:04,995
Speaker 6:  dish and that dish, doesn't that taste different because you know,

224
00:15:05,515 --> 00:15:09,155
Speaker 6:  'cause you made it right. Yeah. It, it it does, it has to. Especially if

225
00:15:09,155 --> 00:15:12,995
Speaker 6:  you're someone that enjoys cooking. And I think tools are, are,

226
00:15:13,015 --> 00:15:16,875
Speaker 6:  are no different. And I think the danger of

227
00:15:16,875 --> 00:15:20,635
Speaker 6:  being in, in, I think in, in a world or a society where the things,

228
00:15:20,655 --> 00:15:24,595
Speaker 6:  the tools we make are only about optimization, only about

229
00:15:25,075 --> 00:15:28,755
Speaker 6:  convenience, only about reducing the cost of labor is that we

230
00:15:29,075 --> 00:15:32,755
Speaker 6:  actually, in, in those tools, those tools can actually alienate us

231
00:15:33,745 --> 00:15:37,075
Speaker 6:  from who we actually are. Because, you know,

232
00:15:37,545 --> 00:15:41,115
Speaker 6:  well, let's, let's go back to music and let's put, you know, let's talk about

233
00:15:41,115 --> 00:15:44,515
Speaker 6:  music and ai, right? Yeah. Today we have

234
00:15:45,015 --> 00:15:48,475
Speaker 6:  AI that can produce really impressive images,

235
00:15:49,125 --> 00:15:53,075
Speaker 6:  video music. Well that's a, that's

236
00:15:53,075 --> 00:15:56,995
Speaker 6:  a tricky word to use. Even songs are tricky words to use. Yeah.

237
00:15:56,995 --> 00:15:59,435
Speaker 6:  It's like, you know, what is this song? But let's, for all intents persons

238
00:15:59,435 --> 00:16:03,235
Speaker 6:  say AI can generate quote unquote songs. And with just a

239
00:16:03,235 --> 00:16:07,075
Speaker 6:  prompt. It's, it's like this, this almost age old thing that people

240
00:16:07,185 --> 00:16:10,515
Speaker 6:  thought they always wanted, which is, if I have a musical idea in my mind,

241
00:16:11,155 --> 00:16:14,235
Speaker 6:  I just want this thing realized, wouldn't that be great? You know, and I

242
00:16:14,235 --> 00:16:17,195
Speaker 6:  don't, maybe perhaps I don't have any music education. I've never taken time,

243
00:16:17,195 --> 00:16:19,515
Speaker 6:  but I just want this idea realized in my head it wouldn't, wouldn't that

244
00:16:19,515 --> 00:16:23,425
Speaker 6:  be great? And in a way, AI kind of does that

245
00:16:23,985 --> 00:16:27,545
Speaker 6:  somewhat today, but then I think the question lies what is,

246
00:16:27,855 --> 00:16:31,665
Speaker 6:  what does it mean? Hmm. And you know, and I'll draw another analogy here,

247
00:16:31,665 --> 00:16:35,305
Speaker 6:  and this is something that I call, I guess it's kind of the bubble gum effect.

248
00:16:35,405 --> 00:16:39,345
Speaker 6:  And what do I mean by that? Right? So If, you ever chewed gum, David?

249
00:16:39,855 --> 00:16:40,145
Speaker 6:  Sure.

250
00:16:40,215 --> 00:16:40,505
Speaker 4:  Yeah.

251
00:16:40,895 --> 00:16:44,825
Speaker 6:  Okay. So when you chew a piece of gum, when you first pop the, the gum in

252
00:16:44,825 --> 00:16:47,825
Speaker 6:  chewing gum or a bubble gum, right. How do you feel?

253
00:16:48,215 --> 00:16:49,625
Speaker 4:  It's great. It's delicious.

254
00:16:50,055 --> 00:16:53,505
Speaker 6:  Yeah. It's, it pops Yeah. It's like the taste is great. How long do you usually

255
00:16:53,575 --> 00:16:55,625
Speaker 6:  chew that piece of gum?

256
00:16:56,565 --> 00:17:00,125
Speaker 4:  I mean, it depends on the gum, but some, some somewhere between like

257
00:17:01,265 --> 00:17:05,045
Speaker 4:  10 seconds and three minutes and then, and then it's over.

258
00:17:05,505 --> 00:17:07,485
Speaker 6:  And when it's over, what do you do with that piece of gum?

259
00:17:08,655 --> 00:17:09,765
Speaker 4:  Throw it away.

260
00:17:10,305 --> 00:17:13,765
Speaker 6:  And do you, after that, how much do you think about that particular piece

261
00:17:13,765 --> 00:17:14,045
Speaker 6:  of gum?

262
00:17:14,655 --> 00:17:16,365
Speaker 4:  Never for one second ever again.

263
00:17:17,075 --> 00:17:20,965
Speaker 6:  Okay. So I think that's analogy I'm trying to

264
00:17:20,965 --> 00:17:24,775
Speaker 6:  draw here is that yes, maybe it's so easy

265
00:17:25,595 --> 00:17:29,325
Speaker 6:  that I can go to a generative AI system

266
00:17:29,465 --> 00:17:33,405
Speaker 6:  and say, render me an image of Barney, the dinosaur

267
00:17:33,765 --> 00:17:37,325
Speaker 6:  withdrawing cash from a ATM, but only

268
00:17:37,325 --> 00:17:41,245
Speaker 6:  broccoli is coming out. I think actually AI would

269
00:17:41,375 --> 00:17:45,005
Speaker 6:  generative I today would do a rather impressive job. Yeah. They seem be like,

270
00:17:45,005 --> 00:17:48,285
Speaker 6:  whoa. Yeah. Yeah. That's, that's it. And I may go and share this with my

271
00:17:48,285 --> 00:17:50,925
Speaker 6:  friend and be like, Hey, check out, look what I made. And then that I think

272
00:17:50,925 --> 00:17:54,765
Speaker 6:  there's, there's actually that social function of that. But

273
00:17:54,765 --> 00:17:58,215
Speaker 6:  then more likely than not, I'd probably be like,

274
00:17:58,785 --> 00:18:02,375
Speaker 6:  great onto the next thing. Yeah. That took all of three minutes,

275
00:18:02,885 --> 00:18:06,255
Speaker 6:  five minutes. The timer probably takes to chew a piece of gum, whatever.

276
00:18:06,255 --> 00:18:10,215
Speaker 6:  Think about that thing again. Maybe a little bit more than the piece of

277
00:18:10,215 --> 00:18:13,945
Speaker 6:  gum I discarded. And I think that's what I mean by bubble

278
00:18:14,265 --> 00:18:17,865
Speaker 6:  ification. In fact, it is kind of almost the, in the limit of the, the,

279
00:18:18,295 --> 00:18:22,225
Speaker 6:  this kind of push of automation. It unquestionably pushed automation

280
00:18:22,565 --> 00:18:26,465
Speaker 6:  to basically try to reduce the cost of labor down to zero in

281
00:18:26,585 --> 00:18:28,305
Speaker 6:  like every case we possibly can.

282
00:18:28,595 --> 00:18:32,385
Speaker 4:  Right. Well, there's a, there's a, there's an argument inside of that.

283
00:18:32,505 --> 00:18:36,145
Speaker 4:  I think that, that you hear a lot with kind of anything creative, which is

284
00:18:36,145 --> 00:18:39,425
Speaker 4:  that the process matters, right? That, that

285
00:18:40,405 --> 00:18:43,545
Speaker 4:  the, the provenance of something and where it came from and the story of

286
00:18:43,545 --> 00:18:47,345
Speaker 4:  the thing and the, the person who made it that stuff is, is

287
00:18:47,495 --> 00:18:51,425
Speaker 4:  sort of indelibly connected to the thing in a way that you don't

288
00:18:51,425 --> 00:18:55,265
Speaker 4:  always understand even. But yes. Matters in some way that it's

289
00:18:55,265 --> 00:18:58,385
Speaker 4:  very hard to quantify. And thus it's very hard to talk about. In some ways

290
00:18:58,695 --> 00:19:02,025
Speaker 4:  that stuff matters and I really buy that premise.

291
00:19:02,605 --> 00:19:06,345
Speaker 4:  But I also, what I wonder then about AI is

292
00:19:06,405 --> 00:19:10,345
Speaker 4:  if what AI is going to create is like some entirely other

293
00:19:11,545 --> 00:19:15,535
Speaker 4:  thing that like maybe what we want from AI is not to

294
00:19:15,535 --> 00:19:19,015
Speaker 4:  make songs that sound like Taylor Swift, but something that sounds completely

295
00:19:19,165 --> 00:19:23,015
Speaker 4:  different. And this is part of why I think your work is so

296
00:19:23,135 --> 00:19:25,015
Speaker 4:  interesting and why I'm curious about how you're thinking about and using

297
00:19:25,195 --> 00:19:29,095
Speaker 4:  ai. Like you, you've made a career out of making computers make noises that

298
00:19:29,095 --> 00:19:32,775
Speaker 4:  they're not supposed to make. And, and, and I think what, what those

299
00:19:33,055 --> 00:19:36,895
Speaker 4:  ultimately make is something different, right? Like you're, you're not writing

300
00:19:37,205 --> 00:19:40,015
Speaker 4:  code to make Taylor Swift songs. You're writing some, you're, you're making

301
00:19:40,015 --> 00:19:43,695
Speaker 4:  something else. And I feel like If, you approach AI

302
00:19:43,915 --> 00:19:46,375
Speaker 4:  as that kind of tool,

303
00:19:48,445 --> 00:19:52,175
Speaker 4:  both to sort of bend it to your will, but also to try to figure out like

304
00:19:52,175 --> 00:19:53,015
Speaker 4:  what sound it makes,

305
00:19:54,055 --> 00:19:57,845
Speaker 6:  I guess my general life philosophy can be

306
00:19:57,845 --> 00:20:01,805
Speaker 6:  summed up or at least in aspirations that there should be room for that too.

307
00:20:02,075 --> 00:20:05,845
Speaker 6:  Okay. And I mean this both in, in a way, like, what I mean by that

308
00:20:05,865 --> 00:20:09,765
Speaker 6:  is that I'm, I think about this in, in terms of music, I think about the

309
00:20:09,765 --> 00:20:13,555
Speaker 6:  terms of actually like people in a society,

310
00:20:13,805 --> 00:20:17,755
Speaker 6:  there should be room for that too, as long as the thing is

311
00:20:17,955 --> 00:20:21,875
Speaker 6:  not like harmful and dangerous. Sure. But also it also

312
00:20:21,875 --> 00:20:25,435
Speaker 6:  means that I guess there sh you know, it's really what I'm trying to say

313
00:20:25,435 --> 00:20:29,315
Speaker 6:  is that it should be a pluralism. Mm. We should have room to, to

314
00:20:29,315 --> 00:20:33,235
Speaker 6:  to have capacity to have a pluralism of, of foil

315
00:20:33,235 --> 00:20:36,275
Speaker 6:  of what I would say of values, aesthetic, social.

316
00:20:38,055 --> 00:20:41,875
Speaker 6:  In fact, that would be my notion of what part of what a a, a

317
00:20:42,115 --> 00:20:45,515
Speaker 6:  a, a civil society might have, but also

318
00:20:45,975 --> 00:20:49,645
Speaker 6:  one of maybe a necessary condition for humans to be able to

319
00:20:49,835 --> 00:20:53,685
Speaker 6:  like flourish, you know, is is this idea to have a

320
00:20:54,605 --> 00:20:58,245
Speaker 6:  capacity for pluralism. And so yeah, in that sense,

321
00:20:58,565 --> 00:21:02,525
Speaker 6:  I think there's room to think about how c can, can we explore, use AI

322
00:21:02,525 --> 00:21:06,485
Speaker 6:  to explore the realm of untapped sounds.

323
00:21:06,905 --> 00:21:09,805
Speaker 6:  But how we do that, first of all, I think is really important because it

324
00:21:09,805 --> 00:21:13,405
Speaker 6:  does go back to what you're saying about the meaning that we ascribe to

325
00:21:14,225 --> 00:21:15,725
Speaker 6:  things that we might call art.

326
00:21:17,025 --> 00:21:19,445
Speaker 4:  All right, we gotta take a break and then we have lots more to talk about.

327
00:21:19,735 --> 00:21:20,405
Speaker 4:  We'll be right back.

328
00:22:09,865 --> 00:22:13,765
Speaker 4:  All right, we're back. So to go back to the idea of computer music,

329
00:22:14,065 --> 00:22:17,605
Speaker 4:  one of the things GA has talked a lot about over the years is the idea of

330
00:22:17,605 --> 00:22:21,565
Speaker 4:  finding new ways to interact with technology and computers

331
00:22:21,715 --> 00:22:25,605
Speaker 4:  like that glove I mentioned at the beginning, that's a new way of

332
00:22:26,015 --> 00:22:29,645
Speaker 4:  using a computer. If, you boil it all the way down and the way he thinks

333
00:22:29,645 --> 00:22:32,885
Speaker 4:  about it is more explicitly playful and more

334
00:22:33,075 --> 00:22:36,805
Speaker 4:  exploratory instead of just trying to like figure out how to use an app

335
00:22:36,875 --> 00:22:40,805
Speaker 4:  most effectively to get work done. It's just a totally

336
00:22:40,805 --> 00:22:44,405
Speaker 4:  different approach to all of this stuff. But that can be a hard

337
00:22:44,455 --> 00:22:48,165
Speaker 4:  shift to pull off, I think, to go from trying to learn

338
00:22:48,265 --> 00:22:51,845
Speaker 4:  how to use a technology to figuring out what's

339
00:22:52,085 --> 00:22:55,965
Speaker 4:  possible with it. But that shift, that transition is I think

340
00:22:55,995 --> 00:22:59,965
Speaker 4:  what God does all day with his students. So I asked him about

341
00:22:59,965 --> 00:23:03,845
Speaker 4:  it. I saw something that, that you either

342
00:23:03,855 --> 00:23:07,405
Speaker 4:  wrote or said about your music and ai AI class where one of your goals was

343
00:23:07,405 --> 00:23:11,285
Speaker 4:  to teach them how to play with ai. We don't talk about AI that

344
00:23:11,285 --> 00:23:14,885
Speaker 4:  way. What, what does that look like? What does it look like to play with

345
00:23:15,615 --> 00:23:16,925
Speaker 4:  these kinds of systems?

346
00:23:17,635 --> 00:23:21,205
Speaker 6:  Well playing? What does it look like? Well, I ask my students, for example,

347
00:23:21,265 --> 00:23:24,965
Speaker 6:  to build interactive tools with AI that they can

348
00:23:24,965 --> 00:23:28,445
Speaker 6:  deploy into their everyday lives, but also often would involve human

349
00:23:28,445 --> 00:23:32,125
Speaker 6:  interaction. Hmm. My students have built instruments that

350
00:23:32,125 --> 00:23:36,045
Speaker 6:  basically track the opening and closing of your hand to,

351
00:23:36,425 --> 00:23:40,005
Speaker 6:  to, to generate singing sound. And turning your hand changes

352
00:23:40,345 --> 00:23:44,285
Speaker 6:  the pitch of the sound, opening and closing your hand kind of

353
00:23:44,395 --> 00:23:45,565
Speaker 6:  articulates the sound.

354
00:23:53,905 --> 00:23:57,885
Speaker 6:  You can't do that with a prompt based system right now.

355
00:23:57,935 --> 00:24:01,085
Speaker 6:  Right, right. That's not what that's about. But this means you need to un

356
00:24:01,145 --> 00:24:05,045
Speaker 6:  go to the process of learning how to use that instrument. And

357
00:24:05,955 --> 00:24:09,925
Speaker 6:  another one, it's, and it's, it's, it's playful. I mean, that's playful and

358
00:24:09,925 --> 00:24:12,965
Speaker 6:  it's interactive. Another playful one is, one of my students made this thing

359
00:24:12,965 --> 00:24:16,685
Speaker 6:  called auto riz. Okay. It, it, it's a, it's a

360
00:24:17,085 --> 00:24:20,205
Speaker 6:  computer vision system that basically is been trained to

361
00:24:21,285 --> 00:24:24,805
Speaker 6:  classify between like to recognize a seductive look

362
00:24:26,265 --> 00:24:30,165
Speaker 6:  and it plays like cheesy seductive music when it, when it detects

363
00:24:30,165 --> 00:24:30,285
Speaker 6:  it.

364
00:24:37,105 --> 00:24:41,085
Speaker 6:  And that's all it does. It's like by the mission of the creator of

365
00:24:41,085 --> 00:24:43,845
Speaker 6:  auto riz. Like, this is such a, this is so dumb.

366
00:24:45,505 --> 00:24:49,315
Speaker 6:  And yet it's really, it's playful. Yeah. It's, it's,

367
00:24:49,315 --> 00:24:53,155
Speaker 6:  and and, and the definition of play is actually something that is

368
00:24:53,155 --> 00:24:56,975
Speaker 6:  not about the outcome, it's about the process. Right.

369
00:24:56,975 --> 00:25:00,095
Speaker 6:  Right. Because If, you are like, I'm playing, but if you're worried about

370
00:25:00,275 --> 00:25:03,995
Speaker 6:  the outcome that's gonna be produced, well that's like, that's work

371
00:25:04,395 --> 00:25:08,035
Speaker 6:  actually. If, you need to walk away from an, an activity with a productive

372
00:25:08,035 --> 00:25:11,595
Speaker 6:  outcome. That's, we'd usually call that work, but play is

373
00:25:12,055 --> 00:25:13,795
Speaker 6:  almost exactly the opposite of that.

374
00:25:14,125 --> 00:25:17,835
Speaker 4:  Right. There's, there's a difference I hadn't really thought about

375
00:25:17,965 --> 00:25:21,675
Speaker 4:  there between thinking about these technologies,

376
00:25:22,015 --> 00:25:25,995
Speaker 4:  AI or otherwise as tools versus thinking about them as instruments.

377
00:25:25,995 --> 00:25:29,795
Speaker 4:  And I think we talk about AI in particular as a, as a tool, right? It is,

378
00:25:29,795 --> 00:25:33,515
Speaker 4:  it is a hammer and your job is to go find nails. And, and that's, that's

379
00:25:33,515 --> 00:25:37,035
Speaker 4:  what it's for. and we don't necessarily know what all the nails are, and

380
00:25:37,035 --> 00:25:40,835
Speaker 4:  that's the exploratory phase, but you're looking for nails. But what you're

381
00:25:40,835 --> 00:25:43,675
Speaker 4:  describing is something very different, which is like, here, here is a thing

382
00:25:43,905 --> 00:25:47,795
Speaker 4:  with a set of capabilities, right. It it can, it can, it can do a

383
00:25:47,795 --> 00:25:51,395
Speaker 4:  bunch of things. And where all of the interesting stuff

384
00:25:51,545 --> 00:25:54,835
Speaker 4:  lies is figuring out what

385
00:25:55,455 --> 00:25:59,195
Speaker 4:  to do with those things and how they mix together

386
00:25:59,775 --> 00:26:03,315
Speaker 4:  and new unexpected combinations of those

387
00:26:03,315 --> 00:26:07,035
Speaker 4:  capabilities that turns into something which is so different from I have

388
00:26:07,035 --> 00:26:08,955
Speaker 4:  a hammer that hits nails and I have to find more nails.

389
00:26:09,115 --> 00:26:12,355
Speaker 6:  I go find more nails. Yeah. Right. Let's, let's put it this way. Let's say

390
00:26:12,375 --> 00:26:15,835
Speaker 6:  we make a Venn diagram, which means let's draw a circle. And that circle

391
00:26:16,235 --> 00:26:19,555
Speaker 6:  represents us that, that the sets of things that we, the AI is potentially

392
00:26:19,905 --> 00:26:22,995
Speaker 6:  good at doing Sure. Or would be good at doing. Let's draw another circle.

393
00:26:23,125 --> 00:26:27,035
Speaker 6:  Let's call that a set of things that humans do wanna do and are

394
00:26:27,035 --> 00:26:30,715
Speaker 6:  perhaps can be good at doing like ping pong or making music or

395
00:26:30,715 --> 00:26:34,555
Speaker 6:  whatnot. And the Venn diagrams that, let's draw these two circles, but that's

396
00:26:34,585 --> 00:26:37,955
Speaker 6:  also, they're intersecting. So there's a region where it's in,

397
00:26:38,025 --> 00:26:41,115
Speaker 6:  basically in space where it's in both circles. It's the stuff that humans

398
00:26:41,255 --> 00:26:44,925
Speaker 6:  are good at doing and AI will be good at doing. I feel like we're kind of

399
00:26:45,275 --> 00:26:47,725
Speaker 6:  automatically and almost unquestionably

400
00:26:49,015 --> 00:26:52,885
Speaker 6:  stuck in that intersection with ai, is that we're

401
00:26:52,885 --> 00:26:56,765
Speaker 6:  constantly looking for things that humans already do. And now we

402
00:26:56,765 --> 00:26:59,765
Speaker 6:  just want to have a AI do that same thing.

403
00:27:01,285 --> 00:27:05,265
Speaker 6:  And in fact, the more the, the quote unquote better an

404
00:27:05,325 --> 00:27:08,625
Speaker 6:  AI can do it, the, the more progress surely that this must be,

405
00:27:09,355 --> 00:27:12,855
Speaker 6:  you know, this, some have called this the touring trap. Actually. It's this

406
00:27:12,925 --> 00:27:16,735
Speaker 6:  idea that we have been set down this

407
00:27:16,765 --> 00:27:20,295
Speaker 6:  path where progress in AI is judged by how

408
00:27:20,355 --> 00:27:23,625
Speaker 6:  indistinguishable a system is to humans.

409
00:27:24,075 --> 00:27:27,985
Speaker 6:  Right. And the danger of that is that that becomes kind of the

410
00:27:28,425 --> 00:27:32,105
Speaker 6:  dominant and perhaps almost the only way we can imagine of

411
00:27:32,105 --> 00:27:35,465
Speaker 6:  thinking about progress in ai. But If, you go back to the Venn diagram, there's

412
00:27:35,465 --> 00:27:39,345
Speaker 6:  a whole region, what AI could potentially do that is good at doing

413
00:27:39,615 --> 00:27:43,545
Speaker 6:  that does not intersect with a sphere, the realm of things that humans are

414
00:27:43,545 --> 00:27:46,985
Speaker 6:  good at doing. You know, how do we explore that perhaps far

415
00:27:47,655 --> 00:27:50,995
Speaker 6:  more vast region of what AI can do?

416
00:27:51,595 --> 00:27:55,155
Speaker 6:  I think that takes exploration, takes play, it takes imagination. And I think,

417
00:27:55,735 --> 00:27:59,595
Speaker 6:  and often in my courses, what we learned is that actually when you

418
00:27:59,695 --> 00:28:03,155
Speaker 6:  remove this external obligation to be useful

419
00:28:03,895 --> 00:28:07,715
Speaker 6:  or competitive, and you can simply be yourself and play,

420
00:28:07,815 --> 00:28:11,595
Speaker 6:  you actually produce things that are so different potentially.

421
00:28:11,875 --> 00:28:15,835
Speaker 6:  Hmm. Arguably will certainly be different, but probably also like that's

422
00:28:15,835 --> 00:28:19,715
Speaker 6:  gonna be more, you more expressive. So the, in my

423
00:28:19,715 --> 00:28:23,635
Speaker 6:  courses, I'm trying to help people use ai, but to try to explore the

424
00:28:23,635 --> 00:28:27,595
Speaker 6:  unexplored space, but also explore the space where AI and humans intersect.

425
00:28:27,975 --> 00:28:31,835
Speaker 6:  But AI is doing things that humans aren't doing. Humans are

426
00:28:31,835 --> 00:28:34,795
Speaker 6:  doing things that AI aren't doing, and it's not about a collision

427
00:28:35,665 --> 00:28:39,525
Speaker 6:  or the actual overlap, but about a kind of like a, A-A-A-A-A

428
00:28:39,765 --> 00:28:43,725
Speaker 6:  critically thought through, but, but like a, like a beneficial amalgamation.

429
00:28:43,965 --> 00:28:47,285
Speaker 6:  Hmm. A a a union of, of two different parts.

430
00:28:48,465 --> 00:28:52,365
Speaker 6:  And what I like about that, even though it's not the way we think of

431
00:28:52,545 --> 00:28:56,005
Speaker 6:  ai, is you, you've, as you're noting what I like about that is it,

432
00:28:56,385 --> 00:29:00,005
Speaker 6:  you know, it, it still keeps human curation and I would say human intention

433
00:29:00,385 --> 00:29:02,885
Speaker 6:  and also what we might call human wisdom

434
00:29:04,185 --> 00:29:07,405
Speaker 6:  in the loop. You know? And, and the idea is that if AI could just generate

435
00:29:07,425 --> 00:29:11,205
Speaker 6:  me, even if it's a new piece of music, but I don't have, you know, the provenance

436
00:29:11,385 --> 00:29:14,245
Speaker 6:  as you said of like, where did this come from? What was the story behind

437
00:29:14,245 --> 00:29:14,445
Speaker 6:  this?

438
00:29:16,005 --> 00:29:19,125
Speaker 6:  I did not come up with this, but I re and I can, I do know who said this,

439
00:29:19,265 --> 00:29:23,125
Speaker 6:  but it's been said by someone, it's on, on, on what the

440
00:29:23,125 --> 00:29:26,885
Speaker 6:  role of art is. The idea that art perhaps is this thing that

441
00:29:27,505 --> 00:29:31,445
Speaker 6:  humans endeavor to try to understand their emotions,

442
00:29:31,465 --> 00:29:34,885
Speaker 6:  but mostly they fail at that. Mm. But that's art, you know, it's a thing

443
00:29:34,905 --> 00:29:38,165
Speaker 6:  we, we try mostly fail at to try to understand ourselves.

444
00:29:38,785 --> 00:29:42,605
Speaker 6:  And so if we, we, we feel like that's, you know, that's what

445
00:29:42,665 --> 00:29:43,485
Speaker 6:  art could be,

446
00:29:44,995 --> 00:29:47,965
Speaker 6:  then yes, the provenance matters and matters

447
00:29:48,515 --> 00:29:49,365
Speaker 6:  fundamentally

448
00:29:51,015 --> 00:29:54,555
Speaker 6:  to know what is trying to be communicated, what is trying to be expressed

449
00:29:54,555 --> 00:29:57,755
Speaker 6:  or even more so. And, you know, if we think of art as a, as a like a lens

450
00:29:57,935 --> 00:30:01,915
Speaker 6:  to see things, you know, if I, if I make a piece of art isn't

451
00:30:01,915 --> 00:30:05,755
Speaker 6:  just saying, Hey, this is what I see, you know? Yeah.

452
00:30:05,755 --> 00:30:08,875
Speaker 6:  It's, it's part of what the artist perhaps sees, but it's also

453
00:30:09,825 --> 00:30:13,645
Speaker 6:  an invitation for the experiencer to say, well, If you,

454
00:30:13,645 --> 00:30:17,285
Speaker 6:  like, you can look through this lens too, and you see the world around, you

455
00:30:17,285 --> 00:30:21,045
Speaker 6:  see yourself and what do you make of it? Hmm. Now that's,

456
00:30:21,145 --> 00:30:25,125
Speaker 6:  that's, you know, a, a good piece of writing is absolutely

457
00:30:25,475 --> 00:30:29,365
Speaker 6:  that, you know, good art in any medium I think is, is

458
00:30:29,425 --> 00:30:32,485
Speaker 6:  is also a lens, but there's film writing music,

459
00:30:34,625 --> 00:30:38,565
Speaker 6:  and so that requires this, I think the human

460
00:30:38,625 --> 00:30:42,605
Speaker 6:  in the loop, if we buy that definition that artists, this,

461
00:30:42,605 --> 00:30:46,285
Speaker 6:  this thing we do to try to mostly try and fail to understand our own

462
00:30:46,795 --> 00:30:47,285
Speaker 6:  motions.

463
00:30:47,995 --> 00:30:51,645
Speaker 4:  Yeah. I will say my, my worry with

464
00:30:52,075 --> 00:30:55,645
Speaker 4:  that way of thinking about it would be that AI in particular

465
00:30:55,875 --> 00:30:58,565
Speaker 4:  runs the risk of kind of undermining that. Because

466
00:30:59,785 --> 00:31:03,525
Speaker 4:  if If you think about AI as sort of the, the like lowest

467
00:31:03,525 --> 00:31:07,285
Speaker 4:  common denominator of all of its training data, what

468
00:31:07,285 --> 00:31:11,085
Speaker 4:  you're getting is basically this mush of all of

469
00:31:11,265 --> 00:31:15,205
Speaker 4:  its inputs that is kind of going to

470
00:31:15,205 --> 00:31:19,085
Speaker 4:  give you mush back and it's gonna give everyone the same kind of mush.

471
00:31:19,345 --> 00:31:22,045
Speaker 4:  and we end up in this place where, okay, you, you have this collaborator

472
00:31:22,045 --> 00:31:25,165
Speaker 4:  that has incredible access to incredible amounts of information,

473
00:31:25,945 --> 00:31:29,405
Speaker 4:  but it, the, like, it it, that's actually not useful in some of the ways

474
00:31:29,405 --> 00:31:33,325
Speaker 4:  that you're talking about that maybe having access

475
00:31:33,365 --> 00:31:36,485
Speaker 4:  to every song ever made is actually a hindrance to making great new songs

476
00:31:36,715 --> 00:31:40,565
Speaker 4:  instead of just making the same kinds of songs that are the average

477
00:31:40,665 --> 00:31:42,285
Speaker 4:  of every other song ever made before.

478
00:31:42,795 --> 00:31:46,365
Speaker 6:  Well it, it's, I mean the, what you're saying is that it's, it's still,

479
00:31:47,035 --> 00:31:50,965
Speaker 6:  it's exactly square in that region of the intersection. That's

480
00:31:50,965 --> 00:31:51,045
Speaker 6:  true.

481
00:31:51,105 --> 00:31:52,965
Speaker 4:  I'm still in the middle of the Venn diagram. You're

482
00:31:52,965 --> 00:31:55,485
Speaker 6:  Still right. That's really in, you're the middle of that Venn diagram. That's,

483
00:31:55,485 --> 00:31:59,375
Speaker 6:  that's kind of where we are in a way that's like artists, all the

484
00:31:59,375 --> 00:32:03,295
Speaker 6:  artists I know generally don't want AI to, maybe they want help

485
00:32:03,475 --> 00:32:06,815
Speaker 6:  to do part of their work, but they don't want AI to do the work

486
00:32:07,915 --> 00:32:10,895
Speaker 6:  in the moment that core writing part is actually done. For me,

487
00:32:12,015 --> 00:32:15,835
Speaker 6:  the activity ceases to be like an activity. It just, it's just

488
00:32:15,835 --> 00:32:19,555
Speaker 6:  like a checkbox. Right. And what meaning I might

489
00:32:19,715 --> 00:32:23,195
Speaker 6:  derive from the activity as time consuming as activity may be,

490
00:32:24,095 --> 00:32:27,995
Speaker 6:  is I think as, as you articulated, is the worries that

491
00:32:27,995 --> 00:32:31,325
Speaker 6:  that would be lost. You know, that that, that meaning

492
00:32:31,945 --> 00:32:35,885
Speaker 6:  and the meaning of doing that activity may be lost. And I think it is

493
00:32:35,995 --> 00:32:39,820
Speaker 6:  this way of thinking that I think that we're talking about is very much counter

494
00:32:39,905 --> 00:32:43,805
Speaker 6:  to the prevalent thinking in ai, which is, I I unfortunately,

495
00:32:43,925 --> 00:32:47,205
Speaker 6:  I I I'd say is unquestioning uncritical

496
00:32:47,955 --> 00:32:51,925
Speaker 6:  like race for optimization race to like,

497
00:32:52,265 --> 00:32:55,725
Speaker 6:  for AI to outperform humans without asking what do we really want from that?

498
00:32:56,425 --> 00:32:58,845
Speaker 6:  And when is that actually good? And when is that not good?

499
00:33:00,035 --> 00:33:02,845
Speaker 4:  Okay. We have to take one more break and then we're gonna talk about where

500
00:33:02,845 --> 00:33:04,965
Speaker 4:  everything is headed from here. We'll be right back.

501
00:33:53,815 --> 00:33:57,805
Speaker 4:  We're back, I don't know if I'm saying this now to warn you

502
00:33:57,865 --> 00:34:01,555
Speaker 4:  or to get you excited or maybe a little bit of both, but

503
00:34:01,555 --> 00:34:05,515
Speaker 4:  things in this conversation are about to get pretty head until now.

504
00:34:05,515 --> 00:34:09,435
Speaker 4:  We've been talking a lot about what you might call capital a art stuff that

505
00:34:09,435 --> 00:34:13,235
Speaker 4:  is explicitly creative done for the sake of creativity.

506
00:34:13,995 --> 00:34:17,835
Speaker 4:  I think that stuff encounters one set of issues as we look to AI

507
00:34:17,835 --> 00:34:21,665
Speaker 4:  and the future of technology. But then all the way on the other

508
00:34:21,695 --> 00:34:25,065
Speaker 4:  side of the spectrum, you have like navigating government

509
00:34:25,065 --> 00:34:28,625
Speaker 4:  bureaucracy and filling out forms. One thing God mentioned to me was

510
00:34:29,465 --> 00:34:33,425
Speaker 4:  figuring out the healthcare system, terrific use of ai, right? I think

511
00:34:33,425 --> 00:34:37,225
Speaker 4:  that's the kind of stuff that almost everyone would be psyched to turn

512
00:34:37,225 --> 00:34:40,905
Speaker 4:  over to some kind of machine. There are tough questions about

513
00:34:40,905 --> 00:34:44,425
Speaker 4:  whether those machines are good enough to be trusted, how we can rely on

514
00:34:44,425 --> 00:34:48,185
Speaker 4:  them, how they fit into businesses, all that stuff. But in general, that

515
00:34:48,235 --> 00:34:52,145
Speaker 4:  feels like a good use of technology. And then there is this

516
00:34:52,145 --> 00:34:56,025
Speaker 4:  whole messy middle where it's stuff like, I don't know, writing

517
00:34:56,025 --> 00:34:59,785
Speaker 4:  emails. There is something meaningfully human about

518
00:34:59,785 --> 00:35:03,665
Speaker 4:  writing an email to your team at work, but all these AI tools

519
00:35:03,725 --> 00:35:07,665
Speaker 4:  are promising to write that email for you. But then on the other side

520
00:35:07,665 --> 00:35:11,105
Speaker 4:  there, AI tool is saying they'll summarize it for you and at some point it's

521
00:35:11,105 --> 00:35:14,665
Speaker 4:  like, what are we even doing here? Are there any humans in this loop

522
00:35:14,665 --> 00:35:18,515
Speaker 4:  anymore at all? So maybe the three parts of the spectrum here

523
00:35:18,575 --> 00:35:22,395
Speaker 4:  are art, life and nonsense. We're happy to give up nonsense

524
00:35:22,395 --> 00:35:26,235
Speaker 4:  to technology, we're reluctant to give up art and I guess it's

525
00:35:26,235 --> 00:35:30,195
Speaker 4:  case by case on life. So how do we draw the lines

526
00:35:30,265 --> 00:35:34,195
Speaker 4:  between those things though? And how do we decide where it's worth outsourcing

527
00:35:34,375 --> 00:35:38,205
Speaker 4:  and offloading and embracing AI and where we shouldn't? And then maybe

528
00:35:38,315 --> 00:35:42,085
Speaker 4:  most importantly, how do we protect the stuff that's worth protecting

529
00:35:42,185 --> 00:35:45,725
Speaker 4:  on that spectrum? That is the next question I asked. Go Wong.

530
00:35:45,985 --> 00:35:47,565
Speaker 4:  He answered by talking about email.

531
00:35:48,745 --> 00:35:51,365
Speaker 6:  So this is actually the other thing that maybe we don't think about. Like,

532
00:35:51,365 --> 00:35:55,325
Speaker 6:  first of all, I don't, for me, I remember the first time I used an email,

533
00:35:55,425 --> 00:35:59,405
Speaker 6:  it was when I first got to college in 1996, this was the Duke University.

534
00:35:59,665 --> 00:36:03,645
Speaker 6:  And I got an email account and I was like, oh dang, this

535
00:36:03,645 --> 00:36:07,485
Speaker 6:  is so convenient. And I wrote my dad, I was like, dad,

536
00:36:07,705 --> 00:36:10,285
Speaker 6:  oh, I'm writing you an email. I don't have anything to say. I don't write

537
00:36:10,285 --> 00:36:13,685
Speaker 6:  you an email. Okay. Bye. It was like an email with no purpose except to say

538
00:36:13,685 --> 00:36:17,525
Speaker 6:  I'm, I, it's so fun to email. I don't know anyone today who is like,

539
00:36:17,525 --> 00:36:21,085
Speaker 6:  email is so fun. No, no, no. This is, that was, that was

540
00:36:21,205 --> 00:36:24,845
Speaker 6:  1996 for me now in 2024 where like, and also

541
00:36:24,875 --> 00:36:28,725
Speaker 6:  email's not something I do my job without. Right. In fact, this

542
00:36:28,725 --> 00:36:32,645
Speaker 6:  interview was set up through email. Yeah. In fact, I imagine you probably

543
00:36:33,235 --> 00:36:36,965
Speaker 6:  like me, are are are inundated by emails and unfortunately on like a

544
00:36:37,305 --> 00:36:41,125
Speaker 6:  yes unfortunately So I thing that was fun and useful and

545
00:36:41,125 --> 00:36:44,685
Speaker 6:  convenient to begin with because of that

546
00:36:44,925 --> 00:36:48,365
Speaker 6:  convenience. You know, you would think like, oh, it's a labor saving thing.

547
00:36:48,765 --> 00:36:52,525
Speaker 6:  Hmm. And it was for email, but because that labor was

548
00:36:52,525 --> 00:36:55,685
Speaker 6:  saved and we are all using it, we've all saved that labor. Suddenly

549
00:36:56,475 --> 00:37:00,325
Speaker 6:  it's like, oh, we have more time to do well more work. Like

550
00:37:00,635 --> 00:37:04,605
Speaker 6:  between now and say. When you first used email, do you feel like your

551
00:37:04,605 --> 00:37:08,565
Speaker 6:  work has gotten any lighter or any easier? No. No. I

552
00:37:08,565 --> 00:37:09,485
Speaker 6:  don't either. No.

553
00:37:09,905 --> 00:37:13,325
Speaker 4:  All of the studies on this are so alarming that it's like you go back and

554
00:37:13,325 --> 00:37:17,085
Speaker 4:  everybody is like, oh, due to all the increased efficiencies,

555
00:37:17,085 --> 00:37:19,925
Speaker 4:  we're all gonna work for like six hours a week. And it's like, yeah, I do

556
00:37:19,925 --> 00:37:23,445
Speaker 4:  the job y'all used to do in six hours a week and now I have 40 other hours

557
00:37:23,465 --> 00:37:24,165
Speaker 4:  of stuff to do.

558
00:37:24,675 --> 00:37:28,045
Speaker 6:  Yeah. And maybe it's like almost, it's like, I mean there's so many different

559
00:37:28,395 --> 00:37:32,325
Speaker 6:  reasons why that is not like, yeah. It's like, oh, now this thing is

560
00:37:32,325 --> 00:37:36,125
Speaker 6:  so, so, so if we apply that analogy to AI and just email

561
00:37:36,255 --> 00:37:39,975
Speaker 6:  today, it's like, if AI does save us more time on the email,

562
00:37:40,485 --> 00:37:42,895
Speaker 6:  does, do you think that'll just give us more time to actually, I don't know,

563
00:37:43,245 --> 00:37:46,535
Speaker 6:  live our lives to make our cook be with our family?

564
00:37:47,115 --> 00:37:51,055
Speaker 6:  You know, go outside. No, I, IIII would

565
00:37:51,055 --> 00:37:54,975
Speaker 6:  be very not Yeah. Like given like where we are with email, I

566
00:37:54,975 --> 00:37:58,695
Speaker 6:  would, I would be very skeptical of of that possibility.

567
00:37:59,855 --> 00:38:03,735
Speaker 6:  I think, however, what might happen is unfortunately with

568
00:38:04,885 --> 00:38:08,335
Speaker 6:  LLMs and kind of this generative AI technologies that we

569
00:38:09,145 --> 00:38:12,815
Speaker 6:  might produce, not just email, but like writing that is deemed good enough

570
00:38:13,825 --> 00:38:17,765
Speaker 6:  for a certain purpose and now we just need to hire less people to do them.

571
00:38:18,195 --> 00:38:21,965
Speaker 6:  Yeah. For example, and this now starting going back in that creative realm,

572
00:38:21,965 --> 00:38:25,805
Speaker 6:  like for example, like script writing for a TV show,

573
00:38:26,575 --> 00:38:30,035
Speaker 6:  you know, and I I I, I have a, I have an uncle who has been in the industry.

574
00:38:30,215 --> 00:38:33,795
Speaker 6:  He, he lives near LA and he, he was talking about, you know, there was a

575
00:38:33,795 --> 00:38:37,605
Speaker 6:  time when the writing room for a TV show would actually be on

576
00:38:37,625 --> 00:38:41,485
Speaker 6:  set talking to, you know, the, the the production team, the, the, the

577
00:38:41,485 --> 00:38:45,405
Speaker 6:  actors. And in that process actually

578
00:38:45,625 --> 00:38:49,525
Speaker 6:  really got both everyone involved to even be like, really figure out what

579
00:38:49,525 --> 00:38:53,045
Speaker 6:  this is about, what each pers each character actually could be about.

580
00:38:53,425 --> 00:38:57,285
Speaker 6:  It opens up all kinds of imagination for where things

581
00:38:57,285 --> 00:39:00,845
Speaker 6:  could go. And then you have the case where, well now we just hire

582
00:39:01,115 --> 00:39:04,925
Speaker 6:  like a kind of interchangeable room of human writers to

583
00:39:04,925 --> 00:39:08,365
Speaker 6:  write like kind of each episode. And in fact we'll get, we'll do the lowest

584
00:39:08,365 --> 00:39:12,205
Speaker 6:  bidder. And then with AI you can kinda imagine, well now

585
00:39:12,285 --> 00:39:16,045
Speaker 6:  that labor has been saved, well we don't need to hire even those interchangeable

586
00:39:16,045 --> 00:39:19,765
Speaker 6:  room of writers. Right. Instead, let's have one person who prompts and

587
00:39:19,845 --> 00:39:23,005
Speaker 6:  oversees and curates the output of the AI system.

588
00:39:23,705 --> 00:39:27,685
Speaker 6:  And the result might not be good, but it might be good

589
00:39:27,685 --> 00:39:31,125
Speaker 6:  enough Yep. To make entertainment

590
00:39:31,515 --> 00:39:34,645
Speaker 6:  that people will pay for. And I think that's a huge,

591
00:39:35,555 --> 00:39:39,405
Speaker 6:  well it's a huge loss of livelihood, but also a huge cultural loss

592
00:39:39,465 --> 00:39:42,805
Speaker 6:  in terms of the kind of art that is being made.

593
00:39:43,885 --> 00:39:46,725
Speaker 6:  'cause now we have shows that we'll be like, well that's entertaining, but

594
00:39:46,825 --> 00:39:50,445
Speaker 6:  is a real, like, entertaining doesn't mean it's good

595
00:39:50,885 --> 00:39:54,445
Speaker 6:  entertaining doesn't mean it's, it's thoughtful or

596
00:39:54,765 --> 00:39:58,485
Speaker 6:  interesting or, or you know, all of or playful. It just means it's

597
00:39:58,675 --> 00:40:01,765
Speaker 6:  something. Okay. It's better than being bored. Yeah.

598
00:40:02,075 --> 00:40:02,645
Speaker 4:  It's something

599
00:40:02,645 --> 00:40:06,365
Speaker 6:  To do. It's, it's content, it's TikTok to use. Yeah. It's like, it's, yeah.

600
00:40:06,365 --> 00:40:09,845
Speaker 6:  It's, it's use this dreaded like content word, which just means

601
00:40:10,175 --> 00:40:14,145
Speaker 6:  stuff that you can do. And you know, I think the fear

602
00:40:14,145 --> 00:40:17,345
Speaker 6:  is that we'd have, we live in a world where we'd have

603
00:40:17,855 --> 00:40:21,145
Speaker 6:  content that is serviceable

604
00:40:22,075 --> 00:40:25,705
Speaker 6:  often, but that may be sublime never. You know,

605
00:40:25,905 --> 00:40:29,665
Speaker 6:  and, and that's a, a a world of generic and

606
00:40:29,685 --> 00:40:33,625
Speaker 6:  age of generic. And yes, we, a lot of kind of our

607
00:40:34,135 --> 00:40:37,985
Speaker 6:  popular culture is things that are already kind of generic

608
00:40:37,985 --> 00:40:41,345
Speaker 6:  things right now the generic things are still made by humans. Right, right.

609
00:40:41,405 --> 00:40:42,305
Speaker 6:  But like yeah,

610
00:40:42,305 --> 00:40:46,065
Speaker 4:  Humans can do bad work too. It is, we're we're just as bad as anything.

611
00:40:46,065 --> 00:40:46,345
Speaker 4:  Yeah.

612
00:40:46,565 --> 00:40:50,065
Speaker 6:  Yes. But at least like, but if we make, we have

613
00:40:50,065 --> 00:40:53,745
Speaker 6:  machines do all of that, how does that change the truly like

614
00:40:54,415 --> 00:40:58,345
Speaker 6:  sublime, the truly expressive things that humans still do? Will

615
00:40:58,345 --> 00:41:02,065
Speaker 6:  that how I can, I don't know how I'll alter, but I can't imagine this not

616
00:41:02,825 --> 00:41:06,585
Speaker 6:  altering it. Yeah. Right. I, I think, and then on this

617
00:41:06,585 --> 00:41:10,465
Speaker 6:  point of where this intersects with livelihood, you know, I think

618
00:41:10,465 --> 00:41:14,185
Speaker 6:  the fear among a lot of artists, you know, myself

619
00:41:14,465 --> 00:41:17,865
Speaker 6:  included, if I, if I were to count myself as an artist, yes. Part of that

620
00:41:17,865 --> 00:41:21,225
Speaker 6:  is that is livelihood, but it's not only livelihood.

621
00:41:21,765 --> 00:41:25,465
Speaker 6:  The fear isn't just that we'd be replaced by a machine.

622
00:41:25,765 --> 00:41:29,465
Speaker 6:  The fear is that we'd be replaced by something that is far more generic

623
00:41:30,405 --> 00:41:33,445
Speaker 6:  and far less interesting. But that's acceptable

624
00:41:34,395 --> 00:41:38,095
Speaker 6:  to the, the, the, the powers that that be because good

625
00:41:38,095 --> 00:41:41,455
Speaker 6:  enough is it might be good enough to make someone a lot of money.

626
00:41:42,025 --> 00:41:42,375
Speaker 4:  Right.

627
00:41:42,955 --> 00:41:46,625
Speaker 6:  Right. That's the larger fear of, of it's not, isn't just

628
00:41:46,625 --> 00:41:49,785
Speaker 6:  livelihood lost, that's a huge issue, but livelihood lost

629
00:41:50,565 --> 00:41:54,385
Speaker 6:  in favor of something that's actually more generic and less interesting.

630
00:41:54,855 --> 00:41:57,785
Speaker 6:  That seems like undesirable on like multiple levels

631
00:41:58,095 --> 00:42:01,985
Speaker 4:  That maybe our standards go down far enough that we don't care

632
00:42:02,015 --> 00:42:02,505
Speaker 4:  anymore,

633
00:42:02,645 --> 00:42:05,385
Speaker 6:  Our standards go down. Yeah. We don't care anymore. And also that people

634
00:42:05,385 --> 00:42:08,545
Speaker 6:  that otherwise have tried to, it's difficult that it is

635
00:42:09,165 --> 00:42:13,025
Speaker 6:  to pursue any kind of art that there may not, may

636
00:42:13,025 --> 00:42:15,025
Speaker 6:  not be room to do that anymore.

637
00:42:15,095 --> 00:42:16,025
Speaker 4:  Yeah, yeah.

638
00:42:16,025 --> 00:42:17,185
Speaker 6:  Even if someone wanted to.

639
00:42:17,895 --> 00:42:21,665
Speaker 4:  Well, and I think it, it, it strikes me that that comes all the way back

640
00:42:21,665 --> 00:42:25,625
Speaker 4:  around to the idea of how to approach these tools,

641
00:42:25,625 --> 00:42:29,425
Speaker 4:  right? Because I think the, the sort of glass half full version of

642
00:42:29,425 --> 00:42:33,265
Speaker 4:  that future you just described is that actually rather than using

643
00:42:34,165 --> 00:42:37,785
Speaker 4:  AI to pull everything down to this lowest common

644
00:42:37,785 --> 00:42:41,625
Speaker 4:  denominator average of everything, we learn to play it like an

645
00:42:41,625 --> 00:42:45,225
Speaker 4:  instrument and we learn to use it to expand that Venn diagram and this whole

646
00:42:46,145 --> 00:42:49,865
Speaker 4:  universe of things that we can't do, but we learn how to do

647
00:42:49,865 --> 00:42:53,705
Speaker 4:  with this tool. Like that If you If you wanna paint like a beautiful future

648
00:42:53,725 --> 00:42:57,465
Speaker 4:  of ai. It's, it's that sort of spirit of play that you're

649
00:42:57,465 --> 00:43:00,985
Speaker 4:  describing comes to everyone in all disciplines of ai,

650
00:43:01,155 --> 00:43:04,545
Speaker 4:  right? Like that's, that's the hope. That's the only thing that might work.

651
00:43:05,615 --> 00:43:09,265
Speaker 6:  That would be nice except, I mean there, but there's every reason unfortunately

652
00:43:09,325 --> 00:43:13,185
Speaker 6:  to, to, to think about the challenges to that scenario where everyone has

653
00:43:13,185 --> 00:43:16,905
Speaker 6:  a tools to play. One is just human nature itself, including my own,

654
00:43:17,195 --> 00:43:20,905
Speaker 6:  which is the kind of playfulness we're talking about is

655
00:43:21,505 --> 00:43:25,045
Speaker 6:  actually investment. Yes. Of time and effort

656
00:43:25,305 --> 00:43:28,925
Speaker 6:  and probably a lot of frustration and confusion. It's part of the process

657
00:43:29,225 --> 00:43:32,925
Speaker 6:  of doing anything I think worthwhile that you, you

658
00:43:33,165 --> 00:43:36,765
Speaker 6:  derive meaning from whether there's a difficult video game or learning a

659
00:43:36,765 --> 00:43:39,965
Speaker 6:  new thing or getting better at an instrument. All of these things come with

660
00:43:40,025 --> 00:43:44,005
Speaker 6:  its, its challenge or, or like basically climbing up to, to to, to a mountain,

661
00:43:44,365 --> 00:43:48,285
Speaker 6:  right? I say climbing the mountain and the other C cha. So that's one

662
00:43:48,285 --> 00:43:52,045
Speaker 6:  challenge is that we actually like, you know, we actually, we need a

663
00:43:52,045 --> 00:43:55,645
Speaker 6:  supportive environment where we do actually can do that, have the time

664
00:43:56,065 --> 00:43:59,925
Speaker 6:  and to do that and, and have the will and the desire,

665
00:43:59,945 --> 00:44:03,925
Speaker 6:  the motivation to do that. But as, as we're saying, like life is

666
00:44:03,925 --> 00:44:07,725
Speaker 6:  not getting easier for, for, for most people. In fact, it's kind of getting

667
00:44:07,725 --> 00:44:11,645
Speaker 6:  harder for most people. And I would love for that timeline to come to pass.

668
00:44:11,665 --> 00:44:15,445
Speaker 6:  But it's, it's difficult. It's, it's, it's increasingly a higher and higher

669
00:44:15,445 --> 00:44:19,245
Speaker 6:  privilege to be able to have time to do things just because,

670
00:44:20,255 --> 00:44:24,115
Speaker 6:  you know, that's, that's the, the, the hard part of our reality.

671
00:44:24,185 --> 00:44:27,835
Speaker 6:  Yeah. And it's only gonna get harder. For example, like I mentioned, the

672
00:44:27,835 --> 00:44:31,645
Speaker 6:  instrument where you use your hand and computer vision and AI to kind of

673
00:44:31,645 --> 00:44:33,725
Speaker 6:  track the opening and closing your hand to, to sing

674
00:44:35,675 --> 00:44:39,495
Speaker 6:  that needs a different way to think about AI that needs a

675
00:44:39,495 --> 00:44:43,355
Speaker 6:  different human computer interaction. That is perhaps not,

676
00:44:43,375 --> 00:44:46,755
Speaker 6:  I'm not saying get rid of prompt based engineering. I'm saying yes,

677
00:44:47,365 --> 00:44:51,075
Speaker 6:  there should be room for this other ways of working with ai. That's,

678
00:44:51,075 --> 00:44:54,835
Speaker 6:  that's interactively at the very least different and actually speaks to all

679
00:44:54,835 --> 00:44:58,075
Speaker 6:  these other dimensions of, of who we are, including our bodies, our physical

680
00:44:58,075 --> 00:45:02,035
Speaker 6:  bodies, but also all the, all the things that, that that humans

681
00:45:02,895 --> 00:45:06,395
Speaker 6:  are, you know, or could be our capacities. They should be more

682
00:45:06,655 --> 00:45:10,275
Speaker 6:  comprehensively considered in in how we do.

683
00:45:10,465 --> 00:45:14,355
Speaker 6:  Unfortunately, I think there's way more evidence

684
00:45:14,455 --> 00:45:18,275
Speaker 6:  to the contrary that AI is not headed in a

685
00:45:18,275 --> 00:45:21,635
Speaker 6:  direction of inclusion, rather extraction.

686
00:45:22,065 --> 00:45:25,755
Speaker 6:  It's as in how can we extract value from people

687
00:45:26,025 --> 00:45:29,435
Speaker 6:  Yeah. With this thing. And it's not a tool for human

688
00:45:29,905 --> 00:45:33,475
Speaker 6:  wellbeing, it's not a tool for human flourishing. It's a tool for maximizing

689
00:45:33,475 --> 00:45:33,835
Speaker 6:  profit,

690
00:45:34,325 --> 00:45:38,315
Speaker 4:  Which involves removing humans from loops in every exactly possible way.

691
00:45:38,455 --> 00:45:41,875
Speaker 4:  And like you mentioned, all of the easy money in this space is at the middle

692
00:45:41,875 --> 00:45:45,635
Speaker 4:  of that Venn diagram. Like there might be other things in other places

693
00:45:45,635 --> 00:45:48,555
Speaker 4:  someday, but like If, you wanna make a lot of money right now it's at the

694
00:45:48,555 --> 00:45:51,875
Speaker 4:  middle of that Venn diagram and it's, how do I take a job humans used to

695
00:45:51,875 --> 00:45:53,155
Speaker 4:  do and take it away from them?

696
00:45:53,575 --> 00:45:56,595
Speaker 6:  But you don't ask, okay, so now we have this and if everyone actually used

697
00:45:56,595 --> 00:46:00,275
Speaker 6:  this thing like email what now? Right. They don't think

698
00:46:00,275 --> 00:46:04,075
Speaker 6:  that's a cultural question that requires, but no, it's, it's

699
00:46:04,075 --> 00:46:07,955
Speaker 6:  that that's not being considered. And yes, I get, having

700
00:46:07,985 --> 00:46:11,955
Speaker 6:  been a, a startup co-founder, I can get that. It's nev

701
00:46:12,515 --> 00:46:16,235
Speaker 6:  starting a business, a startup or running a business is not an easy thing.

702
00:46:16,235 --> 00:46:19,875
Speaker 6:  It's an exceedingly difficult thing. I've been there and yet

703
00:46:20,515 --> 00:46:24,345
Speaker 6:  I think with ai there's an like an added dimension of of of

704
00:46:24,345 --> 00:46:28,265
Speaker 6:  just social accountability. Yeah. Because the things you use are gonna

705
00:46:28,265 --> 00:46:32,205
Speaker 6:  be used by people that you are never gonna meet, but whose lives that you

706
00:46:32,205 --> 00:46:35,885
Speaker 6:  will affect and influence, you'll affect not only their lives and

707
00:46:35,885 --> 00:46:39,205
Speaker 6:  affect their communities and affect their families,

708
00:46:39,855 --> 00:46:40,805
Speaker 6:  their kids.

709
00:46:42,625 --> 00:46:46,245
Speaker 6:  And while you may be like, well how do I survive even to the next

710
00:46:47,155 --> 00:46:50,605
Speaker 6:  quarterly call? Yeah, I get it.

711
00:46:51,155 --> 00:46:55,035
Speaker 6:  Been there. But I think there's an added dimension of

712
00:46:55,035 --> 00:46:58,995
Speaker 6:  of of, i I think social responsibility to think what if everyone,

713
00:46:59,065 --> 00:47:02,915
Speaker 6:  it's not just like, what if the things we made went awry? It's

714
00:47:02,915 --> 00:47:06,835
Speaker 6:  like, what are the things I make actually worked and a lot of

715
00:47:06,835 --> 00:47:10,585
Speaker 6:  people used it. Then what, how does that change the very culture

716
00:47:10,995 --> 00:47:14,785
Speaker 6:  we're living in? I think the through line in all of these is kind of this,

717
00:47:14,855 --> 00:47:18,625
Speaker 6:  this question a cri a kind of insistence to

718
00:47:18,865 --> 00:47:22,825
Speaker 6:  critically question what we do and a aesthetic social,

719
00:47:23,305 --> 00:47:26,665
Speaker 6:  cultural dimension in addition to all the other dimensions that I,

720
00:47:27,015 --> 00:47:30,885
Speaker 6:  well I would mention in there and also another through line is, is

721
00:47:30,955 --> 00:47:34,705
Speaker 6:  play and expression. You know, can we help humans

722
00:47:34,895 --> 00:47:38,685
Speaker 6:  feel tools for people to, to feel more like themselves?

723
00:47:38,945 --> 00:47:42,725
Speaker 6:  That's, I think that's what play is and I think if we can do that,

724
00:47:43,435 --> 00:47:47,285
Speaker 6:  I'll take those as victories. You know, if someone is playing music for

725
00:47:47,285 --> 00:47:51,205
Speaker 6:  the fun of it, like inside the room for no, no one for no one

726
00:47:51,205 --> 00:47:55,175
Speaker 6:  in particular, I think that's a win already. And I think there's

727
00:47:55,175 --> 00:47:58,935
Speaker 6:  things that if we just did it actually makes us, you know,

728
00:47:58,995 --> 00:48:02,975
Speaker 6:  in however small ways makes us feel more ourselves. And,

729
00:48:03,115 --> 00:48:06,895
Speaker 6:  and I, that's my, I still hold out hope if only

730
00:48:06,895 --> 00:48:10,695
Speaker 6:  because hope must be there that we can

731
00:48:11,495 --> 00:48:15,195
Speaker 6:  use technology as a tool for perhaps above all for humans

732
00:48:15,295 --> 00:48:19,075
Speaker 6:  to be more themselves, to feel more themselves. And, and,

733
00:48:19,215 --> 00:48:23,195
Speaker 6:  and to do that you need to feel included, feel safe, feel

734
00:48:23,195 --> 00:48:27,115
Speaker 6:  free to be yourselves and I think feel understood, you know, that's, I think

735
00:48:27,135 --> 00:48:31,115
Speaker 6:  that's, that's my hope. But, but it's my gosh, there's so

736
00:48:31,115 --> 00:48:34,955
Speaker 6:  much work to be done and so many challenges and that's not the way that the

737
00:48:34,955 --> 00:48:38,755
Speaker 6:  world is going, but it's, it's a thing that's, I think it's, it's worth,

738
00:48:39,425 --> 00:48:40,715
Speaker 6:  it's worth working towards.

739
00:48:42,145 --> 00:48:45,515
Speaker 4:  Alright, that is it for the first Vergecast Today. Thank you again to Goong

740
00:48:45,515 --> 00:48:49,155
Speaker 4:  for being here and thank you as always for listening. I'll put a whole bunch

741
00:48:49,155 --> 00:48:52,395
Speaker 4:  of links to all the stuff we talked about in the show notes, the Aina app,

742
00:48:52,395 --> 00:48:55,995
Speaker 4:  all the S Mule stuff, the laptop orchestra, some of the funny stuff his students

743
00:48:55,995 --> 00:48:58,555
Speaker 4:  have worked on. We'll put it all in the show notes. You should check it all

744
00:48:58,555 --> 00:49:02,355
Speaker 4:  out. It is weird and I mean that in the best possible way.

745
00:49:03,185 --> 00:49:07,035
Speaker 4:  Also lots more on all of this stuff, future of music and everything else

746
00:49:07,215 --> 00:49:10,955
Speaker 4:  in this series at The Verge dot com. It's a good website. We like it.

747
00:49:11,415 --> 00:49:14,995
Speaker 4:  As always, If you have thoughts, questions, feelings, or other songs that

748
00:49:14,995 --> 00:49:17,875
Speaker 4:  you think I should learn in that Karina app that's really cool. You can always

749
00:49:17,875 --> 00:49:21,315
Speaker 4:  email us at Vergecast at The Verge dot com or call the hotline

750
00:49:21,315 --> 00:49:24,755
Speaker 4:  eight six six VERGE one one. If, you have thoughts on anything that God and

751
00:49:24,755 --> 00:49:28,155
Speaker 4:  I talked about today, anything from this whole series, really anything at

752
00:49:28,155 --> 00:49:31,715
Speaker 4:  all? Call us. we love hearing from you. This show is produced by Liam James,

753
00:49:31,895 --> 00:49:35,195
Speaker 4:  Wil Poor and Eric Gomez. The Vergecast is a VERGE production and part of

754
00:49:35,195 --> 00:49:38,515
Speaker 4:  the Vox Media Podcast network. We'll be back with your regularly scheduled

755
00:49:38,515 --> 00:49:42,115
Speaker 4:  programming on Tuesday and Friday. The news just keeps happening,

756
00:49:42,205 --> 00:49:45,915
Speaker 4:  y'all just a lot of news. But I did manage to squeeze in a

757
00:49:45,915 --> 00:49:49,675
Speaker 4:  conversation about 13th Century Florence for Tuesday's

758
00:49:49,675 --> 00:49:52,875
Speaker 4:  episode. So that one's a fun one. We'll see you then. Rock and roll

