1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 9dafd27c-1fbc-473d-9230-158bf36bbdb0
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/6155755077619046190/255370180714847303/s93290-US-5684s-1722623394.mp3
Description: The Verge's Nilay Patel, Allison Johnson, and Victoria Song discuss Apple iOS 18.1 beta. upcoming Pixel 9 rumors, Olympics coverage, AI deepfake regulation, and more.

2
00:01:14,955 --> 00:01:18,255
Speaker 3:  for good reason I think. So we're just gonna see what happens. Thank you

3
00:01:18,255 --> 00:01:19,335
Speaker 3:  too for, for joining me.

4
00:01:19,545 --> 00:01:22,615
Speaker 2:  We're here for the ride. I'm here for Saturday Samsung

5
00:01:23,555 --> 00:01:27,175
Speaker 2:  as the, as the local Korean gadget reviewer on staff.

6
00:01:27,655 --> 00:01:29,855
Speaker 2:  I am so here for Saturday Samsung.

7
00:01:30,265 --> 00:01:33,695
Speaker 3:  We're gonna get to what the Saturday Samsung is This I would say,

8
00:01:34,555 --> 00:01:38,335
Speaker 3:  you know, up until now it's been, there's stuff Samsung will give you a free

9
00:01:38,335 --> 00:01:41,975
Speaker 3:  TV if you buy a TV is a real idea generated by Saturday

10
00:01:41,975 --> 00:01:45,895
Speaker 3:  Samsung. This week's is, I think bananas in a different

11
00:01:45,895 --> 00:01:47,975
Speaker 3:  way, but we'll, we'll come to it. There's a bunch of other stuff going on.

12
00:01:47,995 --> 00:01:51,775
Speaker 3:  The iOS 18.1 developer beta with Apple Intelligence, some

13
00:01:51,775 --> 00:01:55,655
Speaker 3:  Apple Intelligence features hit Allison, you played with that V you

14
00:01:55,855 --> 00:01:58,975
Speaker 3:  reviewed the Galaxy Watch Ultra, or as I call it the Apple Watch Ultra.

15
00:01:59,685 --> 00:02:02,935
Speaker 3:  There's some other gadget news. There's a Pixel nine event coming up.

16
00:02:03,715 --> 00:02:07,615
Speaker 3:  We caused an entire furious news cycle about subscription

17
00:02:07,895 --> 00:02:11,455
Speaker 3:  mice. I'm sorry. And then because we have been devices,

18
00:02:12,135 --> 00:02:15,735
Speaker 3:  I am trying a new style of lightning round. Still unsponsored,

19
00:02:16,755 --> 00:02:20,255
Speaker 3:  but it's, you know, get honest, David, David, David and Liam are not around

20
00:02:20,255 --> 00:02:23,295
Speaker 3:  to put me in a box. So we're gonna, we're gonna try something new.

21
00:02:23,985 --> 00:02:24,335
Speaker 2:  Chaos.

22
00:02:25,125 --> 00:02:29,015
Speaker 3:  It's, it's chaos. Chaos. I wanna start with, we should just call it

23
00:02:29,015 --> 00:02:32,895
Speaker 3:  the cast run, although I think, so one of the topics is a little serious,

24
00:02:32,915 --> 00:02:36,575
Speaker 3:  so I dunno if we, oh, we'll get there. We're, we're gonna get there. We're

25
00:02:36,575 --> 00:02:37,535
Speaker 3:  doing it live. Everybody.

26
00:02:39,545 --> 00:02:43,015
Speaker 3:  Let's start with, there's big news in the week just outside of tech. It's

27
00:02:43,015 --> 00:02:46,935
Speaker 3:  the Olympics. It feels like the

28
00:02:46,985 --> 00:02:50,975
Speaker 3:  experience of watching consuming the Olympics is very

29
00:02:50,975 --> 00:02:54,815
Speaker 3:  different this year than in years past. Every other year we run a story that's

30
00:02:54,815 --> 00:02:57,975
Speaker 3:  like streaming the Olympics sucks. We haven't really had to run that story

31
00:02:57,975 --> 00:02:59,405
Speaker 3:  this year. It's

32
00:02:59,485 --> 00:03:03,125
Speaker 2:  'cause it's on TikTok baby. I have not streamed

33
00:03:03,445 --> 00:03:07,085
Speaker 2:  a single second of the Olympics. I'm telling you, I have not streamed a single

34
00:03:07,085 --> 00:03:10,805
Speaker 2:  second of the Olympics. Normally I am all over the gymnastics, but everything

35
00:03:10,845 --> 00:03:13,605
Speaker 2:  I know about the Olympics has just come through doom, doom. Scrolling my

36
00:03:13,605 --> 00:03:17,485
Speaker 2:  TikTok feed and just going like, oh actually that's kind of really

37
00:03:17,485 --> 00:03:21,285
Speaker 2:  wholesome. I really love chocolate muffin guy and yeah,

38
00:03:21,655 --> 00:03:22,645
Speaker 2:  can't remember his name.

39
00:03:22,645 --> 00:03:26,365
Speaker 4:  Yeah. Explaining Chocolate muffin guy. This is a foreign

40
00:03:26,365 --> 00:03:27,125
Speaker 4:  concept to me.

41
00:03:28,035 --> 00:03:31,725
Speaker 2:  It's, it's just this Norwegian athlete who has discovered the chocolate

42
00:03:31,745 --> 00:03:35,445
Speaker 2:  muffin from the Olympic Village cafeteria

43
00:03:35,545 --> 00:03:39,365
Speaker 2:  and he gets increasingly weird with this chocolate muffin. It just like

44
00:03:39,365 --> 00:03:42,605
Speaker 2:  appears in his bedroom and he's in a thong and you're like, what is happening?

45
00:03:43,265 --> 00:03:46,965
Speaker 2:  But, oh, that's what I'm saying. She really loves

46
00:03:47,475 --> 00:03:48,285
Speaker 2:  this chocolate muffin.

47
00:03:48,945 --> 00:03:50,725
Speaker 3:  You know what's happening. That's true.

48
00:03:52,235 --> 00:03:56,205
Speaker 3:  Yeah. Jim trying TikTok is a weird way to feel about America right

49
00:03:56,205 --> 00:03:59,925
Speaker 3:  now. This is hard to know how much American pride you have, depending on

50
00:03:59,925 --> 00:04:03,845
Speaker 3:  what TikTok scroll you're in. Like at this moment it's like I have

51
00:04:03,845 --> 00:04:07,525
Speaker 3:  immense pride in our gymnasts who've crushed it. Once again.

52
00:04:07,745 --> 00:04:11,605
Speaker 3:  Oh no, he's talking again. I have immense pride in this. Like it's

53
00:04:11,605 --> 00:04:15,565
Speaker 3:  what is happening. That's a lot. We have an entire story about

54
00:04:16,425 --> 00:04:20,285
Speaker 3:  the Olympics in TikTok. Mia wrote it. I actually wanted the title

55
00:04:20,305 --> 00:04:21,645
Speaker 3:  to be the Influencer Olympics,

56
00:04:23,305 --> 00:04:25,125
Speaker 3:  but I think that's what people think. That's Coachella.

57
00:04:25,905 --> 00:04:26,125
Speaker 2:  So

58
00:04:27,675 --> 00:04:27,965
Speaker 3:  Both

59
00:04:27,985 --> 00:04:28,805
Speaker 2:  Are fair, but it's true.

60
00:04:29,195 --> 00:04:32,965
Speaker 3:  Yeah, I thought, I thought it was good pun. But it is true that the athletes

61
00:04:33,385 --> 00:04:36,925
Speaker 3:  are young by and large, very young. Except for the 51-year-old Turkish man

62
00:04:37,305 --> 00:04:40,685
Speaker 3:  who got a silver medal in shooting with no equipment whatsoever. Great photo.

63
00:04:41,785 --> 00:04:45,765
Speaker 3:  But it's true that the athletes are by and large, very young. The US

64
00:04:45,765 --> 00:04:48,965
Speaker 3:  women's gymnastics team, literally after winning gold was caught on camera

65
00:04:48,995 --> 00:04:52,765
Speaker 3:  talking about what TikTok they wanted to make. Oh yeah. Which is

66
00:04:52,765 --> 00:04:56,485
Speaker 3:  incredible. Aw. And so you just have this like very native

67
00:04:56,615 --> 00:05:00,085
Speaker 3:  population to this format in this platform in particular.

68
00:05:00,625 --> 00:05:04,365
Speaker 3:  And then there's just the universe of piracy that happens on TikTok. Like

69
00:05:04,505 --> 00:05:08,485
Speaker 3:  if you wanna watch clips of cool sporting events, the TikTok community

70
00:05:08,485 --> 00:05:12,045
Speaker 3:  has you covered. And tiktoks lawyers also apparently have you covered

71
00:05:12,195 --> 00:05:15,725
Speaker 3:  because they don't seem to care about it. And that's like one really

72
00:05:16,045 --> 00:05:19,205
Speaker 3:  interesting way of consuming this, right? You're getting kind of the raw

73
00:05:19,315 --> 00:05:23,085
Speaker 3:  feed by people who are on the ground who speak in the language of video speaking,

74
00:05:23,085 --> 00:05:25,525
Speaker 3:  the language of social video. And then you're getting a bunch of clips of

75
00:05:25,525 --> 00:05:29,245
Speaker 3:  cool stuff curated by a community that does not care about

76
00:05:29,575 --> 00:05:31,085
Speaker 3:  NBC's exclusive rights to stream

77
00:05:32,275 --> 00:05:35,925
Speaker 2:  Pick on TikTok. What I'll say is the commentary on TikTok is much

78
00:05:35,995 --> 00:05:39,765
Speaker 2:  funnier because you just have people filming their screen while

79
00:05:39,765 --> 00:05:43,325
Speaker 2:  they're watching it. Like the, the men's gymnast pommel horse guy, I, I don't

80
00:05:43,325 --> 00:05:46,885
Speaker 2:  know any of these athletes names. I know them by their memes on TikTok. But

81
00:05:46,885 --> 00:05:50,445
Speaker 2:  just like I was watching these two women watching Paul Pommel Horse Guy and

82
00:05:50,445 --> 00:05:54,165
Speaker 2:  they're like, yes, yes Peter Parker. Yes, you better

83
00:05:54,165 --> 00:05:57,685
Speaker 2:  work. You better work. And just having this very sassy

84
00:05:58,055 --> 00:06:02,005
Speaker 2:  commentary about this man doing the pommel horse. And I was just

85
00:06:02,005 --> 00:06:05,645
Speaker 2:  like, this is, I am watching them watch their TV

86
00:06:05,785 --> 00:06:09,245
Speaker 2:  screen and they're just better than like the NBC

87
00:06:09,245 --> 00:06:12,685
Speaker 2:  commentary people who are just like, oh yes, he's on the pommel horse. And

88
00:06:12,685 --> 00:06:16,485
Speaker 2:  he did the skill. Oh so serious. USA so

89
00:06:16,485 --> 00:06:20,325
Speaker 2:  serious. It's like, it's like watching it with your friends except my, my

90
00:06:20,325 --> 00:06:23,125
Speaker 2:  husband doesn't give a crap about the Olympics, so I'm just like, same like

91
00:06:23,715 --> 00:06:27,645
Speaker 2:  yeah, pommel horse guy You can get it. Clark Kent of

92
00:06:27,645 --> 00:06:28,205
Speaker 2:  Pommel horse.

93
00:06:28,785 --> 00:06:32,645
Speaker 3:  So I often say that almost every experience I have with TikTok

94
00:06:32,665 --> 00:06:36,445
Speaker 3:  should be a PhD in media studies. What you are describing as a PhD

95
00:06:36,545 --> 00:06:40,485
Speaker 3:  in media studies, right? Like the layers of abstraction away from the thing

96
00:06:40,485 --> 00:06:44,245
Speaker 3:  itself happening and then the entertainment that random strangers in the

97
00:06:44,325 --> 00:06:48,045
Speaker 3:  internet are providing you. The all of that is just very different. and it

98
00:06:48,045 --> 00:06:51,765
Speaker 3:  feels new this time in a way that two years ago,

99
00:06:51,765 --> 00:06:54,845
Speaker 3:  basic three years ago at the last Olympics,

100
00:06:55,585 --> 00:06:58,805
Speaker 3:  it was it two years ago or three years ago? The last Olympics were freshman

101
00:06:58,805 --> 00:07:02,405
Speaker 2:  Three. They were in 2021 because of, you know,

102
00:07:02,855 --> 00:07:06,645
Speaker 4:  Covid Oh, events. Yeah. Yeah. That's what the Olympics just

103
00:07:06,745 --> 00:07:10,725
Speaker 4:  has always felt to me. Like this big serious important

104
00:07:10,725 --> 00:07:14,685
Speaker 4:  thing And like that's kind of part of the appeal. But there's something great

105
00:07:14,685 --> 00:07:18,525
Speaker 4:  about like yeah, having it kind of distributed among

106
00:07:18,595 --> 00:07:22,525
Speaker 4:  like, you know, from people on the ground and having more fun

107
00:07:22,525 --> 00:07:26,365
Speaker 4:  with it. Like it doesn't have to be this like big booming voice of like

108
00:07:26,845 --> 00:07:30,565
Speaker 4:  NBC will deliver you the Olympics and that is how you will watch them.

109
00:07:31,175 --> 00:07:34,605
Speaker 3:  Right? If you put 18,000 teenagers in the middle of France with cell phones,

110
00:07:34,635 --> 00:07:36,165
Speaker 3:  like something funny is gonna happen.

111
00:07:36,605 --> 00:07:36,805
Speaker 4:  Yeah.

112
00:07:37,495 --> 00:07:39,725
Speaker 3:  Right. That's it. That, that seems correct.

113
00:07:40,225 --> 00:07:43,405
Speaker 2:  You just learn more about different sports. 'cause I don't care about rugby.

114
00:07:43,805 --> 00:07:47,725
Speaker 2:  I love alone Amar, she's hilarious on TikTok. She's just all

115
00:07:47,725 --> 00:07:51,405
Speaker 2:  over my entire feed. Yesterday was just lesbians crying

116
00:07:51,515 --> 00:07:55,085
Speaker 2:  that alone a Marv was not a lesbian because they were just like

117
00:07:55,385 --> 00:07:58,085
Speaker 2:  in love with the women's rugby team. And I was like, and that'll be it

118
00:07:58,305 --> 00:07:59,405
Speaker 3:  For chaos Forecast everybody.

119
00:08:01,255 --> 00:08:04,445
Speaker 3:  We're gonna end it here. No, I mean, but that right. The idea that different

120
00:08:04,475 --> 00:08:08,165
Speaker 3:  communities, different groups can view the thing

121
00:08:08,445 --> 00:08:12,285
Speaker 3:  together in a different way as a community is new. Like that just is a new

122
00:08:12,285 --> 00:08:15,405
Speaker 3:  thing that even like sports Twitter doesn't accomplish,

123
00:08:16,055 --> 00:08:19,885
Speaker 3:  right? It's, it's the native video communication

124
00:08:19,885 --> 00:08:23,485
Speaker 3:  ability of particularly the younger generation that like is interesting and

125
00:08:23,485 --> 00:08:27,405
Speaker 3:  new. And that's one thing that's different about these Olympics. The

126
00:08:27,405 --> 00:08:30,805
Speaker 3:  other thing that's different is that actually NBC and Peacock are doing a

127
00:08:30,805 --> 00:08:34,565
Speaker 3:  good job. I must disclose that NBC is a minority

128
00:08:34,885 --> 00:08:38,605
Speaker 3:  investor in Vox mania apparent company. They're owned by Comcast. I've

129
00:08:38,605 --> 00:08:41,445
Speaker 3:  rarely said anything good about Comcast, so I must disclose, I'm saying something

130
00:08:41,445 --> 00:08:44,925
Speaker 3:  nice about Comcast NBC. That's the first time I believe in history

131
00:08:45,525 --> 00:08:49,205
Speaker 3:  that that has happened. But they are doing a good job. It is actually like

132
00:08:49,215 --> 00:08:52,885
Speaker 3:  refreshing and interesting to see a big

133
00:08:52,885 --> 00:08:56,685
Speaker 3:  legacy broadcaster just go for it with her app.

134
00:08:56,685 --> 00:09:00,605
Speaker 3:  And it's not saying there's not problems there. Random ad breaks in the

135
00:09:00,605 --> 00:09:04,245
Speaker 3:  middle of stuff. There's infinite complaints about just how overwhelming

136
00:09:04,305 --> 00:09:08,125
Speaker 3:  it is. But it's like ev they had, they had every idea

137
00:09:08,505 --> 00:09:12,285
Speaker 3:  and executed every idea. So if you want to just watch

138
00:09:12,285 --> 00:09:15,765
Speaker 3:  highlights of the sports You can just watch highlights of the sports. If

139
00:09:15,765 --> 00:09:19,485
Speaker 3:  you wanna watch the traditional gauzy 1980 style

140
00:09:19,955 --> 00:09:23,685
Speaker 3:  primetime broadcast with completely unnecessary human interest

141
00:09:23,815 --> 00:09:27,525
Speaker 3:  interludes about how the gymnasts grew up, knowing they would be gymnasts.

142
00:09:27,935 --> 00:09:30,445
Speaker 3:  Those stories are always the same. It's like when they were four, they're

143
00:09:30,445 --> 00:09:34,285
Speaker 3:  like, I'm gonna jump on stuff every, every four years. You can get that story

144
00:09:34,315 --> 00:09:38,125
Speaker 3:  from NBC if you want, but that's available to you. They have the Gold Zone,

145
00:09:38,125 --> 00:09:41,805
Speaker 3:  which is basically NFL red zone for the Olympics that they're doing 10 hours

146
00:09:41,965 --> 00:09:45,365
Speaker 3:  a day. And inside of the app when you're watching stuff, You can just be

147
00:09:45,365 --> 00:09:47,965
Speaker 3:  like, I just wanna keep watching this thing that we're whipping around to,

148
00:09:47,965 --> 00:09:51,885
Speaker 3:  which is fascinating. They have multi-view, which is the more standard,

149
00:09:51,885 --> 00:09:55,845
Speaker 3:  just like, here's four feeds at once. They have Al Michaels doing

150
00:09:56,105 --> 00:09:58,445
Speaker 3:  AI powered highlights that You can just listen

151
00:09:58,465 --> 00:10:00,165
Speaker 4:  To. Oh, okay. Which

152
00:10:00,165 --> 00:10:04,005
Speaker 3:  Is super weird. Like it's just weird to have Robot

153
00:10:04,205 --> 00:10:07,925
Speaker 3:  Al Michaels being like the gymnast trumped over stuff again. But they,

154
00:10:07,955 --> 00:10:11,925
Speaker 3:  it's like all the ideas. I had someone in my mentions, 'cause I, I was

155
00:10:11,925 --> 00:10:15,485
Speaker 3:  posting on threads and they're like, all of this is stuff that a normal company

156
00:10:15,485 --> 00:10:19,365
Speaker 3:  would like de-scope to hit the deadline. And N BBC just

157
00:10:19,365 --> 00:10:22,405
Speaker 3:  like did it all. And then on top of it they're like, do all of, it's like

158
00:10:22,405 --> 00:10:25,565
Speaker 3:  pretty good, like Gold Zone is actually really funny. Do you, there's like

159
00:10:25,675 --> 00:10:28,565
Speaker 3:  very funny NFL trauma inside of Gold Zone.

160
00:10:30,305 --> 00:10:32,965
Speaker 3:  So you know, there's Red Zone where, where you like whip around all the games

161
00:10:33,225 --> 00:10:36,565
Speaker 3:  on Sundays and there there used to be two Red Zones. There was the one on

162
00:10:36,565 --> 00:10:40,365
Speaker 3:  DirecTV and there was the one that the NFL did Google bought the

163
00:10:40,365 --> 00:10:44,325
Speaker 3:  rights to Sunday ticket. They took the NFL Red Zone and they shut down the

164
00:10:44,325 --> 00:10:48,125
Speaker 3:  DirecTV red zone. So Scott Hanson, who does the NFL Red Zone

165
00:10:48,145 --> 00:10:52,005
Speaker 3:  is like gonna do it with Google. Now, NBC realized they can't just

166
00:10:52,005 --> 00:10:55,925
Speaker 3:  have Scott Hanson 10 hours a day, 16 days

167
00:10:55,925 --> 00:10:59,685
Speaker 3:  in a row. So they hire Scott Hanson and the guy who used to do the

168
00:10:59,685 --> 00:11:02,885
Speaker 3:  DirecTV Red Zone to do Gold Zone. So they're like back together. Not his

169
00:11:02,885 --> 00:11:06,845
Speaker 3:  rivals, but his friends like all, it's like hilarious that they did

170
00:11:06,845 --> 00:11:06,965
Speaker 3:  that. This

171
00:11:06,965 --> 00:11:09,525
Speaker 4:  This is a PhD. This is a PhD.

172
00:11:10,225 --> 00:11:12,925
Speaker 3:  But they had to do it like somebody at N BBC had to be like, all right, let's

173
00:11:12,925 --> 00:11:15,565
Speaker 3:  like, let's do a red zone for Olympics. How will we do that? We should just

174
00:11:15,565 --> 00:11:16,645
Speaker 3:  get all the red zone guys.

175
00:11:16,965 --> 00:11:17,245
Speaker 4:  Yeah.

176
00:11:17,785 --> 00:11:21,285
Speaker 3:  And like go get some contracts together, like make it happen. and it all

177
00:11:21,285 --> 00:11:25,205
Speaker 3:  is kind of working like it's working better than if

178
00:11:25,245 --> 00:11:28,965
Speaker 3:  I suggested to you NBC was gonna make an app full of

179
00:11:29,285 --> 00:11:32,565
Speaker 3:  Olympics content, your expectations would be very low.

180
00:11:33,585 --> 00:11:36,605
Speaker 3:  And it's called Peacock. But they're actually doing, they're doing good.

181
00:11:37,645 --> 00:11:41,005
Speaker 4:  I love that. I'm an Olympic lover. Yeah, I love that. You know, are you

182
00:11:41,125 --> 00:11:42,965
Speaker 3:  Watching a peacock or are you, are you like full TikTok?

183
00:11:43,945 --> 00:11:47,885
Speaker 4:  So I am, I have to put an asterisk that like I'm a Winter Olympics

184
00:11:47,885 --> 00:11:51,605
Speaker 4:  girl. Mm. And I, I don't know what it is this year, but I've been like

185
00:11:51,675 --> 00:11:54,965
Speaker 4:  shouted down by the Summer Olympics people about how the Winter Olympics

186
00:11:55,025 --> 00:11:58,565
Speaker 4:  is not the real Olympics. And I, I think I just like got tired of it. So

187
00:11:58,565 --> 00:12:02,525
Speaker 4:  I I've kind of been missing out on, on this year's games.

188
00:12:02,585 --> 00:12:02,805
Speaker 4:  You

189
00:12:02,805 --> 00:12:05,485
Speaker 2:  Can't, there's, there's Snoop Dogg, like between

190
00:12:05,485 --> 00:12:07,405
Speaker 4:  Everything I'm getting NBC and

191
00:12:08,725 --> 00:12:12,285
Speaker 2:  Decision to have Snoop do and Flava Flav here. I'm like, this

192
00:12:12,285 --> 00:12:15,925
Speaker 3:  Is what I mean by NBC just went like somewhat NBC is like, I'm, is there

193
00:12:15,925 --> 00:12:18,925
Speaker 3:  a Snoop Dogg budget? And they're like, yes. In addition to that there's Flava

194
00:12:18,925 --> 00:12:22,365
Speaker 3:  Flav budget. Like they just like went for it. It's great.

195
00:12:24,285 --> 00:12:27,605
Speaker 3:  I would, you should click around them people, it's like actually like fairly

196
00:12:27,605 --> 00:12:31,565
Speaker 3:  entertaining just as a tech experience to be like, oh,

197
00:12:31,565 --> 00:12:35,365
Speaker 3:  they like tried this out. Like what if this was all happening? You can

198
00:12:35,595 --> 00:12:39,205
Speaker 3:  kind of see there's news this week that venue sports, which is the big mashup

199
00:12:39,205 --> 00:12:40,645
Speaker 3:  of Disney and Fox and everything

200
00:12:42,165 --> 00:12:45,645
Speaker 3:  ESPN, they're gonna price it like $43 a month, which is

201
00:12:45,815 --> 00:12:48,165
Speaker 3:  crazy. and it's like, what would you get that

202
00:12:48,165 --> 00:12:48,965
Speaker 2:  That's Peloton money.

203
00:12:49,515 --> 00:12:53,325
Speaker 3:  Yeah. Only you sit around. So it actually might be worth $50 a month,

204
00:12:54,985 --> 00:12:58,205
Speaker 3:  but it's kind of like, oh, this is the bar. Like if you want to be anywhere

205
00:12:58,205 --> 00:13:02,085
Speaker 3:  close to that much money, you've gotta deliver a a user experience

206
00:13:02,085 --> 00:13:05,565
Speaker 3:  that is at least this good. Because if it's just a, well, if it's just like

207
00:13:05,645 --> 00:13:09,565
Speaker 3:  a minimum viable product list of things, like no, no one's

208
00:13:09,565 --> 00:13:12,645
Speaker 3:  gonna pay the money. All right. So that's, that's the Olympics. It's going

209
00:13:12,645 --> 00:13:15,725
Speaker 3:  on. I'm curious how people are watching it. Send us a note, you know, there's,

210
00:13:15,725 --> 00:13:19,565
Speaker 3:  there's more time left in it. One thing I'll say is it doesn't seem like

211
00:13:19,565 --> 00:13:22,685
Speaker 3:  the action is on Twitter the way it is with other live sports. Yeah, it is

212
00:13:22,685 --> 00:13:26,125
Speaker 3:  on TikTok and that I'm, there's something happened in there that I think

213
00:13:26,125 --> 00:13:29,565
Speaker 3:  is just fascinating. All right. We should talk about IO S 18.

214
00:13:30,405 --> 00:13:34,005
Speaker 3:  Allison, you got to play, you played with iOS 18.1, which is the developer

215
00:13:34,005 --> 00:13:37,005
Speaker 3:  beta, not the public beta yet, but I think that is getting pretty blurry

216
00:13:37,345 --> 00:13:40,205
Speaker 3:  in that everyone is playing with it. I mean, no one was gonna stay away from

217
00:13:40,205 --> 00:13:41,485
Speaker 3:  Apple Intelligence. Well you played with it. What do you think?

218
00:13:42,075 --> 00:13:45,845
Speaker 4:  Yeah, it's, it's interesting. I think my kind of big takeaway

219
00:13:46,065 --> 00:13:50,045
Speaker 4:  is like, well you get, you get the like visuals of the new Siri,

220
00:13:50,065 --> 00:13:53,525
Speaker 4:  the like glowing border and You can type to talk to Siri.

221
00:13:54,665 --> 00:13:58,205
Speaker 4:  And there's a couple things that are new about Siri that like, it

222
00:13:58,205 --> 00:14:02,045
Speaker 4:  follows context better like between questions, which like

223
00:14:02,265 --> 00:14:05,005
Speaker 4:  Google Assistant has been doing for a little while now. But,

224
00:14:06,225 --> 00:14:09,965
Speaker 4:  so that's kinda like setting the stage and there's these other

225
00:14:09,965 --> 00:14:13,005
Speaker 4:  little things throughout the UI that are like,

226
00:14:13,755 --> 00:14:17,525
Speaker 4:  when and if it all gets put together like that is,

227
00:14:17,555 --> 00:14:21,485
Speaker 4:  that could be really cool. But right now it's just kind of like

228
00:14:22,195 --> 00:14:26,165
Speaker 4:  Siri lights up and then You can rewrite an email and

229
00:14:26,165 --> 00:14:30,045
Speaker 4:  you'll get email summaries in your inbox. and it was just especially

230
00:14:30,045 --> 00:14:33,725
Speaker 4:  funny like when you open the mail

231
00:14:33,825 --> 00:14:37,765
Speaker 4:  app instead of the, like each, you know, email has

232
00:14:37,765 --> 00:14:41,685
Speaker 4:  the first couple lines of the actual email copy. There's just a little summary

233
00:14:41,905 --> 00:14:45,845
Speaker 4:  now and that makes a lot of sense for like a long email. But

234
00:14:45,845 --> 00:14:49,765
Speaker 4:  most of my emails are garbage like promo stuff. So it

235
00:14:49,765 --> 00:14:52,685
Speaker 4:  just summarizes the promotional stuff and it's like

236
00:14:53,245 --> 00:14:56,005
Speaker 4:  lunchbox is ship free and I'm like,

237
00:14:56,845 --> 00:14:56,965
Speaker 3:  Hmm.

238
00:14:58,265 --> 00:15:00,005
Speaker 4:  Didn't need a summary of that, but thank you.

239
00:15:00,515 --> 00:15:04,325
Speaker 3:  That is very much like what's happening with gr and trending topics

240
00:15:04,385 --> 00:15:05,085
Speaker 3:  on Twitter now.

241
00:15:05,465 --> 00:15:06,365
Speaker 4:  Oh no. Where

242
00:15:06,365 --> 00:15:10,045
Speaker 3:  Like the AI just doesn't know that some things are jokes and so the, the

243
00:15:10,045 --> 00:15:13,765
Speaker 3:  trending topics are like when I turn black in

244
00:15:14,005 --> 00:15:16,565
Speaker 3:  relation to like people tweeting about Trump talking about Kamala Harris

245
00:15:16,565 --> 00:15:19,565
Speaker 3:  and it's like, no that's not, that's not right. That's not, that's not what

246
00:15:19,565 --> 00:15:20,685
Speaker 3:  people mean. That's just

247
00:15:20,685 --> 00:15:22,245
Speaker 4:  A joke. Yeah, you didn't get it.

248
00:15:22,955 --> 00:15:23,245
Speaker 3:  Yeah.

249
00:15:24,385 --> 00:15:28,245
Speaker 4:  AI is just so sincere about stuff that it just like, it's

250
00:15:28,245 --> 00:15:30,205
Speaker 4:  funny. We have to laugh about it

251
00:15:30,725 --> 00:15:34,285
Speaker 3:  I guess. So are those the only features that are out yet? It's the summarizing

252
00:15:34,285 --> 00:15:35,485
Speaker 3:  emails and rewriting,

253
00:15:36,365 --> 00:15:36,565
Speaker 4:  Summarizing.

254
00:15:36,595 --> 00:15:40,565
Speaker 3:  I've seen a lot of pictures of people intentionally writing very mean

255
00:15:40,565 --> 00:15:42,645
Speaker 3:  notes and trying to get the AI to make them nicer, which is fascinating.

256
00:15:42,705 --> 00:15:46,525
Speaker 4:  Oh, the one I did, you know, at the end of Willy

257
00:15:46,575 --> 00:15:50,365
Speaker 4:  Wonka when Gene Wilder does that like fizzy lifting drink thing,

258
00:15:50,365 --> 00:15:54,045
Speaker 4:  like you stole fizzy lifting drinks. Yeah. I put that into a note and

259
00:15:55,285 --> 00:15:59,205
Speaker 4:  rewrite it like friendly and it was like, hey, just just so

260
00:15:59,205 --> 00:16:01,645
Speaker 4:  you know, you can't be stealing fizzy lifting drinks.

261
00:16:02,905 --> 00:16:06,645
Speaker 4:  That's just as far as I got without very amusing to me

262
00:16:06,645 --> 00:16:07,165
Speaker 4:  personally.

263
00:16:09,195 --> 00:16:12,925
Speaker 4:  Yeah, there's, there's little bits and bobs here and there that are

264
00:16:13,805 --> 00:16:17,685
Speaker 4:  escaping me at the moment. It's in the emails. No photos in

265
00:16:17,685 --> 00:16:21,645
Speaker 4:  the photos app. You can use like natural language search. So You can

266
00:16:21,745 --> 00:16:25,405
Speaker 4:  be like, you know, you could search for like food before or

267
00:16:25,405 --> 00:16:29,085
Speaker 4:  someone that you've tagged in your photos, but

268
00:16:29,465 --> 00:16:33,365
Speaker 4:  You can be like this person wearing glasses or food we ate in

269
00:16:33,515 --> 00:16:37,325
Speaker 4:  Iceland and it's like pretty good and it comes up with that

270
00:16:37,325 --> 00:16:40,925
Speaker 4:  stuff. So just getting a step closer to like,

271
00:16:41,645 --> 00:16:45,325
Speaker 4:  I just search for my license plate 'cause I can never

272
00:16:45,725 --> 00:16:45,845
Speaker 4:  remember.

273
00:16:46,985 --> 00:16:50,765
Speaker 2:  So like I have this one picture of a chicken that I saw while walking down

274
00:16:50,765 --> 00:16:53,965
Speaker 2:  a street in Brooklyn. So would I just be able and I can never, like

275
00:16:54,545 --> 00:16:58,165
Speaker 2:  no one believes me when I told 'em I ran into a chicken on a sidewalk in

276
00:16:58,365 --> 00:16:58,845
Speaker 2:  Brooklyn. I

277
00:16:58,845 --> 00:16:59,285
Speaker 3:  Don't believe you now

278
00:17:00,645 --> 00:17:00,845
Speaker 2:  I only

279
00:17:01,045 --> 00:17:02,005
Speaker 4:  Have have to see it, only have

280
00:17:02,105 --> 00:17:05,645
Speaker 2:  One photo of it. So would I be able to go to like the photos app and be like,

281
00:17:05,645 --> 00:17:09,125
Speaker 2:  find me the photo of the chicken walking on a sidewalker for in Brooklyn?

282
00:17:09,395 --> 00:17:10,765
Speaker 2:  Yeah. With my foot in there.

283
00:17:11,525 --> 00:17:15,485
Speaker 4:  I think that's the most important use case of ai like

284
00:17:15,665 --> 00:17:16,005
Speaker 4:  at all.

285
00:17:17,785 --> 00:17:20,525
Speaker 3:  Or it will just generate that photo for you and you'll be able to

286
00:17:20,525 --> 00:17:24,205
Speaker 4:  Lie to us. Yeah, You can put a chicken on the sidewalk and then tell people

287
00:17:24,235 --> 00:17:26,165
Speaker 4:  that you saw it and be a liar.

288
00:17:26,865 --> 00:17:30,565
Speaker 3:  So when we saw WWC of iOS 18 was AI

289
00:17:30,565 --> 00:17:34,445
Speaker 3:  everywhere and every app in every corner, it's just gonna make the

290
00:17:34,445 --> 00:17:38,245
Speaker 3:  phone better. And then that was like a one

291
00:17:38,425 --> 00:17:42,325
Speaker 3:  set of ideas. And then the other idea was, and then Siri will become your

292
00:17:42,325 --> 00:17:46,045
Speaker 3:  all-knowing assistant. Is that the experience of 18.1

293
00:17:46,105 --> 00:17:46,965
Speaker 3:  in the beta right now?

294
00:17:47,665 --> 00:17:51,645
Speaker 4:  Not yet. It's like, yeah, it's just little Easter eggs kind of

295
00:17:51,645 --> 00:17:55,125
Speaker 4:  threw out and then the, the thing that is missing from Siri is like

296
00:17:56,145 --> 00:17:59,885
Speaker 4:  the, the big stuff where it's like it under the ai, the the

297
00:17:59,885 --> 00:18:03,485
Speaker 4:  intelligence and the AI I think. Yeah, no the like

298
00:18:03,535 --> 00:18:07,005
Speaker 4:  it'll understand what's on your screen and You can ask for it. You know,

299
00:18:07,125 --> 00:18:10,965
Speaker 4:  You can be like, email this photo to my mom and it'll just go

300
00:18:10,965 --> 00:18:14,845
Speaker 4:  do it for you. Like that is the stuff that's coming. And then the the

301
00:18:14,845 --> 00:18:18,805
Speaker 4:  third party app stuff, 'cause developers are gonna be able to hook into

302
00:18:18,805 --> 00:18:22,325
Speaker 4:  it and, and let you do series stuff in their apps

303
00:18:22,905 --> 00:18:26,365
Speaker 4:  and that's the, the AI dream on our phones.

304
00:18:26,745 --> 00:18:30,445
Speaker 4:  And that's still the like no, it's coming 'cause people

305
00:18:30,475 --> 00:18:31,845
Speaker 4:  have to develop it

306
00:18:32,465 --> 00:18:35,765
Speaker 3:  Is the Well they changed the user experience of of Siri, right? So now when

307
00:18:35,765 --> 00:18:39,605
Speaker 3:  you open it, it like flashes the thing You can, like You can type to it

308
00:18:39,605 --> 00:18:39,805
Speaker 3:  now.

309
00:18:40,675 --> 00:18:42,245
Speaker 4:  Yeah. You double the tap. You've always

310
00:18:42,245 --> 00:18:42,685
Speaker 2:  Been able to

311
00:18:43,955 --> 00:18:47,885
Speaker 4:  This, I learned this and I didn't know it. V what's the,

312
00:18:48,105 --> 00:18:49,685
Speaker 4:  how do you do this?

313
00:18:49,905 --> 00:18:53,685
Speaker 2:  Oh it's, it's, it's an accessibility feature where You can type to Siri.

314
00:18:53,685 --> 00:18:56,965
Speaker 2:  So You can do it even if you don't have the beta. I just think it looks a

315
00:18:56,965 --> 00:19:00,805
Speaker 2:  lot prettier in the beta because yeah, this is another example of them

316
00:19:00,805 --> 00:19:04,525
Speaker 2:  going like, oh, accessibility features. Actually they work for everybody.

317
00:19:04,585 --> 00:19:08,085
Speaker 2:  Yes. Why don't we just like shine them up and make them useful.

318
00:19:08,085 --> 00:19:10,005
Speaker 4:  Yeah. Right. Make it glow.

319
00:19:10,005 --> 00:19:12,845
Speaker 3:  It is, it is actually really interesting how many accessibility features

320
00:19:12,845 --> 00:19:16,325
Speaker 3:  have just made their way into the main Apple Pinch, pinch operating systems.

321
00:19:16,475 --> 00:19:20,365
Speaker 3:  Yeah. Andy Pinch there. There's other ones but tap to type to series

322
00:19:20,505 --> 00:19:24,485
Speaker 3:  the big one with 18.1 because it, it's

323
00:19:24,485 --> 00:19:28,005
Speaker 3:  the thing that takes Siri from being timers and music player

324
00:19:29,025 --> 00:19:32,925
Speaker 3:  to chat bot to assistant to intelligent agent

325
00:19:32,925 --> 00:19:36,725
Speaker 3:  that is taking action across all of your apps. And I, it seems like

326
00:19:36,725 --> 00:19:39,845
Speaker 3:  they've added the stuff but they haven't added any of the backend to make

327
00:19:40,085 --> 00:19:40,565
Speaker 3:  it go yet.

328
00:19:40,955 --> 00:19:44,925
Speaker 4:  Yeah, like I'm asking it questions and it's still just Googling stuff

329
00:19:44,945 --> 00:19:48,765
Speaker 4:  for me. I'm like oh. Oh okay. Well I could have typed this into a Google

330
00:19:48,765 --> 00:19:51,925
Speaker 4:  search bar instead of a Siri text box but,

331
00:19:52,345 --> 00:19:55,685
Speaker 3:  And it seems they've added AI call recording and transcription, which is

332
00:19:55,685 --> 00:19:56,565
Speaker 3:  the feature that I want the most.

333
00:19:57,235 --> 00:19:57,525
Speaker 4:  Yeah,

334
00:19:58,155 --> 00:20:01,765
Speaker 3:  Because if you're a reporter, not because I'm constantly spying on everyone.

335
00:20:02,345 --> 00:20:05,205
Speaker 3:  I'm Richard Nixon everyone. I'm constantly recording everyone. No, I mean

336
00:20:05,205 --> 00:20:09,085
Speaker 3:  we're reporters and like AI call recording and transcript are legitimately

337
00:20:09,085 --> 00:20:12,645
Speaker 3:  useful in our jobs and it's been, we've just been doing, either

338
00:20:12,705 --> 00:20:15,125
Speaker 3:  people are using their Android phones in our office, which lots of people

339
00:20:15,125 --> 00:20:18,565
Speaker 3:  do or there's a set of weird third party stuff. But being able to just use

340
00:20:18,565 --> 00:20:22,485
Speaker 3:  my iPhone, which is my main phone would be really great. So that's

341
00:20:22,485 --> 00:20:23,805
Speaker 3:  here now. Has it worked yet? Have you tried it out?

342
00:20:24,325 --> 00:20:28,045
Speaker 4:  I have not. 'cause I didn't put my sim card in that phone because of

343
00:20:28,305 --> 00:20:32,205
Speaker 4:  eim 'cause of nightmares. But I did try out,

344
00:20:32,425 --> 00:20:35,725
Speaker 4:  so like the voice memos app, we'll do

345
00:20:35,845 --> 00:20:38,925
Speaker 4:  transcriptions now and that's not an Apple Intelligence feature. Like the

346
00:20:39,355 --> 00:20:43,205
Speaker 4:  recording a call I think is Apple Intelligence could be wrong.

347
00:20:44,065 --> 00:20:47,765
Speaker 4:  But the transcription is good. Like I put, I am a

348
00:20:48,095 --> 00:20:52,005
Speaker 4:  Pixel recorder stand. I always have a Pixel phone at

349
00:20:52,045 --> 00:20:55,885
Speaker 4:  a press event, you know, like it is just in my bag

350
00:20:56,065 --> 00:20:59,725
Speaker 4:  and I put it next to the iPhone and it was really good.

351
00:21:00,025 --> 00:21:03,605
Speaker 4:  It was like up there with the Pixel. So I am

352
00:21:03,605 --> 00:21:05,125
Speaker 4:  personally excited about that.

353
00:21:05,755 --> 00:21:09,565
Speaker 3:  Yeah. Yeah I think that that's the one, it's always a grab bag

354
00:21:09,565 --> 00:21:13,525
Speaker 3:  of which feature is the one that hits and I just know that that feature

355
00:21:13,585 --> 00:21:16,605
Speaker 3:  for at least our little community of reporters is gonna hit the hardest.

356
00:21:17,165 --> 00:21:17,285
Speaker 3:  Oh

357
00:21:17,285 --> 00:21:18,405
Speaker 4:  They're gonna love it. See people

358
00:21:18,405 --> 00:21:21,885
Speaker 2:  Excited. I'm so excited. Yeah. Can't wait to cancel my Otter subscription.

359
00:21:22,665 --> 00:21:26,205
Speaker 3:  Oh no. Yeah, I mean like talk about startups getting wiped out. There's like

360
00:21:26,205 --> 00:21:28,245
Speaker 3:  two or three that are gonna go away as soon as it sense.

361
00:21:29,905 --> 00:21:33,405
Speaker 3:  It seems like the rest of the feature is not coming for a while. Right. Like

362
00:21:33,405 --> 00:21:37,365
Speaker 3:  Mark Erman at Bloomberg said some of these might not come until late

363
00:21:37,365 --> 00:21:38,485
Speaker 3:  25 or even 26.

364
00:21:39,115 --> 00:21:42,845
Speaker 4:  Yeah, I think the last thing, I'm so confused on the

365
00:21:43,005 --> 00:21:46,325
Speaker 4:  timeline, but it, the last thing I read I think was that like they would

366
00:21:46,325 --> 00:21:50,245
Speaker 4:  all come to developer betas by the end of the year, but then as far as

367
00:21:50,245 --> 00:21:53,005
Speaker 4:  like being in the public beta, that's 2025.

368
00:21:54,195 --> 00:21:57,645
Speaker 4:  Yeah. It's just a really long staggered kind of

369
00:21:57,875 --> 00:21:58,805
Speaker 4:  rollout it sounds like.

370
00:21:59,465 --> 00:22:03,325
Speaker 3:  And that kinda makes sense. The like the danger with the Siri that can do

371
00:22:03,325 --> 00:22:06,405
Speaker 3:  stuff on your behalf is quite, quite high. I think you have to make sure

372
00:22:06,425 --> 00:22:10,085
Speaker 3:  it works. We're also just not sure how

373
00:22:10,225 --> 00:22:13,365
Speaker 3:  any of the Apple private cloud

374
00:22:14,135 --> 00:22:17,925
Speaker 3:  stuff is going to work. Like right now, is it clear that all this stuff is

375
00:22:17,925 --> 00:22:21,765
Speaker 3:  happening on the phone or in the cloud or where or is it all we all, we know

376
00:22:21,765 --> 00:22:22,605
Speaker 3:  it's all happening on the phone

377
00:22:22,975 --> 00:22:26,965
Speaker 4:  There. So there's a like privacy report You can

378
00:22:27,035 --> 00:22:30,885
Speaker 4:  pull down now that will show you like ev and I think a lot of

379
00:22:30,885 --> 00:22:34,765
Speaker 4:  this is in the cloud. Like it can't, I wouldn't say that for

380
00:22:34,765 --> 00:22:38,405
Speaker 4:  sure, but You can, You can pull this privacy report that shows you

381
00:22:38,405 --> 00:22:42,245
Speaker 4:  everything over like a certain time period that Apple Intelligence has done

382
00:22:42,245 --> 00:22:45,685
Speaker 4:  on your phone. And I guess it's encrypted 'cause it's just like

383
00:22:45,685 --> 00:22:49,565
Speaker 4:  gobbledy like I downloaded it, I was like, well this

384
00:22:49,565 --> 00:22:51,845
Speaker 4:  is secure as hell. 'cause I don't know what any of this is.

385
00:22:53,835 --> 00:22:55,525
Speaker 4:  Yeah. Still kind of unclear.

386
00:22:56,875 --> 00:22:59,685
Speaker 3:  Yeah, I mean I am just like the fundamental architecture of this thing is

387
00:22:59,955 --> 00:23:03,445
Speaker 3:  very new and what happens on your phone versus in the cloud,

388
00:23:04,155 --> 00:23:07,805
Speaker 3:  there's some stuff that Apple's been very clear that Apple happens on your

389
00:23:07,805 --> 00:23:11,285
Speaker 3:  phone like emoji and some of the photo stuff happens on your phone. Some

390
00:23:11,285 --> 00:23:13,725
Speaker 3:  of the other stuff we just, I'm assuming a lot of the Siri stuff is gonna

391
00:23:13,785 --> 00:23:17,605
Speaker 3:  go to the cloud 'cause Siri goes to the cloud right now. So it,

392
00:23:17,625 --> 00:23:21,205
Speaker 3:  it just seems like this rollout's gonna be a lot slower and more

393
00:23:21,205 --> 00:23:24,365
Speaker 3:  deliberate than anyone anticipated based on wwc

394
00:23:25,065 --> 00:23:28,885
Speaker 3:  and it might roll all the way. I mean late 2025 is

395
00:23:28,885 --> 00:23:32,685
Speaker 3:  very close to when we would expect iOS 19 to be coming

396
00:23:32,685 --> 00:23:32,885
Speaker 3:  out.

397
00:23:34,555 --> 00:23:35,125
Speaker 4:  Yeah. So

398
00:23:35,545 --> 00:23:38,285
Speaker 3:  I'm, I'm sort of curious how that will all play, but for now it seems like

399
00:23:38,595 --> 00:23:41,765
Speaker 3:  they've sprinkled in some AI in the places where you would expect it.

400
00:23:42,475 --> 00:23:46,405
Speaker 4:  Yeah, it's all, it does seem pretty safe. It's like, okay, we've been able

401
00:23:46,425 --> 00:23:50,285
Speaker 4:  to, it is kind of the table stake stuff of like, write me an email

402
00:23:50,305 --> 00:23:54,085
Speaker 4:  that's friendly or professional or summarize this for me and

403
00:23:54,085 --> 00:23:57,605
Speaker 4:  like that's all kind of, they're kind of laying the groundwork with that

404
00:23:57,605 --> 00:23:57,845
Speaker 4:  stuff.

405
00:23:58,185 --> 00:24:02,085
Speaker 3:  And then we're gonna get 18.0 first and then 18.1 second. So

406
00:24:02,155 --> 00:24:03,645
Speaker 3:  this is pretty far away it seems

407
00:24:03,645 --> 00:24:06,225
Speaker 4:  Like. Yeah, it's always Alright,

408
00:24:06,975 --> 00:24:10,785
Speaker 3:  Well the, the AI overlords are coming whether we

409
00:24:10,785 --> 00:24:14,425
Speaker 3:  like it or not. All right, so that's iOS 18.1. That's it's interesting. That's

410
00:24:14,425 --> 00:24:18,265
Speaker 3:  out now and they're actually dual tracking it, right? There's iOS 18 in the

411
00:24:18,265 --> 00:24:21,945
Speaker 3:  public betas and 18.1 in the developer betas. And you get the

412
00:24:22,145 --> 00:24:26,105
Speaker 3:  feeling they wanted people to be talking about the AI stuff because the

413
00:24:26,105 --> 00:24:30,025
Speaker 3:  Pixel line event is coming very soon and we know exactly what Google's

414
00:24:30,025 --> 00:24:33,705
Speaker 3:  gonna talk about Pixel line event, but we gotta take a break. Let's do that.

415
00:24:33,705 --> 00:24:35,945
Speaker 3:  Come back and let's talk about the Pixel line event. We'll be right back.

416
00:25:14,505 --> 00:25:17,945
Speaker 1:  part is You can find meta ai right in the apps you already have

417
00:25:18,495 --> 00:25:21,985
Speaker 1:  Instagram, WhatsApp, Facebook, and Messenger.

418
00:26:36,735 --> 00:26:40,675
Speaker 4:  I'm, I'm good with this. We've gotten to the same place with the phone design,

419
00:26:41,015 --> 00:26:44,805
Speaker 4:  but Google's like, I think it's like Samsung and Google

420
00:26:44,825 --> 00:26:48,485
Speaker 4:  are like, how do we make this not an iPhone? And the answer is like a weird

421
00:26:48,545 --> 00:26:52,525
Speaker 4:  camera situation. Yes. and it kinda looks like Google's really

422
00:26:52,525 --> 00:26:56,405
Speaker 4:  leaning into the weird camera situation and I don't know how

423
00:26:56,445 --> 00:26:57,205
Speaker 4:  I still feel about it.

424
00:26:58,085 --> 00:27:00,605
Speaker 3:  I think these are the rivian headlights of phone cameras.

425
00:27:01,225 --> 00:27:02,285
Speaker 4:  Oh, that's it. Yeah.

426
00:27:02,345 --> 00:27:05,845
Speaker 3:  Oh, do you know what I'm saying? Yeah. Like You can love them. You can hate

427
00:27:05,965 --> 00:27:09,765
Speaker 3:  'em. They're not changing them. They're, it's, you know, and You can try

428
00:27:09,765 --> 00:27:11,445
Speaker 3:  to convince me that they look like snake eyes.

429
00:27:13,075 --> 00:27:13,925
Speaker 3:  They don't, you

430
00:27:13,925 --> 00:27:14,445
Speaker 4:  Know. No,

431
00:27:14,635 --> 00:27:17,205
Speaker 3:  Just like You can try to convince me this phone doesn't have a weird forehead.

432
00:27:17,785 --> 00:27:18,125
Speaker 3:  It does.

433
00:27:18,345 --> 00:27:21,005
Speaker 2:  It does. It does. Yeah.

434
00:27:21,095 --> 00:27:24,925
Speaker 3:  Super does. But it's true. The, the, the colors are more vibrant,

435
00:27:24,925 --> 00:27:27,925
Speaker 3:  which is getting away from what Apple is doing, or at least that's what the

436
00:27:27,935 --> 00:27:31,205
Speaker 3:  leaks indicate. So we're expecting a new Pixel nine, a Pixel nine Pro

437
00:27:31,925 --> 00:27:34,405
Speaker 3:  a Pixel nine pro fold, my God, these names

438
00:27:35,825 --> 00:27:39,405
Speaker 3:  and then watch, right? Oh, Pixel Buds Pro two

439
00:27:39,625 --> 00:27:43,125
Speaker 3:  and a Pixel watch three. So that's a pretty complete

440
00:27:43,235 --> 00:27:46,965
Speaker 3:  refresh of the Pixel line. It feels like the fold

441
00:27:46,965 --> 00:27:50,905
Speaker 3:  should be where the action is, but I, I just

442
00:27:50,905 --> 00:27:52,705
Speaker 3:  can't tell how much Google cares about these devices.

443
00:27:53,205 --> 00:27:54,745
Speaker 4:  That's the question, isn't it?

444
00:27:56,375 --> 00:28:00,105
Speaker 4:  It's so they moved the, like the fold was on the

445
00:28:00,105 --> 00:28:03,945
Speaker 4:  product cycle with like it was at IO

446
00:28:04,095 --> 00:28:07,745
Speaker 4:  last time around. Right? And then they've like scooched it, so it's more

447
00:28:07,775 --> 00:28:09,265
Speaker 4:  part of the like, main

448
00:28:10,895 --> 00:28:14,785
Speaker 4:  refresh cycle, which kind of makes sense is like, is this a flagship

449
00:28:14,785 --> 00:28:18,465
Speaker 4:  phone? It's like the most expensive phone you guys sell, but it's a cycle

450
00:28:18,605 --> 00:28:22,465
Speaker 4:  behind, you know, with the like camera hardware and whatnot.

451
00:28:22,645 --> 00:28:26,545
Speaker 4:  So I think it makes sense moving the fold

452
00:28:26,575 --> 00:28:27,905
Speaker 4:  into this position. But

453
00:28:29,535 --> 00:28:33,265
Speaker 4:  it's, it's a lot of phones. I don't know, do we need this many

454
00:28:33,265 --> 00:28:33,585
Speaker 4:  phones?

455
00:28:35,855 --> 00:28:39,465
Speaker 2:  Like how do you think it'll compare to, 'cause you know, Samsung you said

456
00:28:39,465 --> 00:28:43,105
Speaker 2:  like their foldables are just basically coasting at this point. So

457
00:28:43,325 --> 00:28:47,265
Speaker 2:  are we at a point where like the Pixel fold is something that is like

458
00:28:47,765 --> 00:28:50,665
Speaker 2:  yay competition? Or is it just

459
00:28:52,095 --> 00:28:52,865
Speaker 2:  it's here?

460
00:28:53,655 --> 00:28:56,905
Speaker 4:  Yeah, I'm still of the mind that the one plus open

461
00:28:57,445 --> 00:29:01,265
Speaker 4:  got it. Right. Like that is how a foldable like a book

462
00:29:01,265 --> 00:29:04,985
Speaker 4:  style foldable should be shaped. And then Samsung

463
00:29:05,205 --> 00:29:08,585
Speaker 4:  is just committed to this like remote control shaped thing

464
00:29:09,275 --> 00:29:12,625
Speaker 4:  every year. They're like, it's a few millimeter, like this screen is a few

465
00:29:12,865 --> 00:29:15,705
Speaker 4:  millimeters wider and it still feels like a remote control to use.

466
00:29:16,885 --> 00:29:20,705
Speaker 4:  So I'm curious how different this new Pixel fold

467
00:29:20,805 --> 00:29:24,305
Speaker 4:  is. 'cause there's a lot of rumors that it's like thinner and lighter.

468
00:29:24,865 --> 00:29:28,785
Speaker 4:  'cause boy was the previous one heavy. But

469
00:29:28,945 --> 00:29:32,905
Speaker 4:  I do like that that like wider aspect ratio for the

470
00:29:32,905 --> 00:29:36,825
Speaker 4:  cover screen. I don't think it needs to be like, quite as wide

471
00:29:36,965 --> 00:29:40,585
Speaker 4:  as the, the first generation Pixel fold. So

472
00:29:41,375 --> 00:29:41,885
Speaker 4:  we'll see.

473
00:29:42,355 --> 00:29:46,245
Speaker 3:  Yeah. And then the, the, the point of all this is going to

474
00:29:46,245 --> 00:29:49,965
Speaker 3:  be the software, right? The, this is pretty iterative

475
00:29:50,245 --> 00:29:53,845
Speaker 3:  hardware across the board. Some minor camera tweaks, a

476
00:29:53,935 --> 00:29:57,885
Speaker 3:  wacky camera bump. But if you're not paying close attention,

477
00:29:58,365 --> 00:30:01,325
Speaker 3:  I, it's, I think it's gonna be fairly hard to distinguish the hardware from

478
00:30:01,325 --> 00:30:04,965
Speaker 3:  the previous generation. It's the software that's the game. So

479
00:30:05,305 --> 00:30:09,285
Speaker 3:  You can see they're adding Gemini sort of to the first layer of

480
00:30:09,285 --> 00:30:13,205
Speaker 3:  the interface. We've got this new thing called Pixel screenshots, which is

481
00:30:13,205 --> 00:30:16,445
Speaker 3:  basically just Microsoft Recall, which everyone is scared about only manual.

482
00:30:17,105 --> 00:30:20,085
Speaker 3:  So instead of constantly taking screenshots, you have to manually take a

483
00:30:20,085 --> 00:30:24,005
Speaker 3:  screenshot. Very good, a very good differentiation. But I, I'm kind

484
00:30:24,005 --> 00:30:26,245
Speaker 3:  of into it, right? You take a screenshot and you're like, what is this? Or

485
00:30:26,245 --> 00:30:29,565
Speaker 3:  like save it for later there's a chicken walking down the street just like

486
00:30:29,565 --> 00:30:33,445
Speaker 3:  hold onto that for me. Yeah. And then of course there's circle

487
00:30:33,465 --> 00:30:35,925
Speaker 3:  the search, which Google's adding to basically everything at this point.

488
00:30:36,645 --> 00:30:39,645
Speaker 3:  I will, I will coily say that a friend of mine

489
00:30:40,555 --> 00:30:44,005
Speaker 3:  just put like circle the search me in the face the other day. You might imagine

490
00:30:44,105 --> 00:30:47,005
Speaker 3:  who that friend is who was showing me an Android feature

491
00:30:48,385 --> 00:30:51,885
Speaker 3:  and he dunked on me with circle the search, which was pretty funny. But like,

492
00:30:51,885 --> 00:30:55,365
Speaker 3:  this is them saying, okay, there's a new way to think about how this phone

493
00:30:55,365 --> 00:30:58,805
Speaker 3:  works, right? There's AI geminis like right here.

494
00:30:59,305 --> 00:31:02,165
Speaker 3:  And then You can just point at things or tell the phone to look at things

495
00:31:02,465 --> 00:31:05,245
Speaker 3:  and it will generate information for you, which is a very Google approach

496
00:31:05,245 --> 00:31:08,765
Speaker 3:  to it. The question is whether that's enough

497
00:31:09,345 --> 00:31:13,285
Speaker 3:  to make people switch, not even from the iPhone, from their Samsung

498
00:31:13,345 --> 00:31:17,245
Speaker 3:  phones, which obviously are dominant. And of course because it's Google

499
00:31:17,555 --> 00:31:20,885
Speaker 3:  whether they'll stay committed to any of those ideas for more than 20 minutes.

500
00:31:21,665 --> 00:31:23,365
Speaker 3:  And I, I just don't know,

501
00:31:23,675 --> 00:31:27,405
Speaker 4:  It's just a confusing time. I think even just circle to search.

502
00:31:27,475 --> 00:31:31,365
Speaker 4:  Like, and I use it on the, the Samsung phones I've been testing

503
00:31:31,465 --> 00:31:35,165
Speaker 4:  and the Google phones obviously. But like, there's

504
00:31:35,195 --> 00:31:39,125
Speaker 4:  just kind of a lot of different ways to get at circle to

505
00:31:39,125 --> 00:31:43,005
Speaker 4:  search. And you're like, so is this replacing these, like

506
00:31:43,005 --> 00:31:46,925
Speaker 4:  Google lens is still a thing. I can translate something, but like

507
00:31:46,925 --> 00:31:50,645
Speaker 4:  did I need to circle to search that they, they just have a lot of ideas.

508
00:31:50,715 --> 00:31:54,285
Speaker 4:  Yeah. It's like, do you open up Gemini and like take a screenshot of something

509
00:31:54,305 --> 00:31:58,245
Speaker 4:  or a picture of the insider your refrigerator or, I don't know,

510
00:31:58,245 --> 00:32:02,125
Speaker 4:  there's just a lot of like input methods where

511
00:32:02,125 --> 00:32:06,045
Speaker 4:  it just sort of could feel overwhelming. But it's a

512
00:32:06,045 --> 00:32:09,085
Speaker 4:  like, seems like there in that position

513
00:32:10,115 --> 00:32:13,965
Speaker 4:  that we're kind of talking about with smarter Siri where it's like, okay,

514
00:32:14,035 --> 00:32:17,525
Speaker 4:  like you've made the promises, this is gonna make our lives easier,

515
00:32:18,035 --> 00:32:21,895
Speaker 4:  blah blah blah. Right now it's just a bunch of like, You

516
00:32:21,895 --> 00:32:25,135
Speaker 4:  can switch the faces in these photos or you know,

517
00:32:25,715 --> 00:32:29,695
Speaker 4:  put a rain cloud in the sky behind you and like that

518
00:32:29,695 --> 00:32:32,935
Speaker 4:  was all nice and everything. But like, let's see. Something like, can they

519
00:32:32,935 --> 00:32:36,855
Speaker 4:  Yeah. The, the screenshot thing. Sure. That, that would be cool. I'm gonna

520
00:32:37,125 --> 00:32:41,045
Speaker 4:  take a screenshot of a recipe and I have literally

521
00:32:41,045 --> 00:32:44,925
Speaker 4:  have the same like spaghetti and meatball recipe from America's

522
00:32:44,925 --> 00:32:48,125
Speaker 4:  test kitchen bookmarked in my Google photos as a like

523
00:32:48,805 --> 00:32:52,725
Speaker 4:  favorite as if it's a photo of my child. Like I just look at it

524
00:32:52,795 --> 00:32:56,285
Speaker 4:  that often. I'm like too cheap to pay for

525
00:32:56,285 --> 00:32:59,925
Speaker 4:  America's test kitchen and can't commit to writing it down.

526
00:33:00,145 --> 00:33:03,925
Speaker 4:  So I don't know. I think there's a, there's a real use case for

527
00:33:04,755 --> 00:33:06,605
Speaker 4:  help me find this thing in a screenshot.

528
00:33:07,125 --> 00:33:10,085
Speaker 3:  I feel like, you know, I work with both of you very closely in the audience

529
00:33:10,085 --> 00:33:14,005
Speaker 3:  might not know what it's like to work with you, but I bookmarked, I

530
00:33:14,205 --> 00:33:17,445
Speaker 3:  favorited a, a screenshot of a recipe that I make all the time versus there's

531
00:33:17,445 --> 00:33:21,205
Speaker 3:  a photo of a chicken that I can never find is kind of exactly the difference.

532
00:33:21,795 --> 00:33:23,645
Speaker 2:  Yeah, it's just

533
00:33:24,045 --> 00:33:26,085
Speaker 4:  A little bit. Yeah, yeah, yeah.

534
00:33:27,065 --> 00:33:27,485
Speaker 2:  That's

535
00:33:28,665 --> 00:33:29,765
Speaker 3:  It. It tells a whole story.

536
00:33:31,905 --> 00:33:35,565
Speaker 4:  I'm just making my spaghetti and meatballs and bees like where's that chicken?

537
00:33:36,485 --> 00:33:40,045
Speaker 2:  I think about the Brooklyn chicken all the time. It's been so long.

538
00:33:40,425 --> 00:33:44,165
Speaker 2:  I'm not sure this chicken is alive anymore. I may have the only record

539
00:33:44,265 --> 00:33:48,085
Speaker 2:  of this sidewalk walking chicken. Like it's important to me that this chicken

540
00:33:48,085 --> 00:33:49,565
Speaker 2:  lives in my memory.

541
00:33:50,625 --> 00:33:54,605
Speaker 3:  Please send us a note and describe how your photo storage

542
00:33:54,925 --> 00:33:58,565
Speaker 3:  approaches describe your personality. 'cause I think there's, there's a lot

543
00:33:58,565 --> 00:34:00,165
Speaker 3:  in there. It's a rich zone to unpack

544
00:34:00,465 --> 00:34:02,045
Speaker 4:  On a scale of like one to chicken

545
00:34:04,265 --> 00:34:07,365
Speaker 3:  On a scale of, I'm just scrolling for a photo of a chicken endlessly. I swear

546
00:34:07,365 --> 00:34:11,005
Speaker 3:  it's here. Speaking of photos, Allison, you've been

547
00:34:11,285 --> 00:34:14,605
Speaker 3:  covering what can only be described as the, what is a photo apocalypse

548
00:34:15,405 --> 00:34:19,205
Speaker 3:  somewhat closely. You just wrote a piece about the Samsung like sketch

549
00:34:19,325 --> 00:34:23,205
Speaker 3:  A B and they'll put a b in the, the photo for you. It

550
00:34:23,205 --> 00:34:26,805
Speaker 3:  seems like the Pixel nine is just gonna continue moving

551
00:34:26,805 --> 00:34:30,685
Speaker 3:  inextricably down this path with this feature called Add Me that

552
00:34:30,755 --> 00:34:33,125
Speaker 3:  will just add you to photos that you're not in.

553
00:34:34,145 --> 00:34:36,645
Speaker 4:  Yep. That's what it sounds like.

554
00:34:37,845 --> 00:34:40,325
Speaker 3:  I mean, it's only from a leak. It's it's only from a leak.

555
00:34:41,625 --> 00:34:45,545
Speaker 3:  It was in a YouTube video that got removed. It was like an ad and

556
00:34:45,545 --> 00:34:49,455
Speaker 3:  it got, was leaked to YouTube and it got removed. But it just feels

557
00:34:49,455 --> 00:34:53,375
Speaker 3:  like Google has to start communicating about

558
00:34:53,475 --> 00:34:57,455
Speaker 3:  how it will keep our information environment somewhat pure

559
00:34:58,275 --> 00:35:01,895
Speaker 3:  as it adds these features that just

560
00:35:01,895 --> 00:35:03,015
Speaker 3:  polluted more than ever.

561
00:35:03,405 --> 00:35:07,215
Speaker 4:  Yeah, they were really like tiptoeing up to the line I feel like last

562
00:35:07,215 --> 00:35:10,775
Speaker 4:  year. and it was like, well this is just something that people have been

563
00:35:10,775 --> 00:35:14,375
Speaker 4:  doing in Photoshop for a long time and now more people have the tools.

564
00:35:14,955 --> 00:35:18,895
Speaker 4:  And then I feel like Samsung kicked the door down with sketch to image.

565
00:35:18,915 --> 00:35:22,815
Speaker 4:  It was like, put a chicken in your photo, put a blimp, like

566
00:35:23,095 --> 00:35:26,855
Speaker 4:  whatever you wanna do, You can have it. And then, yeah, I feel like the,

567
00:35:27,795 --> 00:35:31,735
Speaker 4:  the flood gates are open and I don't know how like, how

568
00:35:32,785 --> 00:35:36,655
Speaker 4:  aggressive Google will be, like the Add Me to the photo

569
00:35:36,705 --> 00:35:40,285
Speaker 4:  thing. It seems like there could be guardrails on it

570
00:35:40,745 --> 00:35:43,925
Speaker 4:  or it could just be like pure chaos and

571
00:35:44,635 --> 00:35:46,725
Speaker 4:  it's an election year and what could go wrong.

572
00:35:47,745 --> 00:35:51,405
Speaker 2:  So, so many things could go wrong. But I also like, it just reminds me of

573
00:35:51,405 --> 00:35:55,205
Speaker 2:  this one family portrait I have. So like my family's half split in

574
00:35:55,325 --> 00:35:59,205
Speaker 2:  the US versus Korea. And so we all went to Korea to get this family portrait

575
00:35:59,465 --> 00:36:03,165
Speaker 2:  except for one cousin who couldn't go because he was a dual citizen

576
00:36:03,265 --> 00:36:06,885
Speaker 2:  and his military service time was coming up. 'cause all Korean men have to

577
00:36:06,885 --> 00:36:09,725
Speaker 2:  do military service before they're 30 and he's just like, I'm just not gonna

578
00:36:09,725 --> 00:36:13,165
Speaker 2:  do it. So he had to get photoshopped into the family

579
00:36:13,415 --> 00:36:17,325
Speaker 2:  photo. Oh no. And. if you look at it from a distance, you

580
00:36:17,325 --> 00:36:20,485
Speaker 2:  can't tell that he's been photoshopped in. But if you look up close, you're

581
00:36:20,485 --> 00:36:24,325
Speaker 2:  like, well, lighting on his hair is wrong. Yeah. It's coming from a different

582
00:36:24,335 --> 00:36:28,085
Speaker 2:  angle. You can really, like, You can just, he's just textually

583
00:36:28,385 --> 00:36:31,205
Speaker 2:  not the same as everybody else. Yeah. So.

584
00:36:31,985 --> 00:36:32,365
Speaker 4:  Oh no.

585
00:36:32,625 --> 00:36:35,845
Speaker 2:  You can tell. You can tell. And it's just like this huge family portrait

586
00:36:35,905 --> 00:36:39,805
Speaker 2:  and Steve who's like been photoshopped in, so like,

587
00:36:40,145 --> 00:36:44,005
Speaker 2:  did it work? I don't know. Not really. 'cause we can all tell he is

588
00:36:44,005 --> 00:36:47,245
Speaker 2:  not there. So it's sort of like, I don't know. I feel like my auntie stay

589
00:36:47,275 --> 00:36:47,685
Speaker 2:  safe out

590
00:36:47,685 --> 00:36:48,205
Speaker 3:  There. Kids,

591
00:36:48,865 --> 00:36:52,685
Speaker 2:  My auntie would love to have him in this sort of situation,

592
00:36:52,825 --> 00:36:56,805
Speaker 2:  but I also, you know, having aunties with the power to just

593
00:36:57,025 --> 00:36:59,125
Speaker 2:  add you into anything that seems also

594
00:36:59,485 --> 00:37:02,725
Speaker 3:  Dangerous. Well, so it's not, it's not like an AI Add Me feature, at least

595
00:37:02,725 --> 00:37:06,525
Speaker 3:  from the ad that we see. It's like a compositing feature,

596
00:37:06,595 --> 00:37:10,325
Speaker 3:  like a very manual compositing feature where you take a photo

597
00:37:10,395 --> 00:37:14,125
Speaker 3:  with a, and there's, you leave space for literally yourself in the photo

598
00:37:14,945 --> 00:37:18,805
Speaker 3:  and then someone else holds the camera and it, it shows

599
00:37:18,825 --> 00:37:22,685
Speaker 3:  you the old photo and you like go stand where you would

600
00:37:22,685 --> 00:37:25,805
Speaker 3:  have been. And then you take a photo and it stitches all the photos together.

601
00:37:26,735 --> 00:37:30,275
Speaker 3:  So it's, it's very manual and its way, I'm sure there's AI in the

602
00:37:30,275 --> 00:37:33,755
Speaker 3:  background to handle the stitching and alignment and

603
00:37:34,435 --> 00:37:36,715
Speaker 3:  probably the textures and colors and and lighting give

604
00:37:36,715 --> 00:37:37,995
Speaker 4:  You too many fingers. Yeah.

605
00:37:38,655 --> 00:37:41,835
Speaker 3:  But the idea that it's gonna, it's not like deep faking you to put you in

606
00:37:41,835 --> 00:37:45,275
Speaker 3:  the photo. It's taking another photo of you in the, in the same space

607
00:37:45,975 --> 00:37:49,275
Speaker 3:  and then like making a composite. This is, which is still weird.

608
00:37:49,745 --> 00:37:53,435
Speaker 4:  Yeah. It's one of those features that like as a parent now,

609
00:37:53,575 --> 00:37:57,315
Speaker 4:  I'm like, oh, I completely get that like best

610
00:37:57,345 --> 00:38:01,235
Speaker 4:  take where it's like you try and take a picture where there's more than

611
00:38:01,375 --> 00:38:05,275
Speaker 4:  two toddlers in it, like forget it, then nobody's gonna

612
00:38:05,785 --> 00:38:09,755
Speaker 4:  have a good time. But then it's like, oh, you have all this data and You

613
00:38:09,755 --> 00:38:13,595
Speaker 4:  can just kinda like ai it together into one photo That looks right

614
00:38:13,935 --> 00:38:17,875
Speaker 4:  though. Like you get to be in the photo even though you took the

615
00:38:17,875 --> 00:38:21,555
Speaker 4:  photo thing. I'm like, yeah, I am. I'm following along with that.

616
00:38:21,795 --> 00:38:23,595
Speaker 4:  I get that. So

617
00:38:24,105 --> 00:38:27,595
Speaker 2:  It's basically, so you never have to ask someone, oh hey, do you mind taking

618
00:38:27,715 --> 00:38:29,715
Speaker 2:  a photo of us because it's just letting

619
00:38:29,715 --> 00:38:33,275
Speaker 3:  Us No, you have to ask them that. But in a much more

620
00:38:33,275 --> 00:38:35,035
Speaker 3:  complicated and ethically challenged

621
00:38:35,045 --> 00:38:37,205
Speaker 2:  Way. No, I mean like it's in your group. It's in your group. So

622
00:38:37,205 --> 00:38:39,525
Speaker 3:  Like, it would be in your group. So like, yeah, like you, you two would take

623
00:38:39,525 --> 00:38:42,845
Speaker 3:  a photo and then I would, I would take the photo of you two

624
00:38:43,425 --> 00:38:47,405
Speaker 3:  and then I would hand one of you the phone and I would go stand next to where

625
00:38:47,405 --> 00:38:51,245
Speaker 3:  you had been standing and you would take another photo and the Pixel

626
00:38:51,255 --> 00:38:54,325
Speaker 3:  would composite those photos together. So it looked like all three of us

627
00:38:54,325 --> 00:38:54,885
Speaker 3:  are standing there.

628
00:38:54,965 --> 00:38:58,365
Speaker 2:  I hate, I hate talking to strangers and asking them to take a group photo

629
00:38:58,425 --> 00:39:02,205
Speaker 2:  for me and my group. So this is, this is really gonna help me not grow

630
00:39:02,305 --> 00:39:05,165
Speaker 2:  as a person and not talk to strangers more is basically,

631
00:39:05,385 --> 00:39:08,285
Speaker 3:  Oh see, I love being asked that question because then everyone's like, oh,

632
00:39:08,285 --> 00:39:09,925
Speaker 3:  you know what you're doing. Which is great.

633
00:39:10,085 --> 00:39:13,885
Speaker 4:  I know. Oh, people don't know what they're getting into. I'm like,

634
00:39:14,065 --> 00:39:17,525
Speaker 4:  so you've got the Pixel phone, tell me about your

635
00:39:17,595 --> 00:39:21,165
Speaker 4:  choices here. They're like, who's this lady?

636
00:39:22,195 --> 00:39:22,485
Speaker 4:  Yeah.

637
00:39:23,105 --> 00:39:25,685
Speaker 3:  The second someone hands you a phone and you turn it to landscape, you're

638
00:39:25,685 --> 00:39:28,965
Speaker 3:  like, oh my God, we gotta play here. Ready? What

639
00:39:28,965 --> 00:39:30,925
Speaker 4:  Whatcha doing? Yeah,

640
00:39:32,705 --> 00:39:35,765
Speaker 3:  But I even that's weird. Like even the thing I'm describing where it's pretty

641
00:39:35,765 --> 00:39:38,765
Speaker 3:  manual, right? It's not just AI deepp picking you, but you have to, you're

642
00:39:38,765 --> 00:39:39,845
Speaker 3:  making a composite photo.

643
00:39:41,355 --> 00:39:45,325
Speaker 3:  What is a photo? Like it's not a moment in time that that moment quite

644
00:39:45,325 --> 00:39:48,525
Speaker 3:  literally never happened and we're all gonna, we're gonna pass it off as

645
00:39:48,845 --> 00:39:52,165
Speaker 3:  a photo. And I've been thinking about this parent's thing because Allison,

646
00:39:52,165 --> 00:39:55,805
Speaker 3:  every time you write about it, you, you put it in there. Like the experience

647
00:39:55,805 --> 00:39:59,285
Speaker 3:  of being a parent is just trying to is is like,

648
00:39:59,685 --> 00:40:02,565
Speaker 3:  I would like a statistical average of this moment in time that makes me feel

649
00:40:02,565 --> 00:40:05,565
Speaker 3:  good. Right? Yeah. Is like the goal of of like kid photography.

650
00:40:06,155 --> 00:40:08,045
Speaker 4:  Yeah. The reality is too scary.

651
00:40:08,835 --> 00:40:12,325
Speaker 3:  Yeah. Like if I could just round off the edges and be like, this felt great.

652
00:40:12,555 --> 00:40:14,805
Speaker 3:  Yeah. Like at several moments throughout this day,

653
00:40:15,175 --> 00:40:15,525
Speaker 4:  Right.

654
00:40:15,715 --> 00:40:19,565
Speaker 3:  Like that would be fine. I don't actually need a Pixel perfect

655
00:40:19,695 --> 00:40:22,885
Speaker 3:  recreation of this moment in time. Yeah. And it's, it's kind of weird. Like

656
00:40:22,885 --> 00:40:26,725
Speaker 3:  that is sort of the goal. And I, I understand in particular my parents want

657
00:40:26,725 --> 00:40:30,525
Speaker 3:  that, but it's right next to, are you showing

658
00:40:30,845 --> 00:40:33,845
Speaker 3:  somebody an image of a thing that actually happened or are you just creating

659
00:40:33,865 --> 00:40:37,605
Speaker 3:  an illustration? And is that illustration like in the age of

660
00:40:37,605 --> 00:40:41,485
Speaker 3:  influencers Instagram, is that in, is that illustration going to lead to

661
00:40:41,485 --> 00:40:45,365
Speaker 3:  all kinds of weird outcomes that what people's expectations of their own

662
00:40:45,365 --> 00:40:47,805
Speaker 3:  experiences should be like? 'cause they don't know they're looking at something

663
00:40:47,805 --> 00:40:51,085
Speaker 3:  fake in the entire ecosystem.

664
00:40:52,025 --> 00:40:55,245
Speaker 3:  We, we talked about this show in the context of like the, the tr the images

665
00:40:55,305 --> 00:40:58,685
Speaker 3:  of the Trump assassination attempt, the entire ecosystem of

666
00:40:59,215 --> 00:41:02,645
Speaker 3:  validating these images, saying what's real, what's not, which ones are ai,

667
00:41:02,645 --> 00:41:06,085
Speaker 3:  which ones aren't, is just not ready. Like it it

668
00:41:06,475 --> 00:41:10,405
Speaker 3:  that we've talked about it a lot. There are initiatives and alliances and

669
00:41:10,635 --> 00:41:14,485
Speaker 3:  there's labels on meta platforms that are being applied seemingly,

670
00:41:14,925 --> 00:41:18,165
Speaker 3:  randomly. But none of it works. Like no one trusts any of it.

671
00:41:18,745 --> 00:41:22,645
Speaker 3:  And I just like, we are just barreling towards this future of completely

672
00:41:22,965 --> 00:41:26,725
Speaker 3:  synthetic images coming off these phones and I just don't, it

673
00:41:26,725 --> 00:41:28,365
Speaker 3:  doesn't seem like any of the companies give a shit.

674
00:41:29,035 --> 00:41:32,885
Speaker 4:  Yeah. I feel like there's this just shared understanding

675
00:41:32,985 --> 00:41:36,845
Speaker 4:  we have, especially with a phone camera. It's like you just pointed at

676
00:41:36,885 --> 00:41:39,885
Speaker 4:  a thing, you were there, you took a picture, it happened, you put it on,

677
00:41:39,905 --> 00:41:43,285
Speaker 4:  you see it on Instagram. It's like we just have this shared understanding

678
00:41:43,305 --> 00:41:47,165
Speaker 4:  of like, yes, that was a thing that happened. Exactly. You know, like

679
00:41:47,315 --> 00:41:51,085
Speaker 4:  more or less as we saw it, there's, we, we all understand

680
00:41:51,085 --> 00:41:54,805
Speaker 4:  that people pose for photos with big hats on

681
00:41:55,025 --> 00:41:58,965
Speaker 4:  in front of gorgeous landscapes or whatever. But yeah, we're, we're

682
00:41:58,965 --> 00:42:02,675
Speaker 4:  just like coming straight at that moment of like, okay,

683
00:42:02,675 --> 00:42:05,795
Speaker 4:  we're gonna have to start asking some different questions when you're scrolling

684
00:42:05,795 --> 00:42:09,395
Speaker 4:  through Instagram or just like shifting your mindset. And it's not even

685
00:42:09,715 --> 00:42:12,315
Speaker 4:  like when I did the, the sketch to image,

686
00:42:13,635 --> 00:42:17,475
Speaker 4:  I did, you know, mess with a bunch of photos, added cats and pirate

687
00:42:17,525 --> 00:42:21,395
Speaker 4:  chips and like nonsense to them. And I posted a bunch to my

688
00:42:21,395 --> 00:42:24,475
Speaker 4:  Instagram like grid and a post and I went through the

689
00:42:25,275 --> 00:42:28,875
Speaker 4:  requirements they have for like, when you need to add an

690
00:42:29,095 --> 00:42:32,995
Speaker 4:  AI tag and it didn't even meet them. It was

691
00:42:32,995 --> 00:42:36,915
Speaker 4:  like you You can, you know, it was like something about it

692
00:42:36,915 --> 00:42:40,035
Speaker 4:  was my like my own personal photo and I wasn't

693
00:42:41,175 --> 00:42:44,835
Speaker 4:  the way it had manipulated it. It was like, and it's kind of up to you, you

694
00:42:44,835 --> 00:42:48,595
Speaker 4:  know, put a tag on there or don't whatever. Like, okay,

695
00:42:49,575 --> 00:42:50,195
Speaker 4:  That's fine.

696
00:42:51,745 --> 00:42:52,395
Speaker 4:  It's all fine.

697
00:42:52,705 --> 00:42:56,635
Speaker 3:  Well see, the other thing I've noticed a lot of is even just

698
00:42:56,635 --> 00:43:00,155
Speaker 3:  the basic AI features in phones now, they're

699
00:43:00,155 --> 00:43:03,995
Speaker 3:  overcorrecting and over sharpening. And this is beyond like, it looks bad.

700
00:43:05,105 --> 00:43:08,835
Speaker 3:  It's like if you zoom in too far on a lot of modern smartphone

701
00:43:08,835 --> 00:43:12,515
Speaker 3:  photos now you will see that the text is all

702
00:43:12,735 --> 00:43:16,555
Speaker 3:  ai wonky and like signs in the background. 'cause AI has tried to

703
00:43:16,555 --> 00:43:20,075
Speaker 3:  de-noise and sharpen and you've gotten some weird stuff, like really small

704
00:43:20,075 --> 00:43:23,635
Speaker 3:  faces far away. And a lot of smartphone photos now look incredibly

705
00:43:23,635 --> 00:43:27,435
Speaker 3:  distorted and demonic. 'cause there's like three pixels and

706
00:43:27,435 --> 00:43:30,115
Speaker 3:  it's like, well what if I put a face here and it's like, I don't really know

707
00:43:30,115 --> 00:43:33,995
Speaker 3:  what a face looks like yet. So here's a face, a vaguely face shaped DNO

708
00:43:34,085 --> 00:43:37,915
Speaker 3:  thing. I think Samsung phones are, are more egregious with this than

709
00:43:37,915 --> 00:43:41,635
Speaker 3:  iPhones, but they're all doing it to varying degrees and that is

710
00:43:41,635 --> 00:43:45,395
Speaker 3:  already confusing people. Yeah. Like they're like, these photos are AI

711
00:43:45,395 --> 00:43:48,955
Speaker 3:  generated and it's like, you're not wrong, but you're also not right.

712
00:43:49,425 --> 00:43:53,115
Speaker 3:  Like that moment in time did happen and then,

713
00:43:54,815 --> 00:43:58,755
Speaker 3:  you know, a smartphone interpreted this demon face instead

714
00:43:58,755 --> 00:44:00,635
Speaker 3:  of just not having information there.

715
00:44:00,985 --> 00:44:04,555
Speaker 4:  It's weird when it kind of come like comes to the surface where,

716
00:44:04,865 --> 00:44:08,555
Speaker 4:  what was the, the photo of the lady trying on a wedding dress

717
00:44:08,965 --> 00:44:12,595
Speaker 4:  where Yeah, yeah. In the mirror. She's like making two different poses. Oh

718
00:44:12,595 --> 00:44:13,075
Speaker 4:  yeah. And like

719
00:44:13,145 --> 00:44:13,755
Speaker 3:  Everybody

720
00:44:13,755 --> 00:44:17,675
Speaker 4:  Lost their minds and it turned out to be like it was accidentally in panorama

721
00:44:17,705 --> 00:44:21,355
Speaker 4:  mode or something like that. But just even that is like,

722
00:44:22,055 --> 00:44:25,635
Speaker 4:  you know, this stuff has been going on behind the scenes and just sort of

723
00:44:25,635 --> 00:44:29,595
Speaker 4:  like doing everything it can with that data to make a photo that it think

724
00:44:29,815 --> 00:44:33,045
Speaker 4:  it thinks you will like, and then when it kind of goes sideways

725
00:44:33,825 --> 00:44:37,685
Speaker 4:  and is obvious, then it's a real weird moment of like, no.

726
00:44:38,035 --> 00:44:41,725
Speaker 4:  Yeah, actually your phone's been doing this. Like, you just,

727
00:44:41,865 --> 00:44:45,605
Speaker 4:  it just didn't look like a demon until right this moment.

728
00:44:46,355 --> 00:44:49,765
Speaker 3:  Yeah. I I I think there's gonna be some moment

729
00:44:50,615 --> 00:44:50,965
Speaker 3:  where

730
00:44:53,345 --> 00:44:57,205
Speaker 3:  the industry, probably not the industry or governments, the eu,

731
00:44:57,205 --> 00:45:00,405
Speaker 3:  they, they tend, they, they're always doing stuff. They're gonna say,

732
00:45:01,145 --> 00:45:04,765
Speaker 3:  if you enable these features, you have to like hard watermark these photos

733
00:45:05,465 --> 00:45:09,445
Speaker 3:  as being partially generated by ai. 'cause if we don't get all the

734
00:45:09,445 --> 00:45:13,405
Speaker 3:  way there, then we're, we're gonna end up in this

735
00:45:13,675 --> 00:45:16,845
Speaker 3:  extremely weird moment where only

736
00:45:16,845 --> 00:45:20,555
Speaker 3:  professionals with like cryptographic

737
00:45:20,955 --> 00:45:24,915
Speaker 3:  signatures in their DSLRs uploading directly to Getty are the

738
00:45:24,915 --> 00:45:28,715
Speaker 3:  source of any truth. And I don't think that's, that doesn't seem

739
00:45:28,715 --> 00:45:32,155
Speaker 3:  sustainable. Right? Like only if you buy this one like a camera

740
00:45:32,615 --> 00:45:36,355
Speaker 3:  and you have a contract with Reuters, can your photos be trusted

741
00:45:36,785 --> 00:45:37,075
Speaker 3:  like

742
00:45:37,375 --> 00:45:38,675
Speaker 4:  $10,000 camera?

743
00:45:39,025 --> 00:45:42,275
Speaker 3:  Yeah. I, it just doesn't seem, that doesn't seem like what anybody wants.

744
00:45:42,355 --> 00:45:46,155
Speaker 3:  I I'm, I'm hopeful that like consumers correct this

745
00:45:46,285 --> 00:45:50,235
Speaker 3:  first, but I suspect that some government

746
00:45:50,235 --> 00:45:53,315
Speaker 3:  somewhere is gonna have to make a rule and people are gonna have to comply

747
00:45:53,315 --> 00:45:57,255
Speaker 3:  because we're just headed every time. And I'm, I know the Google people

748
00:45:57,255 --> 00:45:59,695
Speaker 3:  are gonna yell at me. 'cause every time I'm like, the what is a photo apocalypse

749
00:45:59,695 --> 00:46:01,935
Speaker 3:  is here because of some Google feature. They're like, no, this is just what

750
00:46:01,935 --> 00:46:04,375
Speaker 3:  people want. And I'm like, but people also want sugar.

751
00:46:05,295 --> 00:46:07,655
Speaker 4:  Like we want a

752
00:46:07,655 --> 00:46:08,535
Speaker 3:  Lot of things. But I love cigarettes.

753
00:46:09,525 --> 00:46:09,815
Speaker 4:  It's

754
00:46:10,205 --> 00:46:11,775
Speaker 3:  Like, what do you want me to do?

755
00:46:13,475 --> 00:46:16,695
Speaker 2:  It always does feel like they have the best case scenario in mind and that

756
00:46:16,695 --> 00:46:20,655
Speaker 2:  humans are not goblins who do horrible things to

757
00:46:20,655 --> 00:46:24,175
Speaker 2:  each other. Sometimes they're like, oh no, everyone will use it in the way

758
00:46:24,175 --> 00:46:27,575
Speaker 2:  that we intend. Which is the nice way. Yeah. And it's just like,

759
00:46:29,215 --> 00:46:29,435
Speaker 2:  no.

760
00:46:29,945 --> 00:46:30,355
Speaker 3:  Yeah. Look

761
00:46:30,355 --> 00:46:33,555
Speaker 2:  Around. You only need one one jerk. You only need one jerk to ruin everything.

762
00:46:35,105 --> 00:46:38,675
Speaker 3:  Look around. All right. Other reviews. V you reviewed the Galax watch Ultra

763
00:46:38,675 --> 00:46:39,395
Speaker 3:  this week. What's

764
00:46:39,855 --> 00:46:41,035
Speaker 2:  Yes, yes. What

765
00:46:41,035 --> 00:46:43,875
Speaker 3:  Kind of, what kind of Apple watch? Is it a good one or a bad watch? It's,

766
00:46:45,145 --> 00:46:48,795
Speaker 2:  It's an Apple Watch Ultra that Sure, that sure is what it is. But like, I,

767
00:46:48,795 --> 00:46:52,755
Speaker 2:  well actually what it really is is that like if you took the Apple Watch

768
00:46:52,925 --> 00:46:56,675
Speaker 2:  Ultra and drew it from memory, that's the Samsung Galaxy Watch Ultra

769
00:46:56,775 --> 00:47:00,715
Speaker 2:  or if you like, took the Apple font, it it's, it's

770
00:47:00,715 --> 00:47:04,635
Speaker 2:  like the Apple Watch Ultra too, but like in an Android font is

771
00:47:04,635 --> 00:47:08,595
Speaker 2:  just basically what you're doing with this. It's, there's just, I was writing

772
00:47:08,615 --> 00:47:12,115
Speaker 2:  the review and then I was like, you know what, it's actually just expedient

773
00:47:12,135 --> 00:47:15,435
Speaker 2:  if I tell you what's different and bullet point, everything that is the same

774
00:47:15,435 --> 00:47:19,155
Speaker 2:  because there's too many things that are the same. And then I started doing

775
00:47:19,155 --> 00:47:21,835
Speaker 2:  the major things that are the same and then I was like, oh, but then there's

776
00:47:21,835 --> 00:47:25,195
Speaker 2:  this and then there's this, and then there's like all these tiny little features

777
00:47:25,195 --> 00:47:29,035
Speaker 2:  that only a psycho like me would know and remember are on

778
00:47:29,085 --> 00:47:32,765
Speaker 2:  each of the watches are there like, oh, You can run against your last race

779
00:47:32,765 --> 00:47:35,925
Speaker 2:  performance if you're a runner. Yeah, apple did that and watch it West Nine

780
00:47:35,925 --> 00:47:38,725
Speaker 2:  and now he's here on the Samsung and like, why is it here on the Samsung?

781
00:47:38,725 --> 00:47:42,685
Speaker 2:  Because, I don't know, it was on Apple. They have an orange shortcut

782
00:47:42,685 --> 00:47:46,485
Speaker 2:  button button that is a quick button on Samsung and the action button

783
00:47:47,105 --> 00:47:50,965
Speaker 2:  on the Apple watch. They, it's the same thing just in a

784
00:47:50,965 --> 00:47:54,365
Speaker 2:  different font. And that's, that's like you could sum up the watch as that.

785
00:47:54,365 --> 00:47:58,165
Speaker 2:  And also it's, it's a squirkle and I don't like it. I don't,

786
00:47:58,285 --> 00:48:02,085
Speaker 2:  I I was had mixed feelings about the Squirkle going in. And then after two

787
00:48:02,085 --> 00:48:05,685
Speaker 2:  weeks of wear, I don't like the squirkle, it is just too

788
00:48:05,705 --> 00:48:09,325
Speaker 2:  chunky on my wrist. I can fit three chopsticks into the gap

789
00:48:09,325 --> 00:48:13,205
Speaker 2:  between the, the the watch and the and

790
00:48:13,205 --> 00:48:16,895
Speaker 2:  my wrist. And it's just a chunker. It's a very

791
00:48:16,995 --> 00:48:20,815
Speaker 2:  chunky, it's a very chunky boy. And so orange, like

792
00:48:21,535 --> 00:48:25,515
Speaker 2:  the Apple Watch Ultra was orange. This is so

793
00:48:26,225 --> 00:48:30,155
Speaker 2:  Radioactively biohazard orange. It's a little upsetting how orange

794
00:48:30,255 --> 00:48:34,035
Speaker 2:  it is. It's, it's, I don't have it on me right now, but it looks like a

795
00:48:34,035 --> 00:48:37,755
Speaker 2:  Halloween watch. I actually ended up appreciating the regular

796
00:48:38,015 --> 00:48:41,835
Speaker 2:  Galaxy Watch seven, which I am wearing right now a whole lot more

797
00:48:42,305 --> 00:48:46,195
Speaker 2:  because I was like, oh, it doesn't hurt me to wear it 'cause it's just

798
00:48:46,215 --> 00:48:50,155
Speaker 2:  too big. It's too big for my wrist. And like the Apple Watch Ultra is also

799
00:48:50,455 --> 00:48:53,835
Speaker 2:  big for my wrist. I can stick a bunch of stuff in here. There's like a gap.

800
00:48:54,415 --> 00:48:58,275
Speaker 2:  But something about it being a square just made it a lot harder for

801
00:48:58,275 --> 00:49:01,915
Speaker 2:  me to wear. When I was testing the sleep apnea feature, it was just like,

802
00:49:01,915 --> 00:49:05,115
Speaker 2:  Nope, we can't get readings from you. So I actually had to wear the other

803
00:49:05,115 --> 00:49:08,755
Speaker 2:  one to get the readings and test that feature out. So it was,

804
00:49:09,775 --> 00:49:12,755
Speaker 2:  it makes it sound bad, but it's actually a really great Android watch. I

805
00:49:12,755 --> 00:49:16,195
Speaker 2:  would say it's like one of the best Android watches that are available. I

806
00:49:16,195 --> 00:49:19,315
Speaker 2:  just, there's a little part of me that hurts

807
00:49:19,905 --> 00:49:23,635
Speaker 2:  that Samsung decided to make an Apple watch in order to create

808
00:49:23,665 --> 00:49:27,515
Speaker 2:  this excellent Android watch. It just felt wrong. It felt wrong to

809
00:49:27,515 --> 00:49:31,475
Speaker 2:  me. 'cause I truly do love how weird and unique the Samsung smart watches

810
00:49:31,545 --> 00:49:34,715
Speaker 2:  have been with their rotating bezels and just

811
00:49:35,105 --> 00:49:38,715
Speaker 2:  Samsung being weird about how it does health stuff.

812
00:49:39,655 --> 00:49:43,395
Speaker 2:  The ages metric is just completely bonkers. Yes. I don't understand

813
00:49:43,455 --> 00:49:44,035
Speaker 2:  why it's there.

814
00:49:44,475 --> 00:49:48,355
Speaker 3:  I was definitely gonna ask you about this, by the way. You said age and that

815
00:49:48,355 --> 00:49:52,075
Speaker 3:  stands for Advanced Glycation end products. Yes. Have they

816
00:49:52,135 --> 00:49:53,035
Speaker 3:  yet told you what that means?

817
00:49:53,575 --> 00:49:57,355
Speaker 2:  So You can, there's, there's a whole like informational

818
00:49:57,355 --> 00:50:00,795
Speaker 2:  section that You can read in the app, in the Samsung Health app about what

819
00:50:00,795 --> 00:50:04,315
Speaker 2:  it is. And it's basically measuring compounds when

820
00:50:04,505 --> 00:50:08,355
Speaker 2:  fats and sugars in your blood oxidize. How

821
00:50:08,355 --> 00:50:11,755
Speaker 2:  is it doing this? Samsung won't say, it just says there's a new three in

822
00:50:11,755 --> 00:50:15,035
Speaker 2:  one active biosensor with more LED colors and whatnot.

823
00:50:15,655 --> 00:50:18,995
Speaker 2:  And there's a little spectrum and it goes from low to high

824
00:50:19,855 --> 00:50:23,555
Speaker 2:  and it, if you're low, I guess you're

825
00:50:23,555 --> 00:50:27,035
Speaker 2:  metabolically younger than your age. And if you're high, I guess you're

826
00:50:27,705 --> 00:50:28,965
Speaker 2:  not your metabolic page.

827
00:50:28,965 --> 00:50:29,965
Speaker 3:  You're measur this through the skin.

828
00:50:30,825 --> 00:50:31,045
Speaker 2:  Yes.

829
00:50:31,785 --> 00:50:35,085
Speaker 3:  Are they they're not like pricking you, the watch isn't like taking your

830
00:50:35,085 --> 00:50:35,245
Speaker 3:  blood.

831
00:50:35,545 --> 00:50:36,805
Speaker 2:  No, everything is noninvasive.

832
00:50:36,805 --> 00:50:39,365
Speaker 3:  Okay. And there's not like a, there's not like a chemical sense. It's not

833
00:50:39,365 --> 00:50:42,525
Speaker 3:  like in your sweat, they're just doing the thing that everyone does. They're

834
00:50:42,525 --> 00:50:45,965
Speaker 2:  Just shining light into your skin. And based on like how it reflects back,

835
00:50:45,965 --> 00:50:49,885
Speaker 2:  they're just telling you things that have not been FDA cleared.

836
00:50:50,075 --> 00:50:53,805
Speaker 2:  It's very experimental. Like I did not know what

837
00:50:53,875 --> 00:50:57,565
Speaker 2:  this, this actual feature was. So you know, when I was getting briefed on

838
00:50:57,565 --> 00:51:00,885
Speaker 2:  it and I went up to Samsung and I'm like, so what is this? And they were

839
00:51:00,885 --> 00:51:04,605
Speaker 2:  basically like, well we have this new sensor, so we thought why not

840
00:51:04,605 --> 00:51:06,325
Speaker 2:  throw this also in here? This

841
00:51:06,325 --> 00:51:07,965
Speaker 3:  Is the, that's the most Samsung answer of all time.

842
00:51:08,265 --> 00:51:12,165
Speaker 2:  Oh no. And, and I was like, oh, okay. So like how, how do you intend

843
00:51:12,165 --> 00:51:15,205
Speaker 2:  people to use it? And they're like, actually we dunno. Like we're just kind

844
00:51:15,205 --> 00:51:18,765
Speaker 2:  of, we just kind of wanna see how people use it. And so a lot of the new

845
00:51:19,005 --> 00:51:22,365
Speaker 2:  features are quote unquote AI powered and you're getting these AI insights

846
00:51:22,505 --> 00:51:26,005
Speaker 2:  and they're telling me things that I don't know what to do with. Because

847
00:51:26,705 --> 00:51:30,325
Speaker 2:  on the one hand the watch was like, Hey, you have not been

848
00:51:30,325 --> 00:51:33,485
Speaker 2:  consistent with your sleep little lady, you should work on that. And then

849
00:51:33,685 --> 00:51:37,085
Speaker 2:  I refreshed the app and it's like, great job staying consistent with your

850
00:51:37,085 --> 00:51:40,645
Speaker 2:  sleep schedule. And I was just like, so which is it? Which is it? Which is

851
00:51:40,925 --> 00:51:44,605
Speaker 2:  it? I had one night where I didn't like have my sleep schedule completely

852
00:51:44,605 --> 00:51:47,885
Speaker 2:  perfect. So I guess that's what it was. Reacting. That's not helpful. And

853
00:51:47,885 --> 00:51:50,365
Speaker 2:  then the ages metric, they're like, well if you wanna improve your score,

854
00:51:50,465 --> 00:51:53,685
Speaker 2:  I'm smack dab in the middle. So I don't know, I guess that means I'm my age

855
00:51:54,285 --> 00:51:56,005
Speaker 2:  metabolically. Who knows. Wait,

856
00:51:56,005 --> 00:51:59,085
Speaker 3:  I just, wait, can I just stick with the age thing for one second? Yes. So

857
00:51:59,085 --> 00:52:03,005
Speaker 3:  they have a metric called age. Yes. Which I

858
00:52:03,005 --> 00:52:06,645
Speaker 3:  think most people know what a metric called age

859
00:52:06,855 --> 00:52:08,645
Speaker 3:  meant to measure, which is your age.

860
00:52:09,345 --> 00:52:09,565
Speaker 2:  Yes.

861
00:52:10,135 --> 00:52:12,845
Speaker 3:  Right. That's a well-known label for a thing.

862
00:52:14,305 --> 00:52:18,295
Speaker 3:  And then it measures, first of all, let me just quote your

863
00:52:18,335 --> 00:52:22,135
Speaker 3:  own review back. You V Yes, yes. Meanwhile, the age metric is baffling is

864
00:52:22,135 --> 00:52:23,055
Speaker 3:  what you have written here.

865
00:52:23,315 --> 00:52:24,815
Speaker 2:  Yes, it is baffling.

866
00:52:25,035 --> 00:52:29,015
Speaker 3:  So then it, it's shining a light through your wrist and,

867
00:52:29,395 --> 00:52:33,135
Speaker 3:  and then somehow from what it, the reflections of that light,

868
00:52:33,675 --> 00:52:37,495
Speaker 3:  it is tracking how protein and fat

869
00:52:37,515 --> 00:52:41,455
Speaker 3:  are oxidized by sugar and then telling you whether that's a low, medium or

870
00:52:41,455 --> 00:52:44,815
Speaker 3:  high. And then they're saying that is something called age. And I just wanna

871
00:52:44,815 --> 00:52:48,175
Speaker 3:  point out how deeply meaningfully confusing that is.

872
00:52:48,885 --> 00:52:52,695
Speaker 2:  It's your metabolic age, right? If they label to do these things, skin

873
00:52:52,695 --> 00:52:53,135
Speaker 2:  outta here,

874
00:52:53,675 --> 00:52:57,535
Speaker 3:  But like, but there's no, but is,

875
00:52:57,635 --> 00:53:01,615
Speaker 3:  but it's like a new metric. Is this a metric that exists in like the literature?

876
00:53:02,515 --> 00:53:06,055
Speaker 2:  It is a metric that exists in research and scientists

877
00:53:06,635 --> 00:53:10,215
Speaker 2:  are studying it, but in a consumer watch it means

878
00:53:10,405 --> 00:53:13,735
Speaker 2:  nothing. It means absolutely nothing

879
00:53:14,235 --> 00:53:18,175
Speaker 2:  in a consumer watch you. So it tells me that I am like kind

880
00:53:18,175 --> 00:53:21,775
Speaker 2:  of in the medium, neither low or high in the yellow

881
00:53:21,775 --> 00:53:25,685
Speaker 2:  section, so to speak. And it's like, okay, so to improve your

882
00:53:25,735 --> 00:53:27,165
Speaker 2:  age's metric, here's what

883
00:53:27,165 --> 00:53:28,405
Speaker 3:  You do. Doesn't know how old you are.

884
00:53:29,115 --> 00:53:31,965
Speaker 2:  Yeah, you do. I did put my like demographic information

885
00:53:31,965 --> 00:53:35,765
Speaker 3:  In. So presumably if you're, it's, is it

886
00:53:35,825 --> 00:53:39,645
Speaker 3:  at all related to your actual age? It's like if you're green, is your metabolic

887
00:53:39,745 --> 00:53:41,245
Speaker 3:  age younger than your actual age?

888
00:53:41,705 --> 00:53:44,125
Speaker 2:  Yes. That's basically what they're telling you. This is the idea

889
00:53:44,145 --> 00:53:44,565
Speaker 3:  That's

890
00:53:44,565 --> 00:53:46,565
Speaker 2:  So messed up. This is the idea. It's okay.

891
00:53:46,565 --> 00:53:47,285
Speaker 3:  And then how did it,

892
00:53:47,515 --> 00:53:51,485
Speaker 2:  It's not messed up. It means nothing. It means nothing. So

893
00:53:51,505 --> 00:53:55,405
Speaker 2:  it doesn't have to be messed up. It's just baffling and it makes

894
00:53:55,405 --> 00:53:55,725
Speaker 2:  no sense.

895
00:53:56,165 --> 00:53:59,005
Speaker 3:  I like new, I like new metrics. Yes. I'm just, I'm just trying to understand

896
00:53:59,005 --> 00:54:02,125
Speaker 3:  this one. So somewhere, someone, somewhere is like

897
00:54:03,005 --> 00:54:06,205
Speaker 3:  a 25-year-old has a metabolism of X

898
00:54:06,825 --> 00:54:10,725
Speaker 3:  And. if you tell us you're 25 by shining a light from a smartwatch into

899
00:54:10,725 --> 00:54:11,125
Speaker 3:  your wrist,

900
00:54:12,235 --> 00:54:15,925
Speaker 2:  Tell you old this, this is a new, this is kind of a new thing that they're

901
00:54:15,925 --> 00:54:18,725
Speaker 2:  doing in wearables. It's not just Samsung. Like

902
00:54:19,875 --> 00:54:23,045
Speaker 2:  basically the Aura recently did a thing where it's like we can tell you what

903
00:54:23,045 --> 00:54:26,845
Speaker 2:  your cardiovascular age is and whether it's aligned with your physical

904
00:54:26,945 --> 00:54:30,845
Speaker 2:  age or not. And so there's

905
00:54:30,845 --> 00:54:34,645
Speaker 2:  just like this obsession with telling you whether you are like physically

906
00:54:34,885 --> 00:54:38,525
Speaker 2:  speaking, aligned with expectations for your actual age or whether

907
00:54:38,765 --> 00:54:42,605
Speaker 2:  you're quote unquote physically physiologically younger than your actual

908
00:54:42,605 --> 00:54:42,925
Speaker 2:  age or

909
00:54:43,175 --> 00:54:46,845
Speaker 3:  Older. I feel like there's a lot of people in Silicon Valley who are drinking

910
00:54:46,845 --> 00:54:49,645
Speaker 3:  the bar at the young in order to live forever. And I'm, I'm not trying to

911
00:54:49,645 --> 00:54:53,245
Speaker 3:  draw a straight line to the Galaxy Watch Ultra. I'm just saying there's a

912
00:54:53,245 --> 00:54:55,005
Speaker 3:  path, perhaps a winding path.

913
00:54:55,695 --> 00:54:58,125
Speaker 2:  There is, but, but it's, I do, it's,

914
00:54:58,445 --> 00:55:02,405
Speaker 4:  I do not want, it's funny, a smart ring to tell me if my heart is dying

915
00:55:02,465 --> 00:55:04,805
Speaker 4:  faster than I'm, well maybe you do.

916
00:55:04,865 --> 00:55:08,565
Speaker 2:  Listen, listen, listen. There are, there are health

917
00:55:08,595 --> 00:55:11,925
Speaker 2:  nuts who want this information. I don't necessarily think it's actually good

918
00:55:11,925 --> 00:55:15,645
Speaker 2:  for their mental health to have this information, but the app was just

919
00:55:15,645 --> 00:55:19,485
Speaker 2:  basically like, hey, so here's how You can improve your ages. Index a

920
00:55:19,505 --> 00:55:23,365
Speaker 2:  metric that means absolutely nothing and you'll never guess what the

921
00:55:23,365 --> 00:55:27,165
Speaker 2:  advice is because it's to eat healthy. Oh yeah. Sleep

922
00:55:27,235 --> 00:55:31,125
Speaker 2:  well and exercise. So I'm just like, oh, thanks. It's always that

923
00:55:31,755 --> 00:55:32,045
Speaker 3:  Gets

924
00:55:32,045 --> 00:55:33,485
Speaker 2:  You never, it's

925
00:55:33,485 --> 00:55:34,525
Speaker 3:  Always that last one.

926
00:55:36,475 --> 00:55:36,965
Speaker 3:  Alright,

927
00:55:38,555 --> 00:55:42,165
Speaker 3:  like all Samsung devices, this, this, all this works best if you're in the

928
00:55:42,165 --> 00:55:44,765
Speaker 3:  Galaxy ecosystem, right? Yes. Yes. I'm assuming none of it works than iPhone.

929
00:55:44,945 --> 00:55:48,165
Speaker 3:  it what gets worse if you have a Pixel or you have a one plus?

930
00:55:48,555 --> 00:55:52,205
Speaker 2:  Well, do you, would you like to have your EKGs and E AFib

931
00:55:52,205 --> 00:55:55,445
Speaker 2:  detection? You will not get that unless you have a Galaxy phone. Would you

932
00:55:55,445 --> 00:55:58,445
Speaker 2:  like to know if you have sleep apnea? You will not get that unless you have

933
00:55:58,445 --> 00:56:02,365
Speaker 2:  a Galaxy phone because that requires the Samsung Health

934
00:56:02,365 --> 00:56:05,605
Speaker 2:  Monitor app, which is separate from Samsung Health, meaning you're just not

935
00:56:05,605 --> 00:56:09,005
Speaker 2:  gonna get that. And some of the AI features are

936
00:56:09,225 --> 00:56:13,125
Speaker 2:  Galaxy phone like only as well, which actually I think that's great

937
00:56:13,125 --> 00:56:16,565
Speaker 2:  for you because the AI was very hit or miss for me

938
00:56:17,185 --> 00:56:20,845
Speaker 2:  and all the advice it gave me, it would be like, Hey, your

939
00:56:21,155 --> 00:56:25,045
Speaker 2:  ages is not ideal. Exercise more. And then

940
00:56:25,045 --> 00:56:28,445
Speaker 2:  the AI was like, you're exercising a little too much, you should rest because

941
00:56:28,445 --> 00:56:32,285
Speaker 2:  it's affecting your sleep. Oh my God. And also you're sleeping well

942
00:56:32,305 --> 00:56:35,565
Speaker 2:  and not well at the same time. So it's like cool.

943
00:56:35,975 --> 00:56:36,845
Speaker 2:  Great, thanks.

944
00:56:37,325 --> 00:56:40,485
Speaker 3:  I feel like it's a big ask to be like, we're you need a new watch and a new

945
00:56:40,485 --> 00:56:40,685
Speaker 3:  phone

946
00:56:42,025 --> 00:56:44,285
Speaker 2:  And a new ring and all of that stuff.

947
00:56:44,825 --> 00:56:48,205
Speaker 3:  So I did last week we asked

948
00:56:48,275 --> 00:56:51,965
Speaker 3:  listeners to send us a note if they were all in on the Samsung ecosystem.

949
00:56:53,445 --> 00:56:57,005
Speaker 3:  I would, I'm just gonna guess and say that if I asked

950
00:56:57,005 --> 00:57:00,885
Speaker 3:  listeners to send us notes about why they were all in on the Apple ecosystem,

951
00:57:01,745 --> 00:57:05,725
Speaker 3:  we would crash the internet, right? Like we would just get a lot I I like

952
00:57:05,825 --> 00:57:09,605
Speaker 3:  say in a breath, like I don't think CarPlay is very good and emails

953
00:57:09,665 --> 00:57:13,565
Speaker 3:  for days, man, all day long. People are like, how dare you.

954
00:57:14,045 --> 00:57:17,205
Speaker 3:  We asked for Galaxy Ecosystem users. We got two emails.

955
00:57:18,105 --> 00:57:21,885
Speaker 3:  No, just putting out there. No. So one

956
00:57:21,885 --> 00:57:25,525
Speaker 3:  person, Sean, thank you Sean for emailing, they

957
00:57:25,695 --> 00:57:28,485
Speaker 3:  wrote in, they're upgrading from a fold four to a fold six,

958
00:57:29,895 --> 00:57:33,405
Speaker 3:  which is, you know, great they're doing it because the, the fold four broke

959
00:57:33,585 --> 00:57:37,565
Speaker 3:  the warranty on the device is CED and they found a way through

960
00:57:37,565 --> 00:57:40,765
Speaker 3:  the cell phone warranty to get a fold six. Yeah,

961
00:57:41,415 --> 00:57:45,285
Speaker 3:  right. They like, they tried to use the licensed

962
00:57:45,285 --> 00:57:47,005
Speaker 3:  Samsung repair company. You break iix

963
00:57:48,715 --> 00:57:52,605
Speaker 3:  that cost money. So then they tried to use their Amex warranty, they were

964
00:57:52,605 --> 00:57:56,165
Speaker 3:  denied. Then Amex called Samsung, they said no,

965
00:57:56,425 --> 00:57:59,285
Speaker 3:  and then they went to cell phone insurance, which they had, and that replaced

966
00:57:59,285 --> 00:58:02,645
Speaker 3:  the got to fold six. So that is one way to stay in the Samsung

967
00:58:02,755 --> 00:58:06,685
Speaker 3:  ecosystem is a nightmare repair journey because you're fold, the seals

968
00:58:06,685 --> 00:58:10,485
Speaker 3:  are fold for Brooke. I would not say this is the type of email we get when

969
00:58:10,485 --> 00:58:13,685
Speaker 3:  we ask why people stay in the Apple ecosystem. So that's one.

970
00:58:14,365 --> 00:58:17,845
Speaker 3:  We got another one. Thank you so much for emailing from Israel.

971
00:58:19,755 --> 00:58:23,325
Speaker 3:  This one says you asked to hear from the Galaxy Ecosystem user. I am that

972
00:58:23,325 --> 00:58:24,485
Speaker 3:  user, which is

973
00:58:25,485 --> 00:58:25,565
Speaker 2:  Singular.

974
00:58:27,295 --> 00:58:30,845
Speaker 3:  Truly appreciate that line they wrote to us. I'm typing this in my Galaxy

975
00:58:31,035 --> 00:58:34,885
Speaker 3:  Tabs eight Ultra. I upgraded from Z fold four to Z fold six. I use the

976
00:58:34,905 --> 00:58:38,725
Speaker 3:  tablet form factor much more than the phone fold factor, which means they

977
00:58:38,725 --> 00:58:42,525
Speaker 3:  have both a Galaxy tablet and they are constantly using their phone and

978
00:58:42,525 --> 00:58:46,125
Speaker 3:  tablet mode. This is the most Android tablet usage I've ever heard of in

979
00:58:46,125 --> 00:58:47,685
Speaker 3:  my entire life. S incredible.

980
00:58:47,685 --> 00:58:47,965
Speaker 2:  You're

981
00:58:48,005 --> 00:58:50,485
Speaker 3:  Breaking, you're, you're, you're whatever you do, everyone's like, that's

982
00:58:50,485 --> 00:58:54,445
Speaker 3:  90% of the usage. They have a Galaxy Watch five Pro, and

983
00:58:54,445 --> 00:58:58,045
Speaker 3:  they're waiting on their Galaxy Watch Ultra. And then they have four TVs

984
00:58:58,045 --> 00:59:01,085
Speaker 3:  including a, a Frame tv. They're washer and dryer Samsung. They have the

985
00:59:01,085 --> 00:59:04,805
Speaker 3:  air dresser, the fridge, the oven, a robot vacuum, a smart things hub, a

986
00:59:04,805 --> 00:59:07,885
Speaker 3:  Blu-ray player, you're my people. And then it says, and something else I'm

987
00:59:07,885 --> 00:59:09,765
Speaker 3:  forgetting, I'm sure. And then

988
00:59:10,145 --> 00:59:10,805
Speaker 2:  Are they Korean?

989
00:59:11,905 --> 00:59:15,365
Speaker 3:  And then at the end it says, my husband uses an iPhone and an iPad.

990
00:59:16,185 --> 00:59:17,205
Speaker 2:  Oh wow.

991
00:59:19,355 --> 00:59:23,125
Speaker 2:  There's a case study. So like, so my

992
00:59:23,125 --> 00:59:26,525
Speaker 3:  Family, so, and just say one Thank you. I love this. I just,

993
00:59:26,955 --> 00:59:30,725
Speaker 3:  I'll, I'm gonna write back, but you, you've neglected to say

994
00:59:30,785 --> 00:59:34,205
Speaker 3:  why you have chosen, so I would love to know wh why.

995
00:59:35,465 --> 00:59:37,765
Speaker 3:  And if anyone else is out there and wants to send us a note with, with their

996
00:59:37,765 --> 00:59:41,205
Speaker 3:  list and why I'm, I'm dying to read them because we know so much

997
00:59:41,575 --> 00:59:45,005
Speaker 3:  about the Apple ecosystem and how people feel about it. Trust me, I, we know

998
00:59:45,065 --> 00:59:48,845
Speaker 3:  so much you are not quiet. I'm dying to know how people at

999
00:59:48,845 --> 00:59:51,965
Speaker 3:  other ecosystems feel about it. And it's like really interesting to get this,

1000
00:59:51,965 --> 00:59:55,885
Speaker 3:  these notes. So keep writing in. I'm, I'm very curious. That said, my husband

1001
00:59:55,885 --> 00:59:58,885
Speaker 3:  used an iPhone. iPad is very funny. Like deeply funny.

1002
00:59:59,115 --> 01:00:01,405
Speaker 4:  Yeah, that's, there's something going on. Oh,

1003
01:00:03,005 --> 01:00:04,165
Speaker 4:  I love it. Someone

1004
01:00:04,305 --> 01:00:05,845
Speaker 3:  Is very excited about RCS.

1005
01:00:06,745 --> 01:00:07,325
Speaker 4:  Oh my gosh.

1006
01:00:09,665 --> 01:00:10,645
Speaker 2:  Oh, that's so good.

1007
01:00:11,155 --> 01:00:15,045
Speaker 3:  Yeah. I mean look it, there's a bunch of Samsung devices

1008
01:00:15,065 --> 01:00:18,685
Speaker 3:  in this house. Somehow I've ended up in the LG think Q ecosystem

1009
01:00:18,995 --> 01:00:22,925
Speaker 3:  because that's our washer and dryer and fridge. Oh boy. God only

1010
01:00:22,925 --> 01:00:26,605
Speaker 3:  knows why. And now I'm like, man, I better, I got an update the other, I

1011
01:00:26,605 --> 01:00:29,925
Speaker 3:  got a notification the other day that new songs for summer were available

1012
01:00:29,925 --> 01:00:32,405
Speaker 3:  for the washer and dryer. And I was like, yes, this is the dream.

1013
01:00:32,865 --> 01:00:33,685
Speaker 4:  Oh my God. But

1014
01:00:33,685 --> 01:00:36,285
Speaker 2:  Wait, wait. Songs for your washer and dryer to play.

1015
01:00:36,675 --> 01:00:40,165
Speaker 3:  Yeah, every quarter LG sends new little icons

1016
01:00:40,585 --> 01:00:42,965
Speaker 3:  for the seasons and then issues new like songs.

1017
01:00:43,955 --> 01:00:45,165
Speaker 2:  This is why they couldn't presso on

1018
01:00:45,165 --> 01:00:48,805
Speaker 4:  There. They couldn't make phones anymore. They've like, we have to divert

1019
01:00:48,805 --> 01:00:50,725
Speaker 4:  those resources to the washer.

1020
01:00:51,675 --> 01:00:54,565
Speaker 3:  There's 50 engineers with Casio keyboards being like,

1021
01:00:56,245 --> 01:00:59,645
Speaker 3:  like, and once a quarter they send them to me and it's great. Oh my God.

1022
01:00:59,805 --> 01:01:00,765
Speaker 3:  I truly, I truly love.

1023
01:01:01,315 --> 01:01:02,005
Speaker 4:  It's incredible.

1024
01:01:02,585 --> 01:01:06,445
Speaker 3:  We can't wrap up the Samsung conversation without talking about

1025
01:01:07,365 --> 01:01:07,845
Speaker 3:  Saturday Samsung.

1026
01:01:08,805 --> 01:01:09,405
Speaker 2:  Saturday Samsung.

1027
01:01:10,145 --> 01:01:13,725
Speaker 3:  So if you don't know, this is what we have started calling Samsung's relentless

1028
01:01:13,765 --> 01:01:17,685
Speaker 3:  attempts to sell more products, which are all born of the

1029
01:01:17,685 --> 01:01:21,645
Speaker 3:  company losing some money. Sales are flat and they

1030
01:01:22,185 --> 01:01:26,045
Speaker 3:  issued an edict saying everyone had to come to work six days a week until

1031
01:01:26,045 --> 01:01:26,685
Speaker 3:  they recovered

1032
01:01:28,415 --> 01:01:32,085
Speaker 3:  crazy. And it's, it's their corporate employees, which means a bunch of suits

1033
01:01:32,085 --> 01:01:35,845
Speaker 3:  are sitting around Samsung headquarters on Saturday being like, we sell more

1034
01:01:35,875 --> 01:01:39,805
Speaker 3:  shit. They're not making any, they're the suits. They, they, they, what

1035
01:01:39,805 --> 01:01:43,085
Speaker 3:  are they gonna do? So that it's only weird promotions. So we have covered,

1036
01:01:43,225 --> 01:01:47,125
Speaker 3:  you get a free tv if you buy a tv, which is amazing, we have covered, you

1037
01:01:47,125 --> 01:01:50,845
Speaker 3:  get a free TV if you buy a phone also amazing. You would think it would

1038
01:01:51,085 --> 01:01:55,045
Speaker 3:  continue in this vein, but no. Last Saturday,

1039
01:01:55,405 --> 01:01:59,205
Speaker 3:  I guess burst of creativity and they

1040
01:01:59,205 --> 01:02:02,765
Speaker 3:  decided that the best way to market the Galaxy Z flip

1041
01:02:03,865 --> 01:02:04,405
Speaker 3:  in America

1042
01:02:06,175 --> 01:02:10,045
Speaker 3:  would be to say that the z flip, it's a folding phone.

1043
01:02:10,325 --> 01:02:14,165
Speaker 3:  A little flippy guy is ideal for busy police officers

1044
01:02:14,165 --> 01:02:15,445
Speaker 3:  to wear his body cam.

1045
01:02:18,225 --> 01:02:20,765
Speaker 2:  That's so Samsung. It's very good. Oh, Samsung.

1046
01:02:21,105 --> 01:02:25,085
Speaker 4:  Oh, it's so bad. Like the, I have put a

1047
01:02:25,115 --> 01:02:29,005
Speaker 4:  folding phone on my shirt. I think I am safe to

1048
01:02:29,005 --> 01:02:32,725
Speaker 4:  say I'm one of the few people who's tried that outside of these police officers.

1049
01:02:32,915 --> 01:02:36,445
Speaker 4:  It's a dumb idea. Like it's just not a good idea that

1050
01:02:36,505 --> 01:02:40,445
Speaker 2:  It just doesn't seem secure. Also, like where

1051
01:02:40,585 --> 01:02:43,165
Speaker 2:  are they? Don't they wear vests?

1052
01:02:44,375 --> 01:02:45,665
Speaker 2:  This, this doesn't look like

1053
01:02:45,665 --> 01:02:48,865
Speaker 3:  They, well they, they worked so they didn't just like issue them like T-Mobile

1054
01:02:48,865 --> 01:02:49,265
Speaker 3:  phones.

1055
01:02:50,935 --> 01:02:54,145
Speaker 3:  They partnered, there's a, there's a post on Samsung's website. It's titled

1056
01:02:54,145 --> 01:02:57,385
Speaker 3:  Samsung Technology is helping police authority protect the public safety,

1057
01:02:57,915 --> 01:03:01,305
Speaker 3:  which is a lot. They partnered with a company

1058
01:03:01,685 --> 01:03:05,265
Speaker 3:  called Visual Labs, which is quote, a leading body camera solution provider

1059
01:03:06,835 --> 01:03:10,545
Speaker 3:  capitalism. And then two police departments in Missouri did a pilot

1060
01:03:10,545 --> 01:03:14,425
Speaker 3:  program with all of this. They have a customized Z flip

1061
01:03:15,535 --> 01:03:19,505
Speaker 3:  that has like slightly different buttons. The the

1062
01:03:19,505 --> 01:03:23,305
Speaker 3:  phones can be set to automatically begin recording when the

1063
01:03:23,305 --> 01:03:26,625
Speaker 3:  phone detects a pursuit or if you're

1064
01:03:26,625 --> 01:03:26,985
Speaker 2:  In the car

1065
01:03:27,415 --> 01:03:30,705
Speaker 3:  When the emergency lights are turned on. And then the video footage is sent

1066
01:03:30,705 --> 01:03:32,265
Speaker 3:  to the Visual Labs cloud.

1067
01:03:33,055 --> 01:03:36,145
Speaker 2:  Okay, this is all very good looking at this. It's all very good looking at

1068
01:03:36,145 --> 01:03:39,185
Speaker 2:  this. That's ridiculous. It's so

1069
01:03:39,185 --> 01:03:39,345
Speaker 4:  Bad.

1070
01:03:39,735 --> 01:03:42,105
Speaker 2:  That is so why

1071
01:03:42,535 --> 01:03:46,345
Speaker 3:  Well get ready V because 25 more police departments are gonna start wearing

1072
01:03:46,605 --> 01:03:48,345
Speaker 3:  he flip as body cams. Oh

1073
01:03:48,375 --> 01:03:49,945
Speaker 4:  Lord. Oh, oh. And

1074
01:03:49,945 --> 01:03:53,305
Speaker 3:  I was wrong. It, there's a partnership with T-Mobile. So they are T-Mobile

1075
01:03:53,305 --> 01:03:53,585
Speaker 3:  phones.

1076
01:03:54,815 --> 01:03:58,545
Speaker 4:  Like what do you, what do you need the rest of the phone on your person

1077
01:03:58,805 --> 01:04:01,185
Speaker 4:  for like, you just need a camera and some,

1078
01:04:01,225 --> 01:04:03,705
Speaker 3:  I just like the idea that people are gonna like flip it open and then like

1079
01:04:03,705 --> 01:04:04,705
Speaker 3:  make a TikTok and like

1080
01:04:04,705 --> 01:04:08,385
Speaker 4:  Close Yeah, yeah. They're gonna start doing some dances and Yeah,

1081
01:04:09,005 --> 01:04:11,025
Speaker 4:  no, I, oh good lord it's not. Okay.

1082
01:04:11,795 --> 01:04:14,865
Speaker 3:  There is a, there's a, this whole, this whole press release is great, but

1083
01:04:16,775 --> 01:04:20,545
Speaker 3:  there's just like other benefits which are basically like this phone has

1084
01:04:20,545 --> 01:04:24,185
Speaker 3:  a camera in it. So one of the benefits is in addition to their uses body-worn

1085
01:04:24,185 --> 01:04:27,945
Speaker 3:  cameras Z flip devices can help improve evidence gathering and transparency.

1086
01:04:28,205 --> 01:04:31,465
Speaker 3:  No, but clearly documenting details of arrests and other interactions.

1087
01:04:32,305 --> 01:04:33,385
Speaker 2:  Absolutely not there,

1088
01:04:33,385 --> 01:04:34,745
Speaker 4:  There's literally a feature on this,

1089
01:04:35,205 --> 01:04:35,465
Speaker 2:  The camera

1090
01:04:35,465 --> 01:04:37,425
Speaker 4:  That's just draw stuff that wasn't there.

1091
01:04:39,935 --> 01:04:42,785
Speaker 4:  Body cameras do not have that. This is such a

1092
01:04:42,785 --> 01:04:43,145
Speaker 3:  Bad idea.

1093
01:04:43,695 --> 01:04:43,985
Speaker 4:  Draw,

1094
01:04:44,455 --> 01:04:46,345
Speaker 2:  Draw in the, the drugs. Just

1095
01:04:46,345 --> 01:04:46,945
Speaker 3:  Put the cocaine here.

1096
01:04:47,445 --> 01:04:49,185
Speaker 2:  Put the cocaine here in the dash.

1097
01:04:50,645 --> 01:04:54,465
Speaker 3:  Yes. The Galaxy Z Flip a quote, the Galaxy Z flip additionally functions

1098
01:04:54,465 --> 01:04:56,785
Speaker 3:  as a digital camera needed for taking pictures of crime

1099
01:07:43,895 --> 01:07:47,875
Speaker 3:  all of the headlines and then we'll like figure out what's going on. The

1100
01:07:47,875 --> 01:07:50,315
Speaker 3:  first one though is just one headline and it's our fault.

1101
01:07:52,405 --> 01:07:56,315
Speaker 3:  Logitech's new CEO Hanukkah. Faber was on decoder this week.

1102
01:07:57,285 --> 01:08:00,755
Speaker 3:  She's the new co. She just started like late last year. We had the old co

1103
01:08:00,985 --> 01:08:02,315
Speaker 3:  bracken Daryl on several times.

1104
01:08:03,955 --> 01:08:07,715
Speaker 3:  I like I, one of my ideas with decoder is there's more than five companies

1105
01:08:07,715 --> 01:08:10,515
Speaker 3:  in the world and so we should talk to all the companies and pay attention

1106
01:08:10,515 --> 01:08:13,555
Speaker 3:  to them. That generally goes well. And Log is one of these companies, I think

1107
01:08:13,555 --> 01:08:17,325
Speaker 3:  it's Undercovered, right? Like they're just around you don't think about

1108
01:08:17,355 --> 01:08:20,565
Speaker 3:  them very much. and it turned out under Brack and Daryl,

1109
01:08:21,075 --> 01:08:24,845
Speaker 3:  like how worked was bonkers. Like he's like, I have 23

1110
01:08:24,845 --> 01:08:28,005
Speaker 3:  direct reports. Everyone's just allowed to do whatever they want. I buy companies,

1111
01:08:28,045 --> 01:08:31,365
Speaker 3:  I leave 'em alone. There's no overlap. It's like everyone have a good time,

1112
01:08:31,365 --> 01:08:35,205
Speaker 3:  which is how you end up with em buying like 50 headphone companies and

1113
01:08:35,205 --> 01:08:36,805
Speaker 3:  they're all just kinda like doing their thing. They're like, I wonder if

1114
01:08:36,805 --> 01:08:39,685
Speaker 3:  they have a strategy. The answer was they do. Which is ev leave Everybody

1115
01:08:39,685 --> 01:08:43,605
Speaker 3:  Alone, which worked just to whatever extent Bracken

1116
01:08:43,605 --> 01:08:47,185
Speaker 3:  Leaves Lodge Tech, he now works for,

1117
01:08:47,825 --> 01:08:51,305
Speaker 3:  I think it's called VF Brands. They own Vans and Supreme and the North Face.

1118
01:08:52,205 --> 01:08:55,585
Speaker 3:  So the guy who ran Log Check is now the CEO of Supreme. Incredible.

1119
01:08:56,515 --> 01:08:59,345
Speaker 3:  We're gonna see how that goes. We're gonna, I'm gonna try to get him back

1120
01:08:59,345 --> 01:09:03,185
Speaker 3:  to be like so Supreme, that sort of thing. And they have a new co Hanukkah

1121
01:09:03,285 --> 01:09:07,265
Speaker 3:  who came from like Unilever and Proctor and Gamble. She has this

1122
01:09:07,265 --> 01:09:10,425
Speaker 3:  background in consumer goods and like marketing really interesting. And I,

1123
01:09:10,425 --> 01:09:13,705
Speaker 3:  you know, I was like, are you gonna change this wacky corporate structure?

1124
01:09:14,125 --> 01:09:18,065
Speaker 3:  And that was what I thought decoder was gonna be bad. Truly. That's what

1125
01:09:18,065 --> 01:09:19,865
Speaker 3:  I thought we were gonna talk. And then we did talk about that for a while.

1126
01:09:20,165 --> 01:09:23,545
Speaker 3:  And then she's like, I wanna build a forever mouse. And

1127
01:09:24,255 --> 01:09:28,025
Speaker 3:  this keeps happening to me on decoder where I'm like, what? Explain.

1128
01:09:28,645 --> 01:09:31,065
Speaker 3:  And she's like, I wanna be able a really beautiful mouse. It's like very

1129
01:09:31,065 --> 01:09:34,185
Speaker 3:  heavy, very premium. And we'll just do software updates forever and that

1130
01:09:34,185 --> 01:09:37,865
Speaker 3:  will be your forever mouse. Like you'll just have it forever. And I'm

1131
01:09:37,865 --> 01:09:39,865
Speaker 3:  thinking, well I've had this mouse forever,

1132
01:09:40,475 --> 01:09:40,825
Speaker 2:  Right?

1133
01:09:41,605 --> 01:09:41,825
Speaker 3:  It

1134
01:09:42,615 --> 01:09:42,905
Speaker 2:  Same,

1135
01:09:43,235 --> 01:09:44,665
Speaker 3:  Right? It's like 10 years old.

1136
01:09:46,525 --> 01:09:49,705
Speaker 2:  Are people replacing mice? No. Yeah.

1137
01:09:49,705 --> 01:09:53,465
Speaker 3:  So very confused. Very confused. And I You can hear if you

1138
01:09:53,465 --> 01:09:56,465
Speaker 3:  listen to it, the transcript reads one way, but you actually go and listen

1139
01:09:56,465 --> 01:10:00,065
Speaker 3:  to Coder. It's very You can, I'm just laughing like, I'm like, what are you

1140
01:10:00,065 --> 01:10:03,705
Speaker 3:  talking about? I'm like, there's only two ways to monetize hardware over

1141
01:10:03,705 --> 01:10:07,625
Speaker 3:  time. It is subscriptions or it is ads. And she's like,

1142
01:10:07,925 --> 01:10:11,065
Speaker 3:  yep, those are the two ways. I'm like, so you're gonna do a subscription

1143
01:10:11,065 --> 01:10:14,385
Speaker 3:  mouse? And she's like, I am and I encourage you to go listen to this because

1144
01:10:14,525 --> 01:10:16,825
Speaker 3:  I'm just like losing my mind like

1145
01:10:17,025 --> 01:10:20,465
Speaker 2:  Laughing. What am I subscribing to with a subscription mouse? Right?

1146
01:10:20,605 --> 01:10:23,145
Speaker 3:  So we it's AI suffer.

1147
01:10:24,045 --> 01:10:24,265
Speaker 2:  No,

1148
01:10:27,255 --> 01:10:31,225
Speaker 2:  what is AI software for my mouse? Yeah. Whatcha you gonna

1149
01:10:31,225 --> 01:10:33,985
Speaker 2:  change the DPI based on what I'm doing?

1150
01:10:34,605 --> 01:10:37,625
Speaker 3:  No, no, no, no, no. It's not even, it's not even that. It like you were like

1151
01:10:37,625 --> 01:10:40,585
Speaker 3:  that. Here's an idea. No, no, it's worse than that. They're, they're putting

1152
01:10:40,585 --> 01:10:44,385
Speaker 3:  the buttons on the mouse so that when you see a field, like any text

1153
01:10:44,385 --> 01:10:48,225
Speaker 3:  field on your computer, the mouse will do the

1154
01:10:48,225 --> 01:10:49,185
Speaker 3:  prompting for you.

1155
01:10:50,185 --> 01:10:50,985
Speaker 2:  Absolutely not. They,

1156
01:10:50,985 --> 01:10:53,265
Speaker 3:  So they've already rolled this out in Options Pro in options Plus.

1157
01:10:53,885 --> 01:10:54,105
Speaker 2:  No,

1158
01:10:54,655 --> 01:10:58,145
Speaker 3:  This is like a thing, this is a thing in in Logic Mice today for like newer

1159
01:10:58,385 --> 01:11:01,865
Speaker 3:  mice. Why older mice are not allowed to have the software? I didn't even

1160
01:11:01,865 --> 01:11:05,665
Speaker 3:  get to 'cause I was just what? Right? Like You can listen to me like

1161
01:11:05,965 --> 01:11:07,665
Speaker 3:  dewire my brain in this conversation.

1162
01:11:09,365 --> 01:11:13,350
Speaker 3:  So I, I was like, so I'm asked and she's like, yeah, this is how our video

1163
01:11:13,350 --> 01:11:16,605
Speaker 3:  conferencing software works. And I was like, but it's,

1164
01:11:16,985 --> 01:11:20,645
Speaker 3:  that's a service. This is a mouse. And she's like, yeah, a mouse. I put like

1165
01:11:20,645 --> 01:11:24,005
Speaker 3:  a beautiful, and I believe she said Diamond and crusted mouse. So that was

1166
01:11:24,005 --> 01:11:27,845
Speaker 3:  decoder. These things happen to and decoder all the time. My god, I, what

1167
01:11:27,845 --> 01:11:31,405
Speaker 3:  I was not expecting was the idea of the

1168
01:11:31,605 --> 01:11:34,805
Speaker 3:  subscription Mouse has just like, it's a news cycle

1169
01:11:35,865 --> 01:11:39,485
Speaker 3:  and it was just like the new see riffing, but now there's a full news cycle

1170
01:11:39,745 --> 01:11:43,405
Speaker 3:  launch tech doing subscription wise. Oh no. And I will tell you

1171
01:11:44,145 --> 01:11:47,965
Speaker 3:  not since like HP was like subscription printers. Have people been this unhappy

1172
01:11:48,375 --> 01:11:50,405
Speaker 3:  about any idea that I've ever heard.

1173
01:11:51,485 --> 01:11:51,765
Speaker 4:  I mean

1174
01:11:52,095 --> 01:11:55,845
Speaker 3:  There, there are YouTubers for making YouTube videos about it. I've seen

1175
01:11:55,875 --> 01:11:59,085
Speaker 3:  tiktoks about it's like crazy. It's like, it is. It's living its own little

1176
01:11:59,085 --> 01:11:59,285
Speaker 3:  life.

1177
01:11:59,425 --> 01:12:02,045
Speaker 4:  People are throwing away their Logitech mice like

1178
01:12:02,265 --> 01:12:06,125
Speaker 2:  No, no, we're not throwing away. This is really gonna be my forever

1179
01:12:06,405 --> 01:12:09,645
Speaker 2:  Logitech mouse now. I'm never giving it up. Yeah,

1180
01:12:09,645 --> 01:12:13,325
Speaker 3:  I will say the mouse to my other computer there is easily 15 years old.

1181
01:12:13,595 --> 01:12:15,485
Speaker 4:  Yeah. Like you just get one out of the box.

1182
01:12:15,515 --> 01:12:18,565
Speaker 3:  It's kind of gross. You don't, I'm not saying it's not a little gross,

1183
01:12:19,515 --> 01:12:22,725
Speaker 4:  They get disgusting, but like you're just in an office, you find one out

1184
01:12:22,725 --> 01:12:25,925
Speaker 4:  of a box. Someone put it there 10 years ago,

1185
01:12:25,925 --> 01:12:28,845
Speaker 2:  You don't see my thumb sweat. It's You can see my thumb sweat on this mouth.

1186
01:12:28,905 --> 01:12:31,325
Speaker 3:  You stop. It's a radio show stop showing people your thumb sweat.

1187
01:12:33,355 --> 01:12:33,845
Speaker 4:  It's just,

1188
01:12:35,745 --> 01:12:36,125
Speaker 2:  You know,

1189
01:12:38,085 --> 01:12:39,525
Speaker 3:  Describe it for the audio listener.

1190
01:12:42,045 --> 01:12:43,445
Speaker 3:  I will tell you, I rarely,

1191
01:12:45,185 --> 01:12:49,085
Speaker 3:  rarely get deeply surprised by what happens on decoder.

1192
01:12:49,755 --> 01:12:52,885
Speaker 3:  It's mostly shit about org charts as the listeners know. And then every now

1193
01:12:52,885 --> 01:12:56,325
Speaker 3:  and again these, these past couple months, the Zoom CEO came on and was like,

1194
01:12:56,815 --> 01:13:00,765
Speaker 3:  we're building AI clones of everyone to put in Zoom meetings. And

1195
01:13:00,765 --> 01:13:04,445
Speaker 3:  again, I just encourage you, You can just listen to me try to try to

1196
01:13:04,525 --> 01:13:07,605
Speaker 3:  respond to that in real time. And look, I'll take the heat on decoder. People

1197
01:13:07,605 --> 01:13:10,365
Speaker 3:  are like, you're too nice, whatever. Like, I'm always trying to be better.

1198
01:13:10,785 --> 01:13:14,605
Speaker 3:  I'm always trying to ask harder questions. Just imagine what it's like

1199
01:13:14,745 --> 01:13:18,565
Speaker 3:  in real time to be faced with someone being like, the future of Zoom

1200
01:13:18,585 --> 01:13:22,125
Speaker 3:  is AI clones and meetings and you have to like

1201
01:13:22,285 --> 01:13:25,725
Speaker 3:  acknowledge that like not die

1202
01:13:26,545 --> 01:13:30,325
Speaker 3:  and then formulate a series of follow up questions that make any sense that

1203
01:13:30,325 --> 01:13:33,365
Speaker 3:  like tell a little story. It's, it's harder than it looks is all I'm trying

1204
01:13:33,365 --> 01:13:35,645
Speaker 3:  to say. And they're serious and you have to keep 'em there, right? They're

1205
01:13:35,645 --> 01:13:38,045
Speaker 3:  dead and you have to Right. And you have to keep 'em talking.

1206
01:13:39,505 --> 01:13:42,685
Speaker 3:  and it is just, it is just much more challenging in subscription mice. You

1207
01:13:42,685 --> 01:13:44,565
Speaker 3:  can hear me just, what are you talking about?

1208
01:13:44,845 --> 01:13:47,645
Speaker 4:  Record Scratch. Yeah. God.

1209
01:13:48,145 --> 01:13:51,485
Speaker 3:  So I was not intending to cause a full news cycle about mice.

1210
01:13:52,105 --> 01:13:55,885
Speaker 3:  It was truly not my intent, but I hope that here we are. It doesn't do

1211
01:13:56,125 --> 01:14:00,045
Speaker 3:  subscription mice. I, I tried very hard to be like, I don't think

1212
01:14:00,045 --> 01:14:00,765
Speaker 3:  that's a good idea.

1213
01:14:00,905 --> 01:14:01,645
Speaker 4:  Please don't.

1214
01:14:03,625 --> 01:14:06,285
Speaker 3:  But you know, logic has this really interesting problem. Like it is interesting

1215
01:14:06,285 --> 01:14:10,245
Speaker 3:  to think of their big problem, which is if you believe that AI

1216
01:14:10,245 --> 01:14:13,125
Speaker 3:  is gonna be important and we've talked about it in this episode,

1217
01:14:14,195 --> 01:14:17,745
Speaker 3:  it might be that you just talk to your phone a bunch more, right? and it

1218
01:14:17,745 --> 01:14:21,425
Speaker 3:  just like does stuff for you. It might be that natural interfaces,

1219
01:14:21,425 --> 01:14:25,385
Speaker 3:  natural language interfaces using the cameras interface. All this stuff starts

1220
01:14:25,385 --> 01:14:28,785
Speaker 3:  to take over from traditional PCs more and more and more. This has been the

1221
01:14:28,785 --> 01:14:31,705
Speaker 3:  dream for a long time. There's a, I, you know, there's a reason that Google

1222
01:14:31,725 --> 01:14:35,695
Speaker 3:  and Microsoft, everybody else calls AI a a platform shift. And if

1223
01:14:35,695 --> 01:14:39,335
Speaker 3:  that happens, like my sales go down, right?

1224
01:14:39,645 --> 01:14:42,415
Speaker 3:  Like we, if we use desktop computers left and we talk to our phones more

1225
01:14:42,415 --> 01:14:46,335
Speaker 3:  or whatever, maybe the say like PC sales will go down and

1226
01:14:46,335 --> 01:14:49,575
Speaker 3:  then my sales go down and then we just like if you're Logitech, you're like,

1227
01:14:49,575 --> 01:14:52,975
Speaker 3:  well how do I preserve the revenue? And, and you're like gold plated diamond

1228
01:14:53,075 --> 01:14:56,575
Speaker 3:  and crusted subscription nights and like maybe that's not the right solution,

1229
01:14:58,515 --> 01:15:01,415
Speaker 3:  But You can see the pressure. It's like, it was, it was a good conversation

1230
01:15:01,415 --> 01:15:05,015
Speaker 3:  that I was not expecting just that little bit to, to resonate as much as

1231
01:15:05,015 --> 01:15:08,775
Speaker 3:  it does. That said we have to find the brother laser printers of mice.

1232
01:15:10,035 --> 01:15:13,735
Speaker 3:  Oh, oh my gosh. The other thing there were tiktoks about subscription mice

1233
01:15:13,735 --> 01:15:16,855
Speaker 3:  and that put me into like people complaining about subscriptions TikTok.

1234
01:15:17,355 --> 01:15:21,255
Speaker 3:  And then I got to one where a very nice like young woman was like

1235
01:15:21,255 --> 01:15:24,815
Speaker 3:  bitching about her. A very nice woman. It got me to one

1236
01:15:25,095 --> 01:15:28,215
Speaker 3:  where I think an influencer, she was just complaining about her subscription

1237
01:15:28,215 --> 01:15:31,855
Speaker 3:  printer and all the comments are like, buy a brother laser printer. And I

1238
01:15:31,855 --> 01:15:32,055
Speaker 3:  was like,

1239
01:15:33,615 --> 01:15:35,575
Speaker 3:  I did it. Like The Verge has accomplished its goal.

1240
01:15:35,605 --> 01:15:38,575
Speaker 4:  Yeah. One thing we stand for. Yeah,

1241
01:15:39,565 --> 01:15:42,215
Speaker 3:  Yeah. Buy one piece of heart. It lasts forever. And I think The Verge stands

1242
01:15:42,215 --> 01:15:45,495
Speaker 3:  for that. Yeah. All right, so that's lightning round one. It feels like we're

1243
01:15:45,495 --> 01:15:48,215
Speaker 3:  on a consensus that we're not gonna do AI powered subscription mice.

1244
01:15:48,595 --> 01:15:52,095
Speaker 4:  No. Don't like it. Don't want it. No, absolutely not. The

1245
01:15:52,095 --> 01:15:55,655
Speaker 3:  Idea that your mouse is the one that creates the prompt for open ai

1246
01:15:56,095 --> 01:15:59,135
Speaker 3:  s like it's bold. You know, it's,

1247
01:15:59,325 --> 01:16:01,615
Speaker 4:  It's an idea that sounds like a mouse wrote it.

1248
01:16:06,115 --> 01:16:09,055
Speaker 3:  If you're a diehard options plus ecosystem person,

1249
01:16:10,185 --> 01:16:14,175
Speaker 3:  write us a note. Alright, so, okay, here's, here's the other two where I'm

1250
01:16:14,175 --> 01:16:17,455
Speaker 3:  just gonna read a bunch of headlines for lightning Round. The first, very

1251
01:16:17,455 --> 01:16:20,575
Speaker 3:  much related to what we've been talking about with photos, there's just a

1252
01:16:20,815 --> 01:16:24,775
Speaker 3:  bunch of deep fake news this week. Everyone knows it's a problem. We

1253
01:16:24,775 --> 01:16:27,855
Speaker 3:  know we gotta stop it. And then some people are like, screw it, we're doing

1254
01:16:27,855 --> 01:16:31,415
Speaker 3:  it anyway. And by some people, I mean Elon Musk who posted a deep fake

1255
01:16:31,585 --> 01:16:35,575
Speaker 3:  video of Kamala Harris that violates X's own policies

1256
01:16:35,575 --> 01:16:39,055
Speaker 3:  about deep fake that were implemented under his ownership.

1257
01:16:39,765 --> 01:16:42,775
Speaker 3:  This isn't like there was an old rule that he disagrees with and he is bulldozing

1258
01:16:42,775 --> 01:16:46,575
Speaker 3:  it. He made a rule about D fakes 'cause everyone knows this problem. And

1259
01:16:46,575 --> 01:16:50,335
Speaker 3:  then he posted this ad with Harris, it's like Harris's campaign

1260
01:16:50,395 --> 01:16:54,055
Speaker 3:  ad with the Beyonce song, but they replaced the voiceover

1261
01:16:54,285 --> 01:16:55,575
Speaker 3:  with her saying Biden is too old.

1262
01:16:57,675 --> 01:17:01,165
Speaker 3:  Okay. You know, like in many ways this is like, this is just

1263
01:17:01,765 --> 01:17:05,645
Speaker 3:  standard political parody. Like if I hired a Kamala Harris impersonator to

1264
01:17:05,645 --> 01:17:09,005
Speaker 3:  do this, would it be a problem? Yeah.

1265
01:17:10,435 --> 01:17:13,395
Speaker 3:  I don't know. But if you already have the rule that's like, don't do this,

1266
01:17:14,015 --> 01:17:16,395
Speaker 3:  you should not. Yeah, you should not as the owner of the platform do it.

1267
01:17:16,535 --> 01:17:19,875
Speaker 3:  So he doesn't care. He doesn't give a shit. But that's where we are, right?

1268
01:17:19,875 --> 01:17:23,715
Speaker 3:  It's like already happening. There's already these like weird moments occurring

1269
01:17:23,715 --> 01:17:27,475
Speaker 3:  because of just bad faith actors on both sides. On in

1270
01:17:27,755 --> 01:17:31,395
Speaker 3:  particular one side. But it's already happening. Then

1271
01:17:31,655 --> 01:17:35,395
Speaker 3:  here's the rest of the headlines. Microsoft wants Congress to

1272
01:17:35,395 --> 01:17:39,325
Speaker 3:  outlaw, AI generated DeepFakes Google tweaks, search

1273
01:17:39,505 --> 01:17:42,005
Speaker 3:  to hide explicit DeepFakes. So if Google

1274
01:17:43,255 --> 01:17:47,245
Speaker 3:  drans your website or knocks your website for having DeepFakes on it, especially,

1275
01:17:47,245 --> 01:17:51,005
Speaker 3:  especially explicit ones search won't show them to people anymore. They're

1276
01:17:51,005 --> 01:17:51,925
Speaker 3:  just gonna go away from search

1277
01:17:54,245 --> 01:17:58,005
Speaker 3:  Congress want to, wants to carve out intimate DAID

1278
01:17:58,005 --> 01:18:01,845
Speaker 3:  fakes, which is basically what you, what there's a, there's a language conversation

1279
01:18:01,845 --> 01:18:05,485
Speaker 3:  here about what you should call these. So you might call these like AI revenge

1280
01:18:05,485 --> 01:18:05,765
Speaker 3:  porn.

1281
01:18:06,505 --> 01:18:06,725
Speaker 2:  Oh

1282
01:18:06,955 --> 01:18:10,525
Speaker 3:  Revenge porn is like a very loaded word for a lot of reasons. So now we're

1283
01:18:10,525 --> 01:18:14,485
Speaker 3:  gonna try AI intimate AI deep fakes. So

1284
01:18:14,485 --> 01:18:18,365
Speaker 3:  you carve that out from section two 30, right? So that means right now if

1285
01:18:18,365 --> 01:18:21,605
Speaker 3:  you post stuff to a platform like Facebook or X or Instagram or whatever,

1286
01:18:21,775 --> 01:18:24,565
Speaker 3:  those platforms are not liable for what you post. That's section two 30.

1287
01:18:24,565 --> 01:18:28,445
Speaker 3:  It makes the internet go round. And they're saying no, actually if you allow

1288
01:18:29,325 --> 01:18:33,125
Speaker 3:  intimate AI deep fakes, you are now liable for them. Like you as, and that

1289
01:18:33,125 --> 01:18:37,045
Speaker 3:  basically means you have to moderate them. So that's one idea. And then

1290
01:18:37,045 --> 01:18:37,805
Speaker 3:  the copyright office

1291
01:18:39,555 --> 01:18:42,485
Speaker 3:  just issued a huge report on AI and copyright, which they've been working

1292
01:18:42,565 --> 01:18:46,165
Speaker 3:  on for quite some time. And their first chapter of this report was about

1293
01:18:46,165 --> 01:18:50,125
Speaker 3:  DeepFakes. And they're, they're You can read. It's very long. It's very good.

1294
01:18:50,125 --> 01:18:52,805
Speaker 3:  It's very easy to read. We will, we'll link to it in the show notes here.

1295
01:18:53,265 --> 01:18:55,845
Speaker 3:  But they're basically like, yeah, copyright law can't do this. We need a

1296
01:18:55,845 --> 01:18:59,045
Speaker 3:  new law. This is a disaster. Here's how we think the new law should work.

1297
01:18:59,705 --> 01:19:03,645
Speaker 3:  So we're just at this moment where everyone is seeing the problem very clearly.

1298
01:19:04,105 --> 01:19:08,005
Speaker 3:  And then lastly, in the Senate, the no fakes

1299
01:19:08,065 --> 01:19:11,805
Speaker 3:  act has been introduced. It's not close to passing, but

1300
01:19:12,065 --> 01:19:15,525
Speaker 3:  yet another bill to ban de fakes. It's to regulate d fakes. So we're at this

1301
01:19:15,525 --> 01:19:19,365
Speaker 3:  point now where everybody sees the problem, right? Google is a

1302
01:19:19,605 --> 01:19:23,365
Speaker 3:  platform, is doing some stuff to stop it. Microsoft as a company is

1303
01:19:23,365 --> 01:19:27,245
Speaker 3:  asking Congress to outlaw it. The government is like, we don't have the

1304
01:19:27,245 --> 01:19:30,565
Speaker 3:  laws to do it. Some parts of the governments are saying we need to do it.

1305
01:19:31,855 --> 01:19:35,755
Speaker 3:  It feels like this is the first big AI regulation that's gonna happen.

1306
01:19:36,455 --> 01:19:38,475
Speaker 3:  It feels like that's also the first one that has to happen.

1307
01:19:38,905 --> 01:19:42,795
Speaker 2:  Deep fakes, bad deep fakes bad. So yeah, I, yeah,

1308
01:19:43,305 --> 01:19:47,035
Speaker 4:  It's like the Spider-Man pointing at Spider-Man meme. And we're like, what?

1309
01:19:47,095 --> 01:19:51,075
Speaker 4:  What's going on? Who's in charge here? Like Elon Musk is

1310
01:19:51,075 --> 01:19:54,035
Speaker 4:  making rules about deep fakes and then not following those rules, not following

1311
01:19:54,035 --> 01:19:55,915
Speaker 4:  them. The government is like, yeah, we don't know.

1312
01:19:56,795 --> 01:20:00,675
Speaker 3:  I agree. Defect's bad. Especially the, the ones that are being used to harass

1313
01:20:00,675 --> 01:20:04,395
Speaker 3:  people and intimidate people and bully people. Like those are,

1314
01:20:05,115 --> 01:20:08,045
Speaker 3:  I think, fully bad And. if you read that copyright office report, they're

1315
01:20:08,045 --> 01:20:11,645
Speaker 3:  like, a lot of our law right now is set up to protect famous people like

1316
01:20:11,645 --> 01:20:15,165
Speaker 3:  that patchwork of state likeness laws and publicity laws

1317
01:20:15,625 --> 01:20:18,525
Speaker 3:  are all basically like if you're a celebrity and Samsung

1318
01:20:19,435 --> 01:20:23,175
Speaker 3:  uses you to sell refrigerators, you can't do that.

1319
01:20:23,275 --> 01:20:26,495
Speaker 3:  And I say that because that is a real case that happened in our justice system

1320
01:20:26,665 --> 01:20:30,135
Speaker 3:  where Samsung made a robot that looked like Vanna White to solve refrigerators

1321
01:20:30,315 --> 01:20:33,135
Speaker 3:  and Vanna White sued Samsung and won Oh my God. Because the robot looked

1322
01:20:33,135 --> 01:20:36,935
Speaker 3:  too much like Vanna White real case. Real, this is a real thing that happened.

1323
01:20:36,995 --> 01:20:39,815
Speaker 3:  Oh my god. And that's like all of, all of our law right now is set up to

1324
01:20:39,815 --> 01:20:43,455
Speaker 3:  deal with that problem, which is wonderful. But it's all for famous. Like

1325
01:20:43,455 --> 01:20:47,415
Speaker 3:  you have to be famous enough so that people can be confused about, but

1326
01:20:47,435 --> 01:20:50,895
Speaker 3:  it, regular people have no protection. 'cause none of them are famous.

1327
01:20:51,515 --> 01:20:54,895
Speaker 3:  So the copyright office is saying, no, this problem is gonna affect regular

1328
01:20:54,895 --> 01:20:58,335
Speaker 3:  people. It's gonna affect teenage girls in schools. It already is. It's gonna

1329
01:20:58,335 --> 01:21:00,735
Speaker 3:  affect teenage boys in schools. It's gonna affect everybody. It's if you

1330
01:21:00,735 --> 01:21:03,975
Speaker 3:  don't like your teacher and You can fake that. Like all of this bullying

1331
01:21:04,045 --> 01:21:06,775
Speaker 3:  that can happen with DeepFakes is real. It's happening now and we don't have

1332
01:21:06,775 --> 01:21:10,735
Speaker 3:  the laws to stop it. So then you say we could stop it. But that brings you

1333
01:21:10,835 --> 01:21:14,415
Speaker 3:  all the way back around to, well what if I just hire a Trump impersonator

1334
01:21:14,635 --> 01:21:17,655
Speaker 3:  to look at a camera and be like, I'm bad. You know? Like

1335
01:21:18,485 --> 01:21:22,295
Speaker 3:  that is straightforwardly like political speech. It should be protected.

1336
01:21:22,715 --> 01:21:26,535
Speaker 3:  The line there is really blurry and I think we're, we're

1337
01:21:26,595 --> 01:21:30,495
Speaker 3:  in for maybe a pendulum swing pretty far in the direction of all of

1338
01:21:30,495 --> 01:21:33,135
Speaker 3:  that is not allowed. And then over time we're gonna have to swing it back

1339
01:21:33,135 --> 01:21:36,255
Speaker 3:  to, well you should probably be allowed to hire the Elvis impersonator, right?

1340
01:21:36,925 --> 01:21:37,415
Speaker 3:  He's dead

1341
01:21:39,665 --> 01:21:43,135
Speaker 3:  right. But should the, should the Taylor Swift impersonator at the one casino

1342
01:21:43,235 --> 01:21:47,215
Speaker 3:  be allowed to act like Taylor Swift? Like probably where, how

1343
01:21:47,215 --> 01:21:49,095
Speaker 3:  are you gonna draw that line? It's pretty unclear right now.

1344
01:21:49,775 --> 01:21:53,695
Speaker 2:  Well what do SNL do at that point? 'cause then their whole thing is

1345
01:21:53,695 --> 01:21:57,455
Speaker 2:  just like, man, I don't envy the people at the

1346
01:21:57,455 --> 01:22:00,655
Speaker 2:  copyright office or anywhere just trying to like proactively

1347
01:22:01,145 --> 01:22:04,255
Speaker 2:  think about how to deal with this. 'cause it's, it's a lot easier when it's

1348
01:22:04,495 --> 01:22:07,375
Speaker 2:  reactionary when you're like, oh, bad thing happened. We need to do something.

1349
01:22:07,435 --> 01:22:11,375
Speaker 2:  But then to actually think of it in a proactive manner and be like, how are

1350
01:22:11,375 --> 01:22:15,215
Speaker 2:  people going to use this? Well, I don't know. We're only as good

1351
01:22:15,275 --> 01:22:18,815
Speaker 2:  as the, we're only as good as the most unhinged teenage

1352
01:22:19,115 --> 01:22:22,135
Speaker 2:  boy in the classroom. Just thinking up of the most

1353
01:22:22,785 --> 01:22:24,575
Speaker 2:  crazy way to use these things. Like

1354
01:22:24,895 --> 01:22:28,855
Speaker 3:  I was that boy and don't allow me to have access to

1355
01:22:29,055 --> 01:22:31,655
Speaker 3:  computers. That was a real mistake the whole time.

1356
01:22:33,325 --> 01:22:37,175
Speaker 3:  Allison, you, I mean you, we talk about photos and cameras and device makers

1357
01:22:37,175 --> 01:22:40,695
Speaker 3:  all the time. You know, Google is changing search to hide explicit

1358
01:22:40,695 --> 01:22:44,415
Speaker 3:  DeepFakes. Microsoft wants regulations. Do you see that expressed in the

1359
01:22:44,415 --> 01:22:47,535
Speaker 3:  products that they're making at all? It's like, it's funny that they're kind

1360
01:22:47,535 --> 01:22:51,415
Speaker 3:  of like asking for someone to make rules on what they can and can't do.

1361
01:22:52,195 --> 01:22:54,495
Speaker 3:  And if they don't have the rules, they won't stop themselves.

1362
01:22:55,205 --> 01:22:59,175
Speaker 4:  Yeah. It's a weird thing of like, they're all sort of

1363
01:22:59,175 --> 01:23:02,815
Speaker 4:  like, well we definitely agree that, you know, fake

1364
01:23:03,035 --> 01:23:06,445
Speaker 4:  bad stuff is bad, so but here,

1365
01:23:06,755 --> 01:23:10,125
Speaker 4:  draw a chicken into your photo or you know, it's sort of like

1366
01:23:11,035 --> 01:23:14,645
Speaker 4:  there's no agreement between like, and we're going to

1367
01:23:15,515 --> 01:23:19,165
Speaker 4:  slow down and wait till we've figured this out. Or you know,

1368
01:23:19,275 --> 01:23:20,685
Speaker 4:  like have some kind of

1369
01:23:22,685 --> 01:23:25,405
Speaker 4:  industry standard that these things are like

1370
01:23:27,075 --> 01:23:30,925
Speaker 4:  abiding by. It's just sort of like, well we're just gonna run as fast

1371
01:23:30,925 --> 01:23:34,885
Speaker 4:  as we can over here. And then like, eh, some people will figure out

1372
01:23:35,815 --> 01:23:36,275
Speaker 4:  the laws,

1373
01:23:36,965 --> 01:23:40,955
Speaker 2:  Right? See Allison and I told you the blurry bumblebee, it absolutely

1374
01:23:41,015 --> 01:23:44,435
Speaker 2:  can destroy the fabric of our society. You said it wouldn't shoot,

1375
01:23:44,855 --> 01:23:45,395
Speaker 2:  but it can,

1376
01:23:45,655 --> 01:23:47,475
Speaker 4:  It all started with the Bumblebee.

1377
01:23:49,585 --> 01:23:53,555
Speaker 3:  Well it's like the, I'll just accuse the no fakes act, which I, I think by

1378
01:23:53,555 --> 01:23:55,635
Speaker 3:  the time you listen to this should be in the Senate.

1379
01:23:57,425 --> 01:24:00,275
Speaker 3:  This is basically a reaction to open AI and Scarlet Cha Hansen,

1380
01:24:03,515 --> 01:24:06,815
Speaker 3:  you know, the New Voice is out with open ai. It's actually really impressive

1381
01:24:06,875 --> 01:24:10,775
Speaker 3:  in like very cool ways. But obviously they made a voice that sounded a lot

1382
01:24:10,775 --> 01:24:14,495
Speaker 3:  like Scar Johansen. She's really mad. She might sue them a whole kerfuffle.

1383
01:24:14,915 --> 01:24:18,295
Speaker 3:  That's a great time for politicians to act, be like a kerfuffle. Here's some

1384
01:24:18,295 --> 01:24:22,175
Speaker 3:  stuff that I'm doing. So they had a discussion graph last year. They

1385
01:24:22,175 --> 01:24:24,175
Speaker 3:  updated it, they have it this year. And now

1386
01:24:25,715 --> 01:24:29,575
Speaker 3:  the goal is to give people a federal property, right?

1387
01:24:29,575 --> 01:24:33,495
Speaker 3:  To approve the use of their voice appearance or likeness and ex and

1388
01:24:33,515 --> 01:24:37,455
Speaker 3:  create liability to people who use their voice appearance

1389
01:24:37,455 --> 01:24:41,135
Speaker 3:  or likeness without permission. Okay. That's like a lot.

1390
01:24:41,315 --> 01:24:44,935
Speaker 3:  But again, SNL exists again. Like

1391
01:24:45,225 --> 01:24:46,735
Speaker 3:  Trump impersonators exist.

1392
01:24:48,895 --> 01:24:52,015
Speaker 3:  I sometimes do a British accent. I don't know if I sound like a British person

1393
01:24:52,025 --> 01:24:55,735
Speaker 3:  who's real. Like there's, there's, I I think we're probably in for a moment

1394
01:24:55,735 --> 01:24:59,495
Speaker 3:  where the pendulum swings way, way too far. And that might be

1395
01:24:59,605 --> 01:25:03,375
Speaker 3:  fine, but I, we just need to like be alert for it. 'cause that pendulum swing

1396
01:25:03,375 --> 01:25:07,295
Speaker 3:  too far might be appropriate given how damaging deep fakes can be, especially

1397
01:25:07,675 --> 01:25:11,575
Speaker 3:  for younger people, especially in these non-consensual intimate ways.

1398
01:25:12,555 --> 01:25:16,485
Speaker 3:  But it's also like, oh man, like you might go, you might just dial impersonation

1399
01:25:16,695 --> 01:25:20,525
Speaker 3:  along the way and we just like, dunno the answer yet. But there's a lot of

1400
01:25:20,525 --> 01:25:22,525
Speaker 3:  that action happening it seems to the company. So that's lightning round

1401
01:25:22,585 --> 01:25:26,085
Speaker 3:  one. Lightning round two sort of related to this last week,

1402
01:25:26,795 --> 01:25:29,845
Speaker 3:  last week, David, I'm just gonna call David after this. He titled the episode

1403
01:25:29,845 --> 01:25:31,285
Speaker 3:  the End of Google as we know it. And I was like, bro,

1404
01:25:33,185 --> 01:25:35,925
Speaker 3:  you just mean search generally. I was like, yeah, I'm going on vacation.

1405
01:25:35,985 --> 01:25:36,645
Speaker 3:  He changed it.

1406
01:25:40,145 --> 01:25:42,605
Speaker 3:  So we changed it. But we, we were, we're, we're basically at a moment and

1407
01:25:42,605 --> 01:25:45,845
Speaker 3:  that was the, the real thesis of last week's episode is that the entire like

1408
01:25:45,985 --> 01:25:49,005
Speaker 3:  web and search market is changing and we keyed that off of

1409
01:25:49,945 --> 01:25:53,805
Speaker 3:  Reddit getting Google to pay it for, to index the site and then

1410
01:25:53,965 --> 01:25:57,205
Speaker 3:  blocking everybody else. So there are some backlash to that, right? That's

1411
01:25:57,205 --> 01:26:00,965
Speaker 3:  the closing of the open web. So suddenly the only way to search

1412
01:26:00,965 --> 01:26:04,765
Speaker 3:  Reddit with a search engine is Google. Weird. That's a weird new outcome

1413
01:26:04,765 --> 01:26:08,605
Speaker 3:  that we didn't have before. Microsoft has, you know, been blocked. Perplexity

1414
01:26:08,605 --> 01:26:12,525
Speaker 3:  is blocked. Reddit, CEO, Steve Offman talked to our own Alex Heath this week.

1415
01:26:13,355 --> 01:26:14,685
Speaker 3:  He's like, everybody needs to pay me.

1416
01:26:16,235 --> 01:26:20,005
Speaker 3:  He's like bing, anthropic, perplexity. We've all been at after them.

1417
01:26:20,225 --> 01:26:23,605
Speaker 3:  And then the big quote is, it's a real pain in the ass to block these companies.

1418
01:26:23,785 --> 01:26:27,165
Speaker 3:  So he just wants money and this looks like the future, right? We just want

1419
01:26:27,165 --> 01:26:29,805
Speaker 3:  money. If you want to, if you wanna build an AI powered search engine at

1420
01:26:29,805 --> 01:26:33,725
Speaker 3:  crawler site. Similarly perplexity, which has gotten itself into

1421
01:26:33,765 --> 01:26:37,725
Speaker 3:  a lot of trouble, especially showing paywall the content without paying

1422
01:26:37,725 --> 01:26:41,685
Speaker 3:  anyone. They're just scraping sites and a lot of trouble about scraping

1423
01:26:41,685 --> 01:26:45,245
Speaker 3:  sites even though it's blocked, which they keep saying like, you don't know

1424
01:26:45,245 --> 01:26:49,165
Speaker 3:  how the web works. And I'm always like, tell me, tell me how you think

1425
01:26:49,165 --> 01:26:53,125
Speaker 3:  it works. And they won't say, but Perplexity has started cutting checks

1426
01:26:53,225 --> 01:26:54,005
Speaker 3:  to publishers

1427
01:26:55,835 --> 01:26:59,525
Speaker 3:  Time Dr. Spiegel, which big German publisher and Fortune are in the first

1428
01:26:59,525 --> 01:27:03,445
Speaker 3:  batch. I assume there will be more. I feel like I need to disclose, OpenAI

1429
01:27:03,475 --> 01:27:06,445
Speaker 3:  pays our company, Vox Media for its search, GPT product.

1430
01:27:07,375 --> 01:27:10,235
Speaker 3:  You can just put that on the list, right? OpenAI is paying the Atlantic and

1431
01:27:10,235 --> 01:27:13,795
Speaker 3:  a bunch of other publishers as well. New York Times is suing OpenAI because

1432
01:27:13,795 --> 01:27:17,755
Speaker 3:  it wants more money than it's presumably been offered. So like this thing

1433
01:27:18,375 --> 01:27:22,195
Speaker 3:  that's like, you've gotta pay search engines have to

1434
01:27:22,215 --> 01:27:26,115
Speaker 3:  pay to index our content on the open web is not just

1435
01:27:26,115 --> 01:27:30,035
Speaker 3:  like a blip. It's the thing that's happening. Like it is

1436
01:27:30,035 --> 01:27:32,875
Speaker 3:  the change that is happening across the industry. Everybody wants to get

1437
01:27:32,875 --> 01:27:35,635
Speaker 3:  paid, especially from the new breed of search engines. 'cause everybody made

1438
01:27:35,835 --> 01:27:38,435
Speaker 3:  the mistake with Google where they didn't ask for money and now Google's

1439
01:27:38,435 --> 01:27:40,755
Speaker 3:  like, we're doing AI anyway and no one can turn Google off.

1440
01:27:42,415 --> 01:27:45,325
Speaker 3:  Right next to that we, I mentioned circle to searches everywhere.

1441
01:27:46,395 --> 01:27:48,845
Speaker 3:  Desktop Chrome is getting something that looks a lot like circle to search.

1442
01:27:48,845 --> 01:27:52,645
Speaker 3:  So you see Google's building AI search ish experiences right into the

1443
01:27:52,645 --> 01:27:56,165
Speaker 3:  browser. And then Chrome is also getting an AI product

1444
01:27:56,165 --> 01:27:59,925
Speaker 3:  comparison mode where if you have a bunch of products open in tabs, Chrome

1445
01:27:59,925 --> 01:28:03,405
Speaker 3:  will just synthesize the tabs and build a comparison for you, which is legitimately

1446
01:28:03,405 --> 01:28:06,965
Speaker 3:  very cool. And at least you get to have to load the webpage. Yeah.

1447
01:28:07,205 --> 01:28:07,805
Speaker 3:  Which is like

1448
01:28:08,365 --> 01:28:09,605
Speaker 4:  A good step. We love that. Yeah.

1449
01:28:11,265 --> 01:28:14,725
Speaker 3:  But you see the, the web is just fully changing like

1450
01:28:15,245 --> 01:28:19,005
Speaker 3:  a, whatever you think AI is doing to productivity or workers

1451
01:28:19,145 --> 01:28:22,925
Speaker 3:  or whatever right now, at this moment, how we think

1452
01:28:22,925 --> 01:28:26,765
Speaker 3:  about the web and in particular search on the web, it's already,

1453
01:28:27,195 --> 01:28:30,805
Speaker 3:  it's the past is passed. Like it's gone. It's in the rear view mirror. We

1454
01:28:30,805 --> 01:28:34,125
Speaker 3:  might only be a little bit far away from it, but it's not coming back

1455
01:28:34,635 --> 01:28:38,285
Speaker 3:  like we are. We're just in a mode where you're gonna sign up to use a search

1456
01:28:38,285 --> 01:28:42,155
Speaker 3:  engine and maybe it will have a different set of sources than the next

1457
01:28:42,155 --> 01:28:45,515
Speaker 3:  search engine based on the deals they have made. Which is gonna be really

1458
01:28:45,515 --> 01:28:45,715
Speaker 3:  weird.

1459
01:28:46,185 --> 01:28:50,155
Speaker 4:  It's just strikes me every time, you know, Reddit comes

1460
01:28:50,155 --> 01:28:53,795
Speaker 4:  up, is that like we're racing to have, you know, Google

1461
01:28:53,845 --> 01:28:57,235
Speaker 4:  wants to answer your question without you clicking on anything you

1462
01:28:57,635 --> 01:29:01,445
Speaker 4:  chatbots wanna answer your question or like robots can answer your

1463
01:29:01,605 --> 01:29:05,325
Speaker 4:  question. But then also like Reddit is incredibly

1464
01:29:05,765 --> 01:29:09,485
Speaker 4:  valuable and it's just people talking to people. Like that's the thing that,

1465
01:29:09,985 --> 01:29:13,445
Speaker 4:  you know, we wanna get to in search now where I'm like,

1466
01:29:14,465 --> 01:29:17,445
Speaker 4:  you know, I have a question about my plant or whatever. It's like, I

1467
01:29:18,135 --> 01:29:21,285
Speaker 4:  don't want the AI generated answer. I don't want

1468
01:29:22,165 --> 01:29:26,045
Speaker 4:  whatever blurb, like I want to read an answer from a

1469
01:29:26,045 --> 01:29:29,965
Speaker 4:  person. And it's just strange to me that that's at the center of this. Like,

1470
01:29:30,515 --> 01:29:33,725
Speaker 4:  what if you could just talk to robots? It's like,

1471
01:29:34,905 --> 01:29:38,005
Speaker 4:  but the robots need to listen to a person who knows the answer.

1472
01:29:39,955 --> 01:29:43,605
Speaker 3:  Yeah. I mean, and Reddit is itself being polluted with AI generated text.

1473
01:29:43,785 --> 01:29:47,605
Speaker 3:  So that's the next turn. And then on top of it all the new information

1474
01:29:47,945 --> 01:29:51,405
Speaker 3:  is a bunch of kids talking to TikTok.

1475
01:29:52,185 --> 01:29:52,805
Speaker 4:  Oh yeah. And like,

1476
01:29:53,665 --> 01:29:56,965
Speaker 3:  And we found, we found out this week that TikTok is one of the biggest customers

1477
01:29:56,965 --> 01:30:00,605
Speaker 3:  of open ai because they're using all that for the recognition algorithms

1478
01:30:00,605 --> 01:30:04,045
Speaker 3:  and sorting and tr like, and you're like, oh, this is a weird little circle

1479
01:30:04,495 --> 01:30:07,845
Speaker 3:  where it's, there are gonna be more silos on the internet than ever before

1480
01:30:08,075 --> 01:30:12,045
Speaker 3:  because the open web did not make enough of the people

1481
01:30:12,045 --> 01:30:14,165
Speaker 3:  who make the original content enough money.

1482
01:30:16,125 --> 01:30:19,985
Speaker 3:  And so like no one wants to repeat that. So now it's like you gotta

1483
01:30:19,985 --> 01:30:23,905
Speaker 3:  pay us. And like, whether that actually comes back to individual redditors

1484
01:30:23,905 --> 01:30:27,305
Speaker 3:  who are contributing remains to be seen. But Reddit itself, Steve Huff's

1485
01:30:27,305 --> 01:30:29,705
Speaker 3:  like, yeah, I'm just gonna block you. Like I got my money from Google and

1486
01:30:29,705 --> 01:30:32,145
Speaker 3:  now everyone else is blocked unless they pay up. And I think You can do it

1487
01:30:32,985 --> 01:30:36,505
Speaker 3:  'cause he just needs Google. Like Google's gonna send Reddit a lot of traffic

1488
01:30:36,525 --> 01:30:39,785
Speaker 3:  and that's fine. You can block Bing and be like, pay up or you're dead.

1489
01:30:40,925 --> 01:30:44,025
Speaker 3:  And I think eventually they're all gonna end up paying because if you have

1490
01:30:44,025 --> 01:30:47,585
Speaker 3:  an AI search product that can't look at Reddit, I think a lot of people are

1491
01:30:47,585 --> 01:30:50,985
Speaker 3:  like, well where's the good stuff? And you're gonna, you're just gonna end

1492
01:30:50,985 --> 01:30:54,865
Speaker 3:  up in a weird loop. We've been covering this a lot. Like we over a

1493
01:30:54,865 --> 01:30:58,545
Speaker 3:  year now, we, we covered sort of the end of the SEO industry and what that

1494
01:30:58,545 --> 01:31:01,745
Speaker 3:  did to the web and all this stuff. 'cause we wanted to mark like, this is

1495
01:31:01,745 --> 01:31:05,625
Speaker 3:  what it looks like now. And I didn't, I honestly did

1496
01:31:05,625 --> 01:31:09,305
Speaker 3:  not expect it to feel as different as fast as it feels today.

1497
01:31:09,335 --> 01:31:13,145
Speaker 3:  Like the, the end of the search era that we knew is I think just

1498
01:31:13,245 --> 01:31:16,745
Speaker 3:  firmly here. And You can see even this week it's just like, here's six more

1499
01:31:16,945 --> 01:31:19,025
Speaker 3:  headlines that are just like, here's how it's changing.

1500
01:31:20,975 --> 01:31:24,775
Speaker 4:  I keep thinking about a friend who she was buying a new mattress

1501
01:31:25,275 --> 01:31:29,095
Speaker 4:  and she's like, I just asked Claude. Yeah, Claude did it for me, Claude.

1502
01:31:29,095 --> 01:31:32,535
Speaker 4:  I was like, yeah, which is the, oh, which one? It's not

1503
01:31:32,535 --> 01:31:35,615
Speaker 4:  Perplexity. It's Anthropic. It's Anthropic. Yeah. Oh

1504
01:31:35,755 --> 01:31:35,975
Speaker 2:  God.

1505
01:31:36,395 --> 01:31:40,095
Speaker 4:  And it was just like, my eyes get wider. I'm like, put the little

1506
01:31:40,535 --> 01:31:44,415
Speaker 4:  mattress review.com that it was scraping. You know, I don't know. That's

1507
01:31:44,415 --> 01:31:46,175
Speaker 4:  a whole minefield too. No,

1508
01:31:46,175 --> 01:31:49,975
Speaker 2:  My husband does it. My husband is like an avid chat GPT as

1509
01:31:50,055 --> 01:31:53,895
Speaker 2:  a search engine user before it even had this project. And

1510
01:31:53,895 --> 01:31:56,935
Speaker 2:  like he's been using Gemini and all the other stuff. And he'll be like, oh

1511
01:31:56,935 --> 01:32:00,040
Speaker 2:  yeah, I just asked that. This is how I built some code for this and this

1512
01:32:00,040 --> 01:32:03,445
Speaker 2:  is how I like, 'cause he just, Google just doesn't give him the answers that

1513
01:32:03,445 --> 01:32:07,005
Speaker 2:  he needs anymore. And so like, I've just been watching them just completely

1514
01:32:07,015 --> 01:32:10,685
Speaker 2:  shift how they use the internet over the last year. And I'm just doing the

1515
01:32:10,685 --> 01:32:14,285
Speaker 2:  same things that I have always done. And I just am like, hmm,

1516
01:32:14,755 --> 01:32:18,645
Speaker 2:  this, this, this feels like getting left behind. Maybe I should try

1517
01:32:18,645 --> 01:32:22,565
Speaker 2:  using it. Maybe I should try using it more in the ways that they

1518
01:32:22,565 --> 01:32:25,525
Speaker 2:  do. But I just run into the same problem as you do Alison, where I'm just

1519
01:32:25,525 --> 01:32:29,405
Speaker 2:  like, I just, I don't like the source that they pull from in these, in

1520
01:32:29,405 --> 01:32:32,685
Speaker 2:  these things. So I'm just like, oh, that's, I don't trust rando blog

1521
01:32:32,685 --> 01:32:35,205
Speaker 2:  site.com. Give me the other

1522
01:32:35,205 --> 01:32:38,805
Speaker 3:  Ones. Hey, I started ran blog site.com all by myself. 10 years all

1523
01:32:38,805 --> 01:32:40,325
Speaker 2:  Are ran. Blog site do com

1524
01:32:40,595 --> 01:32:41,765
Speaker 4:  Respect your elders.

1525
01:32:44,305 --> 01:32:45,845
Speaker 3:  Hey, that was my first website.

1526
01:32:47,725 --> 01:32:51,165
Speaker 3:  You know, I've been using chacha for Olympics trivia. 'cause I assume that

1527
01:32:51,705 --> 01:32:55,405
Speaker 3:  the, you know, like, it's like what's this cable on the fencer is like,

1528
01:32:55,515 --> 01:32:58,005
Speaker 3:  that answer's been the same for a long time, right? Yeah. And so you just

1529
01:32:58,005 --> 01:33:01,525
Speaker 3:  like, I have chat mapped the action button on my iPhone 15 pro

1530
01:33:01,945 --> 01:33:05,765
Speaker 3:  and it works like it just delivers an answer to you and you're like, I didn't

1531
01:33:05,765 --> 01:33:09,045
Speaker 3:  even know. That's right. Right. It's, it's like close enough. Yeah. and

1532
01:33:09,145 --> 01:33:10,325
Speaker 4:  It stakes are low.

1533
01:33:10,635 --> 01:33:13,525
Speaker 3:  Yeah. It's like, it's part of the scoring system. I hope that's true. You

1534
01:33:13,525 --> 01:33:13,885
Speaker 3:  know? Yeah.

1535
01:33:14,185 --> 01:33:17,165
Speaker 4:  You're not like judging the fencing like

1536
01:33:17,525 --> 01:33:17,925
Speaker 3:  Exactly.

1537
01:33:19,385 --> 01:33:20,805
Speaker 4:  The judges are not using that.

1538
01:33:21,805 --> 01:33:25,245
Speaker 3:  Actually we have a big feature we on this week about AI being used to judge

1539
01:33:25,245 --> 01:33:28,445
Speaker 3:  gymnastics. You should read that. It's very good. I mentioned that because

1540
01:33:28,445 --> 01:33:29,685
Speaker 3:  we are way over.

1541
01:33:56,685 --> 01:34:00,485
Speaker 7:  Vergecast this week. Hey, we'd love to hear from you. Give us a call at eight six six

1542
01:34:00,725 --> 01:34:04,045
Speaker 7:  VERGE one. One The. Vergecast is a production of The Verge and Vox Media

1543
01:34:04,045 --> 01:34:07,885
Speaker 7:  Podcast Network. Our show is produced by Andrew Marino and Liam James.

1544
01:34:08,225 --> 01:34:09,845
Speaker 7:  That's it. We'll see you next week.

