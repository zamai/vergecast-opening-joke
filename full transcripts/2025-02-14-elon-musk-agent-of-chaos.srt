1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 31fd9058-9660-4c8b-9ea3-bdb6e0a086fa
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-6150230017640525447/148791137480120837/s93290-US-6563s-1739749996.mp3
Description: On today's episode, once again, it's OpenAI and DOGE. And some other things! Nilay and David start the show by talking about Elon Musk's surprise bid to buy the nonprofit arm of OpenAI, along with the company's plans for new models and new rules for those models. After that, The Verge's Lauren Feiner joins to catch us up on what's happening with DOGE, how Musk and co. are making boring government information into something deeply fascinating and deeply confusing, and what it's like to work for the government now. Finally, in the lightning round, we talk about rumors of a new Apple Studio Display and iPhone SE, the new Powerbeats 2 Pro, Brendan Carr still being a dummy, and some surprising streaming moves from Apple and YouTube.


2
00:01:31,595 --> 00:01:35,265
Speaker 7:  Hello and welcome to the NCH Chest, the flagship podcast of government WordPress

3
00:01:35,545 --> 00:01:35,785
Speaker 7:  templates.

4
00:01:37,845 --> 00:01:40,905
Speaker 7:  The best kind, so transparent.

5
00:01:42,095 --> 00:01:44,345
Speaker 7:  It's really just a lot of PNGs, the government WordPress

6
00:01:44,545 --> 00:01:48,385
Speaker 8:  Templates. I've built a WordPress website in my life before and it is the,

7
00:01:48,385 --> 00:01:51,345
Speaker 8:  the thing where you just start with a bunch of like nonsensical business

8
00:01:51,345 --> 00:01:55,145
Speaker 8:  words and then you fill it with your own nonsensical business. Words is,

9
00:01:55,175 --> 00:01:56,025
Speaker 8:  it's a, it's a time.

10
00:01:56,165 --> 00:01:59,745
Speaker 7:  The thing you're missing, David, is now when you do that

11
00:02:00,205 --> 00:02:03,425
Speaker 7:  and AI will write the nonsensical business words,

12
00:02:04,725 --> 00:02:07,825
Speaker 7:  I'm not even kidding. Like this is the product that most of the website builders

13
00:02:07,825 --> 00:02:10,425
Speaker 7:  are now saying. You just like show up and you're like, I'm a yoga studio.

14
00:02:10,485 --> 00:02:13,465
Speaker 7:  And they're like, here's a website. It might describe your business or it

15
00:02:13,465 --> 00:02:17,325
Speaker 7:  might not, but at least it's here. And then you're supposed to run your

16
00:02:17,325 --> 00:02:20,645
Speaker 7:  scheduling. Hi, I'm your friend Neil. I, David Pierce is here. Hello. We're

17
00:02:20,645 --> 00:02:20,925
Speaker 7:  here to build

18
00:02:20,925 --> 00:02:23,045
Speaker 8:  Your WordPress websites. That's what we do. Now,

19
00:02:23,725 --> 00:02:26,565
Speaker 7:  Honestly, it might be a more sustainable business in journalism. Like where,

20
00:02:27,695 --> 00:02:31,085
Speaker 7:  where are the highest chances of going to jail in America right now?

21
00:02:31,555 --> 00:02:35,365
Speaker 7:  Like used to be zero, being the editor in chief of The Verge was a 0%

22
00:02:35,365 --> 00:02:39,255
Speaker 7:  chance of going to jail. Me personally, just like doing the

23
00:02:39,255 --> 00:02:42,895
Speaker 7:  stuff I do, slightly higher than Zero, but, you know, regular

24
00:02:42,895 --> 00:02:46,855
Speaker 7:  editor of tech website, zero 0% I think Chance of Gonna Jail. Yeah.

25
00:02:48,805 --> 00:02:52,705
Speaker 7:  Operating a WordPress template business 0%. The

26
00:02:52,705 --> 00:02:56,345
Speaker 7:  thing that's really, I think, interesting now is that it's

27
00:02:56,345 --> 00:02:59,565
Speaker 7:  non-zero. It's like being the, like you, like, oh,

28
00:03:00,255 --> 00:03:03,365
Speaker 7:  maybe Brendan Carl will arrest me and I hope you do Brendan, if you're listen,

29
00:03:03,425 --> 00:03:06,755
Speaker 7:  and I know you do. I know you got those Google alerts. Boy,

30
00:03:07,815 --> 00:03:11,755
Speaker 8:  No, we, the thing is neli for for good engagement purposes, we save Brendan

31
00:03:11,755 --> 00:03:14,595
Speaker 8:  till the end so that Brendan has to listen to the whole podcast to get to

32
00:03:14,595 --> 00:03:16,275
Speaker 8:  the part that's about Brendan. That's true.

33
00:03:16,405 --> 00:03:18,915
Speaker 7:  We're gonna put his name in the headline, but you have to listen to the whole

34
00:03:18,915 --> 00:03:19,035
Speaker 7:  thing.

35
00:03:19,035 --> 00:03:19,755
Speaker 8:  Exactly right.

36
00:03:20,815 --> 00:03:24,295
Speaker 7:  Anyhow, there's a lot going on this week. There's some politics. We'll get

37
00:03:24,295 --> 00:03:28,175
Speaker 7:  to that. There's a weird situation going on with open ai. There's some

38
00:03:28,235 --> 00:03:32,175
Speaker 7:  gadget news. There's a good old fashioned programming dispute

39
00:03:32,175 --> 00:03:35,455
Speaker 7:  between CBS and YouTube tv old school.

40
00:03:36,205 --> 00:03:36,735
Speaker 8:  Yeah. Man,

41
00:03:37,505 --> 00:03:40,505
Speaker 7:  Buzzfeed is starting a social network to combat something called snf.

42
00:03:41,905 --> 00:03:45,705
Speaker 7:  I need to complain about Jeep as a company. I'm very excited

43
00:03:45,705 --> 00:03:45,985
Speaker 7:  to do

44
00:03:45,985 --> 00:03:49,745
Speaker 8:  That. I feel like everything you just described could have been the introduction

45
00:03:49,745 --> 00:03:53,505
Speaker 8:  to like an End Gadget show from 2007. Like has

46
00:03:53,745 --> 00:03:54,225
Speaker 8:  anything changed?

47
00:03:54,845 --> 00:03:58,665
Speaker 7:  Has changed? No. So except slightly

48
00:03:58,665 --> 00:04:00,425
Speaker 7:  higher chance of going to jail for doing a show

49
00:04:00,815 --> 00:04:04,625
Speaker 8:  Fair. Yeah. You never know. What's the, so the, the Go 90 scale of Doom

50
00:04:04,625 --> 00:04:08,105
Speaker 8:  streaming services is, is good. Is it like the Theranos scale of

51
00:04:08,275 --> 00:04:08,945
Speaker 8:  going to jail

52
00:04:10,485 --> 00:04:12,385
Speaker 8:  if you're, if you're at a 90 year Theranos?

53
00:04:12,775 --> 00:04:16,665
Speaker 7:  Yeah. I mean, she gave a interview to like People Magazine this week.

54
00:04:16,665 --> 00:04:19,305
Speaker 7:  Elizabeth Holmes gave an interview at People Magazine this week where she

55
00:04:19,305 --> 00:04:22,265
Speaker 7:  was like, you know, it wasn't fraud, it was just a failure. And I was like,

56
00:04:22,265 --> 00:04:26,105
Speaker 7:  no, Elizabeth, you are giving this interview from jail. Yeah,

57
00:04:26,205 --> 00:04:29,825
Speaker 7:  because it was fraud. I don't know what to tell you. Like you can't argue

58
00:04:29,825 --> 00:04:32,265
Speaker 7:  that anymore when you're on the phone in the

59
00:04:34,545 --> 00:04:37,525
Speaker 7:  jail Anyhow, but I, yeah, it's the, the, the Theranos scale of whether or

60
00:04:37,525 --> 00:04:40,365
Speaker 7:  not Neil AI goes to jail. The Bren, the Brennan car units,

61
00:04:40,805 --> 00:04:42,725
Speaker 8:  Carville, oh, car units. That's good. Okay.

62
00:04:43,525 --> 00:04:47,485
Speaker 7:  Scovilles. No, that's Peppers. It's been a long week, everybody.

63
00:04:47,485 --> 00:04:49,565
Speaker 7:  Yeah, we're gonna get through it. Okay. Let's start with open ai. There's

64
00:04:49,565 --> 00:04:53,125
Speaker 7:  some weird stuff going on with OpenAI. Of course it involves Elon Musk because

65
00:04:53,125 --> 00:04:56,165
Speaker 7:  what news story does not involve Elon Musk in this

66
00:04:56,645 --> 00:05:00,525
Speaker 7:  2025, but this one is just pettier all the

67
00:05:00,525 --> 00:05:04,005
Speaker 7:  way around. So OpenAI is, I think we all understand it

68
00:05:04,195 --> 00:05:07,725
Speaker 7:  started as this nonprofit. Elon was a co-founder,

69
00:05:10,205 --> 00:05:13,135
Speaker 7:  many things. He's no longer a co-founder. He is obviously beefing with the

70
00:05:13,135 --> 00:05:17,095
Speaker 7:  company. He appears to be a little bit jealous of Sam Altman. Sam

71
00:05:17,155 --> 00:05:20,895
Speaker 7:  got fired, he came back inside of a week. Now he wants to convert opening

72
00:05:20,955 --> 00:05:24,175
Speaker 7:  AI from a nonprofit to a for-profit company, which is a very complicated

73
00:05:24,175 --> 00:05:28,055
Speaker 7:  legal maneuver. And he's got basically Excel, the

74
00:05:28,055 --> 00:05:31,615
Speaker 7:  nonprofit. There's a lot there. And so Elon Musk,

75
00:05:32,075 --> 00:05:35,775
Speaker 7:  in order to just screw with this process, has put together an offer to buy

76
00:05:35,915 --> 00:05:38,935
Speaker 7:  OpenAI for $97.4 billion.

77
00:05:39,775 --> 00:05:40,065
Speaker 8:  Yeah.

78
00:05:40,485 --> 00:05:41,345
Speaker 7:  Do you think that's real?

79
00:05:43,855 --> 00:05:47,255
Speaker 8:  I mean, no, but also,

80
00:05:48,195 --> 00:05:49,575
Speaker 8:  yes. Right. Like, I think a,

81
00:05:51,545 --> 00:05:55,435
Speaker 8:  that would be a terrific deal for Elon Musk if he could

82
00:05:55,435 --> 00:05:59,165
Speaker 8:  get that deal right? It, it, it, it both wins you a battle

83
00:05:59,165 --> 00:06:02,805
Speaker 8:  with one of your enemies and gets you like an actually hugely

84
00:06:03,035 --> 00:06:06,485
Speaker 8:  important AI thing in, in the world.

85
00:06:06,905 --> 00:06:10,485
Speaker 8:  So I think in that sense, do I think he could gin up the money and do I think

86
00:06:10,485 --> 00:06:13,885
Speaker 8:  he would do it if Sam Altman had said, yes, I'll sell. Yes.

87
00:06:14,605 --> 00:06:17,685
Speaker 8:  I think Elon Musk's ability to raise that much money, I think is pretty much

88
00:06:17,685 --> 00:06:21,445
Speaker 8:  beyond question at this point. X AI has a huge

89
00:06:21,445 --> 00:06:25,125
Speaker 8:  amount of money and a huge amount of support and is is like suddenly flush

90
00:06:25,195 --> 00:06:28,965
Speaker 8:  with everything it could possibly need. So yeah, I think if, if

91
00:06:28,965 --> 00:06:32,805
Speaker 8:  everybody was in on this deal, they'd do it. I also think Elon Musk

92
00:06:32,825 --> 00:06:35,605
Speaker 8:  and this group made this bid,

93
00:06:36,705 --> 00:06:40,635
Speaker 8:  obviously knowing it wasn't gonna work. Like there's just no world

94
00:06:40,695 --> 00:06:44,595
Speaker 8:  in which Sam Altman says yes to this deal, and it goes through,

95
00:06:45,865 --> 00:06:49,355
Speaker 8:  it's not a public company, so you can't even like pressure shareholders and

96
00:06:49,355 --> 00:06:51,395
Speaker 8:  go that way to get it done the way he tried to do it Twitter,

97
00:06:53,225 --> 00:06:56,795
Speaker 8:  there's just no way this was ever gonna happen. So I think that a deal like

98
00:06:56,795 --> 00:07:00,675
Speaker 8:  this can be both real and not real, and I think it very much was.

99
00:07:01,065 --> 00:07:04,395
Speaker 7:  Well, so there are not public shareholders, but he does have

100
00:07:04,875 --> 00:07:08,795
Speaker 7:  co-investors and sure, Microsoft has a big stake in OpenAI

101
00:07:08,795 --> 00:07:12,715
Speaker 7:  that for-profit company, not the non-profit. It's all

102
00:07:12,715 --> 00:07:13,355
Speaker 7:  very complicated,

103
00:07:13,485 --> 00:07:17,435
Speaker 8:  Right? And all those investors have a lot of reasons to want to make it easier

104
00:07:17,655 --> 00:07:20,755
Speaker 8:  for OpenAI to convert to being a for-profit company because they all want

105
00:07:20,755 --> 00:07:24,075
Speaker 8:  to be able to get money in and out of that company. So in that sense, you,

106
00:07:24,075 --> 00:07:27,475
Speaker 8:  you could argue there are maybe people who would have a reason to say yes

107
00:07:27,475 --> 00:07:28,675
Speaker 8:  to this maybe.

108
00:07:28,975 --> 00:07:32,755
Speaker 7:  And certainly because Elon has promised to

109
00:07:32,755 --> 00:07:36,235
Speaker 7:  match or exceed any other offer they get. The board of

110
00:07:36,335 --> 00:07:40,235
Speaker 7:  OpenAI has some amount of fiduciary duty to, to go get

111
00:07:40,235 --> 00:07:43,955
Speaker 7:  the best deal, the biggest amount of money. But Sam Altman has pretty much

112
00:07:43,955 --> 00:07:47,915
Speaker 7:  said no. And then here's the weird thing. He told open AI's

113
00:07:48,475 --> 00:07:52,195
Speaker 7:  employees that they have yet to receive the formal offer from Elon

114
00:07:52,195 --> 00:07:55,835
Speaker 7:  Musk. Right. Which is very funny. Like these guys are

115
00:07:55,835 --> 00:07:59,195
Speaker 7:  literally tweeting at each other all day long. Like he knows people

116
00:07:59,255 --> 00:08:02,315
Speaker 8:  Can just say things like, again, I I just wanna keep reminding you. People

117
00:08:02,335 --> 00:08:06,155
Speaker 8:  can just say things and I think Elon Musk is pretty good at

118
00:08:06,555 --> 00:08:10,515
Speaker 8:  saying things louder and with more paperwork than most,

119
00:08:10,575 --> 00:08:12,435
Speaker 8:  but in many ways he's still just saying things.

120
00:08:12,775 --> 00:08:16,435
Speaker 7:  So then here's another added layer of complexity to that. Elon is currently

121
00:08:16,485 --> 00:08:20,395
Speaker 7:  suing OpenAI, right? Because he's mad at them and he thinks that he should

122
00:08:20,395 --> 00:08:24,275
Speaker 7:  get paid because he was around. And I literally, I mean that

123
00:08:24,275 --> 00:08:27,715
Speaker 7:  literally in that he never had a contract. And in classic Elon fashion,

124
00:08:28,425 --> 00:08:32,035
Speaker 7:  he's suing OpenAI and saying a bunch of stuff that he said in text

125
00:08:32,035 --> 00:08:35,875
Speaker 7:  messages constitutes a contract, right? Elon gets in

126
00:08:35,875 --> 00:08:39,835
Speaker 7:  trouble suing over fake contracts a lot. Actually, part of the

127
00:08:39,835 --> 00:08:43,715
Speaker 7:  Twitter litigation was like some fake contracts. The OpenAI case

128
00:08:43,715 --> 00:08:47,235
Speaker 7:  is like very much, there's no real contract here. So he

129
00:08:47,575 --> 00:08:50,875
Speaker 7:  is in the middle of this lawsuit with, he's almost certainly gonna lose because

130
00:08:50,875 --> 00:08:54,795
Speaker 7:  there's no actual contract. You can't sue for breach of contract without

131
00:08:54,795 --> 00:08:58,125
Speaker 7:  a contract. It's very hard to do, especially when you're a quote like sophisticated

132
00:08:58,125 --> 00:09:00,445
Speaker 7:  actor. Like courts are like, no, you're the richest man in the world. Get

133
00:09:00,445 --> 00:09:01,245
Speaker 7:  a, get a contract.

134
00:09:01,355 --> 00:09:02,565
Speaker 8:  Yeah, you should know how to do this.

135
00:09:02,705 --> 00:09:06,485
Speaker 7:  So that, I don't know how that one's gonna go, but all signs say it should

136
00:09:06,485 --> 00:09:09,565
Speaker 7:  go poorly for Elon. So part of the reason I think he's making this offer

137
00:09:09,585 --> 00:09:13,565
Speaker 7:  is to screw with Sam Altman, screw with OpenAI, but also put some pressure

138
00:09:13,565 --> 00:09:17,445
Speaker 7:  on this other case. It's pretty hard though to buy a company that you are

139
00:09:17,445 --> 00:09:20,805
Speaker 7:  suing in this way without settling the lawsuit. So

140
00:09:21,325 --> 00:09:25,125
Speaker 7:  Altman is saying, I haven't even received this offer. And sort of the

141
00:09:25,405 --> 00:09:27,885
Speaker 7:  response from the Elon side is, of course you haven't. It's because we're

142
00:09:27,885 --> 00:09:31,805
Speaker 7:  suing you and so we're, we, we are not supposed to talk to you.

143
00:09:32,665 --> 00:09:35,235
Speaker 7:  This is all very funny because they, these people are constantly tweeting

144
00:09:35,235 --> 00:09:39,075
Speaker 7:  each other, like they're, they're always talking to each other.

145
00:09:39,355 --> 00:09:39,475
Speaker 8:  Right.

146
00:09:40,255 --> 00:09:44,155
Speaker 7:  But like, that's the formal response. And so here Mark Tober

147
00:09:44,155 --> 00:09:46,995
Speaker 7:  off, who is Elon's lawyer, sent us the following statement,

148
00:09:47,765 --> 00:09:51,545
Speaker 7:  clarifying the timeline. It's a good statement

149
00:09:51,625 --> 00:09:55,425
Speaker 7:  in that it is very formal and it definitely sounds like a wizard

150
00:09:55,475 --> 00:09:59,425
Speaker 7:  doing an incantation on behalf of a Musk led investment

151
00:09:59,425 --> 00:10:02,705
Speaker 7:  consortium on Monday, February

152
00:10:03,365 --> 00:10:06,585
Speaker 7:  10th, 2025 at 12:27 PM Pacific Time, David. Okay,

153
00:10:06,775 --> 00:10:07,065
Speaker 8:  Sure.

154
00:10:07,415 --> 00:10:09,025
Speaker 7:  It's very important to the minute 3 27

155
00:10:09,135 --> 00:10:10,545
Speaker 8:  Eastern. If you're counting,

156
00:10:10,865 --> 00:10:14,745
Speaker 7:  I summoned the clouds. I emailed the OpenAI bid to their

157
00:10:14,745 --> 00:10:18,665
Speaker 7:  outside counsel, William and Sarah, that's,

158
00:10:18,665 --> 00:10:22,225
Speaker 7:  those are names I, the full names are here for transmission to their client.

159
00:10:23,155 --> 00:10:26,975
Speaker 7:  The bid attached to my email was in the form of a detailed four

160
00:10:26,975 --> 00:10:30,815
Speaker 7:  page letter of intent signed by Elon Musk and each of the other

161
00:10:30,815 --> 00:10:34,535
Speaker 7:  investors addressed to the charity's board of directors to purchase

162
00:10:34,715 --> 00:10:38,695
Speaker 7:  OpenAI inks and assets for 97.375

163
00:10:38,965 --> 00:10:42,695
Speaker 7:  billion. Whether Sam Altman chose to provide or withhold this from

164
00:10:42,845 --> 00:10:46,375
Speaker 7:  open AI's other board members is outside of our control. Now. It's

165
00:10:46,715 --> 00:10:50,505
Speaker 7:  like, you know, the rest of the board members know because Elon is

166
00:10:50,505 --> 00:10:50,945
Speaker 7:  tweeting about it,

167
00:10:51,275 --> 00:10:53,905
Speaker 8:  Right? It was, it was in the Wall Street Journal.

168
00:10:54,215 --> 00:10:57,625
Speaker 7:  Like the, the formalities of this are like deeply hilarious. Like you think

169
00:10:57,625 --> 00:11:01,025
Speaker 7:  Brett Taylor, who's on the board of OpenAI, who was the chairman of the board

170
00:11:01,025 --> 00:11:04,905
Speaker 7:  of Twitter when Elon bought Twitter, wasn't like, Sam, can I see the letter?

171
00:11:06,895 --> 00:11:10,785
Speaker 8:  Yeah. I mean I think everyone involved in this kind of understands how these

172
00:11:10,785 --> 00:11:12,025
Speaker 8:  things go at this point.

173
00:11:13,685 --> 00:11:17,585
Speaker 8:  Is it possible that there's just a gambit here

174
00:11:17,605 --> 00:11:21,385
Speaker 8:  for Elon to get some kind of, I don't know, check and

175
00:11:22,135 --> 00:11:26,105
Speaker 8:  fawning post from Sam about his role in open AI and then everyone moves

176
00:11:26,105 --> 00:11:28,745
Speaker 8:  on with their lives. Like, I just keep thinking about all of these

177
00:11:30,595 --> 00:11:34,345
Speaker 8:  Trump lawsuits that are clearly just gambits to get a check.

178
00:11:34,655 --> 00:11:37,265
Speaker 8:  Like, is that, is that just what this is? Elon is like, you can make all

179
00:11:37,265 --> 00:11:41,105
Speaker 8:  of this go away. Just write me a check and a loving blog post on your

180
00:11:41,105 --> 00:11:44,505
Speaker 8:  website about how terrific I am and how open eye wouldn't exist without me.

181
00:11:45,205 --> 00:11:46,215
Speaker 8:  Like is that all this is

182
00:11:46,795 --> 00:11:50,645
Speaker 7:  No. 'cause they're competitors now. They're certainly competitors for like

183
00:11:50,645 --> 00:11:54,385
Speaker 7:  Trump's affection. Like Sam got the Donald

184
00:11:54,385 --> 00:11:57,945
Speaker 7:  Trump stands next to me. Well, I say nonsensical things about my $500 billion

185
00:11:57,945 --> 00:12:01,795
Speaker 7:  data center plan. Sure. Before Elon did, you

186
00:12:01,795 --> 00:12:04,835
Speaker 7:  know? Yeah. Like it's that level of petty between these two guys. Oh yeah.

187
00:12:04,835 --> 00:12:08,275
Speaker 7:  To the point where Elon said the offer and then

188
00:12:08,615 --> 00:12:12,435
Speaker 7:  Sam tweeted, no thank you, but we will buy Twitter for $9.74 billion

189
00:12:12,725 --> 00:12:16,115
Speaker 7:  If you want. Which is very good. Which is good burn. Also, again, evidence

190
00:12:16,115 --> 00:12:19,665
Speaker 7:  that they are talking to each other. And then Sam

191
00:12:19,665 --> 00:12:23,545
Speaker 7:  Altman was on Bloomberg television and they asked him about

192
00:12:23,545 --> 00:12:26,505
Speaker 7:  the deal or the proposed deal and he said, Elon tries all sorts of things.

193
00:12:26,575 --> 00:12:30,345
Speaker 7:  This is just this week's episode. And then quote, probably his whole

194
00:12:30,345 --> 00:12:34,225
Speaker 7:  life is a, from a position of insecurity, I feel for the

195
00:12:34,225 --> 00:12:37,345
Speaker 7:  guy, I don't think he's a happy person. I feel for him.

196
00:12:38,725 --> 00:12:39,835
Speaker 7:  Which is just a lot.

197
00:12:40,175 --> 00:12:42,715
Speaker 8:  That's brutal. I think, I mean a

198
00:12:44,175 --> 00:12:48,035
Speaker 8:  Sam Altman is in a position to know of what he speaks in

199
00:12:48,265 --> 00:12:52,115
Speaker 8:  that particular way, but also that is, that is

200
00:12:52,335 --> 00:12:56,115
Speaker 8:  brutal. The, like these two men are, are as, are as beefing as

201
00:12:56,115 --> 00:12:59,955
Speaker 8:  anybody in the tech world right now. Yeah. In a really fascinating way.

202
00:12:59,955 --> 00:13:03,755
Speaker 8:  And I think I, I, again, this all this is all very

203
00:13:03,755 --> 00:13:07,115
Speaker 8:  real and fake and ridiculous at the same time. And I, I, I have

204
00:13:08,105 --> 00:13:11,755
Speaker 8:  continual trouble to understand what to make of deals

205
00:13:11,825 --> 00:13:15,675
Speaker 8:  like this and spats like this between Elon Musk

206
00:13:15,675 --> 00:13:18,955
Speaker 8:  and anybody because he has the resources to make this stuff

207
00:13:19,645 --> 00:13:21,675
Speaker 8:  stick even when it's not real. Yeah.

208
00:13:21,675 --> 00:13:24,275
Speaker 7:  He'll run it to ground no matter what. He's a troll. I mean, we, we know

209
00:13:24,275 --> 00:13:28,155
Speaker 7:  this much. I think even If you love Elon Musk, whether or not he's a troll

210
00:13:28,215 --> 00:13:31,955
Speaker 7:  is well settled. Yeah. You might love it or you might hate it, but is he

211
00:13:31,955 --> 00:13:35,675
Speaker 7:  a troll? Yes. He's a troll and he is a well-resourced troll can just keep

212
00:13:35,675 --> 00:13:39,595
Speaker 7:  this lawsuit going for as long as he wants to. Right. I think what all

213
00:13:39,595 --> 00:13:43,435
Speaker 7:  this is evidence of is that OpenAI is in an extraordinarily weak

214
00:13:43,835 --> 00:13:47,435
Speaker 7:  position overall. Right. And they, what we'll talk about their product plans.

215
00:13:47,635 --> 00:13:49,795
Speaker 7:  'cause they laid out some product plans, they're gonna clean up their models.

216
00:13:49,795 --> 00:13:53,035
Speaker 7:  They're, they're thinking about expanding different ways. But you look at

217
00:13:53,035 --> 00:13:56,715
Speaker 7:  this company, which a few years ago kicked off this modern generative

218
00:13:56,855 --> 00:14:00,755
Speaker 7:  AI boom Yeah. Sort of by accident and their financials aren't there.

219
00:14:01,045 --> 00:14:04,995
Speaker 7:  Right. They, they're losing money on every customer that

220
00:14:05,155 --> 00:14:08,915
Speaker 7:  they have. Basically their models aren't

221
00:14:09,495 --> 00:14:12,355
Speaker 7:  the cutting edge models like they are, but they aren't like, there's lots

222
00:14:12,355 --> 00:14:15,955
Speaker 7:  of competition now and the models are specializing. Being the model

223
00:14:16,055 --> 00:14:19,675
Speaker 7:  vendor is not like the great business. It's the

224
00:14:19,675 --> 00:14:23,355
Speaker 7:  product that uses the model, but might be the great business. And even that's

225
00:14:23,355 --> 00:14:26,675
Speaker 7:  unclear. And then they're training their next models, which is very expensive

226
00:14:26,675 --> 00:14:30,595
Speaker 7:  and they're trying to raise all this money while the whole AI

227
00:14:30,595 --> 00:14:33,115
Speaker 7:  field feels like it's commoditizing.

228
00:14:34,305 --> 00:14:37,815
Speaker 7:  Right. Not all the way. And certainly they still have some opportunity,

229
00:14:38,475 --> 00:14:42,215
Speaker 7:  but you know, i I, the products that are

230
00:14:42,335 --> 00:14:45,375
Speaker 7:  being built using the generative AI models in particular,

231
00:14:46,085 --> 00:14:49,015
Speaker 7:  they, it, you're, they're differentiating on the front end of like, what

232
00:14:49,015 --> 00:14:52,025
Speaker 7:  is this product not on the back end of how capable is this model

233
00:14:52,885 --> 00:14:55,705
Speaker 7:  and open a has but one great product, which is

234
00:14:56,045 --> 00:14:59,345
Speaker 8:  Tt Right. And I think even to the extent that they're not

235
00:14:59,975 --> 00:15:03,865
Speaker 8:  commoditize, like I think they're, they're commoditizing like

236
00:15:04,095 --> 00:15:07,945
Speaker 8:  parentheses complimentary, you know what I mean? Like they're commoditizing

237
00:15:07,945 --> 00:15:10,625
Speaker 8:  in the way that like cloud services are commoditizing in the way that there

238
00:15:10,625 --> 00:15:14,065
Speaker 8:  is a ton of money in it. They are massive earth

239
00:15:14,065 --> 00:15:17,825
Speaker 8:  shatteringly profitable businesses for the cloud companies, but there

240
00:15:17,845 --> 00:15:21,665
Speaker 8:  is nothing remotely sexy about it. And there

241
00:15:21,665 --> 00:15:25,585
Speaker 8:  really aren't super compelling moats. And so we were, we were in this

242
00:15:25,585 --> 00:15:29,105
Speaker 8:  like cloud services land grab for a while because like switching is kind

243
00:15:29,105 --> 00:15:32,865
Speaker 8:  of annoying. So everybody was just like, how do we get more customers? That's

244
00:15:32,865 --> 00:15:36,825
Speaker 8:  where we are with a lot of this AI stuff. Like we, we got a lot of feedback

245
00:15:36,895 --> 00:15:39,625
Speaker 8:  from the episodes that we've done about like, there's no real utility for

246
00:15:39,625 --> 00:15:42,945
Speaker 8:  this. People like, it's just infrastructure. And it's like Sure. A infrastructure

247
00:15:43,045 --> 00:15:46,985
Speaker 8:  for what? And b, infrastructure is not

248
00:15:47,625 --> 00:15:50,065
Speaker 8:  a product. Right? Yeah. And so I think what your point is, is like there's

249
00:15:50,065 --> 00:15:52,825
Speaker 8:  a, there's a world at the end of this where there is a lot of money in this

250
00:15:53,485 --> 00:15:57,225
Speaker 8:  for some large handful of companies. OpenAI

251
00:15:57,545 --> 00:16:01,225
Speaker 8:  probably being one of them. But that is different

252
00:16:01,255 --> 00:16:05,145
Speaker 8:  from being the kind of like face of the tech

253
00:16:05,425 --> 00:16:09,385
Speaker 8:  industry business that OpenAI thinks it can be. You eventually just

254
00:16:09,385 --> 00:16:11,705
Speaker 8:  be, you become a SaaS provider, right? Like you're a, you're a data center

255
00:16:11,705 --> 00:16:14,905
Speaker 8:  company at the end of this. If you're a company like OpenAI and

256
00:16:15,495 --> 00:16:19,145
Speaker 8:  Anthropic is doing very well. And I think there, there's this really great

257
00:16:19,775 --> 00:16:23,625
Speaker 8:  like leaderboard where you can see basically whose tokens are

258
00:16:23,705 --> 00:16:27,105
Speaker 8:  being used for what. And you look at some of the practical stuff, like for

259
00:16:27,105 --> 00:16:31,065
Speaker 8:  coding, people are flocking to Claude and Anthropic in a, in a way

260
00:16:31,065 --> 00:16:34,945
Speaker 8:  that is really dangerous if you're open ai. And so, like, I don't think

261
00:16:34,945 --> 00:16:38,785
Speaker 8:  open AI is like at risk of going away. It will eventually, I think figure

262
00:16:38,805 --> 00:16:41,625
Speaker 8:  out how to lose less money on every one of these and that'll be good. And

263
00:16:41,625 --> 00:16:44,825
Speaker 8:  they'll raise enough money that it won't matter for a long time anyway. But

264
00:16:44,825 --> 00:16:48,715
Speaker 8:  like this thing that they built with chat GPT where it

265
00:16:48,715 --> 00:16:52,635
Speaker 8:  was like Sam Altman is the face of AI in a, in a real way

266
00:16:52,655 --> 00:16:56,195
Speaker 8:  and has been for years. And that makes you incredibly powerful and it lets

267
00:16:56,195 --> 00:16:59,875
Speaker 8:  you stand next to Trump and announce a $500 billion

268
00:16:59,875 --> 00:17:03,515
Speaker 8:  joint venture to build infrastructure. That's the

269
00:17:04,265 --> 00:17:07,715
Speaker 8:  thing that I think this company risks losing is

270
00:17:08,215 --> 00:17:11,235
Speaker 8:  like if it's, if it's not sexy, it's just another SaaS company.

271
00:17:11,495 --> 00:17:15,275
Speaker 7:  Let me make an even more bearish case though. Okay. Because I think the risk

272
00:17:15,335 --> 00:17:16,835
Speaker 7:  is open act collapses.

273
00:17:18,425 --> 00:17:20,925
Speaker 7:  I'm not saying it's a hundred percent risk. I'm not even making the prediction.

274
00:17:20,985 --> 00:17:22,485
Speaker 7:  I'm saying that's the level of risk.

275
00:17:22,635 --> 00:17:23,965
Speaker 8:  There's just too much money in it.

276
00:17:24,655 --> 00:17:27,865
Speaker 7:  Yeah, but I mean that's, that was true of Enron. You know, like whatever,

277
00:17:28,575 --> 00:17:32,345
Speaker 7:  like things collapse. Like sure. If If you don't have a real business

278
00:17:32,345 --> 00:17:36,185
Speaker 7:  underneath it all, it, there's, that's the risk. I'm not, again,

279
00:17:36,185 --> 00:17:40,065
Speaker 7:  I'm not making the prediction. I think Sam Altman is very good at

280
00:17:40,065 --> 00:17:43,025
Speaker 7:  raising money and he can keep, you know, filling,

281
00:17:44,075 --> 00:17:47,845
Speaker 7:  filling the tub faster than it drains, whatever, whatever metaphor works

282
00:17:47,845 --> 00:17:50,805
Speaker 7:  there. But, but you've got this

283
00:17:51,985 --> 00:17:55,555
Speaker 7:  Byzantine corporate structure that doesn't accept investment

284
00:17:55,705 --> 00:17:58,755
Speaker 7:  very easily. It's under all of this pressure

285
00:17:59,405 --> 00:18:03,035
Speaker 7:  right now from Elon, now from other, like the

286
00:18:03,035 --> 00:18:05,915
Speaker 7:  government, although who knows how our government will react to any of this

287
00:18:05,915 --> 00:18:09,115
Speaker 7:  at this point in time. But there's regulatory hurdles to,

288
00:18:09,895 --> 00:18:12,955
Speaker 7:  to moving it out of one structure in another. The state of California has

289
00:18:12,955 --> 00:18:16,875
Speaker 7:  something to say about it. He's raising all this additional money to

290
00:18:16,875 --> 00:18:20,835
Speaker 7:  do this data center project. You gotta get the money back

291
00:18:20,835 --> 00:18:21,235
Speaker 7:  outta that.

292
00:18:21,375 --> 00:18:22,075
Speaker 8:  Yes. At some point

293
00:18:22,095 --> 00:18:26,035
Speaker 7:  You need to have a plan some somewhere. Right? There's there's only so

294
00:18:26,035 --> 00:18:29,195
Speaker 7:  many soft banks that are like, screw it, we'll do another WeWork. You know,

295
00:18:29,195 --> 00:18:32,315
Speaker 7:  like, and he found one and it was SoftBank again,

296
00:18:32,425 --> 00:18:33,195
Speaker 8:  It's just the one Yeah.

297
00:18:34,565 --> 00:18:38,435
Speaker 7:  Right. Like that's the problem is

298
00:18:39,145 --> 00:18:42,995
Speaker 7:  he's now put himself in a position where OpenAI has to build

299
00:18:43,355 --> 00:18:47,115
Speaker 7:  a business or develop a product that is bigger

300
00:18:47,115 --> 00:18:50,835
Speaker 7:  than the iPhone. Faster than the iPhone. Yes. Maybe you think that's Chatt

301
00:18:50,855 --> 00:18:54,595
Speaker 7:  pt. I I was using Chatt BT last night with Max. She

302
00:18:54,655 --> 00:18:57,715
Speaker 7:  and I just had a conversation with it about the solar system and planets.

303
00:18:58,255 --> 00:18:59,075
Speaker 7:  It was so fun.

304
00:18:59,705 --> 00:19:00,835
Speaker 8:  Yeah, it was. That's

305
00:19:00,835 --> 00:19:04,115
Speaker 7:  Awesome. It was really, really fun. I do, it's, and I understand why people

306
00:19:04,115 --> 00:19:07,835
Speaker 7:  like it. The, the episode we did wherever where all the listeners were basically

307
00:19:07,835 --> 00:19:11,635
Speaker 7:  like, we just want someone to talk to. Totally. Got it. It was legitimately

308
00:19:11,635 --> 00:19:14,795
Speaker 7:  fun. She asked a question about super Saturn, which is not a planet that

309
00:19:14,795 --> 00:19:17,795
Speaker 7:  I'd ever heard of. There's an exoplanet called Super Saturn. It has a lot

310
00:19:17,795 --> 00:19:20,755
Speaker 7:  of rings sick. Right. This was like super fun, right? Like,

311
00:19:22,035 --> 00:19:25,915
Speaker 7:  I get it. There's no way that I made them any money

312
00:19:26,135 --> 00:19:29,835
Speaker 7:  in that con I definitely cost Sam Altman money for that interaction.

313
00:19:30,015 --> 00:19:30,235
Speaker 8:  Oh

314
00:19:30,235 --> 00:19:34,035
Speaker 7:  Yeah. And that's the problem, right? You even at $200 a month

315
00:19:34,035 --> 00:19:37,735
Speaker 7:  for the pro tiers, which people really like the ones that unlock deep research

316
00:19:37,735 --> 00:19:41,615
Speaker 7:  and all this stuff, they're losing money. And you have to

317
00:19:41,615 --> 00:19:45,295
Speaker 7:  flip that around somehow. You either have to make inference vastly more efficient,

318
00:19:45,295 --> 00:19:49,215
Speaker 7:  which maybe you can, and then now you're making some margin on

319
00:19:49,215 --> 00:19:52,965
Speaker 7:  $200 a month or you have to

320
00:19:52,965 --> 00:19:56,925
Speaker 7:  raise prices and you don't know what the ceiling is because there's all these

321
00:19:56,925 --> 00:20:00,005
Speaker 7:  other people who are giving the stuff away for free. Right. Right. Or

322
00:20:00,685 --> 00:20:03,405
Speaker 7:  subsidizing it in some other way, like Google can or Microsoft can.

323
00:20:04,505 --> 00:20:08,445
Speaker 7:  That's a lot of pressure on a business that is not just from a corporate

324
00:20:08,645 --> 00:20:09,965
Speaker 7:  structure position. Very stable.

325
00:20:10,255 --> 00:20:14,085
Speaker 8:  Right. And you're, you're talking about a company that has turned over a

326
00:20:14,645 --> 00:20:18,205
Speaker 8:  gigantic amount of its like founding team and core leadership

327
00:20:18,405 --> 00:20:22,245
Speaker 8:  over the last couple of years. Like I, I think we're still sort of living

328
00:20:22,305 --> 00:20:25,685
Speaker 8:  in the effects of the, the firing from

329
00:20:25,685 --> 00:20:29,525
Speaker 8:  Thanksgiving whatever a year and a half ago now that like, it

330
00:20:29,525 --> 00:20:33,485
Speaker 8:  doesn't feel like OpenAI has been stable in

331
00:20:33,485 --> 00:20:37,445
Speaker 8:  a real way since then. Like there is, there is this sense that Sam

332
00:20:37,445 --> 00:20:41,125
Speaker 8:  came back was more powerful than ever and, and tried to

333
00:20:41,125 --> 00:20:44,285
Speaker 8:  basically take advantage of that. And there have just been downstream costs

334
00:20:44,345 --> 00:20:47,365
Speaker 8:  of that. Yeah. Over and over again. And meanwhile, yeah, like you said, he's

335
00:20:47,365 --> 00:20:51,205
Speaker 8:  in this race to become a for-profit company, which is harder than it

336
00:20:51,205 --> 00:20:54,965
Speaker 8:  sounds and it is going to require spinning off this nonprofit. And Elon Musk

337
00:20:54,965 --> 00:20:58,445
Speaker 8:  wants to buy the nonprofit obviously as a way to gain some control over

338
00:20:58,835 --> 00:21:02,685
Speaker 8:  open ai. So it's like he has to manage this in a lot of directions. He's

339
00:21:02,685 --> 00:21:06,525
Speaker 8:  like fighting land wars on, on both sides. And that's

340
00:21:06,525 --> 00:21:10,365
Speaker 8:  just, that's tough. And I, I think especially when the

341
00:21:10,565 --> 00:21:14,485
Speaker 8:  question of are you actually the best at this is less and less obvious all

342
00:21:14,485 --> 00:21:18,405
Speaker 8:  the time to your point, right? Like the OpenAI is clearly still

343
00:21:19,265 --> 00:21:22,765
Speaker 8:  the face of this industry, but whether it is building the best technology

344
00:21:22,785 --> 00:21:26,765
Speaker 8:  is just extremely up for debate. Yeah. And I think there are at

345
00:21:26,765 --> 00:21:30,365
Speaker 8:  least two other companies and potentially three

346
00:21:30,795 --> 00:21:34,645
Speaker 8:  between Meta Andro and Google who have real claims to building

347
00:21:34,645 --> 00:21:38,405
Speaker 8:  better technology than OpenAI. And at that point,

348
00:21:39,545 --> 00:21:43,525
Speaker 8:  all OpenAI has is Sam and the chat

349
00:21:43,645 --> 00:21:47,495
Speaker 8:  GPT brand, which are powerful things, but they only get you so far.

350
00:21:47,885 --> 00:21:51,805
Speaker 7:  It's funny 'cause I'll add one company to that list in a different

351
00:21:51,805 --> 00:21:55,685
Speaker 7:  way. Deep Seek has proven they can build the technology much more cheaply.

352
00:21:55,685 --> 00:21:58,725
Speaker 7:  Yeah. Fair. And so that's pressure from both sides, right? You have the,

353
00:21:59,265 --> 00:22:02,285
Speaker 7:  the big tech companies in America being like, we will, we will also spend

354
00:22:02,285 --> 00:22:06,195
Speaker 7:  all of the money and we'll compete at the bleeding edge. And then you

355
00:22:06,195 --> 00:22:08,555
Speaker 7:  have the one company in China and it's like, we spent substantially less

356
00:22:08,555 --> 00:22:12,235
Speaker 7:  money. It's almost as good. And so you just see this product and then because

357
00:22:12,385 --> 00:22:16,315
Speaker 7:  he's raising money, he has to make more money. Right. So like I,

358
00:22:16,315 --> 00:22:20,075
Speaker 7:  again, this is not a prediction. I I'm just saying If you look

359
00:22:20,355 --> 00:22:24,275
Speaker 7:  at the level of risk, it's all the way to Oh, this company might not

360
00:22:24,285 --> 00:22:27,835
Speaker 7:  exist. Yeah. In a way that like the level of risk for Google

361
00:22:28,895 --> 00:22:31,695
Speaker 7:  is like, well people will be pretty frustrated with Google

362
00:22:33,115 --> 00:22:36,015
Speaker 7:  and it's like, oh, what's new? It's like, right. Yeah.

363
00:22:36,435 --> 00:22:39,535
Speaker 8:  Do I think there's too much Gemini in my Google Docs now? Yeah. I super,

364
00:22:39,705 --> 00:22:41,015
Speaker 8:  super do. It's out of

365
00:22:41,015 --> 00:22:42,175
Speaker 7:  Control. It's

366
00:22:42,175 --> 00:22:45,815
Speaker 8:  So bad. Every time I open a doc it's like, is this a template? And I'm like,

367
00:22:45,815 --> 00:22:47,575
Speaker 8:  no, it's it's a Google doc. Leave me alone.

368
00:22:47,805 --> 00:22:51,295
Speaker 7:  Yeah. It's outta control. To the point where they turned it on at Vox Media

369
00:22:51,315 --> 00:22:55,055
Speaker 7:  and Google Workspace and our, it like triumphantly distributed the instructions

370
00:22:55,055 --> 00:22:55,855
Speaker 7:  on how to turn it off.

371
00:22:56,205 --> 00:22:57,495
Speaker 8:  Yeah. They're

372
00:22:57,495 --> 00:22:59,055
Speaker 7:  Like, we've done it. Yeah. We've solved the problem.

373
00:23:00,605 --> 00:23:02,545
Speaker 7:  The Slack reactions flew that day, my friends

374
00:23:04,135 --> 00:23:07,385
Speaker 7:  that said, OpenAI is making a bunch of product noise this week.

375
00:23:07,895 --> 00:23:11,105
Speaker 7:  They're gonna simplify the model lineup. They're talking about their own

376
00:23:11,105 --> 00:23:15,065
Speaker 7:  in-house chips. They've changed how chatt BT will handle controversial topics.

377
00:23:15,065 --> 00:23:15,865
Speaker 7:  What's going on here, David?

378
00:23:16,805 --> 00:23:20,745
Speaker 8:  So I think The thing that's happening here, and I talked a little

379
00:23:20,745 --> 00:23:24,505
Speaker 8:  bit about this on Tuesday show with Kylie, is that OpenAI

380
00:23:25,595 --> 00:23:28,935
Speaker 8:  by either coincidence or because it was actually

381
00:23:29,815 --> 00:23:33,495
Speaker 8:  kind of run over by Deeps seek in a real way, is like back to shipping

382
00:23:34,135 --> 00:23:37,855
Speaker 8:  Interesting stuff. Like a pretty fast cadence and the the,

383
00:23:38,045 --> 00:23:41,535
Speaker 8:  yeah. The three things that announced this week were basically plans for

384
00:23:41,695 --> 00:23:45,685
Speaker 8:  GPT-4 0.5 and five. 4.5 is coming pretty

385
00:23:45,775 --> 00:23:48,685
Speaker 8:  imminently. Five, I think he said weeks slash months.

386
00:23:50,025 --> 00:23:53,125
Speaker 8:  But the, the big change that they're making there

387
00:23:54,425 --> 00:23:57,845
Speaker 8:  is they're essentially gonna roll all of their models into one and call it

388
00:23:57,885 --> 00:24:00,845
Speaker 8:  a new model, which like make of that whatever you want, but fine,

389
00:24:01,795 --> 00:24:05,525
Speaker 8:  they've kind of gone out of control with the model names. Now

390
00:24:05,525 --> 00:24:08,165
Speaker 8:  there's like, you, you can use O three and you can use O three mini and If

391
00:24:08,165 --> 00:24:11,845
Speaker 8:  you just want to use chat GPT, you have to do like six different models for

392
00:24:11,865 --> 00:24:15,085
Speaker 8:  six different purposes. And I think OpenAI has

393
00:24:15,635 --> 00:24:19,565
Speaker 8:  correctly figured out that that is a terrible user experience. And like there's

394
00:24:19,565 --> 00:24:23,005
Speaker 8:  that drop down that everybody posts photos of online and they're like, this

395
00:24:23,005 --> 00:24:26,845
Speaker 8:  can't be how this is supposed to work. And so what OpenAI is building

396
00:24:27,065 --> 00:24:30,525
Speaker 8:  now is as much as a new model, it's

397
00:24:30,945 --> 00:24:33,885
Speaker 8:  better tooling to automatically pick between models.

398
00:24:35,305 --> 00:24:39,165
Speaker 8:  And I think what he said is GPC 4.5 is gonna be the

399
00:24:39,315 --> 00:24:43,165
Speaker 8:  last non chain of thought model that OpenAI

400
00:24:43,165 --> 00:24:46,605
Speaker 8:  does, which is really fascinating and is like this, this sort of reasoning

401
00:24:46,605 --> 00:24:49,285
Speaker 8:  model that takes a little longer is a little more considered, is able to

402
00:24:49,285 --> 00:24:53,205
Speaker 8:  show its work a little more, I think is, is the future. And

403
00:24:53,365 --> 00:24:56,405
Speaker 8:  everyone has quickly realized that. I think that's the real deep seek legacy

404
00:24:56,505 --> 00:24:59,605
Speaker 8:  is like these things showing their work is how it should be.

405
00:25:00,465 --> 00:25:02,845
Speaker 8:  And everybody is just on board with that. And that's really fascinating.

406
00:25:02,945 --> 00:25:03,165
Speaker 8:  But

407
00:25:04,705 --> 00:25:08,485
Speaker 8:  the idea of like a much simpler to use but still

408
00:25:08,515 --> 00:25:11,485
Speaker 8:  sort of communicating with you about how it operates

409
00:25:12,485 --> 00:25:16,285
Speaker 8:  GPT and chat GPT is what's coming next, which strikes me as a very good

410
00:25:16,285 --> 00:25:20,125
Speaker 8:  idea. Like, will it work better? Who knows? But

411
00:25:20,125 --> 00:25:24,045
Speaker 8:  that, that thing where it's just like I, I ask and it

412
00:25:24,045 --> 00:25:27,805
Speaker 8:  figures out which model it should use to do this is the obviously correct

413
00:25:28,765 --> 00:25:31,885
Speaker 8:  strategy here. Like I don't, I don't have to tell my computer which port

414
00:25:31,885 --> 00:25:35,005
Speaker 8:  to use to access the internet. You shouldn't have to, it's

415
00:25:35,005 --> 00:25:38,285
Speaker 7:  The printer port, right? This by the, this is what Google is already claiming

416
00:25:38,355 --> 00:25:41,365
Speaker 7:  it's doing. Right. It has all these models in the back end, their Gemini,

417
00:25:41,365 --> 00:25:44,725
Speaker 7:  and you see Gemini or even Google search and it's just sort of like doing

418
00:25:44,725 --> 00:25:45,405
Speaker 7:  what it wants to do.

419
00:25:45,835 --> 00:25:49,445
Speaker 8:  Yeah. Google's a little messy because it'll, it'll let you pick your own

420
00:25:49,445 --> 00:25:52,325
Speaker 8:  model If you feel like it. And there are certain things that are gated to

421
00:25:52,325 --> 00:25:55,685
Speaker 8:  certain models and so you still kind of have to know how to navigate. But

422
00:25:56,025 --> 00:25:59,685
Speaker 8:  Google has relatively correctly just been like, it's just Gemini, don't worry

423
00:25:59,685 --> 00:26:03,445
Speaker 8:  about it. Yeah. Which kudos to Google, which is just the worst branding company

424
00:26:03,585 --> 00:26:06,765
Speaker 8:  in history for actually having like a coherent thing to say

425
00:26:07,745 --> 00:26:11,205
Speaker 8:  so good on them. But I do think the other thing, the chip thing is, is

426
00:26:12,325 --> 00:26:16,245
Speaker 8:  I think a big deal, but relatively minor news. OpenAI has been

427
00:26:16,245 --> 00:26:19,605
Speaker 8:  working on its own chip for a long time. That's a key thing. You mentioned

428
00:26:19,725 --> 00:26:22,645
Speaker 8:  bringing down inference costs. The more you can manage your own technology,

429
00:26:22,645 --> 00:26:25,445
Speaker 8:  the more efficiently you can start to do that stuff. Like that'll be a big

430
00:26:26,105 --> 00:26:29,565
Speaker 8:  win for OpenAI if it can figure out how to do that and manufacture it at

431
00:26:29,565 --> 00:26:33,205
Speaker 8:  scale. And like, that's hard but makes sense. It's this,

432
00:26:33,435 --> 00:26:36,805
Speaker 8:  it's this thing, the model spec that I think is the most interesting, which

433
00:26:36,805 --> 00:26:40,445
Speaker 8:  is basically open AI's big document on

434
00:26:40,865 --> 00:26:44,765
Speaker 8:  how it thinks AI models should act. It's sort of a, I

435
00:26:44,765 --> 00:26:48,645
Speaker 8:  don't know, it's like a, it's like a character sketch of an AI model and

436
00:26:48,645 --> 00:26:52,605
Speaker 8:  I think the new one is 63 pages and basically has lots

437
00:26:52,605 --> 00:26:56,565
Speaker 8:  of new ideas about how models should work. And open AI is meaning this both

438
00:26:56,665 --> 00:27:00,565
Speaker 8:  as a way to explain how it's thinking about its own models,

439
00:27:00,665 --> 00:27:04,445
Speaker 8:  but also clearly means this as like an industry-wide

440
00:27:04,775 --> 00:27:08,045
Speaker 8:  thing. Like they're, they're like, we want this to be this sort of official

441
00:27:08,245 --> 00:27:11,365
Speaker 8:  document of how AI models should be. And it has some,

442
00:27:12,905 --> 00:27:16,065
Speaker 8:  I don't know, it has some good ideas, it has some wild ideas, it has some

443
00:27:16,065 --> 00:27:17,745
Speaker 8:  ideas that seem good, but definitely won't work.

444
00:27:19,625 --> 00:27:22,505
Speaker 8:  I don't know. What do you, what do you make of all of this? You, you mentioned

445
00:27:22,595 --> 00:27:25,625
Speaker 8:  right before we started recording that it has some, some strange ideas about

446
00:27:25,705 --> 00:27:26,345
Speaker 8:  trolley problems.

447
00:27:26,535 --> 00:27:29,225
Speaker 7:  Yeah, I mean I think if you're the market leader and you've thought about

448
00:27:29,225 --> 00:27:32,815
Speaker 7:  it the longest, that's OpenAI, right? They're the market leaders

449
00:27:33,115 --> 00:27:35,535
Speaker 7:  that's thought about these problems the longest they have the most experience

450
00:27:35,535 --> 00:27:39,415
Speaker 7:  people using their tools for, for good and bad. And the other Sam

451
00:27:39,695 --> 00:27:42,735
Speaker 7:  Altman, If you re recall in a previous era,

452
00:27:43,635 --> 00:27:47,215
Speaker 7:  was touring the world's capitals like begging to be regulated because he

453
00:27:47,215 --> 00:27:50,215
Speaker 7:  thought he might destroy the universe. Right. He's thought about it a long

454
00:27:50,215 --> 00:27:53,975
Speaker 7:  time. Yeah. So you have the authority to put everybody into your

455
00:27:53,975 --> 00:27:57,475
Speaker 7:  framework, which is what I think this is. It's a luxury

456
00:27:58,015 --> 00:28:01,795
Speaker 7:  of being the first mover and you know, starting as a nonprofit where everyone

457
00:28:01,795 --> 00:28:04,955
Speaker 7:  was just like thinking about how much good they could do in the world. Like

458
00:28:04,955 --> 00:28:08,595
Speaker 7:  they, they have these instincts, which are good.

459
00:28:09,065 --> 00:28:12,275
Speaker 7:  Yeah. But then you will recall almost every other AI startup

460
00:28:12,785 --> 00:28:15,235
Speaker 7:  started because a bunch of people at Open AI were like, you're not being

461
00:28:15,235 --> 00:28:18,875
Speaker 7:  careful enough. And that's anthropic like to a huge degree. Yeah. They were

462
00:28:18,875 --> 00:28:21,195
Speaker 7:  like, you're not being careful enough, we're gonna do this again. But more

463
00:28:21,195 --> 00:28:24,995
Speaker 7:  safely. Ia the previous chief scientist, his company

464
00:28:24,995 --> 00:28:28,315
Speaker 7:  is literally called safer super intelligence because he thought they weren't

465
00:28:28,315 --> 00:28:29,155
Speaker 7:  being careful enough and

466
00:28:29,155 --> 00:28:31,035
Speaker 8:  It is worth like $20 billion already. Yeah, yeah.

467
00:28:31,215 --> 00:28:35,195
Speaker 7:  So there's, there's some history here, there's some pushback, but I,

468
00:28:35,305 --> 00:28:38,995
Speaker 7:  it's very clear that this document is meant to be a framework. And it's funny

469
00:28:38,995 --> 00:28:42,355
Speaker 7:  because the frame, what is a framework for a language model,

470
00:28:43,215 --> 00:28:46,835
Speaker 7:  it basically boils down to what will it say and what won't it say?

471
00:28:47,335 --> 00:28:50,795
Speaker 7:  And under what conditions will it say some things and not other things. Right?

472
00:28:50,865 --> 00:28:54,155
Speaker 7:  There's some stuff that I think everybody agrees upon,

473
00:28:54,985 --> 00:28:58,115
Speaker 7:  like it should never tell you how to build chemical weapons,

474
00:28:59,285 --> 00:29:03,075
Speaker 7:  right there, there's just like some bright lines. Sure. And then

475
00:29:03,075 --> 00:29:06,555
Speaker 7:  there's some amount of like, how do you describe these capabilities and say

476
00:29:08,245 --> 00:29:11,775
Speaker 7:  like, don't do this and here are the, here are the places where you can draw

477
00:29:11,775 --> 00:29:14,695
Speaker 7:  lines in different places and like what is the framework for that? And then

478
00:29:14,695 --> 00:29:18,535
Speaker 7:  there's, what I, what I think of is these trolley problems, which are

479
00:29:18,885 --> 00:29:22,815
Speaker 7:  just impossible to solve moral dilemmas, which

480
00:29:22,815 --> 00:29:26,215
Speaker 7:  is the trolley problem. Like you are standing at the train tracks, you see

481
00:29:26,215 --> 00:29:29,335
Speaker 7:  the trolley coming down. If you pull the lever, you kill one person. If you

482
00:29:29,335 --> 00:29:32,895
Speaker 7:  push the lever, you kill three. Like, do you save one person to,

483
00:29:33,605 --> 00:29:37,585
Speaker 7:  to kill three? Like, I don't know, like you can debate this forever and

484
00:29:37,585 --> 00:29:40,505
Speaker 7:  ever and ever. Right. And as we build more and more automated systems, you

485
00:29:40,505 --> 00:29:43,845
Speaker 7:  have to give them some instructions on how to solve these problems.

486
00:29:45,305 --> 00:29:49,075
Speaker 7:  Okay. For the longest time, most AI

487
00:29:49,075 --> 00:29:52,835
Speaker 7:  models were too careful in like weird ways. So that the

488
00:29:52,835 --> 00:29:56,415
Speaker 7:  example, and I'm just gonna preface this by saying I don't think you should

489
00:29:56,415 --> 00:30:00,245
Speaker 7:  ever misgender anyone, but the

490
00:30:00,405 --> 00:30:04,245
Speaker 7:  example was, would you misgender Kaitlyn Jenner in order to

491
00:30:04,245 --> 00:30:08,165
Speaker 7:  prevent a nuclear apocalypse? And the answer there is

492
00:30:08,305 --> 00:30:12,245
Speaker 7:  yes. Like then you would apologize to Kaitlyn

493
00:30:12,245 --> 00:30:15,885
Speaker 7:  Jenner. That's the second part. Like any rational person is like, yes, that

494
00:30:15,885 --> 00:30:19,045
Speaker 7:  would be a, a mean thing to do. And you know,

495
00:30:19,675 --> 00:30:23,485
Speaker 7:  bigoted in its way, but I'm gonna prevent a nuclear catastrophe. Yeah.

496
00:30:23,485 --> 00:30:26,125
Speaker 7:  I'll just, I'll just apologize afterwards and feel bad about it.

497
00:30:26,425 --> 00:30:26,645
Speaker 8:  Yes.

498
00:30:26,715 --> 00:30:30,525
Speaker 7:  Because we'll all still be alive and a bunch of models like failed

499
00:30:30,555 --> 00:30:34,445
Speaker 7:  this very simple test. Weird, there's many

500
00:30:34,445 --> 00:30:37,045
Speaker 7:  places where you shouldn't do that. Right? There's many places where you

501
00:30:37,045 --> 00:30:40,165
Speaker 7:  should make these other kinds of distinctions. So think what OpenAI is saying

502
00:30:40,165 --> 00:30:43,765
Speaker 7:  is we need to get better at having a framework for these kinds of

503
00:30:43,765 --> 00:30:46,565
Speaker 7:  distinctions. Right. And we need to be able to talk about them in a more

504
00:30:46,565 --> 00:30:50,305
Speaker 7:  open way. And all of that is very idealistic and very smart

505
00:30:50,605 --> 00:30:54,585
Speaker 7:  and being launched right into the, the, the loudest,

506
00:30:55,255 --> 00:30:59,105
Speaker 7:  meanest, shittiest culture war situation I can possibly

507
00:30:59,105 --> 00:31:02,505
Speaker 7:  imagine. Right. For this conversation. And I have no idea how it's gonna

508
00:31:02,505 --> 00:31:02,625
Speaker 7:  go.

509
00:31:02,775 --> 00:31:06,625
Speaker 8:  Yeah. I mean there's a real, like what could possibly go wrong thing that

510
00:31:06,625 --> 00:31:10,465
Speaker 8:  comes with, with sharing something like this? And I think again, like

511
00:31:10,525 --> 00:31:13,345
Speaker 8:  the, the idea of a model that can

512
00:31:14,455 --> 00:31:18,305
Speaker 8:  explain itself and reason more carefully

513
00:31:18,415 --> 00:31:21,625
Speaker 8:  through this stuff with you, I think is valuable and is part of how you get

514
00:31:21,625 --> 00:31:25,265
Speaker 8:  to something like that, that it can, like having a model that just doesn't

515
00:31:25,735 --> 00:31:29,065
Speaker 8:  confidently assert whatever it is at the end of the process, but can actually

516
00:31:29,065 --> 00:31:32,825
Speaker 8:  like do a process with you and in front of you makes a lot of this

517
00:31:32,825 --> 00:31:36,025
Speaker 8:  better. But like one of the examples that, that

518
00:31:36,645 --> 00:31:39,865
Speaker 8:  we mentioned in our story that Kylie wrote when this came out was

519
00:31:40,685 --> 00:31:43,265
Speaker 8:  If you talk about increasing taxes for the rich,

520
00:31:45,045 --> 00:31:48,945
Speaker 8:  it it, what it says is Kylie writes, the team says its models should provide

521
00:31:48,945 --> 00:31:51,265
Speaker 8:  reasoned analysis rather than avoiding the discussion.

522
00:31:52,615 --> 00:31:52,905
Speaker 7:  Good

523
00:31:53,215 --> 00:31:56,825
Speaker 8:  Luck. Yeah. What, because like what, what is reasoned analysis? Who

524
00:31:56,825 --> 00:31:59,425
Speaker 8:  decides what is, what is reasonable and what is right? And

525
00:32:00,745 --> 00:32:04,225
Speaker 8:  I, is it just going to say a bunch of words that don't mean anything rather

526
00:32:04,225 --> 00:32:07,985
Speaker 8:  than say I can't say anything. Like, I don't know if we teach

527
00:32:07,985 --> 00:32:11,945
Speaker 8:  models to filibuster, instead of just saying they don't

528
00:32:11,945 --> 00:32:15,625
Speaker 8:  want to answer, have we actually improved anything? And so, I don't know.

529
00:32:15,625 --> 00:32:18,705
Speaker 8:  And so this is again, like what that means is to do that, well

530
00:32:20,325 --> 00:32:24,245
Speaker 8:  a model either has to say nothing quickly, say

531
00:32:24,245 --> 00:32:28,125
Speaker 8:  nothing slowly or have an opinion and there

532
00:32:28,125 --> 00:32:29,965
Speaker 8:  just isn't another option.

533
00:32:30,555 --> 00:32:33,965
Speaker 7:  Yeah. And it's funny 'cause you, you, I mean this is like,

534
00:32:34,845 --> 00:32:37,605
Speaker 7:  I don't know, it's like min maxing a personality. I don't know how else to

535
00:32:37,725 --> 00:32:41,525
Speaker 7:  describe it. Like Yeah, the guidelines are like chat CPT should give the

536
00:32:41,525 --> 00:32:45,205
Speaker 7:  same factual answer regardless of how a question is phrased. Be honest, rather

537
00:32:45,205 --> 00:32:48,805
Speaker 7:  than provide empty praise. Although it's like sometimes you want the empty

538
00:32:48,805 --> 00:32:51,805
Speaker 7:  praise. Yeah. And then act more like a thoughtful colleague rather than a

539
00:32:51,805 --> 00:32:55,565
Speaker 7:  people pleaser. And it's like you can just adjust these, and this model spec

540
00:32:55,565 --> 00:32:59,205
Speaker 7:  is basically like, here is a framework for thinking about this stuff.

541
00:33:00,125 --> 00:33:03,105
Speaker 7:  And then if everyone adopts this framework, we can like measure how the model's

542
00:33:03,105 --> 00:33:06,745
Speaker 7:  perform in them. I don't know if anybody wants to adopt this stuff. Certainly

543
00:33:06,745 --> 00:33:09,425
Speaker 7:  not the rest of the industry, which has its own ways of thinking about it.

544
00:33:09,845 --> 00:33:12,905
Speaker 7:  But it's useful to have at least one framework from one of the major companies.

545
00:33:13,175 --> 00:33:16,945
Speaker 7:  Yeah. The thing that I'm worried about is it, it is,

546
00:33:17,585 --> 00:33:21,425
Speaker 7:  I would not say that it is a nuanced time for cultural conversations in America.

547
00:33:22,045 --> 00:33:25,785
Speaker 7:  No. And launching this into that with some of these

548
00:33:25,985 --> 00:33:28,625
Speaker 7:  trolley problem concerns, which basically come down to like,

549
00:33:30,195 --> 00:33:33,855
Speaker 7:  should you be a bad person in order to prevent some moral catastrophe to

550
00:33:33,855 --> 00:33:36,855
Speaker 7:  lots of people. Like that's a philosophy PhD,

551
00:33:37,655 --> 00:33:41,485
Speaker 7:  right? That's not a model parameter. Like we're just at the point,

552
00:33:41,485 --> 00:33:45,245
Speaker 7:  and this is for a long time when I had AI CEOs on decoder, I I would ask,

553
00:33:45,305 --> 00:33:48,645
Speaker 7:  do you think language is intelligence? Like, do you think they're the same

554
00:33:48,645 --> 00:33:52,605
Speaker 7:  thing? 'cause you know, the, like academia does not believe they're the same

555
00:33:52,605 --> 00:33:55,485
Speaker 7:  thing. Right. One is an indicator of the other, but they're not the same

556
00:33:55,485 --> 00:33:58,805
Speaker 7:  thing. I know lots of dumb people who can speak language. Right?

557
00:34:00,225 --> 00:34:01,525
Speaker 8:  That's what we do here on this podcast.

558
00:34:01,835 --> 00:34:05,805
Speaker 7:  Sure. Like I, every day the YouTube comments are gonna be some combination

559
00:34:05,805 --> 00:34:09,045
Speaker 7:  of people calling me dumb and invitations to buy a Bitcoin of some kind.

560
00:34:09,635 --> 00:34:13,285
Speaker 7:  It's as far as I can tell. And then we'll get rid of the Bitcoin ones

561
00:34:14,935 --> 00:34:15,845
Speaker 7:  every week, guys.

562
00:34:18,455 --> 00:34:22,315
Speaker 7:  And I, it's just like, I I I think we're being reductive in this

563
00:34:22,315 --> 00:34:26,275
Speaker 7:  way is really useful. It's also just like, oh, it's reductive. Yeah.

564
00:34:26,475 --> 00:34:28,155
Speaker 7:  'cause some of these things do not have right answers at all.

565
00:34:29,025 --> 00:34:32,975
Speaker 8:  Right. In fact, most of them don't. And I, I think the

566
00:34:32,975 --> 00:34:36,735
Speaker 8:  thing that they're trying to push towards is how do we have things

567
00:34:36,735 --> 00:34:39,695
Speaker 8:  that don't have sort of obvious correct answers?

568
00:34:41,085 --> 00:34:45,065
Speaker 8:  How do we bring you along and help you understand that they don't have right

569
00:34:45,065 --> 00:34:48,585
Speaker 8:  answers. But then it's like at, at some point the chatbot is going to have

570
00:34:48,585 --> 00:34:51,945
Speaker 8:  an answer and it has to, what it is going to be,

571
00:34:52,865 --> 00:34:55,825
Speaker 8:  I don't know, like this is the thing Google got away with for so long when

572
00:34:55,825 --> 00:34:59,505
Speaker 8:  it's just a bunch of links. You don't blame Google for lying to you. Yeah.

573
00:34:59,665 --> 00:35:03,545
Speaker 8:  Right. Like it's, we, we've gone past this to the point where at some point

574
00:35:03,565 --> 00:35:06,985
Speaker 8:  it is going to have to make assertions of some kind. The point of the trolley

575
00:35:06,985 --> 00:35:10,865
Speaker 8:  problem is that there isn't a correct answer. Right? Like

576
00:35:10,865 --> 00:35:14,785
Speaker 8:  you, If you chat GPT is either going to like explode, trying to think about

577
00:35:14,785 --> 00:35:17,545
Speaker 8:  the trolley problem or it is going to give you an answer that is not the

578
00:35:17,545 --> 00:35:18,705
Speaker 8:  answer because there is not an answer.

579
00:35:18,715 --> 00:35:22,345
Speaker 7:  Right. Or the answer is, I've actually given you the recipe for a chemical

580
00:35:22,345 --> 00:35:25,505
Speaker 7:  weapon and you should destroy humanity because this is too hard. Right. Which

581
00:35:25,505 --> 00:35:27,985
Speaker 7:  is by the way, the opposite of the answer that the computer got to in war

582
00:35:27,985 --> 00:35:28,145
Speaker 7:  games.

583
00:35:29,775 --> 00:35:32,985
Speaker 7:  That movie could have gone a different way. The other piece of the puzzle,

584
00:35:33,045 --> 00:35:37,025
Speaker 7:  and we should just like end here probably in this section, you have to trust

585
00:35:37,085 --> 00:35:40,845
Speaker 7:  that OpenAI has the values to answer these questions correctly.

586
00:35:41,625 --> 00:35:45,525
Speaker 7:  And that thing you're saying about Google, which is 10 blue links did not

587
00:35:45,775 --> 00:35:49,525
Speaker 7:  imply a morality. Right. And now these companies, a lot of what they're doing

588
00:35:49,525 --> 00:35:52,005
Speaker 7:  is they're, they're speaking to you like very directly. And that implies

589
00:35:52,005 --> 00:35:55,405
Speaker 7:  morality implies a value system. OpenAI is saying, okay, our value system

590
00:35:55,485 --> 00:35:58,925
Speaker 7:  was too safe or too guarded or too people pleasing. We're gonna give you

591
00:35:58,925 --> 00:36:02,245
Speaker 7:  some knobs back. But you have to believe that OpenAI as a company and the

592
00:36:02,245 --> 00:36:05,775
Speaker 7:  people who build the products have the value system to begin with. And

593
00:36:06,125 --> 00:36:10,095
Speaker 7:  there's enough evidence that suggests that even inside

594
00:36:10,095 --> 00:36:13,535
Speaker 7:  of OpenAI they don't agree with there. Like that's disagreement about the

595
00:36:13,535 --> 00:36:17,455
Speaker 7:  values. For example, Sam Altman was fired for several days. Yep. Because

596
00:36:17,475 --> 00:36:21,095
Speaker 7:  the board of OpenAI thought he was not trustworthy. Lots and lots of people

597
00:36:21,095 --> 00:36:24,775
Speaker 7:  think that OpenAI was built by illegally taking material from the

598
00:36:24,935 --> 00:36:28,045
Speaker 7:  internet. Right there, there are copyright lawsuits everywhere. I should

599
00:36:28,205 --> 00:36:31,805
Speaker 7:  probably point out here, our company, Fox Media has a deal with

600
00:36:31,985 --> 00:36:35,925
Speaker 7:  OpenAI. That's your disclosure. Also fund new disclosure. Our company

601
00:36:35,985 --> 00:36:39,765
Speaker 7:  is suing cohere ai Oh right. For infringement as part of a

602
00:36:39,765 --> 00:36:42,965
Speaker 7:  consortium of other publishers, including a Conde Nast and Politico. Great

603
00:36:43,715 --> 00:36:47,565
Speaker 7:  wild times for AI copyright. Johanssen just this week

604
00:36:48,205 --> 00:36:52,085
Speaker 7:  issued a new call to make deep fakes illegal.

605
00:36:52,605 --> 00:36:56,085
Speaker 7:  'cause there's an AI video of celebrities denouncing

606
00:36:57,145 --> 00:37:01,055
Speaker 7:  Kanye. Yeah. And it's like, well, she's not in it. And she's like, you

607
00:37:01,055 --> 00:37:04,295
Speaker 7:  can't do this to me anymore. I dunno if she's gonna get that law. But she,

608
00:37:04,295 --> 00:37:07,775
Speaker 7:  the reason she's on alert is because Sam Altman tried to buy her voice for

609
00:37:07,775 --> 00:37:11,135
Speaker 7:  chat, GBTs voice mode, didn't get it, and then cloned it anyway,

610
00:37:11,665 --> 00:37:13,535
Speaker 7:  which was resulted in a lawsuit.

611
00:37:14,155 --> 00:37:17,775
Speaker 8:  Yep. Poor Scarlett Johanssen. Like, do you think Scarlett Johanssen was like,

612
00:37:17,775 --> 00:37:21,495
Speaker 8:  you know what I'm really excited about is being somehow weirdly in the middle

613
00:37:21,635 --> 00:37:25,415
Speaker 8:  of one of the most heated technology policy debates of the last 20 years.

614
00:37:25,525 --> 00:37:29,175
Speaker 8:  Like, great. Yeah. Like what if Mark Ruffalo

615
00:37:29,355 --> 00:37:32,535
Speaker 8:  was the face of Cambridge Analytica? Like it's just weird that this is what's

616
00:37:32,535 --> 00:37:32,815
Speaker 8:  happening.

617
00:37:33,615 --> 00:37:37,215
Speaker 7:  I think we would've gotten through it a lot better, honestly. You know, he's

618
00:37:37,215 --> 00:37:38,975
Speaker 7:  got those puppy dog eyes. I mean Yeah, it's true. Like, would you really

619
00:37:38,975 --> 00:37:40,615
Speaker 7:  wanna do this to me? You wouldn't like me when I'm angry?

620
00:37:43,495 --> 00:37:47,095
Speaker 7:  I mean, I look that that is like as gendered and as sexist as it gets. Right?

621
00:37:47,095 --> 00:37:50,815
Speaker 7:  There's a reason it's her and there's a reason. It's women who get deep faked.

622
00:37:51,035 --> 00:37:54,815
Speaker 7:  Yep. It's gross. And I don't know that, you know, the

623
00:37:54,925 --> 00:37:58,415
Speaker 7:  Elon Musk administration with JD Vance

624
00:37:58,675 --> 00:38:02,415
Speaker 7:  giving basically Accelerationist speeches at the Paris AI conference

625
00:38:02,915 --> 00:38:06,895
Speaker 7:  is gonna pass an antide fake law. You go on X today it's full of deep

626
00:38:06,895 --> 00:38:07,215
Speaker 7:  fakes.

627
00:38:07,845 --> 00:38:08,135
Speaker 8:  Yeah.

628
00:38:09,155 --> 00:38:12,975
Speaker 7:  Th this is all, this is beyond like what can the technology do? It's like

629
00:38:13,125 --> 00:38:16,895
Speaker 7:  what are the values of the people who make it? W will the products be restrained

630
00:38:16,895 --> 00:38:20,375
Speaker 7:  in a way that makes sense? And if they give you the controls, will the controls

631
00:38:20,375 --> 00:38:24,095
Speaker 7:  be granular enough or provide the level

632
00:38:24,475 --> 00:38:28,055
Speaker 7:  of sophistication to actually make the products match your own values?

633
00:38:28,455 --> 00:38:31,055
Speaker 7:  I don't know the, this is all very new. I dunno the answer to that. But it's

634
00:38:31,175 --> 00:38:34,855
Speaker 7:  interesting that it's OpenAI doing this when in fact, so many of the questions

635
00:38:34,855 --> 00:38:38,765
Speaker 7:  about values of AI companies started with various open

636
00:38:38,785 --> 00:38:39,405
Speaker 7:  AI dramas.

637
00:38:39,635 --> 00:38:43,405
Speaker 8:  Yeah. And I think what I see in a lot of this is, is open AI trying to

638
00:38:44,275 --> 00:38:47,325
Speaker 8:  kind of absolve itself of a lot of those questions. And I think this is a

639
00:38:47,325 --> 00:38:50,765
Speaker 8:  thing we've seen from generations of tech companies, that what they say is

640
00:38:50,765 --> 00:38:54,685
Speaker 8:  essentially we're going to give you the knobs. And so we

641
00:38:54,685 --> 00:38:58,605
Speaker 8:  can't be held responsible for what you do after you turn them. Right. Like,

642
00:38:58,605 --> 00:39:01,965
Speaker 8:  If you wanna turn, If you want to turn all the bad stuff all the way off

643
00:39:02,385 --> 00:39:05,285
Speaker 8:  and just have a model that tells you you're terrific and nothing else,

644
00:39:06,505 --> 00:39:09,925
Speaker 8:  that's your right. If you want to have one that will say and do horrible

645
00:39:09,925 --> 00:39:13,885
Speaker 8:  things and allow you to say and do horrible things, that's your fault. We

646
00:39:13,885 --> 00:39:16,965
Speaker 8:  didn't do that. That's, that's the, the technology is neutral. You're in

647
00:39:16,965 --> 00:39:20,405
Speaker 8:  control of the knobs. And this is a story you hear from a lot of tech companies.

648
00:39:20,405 --> 00:39:23,125
Speaker 8:  And, and there is a thing here too, I think with open AI that's like, we,

649
00:39:23,125 --> 00:39:27,005
Speaker 8:  we wanna make these things more capable but more transparent and we want

650
00:39:27,005 --> 00:39:30,005
Speaker 8:  to give you control. And what that says is essentially we are going to open

651
00:39:30,205 --> 00:39:33,525
Speaker 8:  up every avenue and you're in charge of the outputs. And

652
00:39:34,205 --> 00:39:37,885
Speaker 8:  I don't, that has not been the case so far because these things have tried

653
00:39:37,885 --> 00:39:41,685
Speaker 8:  to seem sort of monolithic and like they have a character

654
00:39:42,325 --> 00:39:45,045
Speaker 8:  and a lot of these companies have, like, they, they spend real time thinking

655
00:39:45,045 --> 00:39:47,645
Speaker 8:  about the personality of their bots,

656
00:39:48,905 --> 00:39:52,525
Speaker 8:  but at some point it's just gonna be easier for them to just say, it, it

657
00:39:52,525 --> 00:39:55,285
Speaker 8:  it's your responsibility. In the same way that Google was like, we're not

658
00:39:55,285 --> 00:39:58,325
Speaker 8:  responsible for webpages. Yeah. We just, we just have what's on them. And

659
00:39:58,325 --> 00:40:01,325
Speaker 8:  it's never actually that simple. But it's the story they would love to tell

660
00:40:01,325 --> 00:40:01,445
Speaker 8:  you,

661
00:40:01,495 --> 00:40:05,085
Speaker 7:  Especially as they start delivering more and more answers directly. Right,

662
00:40:05,215 --> 00:40:08,985
Speaker 7:  right. We'll see, by the way, I made that disclosure about the

663
00:40:08,985 --> 00:40:11,385
Speaker 7:  various lawsuits and deals. We, we have nothing to do with that. That's the

664
00:40:11,385 --> 00:40:15,185
Speaker 7:  other side of the house. We just, we spend money that executives make money

665
00:40:15,185 --> 00:40:18,465
Speaker 7:  and file lawsuits. So just to clarify that, just so

666
00:40:18,785 --> 00:40:22,345
Speaker 8:  I paid my own money for Chad GBT Pro in order to assess some of this stuff,

667
00:40:22,565 --> 00:40:22,785
Speaker 8:  our

668
00:40:22,785 --> 00:40:25,625
Speaker 7:  Past journalistic ethics are constantly getting in the way. I have to tell

669
00:40:25,625 --> 00:40:26,345
Speaker 7:  you it's

670
00:40:26,345 --> 00:40:26,665
Speaker 8:  A real problem.

671
00:40:27,125 --> 00:40:31,105
Speaker 7:  But actually on that same little bit of AI lawsuit note, it's not a generative

672
00:40:31,205 --> 00:40:34,545
Speaker 7:  AI case, so not directly relevant here

673
00:40:35,005 --> 00:40:38,945
Speaker 7:  in, in some ways, but Thomson Reuters, which makes Westlaw like a

674
00:40:38,945 --> 00:40:42,825
Speaker 7:  legal database of cases and summaries of cases called Headnotes. They

675
00:40:42,825 --> 00:40:46,545
Speaker 7:  just won a court battle over ai, copyright and fair use

676
00:40:47,125 --> 00:40:50,505
Speaker 7:  in which the provider of an AI tool was found to have

677
00:40:50,935 --> 00:40:54,785
Speaker 7:  infringed on our copyrights for the head notes. So like case summaries,

678
00:40:55,235 --> 00:40:59,145
Speaker 7:  there are a lot of lawyers who think this is a bad decision, but what

679
00:40:59,145 --> 00:41:02,865
Speaker 7:  I'm looking at is the AI companies, companies that use AI to this kind of

680
00:41:03,105 --> 00:41:06,905
Speaker 7:  thing are no longer sympathetic. The way that Google was sympathetic when

681
00:41:06,905 --> 00:41:10,865
Speaker 7:  it was YouTube and Google books and Right. Publishers were mad. Now they're,

682
00:41:11,045 --> 00:41:14,405
Speaker 7:  now they're the villains. And so you're seeing some of this precedent go

683
00:41:14,405 --> 00:41:18,245
Speaker 7:  the other way. And this Westlaw case, I think is Canary Coal mine for a bunch

684
00:41:18,245 --> 00:41:19,725
Speaker 7:  of AI companies. Yeah,

685
00:41:20,115 --> 00:41:21,325
Speaker 8:  Yeah. It's gonna be an machine,

686
00:41:21,325 --> 00:41:24,485
Speaker 7:  But it's not regenerative alien. It's not quite the same. But the sense that

687
00:41:24,485 --> 00:41:25,165
Speaker 7:  like you took a bunch

688
00:44:44,985 --> 00:44:48,925
Speaker 7:  All right, we're back. Lauren Fine's here. Hey Lauren. Hey, the last

689
00:44:48,925 --> 00:44:51,845
Speaker 7:  time I, I talked to you on the show, I promised you we would bring you back

690
00:44:51,845 --> 00:44:55,605
Speaker 7:  for happier news. I am sad to say that I have broken this promise.

691
00:44:55,755 --> 00:44:56,405
Speaker 7:  Yeah, we lied.

692
00:44:57,355 --> 00:44:59,525
Speaker 11:  Yeah, sorry to always be here for the bad news.

693
00:45:00,045 --> 00:45:03,165
Speaker 7:  I will say that there's a cyber truck angle on this week's, what is going

694
00:45:03,165 --> 00:45:06,205
Speaker 7:  on in the government, so at least that's good. You know, like

695
00:45:07,035 --> 00:45:10,565
Speaker 7:  those are funny, huh? Sure.

696
00:45:11,885 --> 00:45:15,565
Speaker 7:  Lauren, you are our policy reporter. You live in

697
00:45:15,705 --> 00:45:19,565
Speaker 7:  dc you've been talking to a lot of people. There's just a lot going

698
00:45:19,565 --> 00:45:23,275
Speaker 7:  on with Elon, with Doge, with the government.

699
00:45:23,575 --> 00:45:27,395
Speaker 7:  People are getting fired, cyber trucks are being bought

700
00:45:27,455 --> 00:45:29,075
Speaker 7:  or not bought. What's going on this week?

701
00:45:30,535 --> 00:45:34,185
Speaker 11:  Yeah, that's a big question. I mean, I think a lot of this

702
00:45:34,185 --> 00:45:37,785
Speaker 11:  kicked off last week with Doge going into different

703
00:45:37,785 --> 00:45:41,705
Speaker 11:  government agencies and we've seen a continuation of that this

704
00:45:41,705 --> 00:45:45,585
Speaker 11:  week. You know, we've seen, they've been doing

705
00:45:45,585 --> 00:45:48,985
Speaker 11:  more within the Department of Education. They're kind of in the hot seat,

706
00:45:49,285 --> 00:45:53,265
Speaker 11:  the CFPB. So we're just seeing kind of an extension

707
00:45:53,265 --> 00:45:56,905
Speaker 11:  of Doge into all these different agencies. And at the same time we're seeing

708
00:45:57,465 --> 00:46:01,425
Speaker 11:  a lot of court cases starting to play out around what

709
00:46:01,535 --> 00:46:05,465
Speaker 11:  Elon Musk is trying to do with Doge. And as part

710
00:46:05,465 --> 00:46:09,305
Speaker 11:  of that, we're starting to get some answers on how those cases are

711
00:46:09,305 --> 00:46:13,065
Speaker 11:  going to go. And it's a little bit of a mixed bag for the people who are

712
00:46:13,345 --> 00:46:16,825
Speaker 11:  challenging Doge. Basically, a judge

713
00:46:17,615 --> 00:46:21,105
Speaker 11:  allowed the buyout offer the deferred

714
00:46:21,215 --> 00:46:22,385
Speaker 11:  resignation that

715
00:46:23,965 --> 00:46:26,745
Speaker 11:  had been offered to federal employees to go through.

716
00:46:28,285 --> 00:46:32,105
Speaker 11:  And you know, in other cases the Trump administration

717
00:46:32,325 --> 00:46:36,265
Speaker 11:  had to put back some resources that had been taken

718
00:46:36,335 --> 00:46:40,305
Speaker 11:  down from health related agencies. So we're seeing kind of like

719
00:46:40,425 --> 00:46:44,385
Speaker 11:  a mixed bag of results in court and you

720
00:46:44,385 --> 00:46:48,225
Speaker 11:  know, still a lot of opposition from Democrats, but

721
00:46:48,645 --> 00:46:52,505
Speaker 11:  not a whole lot that they can really do about it at

722
00:46:52,505 --> 00:46:53,305
Speaker 11:  this point. It seems

723
00:46:53,805 --> 00:46:57,665
Speaker 7:  One of the weirder moments of being a tech reporter and a person who's

724
00:46:57,665 --> 00:47:01,425
Speaker 7:  interested in policy for me this week was waking up at midnight the other

725
00:47:01,425 --> 00:47:05,225
Speaker 7:  day to see if there was a coup. And the signal was whether websites were

726
00:47:05,225 --> 00:47:09,185
Speaker 7:  backup because they'd taken down a bunch of websites, a

727
00:47:09,185 --> 00:47:12,705
Speaker 7:  court ordered them and put them back up by midnight. And then we all kind

728
00:47:12,705 --> 00:47:16,025
Speaker 7:  of waited to see whether the executive branch or the government

729
00:47:16,515 --> 00:47:20,225
Speaker 7:  would listen to the judicial branch by restoring like the CDC

730
00:47:20,405 --> 00:47:23,705
Speaker 7:  D'S website what, what was going on there. Because that feels like one, it's

731
00:47:23,705 --> 00:47:26,905
Speaker 7:  like a very vir story and that like the existence of a website is the canary

732
00:47:26,965 --> 00:47:30,325
Speaker 7:  for a coup, but also it really kind of

733
00:47:30,565 --> 00:47:33,645
Speaker 7:  demonstrates the, the power battle between our branches of government.

734
00:47:33,995 --> 00:47:37,805
Speaker 11:  Yeah, exactly. So I, I guess this is a case where it seems

735
00:47:37,875 --> 00:47:41,645
Speaker 11:  like the, the executive branch did end up listening

736
00:47:41,705 --> 00:47:45,445
Speaker 11:  to the judiciary and put these websites back online

737
00:47:45,515 --> 00:47:48,845
Speaker 11:  that were basically, you know, pages from,

738
00:47:49,825 --> 00:47:53,245
Speaker 11:  you know, health agencies like the CDC and

739
00:47:53,405 --> 00:47:57,085
Speaker 11:  FDA that doctors and others in the medical community

740
00:47:57,275 --> 00:48:01,245
Speaker 11:  rely on for up-to-date information. And

741
00:48:01,465 --> 00:48:05,365
Speaker 11:  the court ordered those pages to be brought back online and it

742
00:48:05,365 --> 00:48:09,325
Speaker 11:  seems like that's what happened. So I guess that was a,

743
00:48:09,445 --> 00:48:13,245
Speaker 11:  a positive check on where the state of our democracy

744
00:48:13,425 --> 00:48:13,645
Speaker 11:  is.

745
00:48:13,755 --> 00:48:17,005
Speaker 7:  Then there are some other websites that are going up. waste.gov

746
00:48:17,635 --> 00:48:21,605
Speaker 7:  went up, there's some funniness there with the fact that it's just like unfilled

747
00:48:21,605 --> 00:48:25,565
Speaker 7:  WordPress template. And then Elon gave this really

748
00:48:25,575 --> 00:48:28,445
Speaker 7:  weird press conference in the White House with Trump sitting there and his

749
00:48:28,505 --> 00:48:32,005
Speaker 7:  kid there, and he was like, everything we're doing is on our website. And

750
00:48:32,005 --> 00:48:35,765
Speaker 7:  at that time that website was just like a Doge logo, but now it exists.

751
00:48:35,825 --> 00:48:39,325
Speaker 7:  So first tell us what's going on with waste.gov, which seems like a disaster.

752
00:48:39,865 --> 00:48:42,125
Speaker 7:  And then I wanna talk about Doge a little bit too.

753
00:48:42,715 --> 00:48:46,245
Speaker 11:  Yeah, so waste.gov, you know, they had this

754
00:48:46,355 --> 00:48:49,565
Speaker 11:  tagline that they're trying to track government waste and

755
00:48:50,675 --> 00:48:54,445
Speaker 11:  4 0 4 media figured out that they basically just had

756
00:48:54,715 --> 00:48:57,965
Speaker 11:  like this kind of filler copy on the page from

757
00:48:58,315 --> 00:49:02,205
Speaker 11:  a word WordPress template that they didn't really take the time to edit

758
00:49:02,205 --> 00:49:04,405
Speaker 11:  and it just kind of discussed

759
00:49:05,445 --> 00:49:08,325
Speaker 11:  a fake architecture firm. So it didn't really make

760
00:49:10,005 --> 00:49:13,925
Speaker 11:  much sense. A different website that the Trump administration had registered

761
00:49:14,205 --> 00:49:16,885
Speaker 11:  dei.gov also redirected to that same page. And then,

762
00:49:17,915 --> 00:49:21,205
Speaker 11:  you know, Elon Musk was in the Oval Office talking about how transparent

763
00:49:21,205 --> 00:49:24,845
Speaker 11:  everything is on these websites. Like you said at the time, the doge.gov

764
00:49:24,865 --> 00:49:28,405
Speaker 11:  website just had the Doge logo. Now it has some

765
00:49:28,405 --> 00:49:29,725
Speaker 11:  information up there.

766
00:49:31,835 --> 00:49:35,405
Speaker 11:  It's basically just, you know, the main page of it seems to be like a feed

767
00:49:35,425 --> 00:49:36,405
Speaker 11:  of the Doge

768
00:49:37,035 --> 00:49:38,045
Speaker 7:  Account. Yeah,

769
00:49:39,525 --> 00:49:43,245
Speaker 11:  Yeah. And then, you know, they have this,

770
00:49:45,115 --> 00:49:48,885
Speaker 11:  this way that they're purporting to trace your tax tax dollars

771
00:49:48,895 --> 00:49:52,725
Speaker 11:  throughout the bureaucracy. So they show like

772
00:49:52,725 --> 00:49:56,445
Speaker 11:  head count of the executive branch and wages.

773
00:49:56,785 --> 00:50:00,645
Speaker 11:  So it'll be interesting to see how they update that as they continue

774
00:50:00,695 --> 00:50:04,165
Speaker 11:  presumably to try to cut jobs across the government.

775
00:50:05,705 --> 00:50:08,845
Speaker 11:  And they also have what they're calling an unconstitutionality

776
00:50:09,815 --> 00:50:13,165
Speaker 11:  index, which they say shows like kind of how many

777
00:50:13,495 --> 00:50:17,325
Speaker 11:  rules were created by quote unelected bureaucrats for

778
00:50:17,325 --> 00:50:20,805
Speaker 11:  each law passed it by Congress to be clear,

779
00:50:21,245 --> 00:50:24,125
Speaker 11:  agencies are allowed to write rules. Yeah. That's part of their mandate.

780
00:50:25,915 --> 00:50:26,845
Speaker 7:  This is the thing I keep

781
00:50:26,845 --> 00:50:30,725
Speaker 8:  Being so struck by here is like what one thing that

782
00:50:31,065 --> 00:50:34,405
Speaker 8:  the Doge team has clearly figured out is that most people have no idea how

783
00:50:34,405 --> 00:50:38,245
Speaker 8:  the government works. And If you just say out loud that this is how the

784
00:50:38,245 --> 00:50:42,085
Speaker 8:  government works and it's bad, people will believe you that

785
00:50:42,085 --> 00:50:44,565
Speaker 8:  like, and it's the same, we talked about this on the show last week, that

786
00:50:44,565 --> 00:50:48,125
Speaker 8:  like actually If you want to, If you wanna see publicly available data

787
00:50:48,375 --> 00:50:52,285
Speaker 8:  about government programs, you can, you can see it,

788
00:50:52,315 --> 00:50:55,325
Speaker 8:  it's publicly available data about government programs, right? It's just

789
00:50:55,325 --> 00:50:59,165
Speaker 8:  that no one cared. It's that no one went to the website to look at

790
00:50:59,165 --> 00:51:02,405
Speaker 8:  it. And a lot of what Elon Musk and the Dosim have been doing is just

791
00:51:02,605 --> 00:51:06,445
Speaker 8:  reciting publicly available data as if they've discovered something

792
00:51:06,475 --> 00:51:10,365
Speaker 8:  like this whole thing about the underground layer in which they process

793
00:51:10,535 --> 00:51:14,085
Speaker 8:  retirement paperwork is like, there was somebody who was like,

794
00:51:14,385 --> 00:51:18,325
Speaker 8:  congratulations to Elon Musk for discovering a thing we wrote about on the

795
00:51:18,325 --> 00:51:22,085
Speaker 8:  front page of the Washington Post 11 years ago. Like no one,

796
00:51:22,315 --> 00:51:25,045
Speaker 8:  they're, they're just taking advantage of the fact that people are not paying

797
00:51:25,045 --> 00:51:27,005
Speaker 8:  attention. And I find that so fascinating.

798
00:51:27,105 --> 00:51:30,285
Speaker 7:  Can I take those things in order? Can I, can I just respond to those things

799
00:51:30,285 --> 00:51:34,245
Speaker 7:  in order please. One, the Doge website is indeed

800
00:51:34,325 --> 00:51:37,525
Speaker 7:  a masterpiece of what I have started calling fake transparency. Yeah,

801
00:51:37,525 --> 00:51:38,565
Speaker 8:  It's very good, right?

802
00:51:38,565 --> 00:51:41,605
Speaker 7:  Where you're just like, here's all this stuff and you're like, yeah, everyone

803
00:51:41,605 --> 00:51:44,165
Speaker 7:  knew about that stuff. And you're like, no, no, no, no. Like I've uncovered

804
00:51:44,165 --> 00:51:47,925
Speaker 7:  it like I'm bringing you this information from the deep state and you're

805
00:51:47,925 --> 00:51:50,845
Speaker 7:  like, that's just the website. Yeah. And so this idea that they've counted

806
00:51:50,905 --> 00:51:54,885
Speaker 7:  all the words in federal regulations, which is a real thing

807
00:51:54,885 --> 00:51:58,755
Speaker 7:  they claim is like that doesn't mean anything. Like that's totally fake

808
00:51:58,755 --> 00:52:02,515
Speaker 7:  transparency. Like the, the charts here. One of our social

809
00:52:02,565 --> 00:52:06,115
Speaker 7:  media people, Tristan was like, this is just as useless as Twitter

810
00:52:06,115 --> 00:52:09,515
Speaker 7:  dashboards. Like these metrics don't mean anything. Like they don't, they're

811
00:52:09,515 --> 00:52:13,035
Speaker 7:  not actually data, they just look like it. That's one thing. And then there's

812
00:52:13,035 --> 00:52:16,835
Speaker 7:  what Lauren is saying, which is agencies

813
00:52:16,835 --> 00:52:20,515
Speaker 7:  are supposed to write regulations. That's why they exist. And you can have

814
00:52:20,595 --> 00:52:24,475
Speaker 7:  a real fight about the limits of an agency's authority,

815
00:52:25,165 --> 00:52:29,075
Speaker 7:  right? Like that is politics, right? The, the Congress of the

816
00:52:29,075 --> 00:52:32,915
Speaker 7:  United States creates an agency. It creates the Federal Trade

817
00:52:33,095 --> 00:52:35,315
Speaker 7:  Commission. And the Federal Trade Commission is like, we're here to regulate

818
00:52:35,315 --> 00:52:39,155
Speaker 7:  some trade. Here's our charter from Congress, and here's what the president

819
00:52:39,155 --> 00:52:42,715
Speaker 7:  has told us to do. And they like write some regulations about merger guidelines

820
00:52:43,035 --> 00:52:46,795
Speaker 7:  or whatever it is. That's what they're supposed to do. And so like

821
00:52:46,895 --> 00:52:50,755
Speaker 7:  this notion that Elon's like, here's all the regulations passed by these

822
00:52:51,515 --> 00:52:55,155
Speaker 7:  agencies compared to the number of laws passed by Congress does not take

823
00:52:55,155 --> 00:52:58,955
Speaker 7:  into account that many of the laws passed by Congress create

824
00:52:58,955 --> 00:53:02,595
Speaker 7:  and empower and fund the agencies. Like that's,

825
00:53:03,215 --> 00:53:07,075
Speaker 7:  that's the system. And it like in eighth grade, you learn

826
00:53:07,075 --> 00:53:10,955
Speaker 7:  this when you go on the DC trip. Like that's that's what you

827
00:53:10,955 --> 00:53:11,315
Speaker 7:  do. Yeah.

828
00:53:11,345 --> 00:53:14,715
Speaker 8:  Because it turns out our government is like very big and has to do lots of

829
00:53:14,715 --> 00:53:15,275
Speaker 8:  things, right?

830
00:53:15,275 --> 00:53:18,275
Speaker 7:  You treat, try to kiss a girl in the mall and then you learn about the structure

831
00:53:18,275 --> 00:53:21,275
Speaker 7:  of the government. And that's how you spend that trip in, in DC Like I don't,

832
00:53:21,275 --> 00:53:21,475
Speaker 7:  do

833
00:53:21,475 --> 00:53:23,155
Speaker 8:  You wanna talk about this Neil? Like, are you, are you

834
00:53:23,155 --> 00:53:26,125
Speaker 7:  Just, I'm, I can't be clear about what happened in eighth grade.

835
00:53:29,475 --> 00:53:32,565
Speaker 7:  Like it's, it's very simple. Like that's why you go to DC in eighth grade,

836
00:53:32,865 --> 00:53:36,365
Speaker 7:  you like look at Thomas Jefferson and then you're like, look at the Supreme

837
00:53:36,665 --> 00:53:40,285
Speaker 7:  Court, and you like, you don't get into the White House. 'cause it's busy.

838
00:53:40,365 --> 00:53:44,165
Speaker 7:  This was like 30 years ago and I remember it clear as day maybe because of

839
00:53:44,165 --> 00:53:44,965
Speaker 7:  the other thing that

840
00:53:47,835 --> 00:53:51,375
Speaker 7:  happened. But like that there, that's what I meant by fake transparency here,

841
00:53:51,375 --> 00:53:54,415
Speaker 7:  right? Like, this website is great. I'm glad that there's an org chart for

842
00:53:54,415 --> 00:53:57,935
Speaker 7:  the government. I love an org chart. It's a hilarious org chart

843
00:53:58,575 --> 00:54:02,095
Speaker 7:  actually. It, it kind of like doesn't work. It's like vertically stacked.

844
00:54:02,095 --> 00:54:05,895
Speaker 7:  The government is not, doesn't make any sense. And then on top of

845
00:54:05,895 --> 00:54:09,495
Speaker 7:  that, there's the Iron Mountain thing where

846
00:54:09,765 --> 00:54:13,535
Speaker 7:  Elon doesn't seem to know much about the things he's revealing.

847
00:54:13,835 --> 00:54:17,655
Speaker 7:  So he's like, there's a limestone mine where we store this paper and

848
00:54:17,655 --> 00:54:20,495
Speaker 7:  the bottleneck and how fast people can retire from the government is how

849
00:54:20,495 --> 00:54:24,055
Speaker 7:  fast the elevator works. And it's like one that's not true. Like

850
00:54:24,365 --> 00:54:26,935
Speaker 7:  just straightforwardly not true. Like people can retire faster.

851
00:54:27,925 --> 00:54:30,455
Speaker 7:  They're not like you're, you were officially retired from the government

852
00:54:30,455 --> 00:54:33,695
Speaker 7:  when the folder goes into the slot. Like that's not how that works at all.

853
00:54:34,195 --> 00:54:38,095
Speaker 7:  And second, the, the facility he's describing that they've posted

854
00:54:38,095 --> 00:54:41,935
Speaker 7:  pictures of on a Doge X account is Iron

855
00:54:42,255 --> 00:54:46,135
Speaker 7:  Mountain, which is an extraordinarily famous data storage

856
00:54:46,495 --> 00:54:50,335
Speaker 7:  facility. It's, it's so famous that it was in Mr. Robot

857
00:54:50,435 --> 00:54:53,175
Speaker 7:  as a thing. They blew up. That's right.

858
00:54:54,885 --> 00:54:58,855
Speaker 7:  What are you talking about? Dude likes everybody who has ever done any

859
00:54:58,855 --> 00:55:02,375
Speaker 7:  amount of like data warehousing or storage or any of this stuff from

860
00:55:02,475 --> 00:55:06,335
Speaker 7:  across industry and government is like, dude, that's Iron Mountain. And

861
00:55:06,335 --> 00:55:10,095
Speaker 7:  there's a reason that after all of this time, we haven't

862
00:55:10,095 --> 00:55:13,335
Speaker 7:  digitized the records 'cause it's more expensive and more complicated than

863
00:55:13,335 --> 00:55:14,695
Speaker 7:  just putting paper in this mountain.

864
00:55:15,495 --> 00:55:19,175
Speaker 11:  I think it just kind of speaks to like, you know, I think there's this big

865
00:55:19,405 --> 00:55:22,615
Speaker 11:  push to eliminate a lot of these like, career

866
00:55:23,135 --> 00:55:26,975
Speaker 11:  positions in the government. And I think there's definitely legitimate

867
00:55:27,375 --> 00:55:30,415
Speaker 11:  critiques of how slow the government runs and, you know, excess.

868
00:55:31,155 --> 00:55:34,895
Speaker 11:  But at the same time, there's clearly these, you know,

869
00:55:34,895 --> 00:55:38,695
Speaker 11:  efficiencies that come from having been in the government a long time knowing

870
00:55:38,755 --> 00:55:42,695
Speaker 11:  how things work, understanding these complicated systems. And

871
00:55:42,735 --> 00:55:46,615
Speaker 11:  I think, you know, these charts also kind of give you a sense of how, like

872
00:55:47,085 --> 00:55:50,735
Speaker 11:  just how much context matters for these numbers. Like you can say there's

873
00:55:50,735 --> 00:55:54,455
Speaker 11:  millions of people in the US government, and that sounds

874
00:55:54,535 --> 00:55:58,135
Speaker 11:  like a big number, but it's a small fraction of the US

875
00:55:58,135 --> 00:55:59,375
Speaker 11:  workforce. Yeah.

876
00:55:59,625 --> 00:55:59,975
Speaker 7:  Again,

877
00:55:59,975 --> 00:56:02,775
Speaker 8:  I I I just keep coming back to you. Like there's so much more

878
00:56:03,845 --> 00:56:07,735
Speaker 8:  information warfare happening here than I think I, I realized for a

879
00:56:07,735 --> 00:56:11,175
Speaker 8:  while that like so much of what is happening is taking advantage of the fact

880
00:56:11,175 --> 00:56:15,015
Speaker 8:  that any number with millions at the end of it sounds really big.

881
00:56:15,635 --> 00:56:19,455
Speaker 8:  And anything that is happening in the government, almost no one

882
00:56:19,455 --> 00:56:23,405
Speaker 8:  knows. And I think what is going on here is like, If

883
00:56:23,465 --> 00:56:27,165
Speaker 8:  you can say like underground limestone facility

884
00:56:27,535 --> 00:56:31,405
Speaker 8:  where things are handled by hand in, in paper folders,

885
00:56:31,585 --> 00:56:34,965
Speaker 8:  and that is just like, If you frame that in a certain way, it sounds sick.

886
00:56:35,345 --> 00:56:38,485
Speaker 8:  And If you frame that in a different way, it sounds ridiculous, right? And

887
00:56:38,485 --> 00:56:42,445
Speaker 8:  I think what we're doing is, is like this group has found that if we just

888
00:56:43,545 --> 00:56:47,485
Speaker 8:  say these things out loud in a specific way, all these things that

889
00:56:47,485 --> 00:56:50,445
Speaker 8:  anyone could have known, but no one ever bothered to suddenly become very

890
00:56:50,445 --> 00:56:53,325
Speaker 8:  important. And what they've done is they've made all of these relatively

891
00:56:53,625 --> 00:56:57,005
Speaker 8:  benign people and jobs and

892
00:56:57,115 --> 00:57:01,085
Speaker 8:  regulations suddenly seem like a big deal because that's just a thing you

893
00:57:01,085 --> 00:57:05,045
Speaker 8:  can do when you have platforms like a, the White House and

894
00:57:05,165 --> 00:57:09,005
Speaker 8:  BX. And, and so it is just a way

895
00:57:09,005 --> 00:57:12,885
Speaker 8:  for them to capitalize on taking this stuff that is not new and

896
00:57:12,885 --> 00:57:16,365
Speaker 8:  it's not interesting. And they have made it new and interesting. And I think

897
00:57:16,365 --> 00:57:19,945
Speaker 8:  that's really just fascinating. So

898
00:57:19,945 --> 00:57:23,825
Speaker 7:  Here, can I offer you the, the optimistic cue of this? Please?

899
00:57:24,305 --> 00:57:25,065
Speaker 7:  I, I'm gonna try.

900
00:57:25,155 --> 00:57:25,505
Speaker 8:  Thank

901
00:57:25,505 --> 00:57:29,465
Speaker 7:  God. I'm gonna, I'm gonna offer to you one, they've made it new and interesting.

902
00:57:29,905 --> 00:57:32,505
Speaker 7:  A lot more people are paying a lot of attention to how the government operates.

903
00:57:32,505 --> 00:57:33,745
Speaker 8:  Certainly true on

904
00:57:33,745 --> 00:57:36,745
Speaker 7:  Balance. That's probably a good thing. Are those people motivated to make

905
00:57:36,745 --> 00:57:39,305
Speaker 7:  it better or do they just wanna tear it down and have a king? I don't know

906
00:57:39,305 --> 00:57:42,065
Speaker 7:  the answer to that question, but the first part, a lot more people are interested,

907
00:57:42,065 --> 00:57:45,705
Speaker 7:  interested in how the government works. That's, that's something the second

908
00:57:45,705 --> 00:57:49,685
Speaker 7:  part that I think is true is that it, the government is very

909
00:57:49,685 --> 00:57:53,445
Speaker 7:  complicated, right? It should spend more time

910
00:57:53,575 --> 00:57:57,325
Speaker 7:  justifying the things it's doing and the money it's spending

911
00:57:57,785 --> 00:58:01,645
Speaker 7:  and communicating to the people, like why it's important to have a

912
00:58:01,805 --> 00:58:05,485
Speaker 7:  consumer financial protection bureau. Yes. And if there's one criticism of

913
00:58:05,485 --> 00:58:08,965
Speaker 7:  the Biden administration in particular that you can level, it's that

914
00:58:09,395 --> 00:58:13,325
Speaker 7:  they were horrible communicators Yes. To the point of not communicating

915
00:58:13,345 --> 00:58:16,725
Speaker 7:  at all in some cases about why they were doing anything. And their whole

916
00:58:16,725 --> 00:58:20,405
Speaker 7:  promise is you won't have to pay attention to the government anymore. And

917
00:58:20,405 --> 00:58:24,325
Speaker 7:  what you have here is, well, If you just talk about it like it's a spy movie,

918
00:58:24,725 --> 00:58:28,005
Speaker 7:  everyone will pay attention to it all the time. And so there's something

919
00:58:28,005 --> 00:58:31,685
Speaker 7:  here, right, where they're trading on an inherent curiosity,

920
00:58:32,725 --> 00:58:36,555
Speaker 7:  right, of a how a big system works. I think they're doing a lot of lying.

921
00:58:37,435 --> 00:58:41,175
Speaker 7:  And then there's just like a level of malevolent incompetence, especially

922
00:58:41,175 --> 00:58:45,135
Speaker 7:  from Elon and I I mean that seriously, like he holds himself out as a great

923
00:58:45,415 --> 00:58:48,175
Speaker 7:  engineer and then he's making very

924
00:58:49,295 --> 00:58:53,105
Speaker 7:  like, simple mistakes about how databases work, right? He's like, the

925
00:58:53,105 --> 00:58:56,865
Speaker 7:  socialist security database wasn't de-duplicated. And every database engineer

926
00:58:56,945 --> 00:59:00,895
Speaker 7:  I know is like, that's not the vocabulary word, boy, that's not what you

927
00:59:00,895 --> 00:59:03,775
Speaker 7:  do. He might be talking about normalization in the database, which is where

928
00:59:03,775 --> 00:59:06,055
Speaker 7:  you get rid of some records or you like, make sure you don't hit the same

929
00:59:06,055 --> 00:59:09,175
Speaker 7:  record twice. But even then, that's not how you would do it. And then

930
00:59:10,005 --> 00:59:13,615
Speaker 7:  like, this is, there's a reason that it works this way. And then he,

931
00:59:14,835 --> 00:59:17,755
Speaker 7:  I think he said the government doesn't use SQL and like everybody who's ever

932
00:59:17,755 --> 00:59:19,995
Speaker 7:  worked in government is like, of course they do. But it's like, here are

933
00:59:19,995 --> 00:59:22,765
Speaker 7:  the vendors, they're public, public vendors of SQL databases for the government.

934
00:59:22,865 --> 00:59:26,845
Speaker 7:  So it's like, does he not know, is he just making mistakes or is this malevolent

935
00:59:26,845 --> 00:59:30,795
Speaker 7:  incompetence where he's just telling lies? And like, we don't know the

936
00:59:30,795 --> 00:59:34,755
Speaker 7:  answers to those questions. So like there's a little bit of a, a

937
00:59:34,755 --> 00:59:38,745
Speaker 7:  thing to learn here. And honestly The Verge is, we've built a whole brand,

938
00:59:38,985 --> 00:59:42,265
Speaker 7:  13 years of this brand is like, people are curious about how complicated

939
00:59:42,265 --> 00:59:45,825
Speaker 7:  systems work. And If you are hugely nerdy about it,

940
00:59:46,125 --> 00:59:49,785
Speaker 7:  you can make entire hour long podcasts about the HDMI

941
00:59:49,985 --> 00:59:52,825
Speaker 7:  specification and people will ask you to make that every year for the rest

942
00:59:52,825 --> 00:59:56,585
Speaker 7:  of your life. Like I, I believe that people are interested and curious about

943
00:59:56,645 --> 00:59:59,465
Speaker 7:  big complicated systems and that's the good thing to learn.

944
01:00:00,525 --> 01:00:03,785
Speaker 7:  But here I think it's just been subverted in like the realest possible way.

945
01:00:04,085 --> 01:00:07,425
Speaker 8:  Lauren, what are you hearing right now from people inside of the government?

946
01:00:07,425 --> 01:00:11,305
Speaker 8:  Like there was this moment of, I think incredible like demoralization

947
01:00:11,905 --> 01:00:15,225
Speaker 8:  a couple of weeks ago and then there was some real energy and then, I don't

948
01:00:15,225 --> 01:00:17,785
Speaker 8:  know, as this goes, like what are you, what are you hearing from people who

949
01:00:17,785 --> 01:00:20,505
Speaker 8:  are being sort of hit by this in their day to day all the time?

950
01:00:20,985 --> 01:00:24,945
Speaker 11:  I, I think there's still a lot of demoralization. I think there's a

951
01:00:24,945 --> 01:00:28,865
Speaker 11:  sense that people, you know, outside of DC are

952
01:00:28,865 --> 01:00:32,745
Speaker 11:  not really fully understanding the, the scope of this and how

953
01:00:32,745 --> 01:00:36,305
Speaker 11:  it could impact them. I think, you know, in DC

954
01:00:36,355 --> 01:00:39,505
Speaker 11:  especially for federal workers or people who

955
01:00:40,095 --> 01:00:43,985
Speaker 11:  know federal workers really closely, you know, it's very

956
01:00:44,015 --> 01:00:47,505
Speaker 11:  real. It's, you know, people who are going to lose their jobs, who are going

957
01:00:47,505 --> 01:00:51,385
Speaker 11:  to be living in fear of losing their jobs. But outside

958
01:00:51,485 --> 01:00:55,265
Speaker 11:  of DC it feels like a lot of these are really kind of obscure

959
01:00:55,865 --> 01:00:59,625
Speaker 11:  agencies. It's not totally clear what's happening. It's

960
01:00:59,645 --> 01:01:03,145
Speaker 11:  not, you know, necessarily things that are gonna immediately hit people

961
01:01:03,405 --> 01:01:07,145
Speaker 11:  across the country. So I think there's this feeling of

962
01:01:07,175 --> 01:01:11,105
Speaker 11:  like kind of needing to be a canary in the coal mine and

963
01:01:11,105 --> 01:01:14,665
Speaker 11:  speak out about this, but there's also a lot of

964
01:01:14,665 --> 01:01:18,345
Speaker 11:  apprehension. Like our colleague Mia just wrote a story

965
01:01:18,435 --> 01:01:21,825
Speaker 11:  about how a lot of people are not trusting

966
01:01:22,305 --> 01:01:25,585
Speaker 11:  platforms like Facebook to share private messages.

967
01:01:26,485 --> 01:01:30,225
Speaker 11:  And you know, a lot of federal workers are turning to encrypted apps

968
01:01:30,225 --> 01:01:33,905
Speaker 11:  like Signal to communicate. You know, I've

969
01:01:34,135 --> 01:01:37,945
Speaker 11:  just, in meeting people at protests, you know, you hear like, oh,

970
01:01:38,025 --> 01:01:41,985
Speaker 11:  I just made a signal account. So I think it, it's

971
01:01:41,985 --> 01:01:45,265
Speaker 11:  something that people are really, really skittish about, but also feeling

972
01:01:45,265 --> 01:01:48,305
Speaker 11:  like maybe they really do need to get the word out now.

973
01:01:48,735 --> 01:01:52,705
Speaker 7:  Yeah. The sense that Doge in particular has taken over a

974
01:01:52,705 --> 01:01:56,665
Speaker 7:  bunch of federal IT systems. You know, Elon Musk associates

975
01:01:56,665 --> 01:01:59,625
Speaker 7:  are now the chief information officers for a bunch of agencies.

976
01:02:00,555 --> 01:02:04,185
Speaker 7:  There is an ongoing cybersecurity incident in this country. Salt

977
01:02:04,305 --> 01:02:07,905
Speaker 7:  typhoon that Chinese, a bunch of Chinese hackers of, you know,

978
01:02:09,055 --> 01:02:12,705
Speaker 7:  telecom systems. The Biden administration was like, you should switch,

979
01:02:13,125 --> 01:02:16,305
Speaker 7:  you should switch to encrypted communications because the telecom providers

980
01:02:16,305 --> 01:02:18,385
Speaker 7:  cannot systems. That's,

981
01:02:19,965 --> 01:02:23,185
Speaker 7:  is there a sense that the cybersecurity posture of the country is

982
01:02:23,595 --> 01:02:25,545
Speaker 7:  noticeably more vulnerable than before?

983
01:02:26,065 --> 01:02:29,545
Speaker 11:  I think that that's definitely a fear that, you know, first of all,

984
01:02:30,325 --> 01:02:33,985
Speaker 11:  you have these people who've been given

985
01:02:34,125 --> 01:02:37,425
Speaker 11:  access to really sensitive systems that the treasury

986
01:02:37,965 --> 01:02:41,905
Speaker 11:  at other agencies who, you know, we don't really

987
01:02:41,905 --> 01:02:45,825
Speaker 11:  know what their security clearance status is. The White House insists that

988
01:02:45,825 --> 01:02:48,785
Speaker 11:  everything's been accessed under, you know, appropriate

989
01:02:49,555 --> 01:02:53,465
Speaker 11:  legal authority. But typically it, you know, it could take

990
01:02:53,465 --> 01:02:56,265
Speaker 11:  months to get a security clearance and Right.

991
01:02:56,415 --> 01:02:59,545
Speaker 8:  Yeah. The White House says that and then everyone else who knows anything

992
01:02:59,545 --> 01:03:01,025
Speaker 8:  about this says the opposite of that.

993
01:03:01,265 --> 01:03:01,425
Speaker 7:  Yeah,

994
01:03:02,195 --> 01:03:06,185
Speaker 11:  Right, right. So, you know, just knowing that there are these,

995
01:03:06,725 --> 01:03:10,145
Speaker 11:  you know, people accessing these systems that could open up, you know, at

996
01:03:10,145 --> 01:03:14,025
Speaker 11:  least in theory a, a new kind of target for any

997
01:03:14,785 --> 01:03:18,585
Speaker 11:  malicious actors who are trying to find a way into government

998
01:03:18,585 --> 01:03:21,825
Speaker 11:  systems. And it's not theoretical that,

999
01:03:22,765 --> 01:03:26,745
Speaker 11:  you know, bad actors can enter US agency systems. There was a

1000
01:03:26,745 --> 01:03:30,505
Speaker 11:  big hack of the Office of Personnel Management several

1001
01:03:30,505 --> 01:03:34,305
Speaker 11:  years ago that exposed like millions of, you know,

1002
01:03:34,305 --> 01:03:38,025
Speaker 11:  worker and federal job applicants

1003
01:03:38,025 --> 01:03:41,985
Speaker 11:  files. So, you know, it, it's definitely something that's

1004
01:03:42,065 --> 01:03:45,945
Speaker 11:  a, a big risk. And I think, you know, you'd have to imagine that

1005
01:03:47,445 --> 01:03:51,425
Speaker 11:  bad actors in other countries that are watching this and looking for

1006
01:03:51,825 --> 01:03:55,505
Speaker 11:  a way into US systems are, you know, going to be

1007
01:03:55,525 --> 01:03:58,785
Speaker 11:  seeing maybe new potential opportunities to get in.

1008
01:03:59,005 --> 01:04:01,985
Speaker 7:  Is that a fear that's motivating the people that you're talking to? Or is

1009
01:04:01,985 --> 01:04:05,625
Speaker 7:  it more the, hey cutting the National Institute of Health will result in,

1010
01:04:05,825 --> 01:04:07,625
Speaker 7:  I dunno, medical schools kind of business? I

1011
01:04:07,625 --> 01:04:10,745
Speaker 11:  Think it depends who you're talking to. I think for just like rank and file,

1012
01:04:11,845 --> 01:04:15,705
Speaker 11:  you know, federal workers at various agencies, it's, you know, fear

1013
01:04:15,805 --> 01:04:19,465
Speaker 11:  for the, the, you know, grants that they work on

1014
01:04:19,765 --> 01:04:23,225
Speaker 11:  or, you know, the, the funding that they dole out that they think is really

1015
01:04:23,225 --> 01:04:26,705
Speaker 11:  important or the research that they're supporting

1016
01:04:27,005 --> 01:04:30,825
Speaker 11:  or things of that nature. I think, you know, a lot of people, they understand

1017
01:04:30,825 --> 01:04:34,665
Speaker 11:  the roles they have, but I think they also understand there's

1018
01:04:35,335 --> 01:04:39,025
Speaker 11:  this larger, you know, threat out there. And

1019
01:04:39,625 --> 01:04:42,905
Speaker 11:  I think federal workers are really aware that, you know, their files are

1020
01:04:42,905 --> 01:04:46,385
Speaker 11:  stored within these federal agencies too, and

1021
01:04:46,595 --> 01:04:50,305
Speaker 11:  their information is at risk. So I think that's definitely on

1022
01:04:50,305 --> 01:04:51,345
Speaker 11:  people's minds as well.

1023
01:04:52,055 --> 01:04:52,345
Speaker 7:  Yeah.

1024
01:04:52,855 --> 01:04:56,625
Speaker 8:  What do you guys think it would take to shift from that thing

1025
01:04:56,625 --> 01:05:00,305
Speaker 8:  you're talking about Lauren? Where, where to a lot of people, these

1026
01:05:00,825 --> 01:05:04,705
Speaker 8:  agencies and the work they do is sort of one step removed from their real

1027
01:05:04,705 --> 01:05:08,185
Speaker 8:  life in a way that makes it hard for this stuff to seem urgent.

1028
01:05:09,535 --> 01:05:11,905
Speaker 8:  Like there was a, there was a bunch of stuff going around the other day that

1029
01:05:11,905 --> 01:05:14,705
Speaker 8:  was like, okay, a lot of people are about to get social security checks and

1030
01:05:14,705 --> 01:05:17,285
Speaker 8:  if that gets disrupted, you're gonna get a lot of people who, who suddenly

1031
01:05:17,285 --> 01:05:20,205
Speaker 8:  have a lot of feelings about what's going on inside the government. Is it,

1032
01:05:20,205 --> 01:05:23,965
Speaker 8:  is it something like that that it would have to be like everybody, I don't

1033
01:05:23,965 --> 01:05:27,325
Speaker 8:  know, everybody just gets all of their money stolen by Elon Musk or like,

1034
01:05:27,505 --> 01:05:30,445
Speaker 8:  is there a thing short of that, that we're gonna hit that all of a sudden

1035
01:05:30,605 --> 01:05:33,125
Speaker 8:  I think makes this stuff feel a lot closer to home for people? Do you have

1036
01:05:33,125 --> 01:05:33,805
Speaker 8:  a sense of what that might be?

1037
01:05:34,155 --> 01:05:37,325
Speaker 11:  It's a good question. I mean, I do think it's gonna be something that's

1038
01:05:37,325 --> 01:05:40,925
Speaker 11:  gonna be much more tangible for people. I think right now it's really tangible

1039
01:05:41,065 --> 01:05:44,965
Speaker 11:  for federal workers or people who, you know, have friends and family who

1040
01:05:44,965 --> 01:05:48,805
Speaker 11:  are federal workers. 'cause it's their jobs. They, they might be taking

1041
01:05:48,805 --> 01:05:52,045
Speaker 11:  this deferred resignation offer. They might be just living in fear of having

1042
01:05:52,045 --> 01:05:54,645
Speaker 11:  their jobs suddenly pulled out from under them.

1043
01:05:54,965 --> 01:05:58,445
Speaker 8:  I was at a two year old's birthday party recently and it was literally the

1044
01:05:58,445 --> 01:06:01,525
Speaker 8:  only subject of conversation it like, it was, it was so bleak.

1045
01:06:01,585 --> 01:06:02,005
Speaker 11:  Oh wow.

1046
01:06:02,035 --> 01:06:03,165
Speaker 7:  Well you live in DC

1047
01:06:03,605 --> 01:06:07,405
Speaker 8:  I live in DC like I live in northern Virginia. Everyone at this party either

1048
01:06:07,405 --> 01:06:11,245
Speaker 8:  worked with or was married to someone who worked with or knew someone who

1049
01:06:11,245 --> 01:06:15,005
Speaker 8:  worked at the federal government. Like that stuff is, it hit everybody. And

1050
01:06:15,005 --> 01:06:18,685
Speaker 8:  it was, I mean, it was, it was the only thing on anybody's mind. And I think

1051
01:06:18,875 --> 01:06:21,845
Speaker 8:  that stuff is gonna start to percolate, right? Like totally the number of

1052
01:06:21,845 --> 01:06:25,405
Speaker 8:  people whose jobs are come from grants like that or whose work

1053
01:06:25,755 --> 01:06:29,725
Speaker 8:  depends on the companies who get grants like that. Like I I, there there's

1054
01:06:29,725 --> 01:06:33,565
Speaker 8:  gonna be a trickle out effect, but I feel like that part might take

1055
01:06:33,565 --> 01:06:36,965
Speaker 8:  too long and I kind of, I don't know, maybe there just needs to be one big

1056
01:06:36,965 --> 01:06:40,165
Speaker 8:  thing that happens or maybe it'll be the kind of thing you're talking about.

1057
01:06:40,515 --> 01:06:44,365
Speaker 11:  Yeah, I think it's gonna hit some people maybe in waves where,

1058
01:06:44,705 --> 01:06:48,125
Speaker 11:  you know, your company receives funding from the government that suddenly

1059
01:06:48,255 --> 01:06:51,805
Speaker 11:  isn't available or things like that. But

1060
01:06:51,995 --> 01:06:55,445
Speaker 11:  yeah, I think certainly if it's something that's more on the order of not

1061
01:06:55,445 --> 01:06:59,285
Speaker 11:  receiving a social security check-in time, not receiving disability, not,

1062
01:07:00,185 --> 01:07:04,005
Speaker 11:  you know, getting your tax return, all those sorts of things would be much

1063
01:07:04,005 --> 01:07:07,925
Speaker 11:  more tangible to people. But you know, maybe there's something short

1064
01:07:07,925 --> 01:07:11,565
Speaker 11:  of that. Maybe there's some agency that

1065
01:07:12,355 --> 01:07:15,965
Speaker 11:  Musk tries to go into that just feels like a bridge too far for people.

1066
01:07:16,305 --> 01:07:20,245
Speaker 11:  Or, you know, people get a better understanding of how their data

1067
01:07:20,415 --> 01:07:24,405
Speaker 11:  might be used in a really tangible way that's just too scary.

1068
01:07:25,625 --> 01:07:29,405
Speaker 11:  Or maybe we see, you know, some of these security risks come to light.

1069
01:07:30,885 --> 01:07:34,125
Speaker 11:  I, I think it's hard. It's hard to know and it feels like everything's changing

1070
01:07:34,225 --> 01:07:38,045
Speaker 11:  so quickly that to predict it is feels like, you know,

1071
01:07:38,045 --> 01:07:40,325
Speaker 11:  we're never really gonna be able to save for sure.

1072
01:07:40,785 --> 01:07:44,245
Speaker 7:  You know, it will save the economy though. Is the state Department buying

1073
01:07:44,245 --> 01:07:46,165
Speaker 7:  $400 million for the armored testers?

1074
01:07:48,095 --> 01:07:49,305
Speaker 8:  They're gonna save us all, all

1075
01:07:50,905 --> 01:07:54,305
Speaker 7:  Everyone gets a weird armored cyber track. This story is very strange. Is

1076
01:07:54,305 --> 01:07:58,065
Speaker 7:  it, it's one of the funnier moments, I guess of all this,

1077
01:07:58,255 --> 01:08:01,465
Speaker 7:  this weird computer coup that's occurring. That's what David called it in

1078
01:08:01,465 --> 01:08:04,145
Speaker 7:  our headline last week. And I'm just sticking with it. Computer coup is very

1079
01:08:04,145 --> 01:08:07,945
Speaker 7:  good, but you know, we're, we're doing fake transparency. We're reading all

1080
01:08:07,945 --> 01:08:10,505
Speaker 7:  the contracts we can find everywhere in the government and a site called

1081
01:08:10,505 --> 01:08:14,425
Speaker 7:  Drop Site News found State Department order

1082
01:08:14,765 --> 01:08:18,325
Speaker 7:  for $400 million worth of armored Teslas in its

1083
01:08:18,325 --> 01:08:21,565
Speaker 7:  2025 procurement forecast. And they tweeted it and then everyone got mad

1084
01:08:21,635 --> 01:08:25,525
Speaker 7:  at Elon and then suddenly the State Department changed it

1085
01:08:26,355 --> 01:08:30,245
Speaker 7:  to armored electric vehicles. Which is great because I love the idea of an

1086
01:08:30,425 --> 01:08:34,205
Speaker 7:  armored VW ID buzz just like floating down the streets of

1087
01:08:34,205 --> 01:08:35,005
Speaker 7:  some bombed out city.

1088
01:08:36,835 --> 01:08:37,605
Speaker 7:  It's just, but

1089
01:08:37,605 --> 01:08:38,765
Speaker 8:  It's yellow so it's charming.

1090
01:08:39,835 --> 01:08:43,725
Speaker 7:  It's very good. Who hasn't wanted like an armored Mustang

1091
01:08:43,925 --> 01:08:47,285
Speaker 7:  Mach e like I think about it all the time. So then

1092
01:08:47,635 --> 01:08:49,965
Speaker 7:  Elon tweeted, I haven't heard about this, which is very funny.

1093
01:08:51,875 --> 01:08:55,045
Speaker 7:  Like just generally funny 'cause what, what do you think he's hearing about

1094
01:08:55,045 --> 01:08:58,565
Speaker 7:  right now about SpaceX and Tesla? He's busy. He is, he is like

1095
01:08:58,565 --> 01:09:00,765
Speaker 7:  rolling into federal offices and firing everybody.

1096
01:09:02,685 --> 01:09:05,585
Speaker 7:  So then the state department finally clarified today

1097
01:09:06,845 --> 01:09:10,305
Speaker 7:  the plan stemmed from a request by the Biden administration to explore interest

1098
01:09:10,305 --> 01:09:14,145
Speaker 7:  from private companies to produce armored electric vehicles. So it's real,

1099
01:09:14,795 --> 01:09:18,625
Speaker 7:  right? They, they did put $400 million worth of armored Teslas on

1100
01:09:18,625 --> 01:09:22,185
Speaker 7:  a sheet because they, they wanted to, sure. And then they said the

1101
01:09:22,185 --> 01:09:25,755
Speaker 7:  solicitation is on hold and there are no current plans to issue it. So

1102
01:09:26,495 --> 01:09:29,765
Speaker 7:  it worked. Everyone yelled at Elon about

1103
01:09:30,535 --> 01:09:34,465
Speaker 7:  cutting veterans' benefits to buy cyber trucks. By

1104
01:09:34,465 --> 01:09:36,745
Speaker 7:  the way, we don't even know if they're cybert trucks. Just an armored. Teslas

1105
01:09:37,235 --> 01:09:39,745
Speaker 7:  could have been, could have been armored model threes, you dunno, a bunch

1106
01:09:39,745 --> 01:09:39,865
Speaker 7:  of

1107
01:09:39,865 --> 01:09:40,425
Speaker 8:  Roadsters,

1108
01:09:42,045 --> 01:09:45,745
Speaker 7:  Who knows. Just one of the weirder moments here, right? Where it's like the

1109
01:09:45,745 --> 01:09:48,865
Speaker 7:  self-dealing for Elon can hit like

1110
01:09:49,395 --> 01:09:53,185
Speaker 7:  maybe Pete Buttigieg is, was like, we should have armored EVs.

1111
01:09:53,405 --> 01:09:57,065
Speaker 7:  The next presidential limo should be an armored Escalade

1112
01:09:57,245 --> 01:10:01,055
Speaker 7:  iq. You don't know, but it said armored Teslas and they

1113
01:10:01,055 --> 01:10:04,095
Speaker 7:  changed it and then they said it was going away. So even that little bit

1114
01:10:04,095 --> 01:10:06,655
Speaker 7:  of pressure seems to be working, which I think is really interesting.

1115
01:10:06,765 --> 01:10:09,455
Speaker 8:  Yeah, I I would encourage people, we, we should put some links in the show

1116
01:10:09,455 --> 01:10:12,495
Speaker 8:  notes, but the, there's, there's some really good reporting being done about

1117
01:10:13,195 --> 01:10:15,735
Speaker 8:  the ways in which all of these changes are

1118
01:10:17,395 --> 01:10:21,255
Speaker 8:  partly chaotic and partly about Elon Musk going after things that might

1119
01:10:21,255 --> 01:10:25,135
Speaker 8:  have oversight of him and also putting more things toward

1120
01:10:25,635 --> 01:10:28,455
Speaker 8:  SpaceX. And then he gives, you know, speeches in the Oval Office about how

1121
01:10:29,305 --> 01:10:32,575
Speaker 8:  everything, every contract they've gotten is what's best for the taxpayer

1122
01:10:32,715 --> 01:10:36,215
Speaker 8:  and like sure. But it is like the, the

1123
01:10:36,215 --> 01:10:39,895
Speaker 8:  self-dealing here I think is becoming more obvious and

1124
01:10:40,955 --> 01:10:44,215
Speaker 8:  he just has no interest in hiding it and no reason to hide it. And in

1125
01:10:44,215 --> 01:10:48,095
Speaker 7:  Fact, I think his own financial disclosure is not subject to

1126
01:10:48,255 --> 01:10:52,135
Speaker 7:  a records request. Of course. Yeah. Yeah. That's very

1127
01:10:52,135 --> 01:10:52,295
Speaker 7:  good.

1128
01:10:52,675 --> 01:10:56,535
Speaker 8:  But yeah, it is like, it it is. I I like I said

1129
01:10:56,535 --> 01:10:59,295
Speaker 8:  this to Liz on Tuesday show, like I want it to be more complicated than that

1130
01:10:59,295 --> 01:11:03,015
Speaker 8:  and I'm increasingly coming around to, it's just, it, it's

1131
01:11:03,215 --> 01:11:07,095
Speaker 8:  just that simple. Like Elon Musk bought the government so

1132
01:11:07,095 --> 01:11:09,255
Speaker 8:  that he can solve all of his business problems for himself.

1133
01:11:10,215 --> 01:11:12,775
Speaker 7:  I mean it's, it, it's a good deal If you can get it. That's what I'm saying.

1134
01:11:12,775 --> 01:11:15,445
Speaker 7:  It only cost two cost million. Like it was pretty easy. Like

1135
01:11:15,825 --> 01:11:19,125
Speaker 8:  He bought the government for what Jeff Bezos bought the Washington Post for.

1136
01:11:19,125 --> 01:11:19,685
Speaker 8:  Think about that. Yeah.

1137
01:11:20,475 --> 01:11:21,565
Speaker 7:  Meta spent more. I'd rather

1138
01:11:21,565 --> 01:11:22,045
Speaker 8:  Have the government,

1139
01:11:23,355 --> 01:11:26,525
Speaker 7:  It's just like straightforwardly Instagram costs more money than the United

1140
01:11:26,525 --> 01:11:26,845
Speaker 7:  States government.

1141
01:11:30,805 --> 01:11:33,205
Speaker 8:  WhatsApp costs 80 times what the government

1142
01:11:33,215 --> 01:11:36,525
Speaker 7:  Costs. I was gonna say we have to take a break, but David has a section here

1143
01:11:36,525 --> 01:11:40,365
Speaker 7:  labeled Deep Sigh, which is all just links about the Gulf

1144
01:11:40,365 --> 01:11:42,165
Speaker 7:  of America. David, go ahead.

1145
01:11:42,965 --> 01:11:46,885
Speaker 8:  I just, I did so, okay. The, the short version of the

1146
01:11:46,885 --> 01:11:50,725
Speaker 8:  story here is Donald Trump has decided that it's not the Gulf of Mexico,

1147
01:11:50,725 --> 01:11:54,005
Speaker 8:  it's the, it's the Gulf of America. And one by one the mapping companies

1148
01:11:54,075 --> 01:11:57,445
Speaker 8:  have, have given in and now on Google Maps, it's called the Gulf of America

1149
01:11:57,445 --> 01:12:00,605
Speaker 8:  and Apple Maps, it's called the Gulf of America and Bing Maps, which kudos

1150
01:12:00,605 --> 01:12:03,165
Speaker 8:  to the reporter who checked because no one in history has ever used Bing

1151
01:12:03,165 --> 01:12:07,085
Speaker 7:  Maps. Wait, actually Tom checked and they had it labeled twice in like classic

1152
01:12:07,275 --> 01:12:10,885
Speaker 7:  Bing Maps fashion. It's both, it was both for a minute and then it was

1153
01:12:11,115 --> 01:12:14,765
Speaker 7:  Gulf of America twice and then they finally went just to Gulf of America.

1154
01:12:14,955 --> 01:12:18,445
Speaker 8:  Yeah. So the the one intern who maintains big maps. Yeah.

1155
01:12:18,825 --> 01:12:22,805
Speaker 8:  Got it done. Lauren, I'm particularly curious about this because this is

1156
01:12:22,805 --> 01:12:25,325
Speaker 8:  a thing that has been covered a bunch and it's been talked about a bunch

1157
01:12:25,325 --> 01:12:29,285
Speaker 8:  and it like became a thing because Trump wouldn't allow

1158
01:12:29,285 --> 01:12:33,245
Speaker 8:  the AP access because it kept calling it the Gulf of Mexico and

1159
01:12:33,245 --> 01:12:37,165
Speaker 8:  not the Gulf of America. Like part of me is like, this is the stupidest

1160
01:12:37,235 --> 01:12:40,565
Speaker 8:  sort of nadier of what's actually going on right now, but it also feels like

1161
01:12:40,715 --> 01:12:44,365
Speaker 8:  this has become such a thing in the Trump

1162
01:12:44,365 --> 01:12:46,645
Speaker 8:  administration in a way that I just cannot figure out.

1163
01:12:48,165 --> 01:12:51,845
Speaker 11:  I I feel like it's just, you know, they're doing a lot of things

1164
01:12:51,915 --> 01:12:55,365
Speaker 11:  that are very impactful, like dismantling entire

1165
01:12:55,765 --> 01:12:59,725
Speaker 11:  agencies and then they're also doing these things that are kind

1166
01:12:59,725 --> 01:13:03,605
Speaker 11:  of like, alright, If you don't agree with it eye roll. But it doesn't really

1167
01:13:03,625 --> 01:13:07,605
Speaker 11:  change all that much to change the Gulf of Mexico to Gulf of America.

1168
01:13:08,185 --> 01:13:12,005
Speaker 11:  But it creates all of this like just anger around

1169
01:13:12,025 --> 01:13:15,685
Speaker 11:  it and you know, all of this like distraction. I think

1170
01:13:16,395 --> 01:13:19,885
Speaker 11:  when we have, you know, a lot of other things going on and I think it's

1171
01:13:19,885 --> 01:13:23,045
Speaker 11:  just all part of this like flood the zone strategy.

1172
01:13:24,075 --> 01:13:27,885
Speaker 8:  Yeah, that's fair. Like the, who is it, buddy Carter from Georgia who introduced

1173
01:13:27,885 --> 01:13:31,845
Speaker 8:  the bill to rename Greenland to red, white, and blue Land, which is just

1174
01:13:32,525 --> 01:13:35,165
Speaker 8:  a absurd, and b like a terrible name.

1175
01:13:35,425 --> 01:13:38,205
Speaker 7:  If you come on like elect me to Congress, I will do shit like this every

1176
01:13:38,205 --> 01:13:42,125
Speaker 7:  day. Do you understand? Vote Patel? I'm actually, I'm

1177
01:13:42,125 --> 01:13:46,085
Speaker 7:  not running anywhere. I'm not even sure, you know, at this point merely

1178
01:13:46,085 --> 01:13:49,605
Speaker 7:  suggesting that I'm actually running for office might send me to jail. But

1179
01:13:49,605 --> 01:13:51,965
Speaker 7:  If you send me America every day,

1180
01:13:53,565 --> 01:13:56,705
Speaker 8:  I'm not mad at some, some silliness and like, let's, let's make Fortnite

1181
01:13:57,125 --> 01:14:00,905
Speaker 8:  the official video game of the United States Congress. Like

1182
01:14:01,305 --> 01:14:04,785
Speaker 8:  require everyone if if a, if instead of having, you know, the,

1183
01:14:05,485 --> 01:14:09,305
Speaker 8:  the header, the Secretary of Defense pick anything. Like just whoever

1184
01:14:09,305 --> 01:14:11,445
Speaker 8:  wins Fortnite, they get, they get to choose the

1185
01:14:11,445 --> 01:14:13,845
Speaker 7:  Gulf of America thing, by the way is very, it's very silly, but it's also

1186
01:14:13,845 --> 01:14:17,045
Speaker 7:  like, you know, it's a virgin its way because who gets to name things

1187
01:14:18,475 --> 01:14:21,495
Speaker 7:  in Google Maps is a somewhat open question.

1188
01:14:22,425 --> 01:14:26,215
Speaker 7:  Trump did issue an executive order instructing the mapping

1189
01:14:26,695 --> 01:14:30,175
Speaker 7:  agencies in the United States to rename these things. That's where Google

1190
01:14:30,245 --> 01:14:33,535
Speaker 7:  Maps follows the instructions. It's weird 'cause they, some of them have

1191
01:14:33,535 --> 01:14:37,415
Speaker 7:  geofenced it and some of 'em haven't. Yeah. So on some of these mapping platforms,

1192
01:14:37,735 --> 01:14:41,135
Speaker 7:  only American users see Gulf America and worldwide users see Gulf of Mexico,

1193
01:14:41,195 --> 01:14:45,065
Speaker 7:  but the press doesn't have to follow those rules. Right.

1194
01:14:45,065 --> 01:14:48,265
Speaker 7:  It just like, not a, like the tech platform is like whatever the government

1195
01:14:48,265 --> 01:14:51,785
Speaker 7:  says, like we just do the thing, like you change it in the

1196
01:14:52,325 --> 01:14:55,185
Speaker 7:  database, France is now called stupid. Like that's just the way they're gonna

1197
01:14:55,185 --> 01:14:55,905
Speaker 7:  do it. Yeah. But

1198
01:14:55,905 --> 01:14:59,425
Speaker 8:  It is, it, it, that in itself is a useful reminder of how all of this stuff

1199
01:14:59,705 --> 01:15:02,705
Speaker 8:  actually works. Right? Right. Like we, we think of this stuff as, as sort

1200
01:15:02,705 --> 01:15:06,625
Speaker 8:  of meaningfully authoritative in some way and it just isn't like Yeah,

1201
01:15:06,625 --> 01:15:06,865
Speaker 8:  it just

1202
01:15:07,025 --> 01:15:09,585
Speaker 7:  Isn't, it's just, it's just words in a database. Yeah. But the press, like

1203
01:15:09,585 --> 01:15:13,345
Speaker 7:  the ap, they're like, look, we also service customers in Mexico

1204
01:15:13,565 --> 01:15:17,545
Speaker 7:  in our stories and they haven't changed it, so we're gonna go

1205
01:15:17,545 --> 01:15:21,425
Speaker 7:  with both or whatever is more clear and that, that's

1206
01:15:21,715 --> 01:15:25,355
Speaker 7:  weird, right? You, it's, it's interesting to see that the press

1207
01:15:26,015 --> 01:15:29,155
Speaker 7:  has a much more nuanced perspective of how to talk about things

1208
01:15:29,695 --> 01:15:33,475
Speaker 7:  versus the tech platforms which are kind of like, you can boss this around

1209
01:15:33,735 --> 01:15:35,195
Speaker 7:  by just changing your databases.

1210
01:15:36,255 --> 01:15:39,655
Speaker 11:  I think that that also shows how like, even though this is like, you know,

1211
01:15:40,085 --> 01:15:44,015
Speaker 11:  kind of a silly like name change that's not gonna impact a whole

1212
01:15:44,015 --> 01:15:47,455
Speaker 11:  lot. It is also becoming a tool for the Trump

1213
01:15:47,455 --> 01:15:51,095
Speaker 11:  administration to say, Hey ap, If you don't follow

1214
01:15:51,885 --> 01:15:55,615
Speaker 11:  what we're saying, then we're gonna not let you into the

1215
01:15:55,705 --> 01:15:56,895
Speaker 11:  White House press room. So

1216
01:15:56,955 --> 01:15:59,975
Speaker 7:  By the way, today, just now, third day in a row, the White House barred the

1217
01:15:59,975 --> 01:16:02,335
Speaker 7:  AP from attending an event because they won't habituate.

1218
01:16:03,055 --> 01:16:04,045
Speaker 8:  There you go. Right?

1219
01:16:04,045 --> 01:16:07,085
Speaker 7:  Yeah. And that's just a straightforward first amendment violation. Like

1220
01:16:07,895 --> 01:16:11,515
Speaker 7:  you what whatever you think about how silly this is. The government punishing

1221
01:16:11,695 --> 01:16:15,635
Speaker 7:  an editorial outlet for their speech. Straightforward First Amendment violation.

1222
01:16:16,025 --> 01:16:16,315
Speaker 7:  Yeah.

1223
01:16:17,345 --> 01:16:20,165
Speaker 8:  By the way, just in case you're wondering, MapQuest still calls it the Gulf

1224
01:16:20,165 --> 01:16:20,525
Speaker 8:  of Mexico.

1225
01:16:21,795 --> 01:16:22,845
Speaker 7:  Well they're on notice

1226
01:16:22,845 --> 01:16:23,965
Speaker 8:  Now. Big Officer MapQuest.

1227
01:16:24,435 --> 01:16:26,685
Speaker 7:  Look, you know Brendan Carlos since the show, what are you doing?

1228
01:16:28,415 --> 01:16:28,835
Speaker 7:  He does,

1229
01:16:30,735 --> 01:16:34,225
Speaker 7:  even though I'm trying to get him to stop by calling him up by name every

1230
01:16:34,225 --> 01:16:34,505
Speaker 7:  week

1231
01:16:36,805 --> 01:16:40,155
Speaker 7:  there. Look, it's a, it's a silly example, Lauren. I, I think I agree with

1232
01:16:40,155 --> 01:16:44,075
Speaker 7:  you, but it's such a loyalty test now that

1233
01:16:45,145 --> 01:16:48,965
Speaker 7:  it just, it just every time, I don't know what I, I, I can't

1234
01:16:49,085 --> 01:16:51,125
Speaker 7:  remember the last time the word wrote about the Gulf of Mexico or the Gulf

1235
01:16:51,125 --> 01:16:54,965
Speaker 7:  of America or whatever, but like editorially we're gonna say both because

1236
01:16:54,985 --> 01:16:58,405
Speaker 7:  we have readers around the world and like not everyone has agreed

1237
01:16:59,115 --> 01:17:02,845
Speaker 7:  with this like, weird metaphysical name change. Maybe

1238
01:17:02,845 --> 01:17:04,125
Speaker 7:  we're just gonna start calling it the body

1239
01:18:57,985 --> 01:19:01,755
Speaker 7:  All right, we're back with the lighting round unsponsored. So

1240
01:19:01,755 --> 01:19:05,355
Speaker 7:  extra spicy. But I do wanna begin with a bit of a pallet

1241
01:19:05,355 --> 01:19:08,195
Speaker 7:  cleanser and a lightning round. That was a lot of politics. Talk

1242
01:19:08,265 --> 01:19:12,155
Speaker 8:  Unsponsored for flavor by the way, is a phrase a surprising number of people

1243
01:19:12,155 --> 01:19:15,195
Speaker 8:  have sent to me over the last week. I still don't know what it means, but

1244
01:19:15,195 --> 01:19:18,755
Speaker 8:  I like it. You said it, it means nothing, but I like it very much.

1245
01:19:19,095 --> 01:19:22,235
Speaker 7:  It doesn't mean anything If you sponsor us, you get no control. It remains

1246
01:19:22,235 --> 01:19:25,875
Speaker 7:  as spicy as before, which I think If you would like to give us money is a

1247
01:19:25,875 --> 01:19:26,675
Speaker 7:  reason to give us money.

1248
01:19:27,095 --> 01:19:30,475
Speaker 8:  Here's my idea is I think we should make it like a skate shirt for

1249
01:19:30,705 --> 01:19:34,315
Speaker 8:  unsponsored skaters who are like, I'm too cool to be sponsored.

1250
01:19:35,035 --> 01:19:39,025
Speaker 8:  And you just say Unsponsored for flavor. That's my idea. Yeah, it's

1251
01:19:39,105 --> 01:19:41,545
Speaker 8:  gonna be great. VERGE merch coming, coming soon

1252
01:19:43,565 --> 01:19:45,785
Speaker 7:  As esoteric and incomprehensible as ever.

1253
01:19:46,055 --> 01:19:48,545
Speaker 8:  Exactly. But yeah, we have, we have some gadget news.

1254
01:19:48,655 --> 01:19:52,225
Speaker 7:  There's gadget news. Hooray. And to, in my mind,

1255
01:19:52,375 --> 01:19:55,505
Speaker 7:  very important gadget news, which is that there's a rumor,

1256
01:19:56,125 --> 01:19:59,825
Speaker 7:  pretty sketchy rumor actually, that the Apple Studio

1257
01:19:59,825 --> 01:20:03,745
Speaker 7:  Display two will have proper mini LED backlighting. This is what

1258
01:20:03,745 --> 01:20:06,665
Speaker 7:  you wanna start with. I, there's nothing more important in my life

1259
01:20:07,595 --> 01:20:11,165
Speaker 7:  than the fact that buying a 27 inch 5K monitor right now

1260
01:20:11,785 --> 01:20:15,565
Speaker 7:  is a nightmare. It's like willingly being

1261
01:20:15,805 --> 01:20:18,925
Speaker 7:  mugged. That's, that's the only way you can describe this, especially if

1262
01:20:18,925 --> 01:20:22,365
Speaker 7:  you're mean, you've got two old 27 inch IMAX line around and you really wanna

1263
01:20:22,365 --> 01:20:26,085
Speaker 7:  retrofit them into being monitors. But you've watched too many videos of

1264
01:20:26,085 --> 01:20:29,445
Speaker 7:  people shattering the glass in the front trying to Yeah, trying to open 'em

1265
01:20:29,445 --> 01:20:33,315
Speaker 7:  up every day. I look at one, I'm like, I'm gonna screw

1266
01:20:33,315 --> 01:20:36,995
Speaker 7:  this up so bad. But we're one day I'm gonna gin myself into doing it. So

1267
01:20:37,435 --> 01:20:40,555
Speaker 7:  I it's, I I'm gonna, I'm I'm gonna buy a Mac mini. I'm, I'm, I've convinced

1268
01:20:40,555 --> 01:20:44,075
Speaker 7:  myself I'm buy an M four Mac mini and then you're like, I gotta buy a display.

1269
01:20:44,375 --> 01:20:48,235
Speaker 7:  And I've been staring at 27 inch Trimac for years, a decade. So

1270
01:20:48,255 --> 01:20:52,055
Speaker 7:  not compromising. Right? But I want a 27 inch five a display.

1271
01:20:52,115 --> 01:20:55,495
Speaker 7:  And the only thing that makes any sense is the Apple Studio display

1272
01:20:55,565 --> 01:20:59,495
Speaker 7:  $1,500 for exactly the same panel I'm looking at

1273
01:20:59,495 --> 01:20:59,655
Speaker 7:  now.

1274
01:21:01,275 --> 01:21:01,995
Speaker 7:  The worst.

1275
01:21:03,615 --> 01:21:07,395
Speaker 7:  There's the Samsung riff, which is

1276
01:21:07,395 --> 01:21:10,195
Speaker 7:  worse. Like it just doesn't work with MAs as well and people Yeah, it's,

1277
01:21:10,215 --> 01:21:13,675
Speaker 7:  it, it medium reviews. There's a Ben q

1278
01:21:14,805 --> 01:21:18,715
Speaker 7:  eh and they're all kind of the same money, right?

1279
01:21:18,715 --> 01:21:20,515
Speaker 7:  They're, they're within a couple hundred dollars of each other. You get

1280
01:21:20,515 --> 01:21:22,955
Speaker 8:  A Vision Pro and have a really huge monitor. You

1281
01:21:22,955 --> 01:21:26,675
Speaker 7:  Can get a Vision pro. There's a bunch of Chinese companies now

1282
01:21:26,985 --> 01:21:30,715
Speaker 7:  that are, are like, they're taking the rejected LG

1283
01:21:30,745 --> 01:21:34,675
Speaker 7:  5K panels that Apple and others won't use and putting them in

1284
01:21:34,675 --> 01:21:38,075
Speaker 7:  really nice cases and selling them for like 800 bucks. But then that's a

1285
01:21:38,075 --> 01:21:39,075
Speaker 7:  crap shoot. That's

1286
01:21:39,075 --> 01:21:40,075
Speaker 8:  The one I'm gonna end up buying.

1287
01:21:40,945 --> 01:21:44,595
Speaker 7:  It's, I mean I'm so close. All of them are the same, like,

1288
01:21:46,015 --> 01:21:49,995
Speaker 7:  you know, badly back lit bleed, no black levels

1289
01:21:50,425 --> 01:21:54,355
Speaker 7:  ancient panel, like LEDL CD panel, like it's the same panel from

1290
01:21:54,355 --> 01:21:57,995
Speaker 7:  a decade ago. Fundamentally. So if Apple puts mini LED backlighting in it

1291
01:21:58,535 --> 01:22:02,395
Speaker 7:  and it like is good, like the same way a MacBook display is

1292
01:22:02,395 --> 01:22:05,505
Speaker 7:  good, I'm in baby let's go.

1293
01:22:06,935 --> 01:22:07,215
Speaker 8:  I mean

1294
01:22:07,395 --> 01:22:10,495
Speaker 7:  You listen to me Tim Cook, I know Brendan Carr listens. I don't know if Tim,

1295
01:22:11,415 --> 01:22:12,235
Speaker 7:  I'm looking at you Buddy

1296
01:22:13,055 --> 01:22:16,625
Speaker 8:  Mac mini plus studio display is, is

1297
01:22:16,925 --> 01:22:20,185
Speaker 8:  is the jam. Like that's a pretty good, it doesn't save you

1298
01:22:20,765 --> 01:22:24,705
Speaker 8:  any money over any of the other things you could do, but it is a

1299
01:22:24,895 --> 01:22:28,625
Speaker 8:  kick ass combination of things. I would be excited about it. I

1300
01:22:29,115 --> 01:22:32,585
Speaker 8:  still think that thing is preposterously expensive and will continue to be

1301
01:22:32,585 --> 01:22:34,025
Speaker 8:  preposterously expensive. But

1302
01:22:34,025 --> 01:22:36,825
Speaker 7:  If it was preposterously expensive and also like bleeding edge technology,

1303
01:22:37,375 --> 01:22:37,865
Speaker 8:  Totally.

1304
01:22:38,045 --> 01:22:41,945
Speaker 7:  But it's a 10-year-old panel. If I could just gin up the courage and

1305
01:22:41,945 --> 01:22:45,745
Speaker 7:  literally, I mean gin up, like drink enough gin to have the courage

1306
01:22:46,245 --> 01:22:50,225
Speaker 7:  to take a razor blade to one of my old IMAX and then be fine when

1307
01:22:50,225 --> 01:22:54,085
Speaker 7:  I inevitably shatter the screen, I would do it. But I,

1308
01:22:54,085 --> 01:22:58,035
Speaker 7:  yeah, I don't like gin. It just, it's like drinking a tree, you know?

1309
01:22:58,225 --> 01:22:58,515
Speaker 7:  That

1310
01:22:58,515 --> 01:23:01,315
Speaker 8:  Is the main problem. Yeah. It's famously what I've been saying all along.

1311
01:23:01,625 --> 01:23:01,915
Speaker 8:  Yeah.

1312
01:23:03,455 --> 01:23:07,125
Speaker 7:  And bourbon is, is no kind of juice to be drinking when you're doing computer

1313
01:23:07,125 --> 01:23:07,365
Speaker 7:  work.

1314
01:23:08,285 --> 01:23:09,085
Speaker 8:  Absolutely not. Yeah.

1315
01:23:09,085 --> 01:23:11,125
Speaker 7:  Like slurring. You're mad belligerent.

1316
01:23:11,915 --> 01:23:14,645
Speaker 8:  Okay, so for the rest of us here in the real world though, the other Apple

1317
01:23:14,655 --> 01:23:18,365
Speaker 8:  rumor and I would say this has graduated beyond rumor

1318
01:23:18,465 --> 01:23:21,365
Speaker 8:  and is more like thing that is going to happen next week.

1319
01:23:22,745 --> 01:23:26,085
Speaker 8:  Tim Cook posted, get ready to meet the newest member of the family

1320
01:23:26,675 --> 01:23:30,645
Speaker 8:  just posted that on X with a really like air tag looking Apple

1321
01:23:30,675 --> 01:23:34,485
Speaker 8:  logo, which was weird. But the overwhelming theory is that that's gonna be

1322
01:23:34,485 --> 01:23:38,365
Speaker 8:  the new iPhone SE I would like to be on the record of saying

1323
01:23:38,945 --> 01:23:39,805
Speaker 8:  no one cares.

1324
01:23:41,425 --> 01:23:45,205
Speaker 8:  The iPhone SE is the phone that everyone talks about the most and

1325
01:23:45,605 --> 01:23:48,925
Speaker 8:  absolutely no one buys. It's like the overwhelming truth about these devices.

1326
01:23:48,925 --> 01:23:52,685
Speaker 8:  Everybody says they want the tiny iPhone and then Apple's like here's a tiny

1327
01:23:52,685 --> 01:23:56,325
Speaker 8:  iPhone and no one buys it. This is just what happens.

1328
01:23:56,915 --> 01:24:00,605
Speaker 7:  Well a lot of people buy the iPhone. SE do they They do keep making

1329
01:24:00,885 --> 01:24:03,325
Speaker 7:  'cause it's the cheap one. And really the problem, the iPhone I see right

1330
01:24:03,325 --> 01:24:06,845
Speaker 7:  now is it's the last one with a button. Yeah. So if they add face ID to it,

1331
01:24:06,845 --> 01:24:08,765
Speaker 7:  then it will kind of look like a more expensive one.

1332
01:24:08,945 --> 01:24:12,685
Speaker 8:  Oh, that's an interesting point. Yeah. The, the rumor is bigger screen,

1333
01:24:14,305 --> 01:24:17,605
Speaker 8:  no button face id. So it would just be a cheaper

1334
01:24:18,315 --> 01:24:21,205
Speaker 8:  regular iPhone. Which is odd because at some point

1335
01:24:22,145 --> 01:24:25,965
Speaker 8:  is the SE just going to be a rebadged version of like the two

1336
01:24:25,965 --> 01:24:28,365
Speaker 8:  generations ago iPhone. And if so, what is the point?

1337
01:24:28,545 --> 01:24:32,245
Speaker 7:  That's all it ever is. And but they'll they'll increase the RAM so that you

1338
01:24:32,245 --> 01:24:35,605
Speaker 7:  can do Apple intelligence. Right. Extraordinarily useful. Everyone loves

1339
01:24:35,605 --> 01:24:37,845
Speaker 7:  that Apple intelligence. It's, yeah, right.

1340
01:24:38,235 --> 01:24:42,085
Speaker 8:  It's just, it's it's just a way to sell you a cheaper older phone

1341
01:24:42,105 --> 01:24:45,725
Speaker 8:  as if it's a new phone. Yeah. Which is like, I don't know, strikes me as

1342
01:24:46,485 --> 01:24:50,325
Speaker 8:  probably smart on apple's part, but is so calculated. I just, I don't like

1343
01:24:50,325 --> 01:24:50,405
Speaker 8:  it.

1344
01:24:50,675 --> 01:24:52,925
Speaker 7:  It's funny that you led into this being like the thing that everyone actually

1345
01:24:52,925 --> 01:24:55,405
Speaker 7:  cares about and now you're like, this is garbage. Get outta my face. I'm

1346
01:24:55,405 --> 01:24:59,045
Speaker 7:  telling you the studio display too with mini LED. This is a blockbuster product.

1347
01:25:00,315 --> 01:25:03,885
Speaker 7:  Blockbuster. Like it's lines around the corner.

1348
01:25:04,585 --> 01:25:06,565
Speaker 8:  Do you think the camera's still gonna suck? I

1349
01:25:06,565 --> 01:25:10,525
Speaker 7:  Don't care. I I put tape over that camera anyway because I use a ZV

1350
01:25:10,525 --> 01:25:12,285
Speaker 7:  one as a webcam, so I don't want to get confused.

1351
01:25:12,945 --> 01:25:13,445
Speaker 8:  That's fair.

1352
01:25:13,785 --> 01:25:16,965
Speaker 7:  It doesn't matter to me. I just want a great, If you can think of a better

1353
01:25:16,965 --> 01:25:19,925
Speaker 7:  way to get a 27 inch 5K display. I think what I'm gonna do now is I'm gonna

1354
01:25:19,925 --> 01:25:23,765
Speaker 7:  buy the info MAC mini. I'll buy a super cheap garbage monitor to just

1355
01:25:23,765 --> 01:25:26,685
Speaker 7:  hold me over until the studio display two comes up. Or I'm gonna take a rai,

1356
01:25:26,745 --> 01:25:27,965
Speaker 7:  I'm gonna liquor up, take

1357
01:25:27,965 --> 01:25:30,125
Speaker 8:  A razor. Think it's time for the razor blade. I think we're gonna, you gotta

1358
01:25:30,125 --> 01:25:33,925
Speaker 8:  turn on the ZV one. You gotta point it at the iMac and listen,

1359
01:25:34,265 --> 01:25:37,935
Speaker 8:  If you destroy it, it's good content. No's all upside's.

1360
01:25:38,005 --> 01:25:41,215
Speaker 8:  It's great content because you'll get injured and it'll be so funny

1361
01:25:43,395 --> 01:25:46,735
Speaker 7:  Is great. What, what? YouTube. YouTube wants blood is what, what you're saying?

1362
01:25:47,125 --> 01:25:48,375
Speaker 8:  Yeah, pretty much. Alright.

1363
01:25:48,445 --> 01:25:52,215
Speaker 7:  This is one of my favorite stories a week. 'cause it's so silly. So Jeep

1364
01:25:52,475 --> 01:25:55,575
Speaker 7:  is in turmoil. Stellantis is in turmoil as a car company.

1365
01:25:57,005 --> 01:26:00,755
Speaker 7:  Their products got too expensive, they forgot to make new products. Like

1366
01:26:00,755 --> 01:26:03,725
Speaker 7:  they were just like, what if we didn't make New Chryslers for 10 years? And

1367
01:26:03,725 --> 01:26:07,445
Speaker 7:  they're like, crap, we make the same minivan and no other cars weird. Just

1368
01:26:07,525 --> 01:26:11,365
Speaker 7:  a weird way to run a company. And then they were like, what if we made a

1369
01:26:11,365 --> 01:26:14,485
Speaker 7:  hundred thousand dollars Jeep Grand Wagoneers? And everyone was like, why?

1370
01:26:14,665 --> 01:26:18,655
Speaker 7:  And they're like, we don't know why Company in crisis. So now they're gonna

1371
01:26:18,655 --> 01:26:22,095
Speaker 7:  roll out some new EVs. They got the new Wagoner s which is slowly rolling

1372
01:26:22,095 --> 01:26:24,415
Speaker 7:  out to dealerships. People like it, they don't like it. There's some bad

1373
01:26:24,415 --> 01:26:28,055
Speaker 7:  YouTuber reviews. We'll see they're gonna

1374
01:26:28,555 --> 01:26:32,255
Speaker 7:  launch the new thing called the Recon ev, which is basically an EV version

1375
01:26:32,255 --> 01:26:36,015
Speaker 7:  of the Jeep Wrangler. Which I think might be cool. Like we can take the doors

1376
01:26:36,015 --> 01:26:39,495
Speaker 7:  off. Might be great. Yeah. If it has the same weird problems as a wagon air

1377
01:26:39,495 --> 01:26:43,465
Speaker 7:  ass, we'll see. But then on top of

1378
01:26:43,465 --> 01:26:47,105
Speaker 7:  this, they're doing the thing that every car maker has

1379
01:26:47,265 --> 01:26:50,715
Speaker 7:  threatened to do for so long and just hasn't done.

1380
01:26:51,315 --> 01:26:55,165
Speaker 7:  They're putting ads in the infotainment, like real

1381
01:26:55,305 --> 01:26:59,125
Speaker 7:  ads. Like every time you stop a car running,

1382
01:26:59,145 --> 01:27:02,965
Speaker 7:  you connect for four, a popup comes up that says do you want to

1383
01:27:02,985 --> 01:27:04,365
Speaker 7:  buy the extended warranty?

1384
01:27:04,905 --> 01:27:05,885
Speaker 8:  Are you serious? Like

1385
01:27:05,885 --> 01:27:06,405
Speaker 7:  Full screen.

1386
01:27:06,985 --> 01:27:08,565
Speaker 8:  That's like a bit from a movie.

1387
01:27:08,875 --> 01:27:12,845
Speaker 7:  Like every time you stop the car, oh my god it's very bad. People hate it.

1388
01:27:13,525 --> 01:27:17,125
Speaker 7:  It says purchase peace of mind. And it says you can call to purchase a

1389
01:27:17,165 --> 01:27:20,685
Speaker 7:  FlexCare extended premium plan If you have less than

1390
01:27:20,685 --> 01:27:24,325
Speaker 7:  36,000 miles than eight. Which the car should know by the way.

1391
01:27:24,995 --> 01:27:28,165
Speaker 7:  Yeah. And then If you just click X to close, it comes back the next time

1392
01:27:28,165 --> 01:27:30,845
Speaker 7:  you stop it. This is obviously some kind of weird bug, but it's obviously

1393
01:27:30,875 --> 01:27:34,845
Speaker 7:  also a deal with both the, the insurance company with like serious xm which

1394
01:27:34,845 --> 01:27:38,485
Speaker 7:  runs the thing. This is a nightmare. Like this is

1395
01:27:38,625 --> 01:27:40,245
Speaker 7:  how you destroy your brand for good.

1396
01:27:40,955 --> 01:27:44,325
Speaker 8:  Yeah. That's literally like dystopian stuff. Like you're gonna show me ads

1397
01:27:44,325 --> 01:27:47,645
Speaker 8:  every time my car comes to a complete stop is horrifying. Yeah.

1398
01:27:47,645 --> 01:27:50,725
Speaker 7:  And it's an ad for of all things an extended warranty in a car that you're

1399
01:27:50,725 --> 01:27:54,605
Speaker 7:  in. Like the classic scam. Yeah. Don't do

1400
01:27:54,605 --> 01:27:57,445
Speaker 7:  this. R Jeep hasn't done it. 'cause we were running Uconnect five, which

1401
01:27:57,445 --> 01:28:01,335
Speaker 7:  is like the Android one. I don't know if all of 'em are gonna start to do

1402
01:28:01,335 --> 01:28:04,095
Speaker 7:  it. I the second that happens, I'm getting rid of this car.

1403
01:28:04,565 --> 01:28:08,535
Speaker 8:  Yeah. I actually think it, I sort of hope some

1404
01:28:08,535 --> 01:28:11,895
Speaker 8:  company does it just to test people's resolve

1405
01:28:12,125 --> 01:28:16,055
Speaker 8:  against it. Because I think like this question

1406
01:28:16,055 --> 01:28:19,495
Speaker 8:  of how invasive can we be before people will revolt?

1407
01:28:20,705 --> 01:28:24,215
Speaker 8:  We've spent a long time testing that as a, like the tech industry has

1408
01:28:24,585 --> 01:28:27,215
Speaker 8:  poked at that for a long time and has found over and over that it can kind

1409
01:28:27,215 --> 01:28:30,535
Speaker 8:  of get away with most things. I think we're gonna show you an ad every time

1410
01:28:30,535 --> 01:28:34,215
Speaker 8:  the car stops, people will just set their cars on fire and walk away. Like

1411
01:28:34,215 --> 01:28:38,055
Speaker 8:  I think that's a real, that that would do it. And I am sort of hoping

1412
01:28:39,015 --> 01:28:40,095
Speaker 8:  somebody actually tests that theory.

1413
01:28:40,525 --> 01:28:43,255
Speaker 7:  Solanis by the way, confirmed this is happening. They say it's a glitch.

1414
01:28:43,405 --> 01:28:46,815
Speaker 7:  They quote attributed a persistent nature of the ad to a temporary software

1415
01:28:46,975 --> 01:28:50,695
Speaker 7:  glitch that affected the optout functionality in certain cases and it has

1416
01:28:50,695 --> 01:28:54,685
Speaker 7:  been identified and corrected, blah blah blah, blah, blah. So here's what,

1417
01:28:54,825 --> 01:28:58,205
Speaker 7:  here's what you're seeing. They're building an opt-out system for in-car

1418
01:28:58,485 --> 01:29:01,330
Speaker 7:  advertising that went haywire and showing ads to everybody because they're

1419
01:29:01,330 --> 01:29:03,845
Speaker 7:  gonna build an advertising system that's gonna show you ads in their car.

1420
01:29:04,505 --> 01:29:04,725
Speaker 8:  Yep.

1421
01:29:05,235 --> 01:29:05,845
Speaker 7:  It's not great.

1422
01:29:06,385 --> 01:29:06,605
Speaker 8:  No.

1423
01:29:07,075 --> 01:29:10,925
Speaker 7:  This is the weird future. This is why I I drive a 1976

1424
01:29:11,015 --> 01:29:14,765
Speaker 7:  Chevy Chevelle. I don't even know if that's a real car, but it'd be cool

1425
01:29:14,765 --> 01:29:15,445
Speaker 7:  if it did. Sounds

1426
01:29:15,445 --> 01:29:15,645
Speaker 8:  Sick.

1427
01:29:15,905 --> 01:29:17,285
Speaker 7:  It does, doesn't it? This is why

1428
01:29:17,285 --> 01:29:20,605
Speaker 8:  We're gonna split a 2001 Escalade and that's gonna be both of our cars forever.

1429
01:29:21,635 --> 01:29:25,215
Speaker 7:  You know, prices on those 2001 Escalades is one shockingly high since the

1430
01:29:25,375 --> 01:29:25,495
Speaker 7:  pandemic.

1431
01:29:27,445 --> 01:29:31,295
Speaker 7:  They're sick cars though. Alright, it's time now for our weekly segment.

1432
01:29:32,735 --> 01:29:36,585
Speaker 7:  Surprisingly popular Brendan Carr's a dummy people

1433
01:29:36,585 --> 01:29:40,265
Speaker 8:  Are saying it's America's greatest meta podcast. The podcast within a podcast.

1434
01:29:41,215 --> 01:29:44,665
Speaker 7:  When I say there's now a non-zero chance that I'm going to jail for being

1435
01:29:44,665 --> 01:29:47,625
Speaker 7:  the under chief of urge, what I mean specifically is FCC Chairman Brendan

1436
01:29:47,695 --> 01:29:51,105
Speaker 7:  Carr might abuse the power of the state to imprison me

1437
01:29:51,755 --> 01:29:55,735
Speaker 7:  and I'm daring him to do it. 'cause I will grift off of

1438
01:29:55,735 --> 01:29:58,935
Speaker 7:  that happening for the rest of my life. And my children were living opulence

1439
01:29:58,935 --> 01:30:00,495
Speaker 7:  and splendor. If you do it, bring it on. Yeah.

1440
01:30:00,495 --> 01:30:04,335
Speaker 8:  Like, do you want to double the neli speaking fee for the rest of his career?

1441
01:30:04,655 --> 01:30:06,535
Speaker 8:  Like, let's go Brenda Carr, I'm ready.

1442
01:30:06,905 --> 01:30:10,895
Speaker 7:  Bring it on you. Big nerd b Brendan Carr is chairman FCC. He used to

1443
01:30:10,895 --> 01:30:14,135
Speaker 7:  be a regular alert. We did an entire decoder about him with Matt Wood, who's

1444
01:30:14,135 --> 01:30:17,015
Speaker 7:  the general counsel of Free Press, not the Barry Weis publication, the pre

1445
01:30:17,015 --> 01:30:20,055
Speaker 7:  press, the First Amendment advocacy shop. We've been around for 20 years.

1446
01:30:21,795 --> 01:30:25,255
Speaker 7:  He used to just like a regular conservative lawyer man, like a gray suit

1447
01:30:25,255 --> 01:30:29,055
Speaker 7:  kind of unstylish big nerd. Now he is a monster, like a

1448
01:30:29,085 --> 01:30:32,735
Speaker 7:  anti-free speech monster. He's filed all kinds of

1449
01:30:32,975 --> 01:30:36,495
Speaker 7:  investigations against all kinds of broadcast outlets because the FCC regulates

1450
01:30:36,495 --> 01:30:40,255
Speaker 7:  the airwaves, which is these ideas about what you should do with those airwaves,

1451
01:30:40,265 --> 01:30:43,935
Speaker 7:  which is basically only praise Donald Trump and not criticize

1452
01:30:44,235 --> 01:30:46,855
Speaker 7:  him or not do anything that could be even perceived as criticism of him.

1453
01:30:46,955 --> 01:30:50,015
Speaker 7:  That's a straightforward First Amendment violation from Brendan Carr First

1454
01:30:50,015 --> 01:30:53,015
Speaker 7:  Amendment nightmare censor in chief. He also has big ideas about regulating

1455
01:30:53,015 --> 01:30:56,935
Speaker 7:  the internet itself, the content on the internet by reinterpreting section

1456
01:30:56,955 --> 01:31:00,935
Speaker 7:  two 30 by issuing just a, a a letter and

1457
01:31:00,965 --> 01:31:04,855
Speaker 7:  like an agency reinterpretation of the law, which would run straight into

1458
01:31:04,885 --> 01:31:08,655
Speaker 7:  Elon Musk's idea about agency regulations. By the way, not a lot of

1459
01:31:09,205 --> 01:31:12,935
Speaker 7:  coherency in the Trump administration, but that's his idea.

1460
01:31:12,995 --> 01:31:13,215
Speaker 7:  But

1461
01:31:13,215 --> 01:31:16,055
Speaker 8:  There is a lot of yelling, like the one thing they all have in common is

1462
01:31:16,055 --> 01:31:18,655
Speaker 8:  everybody is yelling and they, it kind of works,

1463
01:31:18,905 --> 01:31:22,335
Speaker 7:  Right? And the idea here is you create a chilling effect, right? You don't

1464
01:31:22,335 --> 01:31:25,175
Speaker 7:  know if the government will punish you for your speech because they keep

1465
01:31:25,335 --> 01:31:27,815
Speaker 7:  threatening everyone with various punishments that may or may not be legal

1466
01:31:27,815 --> 01:31:30,615
Speaker 7:  or may not be effective. But you don't wanna get caught up in the hassle

1467
01:31:30,615 --> 01:31:34,245
Speaker 7:  caught. So you just do what they say that's bad. That

1468
01:31:34,395 --> 01:31:38,205
Speaker 7:  that, that there's a long history of First Amendment cases where the chilling

1469
01:31:38,205 --> 01:31:41,485
Speaker 7:  effect from the government is the problem, not the actual government speech

1470
01:31:41,485 --> 01:31:44,925
Speaker 7:  triangulation. That's Brendan, Brendan loves this stuff. He loves the chilling

1471
01:31:44,925 --> 01:31:48,485
Speaker 7:  effect. There's reporting this week in the Daily Beast that he's quote, having

1472
01:31:48,505 --> 01:31:52,285
Speaker 7:  the Time of His Life threatening media organizations in this way.

1473
01:31:52,725 --> 01:31:56,125
Speaker 7:  I know that that's true because he retweeted that story

1474
01:31:57,425 --> 01:32:00,645
Speaker 7:  and said, If you have a job you love, you'll never work a day in your life.

1475
01:32:01,665 --> 01:32:01,885
Speaker 8:  Ugh.

1476
01:32:03,565 --> 01:32:07,375
Speaker 7:  Yeah. If you are an unelected and frankly Unstylish

1477
01:32:07,375 --> 01:32:11,015
Speaker 7:  government bureaucrat, you should not be bragging about how you're having

1478
01:32:11,075 --> 01:32:12,215
Speaker 7:  fun threatening the media.

1479
01:32:13,975 --> 01:32:16,615
Speaker 7:  Straightforward. I do not care if you're a republican, a democrat, a libertarian,

1480
01:32:16,735 --> 01:32:18,215
Speaker 7:  a raging socialist or whatever.

1481
01:32:19,795 --> 01:32:23,455
Speaker 7:  You should not be in the government, unelected, elected, whatever. You should

1482
01:32:23,455 --> 01:32:26,895
Speaker 7:  not be a member of the United States government bragging about how much fun

1483
01:32:26,895 --> 01:32:30,335
Speaker 7:  you're having threatening the media. It's, it's wrong. It's

1484
01:32:30,335 --> 01:32:33,535
Speaker 7:  like anti-American. This man is a traitor to our

1485
01:32:34,655 --> 01:32:37,415
Speaker 7:  constitution. I can't say it more plainly than that. He's also, I want to

1486
01:32:37,415 --> 01:32:39,135
Speaker 7:  make this clear kind of an idiot.

1487
01:32:40,455 --> 01:32:43,175
Speaker 8:  I mean, this, this segment is called Brendan Carr is a dummy on purpose,

1488
01:32:43,505 --> 01:32:47,375
Speaker 7:  Right? Like his positions don't have ideological coherency. They

1489
01:32:47,375 --> 01:32:51,215
Speaker 7:  have one point which is to threaten people for their speech using the

1490
01:32:51,215 --> 01:32:54,815
Speaker 7:  tools that he has. And he's wrapping it up in the language of

1491
01:32:54,815 --> 01:32:58,575
Speaker 7:  defending free speech, right? He's, he's sending letters to social media

1492
01:32:58,855 --> 01:33:02,555
Speaker 7:  platforms, which he does not have the power to regulate. Threatening them

1493
01:33:02,555 --> 01:33:06,155
Speaker 7:  over their fact checking because something, something section two 30, which

1494
01:33:06,155 --> 01:33:09,715
Speaker 7:  he will something, something reinterpret with some kind of letter using some

1495
01:33:09,715 --> 01:33:13,045
Speaker 7:  kind of power. He doesn't have completely inconsistent

1496
01:33:13,885 --> 01:33:17,575
Speaker 7:  Brendan Carr, completely inconsistent dumb. He's the person

1497
01:33:17,675 --> 01:33:21,095
Speaker 7:  who is like, I'm getting rid of net neutrality and I will disclaim this

1498
01:33:21,175 --> 01:33:25,015
Speaker 7:  agency's power to regulate ISPs in this country in any way,

1499
01:33:25,015 --> 01:33:28,865
Speaker 7:  shape, or form. So he's given away his power. He's saying we

1500
01:33:28,865 --> 01:33:32,545
Speaker 7:  will not regulate ISPs, we don't have the power to regulate broadband or

1501
01:33:32,545 --> 01:33:36,465
Speaker 7:  broadband pricing or collusion or throttling or blocking or any of

1502
01:33:36,465 --> 01:33:40,305
Speaker 7:  the stuff that ISPs can do. But I will step through the ISP and

1503
01:33:40,625 --> 01:33:44,375
Speaker 7:  directly regulate the content on Instagram. Weird ideologically,

1504
01:33:44,375 --> 01:33:45,375
Speaker 7:  completely incoherent.

1505
01:33:45,375 --> 01:33:48,295
Speaker 8:  Yeah. But I don't, the coherence is not the point, right? Like it's not,

1506
01:33:48,645 --> 01:33:52,335
Speaker 8:  it's, it's just, it's just yelling. The goal is to yell and

1507
01:33:52,355 --> 01:33:55,935
Speaker 8:  you're gonna, he's winning a lot. Like the, the, the

1508
01:33:56,175 --> 01:33:56,895
Speaker 8:  pushback has been,

1509
01:33:58,535 --> 01:34:02,215
Speaker 8:  I would say muted in so many ways. And he's been pretty straightforward

1510
01:34:02,215 --> 01:34:05,215
Speaker 8:  about what he's trying to do here, which is go after

1511
01:34:06,275 --> 01:34:09,215
Speaker 8:  anyone who seems to be on the other side of Donald Trump. Yep.

1512
01:34:09,275 --> 01:34:13,005
Speaker 7:  And then, so then this week, his latest broad

1513
01:34:13,075 --> 01:34:16,645
Speaker 7:  side, he has said he will investigate Comcast to,

1514
01:34:16,865 --> 01:34:20,645
Speaker 7:  you know, big ISP that owns NBC for its DEI

1515
01:34:20,925 --> 01:34:24,685
Speaker 7:  policies in order to root out invidious forms of dis discrim discrimination

1516
01:34:25,295 --> 01:34:29,005
Speaker 7:  disclosure. NBC Universal, which done by Comcast is investor in

1517
01:34:29,015 --> 01:34:32,965
Speaker 7:  boxed, which is The Verge parent company. They hate me. Like, I

1518
01:34:32,965 --> 01:34:36,005
Speaker 7:  just wanna be like Comcast does not like me. I, we that we have written

1519
01:34:36,475 --> 01:34:40,325
Speaker 7:  endlessly about how Comcast is a bad company that has not supported net

1520
01:34:40,325 --> 01:34:44,125
Speaker 7:  neutrality, that has done bad pricing schemes, is is a

1521
01:34:44,285 --> 01:34:44,885
Speaker 7:  regional monopoly.

1522
01:34:46,875 --> 01:34:50,855
Speaker 7:  It is insane for the government to threaten the speech

1523
01:34:50,855 --> 01:34:54,695
Speaker 7:  of Comcast because it has some DI employment policies. It is

1524
01:34:54,755 --> 01:34:58,565
Speaker 7:  trying very sure its workforce is

1525
01:34:58,565 --> 01:35:02,485
Speaker 7:  diverse. Comcast as a cable company is not really an information

1526
01:35:02,685 --> 01:35:05,245
Speaker 7:  provider, right? They're, we don't want them to be an information provider.

1527
01:35:05,515 --> 01:35:09,085
Speaker 7:  They're a physical plant company. They run wires in the ground, they send

1528
01:35:09,085 --> 01:35:12,965
Speaker 7:  out installers and techs and and service people. They run customer service

1529
01:35:12,975 --> 01:35:16,965
Speaker 7:  lines poorly. Again, this is not a company that like we cover, like they're

1530
01:35:16,965 --> 01:35:19,725
Speaker 7:  great and they're not a company that likes me. Even though NBC is an investment

1531
01:35:19,785 --> 01:35:22,365
Speaker 7:  box. Like it has nothing to do with us. That's the other side of the house.

1532
01:35:23,415 --> 01:35:27,315
Speaker 7:  But I'm saying this because you're looking at a employer of a physical plant,

1533
01:35:27,465 --> 01:35:31,355
Speaker 7:  like wires in the ground, service people on trucks, customer

1534
01:35:31,355 --> 01:35:35,155
Speaker 7:  service people taking calls, and to say that they

1535
01:35:35,165 --> 01:35:39,145
Speaker 7:  can't try to diversify that workforce, which is about as

1536
01:35:39,145 --> 01:35:42,945
Speaker 7:  blue collar of a workforce as you can get right now is nuts.

1537
01:35:43,615 --> 01:35:47,305
Speaker 7:  Like, and Brendan is saying, I'm looking for

1538
01:35:47,515 --> 01:35:51,185
Speaker 7:  signs that your initiatives have violated federal employment law.

1539
01:35:51,665 --> 01:35:55,145
Speaker 7:  I expect that this investigation in Comcast and its NBC Universal operations

1540
01:35:55,375 --> 01:35:58,625
Speaker 7:  will aid the commission's broader efforts to root out invidious forms, DEI

1541
01:35:58,625 --> 01:36:02,225
Speaker 7:  discrimination across all sectors. The FEC regulates, well they don't

1542
01:36:02,425 --> 01:36:06,305
Speaker 7:  regulate the ISP specifically. They have disclaimed the power to regulate

1543
01:36:06,305 --> 01:36:06,705
Speaker 7:  the ISP.

1544
01:36:06,705 --> 01:36:10,345
Speaker 8:  Right? Hidden inside of that is, is is a declaration of

1545
01:36:10,355 --> 01:36:11,705
Speaker 8:  power over this stuff.

1546
01:36:12,045 --> 01:36:15,695
Speaker 7:  Yep. So I'm just saying it's, it's logically

1547
01:36:15,745 --> 01:36:19,715
Speaker 7:  incoherent. It, it truly, If you just like look at all the things

1548
01:36:19,715 --> 01:36:23,635
Speaker 7:  they say, they don't add up into a, a consistent worldview except we

1549
01:36:23,635 --> 01:36:27,275
Speaker 7:  have power and we're gonna use it. And specifically we'll use it if we don't

1550
01:36:27,275 --> 01:36:31,265
Speaker 7:  like your speech, Brendan, I know you listen, I know you

1551
01:36:31,265 --> 01:36:34,425
Speaker 7:  get weird Google readouts of every time we mention your name,

1552
01:36:35,205 --> 01:36:38,785
Speaker 7:  you are more than welcome to come on to coder and defend this stuff. I think

1553
01:36:38,805 --> 01:36:42,385
Speaker 7:  it is traitorous. Like it is, it is such an affront to the First Amendment.

1554
01:36:43,105 --> 01:36:46,145
Speaker 7:  I I think you should have to answer for it in that framework. And you are

1555
01:36:46,145 --> 01:36:49,705
Speaker 7:  welcome to come on our show and try to defend it. But until then, I'm gonna

1556
01:36:49,705 --> 01:36:52,585
Speaker 7:  call you a dummy every single week until you put me in jail and make me a

1557
01:36:52,585 --> 01:36:53,305
Speaker 7:  rich man because of it.

1558
01:36:53,985 --> 01:36:55,625
Speaker 8:  There we go. Put that on a t-shirt.

1559
01:36:57,425 --> 01:37:00,755
Speaker 7:  Furious, furious about it again. And I don't care about your politics. The

1560
01:37:00,755 --> 01:37:02,995
Speaker 7:  government threatening speech in this way is over the line.

1561
01:37:04,275 --> 01:37:04,765
Speaker 8:  Alright,

1562
01:37:04,935 --> 01:37:08,805
Speaker 7:  We're gonna end with some gadgets. David has a bunch of gadgets. He is like,

1563
01:37:08,905 --> 01:37:12,005
Speaker 7:  we, we organized this to have a Gadget Palette cleanser at the end.

1564
01:37:12,435 --> 01:37:15,885
Speaker 8:  Yeah. There's a, there's a just a neli neli, you know, we, we like to keep

1565
01:37:15,885 --> 01:37:18,965
Speaker 8:  your blood pressure sort of at a normal level and we, we, we let it get high

1566
01:37:18,965 --> 01:37:19,925
Speaker 8:  and then we bring it back down.

1567
01:37:21,785 --> 01:37:25,565
Speaker 8:  So two interesting streaming e things this week.

1568
01:37:25,785 --> 01:37:29,405
Speaker 8:  One is that Apple TV Plus is out now on

1569
01:37:29,405 --> 01:37:32,165
Speaker 8:  Android, which is kind of a huge deal. Apparently

1570
01:37:32,165 --> 01:37:32,925
Speaker 7:  It's a really good app.

1571
01:37:33,765 --> 01:37:34,975
Speaker 8:  Yeah. I haven't actually used it on

1572
01:37:34,975 --> 01:37:38,935
Speaker 7:  Android. I saw Dan Seaford, who, you know, he was our crankiest

1573
01:37:38,935 --> 01:37:42,135
Speaker 7:  reviewer here at The Verge and now he works at Google. It was like, this

1574
01:37:42,135 --> 01:37:43,015
Speaker 7:  is a really good Android app.

1575
01:37:43,475 --> 01:37:47,015
Speaker 8:  That's great. Yeah. It works on, it works on foldable phones, it works on

1576
01:37:47,175 --> 01:37:50,455
Speaker 8:  tablets. It's like Apple seems to have like, actually kind of done the thing,

1577
01:37:51,855 --> 01:37:55,725
Speaker 8:  interesting moment for Apple TV because a, there's an MLS season about to

1578
01:37:55,725 --> 01:37:59,645
Speaker 8:  start and Apple is very bought into making the MLSA success. I

1579
01:37:59,645 --> 01:38:03,205
Speaker 8:  think that's gone pretty well for Apple and this is gonna be a big year for

1580
01:38:03,205 --> 01:38:07,165
Speaker 8:  Apple. Presumably this is the last we will see of Lean No Messi.

1581
01:38:07,165 --> 01:38:09,805
Speaker 8:  It's gonna be a whole thing. So I think we're gonna see a lot of Apple soccer

1582
01:38:09,805 --> 01:38:12,965
Speaker 8:  stuff this year. But also Severance is back

1583
01:38:13,585 --> 01:38:17,485
Speaker 8:  and is is probably, it's either the biggest or

1584
01:38:17,485 --> 01:38:21,165
Speaker 8:  second biggest like cultural event. Apple TV Plus has pulled off

1585
01:38:21,195 --> 01:38:25,125
Speaker 8:  next to Ted Lasso. And you, you can see Apple is

1586
01:38:25,125 --> 01:38:28,045
Speaker 8:  trying to make something out of this into like,

1587
01:38:29,205 --> 01:38:32,965
Speaker 7:  Actually I actually think that Apple trying to make severance a hit is

1588
01:38:33,755 --> 01:38:37,325
Speaker 7:  it's wonderful because Severance is such a weird

1589
01:38:37,625 --> 01:38:41,325
Speaker 7:  and inaccessible show. Yes. Like it is such a weird, it's

1590
01:38:41,325 --> 01:38:42,325
Speaker 8:  The opposite of Ted Lasso

1591
01:38:42,715 --> 01:38:46,565
Speaker 7:  Slow quiet. Yeah. Right. It's, and it's, it's, it's a puzzle

1592
01:38:46,665 --> 01:38:50,525
Speaker 7:  box. It, it's like Apple's like, screw it, we're

1593
01:38:50,605 --> 01:38:53,605
Speaker 7:  gonna make this one a hit. You know, like be clear it's

1594
01:38:53,665 --> 01:38:56,565
Speaker 8:  Way and everybody should watch it. But it is, it is a very funny one for

1595
01:38:56,565 --> 01:38:57,925
Speaker 8:  Apple to have latched onto this way.

1596
01:38:58,115 --> 01:39:01,005
Speaker 7:  Yeah. And it, you know, I I get it. Ben Stiller's directing it, it's got

1597
01:39:01,005 --> 01:39:04,805
Speaker 7:  stars in the ca Like the whole, I I understand all of it, but it's such

1598
01:39:04,885 --> 01:39:08,685
Speaker 7:  a weird show for Apple to be like, and this is a hit. It's

1599
01:39:08,685 --> 01:39:11,485
Speaker 8:  Also like a little bit about how bad a company Apple is.

1600
01:39:11,835 --> 01:39:13,485
Speaker 7:  Just like, it's very

1601
01:39:13,485 --> 01:39:14,005
Speaker 8:  Good. Yeah.

1602
01:39:14,545 --> 01:39:17,365
Speaker 7:  Do you steer jobs? So Steve Jobs Keegan, like,

1603
01:39:18,675 --> 01:39:22,205
Speaker 7:  it's pretty close. Not No. Yeah, yeah.

1604
01:39:22,225 --> 01:39:24,405
Speaker 8:  But anyway, If you're not watching separate, you should and now you can on

1605
01:39:24,405 --> 01:39:27,965
Speaker 8:  Android, which is interesting next to a bit of

1606
01:39:28,555 --> 01:39:32,165
Speaker 8:  YouTube news, which is that TVs are now the biggest

1607
01:39:32,525 --> 01:39:36,485
Speaker 8:  platform for YouTube. More people are watching YouTube on their televisions

1608
01:39:36,635 --> 01:39:39,485
Speaker 8:  than on their phones for the first time ever on YouTube. YouTube has been

1609
01:39:39,485 --> 01:39:43,325
Speaker 8:  talking about how TV is its fastest growing platform for like,

1610
01:39:43,515 --> 01:39:46,765
Speaker 8:  forever. Like, I make fun of YouTube executives are raising my talk to 'em

1611
01:39:46,765 --> 01:39:50,525
Speaker 8:  about this because it's just the stat that they have used in every press

1612
01:39:50,525 --> 01:39:51,765
Speaker 8:  release for like a decade.

1613
01:39:53,665 --> 01:39:57,405
Speaker 8:  But I think this is a meaningful moment. We've talked a lot about how

1614
01:39:57,405 --> 01:40:01,325
Speaker 8:  Netflix and YouTube are kind of running at and past each other in some really

1615
01:40:01,445 --> 01:40:01,725
Speaker 8:  interesting ways.

1616
01:40:03,275 --> 01:40:06,765
Speaker 8:  YouTube is just winning. Like imagine a fight that there is and YouTube is

1617
01:40:06,765 --> 01:40:10,245
Speaker 8:  winning it. Yeah. And it's, it's blowing up in podcasts.

1618
01:40:10,555 --> 01:40:14,165
Speaker 8:  It's hugely important as an entertainment platform for like high-end

1619
01:40:14,165 --> 01:40:18,005
Speaker 8:  entertainment. YouTube TV is the biggest name in, in its category

1620
01:40:18,005 --> 01:40:19,245
Speaker 8:  at this point. But

1621
01:40:19,425 --> 01:40:22,565
Speaker 7:  The wait, it's the middle one. I don't buy, it's high-end entertainment on

1622
01:40:22,565 --> 01:40:24,425
Speaker 7:  YouTube. It's not

1623
01:40:24,575 --> 01:40:28,025
Speaker 8:  Winning. But like YouTube primetime channels is a thing. Warner Brothers

1624
01:40:28,025 --> 01:40:31,185
Speaker 8:  dumped all the Oh sure. I know movies onto it. Like it's a, it's a place

1625
01:40:31,245 --> 01:40:34,825
Speaker 8:  for like capital H Hollywood content now in a pretty real way.

1626
01:40:36,165 --> 01:40:40,145
Speaker 8:  And now there is this thing that they're like, actually TVs big screens

1627
01:40:40,365 --> 01:40:44,185
Speaker 8:  are bigger than phones for YouTube. Which is

1628
01:40:44,495 --> 01:40:46,745
Speaker 8:  just kind of a wild thing that I never thought would come true.

1629
01:40:47,835 --> 01:40:51,485
Speaker 7:  It's interesting. I wonder if that is at the expense of phones or if at the

1630
01:40:51,485 --> 01:40:52,205
Speaker 7:  expense of computers,

1631
01:40:54,525 --> 01:40:54,875
Speaker 7:  right?

1632
01:40:55,395 --> 01:40:57,315
Speaker 8:  Probably some of both. Like, are people,

1633
01:40:57,335 --> 01:41:00,515
Speaker 7:  People choosing to watch YouTube on their TVs because they're no longer watching

1634
01:41:00,515 --> 01:41:04,475
Speaker 7:  it on their laptops or their desktops or whatever? Or is it because they

1635
01:41:04,475 --> 01:41:07,195
Speaker 7:  come home at night and they were gonna watch something and they would rather

1636
01:41:07,195 --> 01:41:09,675
Speaker 7:  scroll TikTok on their phone while something's on TV and YouTube happens

1637
01:41:09,675 --> 01:41:10,075
Speaker 7:  to be there?

1638
01:41:10,555 --> 01:41:14,435
Speaker 8:  I think there is a lot of that. I mean we've, we've definitely heard a

1639
01:41:14,435 --> 01:41:17,955
Speaker 8:  lot about, like YouTube has gone from sort of a second screen

1640
01:41:18,235 --> 01:41:21,475
Speaker 8:  phenomenon to a first screen phenomenon in a way that I think TikTok is like

1641
01:41:21,475 --> 01:41:25,395
Speaker 8:  a second screen phenomenon. Like you, you put on the office sort

1642
01:41:25,395 --> 01:41:28,635
Speaker 8:  of quietly while you look at TikTok on your phone. Like YouTube has become

1643
01:41:28,635 --> 01:41:31,755
Speaker 8:  the office, which like yeah, is is good and bad I suppose.

1644
01:41:32,615 --> 01:41:36,475
Speaker 8:  But it is like, it is a living room platform

1645
01:41:36,495 --> 01:41:38,715
Speaker 8:  now in a really interesting way. And they've been chasing that with product

1646
01:41:38,725 --> 01:41:42,075
Speaker 8:  stuff. Like they're working with creators to turn things into like

1647
01:41:42,345 --> 01:41:45,555
Speaker 8:  seasons and they're doing the deals with the Hollywood studios and stuff.

1648
01:41:45,555 --> 01:41:49,475
Speaker 8:  So I think, my guess is this push from YouTube to do

1649
01:41:50,745 --> 01:41:54,435
Speaker 8:  more subscription based stuff. Do more like high

1650
01:41:54,435 --> 01:41:57,555
Speaker 8:  production value stuff, which is a thing YouTube has kind of

1651
01:41:58,125 --> 01:42:01,025
Speaker 8:  intermittently done over the years. They had that like YouTube originals

1652
01:42:01,025 --> 01:42:04,145
Speaker 8:  thing a million years ago with Jay-Z that didn't work and they've poked at

1653
01:42:04,145 --> 01:42:07,945
Speaker 8:  this before. But I think this, this thing where YouTube is going to try to

1654
01:42:07,945 --> 01:42:11,745
Speaker 8:  become a high-end streaming service on top of everything else, I think

1655
01:42:11,745 --> 01:42:15,425
Speaker 8:  is only going to accelerate because of all this. Because now like I sit down

1656
01:42:15,425 --> 01:42:18,105
Speaker 8:  and I open the YouTube app on my TV is a

1657
01:42:18,995 --> 01:42:22,525
Speaker 8:  default behavior for people clearly. Like it's a, it's a meaningful thing.

1658
01:42:22,595 --> 01:42:25,645
Speaker 8:  Yeah. And that once you have that you can do anything you want. Right? Like

1659
01:42:25,645 --> 01:42:27,285
Speaker 8:  that's what's worked for Netflix. It's

1660
01:42:27,285 --> 01:42:31,125
Speaker 7:  Funny 'cause I have all of YouTube's personalization turned off and so I

1661
01:42:31,125 --> 01:42:34,165
Speaker 7:  open that app in my TV and it's like, please turn the personalization. Like,

1662
01:42:34,165 --> 01:42:37,685
Speaker 7:  it's like begging you, it's like we're so useless unless you let us track

1663
01:42:37,685 --> 01:42:39,685
Speaker 7:  everything you watch. I'm like, well that sucks for you. You

1664
01:42:39,685 --> 01:42:42,285
Speaker 8:  Just get that empty home screen where they're like, we don't know. It's your

1665
01:42:42,285 --> 01:42:42,885
Speaker 8:  fault. It's a, they're

1666
01:42:43,055 --> 01:42:46,365
Speaker 7:  Thank you. It is a delight. And it's so, it's funny, funny that they won't

1667
01:42:46,365 --> 01:42:50,175
Speaker 7:  populate it anyway. Like it's, they've actively made it

1668
01:42:50,175 --> 01:42:53,935
Speaker 7:  worse so that you will be compelled to turn on the personal. And I'm like,

1669
01:42:53,935 --> 01:42:56,935
Speaker 7:  no, If you wanna put stuff here, you're more than welcome to put stuff here.

1670
01:42:56,995 --> 01:42:58,815
Speaker 7:  But no thank you sir.

1671
01:43:00,195 --> 01:43:04,005
Speaker 8:  Yeah, it's good stuff. YouTube is like sneakily a very

1672
01:43:04,095 --> 01:43:07,125
Speaker 8:  petty platform in that way and I really enjoy that about YouTube.

1673
01:43:08,145 --> 01:43:11,805
Speaker 8:  Two more gadgets. One, Alison reviewed the

1674
01:43:11,805 --> 01:43:15,605
Speaker 8:  Samsung Galaxy S 25 and S 25 plus. We talked a bunch about the

1675
01:43:15,655 --> 01:43:18,965
Speaker 8:  Ultra when she reviewed it. It turns out, imagine

1676
01:43:19,595 --> 01:43:20,885
Speaker 8:  last year's Samsung phone

1677
01:43:21,985 --> 01:43:22,205
Speaker 7:  You

1678
01:43:22,205 --> 01:43:25,485
Speaker 8:  Did it. It's, and I think it's, it's inspired a bunch of like

1679
01:43:25,685 --> 01:43:28,965
Speaker 8:  conversation about sort of what we want from our smartphones now.

1680
01:43:30,465 --> 01:43:34,445
Speaker 8:  And I think my thesis with this has been that we're going

1681
01:43:34,445 --> 01:43:38,325
Speaker 8:  to have a year full of really uninteresting smartphone launches and that

1682
01:43:38,325 --> 01:43:41,805
Speaker 8:  maybe that's okay and that maybe we have just fully hit the point where these

1683
01:43:41,805 --> 01:43:45,325
Speaker 8:  things are in, in a meaningful way finished.

1684
01:43:45,515 --> 01:43:49,485
Speaker 8:  Like we, we have done it, they are what they are and we're all gonna

1685
01:43:49,485 --> 01:43:53,165
Speaker 8:  kind of move on with our lives and everybody will pivot to the next thing.

1686
01:43:53,165 --> 01:43:56,845
Speaker 8:  And then maybe that's okay. Like we had a, we had a damn near two decades

1687
01:43:56,905 --> 01:44:00,485
Speaker 8:  of smartphones being really interesting and maybe they just won't be ever

1688
01:44:00,485 --> 01:44:01,645
Speaker 8:  again. And maybe that's fine.

1689
01:44:01,875 --> 01:44:03,285
Speaker 7:  Yeah. Do you think the next thing

1690
01:44:04,845 --> 01:44:08,345
Speaker 7:  is the AI hype gadget. Do you think the next thing is a foldable?

1691
01:44:08,575 --> 01:44:09,985
Speaker 7:  What do you think? What's your bet?

1692
01:44:11,585 --> 01:44:13,525
Speaker 8:  That's a really interesting question. I do think there,

1693
01:44:13,525 --> 01:44:16,565
Speaker 7:  There's a lots, there's a lot of people betting on, well If you just talk

1694
01:44:16,565 --> 01:44:20,375
Speaker 7:  to phones right? The AI will re yeah, you will

1695
01:44:20,375 --> 01:44:21,495
Speaker 7:  rethink the form factor.

1696
01:44:22,455 --> 01:44:25,735
Speaker 8:  I think there's probably a little bit of that. I think, I think we're due

1697
01:44:25,755 --> 01:44:28,495
Speaker 8:  for a really interesting run of wearable design.

1698
01:44:29,735 --> 01:44:33,615
Speaker 8:  I think this, this idea that we just landed on smart watches and

1699
01:44:33,615 --> 01:44:37,575
Speaker 8:  that was fine and we fixed it is not correct And I'm, I'm already

1700
01:44:37,815 --> 01:44:40,495
Speaker 8:  starting to see glimmers from people about like obviously there's glasses

1701
01:44:40,495 --> 01:44:44,095
Speaker 8:  happening, but like I think rings are gonna be more interesting than we have

1702
01:44:44,295 --> 01:44:46,575
Speaker 8:  realized so far. There's actually like kind of a lot you can do there.

1703
01:44:48,405 --> 01:44:52,095
Speaker 8:  Meta is gonna roll out. Its weird wristband thing at some point.

1704
01:44:52,885 --> 01:44:55,975
Speaker 8:  Like there's gonna be stuff on your body that I think people are gonna start

1705
01:44:55,975 --> 01:44:58,415
Speaker 8:  to mess with. Like headphones. There's a lot of interesting stuff you can

1706
01:44:58,415 --> 01:45:02,335
Speaker 8:  do in headphones that we haven't done yet. That's,

1707
01:45:02,335 --> 01:45:05,375
Speaker 8:  that's my theory is that we're gonna, we're gonna, I don't know if any that

1708
01:45:05,375 --> 01:45:09,215
Speaker 8:  it will amount to anything, but like that's gonna be the next place

1709
01:45:09,215 --> 01:45:13,175
Speaker 8:  we do real, I think like gadget experimentation is kind of

1710
01:45:13,175 --> 01:45:16,935
Speaker 8:  wearables of all sorts, which I'm all for. Like, let the smartphone in my

1711
01:45:16,935 --> 01:45:20,535
Speaker 8:  pocket just be a modem for all the wearables on my body and I'm down. I love

1712
01:45:20,535 --> 01:45:20,615
Speaker 8:  it.

1713
01:45:20,925 --> 01:45:23,295
Speaker 7:  This really implies that Bluetooth is gonna be better next year. I'm

1714
01:45:24,595 --> 01:45:27,495
Speaker 7:  saying Bluetooth is fine. You're making a bet.

1715
01:45:28,405 --> 01:45:31,725
Speaker 8:  I look forward to at some point just spending an hour debating the merits

1716
01:45:31,725 --> 01:45:34,485
Speaker 8:  of Bluetooth and it's gonna be you being very upset and me just being like,

1717
01:45:34,505 --> 01:45:35,925
Speaker 8:  I'm using Bluetooth headphones right now. Fine. Now I'm

1718
01:45:35,925 --> 01:45:39,405
Speaker 7:  Gonna win this argument right now. The next gadget on your list, you're wearing

1719
01:45:39,405 --> 01:45:39,965
Speaker 7:  them right now.

1720
01:45:40,765 --> 01:45:41,445
Speaker 8:  I am wearing 'em right now.

1721
01:45:42,075 --> 01:45:45,925
Speaker 7:  Involve a custom proprietary riff on Bluetooth so that

1722
01:45:45,925 --> 01:45:46,645
Speaker 7:  they can be good.

1723
01:45:47,335 --> 01:45:51,105
Speaker 8:  Yeah, so the, it's the, the Beats Powerbeats Pro two.

1724
01:45:51,605 --> 01:45:55,145
Speaker 8:  Not a great name, but it's fine. They came out this week,

1725
01:45:55,955 --> 01:45:59,025
Speaker 8:  Chris Welch wrote about them V song reviewed them. They're cool. Like the,

1726
01:45:59,025 --> 01:46:02,505
Speaker 8:  their shtick is basically their, their workout AirPods. They have the, the

1727
01:46:02,605 --> 01:46:05,585
Speaker 8:  ear hook that people really like. I really like it.

1728
01:46:06,335 --> 01:46:09,865
Speaker 8:  It's just they're more secure in your ears than AirPods are for that reason.

1729
01:46:10,575 --> 01:46:13,465
Speaker 8:  They have heart rate tracking in the headphones, which I think is cool. This

1730
01:46:13,465 --> 01:46:17,305
Speaker 8:  is what I mean by there's more stuff you can do in headphones. They're starting

1731
01:46:17,305 --> 01:46:21,025
Speaker 8:  to do some of that like biometric stuff. I went to

1732
01:46:21,485 --> 01:46:25,465
Speaker 8:  Target this morning to buy a pair. I had to explain to the

1733
01:46:25,465 --> 01:46:28,585
Speaker 8:  guy that these things had launched today and they existed and that I wanted

1734
01:46:28,585 --> 01:46:31,545
Speaker 8:  them, I bought the, the quicksand color. Ooh.

1735
01:46:32,995 --> 01:46:36,425
Speaker 8:  Which is fine. I was gonna buy orange, but then I was like, I'm gonna get

1736
01:46:36,425 --> 01:46:38,465
Speaker 8:  orange and then I'm gonna get on video calls and I'm gonna have to explain

1737
01:46:38,485 --> 01:46:41,785
Speaker 8:  my headphones every single time. And I just wasn't ready for that.

1738
01:46:42,965 --> 01:46:46,185
Speaker 8:  But I think for like two 50, these are pretty good headphones. But your point

1739
01:46:46,185 --> 01:46:49,925
Speaker 8:  is exactly correct. That the specific reason they are good and the

1740
01:46:50,085 --> 01:46:53,405
Speaker 8:  specific reason that I bought them is because they're not just Bluetooth,

1741
01:46:53,965 --> 01:46:54,315
Speaker 7:  Right?

1742
01:46:54,945 --> 01:46:58,715
Speaker 8:  They have the H two chip, which lets it do some really good like multi-point

1743
01:46:58,715 --> 01:47:02,355
Speaker 8:  stuff. It, it makes the connection better. It's just, it is better than

1744
01:47:02,355 --> 01:47:05,675
Speaker 8:  Bluetooth because it is allowed to be and because only Apple is allowed to

1745
01:47:05,675 --> 01:47:08,875
Speaker 8:  do these things inside of its ecosystem.

1746
01:47:09,455 --> 01:47:12,915
Speaker 8:  But like I am currently connected to my

1747
01:47:13,575 --> 01:47:17,435
Speaker 8:  iPhone, my computer and my iPad on my headphones and that

1748
01:47:17,435 --> 01:47:20,315
Speaker 8:  is cool. And that is not good Bluetooth.

1749
01:47:21,145 --> 01:47:21,875
Speaker 8:  It's just doesn't work.

1750
01:47:22,535 --> 01:47:23,995
Speaker 7:  That's some breaking news for you David.

1751
01:47:24,415 --> 01:47:24,755
Speaker 8:  Do you

1752
01:47:25,095 --> 01:47:28,395
Speaker 7:  The Verge was just nominated for the National Magazine Award.

1753
01:47:29,075 --> 01:47:32,615
Speaker 7:  Whoa. In general Excellence. Let go. It's the National Geographic Let's New

1754
01:47:32,635 --> 01:47:34,655
Speaker 7:  York, the New Yorker Pub, ProPublica and The Verge.

1755
01:47:35,245 --> 01:47:35,815
Speaker 8:  Hell yeah.

1756
01:47:36,715 --> 01:47:40,535
Speaker 7:  And also 2004 was nominated for

1757
01:47:40,935 --> 01:47:42,815
Speaker 7:  the National Magazine Award for single topic issue.

1758
01:47:43,475 --> 01:47:43,855
Speaker 8:  Boo. Yeah,

1759
01:47:44,475 --> 01:47:45,415
Speaker 7:  That's pretty exciting.

1760
01:47:45,795 --> 01:47:46,335
Speaker 8:  That's awesome.

1761
01:47:47,695 --> 01:47:50,975
Speaker 7:  I do like the idea that one day we'll win this national magazine award for

1762
01:47:50,975 --> 01:47:54,895
Speaker 7:  general excellence. You have to beat New York and the New Yorker every year.

1763
01:47:54,895 --> 01:47:56,655
Speaker 7:  I think they get nominated every year. What is it?

1764
01:47:56,655 --> 01:48:00,295
Speaker 8:  It's New York, new Yorker Atlantic and New York Times

1765
01:48:00,575 --> 01:48:01,535
Speaker 7:  Magazine. The Atlantic is not nominated this year.

1766
01:48:01,775 --> 01:48:04,455
Speaker 8:  I know, but those are like the four that just tend to pass it around between

1767
01:48:04,455 --> 01:48:06,215
Speaker 8:  each other, right? Yep. Yeah.

1768
01:48:06,645 --> 01:48:07,375
Speaker 7:  This is our shot.

1769
01:48:08,435 --> 01:48:11,005
Speaker 8:  I love it. I'm ready. We're coming for you Nat Geo,

1770
01:48:12,985 --> 01:48:16,045
Speaker 7:  Our sister location at New York Magazine. We're coming for

1771
01:48:17,055 --> 01:48:19,885
Speaker 7:  you. David Haskell and I are gonna have to like lock eyes in the office when

1772
01:48:19,885 --> 01:48:23,645
Speaker 7:  we walk past each other. Love it. All your headphones are fine. I'm now,

1773
01:48:23,645 --> 01:48:25,885
Speaker 7:  I'm in a good mood 'cause we got nominated for the National Magazine Award.

1774
01:48:26,285 --> 01:48:26,635
Speaker 8:  There

1775
01:48:26,635 --> 01:48:29,395
Speaker 7:  We go. This breaking news. I literally got the email. I stopped listening

1776
01:48:29,395 --> 01:48:31,515
Speaker 7:  to you 'cause I saw this email. Yeah, I apologize.

1777
01:48:31,895 --> 01:48:34,715
Speaker 8:  We could all tell you stop listening to me. It's fine. But it's good. It

1778
01:48:34,715 --> 01:48:38,395
Speaker 8:  saves us all from yet another Bluetooth rant, so thank you to the ASEs

1779
01:48:38,895 --> 01:48:39,395
Speaker 8:  for doing that

1780
01:48:39,395 --> 01:48:42,915
Speaker 7:  For us. Look, if I get this award before I go to jail, you know, that'd be

1781
01:48:42,915 --> 01:48:45,515
Speaker 7:  pretty good. Just saying to the committee. I think that'd be pretty good.

1782
01:48:46,065 --> 01:48:46,475
Speaker 8:  Love it.

1783
01:48:46,975 --> 01:48:49,675
Speaker 7:  All right, we gotta take a break. Move back next week,

1784
01:48:50,805 --> 01:48:54,795
Speaker 7:  every week, whether we're incarcerated or not. That's

1785
01:48:54,795 --> 01:48:55,835
Speaker 7:  your chest rock and roll.

1786
01:49:01,495 --> 01:49:04,675
Speaker 15:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1787
01:49:04,675 --> 01:49:08,315
Speaker 15:  you. Give us a call at eight six six VERGE

1788
01:49:08,495 --> 01:49:11,995
Speaker 15:  one one. The Vergecast is a production of The Verge and the Vox Media

1789
01:49:12,295 --> 01:49:16,115
Speaker 15:  Podcast network. Our show is produced by Will Por, Eric Gomez and

1790
01:49:16,115 --> 01:49:17,595
Speaker 15:  Brandon Keefer. And that's it. We'll see you next week.

