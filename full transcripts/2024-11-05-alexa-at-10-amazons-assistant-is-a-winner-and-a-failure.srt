1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: ade6240d-c72a-4842-be37-07531966a6cc
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/4996903728059485304/3063947398153752635/s93290-US-4460s-1730804571.mp3
Description: November 6th marks 10 years to the day since Amazon surprise-launched a new, cylindrical device called the Echo. It introduced the world to smart speakers, and to the idea that you might be able to get stuff done just by shouting aloud in your living room. But a decade in, what has Alexa really accomplished? The Verge's Jennifer Pattison Tuohy joins the show to talk through the history of Alexa, Amazon's struggles to improve and extend its voice assistant, and the promise of a language model overhaul that might in theory make Alexa far more useful. There's a chance Alexa's second decade might be even more interesting than the first.

2
00:00:03,025 --> 00:00:06,795
Speaker 1:  Welcome To The Vergecast, the flagship podcast of Farfield Microphones.

3
00:00:07,095 --> 00:00:10,555
Speaker 1:  I'm your friend David Pierce, and I am sitting here updating maybe the single

4
00:00:10,665 --> 00:00:14,155
Speaker 1:  most important list in my life. I'm a person who thinks

5
00:00:14,305 --> 00:00:17,115
Speaker 1:  enlists If. you haven't figured that out from listening to their show in

6
00:00:17,115 --> 00:00:21,035
Speaker 1:  recent years. I have a list of all of the stuff that I have

7
00:00:21,035 --> 00:00:24,435
Speaker 1:  to do. I have lists for all of my like frequent flyer accounts.

8
00:00:24,485 --> 00:00:27,875
Speaker 1:  Everything is bullet points, everything is lists. It's just how my brand

9
00:00:27,875 --> 00:00:31,755
Speaker 1:  works. But the list that is the most important to me is one I've been

10
00:00:31,755 --> 00:00:35,315
Speaker 1:  keeping for the last several years, and it's called How to Be a

11
00:00:35,315 --> 00:00:38,955
Speaker 1:  Grownup. So I'm sure I've complained about this on the show before, but they

12
00:00:38,955 --> 00:00:42,555
Speaker 1:  never tell you when you're a kid that being an adult is just like a series

13
00:00:42,615 --> 00:00:46,555
Speaker 1:  of small maintenance tasks and none of them are very hard, but they're

14
00:00:46,555 --> 00:00:50,075
Speaker 1:  impossible to remember to do. And because you don't do them, things

15
00:00:50,365 --> 00:00:54,035
Speaker 1:  break horribly. So over the years, as I've discovered each of these

16
00:00:54,085 --> 00:00:57,915
Speaker 1:  tasks, I've added it to the list and I've added reminders so that now

17
00:00:57,935 --> 00:01:01,835
Speaker 1:  at intervals, every three months, let's say I get one that's like, Hey, change

18
00:01:01,835 --> 00:01:05,795
Speaker 1:  the air filter in the furnace so that the air isn't disgusting in

19
00:01:05,795 --> 00:01:09,515
Speaker 1:  your house. And all of these tasks are like quick and simple. It's just

20
00:01:09,955 --> 00:01:12,755
Speaker 1:  remembering to do them or even that they're a thing you're supposed to do

21
00:01:13,375 --> 00:01:16,195
Speaker 1:  is the problem. The most recent one, by the way, is

22
00:01:17,435 --> 00:01:21,315
Speaker 1:  remember to change the air filter on the dehumidifier that I

23
00:01:21,315 --> 00:01:25,155
Speaker 1:  put in the laundry room because the laundry room stinks because of all the

24
00:01:25,355 --> 00:01:29,275
Speaker 1:  moisture. Like there are like eight grownup tasks that backed all

25
00:01:29,275 --> 00:01:33,115
Speaker 1:  the way into change the air filter. So now every couple of months

26
00:01:33,395 --> 00:01:37,235
Speaker 1:  I have a task that is going to remind me to change the air filter, or at

27
00:01:37,235 --> 00:01:40,675
Speaker 1:  least just like hose it down so that the laundry room doesn't stink.

28
00:01:41,425 --> 00:01:45,355
Speaker 1:  This is being an adult. It's not that much fun. But here we are anyway.

29
00:01:45,495 --> 00:01:49,315
Speaker 1:  We are not here to talk about lists or dehumidifiers. We are here to talk

30
00:01:49,315 --> 00:01:53,115
Speaker 1:  about Alexa. So this week, Wednesday specifically, is the

31
00:01:53,135 --> 00:01:56,955
Speaker 1:  10 year anniversary of the very first Amazon Echo. You might

32
00:01:57,115 --> 00:02:00,395
Speaker 1:  remember the one that kinda looked like a can of tennis balls or like a Pringles

33
00:02:00,455 --> 00:02:04,235
Speaker 1:  can. It had the blue ring, it had the far field microphone in a

34
00:02:04,235 --> 00:02:07,995
Speaker 1:  surprising way. It was like a fully formed vision for a new

35
00:02:08,015 --> 00:02:11,955
Speaker 1:  way of computing. And 10 years later, in a strange

36
00:02:11,955 --> 00:02:15,195
Speaker 1:  way, I think it has actually been both more and less

37
00:02:15,495 --> 00:02:19,115
Speaker 1:  transformative than we might have expected. And so we're gonna spend the

38
00:02:19,115 --> 00:02:22,795
Speaker 1:  whole hour with Gen Tui talking through all of that. How

39
00:02:23,015 --> 00:02:26,635
Speaker 1:  far have we gotten? Where is left to go? Is this actually the right

40
00:02:26,745 --> 00:02:30,635
Speaker 1:  path for the future of computing? Alexa got a lot of people excited.

41
00:02:30,635 --> 00:02:34,515
Speaker 1:  Google with Google Assistant has made huge moves towards this

42
00:02:34,515 --> 00:02:38,475
Speaker 1:  same kind of thing. Siri was around before Alexa,

43
00:02:38,575 --> 00:02:42,435
Speaker 1:  but has changed a lot because of Alexa, I think. And now with ai we

44
00:02:42,435 --> 00:02:46,395
Speaker 1:  have this giant push towards something very similar. And

45
00:02:46,395 --> 00:02:50,195
Speaker 1:  so the question as always is, is this what we want? Is this the

46
00:02:50,195 --> 00:02:53,995
Speaker 1:  right answer? And what can we learn from the first 10 years of Alexa

47
00:02:54,125 --> 00:02:57,875
Speaker 1:  about what it'll take to get there? We're gonna get to all of that, but I

48
00:02:57,875 --> 00:03:01,845
Speaker 1:  should just warn you before we start, turn off your Alexa devices. I'm sure

49
00:03:01,845 --> 00:03:05,165
Speaker 1:  I've set them off several times already and for that I'm very sorry, but

50
00:03:05,385 --> 00:03:08,925
Speaker 1:  we are going to say that word So, so, so many times

51
00:03:09,345 --> 00:03:12,725
Speaker 1:  in the next hour, go on a walk, put on headphones,

52
00:03:13,325 --> 00:03:17,165
Speaker 1:  I don't know, like put pillows around you so that you're in just a little

53
00:03:17,365 --> 00:03:20,845
Speaker 1:  fort where it's you and The Vergecast. Do what you gotta do, but turn the

54
00:03:20,845 --> 00:03:23,645
Speaker 1:  speakers off. 'cause otherwise it's gonna, it's gonna get rough out there.

55
00:03:23,725 --> 00:03:26,605
Speaker 1:  I have also set mine off two times since I've been sitting here.

56
00:03:27,975 --> 00:03:30,685
Speaker 1:  We're gonna get through it together. Friends, all that is coming up in just

57
00:03:30,685 --> 00:03:34,285
Speaker 1:  a second. But first, literally I have to go turn off 30 or 40 devices

58
00:03:34,505 --> 00:03:38,285
Speaker 1:  or else this is gonna get really, really, really messy. This is The Vergecast.

59
00:03:38,415 --> 00:03:39,125
Speaker 1:  We'll be right back.

60
00:04:15,485 --> 00:04:19,415
Speaker 1:  Welcome back. All right, let's just get into Alexa stuff. But before we do,

61
00:04:19,435 --> 00:04:22,735
Speaker 1:  let me just really quickly set the scene. So it's

62
00:04:23,095 --> 00:04:26,975
Speaker 1:  November 6th, 2014, 10 years ago this Wednesday,

63
00:04:27,555 --> 00:04:31,375
Speaker 1:  The Verge dot com. It's a website. We're like three years old and

64
00:04:31,635 --> 00:04:35,415
Speaker 1:  all of a sudden Amazon just drops this

65
00:04:35,415 --> 00:04:39,215
Speaker 1:  thing on its website. It's, it was in the morning of that

66
00:04:39,215 --> 00:04:43,135
Speaker 1:  day and it just appeared. Let me just read you the headline from our

67
00:04:43,135 --> 00:04:45,815
Speaker 1:  story. This is at 11:56 AM on

68
00:04:46,215 --> 00:04:49,455
Speaker 1:  November 6th, 2014. It's by Chris Welch. And I love this headline very much.

69
00:04:50,155 --> 00:04:53,935
Speaker 1:  It says, Amazon just surprised everyone with a crazy speaker that talks to

70
00:04:53,935 --> 00:04:57,785
Speaker 1:  you, kind of tells you everything you need to know, right? The

71
00:04:57,785 --> 00:05:00,905
Speaker 1:  thing was called Echo, it was priced at $199.

72
00:05:01,645 --> 00:05:05,585
Speaker 1:  You could buy one for $99. If, you were a prime member. And If,

73
00:05:05,585 --> 00:05:09,465
Speaker 1:  you got an invite. It, it was very odd thing. We'd never

74
00:05:09,545 --> 00:05:13,465
Speaker 1:  really seen anything like it before. And going back and reading over

75
00:05:13,465 --> 00:05:17,305
Speaker 1:  those early days, it's very funny to see the

76
00:05:17,545 --> 00:05:21,225
Speaker 1:  reaction to these things that it was like, why does this exist? What is it

77
00:05:21,225 --> 00:05:25,105
Speaker 1:  gonna be for? Why do I have this? And the big idea from

78
00:05:25,125 --> 00:05:28,025
Speaker 1:  Amazon was not voice assistance in general,

79
00:05:29,005 --> 00:05:32,705
Speaker 1:  but that it could be a different finite

80
00:05:33,145 --> 00:05:36,385
Speaker 1:  physical thing, that it could be a piece of furniture in your house and that

81
00:05:36,385 --> 00:05:38,305
Speaker 1:  that might change how you use it.

82
00:05:40,115 --> 00:05:43,395
Speaker 1:  I think that kind of worked. Amazon was right about a lot of things, but

83
00:05:43,395 --> 00:05:47,275
Speaker 1:  it was also wrong about a lot of things. And I think If, you rewind 10

84
00:05:47,275 --> 00:05:51,115
Speaker 1:  years, a lot has changed and also nothing

85
00:05:51,175 --> 00:05:54,555
Speaker 1:  has changed. Anyway, we have lots to cover. And there's also maybe,

86
00:05:55,085 --> 00:05:58,965
Speaker 1:  maybe some huge Alexa news coming for us that might

87
00:05:59,265 --> 00:06:02,645
Speaker 1:  change the next decade for all of this too. Lots to get to.

88
00:06:03,215 --> 00:06:06,685
Speaker 1:  We're doing this for the whole show. Let's just get into it. Jen Tuy.

89
00:06:06,895 --> 00:06:07,245
Speaker 1:  Hello.

90
00:06:07,705 --> 00:06:08,965
Speaker 3:  Hi, David. How are you doing?

91
00:06:09,665 --> 00:06:13,485
Speaker 1:  I'm good. I have, I have a sick child, So I. Haven't left my house in several

92
00:06:13,485 --> 00:06:17,405
Speaker 1:  days or slept in several days. So, I'm not a hundred percent

93
00:06:17,405 --> 00:06:21,285
Speaker 1:  sure not where I am or what's going on, but otherwise, otherwise we're

94
00:06:21,285 --> 00:06:22,005
Speaker 1:  good. Oh,

95
00:06:22,885 --> 00:06:25,405
Speaker 3:  I like delirious David. It could be really fun.

96
00:06:25,885 --> 00:06:29,085
Speaker 1:  This, this might be the longest VERGE cast in history. It's very possible.

97
00:06:29,725 --> 00:06:33,565
Speaker 1:  I, I take no responsibility for anything I say in the course of this segment.

98
00:06:33,835 --> 00:06:36,485
Speaker 1:  Okay. I should just say that at the beginning of every Vergecast, actually,

99
00:06:36,485 --> 00:06:39,845
Speaker 1:  that would be like the, the legal disclaimer that's like, this is all based

100
00:06:39,865 --> 00:06:42,645
Speaker 1:  on fictional people and characters, not real.

101
00:06:43,795 --> 00:06:47,645
Speaker 1:  Okay. So we're here to talk about Alexa because I guess Alexa

102
00:06:47,825 --> 00:06:50,725
Speaker 1:  is 10. I was trying to think back to this and I'm like, what was the beginning

103
00:06:51,505 --> 00:06:55,205
Speaker 1:  of Alexa? Because if, if I remember correctly, 10 years ago,

104
00:06:55,775 --> 00:06:58,645
Speaker 1:  Alexa started in a deeply strange way.

105
00:06:59,845 --> 00:07:03,385
Speaker 3:  Yes. It really was a very interesting launch. I think

106
00:07:03,895 --> 00:07:07,705
Speaker 3:  very much influenced by the spectacular failure of the fire

107
00:07:07,705 --> 00:07:09,785
Speaker 3:  phone a few months prior. Oh,

108
00:07:10,205 --> 00:07:11,905
Speaker 1:  Was that really only a few months before

109
00:07:12,045 --> 00:07:15,625
Speaker 3:  It happened? Just prior and Oh wow. I think I actually

110
00:07:15,685 --> 00:07:19,545
Speaker 3:  wasn't deeply into tech journalism at the time, but I, I remember

111
00:07:19,615 --> 00:07:23,185
Speaker 3:  some of this. I mean, you, you reviewed the original Echo, so you were right

112
00:07:23,185 --> 00:07:27,025
Speaker 3:  in the midst of all of this. But looking back, doing some research over the

113
00:07:27,025 --> 00:07:30,145
Speaker 3:  last few weeks, basically it sounded like

114
00:07:30,765 --> 00:07:34,425
Speaker 3:  Amazon had this really quite impressive new technology. They were looking

115
00:07:34,425 --> 00:07:38,145
Speaker 3:  to debut, but they were so burned by how badly the fire

116
00:07:38,145 --> 00:07:41,665
Speaker 3:  phone went down that they sort of stealth launched

117
00:07:42,085 --> 00:07:45,865
Speaker 3:  the, their, their New voice assistant Alexa, which came in the new

118
00:07:46,175 --> 00:07:50,025
Speaker 3:  Echo Smart speaker. And this was the first consumer smart speaker

119
00:07:50,455 --> 00:07:54,185
Speaker 3:  that anyone had seen. It just dropped with I think just a press

120
00:07:54,185 --> 00:07:58,025
Speaker 3:  release and a video. And there was a lot of, you know, reporting

121
00:07:58,025 --> 00:08:01,185
Speaker 3:  of like, whoa, where did this come from? This was, you know, a sort of stealth

122
00:08:01,185 --> 00:08:04,665
Speaker 3:  launch. And it was announced as their, you know, this was

123
00:08:05,545 --> 00:08:08,865
Speaker 3:  a smart speaker that you could talk to control with your voice.

124
00:08:09,975 --> 00:08:13,785
Speaker 3:  It's had sort of a few key experiences at the start, which was

125
00:08:13,785 --> 00:08:16,905
Speaker 3:  playing music, answering questions from Wikipedia.

126
00:08:18,125 --> 00:08:21,985
Speaker 3:  And it was, you know, it was a really exciting idea. And the tech

127
00:08:21,985 --> 00:08:25,545
Speaker 3:  press was very, you know, very into it. Lots of, oh my goodness,

128
00:08:25,645 --> 00:08:29,585
Speaker 3:  what's Amazon done here? And, you know, where, where is this gonna go?

129
00:08:29,645 --> 00:08:32,745
Speaker 3:  And there was a lot of excitement, but you couldn't actually get one. And

130
00:08:32,785 --> 00:08:36,745
Speaker 3:  I think they didn't even send out review units. This was how, how much they

131
00:08:36,745 --> 00:08:40,025
Speaker 3:  were, you know, learned from their previous mistakes. And I think,

132
00:08:40,255 --> 00:08:43,425
Speaker 3:  correct me if I'm wrong, the verb actually went out and bought one in order

133
00:08:43,425 --> 00:08:47,345
Speaker 3:  to get to review it. And this was, and it, you had to

134
00:08:47,345 --> 00:08:50,225
Speaker 3:  get on a wait list, which is something that Amazon's done subsequently with

135
00:08:50,225 --> 00:08:54,065
Speaker 3:  their sort of day one edition products that don't necessarily

136
00:08:54,295 --> 00:08:58,205
Speaker 3:  have a very clear use case yet. And they're kind

137
00:08:58,205 --> 00:09:02,085
Speaker 3:  of putting it out there to sort of explore where the community might

138
00:09:02,085 --> 00:09:05,605
Speaker 3:  might take it. And that was very much what Echo and Alexa

139
00:09:06,085 --> 00:09:09,405
Speaker 3:  launched as, as a sort of experiment.

140
00:09:10,125 --> 00:09:12,965
Speaker 3:  Although it, you know, they revealed later that they had been spending years

141
00:09:12,965 --> 00:09:16,525
Speaker 3:  working on this technology in particular the Farfield

142
00:09:16,635 --> 00:09:20,605
Speaker 3:  microphones, which gave you this really cool new ability

143
00:09:20,665 --> 00:09:24,525
Speaker 3:  to be able to talk to a voice assistant from anywhere in the room and not

144
00:09:24,525 --> 00:09:28,485
Speaker 3:  be tied to a computer or a phone. And privately, you know, the ambition

145
00:09:28,595 --> 00:09:31,885
Speaker 3:  that they revealed later was that this would be

146
00:09:32,675 --> 00:09:36,365
Speaker 3:  akin to Star Trek's computer, an all knowing ever

147
00:09:36,365 --> 00:09:40,045
Speaker 3:  present computer that can know what you want,

148
00:09:40,665 --> 00:09:44,205
Speaker 3:  do anything you ask it to, and you can converse with in natural

149
00:09:44,645 --> 00:09:48,285
Speaker 3:  language and do anything, control your lights,

150
00:09:49,035 --> 00:09:52,085
Speaker 3:  destroy your enemies, put up your shields.

151
00:09:53,225 --> 00:09:54,965
Speaker 3:  But you know, you get the, you get the idea,

152
00:09:55,285 --> 00:09:59,165
Speaker 1:  Small ambitions, really super, super chill idea from Amazon at

153
00:09:59,165 --> 00:09:59,365
Speaker 1:  the time.

154
00:10:00,505 --> 00:10:04,445
Speaker 3:  So really was exciting technology. So yeah, it, it came out in

155
00:10:04,445 --> 00:10:07,325
Speaker 3:  2014, November 6th was when they announced it.

156
00:10:08,565 --> 00:10:11,725
Speaker 3:  I think you could start to buy it within a few months.

157
00:10:12,445 --> 00:10:16,125
Speaker 3:  I believe the first review unit you reviewed was January time.

158
00:10:16,475 --> 00:10:20,005
Speaker 3:  Okay. It wasn't until spring of 2015

159
00:10:20,435 --> 00:10:24,085
Speaker 3:  that regular people could buy it. And then it finally went on

160
00:10:24,085 --> 00:10:27,805
Speaker 3:  general sale. You didn't have to sign up in,

161
00:10:28,165 --> 00:10:32,005
Speaker 3:  I think it was July of 2015. Wow. And I got mine under the

162
00:10:32,125 --> 00:10:32,525
Speaker 3:  Christmas tree.

163
00:10:34,045 --> 00:10:38,005
Speaker 3:  I put it on my Christmas list, my husband got it for me, and he's,

164
00:10:38,005 --> 00:10:41,725
Speaker 3:  he presented to me Christmas morning, he said, I have no idea

165
00:10:41,755 --> 00:10:45,045
Speaker 3:  what this is, why you want it, but it was on your list.

166
00:10:45,385 --> 00:10:47,565
Speaker 1:  You got the original tennis ball can,

167
00:10:47,745 --> 00:10:49,445
Speaker 3:  The original tennis ball. Yeah. Amazing.

168
00:10:50,125 --> 00:10:53,445
Speaker 1:  I still have mine around here somewhere. I Oh, did you? And I assume it still

169
00:10:53,445 --> 00:10:53,645
Speaker 1:  works.

170
00:10:54,155 --> 00:10:56,965
Speaker 3:  Mine does. Mine's in my husband's garage. That's awesome. And he uses it

171
00:10:56,965 --> 00:11:00,485
Speaker 3:  for music and it works. It works brilliantly. And actually, you know,

172
00:11:00,985 --> 00:11:04,845
Speaker 3:  at the same time for Christmas, I got him something off his list. I had

173
00:11:05,005 --> 00:11:08,925
Speaker 3:  no idea what it was either, it's something called a Craig Pocket Hole

174
00:11:09,105 --> 00:11:11,685
Speaker 3:  jig. I don't know if that, do you have any idea what that

175
00:11:12,225 --> 00:11:16,005
Speaker 1:  No, that sounds like something you can't legally say on a podcast. Like we're

176
00:11:16,005 --> 00:11:17,765
Speaker 1:  gonna get an explicit tag for that. I don't

177
00:11:17,765 --> 00:11:20,885
Speaker 3:  Know. So yeah, so, you know, this is a small peek into the chewy household

178
00:11:20,885 --> 00:11:23,885
Speaker 3:  there, but yes, it was quite funny. I mean, these two very different

179
00:11:24,715 --> 00:11:28,645
Speaker 3:  gadgets, what a pocket whole jig does, I found out is helps

180
00:11:28,795 --> 00:11:32,685
Speaker 3:  make holes in wood so you can build furniture. And in fact, he built

181
00:11:32,685 --> 00:11:35,925
Speaker 3:  the piece of furniture I'm podcasting from right now with that. So that's

182
00:11:35,925 --> 00:11:36,685
Speaker 3:  a nice full circle.

183
00:11:37,225 --> 00:11:40,805
Speaker 1:  Big Christmas in the Chewy household. Yes. 2015. So what's,

184
00:11:40,825 --> 00:11:44,605
Speaker 1:  what's wild about that is a, you've just brought up a bunch of really

185
00:11:44,605 --> 00:11:47,845
Speaker 1:  awful memories of trying to wrangle an echo to review.

186
00:11:48,625 --> 00:11:49,165
Speaker 1:  So thank you for

187
00:11:49,165 --> 00:11:50,565
Speaker 3:  That. I want to hear that story.

188
00:11:51,025 --> 00:11:54,765
Speaker 1:  So I think the, the two hardest times I've ever

189
00:11:54,765 --> 00:11:58,285
Speaker 1:  worked for a review unit of something were the, the

190
00:11:58,605 --> 00:12:01,805
Speaker 1:  original Snap spectacles, which I drove

191
00:12:02,825 --> 00:12:06,605
Speaker 1:  two and a half hours to a vending machine in Big Sir.

192
00:12:07,185 --> 00:12:11,005
Speaker 1:  And got there right after they sold out the last one. So I bought them from

193
00:12:11,045 --> 00:12:14,925
Speaker 1:  a stranger for $600. That was how I got Snap Spectacles,

194
00:12:14,965 --> 00:12:18,845
Speaker 1:  ssp the first time Snap Spec. I literally scalped Snap spectacles. And

195
00:12:18,845 --> 00:12:22,805
Speaker 1:  then the amount of time that I spent trying to

196
00:12:23,005 --> 00:12:26,645
Speaker 1:  convince someone at Amazon to send me an echo, I mean

197
00:12:26,785 --> 00:12:30,565
Speaker 1:  months of being like, I don't, I don't know how to be clear

198
00:12:30,565 --> 00:12:34,085
Speaker 1:  about this. We would like to review this. Can I have one please to review.

199
00:12:34,385 --> 00:12:35,645
Speaker 1:  And they were just like, ah,

200
00:12:36,765 --> 00:12:36,885
Speaker 3:  I

201
00:12:36,885 --> 00:12:40,725
Speaker 1:  Dunno. And so we just, we just bought one and reviewed it and I

202
00:12:40,725 --> 00:12:44,605
Speaker 1:  had forgotten the context they were launching this thing into, because

203
00:12:44,665 --> 00:12:48,205
Speaker 1:  it is really funny to look back, especially with

204
00:12:48,545 --> 00:12:52,365
Speaker 1:  how important Alexa really quickly became, like, I think

205
00:12:52,365 --> 00:12:55,925
Speaker 1:  it's very telling that it went from a, a secret launch to under your

206
00:12:56,165 --> 00:12:59,445
Speaker 1:  Christmas tree in 12 months. Yes. Like, it, it, once it happened, it happened

207
00:12:59,445 --> 00:13:00,725
Speaker 1:  really fast. It did,

208
00:13:00,725 --> 00:13:00,965
Speaker 3:  Yeah.

209
00:13:01,025 --> 00:13:04,845
Speaker 1:  But the way that it happened, it's, I could never tell if Amazon

210
00:13:04,845 --> 00:13:08,725
Speaker 1:  just wasn't sure if it was anything or was desperately afraid it

211
00:13:08,725 --> 00:13:12,485
Speaker 1:  was Fire Phone 2.0. And I think you're probably right that they were afraid

212
00:13:12,485 --> 00:13:16,205
Speaker 1:  it was Fire Phone 2.0 because like you said, it has also come out since then

213
00:13:16,205 --> 00:13:19,605
Speaker 1:  that this is a thing they'd been working on forever. Jeff Bezos was very

214
00:13:19,605 --> 00:13:22,965
Speaker 1:  behind it and very excited about it and had big ideas about it. They were

215
00:13:22,965 --> 00:13:26,925
Speaker 1:  just terrified it wasn't gonna go well, I guess. Yeah. And this was

216
00:13:26,925 --> 00:13:30,405
Speaker 1:  before Amazon was just in full on like Yolo hardware mode. We're just gonna

217
00:13:30,405 --> 00:13:33,485
Speaker 1:  try stuff and who cares if it goes well? Like it was a big bet that they

218
00:13:33,485 --> 00:13:35,565
Speaker 1:  kind of didn't want you to think was a big bet.

219
00:13:35,745 --> 00:13:39,645
Speaker 3:  Yes, yes. I mean, it was almost sort of like a, you know, as said,

220
00:13:39,645 --> 00:13:43,405
Speaker 3:  like a test gadget. Like, oh, maybe even if a developer gadget, because you

221
00:13:43,405 --> 00:13:46,765
Speaker 3:  know, that's what they pushed really hard was developers developing skills

222
00:13:46,985 --> 00:13:50,405
Speaker 3:  for Alexa to give it more capabilities.

223
00:13:51,105 --> 00:13:54,325
Speaker 3:  And that's something they've done, they've followed through on in the 10

224
00:13:54,325 --> 00:13:57,485
Speaker 3:  years we've had the Echo devices. You know, it's not

225
00:13:58,445 --> 00:14:02,365
Speaker 3:  a locked down proprietary device that, you know, is trying to lock you

226
00:14:02,605 --> 00:14:06,525
Speaker 3:  into. Oh. So all of my devices in my office

227
00:14:06,945 --> 00:14:10,645
Speaker 3:  are not designed to respond, but unfortunately the devices

228
00:14:10,825 --> 00:14:13,805
Speaker 3:  in my living room, gimme one second. Yeah,

229
00:14:13,805 --> 00:14:14,165
Speaker 1:  You're good.

230
00:14:15,335 --> 00:14:19,155
Speaker 3:  Alexa. Turn on Do not disturb. Sorry about that. I think I've got 'em all.

231
00:14:19,615 --> 00:14:23,435
Speaker 3:  So it wasn't a lockdown proprietary device designed

232
00:14:23,535 --> 00:14:27,355
Speaker 3:  to sort of keep you in Amazon's ecosystem. You know, say like an iPhone

233
00:14:27,355 --> 00:14:31,075
Speaker 3:  was at the time. It really was a sort of, let's see what this can do.

234
00:14:31,615 --> 00:14:34,875
Speaker 3:  And you know, they developed, they had software development kits, they even

235
00:14:34,875 --> 00:14:38,195
Speaker 3:  had Far phone, the far field microphone kit. They,

236
00:14:38,755 --> 00:14:42,715
Speaker 3:  they allowed other manufacturers to put the technology

237
00:14:42,865 --> 00:14:46,475
Speaker 3:  into their devices. It really was sort of like, let's see what we can do

238
00:14:46,475 --> 00:14:50,035
Speaker 3:  with this new technology. Which was exciting. It was very exciting time,

239
00:14:50,155 --> 00:14:50,355
Speaker 3:  I think.

240
00:14:51,065 --> 00:14:54,885
Speaker 1:  And it worked really well. Like, I went back and read my review

241
00:14:54,885 --> 00:14:58,845
Speaker 1:  from January of 2015, and on the one hand I had a bunch

242
00:14:58,845 --> 00:15:02,245
Speaker 1:  of issues using the thing, but on the other hand, I still have all those

243
00:15:02,245 --> 00:15:06,045
Speaker 1:  same issues 10 years later, which we'll get to. But I think the

244
00:15:06,045 --> 00:15:09,965
Speaker 1:  extent to which Amazon made sort of a small

245
00:15:09,965 --> 00:15:13,725
Speaker 1:  number of promises about what this thing was gonna be really good at and

246
00:15:13,725 --> 00:15:17,685
Speaker 1:  was basically a hundred percent correct. Yeah. Is really kind of

247
00:15:17,685 --> 00:15:20,005
Speaker 1:  incredible. They're like, you're gonna use this thing for music, you're gonna

248
00:15:20,005 --> 00:15:23,285
Speaker 1:  use it for basic informational lookups. You're gonna use it to set timers,

249
00:15:23,285 --> 00:15:27,125
Speaker 1:  you're gonna use it to convert, you know, grams to ounces and like check,

250
00:15:27,175 --> 00:15:30,405
Speaker 1:  check and check. They, they like really

251
00:15:30,735 --> 00:15:34,205
Speaker 1:  understood what the thing was for, from the very beginning in a way that

252
00:15:34,325 --> 00:15:37,645
Speaker 1:  I had forgotten how correct. They, they really were,

253
00:15:38,755 --> 00:15:42,485
Speaker 3:  They were for I, I agree. And you know, it did have, in those early days,

254
00:15:42,665 --> 00:15:46,005
Speaker 3:  it was a big success. You know, I think within the first two years they'd

255
00:15:46,325 --> 00:15:50,005
Speaker 3:  sold 5 million devices as the reports I'd seen, which I mean,

256
00:15:50,115 --> 00:15:53,845
Speaker 3:  it's not iPhone, but, you know, it's still, it's impressive for a brand new

257
00:15:54,335 --> 00:15:55,325
Speaker 3:  piece of technology.

258
00:15:55,675 --> 00:15:58,965
Speaker 1:  Yeah. I mean, it's easy to, it's easy to forget now, but like smart speaker

259
00:15:59,105 --> 00:16:02,085
Speaker 1:  was not a thing. No. That existed before. Like, nobody even knew how to,

260
00:16:02,345 --> 00:16:06,245
Speaker 1:  why you would want this in your house or what it would do. And the,

261
00:16:06,845 --> 00:16:10,045
Speaker 1:  I I don't know, If you remember the moment, but I do of like

262
00:16:10,635 --> 00:16:14,205
Speaker 1:  walking in and the, the moment where you're just like, play,

263
00:16:14,625 --> 00:16:17,445
Speaker 1:  you know, play whatever song, play Beyonce, play Taylor Swift, and it just

264
00:16:17,445 --> 00:16:20,965
Speaker 1:  does. Yeah. And all of a sudden you're like, oh, I want everything to work

265
00:16:20,965 --> 00:16:24,885
Speaker 1:  like that. And like, spoiler alert, it doesn't. But there were e even

266
00:16:24,885 --> 00:16:27,325
Speaker 1:  at the very beginning, there were just enough of those moments that it was

267
00:16:27,325 --> 00:16:31,245
Speaker 1:  like, as soon as you were in a room with an echo, you kind of got

268
00:16:31,245 --> 00:16:34,685
Speaker 1:  it. And I feel like it was, there was like a really cool virality to that

269
00:16:34,685 --> 00:16:36,565
Speaker 1:  thing at the very beginning that was very powerful.

270
00:16:36,875 --> 00:16:40,845
Speaker 3:  Yeah. And what, for me as, as someone who was really just getting, I

271
00:16:40,845 --> 00:16:44,325
Speaker 3:  said into sort of tech journalism, but had always been a tech hobbyist, it

272
00:16:44,325 --> 00:16:48,205
Speaker 3:  was a great, it was a real revelation to bring this type of technology

273
00:16:48,315 --> 00:16:52,125
Speaker 3:  into my home as opposed to being in my hand or on my lap or on my

274
00:16:52,125 --> 00:16:56,085
Speaker 3:  desktop. Especially for my family. I mean, my kids were very young at,

275
00:16:56,105 --> 00:16:59,645
Speaker 3:  at this age. I mean, I think my son was three when Alexa,

276
00:17:00,385 --> 00:17:04,285
Speaker 3:  or three or four, and my daughter was maybe two. And you

277
00:17:04,285 --> 00:17:07,965
Speaker 3:  know, we were, it was great to be able to just call out and, you know,

278
00:17:08,435 --> 00:17:12,045
Speaker 3:  play Beyonce and have a dance party and tell

279
00:17:12,045 --> 00:17:15,685
Speaker 3:  stories and jokes and it really, it, it helped

280
00:17:15,895 --> 00:17:19,805
Speaker 3:  bring technology into a much more usable

281
00:17:19,975 --> 00:17:23,805
Speaker 3:  space in our homes. And, you know, I even at that age, at that year,

282
00:17:23,865 --> 00:17:27,645
Speaker 3:  in that time, I was feeling that sort of parent guilt

283
00:17:27,665 --> 00:17:31,365
Speaker 3:  of being on devices and, but, but there's so much still that's great

284
00:17:31,415 --> 00:17:35,125
Speaker 3:  about technology for being a parent, you know, playing

285
00:17:35,135 --> 00:17:38,965
Speaker 3:  music and reading audio books, things that we, you know,

286
00:17:39,045 --> 00:17:41,485
Speaker 3:  I al I wanted to do with my kids, but I didn't wanna have to pull out my

287
00:17:41,485 --> 00:17:44,845
Speaker 3:  phone to do it. So it really opened that sort of accessibility

288
00:17:45,385 --> 00:17:49,205
Speaker 3:  to technology in the home. And that's, and I felt like there was

289
00:17:49,265 --> 00:17:52,845
Speaker 3:  so much potential and so much that was exciting at the time,

290
00:17:53,625 --> 00:17:57,205
Speaker 3:  but it also felt like we never really went that far beyond.

291
00:17:57,745 --> 00:18:01,285
Speaker 3:  and we did get a lot of promises

292
00:18:01,475 --> 00:18:05,165
Speaker 3:  from not, maybe not promises might be too strong a word, but a lot of very

293
00:18:05,225 --> 00:18:09,165
Speaker 3:  strong indications from Amazon and, and Jeff Bezos and, and

294
00:18:09,395 --> 00:18:12,725
Speaker 3:  Dave Limp the devices and services head at the time that

295
00:18:13,195 --> 00:18:17,005
Speaker 3:  this was going to be so much more, this was

296
00:18:17,005 --> 00:18:20,885
Speaker 3:  going to be the Star Trek computer or this, that was what they

297
00:18:20,885 --> 00:18:24,405
Speaker 3:  were working towards. And it has been really interesting

298
00:18:24,665 --> 00:18:27,685
Speaker 3:  to kind of see that they have not got there

299
00:18:28,865 --> 00:18:32,605
Speaker 3:  and that we do use Alexa today very much. And I was,

300
00:18:32,605 --> 00:18:36,125
Speaker 3:  when I was sort of going back through the research here, I looked at my list

301
00:18:36,265 --> 00:18:40,045
Speaker 3:  of what I asked my device to do today versus

302
00:18:40,195 --> 00:18:44,005
Speaker 3:  what I did, you know, five years ago or seven years ago. And

303
00:18:44,005 --> 00:18:47,685
Speaker 3:  it really hasn't changed that much. And that, that that sort of

304
00:18:48,225 --> 00:18:52,115
Speaker 3:  key difference feels like a big failure. And I'll

305
00:18:52,115 --> 00:18:55,995
Speaker 3:  tell you the reason why is because it's just not that reliable is, you know,

306
00:18:55,995 --> 00:18:59,115
Speaker 3:  the long and the short of it. And this is something I've seen, you know,

307
00:18:59,215 --> 00:19:02,435
Speaker 3:  I'm, I'm in many user groups, Facebook groups,

308
00:19:03,075 --> 00:19:06,835
Speaker 3:  discords, Reddits forums where people who

309
00:19:06,935 --> 00:19:10,715
Speaker 3:  use Echo and Alexa complain or praise

310
00:19:10,855 --> 00:19:14,795
Speaker 3:  or discuss the device and, and the assistant. And it just seems

311
00:19:14,795 --> 00:19:18,115
Speaker 3:  like there's a universal consensus that it kind of hits a plateau

312
00:19:19,255 --> 00:19:23,035
Speaker 3:  and it can still mostly do those basic things,

313
00:19:23,655 --> 00:19:26,355
Speaker 3:  but there really hasn't been that next

314
00:19:27,435 --> 00:19:31,315
Speaker 3:  exciting use case that next exciting development for

315
00:19:31,655 --> 00:19:35,475
Speaker 3:  the technology. There's some really interesting use cases, especially around

316
00:19:35,475 --> 00:19:39,115
Speaker 3:  accessibility and elder care where, you know, they're

317
00:19:39,475 --> 00:19:42,675
Speaker 3:  valuable use cases. People being able, you know, who have

318
00:19:43,075 --> 00:19:47,035
Speaker 3:  accessibility challenges, being able to, you know, open your shades

319
00:19:47,035 --> 00:19:50,515
Speaker 3:  or turn your lights on and off, being able to, we've got, we had a,

320
00:19:51,005 --> 00:19:54,195
Speaker 3:  we've written, I've written a number of pieces about using Amazon devices

321
00:19:54,455 --> 00:19:58,435
Speaker 3:  to look after elderly parents. There's so many key use cases,

322
00:19:58,695 --> 00:20:02,235
Speaker 3:  but they are solving specific problems. That

323
00:20:02,595 --> 00:20:06,555
Speaker 3:  ultimate idea of this ambient voice

324
00:20:06,555 --> 00:20:10,315
Speaker 3:  assistant that can manage your home, that can

325
00:20:10,335 --> 00:20:14,195
Speaker 3:  be like computer in Star Trek and

326
00:20:14,375 --> 00:20:18,235
Speaker 3:  be this sort of omniscient, omnipresent, artificial intelligence

327
00:20:18,895 --> 00:20:22,595
Speaker 3:  hasn't arrived. Maybe a lot of people don't want it, but I do.

328
00:20:23,655 --> 00:20:27,515
Speaker 3:  And I'm, I was excited to see that come and I feel like we just almost

329
00:20:27,515 --> 00:20:31,395
Speaker 3:  feel like further away from that than we were. A lot of

330
00:20:31,395 --> 00:20:34,915
Speaker 3:  problems I encounter when using Alexa in my home. Is

331
00:20:35,495 --> 00:20:39,395
Speaker 3:  it not doing what I expect it to do if I have set up sort of a

332
00:20:39,395 --> 00:20:43,355
Speaker 3:  more complicated routine that I've tried to get it to do things like lock

333
00:20:43,375 --> 00:20:47,365
Speaker 3:  my back door, adjust my thermostat, turn the lights off, dim them in

334
00:20:47,365 --> 00:20:51,005
Speaker 3:  this room, you know, turn the TV on all, you know,

335
00:20:51,405 --> 00:20:54,325
Speaker 3:  multiple different things at once, which is, you know, one of the sort of

336
00:20:54,555 --> 00:20:58,405
Speaker 3:  core use cases now of the smart home side of Alexa. Nine times

337
00:20:58,465 --> 00:21:02,045
Speaker 3:  out of 10 something doesn't work or it doesn't hear me correctly and it does

338
00:21:02,045 --> 00:21:05,885
Speaker 3:  something differently from what I've asked it to do. And so that's

339
00:21:06,005 --> 00:21:09,005
Speaker 3:  happened, that happens to me so often that I have basically got to the point

340
00:21:09,025 --> 00:21:12,845
Speaker 3:  now where all I use it for is to turn lights on and off other than

341
00:21:12,845 --> 00:21:16,685
Speaker 3:  those core use cases of playing music in certain timers, which I think

342
00:21:16,965 --> 00:21:19,805
Speaker 3:  everyone can agree Alexa is great at. It's

343
00:21:19,805 --> 00:21:23,005
Speaker 1:  Very good at it. Well, and and I think, I think that's, that's kind of the

344
00:21:23,585 --> 00:21:27,045
Speaker 1:  key open question for me with a lot of this stuff is

345
00:21:27,825 --> 00:21:31,725
Speaker 1:  who and what is to blame for that fact, because I actually think there

346
00:21:31,725 --> 00:21:35,325
Speaker 1:  are a bunch of possibilities, right? There's, there's one world in which

347
00:21:35,325 --> 00:21:39,085
Speaker 1:  you say, okay, Amazon just didn't do it right.

348
00:21:39,515 --> 00:21:43,325
Speaker 1:  Like you, you could as a sort of failure of technology. The the far field

349
00:21:43,325 --> 00:21:45,965
Speaker 1:  microphone doesn't work, the natural language processing doesn't work, whatever

350
00:21:46,535 --> 00:21:49,085
Speaker 1:  cards on the table. I actually don't think it's that one, but I'd be curious

351
00:21:49,085 --> 00:21:53,005
Speaker 1:  to know, If, you do, there's a version of it that says it's kind

352
00:21:53,005 --> 00:21:56,805
Speaker 1:  of an ecosystem problem that actually what happened is there's just not enough

353
00:21:56,805 --> 00:21:59,725
Speaker 1:  stuff that it's even theoretically able to do that. Like

354
00:22:01,105 --> 00:22:04,525
Speaker 1:  the problem with building a Star Trek computer is like the Star Trek computer

355
00:22:04,545 --> 00:22:08,285
Speaker 1:  can do an awful lot of stuff and you have to build all that stuff and they

356
00:22:08,285 --> 00:22:12,125
Speaker 1:  never did. And then there's a third thing that that basically says

357
00:22:12,125 --> 00:22:15,565
Speaker 1:  like maybe this just isn't the right interface to do

358
00:22:16,025 --> 00:22:20,005
Speaker 1:  all of that stuff. And I think there might even be other possibilities,

359
00:22:20,025 --> 00:22:23,805
Speaker 1:  but I keep coming back to like, I agree that we didn't get the Star Trek

360
00:22:23,965 --> 00:22:26,445
Speaker 1:  computer, right? Like, I think everyone agrees we didn't get the Star Trek

361
00:22:26,565 --> 00:22:30,525
Speaker 1:  computer, but why didn't we, the longer we

362
00:22:30,525 --> 00:22:34,445
Speaker 1:  go, the more complicated that question becomes to me. And

363
00:22:34,445 --> 00:22:36,885
Speaker 1:  I'm, I know you've thought a lot about it this in research a lot about it.

364
00:22:36,885 --> 00:22:40,845
Speaker 1:  Like do you do, do you have a thing over the decade of Alexa you

365
00:22:40,845 --> 00:22:43,165
Speaker 1:  can pinpoint to be like, this is where we kinda lost the thread.

366
00:22:44,195 --> 00:22:45,725
Speaker 3:  Yeah. I think capitalism

367
00:22:48,795 --> 00:22:52,765
Speaker 3:  fair, I think Alexa and Echo started out as a, you

368
00:22:52,765 --> 00:22:55,925
Speaker 3:  know, a very ambitious, exciting new technology.

369
00:22:56,745 --> 00:23:00,645
Speaker 3:  And then Amazon was like, oh wow, we've sold, we've sold 5 million

370
00:23:00,665 --> 00:23:04,245
Speaker 3:  of these in two years. Let's make hundreds more

371
00:23:04,865 --> 00:23:08,805
Speaker 3:  of all shapes and sizes. Let's put it in a ring. Let's put it in

372
00:23:08,955 --> 00:23:12,765
Speaker 3:  earbuds, let's put it in cars. Let's, let's make small ones fat

373
00:23:12,765 --> 00:23:16,485
Speaker 3:  ones short. Ones tall ones, ones with screens. And

374
00:23:16,635 --> 00:23:20,605
Speaker 3:  they, I think the company focused far too much

375
00:23:20,605 --> 00:23:24,005
Speaker 3:  time developing new, killing

376
00:23:24,435 --> 00:23:27,925
Speaker 3:  more echo devices. Yeah. When they didn't work.

377
00:23:29,225 --> 00:23:32,845
Speaker 3:  Trying to expand this technology into

378
00:23:32,935 --> 00:23:36,765
Speaker 3:  every corner of our lives. Having encouraging other companies to make

379
00:23:36,935 --> 00:23:40,445
Speaker 3:  Alexa powered speakers and devices, putting it in

380
00:23:40,445 --> 00:23:44,325
Speaker 3:  microwaves and clocks and a, a lot of energy

381
00:23:44,325 --> 00:23:48,085
Speaker 3:  on that. Not enough energy on actually

382
00:23:48,085 --> 00:23:52,045
Speaker 3:  developing the core technology. And I'm not an engineer and as you say, I,

383
00:23:52,325 --> 00:23:55,805
Speaker 3:  I think that has been a challenge that whether the technology is actually

384
00:23:55,805 --> 00:23:59,165
Speaker 3:  at the point where we could have had a better Alexa sooner than

385
00:23:59,615 --> 00:24:03,525
Speaker 3:  today or a few years time, but that doesn't mean it couldn't have got

386
00:24:03,525 --> 00:24:03,805
Speaker 3:  better

387
00:24:05,585 --> 00:24:09,245
Speaker 3:  in incrementally. And there have been, you know, there's a many more

388
00:24:10,135 --> 00:24:14,055
Speaker 3:  capabilities of the voice assistant today than there was

389
00:24:14,205 --> 00:24:17,815
Speaker 3:  when it started. But the, the interface which you

390
00:24:17,965 --> 00:24:21,855
Speaker 3:  alluded to is the problem. There, there is so many things it can do.

391
00:24:22,705 --> 00:24:26,575
Speaker 3:  Maybe not very many things you want it to do, but there are many, many things

392
00:24:26,715 --> 00:24:30,615
Speaker 3:  it can do. You know, fart jokes is the obvious one.

393
00:24:31,485 --> 00:24:33,135
Speaker 3:  Lots of lots of great

394
00:24:34,805 --> 00:24:38,175
Speaker 3:  comedic and entertainment things, not as so many

395
00:24:39,075 --> 00:24:42,975
Speaker 3:  useful things. Like you want an as when you think of an assistant, I want

396
00:24:42,995 --> 00:24:46,855
Speaker 3:  an assistant to, you know, order me a pizza or I want an assistant

397
00:24:46,875 --> 00:24:50,695
Speaker 3:  to plan my day for me. Those types of things never got better. You could

398
00:24:50,695 --> 00:24:54,015
Speaker 3:  connect your Google account, your Google calendar to it, or you could

399
00:24:54,475 --> 00:24:58,215
Speaker 3:  set up, you know, your photos through Amazon photos, but there just

400
00:24:58,215 --> 00:25:02,015
Speaker 3:  weren't very many core good use

401
00:25:02,015 --> 00:25:05,735
Speaker 3:  cases that meant I need to use this device every day. It was very,

402
00:25:05,765 --> 00:25:09,575
Speaker 3:  it's been very much, it's fun to use this device, but there

403
00:25:09,575 --> 00:25:13,535
Speaker 3:  was no kind of breakthrough. This is why I have to have this device

404
00:25:13,535 --> 00:25:17,455
Speaker 3:  in my home other than some of those niche, more niche use cases we discussed

405
00:25:17,455 --> 00:25:21,235
Speaker 3:  earlier. And I I feel like, I mean, Amazon's a hardware company,

406
00:25:21,595 --> 00:25:25,515
Speaker 3:  although it started out as a website, it's really ma it has

407
00:25:25,535 --> 00:25:29,275
Speaker 3:  become a hardware company. Yeah. Every, I mean there's a, you know, the

408
00:25:29,335 --> 00:25:33,315
Speaker 3:  in VERGE law and tech journalist and law, when you go to an Amazon event

409
00:25:33,315 --> 00:25:36,955
Speaker 3:  event, one of their four hardware events, you were prepared to write up

410
00:25:36,955 --> 00:25:39,475
Speaker 3:  75 new gadgets in 30 minutes.

411
00:25:40,185 --> 00:25:44,115
Speaker 1:  What was it? Was it 2018 the year that it was just 700,000

412
00:25:44,295 --> 00:25:47,955
Speaker 1:  new Alexa gadgets all at once? That was the year of the clock and the microwave,

413
00:25:47,955 --> 00:25:51,315
Speaker 1:  right? And yeah, everything, anything that exists in your house now it has

414
00:25:51,315 --> 00:25:51,915
Speaker 1:  Alexa now

415
00:25:51,915 --> 00:25:52,395
Speaker 3:  It has it. Yes.

416
00:25:52,455 --> 00:25:55,835
Speaker 1:  Do you want that? Doesn't matter. You have it now. That was the, that was

417
00:25:55,835 --> 00:25:56,555
Speaker 1:  the pitch. But

418
00:25:56,555 --> 00:26:00,355
Speaker 3:  I could, I mean, I can see the value there, like connect all the things

419
00:26:00,855 --> 00:26:04,315
Speaker 3:  and then Alexa can do everything you want it to do. But it was the getting

420
00:26:04,365 --> 00:26:07,235
Speaker 3:  Alexa to do everything you wanted to do part that they never

421
00:26:08,105 --> 00:26:11,995
Speaker 3:  made better. You had to use the Alexa app, which is one of, is

422
00:26:12,235 --> 00:26:15,635
Speaker 3:  probably the single worst piece of smart home software I have ever used

423
00:26:16,975 --> 00:26:20,435
Speaker 3:  it. I agree with that. It has incrementally got better, but it is still

424
00:26:21,055 --> 00:26:22,435
Speaker 3:  really difficult. I

425
00:26:22,435 --> 00:26:25,755
Speaker 1:  Was just about to say it's, it's night and day better than it was even a

426
00:26:25,755 --> 00:26:29,195
Speaker 1:  few years ago and it's still pretty bad. Like it's, it's rough.

427
00:26:30,655 --> 00:26:34,555
Speaker 3:  And they've tried to address that, you know, they came out with the new Echo

428
00:26:34,575 --> 00:26:38,115
Speaker 3:  hub, so you have an interface that you can use. Yeah. But again,

429
00:26:38,895 --> 00:26:42,795
Speaker 3:  it was the voice interface is what needed to get better. The voice interface

430
00:26:42,795 --> 00:26:46,325
Speaker 3:  needed to be, we need to be able to talk to our voice

431
00:26:46,325 --> 00:26:50,205
Speaker 3:  assistant in the way you can talk to str well not you can, but

432
00:26:50,265 --> 00:26:54,245
Speaker 3:  Picard could talk to Star Trek's computer. Right. Just say what you're thinking,

433
00:26:54,385 --> 00:26:58,365
Speaker 3:  say what you want to happen and it do it for you. It understand and

434
00:26:58,365 --> 00:27:02,245
Speaker 3:  be able to do it for you. And we've never got anywhere closer. Alexa's never

435
00:27:02,245 --> 00:27:06,045
Speaker 3:  understood better what you want than she did on day

436
00:27:06,045 --> 00:27:08,245
Speaker 3:  one. It, it, that never got better. Which

437
00:27:08,285 --> 00:27:11,005
Speaker 1:  I, I think is that, I think that's the perfect way to put it. Because one

438
00:27:11,005 --> 00:27:14,725
Speaker 1:  of the things I always enjoy about using Alexa, just from a sort of like

439
00:27:15,445 --> 00:27:19,285
Speaker 1:  perverse doing technology science work, is

440
00:27:19,305 --> 00:27:23,165
Speaker 1:  you go in and you look at something that went wrong and just

441
00:27:23,505 --> 00:27:27,245
Speaker 1:  the question is always like, did it misunderstand me? And actually it very

442
00:27:27,245 --> 00:27:31,165
Speaker 1:  rarely does. At least in my case, I go in and it, like, I can

443
00:27:31,185 --> 00:27:35,005
Speaker 1:  say even very complicated long, you know, multi

444
00:27:35,195 --> 00:27:39,125
Speaker 1:  word, long sentence things. It's transcription is very

445
00:27:39,125 --> 00:27:42,325
Speaker 1:  good and, and it's, it's natural language processing is very good. And so

446
00:27:42,325 --> 00:27:45,805
Speaker 1:  it's like it knows what I want. Right? Like that is not

447
00:27:46,185 --> 00:27:50,085
Speaker 1:  the challenge. No. And you could think that yelling at a speaker from across

448
00:27:50,085 --> 00:27:53,325
Speaker 1:  the room would be a hard problem. And it, and it is, but actually again,

449
00:27:53,325 --> 00:27:57,165
Speaker 1:  from pretty close to the very beginning, Amazon more or

450
00:27:57,285 --> 00:28:01,085
Speaker 1:  less solved that problem. Yeah. There's somewhere after that first

451
00:28:01,085 --> 00:28:04,925
Speaker 1:  step of like, I have declared my intention to my device and

452
00:28:04,925 --> 00:28:08,845
Speaker 1:  my device has registered that intention correctly and that it falls apart

453
00:28:09,105 --> 00:28:12,845
Speaker 1:  and it has never, that part, like you're saying, has never gotten, I I would

454
00:28:12,845 --> 00:28:16,485
Speaker 1:  say even meaningfully better, much less very good.

455
00:28:16,905 --> 00:28:20,845
Speaker 3:  No, I agree. And and that's where the Star Trek computer idea came

456
00:28:20,845 --> 00:28:24,805
Speaker 3:  in. You know, that this device could sort of manage your home

457
00:28:24,905 --> 00:28:28,765
Speaker 3:  for you. I mean, Alexa leaned hard into the smart home as

458
00:28:28,765 --> 00:28:31,645
Speaker 3:  soon as they realized that this was, you know, a great use case for voice

459
00:28:31,645 --> 00:28:35,005
Speaker 3:  control. You know, even though it wasn't part of the initial plan,

460
00:28:35,705 --> 00:28:39,325
Speaker 3:  but to begin with it was incredibly clunky. And

461
00:28:39,505 --> 00:28:43,125
Speaker 3:  you know, I I've spent the last eight years

462
00:28:43,385 --> 00:28:47,125
Speaker 3:  trying to use Alexa to run my smart home smoothly.

463
00:28:47,625 --> 00:28:51,365
Speaker 3:  And my sort of touch point throughout this time has been like a good

464
00:28:51,365 --> 00:28:54,565
Speaker 3:  morning routine. Like this is what you always hear about when you talk about

465
00:28:55,575 --> 00:28:58,925
Speaker 3:  voice control and smart home. It's like you can set up this good morning

466
00:28:58,925 --> 00:29:01,925
Speaker 3:  routine, goodnight routine. I'm leaving routine and your whole home will

467
00:29:01,925 --> 00:29:05,365
Speaker 3:  just work magically. And I, from the days of Wink,

468
00:29:06,085 --> 00:29:08,685
Speaker 3:  remember Wink. Oh yeah. That was one of the first times I try

469
00:29:08,685 --> 00:29:11,365
Speaker 1:  Not to remember Wink, but every once in a while

470
00:29:12,195 --> 00:29:15,525
Speaker 3:  That was one of those first smart home hubs that worked with Alexa

471
00:29:16,165 --> 00:29:20,085
Speaker 3:  and you used to have to say trigger my good morning routine. And my

472
00:29:20,085 --> 00:29:23,845
Speaker 3:  husband still says trigger to every time he asks Alexa to do

473
00:29:23,845 --> 00:29:27,085
Speaker 3:  something. Oh my goodness. Sorry.

474
00:29:27,795 --> 00:29:30,925
Speaker 1:  This is good. We're leaving all of these in. This is, this is the point people,

475
00:29:31,245 --> 00:29:31,405
Speaker 1:  this

476
00:29:31,405 --> 00:29:32,645
Speaker 3:  Is there so many in my house,

477
00:29:34,775 --> 00:29:38,685
Speaker 3:  Alexa do not disturb. See this? It never

478
00:29:38,685 --> 00:29:42,445
Speaker 3:  gets right. But yes. So, and over the years I have

479
00:29:42,495 --> 00:29:45,765
Speaker 3:  tried to, you know, it sort of as I've been my touch point, like whether

480
00:29:45,825 --> 00:29:49,525
Speaker 3:  we have reached a, a sort of magical moment where

481
00:29:49,905 --> 00:29:53,725
Speaker 3:  the smart home just works and you know, this kind of goes back, you know,

482
00:29:53,915 --> 00:29:57,605
Speaker 3:  circles back to Star Trek with, you know, I think the

483
00:29:57,685 --> 00:30:01,325
Speaker 3:  infamous line and I can deliver it in an English accent

484
00:30:01,625 --> 00:30:02,645
Speaker 3:  and I will do my best

485
00:30:04,245 --> 00:30:07,245
Speaker 3:  computer tea L Gray hot.

486
00:30:08,025 --> 00:30:09,165
Speaker 3:  Ooh, that was good.

487
00:30:10,885 --> 00:30:14,005
Speaker 3:  I don't do a good card. Sorry, I have to get my Shakespeare on. It was solid.

488
00:30:14,405 --> 00:30:18,245
Speaker 3:  I I approved that, but yes. You know, so, and for me it was

489
00:30:18,245 --> 00:30:22,085
Speaker 3:  coffee, black strong and I wanted, I

490
00:30:22,085 --> 00:30:25,935
Speaker 3:  wanted to get outta my bed, I wanted the lights to turn on. I

491
00:30:25,935 --> 00:30:29,655
Speaker 3:  wanted BBC radio two to start playing. I wanted a

492
00:30:29,655 --> 00:30:33,055
Speaker 3:  coffee to start brewing, maybe, you know, my shower to start running

493
00:30:33,265 --> 00:30:36,775
Speaker 3:  thermostat to adjust and then say 20 minutes later,

494
00:30:37,235 --> 00:30:41,095
Speaker 3:  the kids' lights in their room go on, their alarm goes off. You know,

495
00:30:41,475 --> 00:30:45,255
Speaker 3:  things that I would have to go and do manually myself. But having

496
00:30:45,415 --> 00:30:49,215
Speaker 3:  a smart home assistant do all of this for me, save me time in the morning,

497
00:30:49,485 --> 00:30:53,095
Speaker 3:  make me less stressed, make everyone happy. Right. And so

498
00:30:53,315 --> 00:30:57,215
Speaker 3:  the first problem I came across was a personal one,

499
00:30:57,235 --> 00:31:00,815
Speaker 3:  but one that really kind of goes to another key area where

500
00:31:01,355 --> 00:31:05,055
Speaker 3:  the smart home in general and Alexa hasn't succeeded, which is context.

501
00:31:05,475 --> 00:31:09,455
Speaker 3:  So my husband has been for most of his career a shift worker.

502
00:31:09,595 --> 00:31:13,295
Speaker 3:  So he works 24 hours shifts and then he would come home and sleep for

503
00:31:13,715 --> 00:31:17,535
Speaker 3:  12 hours. So for me, having a motion sensor or using Voice

504
00:31:17,675 --> 00:31:21,495
Speaker 3:  to start my morning routine was not gonna work because

505
00:31:21,615 --> 00:31:24,655
Speaker 3:  I would wake him up. And also I only wanted the bedroom lights to turn on

506
00:31:24,655 --> 00:31:28,055
Speaker 3:  when he wasn't there, not when he was there. So, you know, there was nothing,

507
00:31:28,055 --> 00:31:31,655
Speaker 3:  there's nothing that Alexa could do to differentiate, didn't have the context

508
00:31:31,915 --> 00:31:35,535
Speaker 3:  of what was happening in, in the room. Then the other issue was

509
00:31:35,635 --> 00:31:39,495
Speaker 3:  trying to use a motion sensor to trigger a routine is something

510
00:31:39,495 --> 00:31:43,015
Speaker 3:  you've only really been able to do in the last couple of years. Motion sensing

511
00:31:43,525 --> 00:31:47,295
Speaker 3:  support for Alexa was really spotty for years. It

512
00:31:47,295 --> 00:31:51,135
Speaker 3:  worked through ZigBee initially, but nine times out of 10, a

513
00:31:51,135 --> 00:31:54,495
Speaker 3:  ZigBee motion sensor you connected to Echo would either not stay connected

514
00:31:54,915 --> 00:31:58,295
Speaker 3:  or wouldn't work. Yeah. And again, maybe not

515
00:31:58,375 --> 00:32:01,575
Speaker 3:  Amazon's problem might, but you know, when you're connecting to different

516
00:32:01,585 --> 00:32:05,095
Speaker 3:  ecosystems, you need your smart assistant to be able to troubleshoot these

517
00:32:05,095 --> 00:32:08,855
Speaker 3:  things for you so that you don't have to spend 30 hours

518
00:32:09,175 --> 00:32:12,575
Speaker 3:  a week troubleshooting your smart home to get these things to work. Right.

519
00:32:13,035 --> 00:32:17,015
Speaker 3:  And then the fresh brewed coffee, this is

520
00:32:17,255 --> 00:32:20,975
Speaker 3:  a capability that we should have had years ago. You

521
00:32:21,075 --> 00:32:24,815
Speaker 3:  can rig up a smart plug If, you have the right type of

522
00:32:25,315 --> 00:32:29,135
Speaker 3:  coffee machine that has a physical on off

523
00:32:29,135 --> 00:32:32,735
Speaker 3:  switch, or you have a smart Alexa connected coffee

524
00:32:32,735 --> 00:32:36,535
Speaker 3:  machine that on average costs between one

525
00:32:36,535 --> 00:32:40,525
Speaker 3:  and $2,000. There's one from spin and then there's

526
00:32:40,525 --> 00:32:44,365
Speaker 3:  one recently from Bosch. There. There's

527
00:32:44,365 --> 00:32:48,325
Speaker 3:  just, there's just so much friction. So you, I'm testing the Bosch coffee

528
00:32:48,565 --> 00:32:52,205
Speaker 3:  maker and in theory it should make me a latte when I say

529
00:32:52,515 --> 00:32:56,405
Speaker 3:  good morning, but okay, I spend in preparation

530
00:32:56,505 --> 00:33:00,405
Speaker 3:  for, when I was sort of researching all this, I spent about

531
00:33:00,515 --> 00:33:03,645
Speaker 3:  four hours trying to get her to work. This is,

532
00:33:03,645 --> 00:33:07,605
Speaker 1:  This is the like key Alexa calculus that has

533
00:33:07,605 --> 00:33:11,485
Speaker 1:  never tipped in Alexa's favor, right? Yes. You can. A lot of what you just

534
00:33:11,485 --> 00:33:13,805
Speaker 1:  described, you can in theory right.

535
00:33:14,465 --> 00:33:14,685
Speaker 3:  Set

536
00:33:14,685 --> 00:33:18,565
Speaker 1:  Up, right? Like the, the one of the things Amazon has tried to do to

537
00:33:18,585 --> 00:33:22,045
Speaker 1:  its credit is enable some of this stuff, right? Yes. And like, I get a lot

538
00:33:22,045 --> 00:33:25,805
Speaker 1:  of crap on this show for hating Apple shortcuts because

539
00:33:25,945 --> 00:33:29,845
Speaker 1:  it gives you a lot of power, but it is essentially a, a like really complicated

540
00:33:29,845 --> 00:33:33,525
Speaker 1:  scripting language. Yeah. And so Sure right. If you are willing to do the

541
00:33:33,525 --> 00:33:37,485
Speaker 1:  work to learn all of that. Great. The problem is then

542
00:33:37,665 --> 00:33:41,005
Speaker 1:  you, you've spent a bunch of money, you've done a ton of work, it doesn't

543
00:33:41,005 --> 00:33:44,205
Speaker 1:  work all the time. And then now I'm doing a bunch of maintenance. Yes. And

544
00:33:44,205 --> 00:33:48,125
Speaker 1:  so at each step along the way, you've lost a significant part of what makes

545
00:33:48,125 --> 00:33:51,785
Speaker 1:  this thing compelling in the first place. And now you're just doing,

546
00:33:52,045 --> 00:33:55,705
Speaker 1:  you're, you're like doing a bit trying to get Alexa to work, right? It feels

547
00:33:55,735 --> 00:33:58,385
Speaker 1:  cool, but it's not actually less work.

548
00:33:58,805 --> 00:33:59,665
Speaker 3:  No, it's more work.

549
00:34:00,025 --> 00:34:03,905
Speaker 1:  Right. And so you, you get to a point where the, the math of

550
00:34:03,925 --> 00:34:07,505
Speaker 1:  how much time am I spending to make coffee every morning just

551
00:34:07,535 --> 00:34:11,145
Speaker 1:  doesn't tip in Alexa's favor. And I it's true with so

552
00:34:11,545 --> 00:34:15,345
Speaker 1:  many things. And that, that has always been the thing for me that it's like,

553
00:34:15,345 --> 00:34:18,785
Speaker 1:  yeah, I could make the good morning routine work, but it would only work

554
00:34:18,785 --> 00:34:22,225
Speaker 1:  half the time and I'm gonna go have to fix all the things it doesn't get

555
00:34:22,225 --> 00:34:25,785
Speaker 1:  right every morning. And it's just not, I'm I, I'll just walk into the room

556
00:34:25,785 --> 00:34:27,105
Speaker 1:  and turn the light on. That works.

557
00:34:28,845 --> 00:34:31,025
Speaker 3:  Oh David, it works. You're breaking my heart.

558
00:34:31,925 --> 00:34:34,945
Speaker 1:  No, but it's it, and it kills me too because I want all that stuff to work

559
00:34:35,085 --> 00:34:38,425
Speaker 1:  and like once a year I go through the work of setting all this stuff up again

560
00:34:38,685 --> 00:34:41,505
Speaker 1:  and then the third time in a row it doesn't work. Doesn't, I'm like, ah,

561
00:34:41,505 --> 00:34:43,265
Speaker 1:  nevermind. And I just tear it all out of the wall again

562
00:34:43,565 --> 00:34:47,465
Speaker 3:  Or it works. But there's one thing actually that you didn't, you'd

563
00:34:47,465 --> 00:34:51,385
Speaker 3:  forgot that you don't want it to do that one time like right. Your son's

564
00:34:51,505 --> 00:34:54,145
Speaker 3:  sleeping and they don't have to go to school that day, but it still turns

565
00:34:54,145 --> 00:34:57,185
Speaker 3:  the light on in their room. Yeah. You know, so, and that's, again, that goes

566
00:34:57,185 --> 00:35:00,785
Speaker 3:  back to the context, you know, it, it needs to know more about

567
00:35:01,055 --> 00:35:04,625
Speaker 3:  your, your home and be more intelligent and not just

568
00:35:04,655 --> 00:35:05,665
Speaker 3:  command and control.

569
00:35:06,425 --> 00:35:10,345
Speaker 1:  That is actually a perfect segue into all of where this stuff is headed,

570
00:35:10,905 --> 00:35:14,745
Speaker 1:  which I think we're 10 years in and we're kind of at, at the beginning of

571
00:35:14,745 --> 00:35:18,705
Speaker 1:  maybe a next decade of new things on this front. So we're gonna take a

572
00:35:18,705 --> 00:35:21,025
Speaker 1:  really quick break. Will you stick around and talk to me about all that stuff

573
00:35:21,025 --> 00:35:21,185
Speaker 1:  too?

574
00:35:21,565 --> 00:35:22,985
Speaker 3:  Of course, I will be right here.

575
00:35:22,985 --> 00:35:24,705
Speaker 1:  Alright, good. We'll be right back.

576
00:36:39,505 --> 00:36:43,365
Speaker 1:  All right, we're back. Jen two is still here. Hi Jen. Hi. I just

577
00:36:43,365 --> 00:36:47,205
Speaker 1:  like talking about Alexa. This is fun. This is, this is delightful. I have

578
00:36:47,205 --> 00:36:51,165
Speaker 1:  been on the hill of voice assistance. We'll change everything for a

579
00:36:51,325 --> 00:36:55,165
Speaker 1:  very long time and I have mostly been wrong. So let's talk

580
00:36:55,165 --> 00:36:58,005
Speaker 1:  about the ways the future might prove me and you. Right.

581
00:36:59,465 --> 00:37:01,485
Speaker 3:  One day. Yeah, because I, how long do we get?

582
00:37:02,375 --> 00:37:05,645
Speaker 1:  Let's say 10 more years. Okay. I think the, the first decade, not so much

583
00:37:05,645 --> 00:37:08,965
Speaker 1:  for us. Decade number two, maybe we're

584
00:37:08,965 --> 00:37:09,525
Speaker 3:  Right in there

585
00:37:09,955 --> 00:37:13,805
Speaker 1:  Hard maybe. So let's talk about kind of where we are at

586
00:37:13,835 --> 00:37:17,605
Speaker 1:  this moment, both in terms of what Alexa is and

587
00:37:17,605 --> 00:37:21,085
Speaker 1:  kind of what we know about where it's about to go. And I think

588
00:37:22,345 --> 00:37:25,965
Speaker 1:  you, I think relatively recently got to go see some version of like

589
00:37:26,545 --> 00:37:29,885
Speaker 1:  the, the perfect like platonic ideal of an Alexa

590
00:37:30,385 --> 00:37:33,965
Speaker 1:  system, right? That like Amazon's fever dream of what your house might look

591
00:37:33,965 --> 00:37:34,085
Speaker 1:  like.

592
00:37:34,905 --> 00:37:38,845
Speaker 3:  Yes. I did. I went for a tour of Amazon's

593
00:37:38,845 --> 00:37:41,965
Speaker 3:  smart Home lab, which is in their Seattle headquarters.

594
00:37:42,665 --> 00:37:46,565
Speaker 3:  And I was, I was excited when they suggested

595
00:37:46,565 --> 00:37:49,925
Speaker 3:  the idea and they said, you know, we can show you everything we have set

596
00:37:49,925 --> 00:37:53,445
Speaker 3:  up. And it's sort of like an idealistic, like this is what

597
00:37:54,025 --> 00:37:57,685
Speaker 3:  the sort of ultimate Alexa powered, ambient

598
00:37:57,735 --> 00:38:01,605
Speaker 3:  smart home looks like. You know, everything is connected and set up

599
00:38:01,625 --> 00:38:05,405
Speaker 3:  and you just need to say a few words. I was hoping

600
00:38:05,865 --> 00:38:09,405
Speaker 3:  for a bit more motion sensing action, like ambient action. But

601
00:38:09,615 --> 00:38:13,565
Speaker 3:  there was, you know, this was, this was kind of like their ideal So I was

602
00:38:13,565 --> 00:38:15,765
Speaker 3:  expecting honestly to be kind of blown away.

603
00:38:16,045 --> 00:38:19,725
Speaker 1:  Yeah. I mean if ever it's going to feel like magic. It should be Yes. Literally

604
00:38:20,145 --> 00:38:23,525
Speaker 1:  Amazon with infinite resources and all of the people who made this thing

605
00:38:23,785 --> 00:38:26,525
Speaker 1:  at its disposal Yes. Should be able to make it feel like magic.

606
00:38:26,835 --> 00:38:27,325
Speaker 3:  They should,

607
00:38:29,465 --> 00:38:31,565
Speaker 1:  I'm not, I'm not looking forward to where we're headed here.

608
00:38:33,475 --> 00:38:37,125
Speaker 3:  Yeah, it it, it was, I'm gonna say it, it was very

609
00:38:37,125 --> 00:38:38,445
Speaker 3:  disappointing. Oh

610
00:38:38,465 --> 00:38:40,005
Speaker 1:  No, how so? It

611
00:38:40,005 --> 00:38:43,405
Speaker 3:  Was a very nice apartment, beautiful views.

612
00:38:43,855 --> 00:38:46,125
Speaker 3:  Sunny day too. Seattle. Okay.

613
00:38:47,665 --> 00:38:51,565
Speaker 3:  But it was pedestrian, you

614
00:38:51,565 --> 00:38:54,845
Speaker 3:  know, they had everything they, you know, hundreds of devices that could

615
00:38:54,845 --> 00:38:58,405
Speaker 3:  work with the voice assistant, echo devices

616
00:38:58,535 --> 00:39:02,245
Speaker 3:  everywhere, you know, robot vacuums, ring

617
00:39:02,245 --> 00:39:05,885
Speaker 3:  devices, fire TVs, and then lots of third party things.

618
00:39:06,865 --> 00:39:10,165
Speaker 3:  But they, they had like Lutron, Casser lights had switch bot,

619
00:39:10,615 --> 00:39:14,525
Speaker 3:  robot fingers, tons of fun gadgets. But

620
00:39:15,285 --> 00:39:19,125
Speaker 3:  ultimately it was all still command and control. And you know, as we

621
00:39:19,125 --> 00:39:23,045
Speaker 3:  talked about, and Amazon has said along

622
00:39:23,155 --> 00:39:26,405
Speaker 3:  with, in the same breath of, you know, this, our aspiration is a Star Trek

623
00:39:26,565 --> 00:39:30,085
Speaker 3:  computer. Our aspiration is an ambient smart home,

624
00:39:30,605 --> 00:39:34,485
Speaker 3:  a sort of a home that responds to you proactively

625
00:39:34,715 --> 00:39:38,365
Speaker 3:  without you having to do anything or do something very minimal.

626
00:39:38,555 --> 00:39:42,325
Speaker 3:  Just say a few words or maybe press a button and

627
00:39:42,325 --> 00:39:45,485
Speaker 3:  what you want happens. And they had set up in

628
00:39:46,305 --> 00:39:49,845
Speaker 3:  the house, different routines in each room. So there was like a,

629
00:39:50,685 --> 00:39:54,405
Speaker 3:  a watch TV routine. There was a leaving the home routine

630
00:39:54,705 --> 00:39:58,605
Speaker 3:  and of course a good morning routine, which I was excited to

631
00:39:58,625 --> 00:40:02,605
Speaker 3:  try out it always a good morning, go to bed and got ready to have

632
00:40:02,605 --> 00:40:06,165
Speaker 3:  to start my day. And again, the no motion

633
00:40:06,165 --> 00:40:09,725
Speaker 3:  sensors So I had to, I had to say a command

634
00:40:10,585 --> 00:40:14,485
Speaker 3:  to get things going. And it did the normal stuff, turned the lights on,

635
00:40:14,755 --> 00:40:18,525
Speaker 3:  started the shower, which is something I've always wanted. But again, really

636
00:40:18,525 --> 00:40:22,405
Speaker 3:  expensive. If you've looked into Mowen or cola smart showers,

637
00:40:22,635 --> 00:40:26,485
Speaker 3:  it's not cheap. So, you know, it just didn't sort of have

638
00:40:26,485 --> 00:40:30,365
Speaker 3:  that ambient feel. And, and also it

639
00:40:30,365 --> 00:40:34,285
Speaker 3:  was really slow and really laggy. And again,

640
00:40:34,425 --> 00:40:38,285
Speaker 3:  I'm like, guys, this is, this is your sort of

641
00:40:38,565 --> 00:40:42,365
Speaker 3:  ultimate setup and you're basically just reaffirming my feeling

642
00:40:42,365 --> 00:40:46,135
Speaker 3:  that this is still basically a remote control

643
00:40:46,235 --> 00:40:50,055
Speaker 3:  for your home. Which, which is great, that has its use case, but

644
00:40:50,055 --> 00:40:53,935
Speaker 3:  it is not Star Trek computer and it is not an ambient smart home.

645
00:40:54,895 --> 00:40:58,655
Speaker 1:  I actually, I love that distinction. 'cause I think one other thing you and

646
00:40:58,655 --> 00:41:00,895
Speaker 1:  I have talked about over the years and that I've been forever fascinated

647
00:41:00,895 --> 00:41:04,815
Speaker 1:  by is universal remotes, which Yeah. Which I actually think as a

648
00:41:04,815 --> 00:41:08,295
Speaker 1:  gadget are, are sort of woefully underdeveloped and under

649
00:41:08,775 --> 00:41:11,895
Speaker 1:  considered. And the idea that I can just sit and hold a thing and it becomes

650
00:41:11,895 --> 00:41:15,615
Speaker 1:  like a magic wand for all of my gear strikes me as a

651
00:41:15,725 --> 00:41:18,735
Speaker 1:  much more attainable than some of the other stuff we're working on. And b,

652
00:41:18,995 --> 00:41:22,935
Speaker 1:  really cool and useful, like I, I have, I have six or

653
00:41:22,935 --> 00:41:26,815
Speaker 1:  seven remotes just for the stuff here in my basement. And in

654
00:41:26,815 --> 00:41:30,655
Speaker 1:  terms of like actual human utility, putting all of that into one usable

655
00:41:30,655 --> 00:41:33,925
Speaker 1:  device makes a lot of sense. And So I think to your point, it's not nothing

656
00:41:34,385 --> 00:41:38,045
Speaker 1:  to turn this into something that can go do an individual

657
00:41:38,095 --> 00:41:42,005
Speaker 1:  thing on your behalf. Right. Having like one interface for all of that stuff,

658
00:41:42,035 --> 00:41:45,685
Speaker 1:  even a command at a time is not nothing. It's also nowhere

659
00:41:45,995 --> 00:41:49,405
Speaker 1:  near the like big ambient computing

660
00:41:49,675 --> 00:41:53,605
Speaker 1:  something. It can do everything for you. And a, a word

661
00:41:53,605 --> 00:41:56,845
Speaker 1:  that Amazon has used more and more over time is proactively correct. Right?

662
00:41:56,845 --> 00:42:00,205
Speaker 1:  Like the idea is not just that you should be able to ask it to do it for

663
00:42:00,205 --> 00:42:04,125
Speaker 1:  you and it will, yeah. It should be able to do it for you. Yes. Like just

664
00:42:04,125 --> 00:42:07,365
Speaker 1:  the fact that you have to say good morning in order to tell it that you've

665
00:42:07,365 --> 00:42:10,605
Speaker 1:  woken up and to start the routine is actually a failure of the system.

666
00:42:11,265 --> 00:42:14,845
Speaker 1:  It should just know you've woken up and do the things for you. Right. Like,

667
00:42:14,845 --> 00:42:18,005
Speaker 1:  and again, all that technology is there. Yes. There's these people, like

668
00:42:18,005 --> 00:42:20,805
Speaker 1:  again, it costs 'em billion dollars, but like you can buy the eight sleep

669
00:42:20,805 --> 00:42:24,765
Speaker 1:  thing that tracks your sleep and the, the, the pieces of the puzzle are

670
00:42:24,765 --> 00:42:28,205
Speaker 1:  sitting there and Alexa was supposed to be the thing that put them all together

671
00:42:28,345 --> 00:42:32,245
Speaker 1:  in such a way that you didn't have to and, and right now we're still on the

672
00:42:32,275 --> 00:42:33,925
Speaker 1:  universal remote phase.

673
00:42:34,235 --> 00:42:37,765
Speaker 3:  Well, and to Alexa's credit, it is, if there is one thing that is

674
00:42:37,955 --> 00:42:41,765
Speaker 3:  more difficult to navigate than voice control, it is a universal

675
00:42:41,765 --> 00:42:42,085
Speaker 3:  remote.

676
00:42:43,435 --> 00:42:44,925
Speaker 1:  Fair. Fair.

677
00:42:45,975 --> 00:42:46,325
Speaker 3:  There

678
00:42:46,325 --> 00:42:49,405
Speaker 1:  Are like eight Logitech harmony owners out there screaming and yelling at

679
00:42:49,405 --> 00:42:52,725
Speaker 1:  me. I want you to know I hear you and, and I love you, but it's true.

680
00:42:53,945 --> 00:42:57,645
Speaker 3:  But you know, universal remotes have their place, but there's also a reason

681
00:42:57,645 --> 00:43:01,445
Speaker 3:  why there are very few of them around. Yes. But what

682
00:43:01,945 --> 00:43:05,805
Speaker 3:  the problem with the universal remote and with Alexa as it

683
00:43:05,925 --> 00:43:09,565
Speaker 3:  is today, is that specific nomenclature, the fact that you

684
00:43:09,875 --> 00:43:13,645
Speaker 3:  have to say specific words to get Alexa

685
00:43:13,665 --> 00:43:17,205
Speaker 3:  to do something or press a specific sequence of buttons on your

686
00:43:17,205 --> 00:43:21,085
Speaker 3:  universal remote. Right. But yeah, it's, it's that the ambient part

687
00:43:21,945 --> 00:43:25,805
Speaker 3:  is, is the intelligence and none of neither of

688
00:43:25,805 --> 00:43:29,285
Speaker 3:  those things currently have the intelligence. And you know,

689
00:43:30,095 --> 00:43:34,005
Speaker 3:  there are, that's where, that's the next step

690
00:43:34,025 --> 00:43:37,925
Speaker 3:  in the smart home. Whether it's Amazon that can bring that intelligence or

691
00:43:38,035 --> 00:43:41,565
Speaker 3:  another company. There are many, many trying. Yes, there are.

692
00:43:41,765 --> 00:43:45,285
Speaker 3:  That's what we need. We need the systems that can understand

693
00:43:45,285 --> 00:43:49,085
Speaker 3:  context, that can understand everything that's in our

694
00:43:49,085 --> 00:43:52,605
Speaker 3:  home. I mean, when I spoke with Dave Limp, when they had their last event

695
00:43:53,225 --> 00:43:56,965
Speaker 3:  in October of 2023, he, you know, he said, we, we have

696
00:43:56,965 --> 00:44:00,605
Speaker 3:  feeding all of every smart home manual ever made into Alexa

697
00:44:00,785 --> 00:44:04,605
Speaker 3:  to make it more intelligent so that it knows the devices in your home.

698
00:44:04,905 --> 00:44:08,485
Speaker 3:  That's what it, it needs that knowledge, but then it also needs the

699
00:44:08,485 --> 00:44:12,445
Speaker 3:  intelligence to be able to act on them. For example, I have a

700
00:44:12,575 --> 00:44:16,485
Speaker 3:  smart faucet in my kitchen and if I go

701
00:44:16,505 --> 00:44:20,405
Speaker 3:  up to it to, or I can use Amazon, I can use

702
00:44:20,405 --> 00:44:24,005
Speaker 3:  Alexa to have it do fun things like dispense two

703
00:44:24,155 --> 00:44:28,045
Speaker 3:  cups of water at 90 degrees, you know, perfect for baby

704
00:44:28,045 --> 00:44:31,685
Speaker 3:  bottle. You can set these presets. But I have to,

705
00:44:31,955 --> 00:44:35,685
Speaker 3:  I'll be standing in front of the faucet and I will have to say, Alexa,

706
00:44:36,065 --> 00:44:39,725
Speaker 3:  ask Moen to dispense two cups of water at 90

707
00:44:39,725 --> 00:44:43,045
Speaker 3:  degrees. And sometimes it will say

708
00:44:43,475 --> 00:44:47,405
Speaker 3:  okay and sometimes it will say, I can't do that. Please set

709
00:44:47,405 --> 00:44:50,925
Speaker 3:  up your devices again. Or something along those lines like who's mul?

710
00:44:51,115 --> 00:44:55,085
Speaker 3:  Yeah. And then my, my husband was watching me try and play

711
00:44:55,085 --> 00:44:58,365
Speaker 3:  with this the other day and he said, shouldn't it just know that you're standing

712
00:44:58,425 --> 00:45:02,405
Speaker 3:  in front of the kitchen f it? Yes. And when you say I want two cups of hot

713
00:45:02,405 --> 00:45:06,245
Speaker 3:  water, just do it. And it should and it could.

714
00:45:06,755 --> 00:45:10,725
Speaker 3:  That is a future we could have. There is enough technology just even in echo

715
00:45:10,725 --> 00:45:14,445
Speaker 3:  devices and also in devices like from Apple

716
00:45:14,785 --> 00:45:17,765
Speaker 3:  and Google that you'd have in your home there smarts. Because things like

717
00:45:17,885 --> 00:45:21,525
Speaker 3:  UWB radar, there's the technology is there.

718
00:45:21,925 --> 00:45:25,045
Speaker 1:  I mean, for God's sake, Amazon owns Ring which makes a drone with a camera

719
00:45:25,115 --> 00:45:26,925
Speaker 1:  that will fly around your house. Like

720
00:45:27,075 --> 00:45:27,685
Speaker 3:  Allegedly.

721
00:45:28,395 --> 00:45:32,245
Speaker 1:  Yeah. Fair it allegedly. We'll do that. It's a fair

722
00:45:32,245 --> 00:45:35,885
Speaker 1:  point. But yeah, like all again, and I think we should just

723
00:45:36,105 --> 00:45:39,725
Speaker 1:  say, and then move on from the fact that like all of this

724
00:45:39,845 --> 00:45:43,205
Speaker 1:  requires a deep like privacy

725
00:45:43,535 --> 00:45:47,205
Speaker 1:  trade off. Yes. In a way that should make people uncomfortable. Yes. And

726
00:45:47,205 --> 00:45:50,885
Speaker 1:  is is a trade off that we are going to need to reckon with as people if

727
00:45:50,955 --> 00:45:54,885
Speaker 1:  that technology is ever going to get to what you're describing, we

728
00:45:54,885 --> 00:45:58,845
Speaker 1:  are going to have to make some like very conscious decisions about, I

729
00:45:58,845 --> 00:46:02,445
Speaker 1:  want my speaker to be aware that I'm standing

730
00:46:02,865 --> 00:46:06,645
Speaker 1:  at the sink and we should just leave that conversation for another time.

731
00:46:06,645 --> 00:46:10,205
Speaker 1:  Because I think that is a place where like everyone is going to have to make

732
00:46:10,565 --> 00:46:13,645
Speaker 1:  personal decisions about how they feel about that stuff. But that is what

733
00:46:13,645 --> 00:46:13,765
Speaker 1:  it

734
00:46:13,765 --> 00:46:16,645
Speaker 3:  Requires. I do need to just interject because we've gone in almost an hour

735
00:46:16,665 --> 00:46:20,165
Speaker 3:  and I haven't mentioned the M word, but this is where Matta could help. Oh

736
00:46:20,165 --> 00:46:20,365
Speaker 3:  God.

737
00:46:21,155 --> 00:46:22,725
Speaker 1:  Well that's the everybody,

738
00:46:26,465 --> 00:46:30,445
Speaker 3:  So whilst it's up to the platforms, how they implement and

739
00:46:30,545 --> 00:46:34,365
Speaker 3:  my money would be on Amazon using the cloud. Gosh, not sure why,

740
00:46:34,385 --> 00:46:38,085
Speaker 3:  but anyway, a platform, a smart home platform using

741
00:46:38,085 --> 00:46:42,005
Speaker 3:  Matter can keep all of that information entirely local

742
00:46:43,065 --> 00:46:46,965
Speaker 3:  and use that data locally. So, but we'll put a pin in that.

743
00:46:47,285 --> 00:46:47,405
Speaker 3:  Yeah,

744
00:46:47,655 --> 00:46:50,725
Speaker 1:  We'll, we'll, we'll we'll have many more chances to have that version of

745
00:46:50,725 --> 00:46:53,645
Speaker 1:  the conversation. But to your point, this is

746
00:46:54,505 --> 00:46:57,285
Speaker 1:  the future everyone imagines. Right? I think that the really interesting

747
00:46:57,615 --> 00:47:01,565
Speaker 1:  point we're at right now is anyone thinking about this kind of

748
00:47:01,565 --> 00:47:05,405
Speaker 1:  ambient computing future thinks about it roughly the same way. The goal

749
00:47:05,405 --> 00:47:08,885
Speaker 1:  is the same. And we're at a moment right now where everyone thinks it's the

750
00:47:08,885 --> 00:47:12,725
Speaker 1:  same technology that's gonna get us there. Which again brings us

751
00:47:12,725 --> 00:47:14,365
Speaker 1:  to Dave Limp a year ago

752
00:47:16,065 --> 00:47:19,245
Speaker 1:  who I, I don't know, I guess in retrospect like

753
00:47:20,105 --> 00:47:23,605
Speaker 1:  way pre announced a gigantic reboot of Alexa.

754
00:47:23,905 --> 00:47:25,725
Speaker 1:  Is that is, am I overstating it to put

755
00:47:25,725 --> 00:47:29,605
Speaker 3:  It like that? No, no. I mean they, they, he got on stage and demoed a

756
00:47:29,625 --> 00:47:33,565
Speaker 3:  new, very much more conversational Alexa. That was the kind

757
00:47:33,565 --> 00:47:37,405
Speaker 3:  of the big show off was being able to talk to the voice

758
00:47:37,405 --> 00:47:41,245
Speaker 3:  assistant naturally not having to constantly repeat the

759
00:47:41,245 --> 00:47:45,125
Speaker 3:  wake word and have it, it had more personality. Like it

760
00:47:45,125 --> 00:47:48,885
Speaker 3:  was kind of joking with him. And it it that sort of natural

761
00:47:49,045 --> 00:47:53,005
Speaker 3:  conversation, which was again the sort of the North star, the Star Trek

762
00:47:53,245 --> 00:47:56,605
Speaker 3:  computer being able to speak naturally and have a conversation

763
00:47:57,065 --> 00:48:01,005
Speaker 3:  is something they've been working towards. And this demo did show

764
00:48:01,195 --> 00:48:04,085
Speaker 3:  that it was almost a reality.

765
00:48:05,315 --> 00:48:08,965
Speaker 3:  They also at this, at that event talked about how

766
00:48:09,415 --> 00:48:12,525
Speaker 3:  Alexa was being able to do more of what we've been talking about here and

767
00:48:12,525 --> 00:48:15,525
Speaker 3:  sort of do more intelligent

768
00:48:16,405 --> 00:48:20,205
Speaker 3:  responses. One of the things is sort of multi-step routine. So

769
00:48:20,425 --> 00:48:24,405
Speaker 3:  you could create something just by telling Alexa

770
00:48:24,605 --> 00:48:27,925
Speaker 3:  a list of things you want it to accomplish, you know, more like, like to

771
00:48:27,925 --> 00:48:31,365
Speaker 3:  a computer and have it just

772
00:48:32,085 --> 00:48:35,205
Speaker 3:  reel those off without you having to go into an app and set up a routine

773
00:48:36,025 --> 00:48:39,725
Speaker 3:  and then set up a trigger so you don't have to spend your time programming

774
00:48:39,725 --> 00:48:42,045
Speaker 3:  your smart home. You just think, oh actually I want this to happen right

775
00:48:42,045 --> 00:48:46,005
Speaker 3:  now. Alexa do it for me and it will do it. And then,

776
00:48:46,105 --> 00:48:49,485
Speaker 3:  you know, this, this is a sort of their new vision,

777
00:48:51,385 --> 00:48:55,245
Speaker 3:  how it's going to get there. I mean ultimately what I would like is

778
00:48:55,245 --> 00:48:58,525
Speaker 3:  something like that un I would like to be able to have Alexa

779
00:48:58,985 --> 00:49:02,365
Speaker 3:  follow through on the commands that I have, but also understand

780
00:49:02,945 --> 00:49:06,925
Speaker 3:  the context of them. So for example, an idea I sort of came

781
00:49:06,925 --> 00:49:10,815
Speaker 3:  up with what I would love to be able to say something like, Alexa tell

782
00:49:10,815 --> 00:49:13,815
Speaker 3:  my son not to forget his science project. You know, set the alarm when he

783
00:49:13,815 --> 00:49:17,415
Speaker 3:  leaves in the morning, then unlock the back door at 4:00 PM for the plumber,

784
00:49:17,885 --> 00:49:21,775
Speaker 3:  lock it again at five and then preheat the oven at six

785
00:49:21,775 --> 00:49:25,455
Speaker 3:  o'clock. But if I'm running late, adjust the time, you know, so you don't

786
00:49:25,455 --> 00:49:29,335
Speaker 3:  burn the house down and, and it would need, but Amazon's

787
00:49:29,335 --> 00:49:32,415
Speaker 3:  gonna need a lot more context to be able to do this type of thing. And this

788
00:49:32,415 --> 00:49:36,255
Speaker 3:  is the type of thing that we're already seeing hints of in

789
00:49:36,315 --> 00:49:39,815
Speaker 3:  the other ecosystems. Like that's some of what Siri

790
00:49:40,235 --> 00:49:44,135
Speaker 3:  may be able to do in terms of the intelligence Siri, not the smart

791
00:49:44,135 --> 00:49:47,855
Speaker 3:  home control but understanding like that I'm running late. Oh now there goes

792
00:49:48,005 --> 00:49:48,295
Speaker 3:  Siri,

793
00:49:50,045 --> 00:49:50,655
Speaker 1:  This is great.

794
00:49:51,465 --> 00:49:52,855
Speaker 3:  She's on it. Don't worry.

795
00:49:53,555 --> 00:49:57,015
Speaker 1:  I'm glad I look forward to what she comes back with. Netting to the internet

796
00:49:57,625 --> 00:50:01,615
Speaker 1:  there it's info check the whole app sir. Everybody alright

797
00:50:01,615 --> 00:50:02,055
Speaker 1:  keep going.

798
00:50:02,905 --> 00:50:04,975
Speaker 3:  Sorry, I forgot about that one.

799
00:50:05,085 --> 00:50:08,535
Speaker 1:  This is great. We're leaving all of this in. This is the most, this is a

800
00:50:08,535 --> 00:50:10,215
Speaker 1:  real show not tell thing we're doing here. I love it.

801
00:50:10,215 --> 00:50:13,775
Speaker 3:  I can go with Google next. Hey Google, you pipe up.

802
00:50:14,435 --> 00:50:17,295
Speaker 3:  No he it doesn't. Listen there's one right here.

803
00:50:18,885 --> 00:50:20,495
Speaker 3:  Nothing. Nothing.

804
00:50:21,325 --> 00:50:25,295
Speaker 1:  Yeah. Yeah. This is why the Alexa team is not afraid of Google and

805
00:50:25,295 --> 00:50:25,495
Speaker 1:  Apple.

806
00:50:28,395 --> 00:50:32,325
Speaker 3:  Okay, sorry. But yeah, and I that context,

807
00:50:32,555 --> 00:50:36,445
Speaker 3:  context aware voice assistant is the kind of, you know, the holy

808
00:50:36,605 --> 00:50:40,365
Speaker 3:  grail for me that is the computer, that is the ambient technology

809
00:50:40,425 --> 00:50:43,165
Speaker 3:  and you know, you could, you know, speaking of like the type of technology

810
00:50:43,165 --> 00:50:46,645
Speaker 3:  that Apple said it's bringing to Siri with a contextually aware Siri,

811
00:50:47,065 --> 00:50:51,005
Speaker 3:  you could imagine, and this may be something a little bit

812
00:50:51,005 --> 00:50:53,045
Speaker 3:  out there and might scare most people.

813
00:50:53,645 --> 00:50:56,445
Speaker 5:  I don't have an answer for that. Is there something else I can help with?

814
00:50:58,765 --> 00:51:02,095
Speaker 3:  Is that, is something like this. Okay, so it's, so night.

815
00:51:02,755 --> 00:51:06,575
Speaker 3:  I'm working in my home office. My mom is bringing the kids

816
00:51:06,575 --> 00:51:10,415
Speaker 3:  home and she's texted me that she's bringing a frozen pizza. So

817
00:51:10,835 --> 00:51:14,295
Speaker 3:  my smart assistant sees the text message, knows that it's my mom and not

818
00:51:14,395 --> 00:51:15,575
Speaker 3:  me 'cause I'm also mom

819
00:51:17,195 --> 00:51:21,095
Speaker 3:  and starts preheating the oven, you know, so that

820
00:51:21,155 --> 00:51:25,055
Speaker 3:  the frozen pizza can go in as soon as they get home, unlocks the

821
00:51:25,055 --> 00:51:28,495
Speaker 3:  door at six 30, which is when they said they were gonna arrive

822
00:51:28,905 --> 00:51:32,495
Speaker 3:  turns on the air purifier because it knows the oven's on. And then after

823
00:51:32,495 --> 00:51:36,135
Speaker 3:  we've had dinner, the robot vacuum will just automatically start

824
00:51:36,575 --> 00:51:40,375
Speaker 3:  a quiet, clean and mop of the kitchen. And then if someone

825
00:51:40,475 --> 00:51:44,455
Speaker 3:  has thought to shut the dishwasher, it will start running automatically.

826
00:51:44,715 --> 00:51:48,375
Speaker 3:  And then I sit down on the couch and without having to say

827
00:51:48,655 --> 00:51:52,535
Speaker 3:  anything or do anything because UWB sensors in my home

828
00:51:52,675 --> 00:51:56,575
Speaker 3:  pod minis or my Alexa soundbar know that we're sitting on the

829
00:51:56,575 --> 00:52:00,095
Speaker 3:  couch. You know, the lights dimmed down, the TV turns on

830
00:52:00,555 --> 00:52:04,495
Speaker 3:  and it knows the kids are there. So it starts playing something

831
00:52:04,515 --> 00:52:08,215
Speaker 3:  family friendly that it knows we like watching, like say last night's recording

832
00:52:08,215 --> 00:52:12,095
Speaker 3:  of the voice. Yes, I do like the voice and we know, and then at eight 30

833
00:52:12,395 --> 00:52:16,375
Speaker 3:  it turns off because it knows it's bedtime. Maybe my

834
00:52:17,165 --> 00:52:20,855
Speaker 3:  Moen smart faucet, if they had one for the bath, would start running a bath

835
00:52:20,915 --> 00:52:24,855
Speaker 3:  for me at the right temperature, turn the lights on upstairs, play

836
00:52:24,855 --> 00:52:28,295
Speaker 3:  some soothing music. And I haven't had to do anything

837
00:52:29,315 --> 00:52:33,215
Speaker 3:  my assistant and my home have has pulled

838
00:52:33,215 --> 00:52:37,095
Speaker 3:  all that together for me. I mean, is that the dream or

839
00:52:37,095 --> 00:52:39,015
Speaker 3:  does that slightly terrify you? I,

840
00:52:39,495 --> 00:52:43,175
Speaker 1:  I mean both to be honest. And I think, I think that's mostly

841
00:52:43,435 --> 00:52:47,255
Speaker 1:  the dream. I think there are bits and pieces of that that I think

842
00:52:47,395 --> 00:52:51,255
Speaker 1:  the, the sort of perfectly proactive assistant is actually the wrong

843
00:52:51,275 --> 00:52:55,095
Speaker 1:  answer. Like, one thing I've learned over time talking to people who work

844
00:52:55,295 --> 00:52:59,135
Speaker 1:  at streaming companies is that the idea of you just sit down and

845
00:52:59,135 --> 00:53:03,095
Speaker 1:  it plays the perfect thing for you is actually not correct. It's, it's the

846
00:53:03,095 --> 00:53:06,295
Speaker 1:  like the, the Amazon thing of like, what if they could just ship you the

847
00:53:06,295 --> 00:53:08,935
Speaker 1:  stuff that you want? Like people like to shop right? People like to browse.

848
00:53:08,955 --> 00:53:11,895
Speaker 1:  And even the thing that is technically the perfect thing for you to watch

849
00:53:11,895 --> 00:53:12,095
Speaker 1:  David.

850
00:53:12,095 --> 00:53:14,495
Speaker 3:  David, you don't have a 13-year-old and a 16-year-old.

851
00:53:15,635 --> 00:53:18,775
Speaker 1:  Listen, I didn't say it's a good thing that people like to do this, but people

852
00:53:18,795 --> 00:53:22,655
Speaker 1:  do like to do this. And, and, and the idea of like, If,

853
00:53:22,655 --> 00:53:25,805
Speaker 1:  you sit down even if what you want to watch is the voice, if it just puts

854
00:53:25,805 --> 00:53:29,045
Speaker 1:  the voice on for you, it feels weird. And So, I think there are, there are

855
00:53:29,045 --> 00:53:32,165
Speaker 1:  things about like if my house just starts drawing me a bath at bath time,

856
00:53:32,355 --> 00:53:35,565
Speaker 1:  there's something odd about that. But if it, if it can chime in and be like,

857
00:53:35,565 --> 00:53:35,725
Speaker 1:  Hey,

858
00:53:35,935 --> 00:53:36,845
Speaker 3:  Would you like me to?

859
00:53:37,055 --> 00:53:39,925
Speaker 1:  Right. That, that starts to feel really valuable. Right. So, I think there's

860
00:53:39,925 --> 00:53:43,685
Speaker 1:  a really interesting interface question there. Yeah. Where

861
00:53:43,915 --> 00:53:46,485
Speaker 1:  it's like Friday night and it's like, oh, is it movie night tonight? And

862
00:53:46,485 --> 00:53:49,565
Speaker 1:  you're like, yes it is. And then it does the whole thing. Whereas instead

863
00:53:49,565 --> 00:53:53,125
Speaker 1:  you have to come, it's proactive. Right. And, and, and when you have a personal

864
00:53:53,125 --> 00:53:55,445
Speaker 1:  assistant, you talk to them. Right. It's not like, I know I

865
00:53:55,445 --> 00:53:56,245
Speaker 3:  Don't want to talk,

866
00:53:57,345 --> 00:54:00,925
Speaker 1:  But like If, you had a human personal assistant, you would talk to them and

867
00:54:00,925 --> 00:54:03,845
Speaker 1:  they would ask you questions and that's, that's a fine set of interactions.

868
00:54:03,955 --> 00:54:07,805
Speaker 1:  Yeah. You wouldn't have to say their name and then a series of words in the

869
00:54:07,805 --> 00:54:10,725
Speaker 1:  correct order. Right. So, I think there's, there's a bunch of really interesting

870
00:54:10,795 --> 00:54:14,725
Speaker 1:  interface questions there, but with the, with

871
00:54:14,725 --> 00:54:17,685
Speaker 1:  the LLM stuff that everybody is betting on right now. Yeah.

872
00:54:19,355 --> 00:54:23,175
Speaker 1:  The thing I keep thinking about is that what it will definitely do, I feel

873
00:54:23,175 --> 00:54:26,295
Speaker 1:  very good about the fact that large language models are going to make

874
00:54:26,725 --> 00:54:30,615
Speaker 1:  understanding your queries better.

875
00:54:30,625 --> 00:54:34,335
Speaker 1:  Right. Like we, we have ample evidence now that for things like speech to

876
00:54:34,335 --> 00:54:37,695
Speaker 1:  text and for natural language processing and text to speech LLMs are very

877
00:54:37,695 --> 00:54:40,855
Speaker 1:  good. They're better than the systems we had before So I. Think the idea

878
00:54:40,855 --> 00:54:44,575
Speaker 1:  that I'm going to be able to say that paragraph of things

879
00:54:44,575 --> 00:54:47,815
Speaker 1:  that you just said to Alexa and it will understand them.

880
00:54:48,735 --> 00:54:52,555
Speaker 1:  I feel very good about that. Yeah. It but as we've seen for 10 years,

881
00:54:52,575 --> 00:54:56,555
Speaker 1:  that's only part of the process. Yeah. And LLMs don't solve any of the other

882
00:54:56,555 --> 00:55:00,395
Speaker 1:  parts of the process. Yeah. And I think it, it, like, I think your,

883
00:55:00,395 --> 00:55:04,355
Speaker 1:  your point about what Google and Apple can do here is, is really important

884
00:55:04,355 --> 00:55:06,875
Speaker 1:  because A, they both have access to your phone, which gives them location

885
00:55:06,875 --> 00:55:09,955
Speaker 1:  data, it gives them lots of information about your contacts, which is really

886
00:55:09,955 --> 00:55:13,275
Speaker 1:  important. All this kind of stuff. I also think Google has a big advantage

887
00:55:13,275 --> 00:55:16,595
Speaker 1:  because it's connected to things like your calendar and your email

888
00:55:17,305 --> 00:55:21,115
Speaker 1:  more intimately in a lot of ways. And also has like your web search, which

889
00:55:21,115 --> 00:55:24,075
Speaker 1:  is very powerful. Apple gets a lot of that because if it's Google deals,

890
00:55:24,105 --> 00:55:27,635
Speaker 1:  like there's some weird antitrust things going on here, but Amazon doesn't

891
00:55:27,635 --> 00:55:30,835
Speaker 1:  have any of that and really doesn't have a path to getting any of that in

892
00:55:30,835 --> 00:55:30,915
Speaker 1:  any

893
00:55:30,915 --> 00:55:33,875
Speaker 3:  Way that can tell unless feed it to it. Which is an extra step on yours.

894
00:55:33,895 --> 00:55:35,155
Speaker 3:  On your part, right?

895
00:55:35,345 --> 00:55:39,235
Speaker 1:  Sure. And I think like do I want to give Alexa

896
00:55:39,495 --> 00:55:43,355
Speaker 1:  access to my Gmail is again, personal question choice. There are

897
00:55:43,475 --> 00:55:46,315
Speaker 1:  probably upsides to doing that over time. Like

898
00:55:46,315 --> 00:55:48,675
Speaker 3:  Kinda like trip it forward my emails. Right?

899
00:55:48,675 --> 00:55:49,075
Speaker 1:  Exactly.

900
00:55:49,545 --> 00:55:50,035
Speaker 3:  Exactly.

901
00:55:50,175 --> 00:55:54,075
Speaker 1:  But I think like I, I have found myself using Gemini a lot to look

902
00:55:54,095 --> 00:55:57,875
Speaker 1:  for information in my Google stuff and that's really

903
00:55:58,155 --> 00:56:01,195
Speaker 1:  powerful and Amazon just has none of that. And so Alexa has none of that.

904
00:56:01,335 --> 00:56:05,315
Speaker 1:  And I think if I were to like Galaxy Brain, all of the

905
00:56:05,315 --> 00:56:08,835
Speaker 1:  crazy gadget launches Amazon has had over the years, they're so

906
00:56:08,835 --> 00:56:12,555
Speaker 1:  desperate to put Alexa in front of you at all times that you will just

907
00:56:12,835 --> 00:56:16,115
Speaker 1:  accidentally end up giving it all of that information. Seriously. Like and

908
00:56:16,115 --> 00:56:19,915
Speaker 1:  I think that's not a crazy strategy, right? To say if you're wearing

909
00:56:20,345 --> 00:56:23,760
Speaker 1:  glasses that have Alexa baked in, you're gonna talk to to Alexa more and

910
00:56:23,760 --> 00:56:26,485
Speaker 1:  thus you're gonna tell it more information. You're gonna have it remind you

911
00:56:26,485 --> 00:56:30,285
Speaker 1:  of more stuff. It might gain more information just by listening.

912
00:56:30,285 --> 00:56:33,925
Speaker 1:  Like there are a lot of things they can do with that kind of ubiquity,

913
00:56:34,305 --> 00:56:37,285
Speaker 1:  but A, it doesn't have that B, it's not going to anytime soon. And C, even

914
00:56:37,285 --> 00:56:41,205
Speaker 1:  that I don't think is enough So, I get to this point where I'm like maybe

915
00:56:41,855 --> 00:56:45,005
Speaker 1:  Alexa is about to get a lot better at the thing it was already good at and

916
00:56:45,225 --> 00:56:48,525
Speaker 1:  not that much better at anything else. But maybe that's cynical.

917
00:56:49,245 --> 00:56:52,925
Speaker 3:  Well it definitely feels like there is a big reset coming. I mean we got

918
00:56:52,945 --> 00:56:56,885
Speaker 3:  the tease of the new Alexa or the remarkable Alexa as it's been

919
00:56:57,285 --> 00:57:01,125
Speaker 3:  reportedly referred to in there was a big piece

920
00:57:01,185 --> 00:57:04,765
Speaker 3:  by the Wall Street Journal sort of talking about all the problems

921
00:57:05,145 --> 00:57:09,085
Speaker 3:  Amazon has had. And that was the rumor is that it's remarkable.

922
00:57:09,445 --> 00:57:13,405
Speaker 3:  I think. I'm not sure about remarkable, I'm not a big fan of that, but anyway,

923
00:57:13,675 --> 00:57:16,965
Speaker 1:  Also kind of a sick burn of 10 years of Alexa. Yeah. That's like we had

924
00:57:17,015 --> 00:57:19,685
Speaker 1:  pedestrian and Alexa now we have Remarkable. Yeah,

925
00:57:19,915 --> 00:57:22,885
Speaker 3:  It's tough. But you know, they have a new hardware Chief

926
00:57:23,775 --> 00:57:25,845
Speaker 3:  Panos pane who friend

927
00:57:25,845 --> 00:57:26,365
Speaker 1:  Of The Vergecast

928
00:57:26,465 --> 00:57:30,245
Speaker 3:  Friend of The Verge cast who did not want to talk about New Alexa the last

929
00:57:30,245 --> 00:57:34,205
Speaker 3:  time he did not had him on. No, he did not. Not at all. But he did

930
00:57:34,275 --> 00:57:37,925
Speaker 3:  hint quite heavily that it's, it's coming.

931
00:57:38,225 --> 00:57:41,885
Speaker 3:  And this year I picked up on in that conversation

932
00:57:42,225 --> 00:57:45,845
Speaker 3:  and I think there is a big reset and we

933
00:57:45,845 --> 00:57:49,205
Speaker 3:  haven't had a fall event this year. It's the first time in a while. So we

934
00:57:49,205 --> 00:57:53,085
Speaker 3:  haven't had dozens of new devices. Mo we've instead there's been dribs

935
00:57:53,085 --> 00:57:56,845
Speaker 3:  and drabs of news coming out over the last few weeks that you

936
00:57:56,845 --> 00:58:00,525
Speaker 3:  would've totally been part of a big event. Like there's a new outdoor arrow

937
00:58:00,995 --> 00:58:04,325
Speaker 3:  ring has finally got 24 7 recording new Kindle,

938
00:58:05,035 --> 00:58:09,005
Speaker 3:  lots of Kindles, so many Kindles. There's also more devices going

939
00:58:09,005 --> 00:58:11,685
Speaker 3:  away. Oh, interesting. We know the echo dot with clock

940
00:58:12,445 --> 00:58:16,325
Speaker 3:  RIP that went a few months ago. Rumor has it, the Echo show

941
00:58:16,325 --> 00:58:19,725
Speaker 3:  15 is on the outs. It's been outta stock for a long time.

942
00:58:21,035 --> 00:58:24,845
Speaker 3:  More services being killed Alexa together, which was their

943
00:58:24,915 --> 00:58:28,245
Speaker 3:  home care service, which I really, I thought was a, a great service.

944
00:58:28,705 --> 00:58:32,325
Speaker 3:  But you know, we are seeing a big reset. We've seen Andy chassis has been

945
00:58:32,525 --> 00:58:36,285
Speaker 3:  reported that he's sort of culling the devices and services division,

946
00:58:36,465 --> 00:58:40,445
Speaker 3:  but I, I'm hoping that this is all in a sort of streamlined

947
00:58:40,445 --> 00:58:44,045
Speaker 3:  effort for what I said at the beginning of the conversation is that they,

948
00:58:44,115 --> 00:58:47,925
Speaker 3:  they had too much emphasis on gadgets and gizmos

949
00:58:47,925 --> 00:58:51,885
Speaker 3:  and not enough on the core value proposition

950
00:58:51,945 --> 00:58:55,445
Speaker 3:  of what the assistant can offer and trimming the craft

951
00:58:55,745 --> 00:58:59,605
Speaker 3:  and hopefully bringing everything together in

952
00:58:59,645 --> 00:59:03,005
Speaker 3:  a much more focused, usable way

953
00:59:03,355 --> 00:59:06,565
Speaker 3:  with a, you know, better ui, better voice interface

954
00:59:08,155 --> 00:59:12,005
Speaker 3:  powered by LLMs. I think, I know they have their own

955
00:59:12,055 --> 00:59:15,765
Speaker 3:  Alexa MLLM, but there's been reports that they're also gonna be using,

956
00:59:16,125 --> 00:59:20,085
Speaker 3:  I think, is it Claude? So there's, there's lots going on.

957
00:59:20,475 --> 00:59:23,205
Speaker 3:  It's just a question of are they gonna get it right,

958
00:59:24,345 --> 00:59:28,245
Speaker 3:  is it going to be a new hardware shift? Which would

959
00:59:28,245 --> 00:59:31,885
Speaker 3:  be a tough sell because there's a lot of, a lot of

960
00:59:31,885 --> 00:59:35,525
Speaker 3:  echoes out there. Yeah. But also I kind of feel like we need it, like the

961
00:59:35,555 --> 00:59:38,565
Speaker 3:  Echo has sort of become, you know, a throwaway gadget.

962
00:59:39,905 --> 00:59:43,525
Speaker 3:  You know, I saw so many reports in my research of people like, well I have

963
00:59:43,525 --> 00:59:47,045
Speaker 3:  three or four echo dots in cupboards and

964
00:59:47,045 --> 00:59:50,965
Speaker 3:  drawers and just don't use them because they're commoditized cheap and

965
00:59:50,965 --> 00:59:54,805
Speaker 3:  you can buy them for $18 and there's no real expectation of anything

966
00:59:54,935 --> 00:59:58,805
Speaker 3:  great coming out of a cheap piece of hardware on

967
00:59:58,805 --> 01:00:02,765
Speaker 3:  your table. Yeah. You know, I, there's it, it's,

968
01:00:02,795 --> 01:00:06,605
Speaker 3:  it's definitely a moment where I think they have

969
01:00:06,745 --> 01:00:08,765
Speaker 3:  the opportunity to do something pretty

970
01:00:10,475 --> 01:00:13,995
Speaker 3:  exciting just like they did 10 years ago when they launched Alexa

971
01:00:15,175 --> 01:00:19,075
Speaker 3:  on November 6th, 2014. But are we gonna

972
01:00:19,075 --> 01:00:19,315
Speaker 3:  get it?

973
01:00:19,735 --> 01:00:23,035
Speaker 1:  Do you think Amazon is right to still call it Alexa? I think

974
01:00:23,465 --> 01:00:27,395
Speaker 1:  what question we've been asking a lot is are the, are the

975
01:00:27,415 --> 01:00:30,235
Speaker 1:  old ways of thinking about these voice assistants the right way to think

976
01:00:30,235 --> 01:00:33,835
Speaker 1:  about the next wave of them? And Google is basically killing Google

977
01:00:33,835 --> 01:00:37,675
Speaker 1:  assistant in favor of Gemini. Apple still seems to be using

978
01:00:37,865 --> 01:00:41,355
Speaker 1:  Siri, but Apple Intelligence is very much the sort of bigger

979
01:00:41,715 --> 01:00:45,555
Speaker 1:  umbrella brand than Siri is at this point. Amazon appears

980
01:00:45,575 --> 01:00:49,315
Speaker 1:  to still be all the way bought into Alexa as the thing

981
01:00:49,455 --> 01:00:53,435
Speaker 1:  and is very much going to sort of bake new technology

982
01:00:53,505 --> 01:00:56,675
Speaker 1:  into all of the existing stuff rather than just kind of wiping the slate

983
01:00:56,675 --> 01:00:58,635
Speaker 1:  and starting over. Do you think that's the right call?

984
01:00:58,905 --> 01:01:02,795
Speaker 3:  Yeah, so from talking to all three people at all three of

985
01:01:03,195 --> 01:01:07,075
Speaker 3:  companies about this, it sounds like the technology, there's a, there's

986
01:01:07,075 --> 01:01:11,035
Speaker 3:  a real disconnect between what the current voice assistants can do and what

987
01:01:11,255 --> 01:01:14,875
Speaker 3:  the LLMs can do. And the LLMs cannot

988
01:01:15,215 --> 01:01:19,195
Speaker 3:  do a lot of what the current voice assistants can do. Right. Like I

989
01:01:19,195 --> 01:01:23,115
Speaker 3:  had heard rumors from one company that they basically were having to come,

990
01:01:23,295 --> 01:01:26,755
Speaker 3:  you know, rebuild everything they had done for their previous voice assistant

991
01:01:26,775 --> 01:01:30,155
Speaker 3:  for their new LLM. And it's like, why are we doing this, this, you know,

992
01:01:30,835 --> 01:01:34,675
Speaker 3:  reinventing the wheel. So it feels to me like a merge, a

993
01:01:34,675 --> 01:01:38,635
Speaker 3:  meshing or emerging of the two makes the most sense. But again, not an

994
01:01:38,795 --> 01:01:42,195
Speaker 3:  engineer. This may be a technical impossibility. There may be too much technical

995
01:01:42,195 --> 01:01:45,915
Speaker 3:  baggage buried underneath Alexa and Siri and Google's

996
01:01:45,915 --> 01:01:49,515
Speaker 3:  assistant that just will not mesh with the future pathway.

997
01:01:49,975 --> 01:01:53,595
Speaker 3:  But I think we need to keep the names, we do not need new names.

998
01:01:54,035 --> 01:01:57,755
Speaker 3:  That's fair. That, that's just, that seems like a non-starter to me and

999
01:01:57,755 --> 01:02:01,275
Speaker 3:  especially for Amazon. I mean they spent a decade building a very

1000
01:02:01,635 --> 01:02:05,315
Speaker 3:  recognizable brand that they constantly tell me people love, like people

1001
01:02:05,385 --> 01:02:07,075
Speaker 3:  love their Alexa. I think that's true.

1002
01:02:07,315 --> 01:02:10,915
Speaker 1:  I think people think Siri sucks and Alexa is great.

1003
01:02:11,095 --> 01:02:12,395
Speaker 1:  Yes, yes. Limited but great

1004
01:02:12,495 --> 01:02:13,395
Speaker 3:  And Google's smarter.

1005
01:02:14,305 --> 01:02:18,035
Speaker 1:  Yeah, well Google assistant is kind of, it's like the, the nerdy weird kid

1006
01:02:18,035 --> 01:02:20,965
Speaker 1:  in the corner, right? That it's like, it's very cool, but no one really talks

1007
01:02:20,965 --> 01:02:24,845
Speaker 1:  about it. But it's, it does some impressive stuff. But

1008
01:02:24,845 --> 01:02:27,485
Speaker 1:  I do think it's true that I think, I think you could have made the case that

1009
01:02:27,485 --> 01:02:31,165
Speaker 1:  walking away from Siri would've been a smart move for Apple. Yes. Just given

1010
01:02:31,225 --> 01:02:34,445
Speaker 1:  the baggage of Siri. I don't think Alexa has that baggage. I think no, people

1011
01:02:36,265 --> 01:02:40,085
Speaker 1:  mostly have settled into this thing is for music and timers, but like you

1012
01:02:40,085 --> 01:02:43,885
Speaker 1:  said, it costs $18, so who cares? And now if Amazon

1013
01:02:43,885 --> 01:02:47,765
Speaker 1:  can say, oh, this thing that you bought now has incredible new capabilities,

1014
01:02:48,505 --> 01:02:52,125
Speaker 1:  that's really powerful. Yeah. I I I, it remains to be seen

1015
01:02:52,125 --> 01:02:55,845
Speaker 1:  whether that is the case. And I think with a lot of LLMs, we continue to

1016
01:02:55,845 --> 01:02:59,565
Speaker 1:  be sold a bill of goods that is not based in reality

1017
01:02:59,625 --> 01:03:03,525
Speaker 1:  in a lot of ways. Yeah. But also I tend to think you're right.

1018
01:03:03,645 --> 01:03:07,085
Speaker 1:  I I I could imagine a world in which Amazon is like, here's a whole new brand,

1019
01:03:07,085 --> 01:03:10,165
Speaker 1:  we're changing everything. But also like these things are everywhere.

1020
01:03:11,235 --> 01:03:15,205
Speaker 1:  They're pretty simple hardware and it's all in the cloud anyway.

1021
01:03:15,305 --> 01:03:19,285
Speaker 1:  So like if they can just suddenly upgrade everyone's speakers in some massive

1022
01:03:19,345 --> 01:03:21,285
Speaker 1:  way, that becomes, that becomes pretty powerful.

1023
01:03:21,675 --> 01:03:25,565
Speaker 3:  Well my original Echo, which is in my husband's garage according

1024
01:03:25,565 --> 01:03:29,445
Speaker 3:  to Dave Limp at the event last year will be updated to support the new Alexa.

1025
01:03:29,625 --> 01:03:30,045
Speaker 3:  It might have

1026
01:03:30,045 --> 01:03:31,405
Speaker 1:  Another 10 years in it, it might,

1027
01:03:31,505 --> 01:03:35,045
Speaker 3:  Who knows. Yeah. I mean that's pretty impressive. And that's, that's a whole

1028
01:03:35,045 --> 01:03:38,865
Speaker 3:  infrastructure that I don't see Amazon getting rid of and

1029
01:03:38,865 --> 01:03:42,145
Speaker 3:  yeah, I think that is, that's the, that's the right move.

1030
01:03:42,775 --> 01:03:46,745
Speaker 3:  Make Alexa smarter and it the capabilities and the

1031
01:03:46,745 --> 01:03:50,305
Speaker 3:  possibilities just get, get really interesting. I'm excited to see what they

1032
01:03:50,305 --> 01:03:52,545
Speaker 3:  can do. I just want them to hurry up and do it. I

1033
01:03:52,545 --> 01:03:56,105
Speaker 1:  Know you and me and everybody at this point, but here's what we're gonna

1034
01:03:56,105 --> 01:03:58,785
Speaker 1:  do. You're gonna have to come back when they launch it, which as far as we

1035
01:03:58,785 --> 01:04:00,305
Speaker 1:  know might be in like the next couple of weeks, like next

1036
01:04:00,305 --> 01:04:00,865
Speaker 3:  Week. Yeah.

1037
01:04:01,485 --> 01:04:05,185
Speaker 1:  But we're both going to review whatever the new Alexa is on the very first

1038
01:04:05,295 --> 01:04:08,465
Speaker 1:  echo. I'm gonna find mine, it's somewhere and we're gonna talk about it.

1039
01:04:08,465 --> 01:04:11,425
Speaker 1:  We're gonna have 10-year-old hardware with brand new software and we're gonna

1040
01:04:11,425 --> 01:04:13,145
Speaker 1:  see if it, if magic has really happened

1041
01:04:13,145 --> 01:04:15,265
Speaker 3:  In that. Sounds like a good plan. Awesome.

1042
01:04:15,265 --> 01:04:18,545
Speaker 1:  Alright Jen, thank you as always. Again, I suspect we will see you again

1043
01:04:18,545 --> 01:04:18,985
Speaker 1:  very soon.

1044
01:04:19,725 --> 01:04:21,465
Speaker 3:  Sounds good. Thanks. See

1045
01:04:21,465 --> 01:04:21,585
Speaker 1:  Ya.

1046
01:04:22,005 --> 01:04:22,225
Speaker 3:  Bye.

1047
01:04:23,215 --> 01:04:26,545
Speaker 1:  Alright, we gotta take one more break and then we're gonna come back and

1048
01:04:26,545 --> 01:04:29,185
Speaker 1:  take a question from the Vergecast hotline. We'll be right back.

1049
01:05:43,915 --> 01:05:46,695
Speaker 1:  All right, we're back. Let's get to the hotline as always. The number is

1050
01:05:46,695 --> 01:05:50,575
Speaker 1:  8 6 6 VERGE one one. The email is vergecast at The Verge dot

1051
01:05:50,575 --> 01:05:54,295
Speaker 1:  com. We love all of your questions. Please keep sending them,

1052
01:05:54,485 --> 01:05:58,335
Speaker 1:  keep calling in. Hearing from you is the absolute best. We got a

1053
01:05:58,385 --> 01:06:02,255
Speaker 1:  bunch more episodes to do this year, cannot wait for your

1054
01:06:02,255 --> 01:06:05,495
Speaker 1:  hotline questions. Love everything that everybody's been sending. Thank you

1055
01:06:05,495 --> 01:06:09,215
Speaker 1:  as always. This week we have both a question

1056
01:06:09,355 --> 01:06:12,935
Speaker 1:  and a product idea about AirPods. Hear it.

1057
01:06:14,195 --> 01:06:17,895
Speaker 6:  Hey Vergecast, this is Kyle calling in from Texas and I had a question for

1058
01:06:17,895 --> 01:06:21,055
Speaker 6:  y'all. I was listening to your episode on the new hearing aid updates to

1059
01:06:21,055 --> 01:06:24,775
Speaker 6:  Apple's AirPods. And while the new update sounds exciting and I'm looking

1060
01:06:24,775 --> 01:06:27,695
Speaker 6:  forward to seeing how people benefit from the testing capabilities and hearing

1061
01:06:27,715 --> 01:06:31,535
Speaker 6:  aid functionality, your discussion got me thinking don't

1062
01:06:31,535 --> 01:06:34,415
Speaker 6:  kill me. But I sometimes do the same thing that y'all mentioned where if

1063
01:06:34,415 --> 01:06:38,135
Speaker 6:  I'm ordering something I just take one of 'em out. And in most social settings

1064
01:06:38,375 --> 01:06:41,935
Speaker 6:  nowadays that seems to pretty consistently communicate that you're listening

1065
01:06:41,935 --> 01:06:45,775
Speaker 6:  to the other person, especially since nearly all headphones, pauses, whatever

1066
01:06:45,775 --> 01:06:49,655
Speaker 6:  you're listening to. But since that doesn't work for those using AirPods

1067
01:06:49,855 --> 01:06:53,775
Speaker 6:  as hearing aids, I had an idea, what if Apple added LEDs

1068
01:06:53,775 --> 01:06:57,575
Speaker 6:  to their headphones? This would, hi, keep copying Samsung's

1069
01:06:57,835 --> 01:07:01,615
Speaker 6:  new models. But even though it would take some time for everyone to learn,

1070
01:07:01,875 --> 01:07:05,495
Speaker 6:  it might allow you to tell if someone else wearing AirPods can hear you

1071
01:07:05,815 --> 01:07:09,335
Speaker 6:  speaking to 'em. It would kind of be like the way Apple tried to have eyesight

1072
01:07:09,335 --> 01:07:12,935
Speaker 6:  on the Vision Pro communicate where the user is paying attention, but

1073
01:07:12,935 --> 01:07:16,855
Speaker 6:  hopefully not in a way that's deeply in the uncanny valley. I thought

1074
01:07:16,855 --> 01:07:20,575
Speaker 6:  it was a fun idea, but maybe we'll instead we'll just get used to people

1075
01:07:20,575 --> 01:07:24,335
Speaker 6:  wearing headphones in public. But I was really interested to hear if y'all

1076
01:07:24,335 --> 01:07:28,215
Speaker 6:  thought it might work. Thanks again so much for taking my call. Love

1077
01:07:28,215 --> 01:07:29,375
Speaker 6:  the show. Bye.

1078
01:07:29,965 --> 01:07:33,455
Speaker 1:  Okay, I shared this call for two reasons. One because

1079
01:07:33,975 --> 01:07:37,535
Speaker 1:  I actually think it's an awesome idea and I think the idea of putting

1080
01:07:37,885 --> 01:07:41,215
Speaker 1:  some kind of notification LED on AirPods

1081
01:07:41,715 --> 01:07:44,855
Speaker 1:  to signal really whatever you want it to signal, but in this case it would

1082
01:07:44,855 --> 01:07:48,415
Speaker 1:  be to signal that they're in hearing aid mode actually fits with what we're

1083
01:07:48,415 --> 01:07:52,255
Speaker 1:  seeing from a lot of technology right now, right? He mentioned

1084
01:07:52,515 --> 01:07:56,255
Speaker 1:  the face stuff on the Vision Pro, but to me this sounds more like

1085
01:07:56,255 --> 01:08:00,015
Speaker 1:  what you see on the latest run of Smart Glasses. Probably the easiest

1086
01:08:00,015 --> 01:08:03,615
Speaker 1:  example is the RayBan Meta Smart Glasses where they actually

1087
01:08:03,895 --> 01:08:07,335
Speaker 1:  have a light that says essentially you're being recorded.

1088
01:08:07,875 --> 01:08:11,575
Speaker 1:  And I think that stuff is still pretty new and we're still figuring it out.

1089
01:08:11,575 --> 01:08:14,815
Speaker 1:  And I mean you rewind back to Google Glass and the idea of there being a

1090
01:08:14,815 --> 01:08:18,805
Speaker 1:  light suggesting that you were being recorded didn't actually help

1091
01:08:18,805 --> 01:08:22,565
Speaker 1:  anybody and just made everything feel gross and bad and didn't solve a lot

1092
01:08:22,565 --> 01:08:26,525
Speaker 1:  of problems. But I think we're at a point now where those kinds

1093
01:08:26,525 --> 01:08:30,125
Speaker 1:  of things can be communicated, right, where you're saying,

1094
01:08:30,555 --> 01:08:34,485
Speaker 1:  okay, this is, this is listening to you in some way. And I actually think

1095
01:08:34,485 --> 01:08:38,445
Speaker 1:  the idea of using an LED to mean essentially you are being

1096
01:08:38,805 --> 01:08:42,765
Speaker 1:  captured by my device in some meaningful way kind of works, right? Whether

1097
01:08:42,795 --> 01:08:46,365
Speaker 1:  it's the camera is on and I can see you and I'm taking video of you

1098
01:08:46,665 --> 01:08:50,365
Speaker 1:  or my headphones are capturing what you're saying and feeding it to me.

1099
01:08:50,365 --> 01:08:54,005
Speaker 1:  Either one of those If you that all the way down is just you are being captured

1100
01:08:54,145 --> 01:08:57,965
Speaker 1:  in some way. And I think if that's all we need these LEDs to signify,

1101
01:08:58,475 --> 01:09:02,325
Speaker 1:  that can really work. It'll take some marketing work to get people to

1102
01:09:02,325 --> 01:09:05,325
Speaker 1:  understand that and it'll take some time for it to sort of societally catch

1103
01:09:05,325 --> 01:09:09,205
Speaker 1:  on. But I would actually rather land in

1104
01:09:09,205 --> 01:09:13,125
Speaker 1:  that place than just pure, constant acceptance

1105
01:09:13,225 --> 01:09:16,845
Speaker 1:  of headphones. Because if we get to the point where you're wearing headphones

1106
01:09:16,985 --> 01:09:20,885
Speaker 1:  and I still assume that I can talk to you, I think that's bad and

1107
01:09:20,885 --> 01:09:24,325
Speaker 1:  I think that's wrong. Frankly, in a lot of ways, headphones as a signal

1108
01:09:24,665 --> 01:09:28,525
Speaker 1:  of leave Me alone are actually really powerful and valuable.

1109
01:09:28,745 --> 01:09:32,605
Speaker 1:  And also it's just true that a lot of the times when you're wearing

1110
01:09:32,605 --> 01:09:36,005
Speaker 1:  headphones you can't hear other people. So having a much more aggressive

1111
01:09:36,005 --> 01:09:39,845
Speaker 1:  signal that says, I can hear you. I am capturing you in some

1112
01:09:39,845 --> 01:09:43,405
Speaker 1:  way works for me. So I, think that's a great idea. I hope Apple does something

1113
01:09:43,405 --> 01:09:47,005
Speaker 1:  like it. I think we might be giving Samsung a little too much credit.

1114
01:09:47,005 --> 01:09:50,845
Speaker 1:  Samsung just like put LEDs on the Galaxy Buds

1115
01:09:50,845 --> 01:09:54,445
Speaker 1:  three pro that don't really do anything. I think they light up when you

1116
01:09:54,555 --> 01:09:58,525
Speaker 1:  turn on the find my feature, which like, sure, but they

1117
01:09:58,525 --> 01:10:02,405
Speaker 1:  can't show battery, they don't show when they're, you know, in various

1118
01:10:02,575 --> 01:10:05,565
Speaker 1:  modes. There are a bunch of things they could do there. I think the lights

1119
01:10:06,125 --> 01:10:09,805
Speaker 1:  actually look good in Samsung's case, they're mostly just aesthetic, but

1120
01:10:10,275 --> 01:10:13,285
Speaker 1:  it's proof that you could do something like this. And I think that's very

1121
01:10:13,285 --> 01:10:16,125
Speaker 1:  compelling. So I'm in favor of this idea a hundred percent. I hope Apple

1122
01:10:16,195 --> 01:10:19,645
Speaker 1:  does it, but the other reason I share this is because we get a lot of feedback

1123
01:10:19,645 --> 01:10:23,485
Speaker 1:  from people when Chris Welch and I talked about the hearing aid mode on the

1124
01:10:23,595 --> 01:10:27,285
Speaker 1:  AirPods, and especially when we talked about the societal

1125
01:10:27,535 --> 01:10:31,285
Speaker 1:  shift that is coming for headphones, and I think I've been

1126
01:10:31,285 --> 01:10:35,085
Speaker 1:  pretty blase about it to a certain extent, right?

1127
01:10:35,085 --> 01:10:38,725
Speaker 1:  Like I look at my nephews who are both teenagers,

1128
01:10:38,875 --> 01:10:42,805
Speaker 1:  they most of the time just have one AirPod in and I don't really think anything

1129
01:10:42,805 --> 01:10:46,645
Speaker 1:  of it, but then I talk to lots of other people who view that as

1130
01:10:47,315 --> 01:10:51,205
Speaker 1:  rude and off-putting and deliberately removing yourself from a

1131
01:10:51,205 --> 01:10:54,565
Speaker 1:  social situation in some way. I'm confident they don't see it that way,

1132
01:10:55,265 --> 01:10:58,885
Speaker 1:  but it might be that way. Anyway, So, I think that

1133
01:10:59,175 --> 01:11:03,165
Speaker 1:  piece of it is complicated and it's not gonna get easier. But

1134
01:11:03,165 --> 01:11:06,925
Speaker 1:  if we want these devices to be these kinds of always on

1135
01:11:07,675 --> 01:11:11,645
Speaker 1:  augmented reality devices that are genuinely useful for people's

1136
01:11:11,645 --> 01:11:15,445
Speaker 1:  health, but also more and more useful just in day-to-day life for

1137
01:11:15,445 --> 01:11:19,165
Speaker 1:  people, that is going to require

1138
01:11:19,175 --> 01:11:22,885
Speaker 1:  these huge societal changes. So either we get, you know, the LEDs

1139
01:11:23,105 --> 01:11:27,085
Speaker 1:  or we're gonna have to figure out the right and wrong ways to wear

1140
01:11:27,085 --> 01:11:30,245
Speaker 1:  headphones. And I would just wanna know what everybody thinks about that.

1141
01:11:30,315 --> 01:11:33,085
Speaker 1:  This is the thing we've gotten a lot of feedback about. I'd like to hear

1142
01:11:33,195 --> 01:11:37,045
Speaker 1:  your thoughts too. How should we think about headphones

1143
01:11:37,385 --> 01:11:41,085
Speaker 1:  in the real world, especially as headphones become more than

1144
01:11:41,155 --> 01:11:44,965
Speaker 1:  just music devices, as they become something that actually augments your

1145
01:11:44,965 --> 01:11:48,125
Speaker 1:  ability to go through your life while also being a way to listen to music

1146
01:11:48,125 --> 01:11:51,605
Speaker 1:  And TikTok, how do we navigate that? What is the right

1147
01:11:51,785 --> 01:11:55,725
Speaker 1:  answer for headphones? Should we enter a world where we're all wearing headphones

1148
01:11:55,785 --> 01:11:59,605
Speaker 1:  all the time and we just kind of figure it out? Should headphones just be

1149
01:11:59,605 --> 01:12:02,645
Speaker 1:  outlawed forever? And things that are hearing aids should look like hearing

1150
01:12:02,645 --> 01:12:05,685
Speaker 1:  aids? Is there an interesting answer somewhere in between both

1151
01:12:06,445 --> 01:12:09,845
Speaker 1:  culturally and in product? I wanna hear all your feedback. This has turned

1152
01:12:09,845 --> 01:12:12,885
Speaker 1:  out to be a much bigger, more interesting thing than I expected.

1153
01:12:13,945 --> 01:12:17,085
Speaker 1:  So please hit us up. I want to hear everything. We're gonna keep talking

1154
01:12:17,085 --> 01:12:21,045
Speaker 1:  about this in more and more ways in the coming months because I think this

1155
01:12:21,045 --> 01:12:24,445
Speaker 1:  matters in some pretty big ways. Alright, that is it for The Vergecast today.

1156
01:12:24,445 --> 01:12:27,645
Speaker 1:  Thank you to everybody who was on the show. Thank you for listening. Thank

1157
01:12:27,645 --> 01:12:31,205
Speaker 1:  you for calling the hotline. Thank you for being part of this with us. There's

1158
01:12:31,205 --> 01:12:34,165
Speaker 1:  lots more about everything we talked about, including this like mini package

1159
01:12:34,185 --> 01:12:37,845
Speaker 1:  of stories we did about the legacy and future of Alexa at the

1160
01:12:37,845 --> 01:12:41,085
Speaker 1:  verse.com. I'll link to all of it in the show notes, but as always, read

1161
01:12:41,085 --> 01:12:45,045
Speaker 1:  the verse.com. It's a big week this week. Go vote If. you haven't

1162
01:12:45,045 --> 01:12:47,685
Speaker 1:  voted yet. And if you're listening to this on Tuesday in the United States,

1163
01:12:48,185 --> 01:12:51,965
Speaker 1:  go vote. Read our election package. Lots of great stories in there.

1164
01:12:51,965 --> 01:12:55,805
Speaker 1:  Read the endorsement, go vote. Best of luck. To everybody out there, as

1165
01:12:55,805 --> 01:12:59,365
Speaker 1:  always, If, you have thoughts, questions, feelings, or other Alexa devices

1166
01:12:59,425 --> 01:13:02,285
Speaker 1:  you would like to set off inside of my house. You can always email us at

1167
01:13:02,445 --> 01:13:05,565
Speaker 1:  vergecast at The Verge dot com or call the hotline. It's six six VERGE one

1168
01:13:05,565 --> 01:13:09,205
Speaker 1:  one. we love hearing from you. This show is produced by Liam James, Wil Poor

1169
01:13:09,225 --> 01:13:12,565
Speaker 1:  and Eric Gomez. The Vergecast is VERGE production and part of the Vox Media

1170
01:13:12,565 --> 01:13:15,845
Speaker 1:  podcast network. Neil, and I'll be back on Friday to talk about,

1171
01:13:16,685 --> 01:13:20,045
Speaker 1:  honestly, I couldn't even guess at this point because it's an election week

1172
01:13:20,145 --> 01:13:23,805
Speaker 1:  and I suspect we'll be talking about that plus some other product news plus

1173
01:13:23,885 --> 01:13:27,285
Speaker 1:  a bunch of Apple stuff that's shipping this week. Lots to do. We'll see you

1174
01:13:27,285 --> 01:13:28,125
Speaker 1:  then. Rock and roll

