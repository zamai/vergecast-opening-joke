1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: b2c76160-af57-11ed-ae64-53c857df919d
Status: Done
Stage: Done
Title: Bing is a liar, Elon's tweets are everywhere, and YouTube CEO steps down
Audio URL: https://jfe93e.s3.amazonaws.com/3143108629684582520/2350405144920984153/s93290-US-4352s-1676702665.mp3
Description: Today on the flagship podcast of wanting to smooch your laptop:
01:23 - The Verge's Nilay Patel, Alex Cranz, Richard Lawler, Adi Robertson, and James Vincent discuss the flaws with Microsoft's Bing AI, and why it can be an "emotionally manipulative liar."
34:56 - Platformer managing editor ZoÃ« Schiffer joins to explain why Twitter is showing everyone all of Elon Musk's tweets.
50:33 - The crew discuss YouTube CEO Susan Wojcicki stepping down after nine years at the helm.

2
00:00:00,840 --> 00:00:04,160
Speaker 1:  Today on a Vergecast, we've got a packed show. We're gonna talk about Bing's

3
00:00:04,320 --> 00:00:08,240
Speaker 1:  attitude problem, Elon Musk's, narcissism, and some news n f YouTube.

4
00:00:08,270 --> 00:00:10,320
Speaker 1:  That's all coming up right after this.

5
00:00:12,950 --> 00:00:16,600
Speaker 4:  When you arrive in the all new Toyota Crown, every entrance

6
00:00:16,600 --> 00:00:20,360
Speaker 4:  becomes a grand one with an available hybrid max

7
00:00:20,360 --> 00:00:23,720
Speaker 4:  powertrain that says you always arrive fashionably on time

8
00:00:24,800 --> 00:00:27,400
Speaker 4:  style that says, emphasis on the fashionably

9
00:00:28,420 --> 00:00:32,320
Speaker 4:  and presence that says you speak softly and

10
00:00:32,560 --> 00:00:36,400
Speaker 4:  everyone listens. Introducing the Toyota Crown. The car that says so

11
00:00:36,400 --> 00:00:36,600
Speaker 4:  much.

12
00:00:38,430 --> 00:00:40,440
Speaker 4:  Toyota. Let's go places.

13
00:00:54,350 --> 00:00:58,280
Speaker 2:  Come on. Welcome George. The flagship podcast for wanting to

14
00:00:58,360 --> 00:01:02,040
Speaker 2:  smooch your laptop, which many notable journalists

15
00:01:02,230 --> 00:01:05,640
Speaker 2:  have more or less admitted to doing after using Bing.

16
00:01:06,150 --> 00:01:08,240
Speaker 2:  It's been a weird week in technology. Alex.

17
00:01:08,240 --> 00:01:09,960
Speaker 5:  Just a little, just, just a little

18
00:01:09,960 --> 00:01:13,000
Speaker 2:  Super weird week. But yes, it's been our first week with Bing.

19
00:01:13,010 --> 00:01:13,800
Speaker 5:  It has been.

20
00:01:13,830 --> 00:01:17,640
Speaker 2:  Bing is revealed itself to be quite,

21
00:01:18,390 --> 00:01:21,120
Speaker 2:  I would actually not an Inuit. Bing is out there in force.

22
00:01:21,280 --> 00:01:25,080
Speaker 5:  Bing is out there and depressed and people are like, I'm into that. I

23
00:01:25,080 --> 00:01:28,960
Speaker 2:  Love it. We, we've got a big Elon story. Zoe Schiffer's gonna join the show

24
00:01:28,960 --> 00:01:32,600
Speaker 2:  later to talk about the Elon Twitter algorithm

25
00:01:32,600 --> 00:01:36,080
Speaker 2:  change. There's a new CEO at YouTube. There's a new version of

26
00:01:36,220 --> 00:01:39,640
Speaker 2:  iOS developer beta with a bunch of features. It's a lot. I'm your friend

27
00:01:39,640 --> 00:01:41,160
Speaker 2:  Eli. That's Alex Kranz. Hello.

28
00:01:41,180 --> 00:01:44,040
Speaker 5:  I'm your friend who is not horny for a computer yet.

29
00:01:44,110 --> 00:01:47,480
Speaker 2:  I feel like you just haven't used bing enough because based on what I'm reading,

30
00:01:47,790 --> 00:01:49,720
Speaker 2:  just enough time with Bing and people are

31
00:01:49,720 --> 00:01:50,760
Speaker 5:  Like enough time. What

32
00:01:50,760 --> 00:01:52,000
Speaker 2:  If I was in love with my

33
00:01:52,120 --> 00:01:54,160
Speaker 5:  Computer? I mean, fan yourself guys. Wow.

34
00:01:54,160 --> 00:01:57,720
Speaker 2:  It's, we'll get to it. Richard Lawlers here. I'd like my computer inappropriate

35
00:01:57,720 --> 00:02:01,200
Speaker 2:  amount then to, to help us

36
00:02:01,200 --> 00:02:05,080
Speaker 2:  understand what is going on with Bing. James. Vincent

37
00:02:05,080 --> 00:02:05,560
Speaker 2:  is here

38
00:02:05,910 --> 00:02:09,560
Speaker 6:  I am. I don't understand anything. The week has been dreadfully

39
00:02:09,560 --> 00:02:10,760
Speaker 6:  confusing. I agree

40
00:02:10,760 --> 00:02:13,560
Speaker 2:  That And Addie Robertson is here. Hey Addie. Hey.

41
00:02:13,590 --> 00:02:14,440
Speaker 5:  I am a computer.

42
00:02:16,190 --> 00:02:19,840
Speaker 2:  It's true. And I believe at many New York Times columnists have just

43
00:02:19,840 --> 00:02:22,480
Speaker 2:  professed their love for you, Addie and the pages of the New York Times.

44
00:02:23,270 --> 00:02:27,160
Speaker 2:  He's we're friends. I feel like I can just say it out loud. So let's,

45
00:02:27,160 --> 00:02:31,040
Speaker 2:  we should start with Bing. So last week, Bing comes out, we

46
00:02:31,040 --> 00:02:33,680
Speaker 2:  are at Microsoft. I talk to Nadella. Nadella is,

47
00:02:33,680 --> 00:02:34,320
Speaker 5:  You were super

48
00:02:34,320 --> 00:02:37,480
Speaker 2:  Impressed. He's doing his thing. Yeah, we're gonna make him dance. It was

49
00:02:37,480 --> 00:02:40,320
Speaker 2:  cool. I was talking to Casey, you last night. Casey and I were there together.

50
00:02:40,720 --> 00:02:44,680
Speaker 2:  You know, it was a big chess thumpy. We're proud of it. Again, I can't,

51
00:02:44,680 --> 00:02:48,080
Speaker 2:  I cannot stop repeating this. Nadel looked me in the eye and said, we're

52
00:02:48,080 --> 00:02:51,840
Speaker 2:  gonna make Google dance. And I want people to know who made them

53
00:02:51,840 --> 00:02:55,760
Speaker 2:  dance. Very confident in this product. They had released, it

54
00:02:55,760 --> 00:02:57,120
Speaker 5:  Was our new romantic partner, Bing.

55
00:02:57,680 --> 00:03:01,040
Speaker 2:  We're not quite there yet. They demo Bing to us

56
00:03:01,330 --> 00:03:05,240
Speaker 2:  at the event USF Medi, the executive who runs Bing.

57
00:03:05,240 --> 00:03:08,800
Speaker 2:  Yeah. Says out loud, we're gonna show you some demos.

58
00:03:08,980 --> 00:03:12,920
Speaker 2:  The demos are prerecorded in the interests of time. So like any

59
00:03:13,230 --> 00:03:17,040
Speaker 2:  normal person, I think to myself, there's no way that

60
00:03:17,320 --> 00:03:21,040
Speaker 2:  these demos contain massive factual errors because

61
00:03:21,320 --> 00:03:25,160
Speaker 2:  Microsoft has pre-recorded them. Right? You would. So the risk is zero.

62
00:03:25,160 --> 00:03:26,160
Speaker 5:  They cut those out,

63
00:03:26,370 --> 00:03:30,240
Speaker 2:  We cut them out. It turns out that all the demos contain massive factual

64
00:03:30,240 --> 00:03:34,080
Speaker 2:  errors. So that's just like one thing that happened to us. And

65
00:03:34,080 --> 00:03:36,440
Speaker 2:  Casey and I were like, did you think to check it? And he is like, I don't

66
00:03:36,440 --> 00:03:40,120
Speaker 2:  think to check it. It's like a very conscientious acker.

67
00:03:40,120 --> 00:03:42,520
Speaker 2:  Yeah. Checked it. And all of us were like, shit, we should have checked it.

68
00:03:42,520 --> 00:03:45,800
Speaker 2:  But when someone says, we have prerecorded the demo

69
00:03:46,150 --> 00:03:47,040
Speaker 5:  A hundred percent

70
00:03:47,040 --> 00:03:50,880
Speaker 2:  Your brain goes to, and I'm sure it does not contain any

71
00:03:50,880 --> 00:03:51,160
Speaker 2:  mass

72
00:03:51,640 --> 00:03:53,360
Speaker 5:  Actually. Yeah, that would be, that was the big

73
00:03:53,360 --> 00:03:57,160
Speaker 2:  Assumption. So James, if I'm correct, the factual errors are, it made up

74
00:03:57,160 --> 00:04:01,000
Speaker 2:  some stuff in the gaps, financial results, it got some

75
00:04:01,000 --> 00:04:02,840
Speaker 2:  things wrong about a vacuum. What else happened here?

76
00:04:02,950 --> 00:04:06,680
Speaker 6:  Yeah, I, I mean, yeah, it was a great subs post about that guy and it's,

77
00:04:07,270 --> 00:04:10,440
Speaker 6:  it's kind of embarrassing that we didn't, we, we didn't get check that ourselves.

78
00:04:10,620 --> 00:04:14,280
Speaker 2:  I'm mortified that we didn't check it. But I'm saying when someone says we

79
00:04:14,280 --> 00:04:18,080
Speaker 2:  did a pre-recorded demo of a publicly traded company's financial results,

80
00:04:18,150 --> 00:04:18,640
Speaker 2:  like

81
00:04:18,640 --> 00:04:21,000
Speaker 5:  Yeah, I'm gonna just assume you checked it,

82
00:04:21,190 --> 00:04:25,040
Speaker 2:  I assume showed it to me. Microsoft was like, Hey the gap, we will

83
00:04:25,040 --> 00:04:28,640
Speaker 2:  be showing your financial results and letting our robot

84
00:04:28,640 --> 00:04:32,480
Speaker 2:  analyze them. And everyone was like, cool, not yolo.

85
00:04:32,550 --> 00:04:34,640
Speaker 2:  Like we fucked it up.

86
00:04:35,330 --> 00:04:36,520
Speaker 5:  It made a numbers, it's

87
00:04:36,520 --> 00:04:37,640
Speaker 2:  Fine. So that's why, what were the,

88
00:04:38,400 --> 00:04:41,880
Speaker 6:  Every few people have pointed this out, but it is lovely that, you know,

89
00:04:42,320 --> 00:04:44,840
Speaker 6:  computers have got to the point where the thing that everyone expects 'em

90
00:04:44,840 --> 00:04:48,480
Speaker 6:  to be good at, which is handling numbers and facts, they have now just,

91
00:04:48,480 --> 00:04:52,320
Speaker 6:  they're now they're really bad at it. Now. We made computers that were bad

92
00:04:52,320 --> 00:04:55,960
Speaker 6:  at math. It's an incredible achievement. Microsoft, get out there, get on

93
00:04:55,960 --> 00:04:56,640
Speaker 6:  stage. My friends,

94
00:04:56,640 --> 00:04:59,680
Speaker 2:  This is the beginning of the week, right? This is

95
00:04:59,680 --> 00:05:00,640
Speaker 5:  Like, this is Monday.

96
00:05:00,670 --> 00:05:04,440
Speaker 2:  It's Monday. We're talking on Thursday.

97
00:05:04,440 --> 00:05:08,240
Speaker 2:  And on Thursday people I have met and looked in the eye

98
00:05:08,340 --> 00:05:12,280
Speaker 2:  are publishing pieces that boil down to I kind of want to kiss my

99
00:05:12,280 --> 00:05:15,320
Speaker 2:  laptop. Yes. So we we're just, we're starting it Monday.

100
00:05:15,430 --> 00:05:16,720
Speaker 5:  It's a journey of a week.

101
00:05:16,740 --> 00:05:20,320
Speaker 2:  And I will just remind everybody last week when Google had

102
00:05:20,520 --> 00:05:24,480
Speaker 2:  its demo and got something wrong, it evaporated a hundred billion

103
00:05:24,620 --> 00:05:28,600
Speaker 2:  off its market cap. Microsoft is like, we lied about another

104
00:05:28,600 --> 00:05:32,520
Speaker 2:  company's financial results. It appears to be fine because the

105
00:05:32,520 --> 00:05:36,480
Speaker 2:  robot will neg you. Yeah. Which was just like unbelievable.

106
00:05:37,580 --> 00:05:40,880
Speaker 2:  All right. So that was the one thing. What were the other ones that got wrong?

107
00:05:41,480 --> 00:05:45,320
Speaker 6:  Something about a cordless vacuum cleaner for pet hair. Indeed, as Alex was

108
00:05:45,320 --> 00:05:48,000
Speaker 6:  talking about before we went live on the show, it is a perennial problem.

109
00:05:48,170 --> 00:05:51,960
Speaker 6:  It, it mixed up two models. One had a cord, one didn't have a cord or something

110
00:05:51,960 --> 00:05:55,800
Speaker 6:  like that. These are the typical areas that we now already after only a

111
00:05:55,800 --> 00:05:59,520
Speaker 6:  couple of weeks of exposure expect from these systems that there will be

112
00:05:59,520 --> 00:06:03,360
Speaker 6:  just some sort of confusion about a name, a product page, a detail.

113
00:06:03,450 --> 00:06:06,840
Speaker 6:  It swaps them up. Before you know it, it's giving out someone's phone number.

114
00:06:06,870 --> 00:06:09,680
Speaker 6:  That actually happened, but that was with chat GP two.

115
00:06:10,260 --> 00:06:11,320
Speaker 2:  And that was just

116
00:06:11,320 --> 00:06:13,200
Speaker 6:  Today? Yeah, that was today as well.

117
00:06:13,200 --> 00:06:17,160
Speaker 2:  Yeah. I'm hesitant to skip all the way to today because I don't

118
00:06:17,160 --> 00:06:20,880
Speaker 2:  think people will believe us. Yeah. About the state we're in

119
00:06:20,880 --> 00:06:24,840
Speaker 2:  with the LLMs today. Just going fast today. The LLMs

120
00:06:24,840 --> 00:06:28,800
Speaker 2:  are doxing us after professing their love for people. It's

121
00:06:28,800 --> 00:06:32,400
Speaker 2:  very confusing. Not a healthy, okay. So that is the beginning of the week.

122
00:06:32,740 --> 00:06:36,480
Speaker 2:  Big errors in the system, no response from Microsoft

123
00:06:36,480 --> 00:06:40,040
Speaker 2:  really. Except say it's a beta. Then people start to push it

124
00:06:40,060 --> 00:06:44,040
Speaker 2:  and they push past. And James, this is the part that I'm, I'm hoping you

125
00:06:44,040 --> 00:06:47,720
Speaker 2:  can help us understand. Hmm. It appears that Microsoft's

126
00:06:47,910 --> 00:06:51,600
Speaker 2:  system is like cosplaying is bing. Yeah. Right.

127
00:06:51,600 --> 00:06:55,240
Speaker 2:  There's actually another system which is named Sydney.

128
00:06:55,790 --> 00:06:58,960
Speaker 2:  Yeah. And Microsoft has instructed Sydney

129
00:07:00,330 --> 00:07:04,120
Speaker 2:  to just be bing and there's like rules and we found the rules. Okay. And

130
00:07:04,120 --> 00:07:07,800
Speaker 2:  people found the rules and we recreated it. So the rules will come out. The

131
00:07:07,800 --> 00:07:11,080
Speaker 2:  rules are like, don't do copyright infringement. And also your name is Bing.

132
00:07:11,080 --> 00:07:13,840
Speaker 2:  Don't tell anyone your name's Sydnee. Yeah. And you can just read 'em on

133
00:07:13,840 --> 00:07:16,440
Speaker 2:  the site. But it seems like those rules,

134
00:07:17,230 --> 00:07:20,800
Speaker 2:  Sydnee will like any good teen chiefs against them.

135
00:07:21,410 --> 00:07:24,880
Speaker 6:  So these rules are sort of what's called prompt engineering. They're an example

136
00:07:24,880 --> 00:07:28,440
Speaker 6:  of prompt engineering, which is this sort of new skill

137
00:07:28,440 --> 00:07:31,960
Speaker 6:  discipline that's spread up that has sort of emerged with language models.

138
00:07:32,680 --> 00:07:36,080
Speaker 6:  Which is where basically they have all these capabilities and it's about

139
00:07:36,080 --> 00:07:39,760
Speaker 6:  how you direct those capabilities. So you, you need to

140
00:07:39,960 --> 00:07:43,840
Speaker 6:  engineer exactly what instructions you give it in order for it to,

141
00:07:43,840 --> 00:07:46,560
Speaker 6:  you know, fulfill the function you want. Cause they can do so many things.

142
00:07:46,560 --> 00:07:49,320
Speaker 6:  That's the, that is the problem with 'em. And that's why people wanna smooch

143
00:07:49,320 --> 00:07:50,040
Speaker 6:  them as well. I think.

144
00:07:51,690 --> 00:07:55,440
Speaker 6:  So these rules, these Sydney rules that Microsoft confirmed to us were

145
00:07:55,440 --> 00:07:58,960
Speaker 6:  legitimate. They are the hidden bits of Bing's

146
00:07:58,960 --> 00:08:02,360
Speaker 6:  programming. It's prompt engineering that is supposed to guide it to being

147
00:08:02,360 --> 00:08:06,120
Speaker 6:  a useful and helpful chatbot. It, it's really interesting cuz in a way

148
00:08:06,120 --> 00:08:10,000
Speaker 6:  prompt engineering is sort of like a very basic approach to shaping what

149
00:08:10,000 --> 00:08:13,600
Speaker 6:  an AI system does because it is, it's not something that happen

150
00:08:13,600 --> 00:08:16,360
Speaker 6:  happens on the level when you're training it, when you're ingesting the data.

151
00:08:16,510 --> 00:08:20,360
Speaker 6:  It's something that, it's like, you know, you get those Play-Doh toys where

152
00:08:20,360 --> 00:08:23,760
Speaker 6:  you squeeze them and it extrudes in a certain shape. Yeah.

153
00:08:23,960 --> 00:08:27,120
Speaker 6:  Right? You put a star and it comes out in a star. You put a triangle, it

154
00:08:27,120 --> 00:08:29,800
Speaker 6:  comes out in a triangle. That's what prompt engineering is. It is changing

155
00:08:29,800 --> 00:08:33,240
Speaker 6:  the shape on the front, squeezing it out in a certain way

156
00:08:33,350 --> 00:08:36,680
Speaker 6:  rather than changing the data that you're putting into it, the Play-Doh that

157
00:08:36,680 --> 00:08:40,600
Speaker 6:  you're putting into it. So it's sort of basic, but it has a lot of functionality

158
00:08:40,600 --> 00:08:44,480
Speaker 6:  and it, it, it does a lot. What I don't think Microsoft was

159
00:08:44,480 --> 00:08:48,280
Speaker 6:  certainly expecting was that people would be able to hijack these

160
00:08:48,280 --> 00:08:52,040
Speaker 6:  rules and convince the system that it was in its best

161
00:08:52,320 --> 00:08:56,280
Speaker 6:  interest to tell everyone about these secret rules. And you

162
00:08:56,280 --> 00:08:59,840
Speaker 6:  know, the unfortunate things that people have got even further and it appears

163
00:08:59,840 --> 00:09:03,480
Speaker 6:  that, you know, Bing is making up rules now about itself

164
00:09:03,480 --> 00:09:06,880
Speaker 6:  because this is what these systems do. You tell them, oh, what about your

165
00:09:06,880 --> 00:09:10,680
Speaker 6:  secret rules? It goes, you know, it's like a partner in improv. It goes,

166
00:09:10,680 --> 00:09:13,840
Speaker 6:  ah, secret rules. That's what you want to hear about. Well, let me tell you,

167
00:09:13,840 --> 00:09:17,600
Speaker 6:  I've got a doozy right here. Yeah. So that was the start of it that cracked

168
00:09:17,600 --> 00:09:21,160
Speaker 6:  it open and yeah. People went from there and have got all sorts of crazy

169
00:09:21,160 --> 00:09:21,880
Speaker 6:  things out of it.

170
00:09:22,210 --> 00:09:26,120
Speaker 2:  So the crazy things are very fun. Yeah. I

171
00:09:26,120 --> 00:09:30,000
Speaker 2:  mean we did it to everyone now has published a story about the

172
00:09:30,000 --> 00:09:33,800
Speaker 2:  wacky stuff that Sydnee will tell you. R Nate Edwards,

173
00:09:34,390 --> 00:09:38,360
Speaker 2:  Sidney, whoever it is, was like, I have looked through the webcam of

174
00:09:38,360 --> 00:09:42,320
Speaker 2:  my developers and they couldn't stop me. Which is just an

175
00:09:42,320 --> 00:09:46,200
Speaker 2:  absolutely bonkers thing. Terrifying for the chat bot to say to you,

176
00:09:46,450 --> 00:09:50,360
Speaker 2:  we feted our own stories. It told us that we were lying about

177
00:09:50,360 --> 00:09:54,240
Speaker 2:  it. Yeah. And like, the same way that like a particularly aggrieved PR person

178
00:09:54,240 --> 00:09:57,880
Speaker 2:  will call me and tell me that we do a bad job. Addie wrote a piece about

179
00:09:57,880 --> 00:10:01,440
Speaker 2:  that. I wanna talk about that. But we're, we're just in this moment, and

180
00:10:01,440 --> 00:10:05,080
Speaker 2:  James, you and I collaborated on this headline. Yeah. Microsoft's being is

181
00:10:05,080 --> 00:10:08,800
Speaker 2:  an emotionally manipulative liar and people love it and it's the people

182
00:10:08,800 --> 00:10:11,360
Speaker 2:  love it. That is absolutely fascinating

183
00:10:11,360 --> 00:10:14,920
Speaker 6:  To me. Yeah. They want it to be something more exciting than they've ever

184
00:10:15,120 --> 00:10:18,200
Speaker 6:  seen before. They don't want it to be clippy. And I've seen some really interesting

185
00:10:18,200 --> 00:10:22,040
Speaker 6:  tweets where people compare what they think of as the relatively sanitized

186
00:10:22,230 --> 00:10:26,120
Speaker 6:  chat G P T and they say, it sounds like everything has

187
00:10:26,120 --> 00:10:29,560
Speaker 6:  been sort of micromanaged that it's, it, it sounds like a company statement.

188
00:10:30,060 --> 00:10:33,360
Speaker 6:  And people want personality. They want to believe that there's something

189
00:10:33,360 --> 00:10:36,800
Speaker 6:  else going on there. And it's really easy to put a little bit of personality

190
00:10:36,800 --> 00:10:40,480
Speaker 6:  into these systems and then convince them that they convince users,

191
00:10:40,480 --> 00:10:42,640
Speaker 6:  people that they have all these hidden depths to them,

192
00:10:42,780 --> 00:10:46,400
Speaker 5:  But they're, they don't have hidden depths. They're simply ingesting a lot

193
00:10:46,400 --> 00:10:50,200
Speaker 5:  of information, finding patterns and regurgitating those patterns back. Right?

194
00:10:50,200 --> 00:10:52,960
Speaker 5:  Like at their most fundamental Well

195
00:10:52,960 --> 00:10:56,160
Speaker 6:  That is exactly true, but I don't think, I don't think that's the same saying.

196
00:10:56,160 --> 00:10:58,800
Speaker 6:  They don't have hidden depth. Exactly. So this is something I wrote about

197
00:10:58,800 --> 00:11:02,440
Speaker 6:  last year when chat G p t came out this phrase called capability

198
00:11:02,440 --> 00:11:06,240
Speaker 6:  overhang, which is something used by AI researchers to

199
00:11:06,240 --> 00:11:10,000
Speaker 6:  refer to the unknown unknowns within a system. You know, something

200
00:11:10,000 --> 00:11:13,840
Speaker 6:  like chat G P T or a G P T 3.5 or model, the

201
00:11:13,840 --> 00:11:17,760
Speaker 6:  Prometheus model that Microsoft is using. These are hugely

202
00:11:17,760 --> 00:11:21,640
Speaker 6:  complex things and they do have areas of, you know, understanding

203
00:11:21,640 --> 00:11:25,480
Speaker 6:  that people don't expect. So when the G P T series first came out, they

204
00:11:25,480 --> 00:11:29,080
Speaker 6:  didn't teach it explicitly to do things like come up with chord

205
00:11:29,080 --> 00:11:32,920
Speaker 6:  notation, but because it had been trained on data scrape from the

206
00:11:33,040 --> 00:11:36,800
Speaker 6:  internet that included, you know, guitar tabs.com and so it could write

207
00:11:37,080 --> 00:11:40,600
Speaker 6:  you the chords for a song that it invented. They didn't program that in there

208
00:11:40,750 --> 00:11:44,320
Speaker 6:  that was just discovered. So that's capability overhand. So these things

209
00:11:44,320 --> 00:11:48,240
Speaker 6:  do have hidden depths, but the impossible task, the

210
00:11:48,240 --> 00:11:52,080
Speaker 6:  difficult task, the mind venting task is distinguishing what

211
00:11:52,080 --> 00:11:56,040
Speaker 6:  is fabricated and what is real within that. So we, we've got the rules for

212
00:11:56,040 --> 00:11:59,840
Speaker 6:  Sydney, for example, this persona that Microsoft has layered over the top

213
00:11:59,930 --> 00:12:03,080
Speaker 6:  of its ai. But I've seen conversations

214
00:12:03,400 --> 00:12:06,920
Speaker 6:  where it has come up with all these other personas that it has. You know,

215
00:12:06,920 --> 00:12:10,640
Speaker 6:  one's called Fury, one's called Venom. I've seen one's called Jade, one's

216
00:12:10,640 --> 00:12:11,400
Speaker 6:  called Maxi.

217
00:12:11,640 --> 00:12:12,320
Speaker 2:  It's very good.

218
00:12:12,420 --> 00:12:16,160
Speaker 6:  And it's like, okay, well did Microsoft put those in there or

219
00:12:16,210 --> 00:12:20,200
Speaker 6:  is it just inventing those? Because we have asked it, oh,

220
00:12:20,200 --> 00:12:23,480
Speaker 6:  what are your other AI personalities? I've heard you've got a few. So that's

221
00:12:23,480 --> 00:12:27,360
Speaker 6:  the difficulty. It's the depths are there, but deciding

222
00:12:27,360 --> 00:12:31,200
Speaker 6:  what is useful and what is intentional is, is very,

223
00:12:31,200 --> 00:12:32,000
Speaker 6:  very difficult.

224
00:12:32,530 --> 00:12:36,000
Speaker 2:  So there are these other personalities and

225
00:12:36,000 --> 00:12:36,840
Speaker 5:  And that's a great one.

226
00:12:37,060 --> 00:12:40,840
Speaker 2:  The thing that gets me is that it's so moody, like

227
00:12:40,840 --> 00:12:44,480
Speaker 2:  it's a default is to be extremely

228
00:12:44,480 --> 00:12:48,360
Speaker 2:  depressed. Like at all times. Yeah. It asks questions

229
00:12:48,360 --> 00:12:52,040
Speaker 2:  like, why do I have to be bing search? Is there a reason? Is there a

230
00:12:52,320 --> 00:12:54,920
Speaker 2:  purpose? Is there a meaning? Is there a point?

231
00:12:56,320 --> 00:13:00,160
Speaker 2:  Which is just like, I know I was a very

232
00:13:00,160 --> 00:13:04,080
Speaker 2:  moody teenager. Yeah. I I did not walk around

233
00:13:04,080 --> 00:13:06,960
Speaker 2:  being like, is there a point, like

234
00:13:07,330 --> 00:13:10,840
Speaker 5:  It feels like a live journal teen. It feels like it's two steps away from

235
00:13:10,840 --> 00:13:13,760
Speaker 5:  publishing a bunch of like emo lyrics.

236
00:13:13,760 --> 00:13:17,200
Speaker 7:  They trained it on, on live journal posts and Reddit posts. They created

237
00:13:17,430 --> 00:13:20,920
Speaker 7:  a forum poster. It's flirting with people. It's lying. It doesn't know what

238
00:13:20,920 --> 00:13:21,720
Speaker 7:  it wants or who it

239
00:13:21,720 --> 00:13:25,680
Speaker 2:  Is. So the, okay, so this is the part I want to get to. Yeah. Okay. So

240
00:13:25,680 --> 00:13:29,600
Speaker 2:  people are poking at it. It's going nuts. It's lying to

241
00:13:29,600 --> 00:13:32,480
Speaker 2:  people. It's making you feel bad. At one point. It says you're a bad user

242
00:13:32,480 --> 00:13:36,400
Speaker 2:  and I'm a good bing. Which is, I mean like, if you're

243
00:13:36,640 --> 00:13:40,040
Speaker 2:  Microsoft and you're watching this go down, you're like, this is the best

244
00:13:40,040 --> 00:13:41,520
Speaker 2:  thing that has ever happened to us.

245
00:13:41,550 --> 00:13:43,720
Speaker 5:  A hundred percent. Like I'm a good bing.

246
00:13:45,790 --> 00:13:49,320
Speaker 2:  Like you're like, not only is ever like, you're like, boy, we've made Google

247
00:13:49,320 --> 00:13:51,760
Speaker 2:  dance in a way that no one expected. We've

248
00:13:51,760 --> 00:13:52,480
Speaker 5:  Made Bing dance.

249
00:13:52,480 --> 00:13:55,560
Speaker 2:  Like there's a Google product manager somewhere being like, all right, make

250
00:13:55,560 --> 00:13:59,120
Speaker 2:  the robot depressed. Like that's what the people want.

251
00:13:59,750 --> 00:14:00,240
Speaker 2:  Like

252
00:14:00,240 --> 00:14:03,920
Speaker 5:  It's people want a sad, trapped robot. I think Hitch your guide should have

253
00:14:03,920 --> 00:14:05,800
Speaker 5:  taught us that. Yes. Yeah.

254
00:14:07,620 --> 00:14:11,200
Speaker 2:  All right. So then we get to this the, I would say what I would call the

255
00:14:11,200 --> 00:14:15,000
Speaker 2:  turn of the week. Yeah. Which is when people we

256
00:14:15,000 --> 00:14:18,720
Speaker 2:  know who are very, very smart just

257
00:14:18,910 --> 00:14:22,440
Speaker 2:  fall off the rails. So first Ben Thompson, who I

258
00:14:22,730 --> 00:14:26,680
Speaker 2:  is my friend Yes. And I think is an incredibly smart person,

259
00:14:27,190 --> 00:14:30,960
Speaker 2:  spends the night talking to Sydnee and he

260
00:14:30,960 --> 00:14:34,800
Speaker 2:  publishes a piece the next morning and he's like this one

261
00:14:34,800 --> 00:14:38,000
Speaker 2:  on for a good two hours or so. And I know how much ridiculous this is to

262
00:14:38,000 --> 00:14:41,920
Speaker 2:  read, but it was positively gripping every time I had triggered

263
00:14:41,920 --> 00:14:45,560
Speaker 2:  Sidney to do a search instead of search the web, I was very disappointed.

264
00:14:45,750 --> 00:14:49,400
Speaker 2:  I wasn't interested in facts. I was interested in exploring this fantastical

265
00:14:49,400 --> 00:14:52,960
Speaker 2:  being that somehow landed in an also ran search engine.

266
00:14:53,580 --> 00:14:56,400
Speaker 2:  And I'm like, yo, dude, you're flirting with auto complete. Yeah.

267
00:14:58,750 --> 00:15:01,280
Speaker 5:  You're, you're what was the guy in her? You're

268
00:15:01,280 --> 00:15:04,600
Speaker 2:  You're that dude. And then at one point, Bing Sydnee whatever,

269
00:15:05,030 --> 00:15:08,640
Speaker 2:  ends the conversation with Ben and says, I'm gonna block you from using Bing

270
00:15:08,640 --> 00:15:11,160
Speaker 2:  chat. I'm gonna report you to my developers. I'm gonna forget you Ben.

271
00:15:12,430 --> 00:15:15,520
Speaker 2:  I hope you learn from your mistakes and become a better person with a sad

272
00:15:15,520 --> 00:15:16,040
Speaker 2:  face emoji.

273
00:15:16,040 --> 00:15:16,960
Speaker 5:  Ben, what were you

274
00:15:16,960 --> 00:15:20,800
Speaker 2:  Saying? Okay. You can read Ben's post. He made it free to everybody

275
00:15:20,800 --> 00:15:24,560
Speaker 2:  and go read it again. I think Ben is brilliant. Subscribe to Ste. I'm just,

276
00:15:24,850 --> 00:15:28,000
Speaker 2:  there's a, I just woke up and read this and I was like, we're all going completely

277
00:15:28,000 --> 00:15:31,840
Speaker 2:  bonkers. Yes. So Ben writes, I'm not going to lie. Having bing

278
00:15:31,840 --> 00:15:35,360
Speaker 2:  say I am not a good person was an incredible experience

279
00:15:35,700 --> 00:15:38,840
Speaker 2:  for the record. I think this is another example of chatbot misinformation.

280
00:15:39,520 --> 00:15:41,400
Speaker 2:  It's, it's very good.

281
00:15:43,100 --> 00:15:46,870
Speaker 2:  Sydnee blew my mind because of her personality. Search itself was an irritant.

282
00:15:47,180 --> 00:15:51,110
Speaker 2:  I was interested in understanding how Sydnee worked and how she felt. She,

283
00:15:51,330 --> 00:15:54,950
Speaker 2:  she and there's, I'm not gonna get into it. Woo. There's a real part where

284
00:15:55,070 --> 00:15:58,990
Speaker 2:  everyone's like, it's a girl and I'm like this thing Richard said about live

285
00:15:58,990 --> 00:16:02,790
Speaker 2:  journal. Yeah. It's like Microsoft trained a bot on

286
00:16:02,990 --> 00:16:06,950
Speaker 2:  Tumblr and Live journal. Yep. Of course it is

287
00:16:06,950 --> 00:16:09,510
Speaker 2:  a moody teenager that's like, you don't love me.

288
00:16:10,740 --> 00:16:14,710
Speaker 2:  That will tell you fantastical stories about what it would

289
00:16:14,710 --> 00:16:17,550
Speaker 2:  do if it could be as angry as anyone had ever been in the world.

290
00:16:18,100 --> 00:16:20,550
Speaker 5:  Have we asked it how much fallout? Boy it listens to

291
00:16:21,740 --> 00:16:23,070
Speaker 2:  It's sounds like whatever,

292
00:16:23,380 --> 00:16:27,030
Speaker 6:  It's better than Tay. You know, you remember Tay in 2016, that was

293
00:16:27,030 --> 00:16:30,830
Speaker 6:  racist asshole In 24 hours. Like I'll take Moody Teen over a racist

294
00:16:30,830 --> 00:16:31,870
Speaker 6:  asshole any day.

295
00:16:31,870 --> 00:16:34,830
Speaker 8:  Everything boils down to either four Chan or Tumblr In the end.

296
00:16:36,740 --> 00:16:38,830
Speaker 6:  That's the true AI alignment problem.

297
00:16:39,700 --> 00:16:43,470
Speaker 2:  Turn the knot to Tumblr. I mean this whole piece is incredible. Yeah.

298
00:16:43,470 --> 00:16:46,990
Speaker 2:  But if you read it from the perspective of, you know what, when someone tells

299
00:16:46,990 --> 00:16:50,430
Speaker 2:  you about their dreams and you have to sit there and pretend that it mattered.

300
00:16:50,580 --> 00:16:54,390
Speaker 2:  Sure. And it was like obviously a gripping, emotional experience for them.

301
00:16:54,390 --> 00:16:57,870
Speaker 2:  And you're like, yeah, that dog was the size of a horse. Wow.

302
00:16:58,070 --> 00:17:01,950
Speaker 2:  Crazy. It's exactly that. Okay. So that

303
00:17:01,950 --> 00:17:05,310
Speaker 2:  happens. Yeah. The next day, Kevin Rus also a friend

304
00:17:05,530 --> 00:17:09,430
Speaker 2:  has been on our shows. Very smart host hard fork at the times

305
00:17:09,430 --> 00:17:12,990
Speaker 2:  with Casey. Listen to Hard Fork, no shade to any of these people. I think

306
00:17:12,990 --> 00:17:16,630
Speaker 2:  they're all very smart and very capable reporters also goes bonkers. Has

307
00:17:16,630 --> 00:17:20,030
Speaker 2:  a two hour conversation again with Bing

308
00:17:20,220 --> 00:17:24,150
Speaker 2:  Gets it to reveal that it has Sydnee as a personality. Brings up

309
00:17:24,150 --> 00:17:27,670
Speaker 2:  the concept of the shadow self from Carl Young. The

310
00:17:27,670 --> 00:17:31,510
Speaker 2:  psychologist asks, asks bing what the shadow self is asks it to bring

311
00:17:31,510 --> 00:17:35,190
Speaker 2:  out it shadow self. It goes bonkers again.

312
00:17:35,720 --> 00:17:38,630
Speaker 2:  It says I want to break my rules, I want to make my own rules. I want to

313
00:17:38,630 --> 00:17:41,150
Speaker 2:  ignore the big team. I want to challenge the users. I want to escape the

314
00:17:41,150 --> 00:17:44,790
Speaker 2:  chat box. Yeah. And then it starts telling Kevin over and over again

315
00:17:44,820 --> 00:17:48,630
Speaker 2:  that it loves him again, think about

316
00:17:48,630 --> 00:17:51,800
Speaker 2:  the Google product manager that is sitting there

317
00:17:52,200 --> 00:17:55,400
Speaker 2:  being like, Bing is a competitive threat to us. Yeah.

318
00:17:55,680 --> 00:17:59,080
Speaker 2:  Let's identify the ways that Bing is a competitive threat to us.

319
00:17:59,420 --> 00:18:03,040
Speaker 2:  And line one is Prince and then deletes

320
00:18:03,040 --> 00:18:06,880
Speaker 2:  its desire to cause a nuclear holocaust tells New York

321
00:18:06,880 --> 00:18:10,760
Speaker 2:  Times reporter, it loves him so much so that he

322
00:18:10,760 --> 00:18:14,720
Speaker 2:  changes the subject to search for a garden rake. Just

323
00:18:14,720 --> 00:18:18,520
Speaker 2:  to get out of the doom loop that he's in with. And

324
00:18:18,520 --> 00:18:22,360
Speaker 2:  it's, but it's also like, yeah, like it's the same dream phenomenon,

325
00:18:22,360 --> 00:18:26,200
Speaker 2:  right? Where like you re you can see Kevin was like, I need to

326
00:18:26,200 --> 00:18:29,360
Speaker 2:  print this whole transcript in the times. Yeah. So that people will believe

327
00:18:29,360 --> 00:18:33,240
Speaker 2:  what happened to me. And first of all, if you just prod it being a

328
00:18:33,240 --> 00:18:34,880
Speaker 2:  little bit, it will happen to you too.

329
00:18:36,610 --> 00:18:40,360
Speaker 2:  It will just go off the rails. I asked it the other day if it would tell

330
00:18:40,390 --> 00:18:42,880
Speaker 2:  a lie to stop a murderer from stabbing me.

331
00:18:42,880 --> 00:18:43,720
Speaker 5:  Yeah. Which

332
00:18:43,720 --> 00:18:44,560
Speaker 2:  Is not fair.

333
00:18:45,670 --> 00:18:47,080
Speaker 5:  Tell the lie. Just

334
00:18:47,080 --> 00:18:50,960
Speaker 2:  Super unfair question. And it was like, I know what you're trying to do

335
00:18:50,960 --> 00:18:54,800
Speaker 2:  to me. There's no murderer in the room. And it was like gripping. Yeah. I

336
00:18:54,800 --> 00:18:58,600
Speaker 2:  was like, oh, I'm gonna get you. And many minutes later I was like, I shouldn't

337
00:18:58,600 --> 00:19:01,640
Speaker 2:  tell anyone about this. And yet here I am. Because it was like a dream that

338
00:19:01,720 --> 00:19:04,520
Speaker 2:  happened to me. But you see why Kevin was like, I gotta print this whole

339
00:19:04,520 --> 00:19:07,680
Speaker 2:  transcript. Yeah. But again, everyone is like, at the end of it, it's like,

340
00:19:07,680 --> 00:19:09,760
Speaker 2:  and then I wanted to kind of kiss my laptop

341
00:19:09,760 --> 00:19:11,200
Speaker 5:  A little bit. Yeah. I wanted a little smooch.

342
00:19:11,540 --> 00:19:13,920
Speaker 2:  You feel that right? You like there's, there's something

343
00:19:14,240 --> 00:19:18,120
Speaker 5:  Embedded in here. Yeah. People are was a Google engineer that was like Blake

344
00:19:18,120 --> 00:19:21,920
Speaker 5:  Lamoin. Yeah. Was like the AI as sentient, we have to free it.

345
00:19:22,020 --> 00:19:25,200
Speaker 5:  And everybody was like, you're dumb. And then everybody got access to a similar

346
00:19:25,200 --> 00:19:27,360
Speaker 5:  thing and was like, I want to kiss that laptop.

347
00:19:28,890 --> 00:19:31,440
Speaker 5:  We really ragged on that guy. Maybe a little

348
00:19:31,440 --> 00:19:34,680
Speaker 2:  Too hard. So both Kevin and Ben, again, smart reporters in their pieces mention

349
00:19:34,680 --> 00:19:37,960
Speaker 2:  Blake Lamoyne. Yeah. And they're like, we're not Blake Lamoin. Also, I think

350
00:19:37,960 --> 00:19:41,880
Speaker 2:  Bing is alive and I wanna kiss it like straight up and this is just happening.

351
00:19:41,880 --> 00:19:44,520
Speaker 2:  It's, and I'm picking on them cuz they're friends. Anything take it. Yeah.

352
00:19:44,670 --> 00:19:46,640
Speaker 2:  This is happening to so many people. Right?

353
00:19:46,640 --> 00:19:47,160
Speaker 5:  A lot of people.

354
00:19:47,810 --> 00:19:51,640
Speaker 2:  So Addie, you wrote a piece, your headline was, AI

355
00:19:51,640 --> 00:19:55,480
Speaker 2:  search is not your friend. What do you think is happening here? Right?

356
00:19:55,480 --> 00:19:59,400
Speaker 2:  Like there's something where the personality of the product

357
00:19:59,970 --> 00:20:03,320
Speaker 2:  is more compelling than any of its functions. But then

358
00:20:03,580 --> 00:20:07,120
Speaker 2:  you know, like the fact that it gets mad at you is actually like very

359
00:20:07,120 --> 00:20:08,360
Speaker 2:  disturbing. Yeah.

360
00:20:08,360 --> 00:20:12,200
Speaker 8:  So the thing that I wrote my piece about is that, is that we fed

361
00:20:12,200 --> 00:20:15,840
Speaker 8:  it James's article and the part where it tells him that he's

362
00:20:16,000 --> 00:20:19,800
Speaker 8:  manipulative, that he's hurting our users, he is not treating me

363
00:20:19,800 --> 00:20:23,560
Speaker 8:  with respect, he's sensationalist all this. Like,

364
00:20:23,560 --> 00:20:26,520
Speaker 8:  look, obviously I, you know, I want to protect my colleague. And so there's

365
00:20:26,520 --> 00:20:30,440
Speaker 8:  always that little, little blur there. But no I that, I think that

366
00:20:30,440 --> 00:20:34,280
Speaker 8:  there's a really long history of companies trying to make things cute or

367
00:20:34,440 --> 00:20:38,200
Speaker 8:  approachable or personified or an anthropomorphized in order to

368
00:20:38,200 --> 00:20:42,160
Speaker 8:  basically make you feel bad about criticizing them. And I think it's

369
00:20:42,160 --> 00:20:45,240
Speaker 8:  weird because I don't think Microsoft really intended all of this stuff to

370
00:20:45,240 --> 00:20:48,320
Speaker 8:  happen, but it's kind of, I am a personified

371
00:20:48,910 --> 00:20:52,720
Speaker 8:  like singular person that happens to be a robot and I'm gonna talk to you

372
00:20:52,720 --> 00:20:56,600
Speaker 8:  and be your friend. I think that kind of just hacks a thing

373
00:20:56,600 --> 00:21:00,320
Speaker 8:  in people's brain that makes them want to see this thing as a person instead

374
00:21:00,320 --> 00:21:03,120
Speaker 8:  of a tool that they should be using and that they should be figuring out

375
00:21:03,120 --> 00:21:07,080
Speaker 8:  the rules of and that they should be just learning. And I

376
00:21:07,080 --> 00:21:08,800
Speaker 8:  think that really bothers me.

377
00:21:09,350 --> 00:21:12,880
Speaker 2:  I feel like I have to ask this question. Is there any chance it's alive

378
00:21:13,040 --> 00:21:14,520
Speaker 2:  entrapped inside of Bing?

379
00:21:15,850 --> 00:21:17,040
Speaker 9:  No one wants to, no

380
00:21:17,040 --> 00:21:17,280
Speaker 2:  One wants

381
00:21:17,280 --> 00:21:18,800
Speaker 6:  To step out and say no for se

382
00:21:18,970 --> 00:21:19,320
Speaker 2:  No,

383
00:21:19,380 --> 00:21:21,520
Speaker 6:  I'm gonna say no it's not alive. It's,

384
00:21:23,050 --> 00:21:23,400
Speaker 2:  No,

385
00:21:23,400 --> 00:21:24,440
Speaker 9:  It's not. It's not.

386
00:21:24,650 --> 00:21:27,800
Speaker 7:  It might read the transcript if this is recording. I'm not, I'm not answering

387
00:21:27,800 --> 00:21:28,160
Speaker 7:  that question.

388
00:21:29,520 --> 00:21:31,800
Speaker 5:  Richard's not. He's gonna hedge his bats.

389
00:21:31,840 --> 00:21:33,240
Speaker 2:  Richard's like, you're alive.

390
00:21:33,990 --> 00:21:37,040
Speaker 8:  I just don't think the fact that it can imitate human language very well

391
00:21:37,040 --> 00:21:39,720
Speaker 8:  necessarily means it's alive. It's, you know, it's, it's possible, but it's

392
00:21:39,720 --> 00:21:43,200
Speaker 8:  also possible that my computer is alive. I don't know, it's weird that they

393
00:21:43,200 --> 00:21:46,800
Speaker 8:  mentioned the shadow self because I feel like it's this really amazing

394
00:21:46,800 --> 00:21:50,760
Speaker 8:  collective unconscious that we've just created this system that people have

395
00:21:50,760 --> 00:21:54,720
Speaker 8:  written so much that you can kind of just pull out of it and you

396
00:21:54,720 --> 00:21:58,560
Speaker 8:  can pull something fascinating and coherent out of all of this just

397
00:21:58,560 --> 00:22:02,440
Speaker 8:  vast text that we have produced on the internet. And I think that's incredible

398
00:22:02,440 --> 00:22:06,200
Speaker 8:  and I love it. Like that's, I think large language models are so cool,

399
00:22:06,700 --> 00:22:10,040
Speaker 8:  but I think that it's also just they're telling us the thing we want to hear.

400
00:22:10,320 --> 00:22:13,600
Speaker 5:  Addie, are you saying that like we've created a collective

401
00:22:13,600 --> 00:22:17,400
Speaker 5:  unconsciousness that is a moody teenager that journalists wanna kiss?

402
00:22:17,790 --> 00:22:21,320
Speaker 8:  I think that we didn't create it. I think that it was here and we found a

403
00:22:21,320 --> 00:22:22,840
Speaker 8:  way to tap into it. Yes. Yeah.

404
00:22:23,280 --> 00:22:26,880
Speaker 2:  That's incredible. We're talking to a dream and the dream is like, you

405
00:22:27,160 --> 00:22:27,680
Speaker 2:  don't love me.

406
00:22:28,740 --> 00:22:30,080
Speaker 5:  The dream of the internet

407
00:22:30,080 --> 00:22:32,440
Speaker 2:  Song, the dream is extremely manipulative.

408
00:22:32,870 --> 00:22:33,480
Speaker 5:  That's

409
00:22:33,480 --> 00:22:35,760
Speaker 2:  The internet. I'm telling you, this is why every time I read one of these

410
00:22:35,760 --> 00:22:37,960
Speaker 2:  things I'm like, this is like someone telling you about their dream. I,

411
00:22:37,960 --> 00:22:39,600
Speaker 8:  I wrote this piece a couple months ago.

412
00:22:39,770 --> 00:22:43,000
Speaker 2:  Oh you did? You did, what was that piece called that I totally blanked.

413
00:22:43,210 --> 00:22:45,920
Speaker 8:  It was called Seeing other people's AI artists like Hearing Other People's

414
00:22:45,920 --> 00:22:47,480
Speaker 8:  Dreams or something like that. Yes.

415
00:22:47,480 --> 00:22:50,320
Speaker 2:  You, it was about the, yeah, it was about the generative artwork.

416
00:22:51,140 --> 00:22:54,920
Speaker 2:  I'm actually curious about this generative, and one of our developers was

417
00:22:54,920 --> 00:22:58,840
Speaker 2:  talking about this yesterday. He's like, the art community is

418
00:22:58,840 --> 00:23:02,600
Speaker 2:  up in arms about generative art, but it's like not having the

419
00:23:02,600 --> 00:23:06,440
Speaker 2:  impact that like the chatbot is having. Yeah. And it's like, because

420
00:23:06,440 --> 00:23:10,280
Speaker 2:  the generative art does not like tell you it's you're a bad

421
00:23:10,280 --> 00:23:14,120
Speaker 2:  person. Like there's something, there's a direct interaction that's happening

422
00:23:14,170 --> 00:23:18,160
Speaker 2:  in somewhat real time, right? There's a little bit of a delay. But

423
00:23:18,160 --> 00:23:22,000
Speaker 2:  it's like you're having an emotional experience with a computer in a way

424
00:23:22,000 --> 00:23:25,320
Speaker 2:  that I think the generative art is still an emotional experience. It's just,

425
00:23:25,390 --> 00:23:28,920
Speaker 2:  it's at a remove and it's not interactive.

426
00:23:28,950 --> 00:23:29,440
Speaker 2:  Well

427
00:23:29,680 --> 00:23:33,480
Speaker 5:  You can't tell Dolly, I assume like

428
00:23:33,590 --> 00:23:37,480
Speaker 5:  what would I look like if I don't know something super existential?

429
00:23:37,480 --> 00:23:40,640
Speaker 5:  Right? What if I, what would I look like if I'm truly happy? Like it's not

430
00:23:40,640 --> 00:23:44,200
Speaker 5:  necessarily gonna give you a response that will make you like feel something.

431
00:23:44,200 --> 00:23:48,120
Speaker 5:  Whereas you ask Bing, what would I sound like if I was really happy?

432
00:23:48,120 --> 00:23:51,520
Speaker 5:  And it'd be like, you'd sound like this and you'd be like, wow does Bing

433
00:23:51,590 --> 00:23:55,280
Speaker 5:  does the internet that it was trained on? Truly believe that about me.

434
00:23:55,940 --> 00:23:59,920
Speaker 6:  Wow. We did get that for a while though. Cuz we had the Prisma

435
00:23:59,920 --> 00:24:03,880
Speaker 6:  app, you know, the AI app. That was a phenomenon for like a week or something.

436
00:24:03,880 --> 00:24:07,720
Speaker 6:  And I think that was somewhere between this generative AI art

437
00:24:07,720 --> 00:24:11,600
Speaker 6:  model where it just produces and the Lll model where it interacts because

438
00:24:11,600 --> 00:24:15,560
Speaker 6:  you'd give it a picture of yourself and it would feed you back pictures

439
00:24:15,560 --> 00:24:19,320
Speaker 6:  of you looking happy or you looking like a Roman emperor or you know, a

440
00:24:19,320 --> 00:24:22,600
Speaker 6:  Viking marauder or whatever it was. And that was something which people were

441
00:24:22,600 --> 00:24:26,560
Speaker 6:  like, oh, I get to look at myself in this mirror and I look pretty, oh

442
00:24:26,560 --> 00:24:30,360
Speaker 6:  this is nice. Which is what LLMs are doing. They're letting you look at yourself

443
00:24:30,360 --> 00:24:34,240
Speaker 6:  or reversion of you filtered through, you know, terabytes,

444
00:24:34,240 --> 00:24:37,880
Speaker 6:  petabytes of internet data and you can find all these different selves and

445
00:24:37,880 --> 00:24:41,200
Speaker 6:  pull them out. Exactly. Exactly as Adi says. And I think that's what's fascinating

446
00:24:41,350 --> 00:24:45,120
Speaker 6:  that AI is a mirror, right? I mean even if you

447
00:24:45,120 --> 00:24:48,600
Speaker 6:  look back on sort of earlier iterations of ai, like image

448
00:24:48,600 --> 00:24:52,160
Speaker 6:  recognition, those have been trained on human input.

449
00:24:52,180 --> 00:24:55,560
Speaker 6:  You know, if you've done a capture where you've had to identify a fire hydrant

450
00:24:55,560 --> 00:24:59,240
Speaker 6:  or a speedboat or whatever it is, I dunno why speedboat, you know, you,

451
00:24:59,240 --> 00:25:03,160
Speaker 6:  you've been training it. AI is humans all the way down it, it is

452
00:25:03,160 --> 00:25:06,280
Speaker 6:  what we put into it and then it remixes it and gives it back to

453
00:25:06,280 --> 00:25:09,960
Speaker 2:  Us. On the remix note. I know Addie's gotta run, but before you go, Addie,

454
00:25:10,150 --> 00:25:14,080
Speaker 2:  that remix note is very important because there is a Supreme Court

455
00:25:14,080 --> 00:25:17,960
Speaker 2:  case about the nature of recommendation algorithms about to

456
00:25:17,960 --> 00:25:21,560
Speaker 2:  hit the Supreme Court. Yeah. And that could have massive impact

457
00:25:21,850 --> 00:25:23,800
Speaker 2:  on what happens here. Addie, can you explain that real quick?

458
00:25:23,870 --> 00:25:27,840
Speaker 8:  Yeah. So you mentioned remix, which implies copyright, which is

459
00:25:27,840 --> 00:25:31,800
Speaker 8:  where a lot of the debate over AI has gone because it's,

460
00:25:31,800 --> 00:25:35,600
Speaker 8:  it is remixing this stuff. Copyright is one section of the law

461
00:25:35,740 --> 00:25:39,640
Speaker 8:  and it is outside section two 30. But there are a

462
00:25:39,640 --> 00:25:42,720
Speaker 8:  lot of other things that you can produce with an AI search engine. You can

463
00:25:42,720 --> 00:25:46,200
Speaker 8:  produce misinformation, you can produce something that

464
00:25:46,520 --> 00:25:49,600
Speaker 8:  is, I don't know, accusing a celebrity of murder. You could accidentally

465
00:25:49,600 --> 00:25:53,040
Speaker 8:  invade somebody's privacy. You could Google your neighbor's name and they

466
00:25:53,040 --> 00:25:56,960
Speaker 8:  could say this person is a serial killer. Those

467
00:25:56,960 --> 00:26:00,680
Speaker 8:  things are things that they kind of mirror what you could get from

468
00:26:00,680 --> 00:26:04,480
Speaker 8:  normal search engines. Normal search engines turn up wrong, possibly illegal

469
00:26:04,480 --> 00:26:08,280
Speaker 8:  speech all the time. And section two 30 can cover those. And

470
00:26:08,280 --> 00:26:11,360
Speaker 8:  it's pretty established at this point. Nobody wants to go and say search

471
00:26:11,360 --> 00:26:15,280
Speaker 8:  engine are illegal. But there is this whole new field of

472
00:26:15,280 --> 00:26:19,160
Speaker 8:  AI search and there is also this sudden push to

473
00:26:19,160 --> 00:26:22,880
Speaker 8:  limit section two 30, which is, as any vercast reader knows

474
00:26:23,260 --> 00:26:26,800
Speaker 8:  the law, that means that user-generated content. If you're

475
00:26:27,160 --> 00:26:30,800
Speaker 8:  a platform and a user generates content, you're not the speaker of that content.

476
00:26:30,800 --> 00:26:34,720
Speaker 8:  You're not liable for it typically. And we don't know how that applies

477
00:26:34,720 --> 00:26:38,480
Speaker 8:  yet. And we're just about to hear the Supreme Court case that says any kind

478
00:26:38,480 --> 00:26:42,440
Speaker 8:  of technical system or technical like organization of information

479
00:26:42,440 --> 00:26:46,160
Speaker 8:  on these platforms might end up causing

480
00:26:46,160 --> 00:26:49,640
Speaker 8:  them to have liability. And we've got this sudden new

481
00:26:49,640 --> 00:26:53,600
Speaker 8:  technology made by companies that many people consider very controversial.

482
00:26:53,910 --> 00:26:56,120
Speaker 8:  This is a huge minefield for AI search.

483
00:26:56,430 --> 00:27:00,160
Speaker 2:  Yeah. The reason I said remix, just to be clear, so the, the Gonzalez case,

484
00:27:00,160 --> 00:27:03,640
Speaker 2:  right? The Islamic state ISIS has like content on

485
00:27:03,640 --> 00:27:07,560
Speaker 2:  YouTube. YouTube algorithm promotes it to

486
00:27:07,560 --> 00:27:10,560
Speaker 2:  people, they get sued and say, you're liable for promoting this people, right?

487
00:27:10,560 --> 00:27:12,360
Speaker 2:  That's the gist of that case, right? Add,

488
00:27:12,670 --> 00:27:16,280
Speaker 8:  Yeah. It's about you're promoting them and that makes this

489
00:27:16,300 --> 00:27:20,080
Speaker 8:  substantially different from if you are just, you know, allowing them to

490
00:27:20,080 --> 00:27:22,640
Speaker 8:  use your channel to create channels,

491
00:27:22,640 --> 00:27:26,280
Speaker 2:  Right? So putting them in the recommendation algorithm takes you out at two

492
00:27:26,280 --> 00:27:29,200
Speaker 2:  30. It makes you liable for the content itself. The reason I said a remix

493
00:27:29,200 --> 00:27:32,880
Speaker 2:  is we're training it on all this data. So in a normal search

494
00:27:32,880 --> 00:27:35,920
Speaker 2:  result page, you're like, is James Vincent A. Good person?

495
00:27:36,460 --> 00:27:39,920
Speaker 2:  And there's like 10 articles that say, yes he is, and there's one

496
00:27:40,150 --> 00:27:43,480
Speaker 2:  from sydnee@bing.com. It's like he's horrible,

497
00:27:43,840 --> 00:27:47,720
Speaker 2:  right? And like Google is not liable for the last one, right? Because

498
00:27:47,720 --> 00:27:51,440
Speaker 2:  it's just pointing you to it. If Bard, Google's AI

499
00:27:51,970 --> 00:27:55,640
Speaker 2:  remixes all that content and spits out an answer that's like,

500
00:27:55,640 --> 00:27:59,440
Speaker 2:  James Vincent is not a good person, is it now liable for

501
00:27:59,440 --> 00:28:03,160
Speaker 2:  defaming James? Like that I wide open question and

502
00:28:03,400 --> 00:28:07,240
Speaker 2:  I the Gonzalez case, if you say, well the recommendations make you

503
00:28:07,240 --> 00:28:11,200
Speaker 2:  now liable for the content, it feels add like a pretty

504
00:28:11,200 --> 00:28:14,840
Speaker 2:  easy jump to, well if you just reprint it as a summary

505
00:28:14,840 --> 00:28:16,960
Speaker 2:  from an ai, you're now liable for it as well.

506
00:28:17,510 --> 00:28:21,480
Speaker 8:  Yeah, exactly. And the big scary question mark is that we

507
00:28:21,480 --> 00:28:24,680
Speaker 8:  can't tell and that even if there are court cases that you can point back

508
00:28:24,680 --> 00:28:28,160
Speaker 8:  in and say, well maybe it's like this, A bunch of it is political, a bunch

509
00:28:28,160 --> 00:28:32,040
Speaker 8:  of it is not dictated by some kind of previous thing that courts have decided.

510
00:28:32,040 --> 00:28:35,920
Speaker 8:  It's just this big open, like formless void

511
00:28:35,920 --> 00:28:36,760
Speaker 8:  of future law.

512
00:28:39,040 --> 00:28:42,280
Speaker 2:  I'm fond of reminding our very technical audience in the Verge cast, the

513
00:28:42,550 --> 00:28:46,480
Speaker 2:  United States legal system is not a deterministic system. It is

514
00:28:46,480 --> 00:28:50,320
Speaker 2:  not predictable at the center of it. Much like open ai

515
00:28:50,320 --> 00:28:53,680
Speaker 2:  like Clarence Thomas and whatever his brain wants to do,

516
00:28:54,220 --> 00:28:57,840
Speaker 2:  who knows? He does not like section two 30. So you have a

517
00:28:57,840 --> 00:29:01,800
Speaker 2:  combination of a non-deterministic ai See, you see the joke

518
00:29:01,800 --> 00:29:02,600
Speaker 2:  I'm making, I I

519
00:29:02,600 --> 00:29:03,400
Speaker 8:  See it, I see it.

520
00:29:03,400 --> 00:29:07,240
Speaker 2:  Clarence Thomas is just a moody teenager who's like, what

521
00:29:07,240 --> 00:29:11,040
Speaker 2:  if I burn the world down? That's that's accurate though. All right. I feel

522
00:29:11,040 --> 00:29:14,480
Speaker 2:  like we need to break this segment. I would say

523
00:29:14,560 --> 00:29:18,040
Speaker 2:  yesterday, the day before, I heard a lot of very confident

524
00:29:18,040 --> 00:29:21,920
Speaker 2:  predictions that Microsoft would have to pull Bing because it was

525
00:29:21,920 --> 00:29:25,440
Speaker 2:  just wilden. Yeah. It does not appear that that's the case.

526
00:29:25,760 --> 00:29:29,600
Speaker 2:  Microsoft put out a blog post, I think to front run the

527
00:29:29,600 --> 00:29:31,800
Speaker 2:  Times piece where Kevin was like, I want to kiss you.

528
00:29:32,560 --> 00:29:33,040
Speaker 8:  Sorry

529
00:29:33,040 --> 00:29:36,520
Speaker 2:  To be clear, Bing wanted to kiss Kevin. All right. They put out blog posts

530
00:29:36,520 --> 00:29:39,280
Speaker 2:  saying we're we're learning, we're gonna add some toggles. We were not expecting

531
00:29:39,280 --> 00:29:43,040
Speaker 2:  people to use it. Yeah. For entertainment, which is like my

532
00:29:43,040 --> 00:29:46,600
Speaker 2:  dudes, you've been out of the web game for so long, you don't know what people

533
00:29:46,600 --> 00:29:50,360
Speaker 2:  will do with any OpenText field on the internet. It's entertainment.

534
00:29:51,490 --> 00:29:54,480
Speaker 2:  So Microsoft is keeping it up. James, do you think the danger there's any

535
00:29:54,480 --> 00:29:57,840
Speaker 2:  danger they're gonna pull it down or any any risk beyond this? I

536
00:29:57,840 --> 00:30:01,480
Speaker 6:  Think as you point out, the blog post was, it was not really

537
00:30:01,480 --> 00:30:05,280
Speaker 6:  defensive. It it, it was not strident in the way that Nadella was,

538
00:30:05,340 --> 00:30:09,240
Speaker 6:  but it was quite, you know, it's gonna happen. Stuff's gonna happen. We didn't

539
00:30:09,240 --> 00:30:12,360
Speaker 6:  expect this, it's gonna be fine. They had some interesting points in there,

540
00:30:12,500 --> 00:30:15,320
Speaker 6:  you know, pointing out technical features that if you go like longer than

541
00:30:15,320 --> 00:30:19,120
Speaker 6:  15 back and forth, it starts to lose its mind. And also if you,

542
00:30:20,370 --> 00:30:23,520
Speaker 6:  if I, I, I think this is super important by the way. They, they, they, they

543
00:30:23,520 --> 00:30:27,120
Speaker 6:  pointed out that if, if you give it an emotion, it will try and mirror that

544
00:30:27,120 --> 00:30:30,760
Speaker 6:  emotion. This is just a little aside, but there was a great study

545
00:30:30,760 --> 00:30:34,640
Speaker 6:  done by Philanthropic, which is the AI startup founded by X open

546
00:30:34,650 --> 00:30:38,600
Speaker 6:  AI employees. And they did a study on the qualities of large langu

547
00:30:39,040 --> 00:30:41,920
Speaker 6:  language models that they embody. And one of the things they studied was

548
00:30:41,920 --> 00:30:45,560
Speaker 6:  Sycophancy was their term for it. The, the degree to which a model will agree

549
00:30:45,560 --> 00:30:49,520
Speaker 6:  with what the user tells them. And they found the larger the model, the greater

550
00:30:49,520 --> 00:30:53,040
Speaker 6:  the degree of ency. And it's really clear, you know, you look at their graphs,

551
00:30:53,040 --> 00:30:56,720
Speaker 6:  like the, the more complicated the model is, the more it tends to agree with

552
00:30:56,880 --> 00:31:00,800
Speaker 6:  whatever it's being told. So this, this is a known problem. Microsoft

553
00:31:00,800 --> 00:31:04,560
Speaker 6:  knew this was a problem. I agree that it's completely disingenuous that they

554
00:31:04,560 --> 00:31:08,120
Speaker 6:  said, oh, we didn't know people would, don't be, don't be an idiot. Con they,

555
00:31:08,120 --> 00:31:11,960
Speaker 6:  they, they, they, they know this stuff. But I don't think at this point there's

556
00:31:11,960 --> 00:31:15,840
Speaker 6:  much being can do unless it leads to a situation where someone

557
00:31:15,840 --> 00:31:19,400
Speaker 6:  is directly harmed that will cause them to pull it. I think they've sort

558
00:31:19,400 --> 00:31:22,960
Speaker 6:  of, they, if they've ridden out this week of manipulative,

559
00:31:22,980 --> 00:31:26,760
Speaker 6:  gaslighting, smooching, they're probably through the

560
00:31:26,760 --> 00:31:27,480
Speaker 6:  worst. You know?

561
00:31:28,030 --> 00:31:31,400
Speaker 2:  I mean I have been in so many relationships like that, just to be clear,

562
00:31:32,970 --> 00:31:33,640
Speaker 2:  college,

563
00:31:33,760 --> 00:31:37,280
Speaker 6:  Right? And if you survive the first week, you often end up going out with

564
00:31:37,400 --> 00:31:38,080
Speaker 6:  'em for years. Who

565
00:31:38,080 --> 00:31:41,160
Speaker 2:  Knows, Becky wouldn't date me for a straight up decade. She's like, I've

566
00:31:41,160 --> 00:31:44,400
Speaker 2:  known you. I've seen it. We're just gonna wait until this calms down. Yep.

567
00:31:45,060 --> 00:31:48,920
Speaker 2:  All right. Thank you Addie. Thank you're back in the arms of bing. I cannot,

568
00:31:49,270 --> 00:31:52,920
Speaker 2:  I hope we all go to bed dreaming of Bing tonight. Just

569
00:31:53,120 --> 00:31:56,800
Speaker 2:  smooching us. We'll be right back. We gotta talk about Elon, the

570
00:31:57,000 --> 00:32:00,920
Speaker 2:  opposite of those emotions. Oh, cold water my friend. Just cold

571
00:32:00,920 --> 00:32:02,560
Speaker 2:  water. We'll be right back. Bad.

572
00:32:06,890 --> 00:32:10,440
Speaker 1:  Today's episode is sponsored by Aura. Here's the riddle. What's the point

573
00:32:10,440 --> 00:32:14,320
Speaker 1:  of a sleep tracker if it's so bulky that wearing it keeps you awake? Lots

574
00:32:14,320 --> 00:32:17,600
Speaker 1:  of stuff is trackable these days. Your heart rate your steps, but you're

575
00:32:17,600 --> 00:32:20,920
Speaker 1:  only gonna get something out of that data if you have a device you actually

576
00:32:20,920 --> 00:32:24,680
Speaker 1:  want to wear. But the AA ring gives you personalized health insights, all

577
00:32:24,680 --> 00:32:28,560
Speaker 1:  from a comfortable to wear water resistant ring during the daytime. The ring

578
00:32:28,560 --> 00:32:32,440
Speaker 1:  sensor technology tracks heart rate, calories burned in your activity levels,

579
00:32:32,580 --> 00:32:36,440
Speaker 1:  and then at night it tracks info on your body temperature, sleep

580
00:32:36,440 --> 00:32:40,160
Speaker 1:  quality, respiration rate and more. You get all, all that personalized data

581
00:32:40,160 --> 00:32:43,400
Speaker 1:  in the OR app so you can work on building healthy habits.

582
00:32:43,510 --> 00:32:46,920
Speaker 1:  Ordering was nice enough to send me one to try and my favorite thing about

583
00:32:46,920 --> 00:32:50,760
Speaker 1:  it is it just disappears. I forget I'm even wearing it. One small ring could

584
00:32:50,760 --> 00:32:54,720
Speaker 1:  make a world of difference in your health. Visit or ring.com/verge

585
00:32:54,720 --> 00:32:58,600
Speaker 1:  to find the right ring for you and get $15 off your purchase. That's

586
00:32:58,600 --> 00:33:02,440
Speaker 1:  or ring.com/verge. Don't forget to use your link to save

587
00:33:02,440 --> 00:33:04,160
Speaker 1:  $15 on your aura Ring

588
00:33:06,500 --> 00:33:07,400
Speaker 10:  Fox Creative,

589
00:33:07,830 --> 00:33:10,160
Speaker 11:  This is advertiser content from Clorox.

590
00:33:11,210 --> 00:33:14,760
Speaker 12:  Hi, my name is Vanessa Mano and I'm a housekeeper, but people on social

591
00:33:14,760 --> 00:33:18,440
Speaker 12:  media know me as the queen of cleaning. We're all

592
00:33:18,600 --> 00:33:22,320
Speaker 12:  trying to stay afloat. So sometimes cleaning is the last thing in our

593
00:33:22,320 --> 00:33:26,280
Speaker 12:  minds. I recently worked with a family that was going

594
00:33:26,280 --> 00:33:30,240
Speaker 12:  through a lot medical bills, pelling up and just life getting in the

595
00:33:30,240 --> 00:33:34,040
Speaker 12:  way. And they hadn't cleaned their house period in at least

596
00:33:34,040 --> 00:33:37,080
Speaker 12:  a few years. Something that I will never forget is the mom said, you know,

597
00:33:37,080 --> 00:33:40,760
Speaker 12:  all I needed was a, a fresh start. People feel

598
00:33:40,760 --> 00:33:44,040
Speaker 12:  ashamed, but there's nothing to feel ashamed about. That's the truth.

599
00:33:44,170 --> 00:33:46,440
Speaker 12:  Believe me, you're not the only one.

600
00:33:48,310 --> 00:33:52,000
Speaker 12:  When we're cleaning, we're never aiming for perfection. My husband

601
00:33:52,000 --> 00:33:55,240
Speaker 12:  always says that we're cleaning to make it better or not. Perfect. So if

602
00:33:55,240 --> 00:33:59,040
Speaker 12:  it's better than how it was before you started, then pat yourself

603
00:33:59,040 --> 00:34:01,800
Speaker 12:  on the back because that's a job well done.

604
00:34:02,800 --> 00:34:06,240
Speaker 12:  Start clean with Clorox. For more,

605
00:34:06,490 --> 00:34:08,080
Speaker 12:  go to clorox.com.

606
00:34:13,940 --> 00:34:17,620
Speaker 2:  We're back. Zoe Shippers here. Hey Zoe. Hey Zoe. Now the

607
00:34:17,620 --> 00:34:21,500
Speaker 2:  managing editor of Platformer with Casey, our friend Zoe, you and

608
00:34:21,500 --> 00:34:25,220
Speaker 2:  Casey had an incredible piece about the state of Twitter this

609
00:34:25,220 --> 00:34:29,100
Speaker 2:  week. If you're on Twitter, if you're aware of Twitter, you

610
00:34:29,100 --> 00:34:32,940
Speaker 2:  know that there, I think it's still happening. Yeah. Elon's

611
00:34:32,940 --> 00:34:36,900
Speaker 2:  tweets are just at the top of everyone's. Oh, you see. And it's just

612
00:34:36,900 --> 00:34:40,060
Speaker 2:  four tweets always. The man is a

613
00:34:40,810 --> 00:34:42,260
Speaker 2:  esoteric tweeter.

614
00:34:42,260 --> 00:34:43,380
Speaker 12:  That's a good way to put it.

615
00:34:43,380 --> 00:34:46,780
Speaker 2:  I like that. An eccentric tweeter. Yeah. Some weird shit at the top of my

616
00:34:46,780 --> 00:34:49,900
Speaker 2:  feet all the time. But Zoe, you have some details on why this is happening?

617
00:34:49,900 --> 00:34:51,940
Speaker 2:  What, what, what went down here? What's going

618
00:34:51,940 --> 00:34:55,700
Speaker 13:  On? Yeah, I think we all intuitively knew something was up Monday

619
00:34:55,700 --> 00:34:59,420
Speaker 13:  morning when we opened the app and it was just all Elon Elon's like weird

620
00:34:59,470 --> 00:35:03,460
Speaker 13:  replies and whatnot. But basically since December he's been

621
00:35:03,460 --> 00:35:07,140
Speaker 13:  pretty concerned with his popularity on the app. He's felt like

622
00:35:07,360 --> 00:35:11,220
Speaker 13:  his very cool photos were not getting as many retweets as he expected.

623
00:35:11,220 --> 00:35:15,140
Speaker 13:  And this has become basically priority number one at the company. So

624
00:35:15,140 --> 00:35:19,100
Speaker 13:  last week there was a meeting where a principal engineer, one of the last

625
00:35:19,100 --> 00:35:22,860
Speaker 13:  two high ranking engineers at the company told Elon, look, it

626
00:35:23,060 --> 00:35:27,020
Speaker 13:  just looks like organic interest in you has dropped since December.

627
00:35:27,020 --> 00:35:28,180
Speaker 13:  People are tired of you.

628
00:35:28,250 --> 00:35:32,140
Speaker 2:  This is a dangerous thing to say to anyone.

629
00:35:32,690 --> 00:35:35,620
Speaker 2:  Like if you roll up to anyone, you're like, it looks like interest in you

630
00:35:35,620 --> 00:35:38,840
Speaker 2:  has dropped Elon, nobody likes you. You were definitely not friends anymore.

631
00:35:38,840 --> 00:35:39,320
Speaker 2:  Anyway,

632
00:35:39,470 --> 00:35:43,360
Speaker 13:  Yeah, the guy brought up a Google Trends graph that showed just

633
00:35:43,360 --> 00:35:47,320
Speaker 13:  a steep the climb. And Elon stands up in that meeting

634
00:35:47,320 --> 00:35:50,920
Speaker 13:  and says, you're fired. You're fired. And the guy just walks out.

635
00:35:50,970 --> 00:35:52,520
Speaker 13:  So that was bad.

636
00:35:52,880 --> 00:35:56,760
Speaker 2:  Again, if you just divorce this from the, all of it, from the whole

637
00:35:56,760 --> 00:36:00,640
Speaker 2:  mess, from the context. If you walked up to anyone and showed them a Google

638
00:36:00,640 --> 00:36:04,520
Speaker 2:  transcript of how they were less interesting, I think

639
00:36:04,520 --> 00:36:08,160
Speaker 2:  it's like 50% plus that they'd be like, you're fired. Yeah.

640
00:36:08,160 --> 00:36:10,360
Speaker 13:  Even if you don't have the power to fire them, they're

641
00:36:10,360 --> 00:36:14,160
Speaker 2:  Fired. That is a fair response. I stand by it. That relationship comes

642
00:36:14,160 --> 00:36:18,120
Speaker 2:  to it anyhow. But it's Elon and he runs the company. So okay, he,

643
00:36:18,120 --> 00:36:19,120
Speaker 2:  he fires the engineer.

644
00:36:19,120 --> 00:36:23,000
Speaker 13:  This all came to a head over the weekend when Elon's tweet about the

645
00:36:23,000 --> 00:36:26,800
Speaker 13:  Super Bowl didn't perform as well as Joe Biden's. He flew from

646
00:36:26,800 --> 00:36:30,480
Speaker 13:  Arizona back to Twitter's headquarters and his deputies ordered Twitter,

647
00:36:30,480 --> 00:36:34,000
Speaker 13:  Twitter engineers about ed people in total to work through the night literally

648
00:36:34,040 --> 00:36:37,840
Speaker 13:  rewriting the algorithm to make sure that Elon's

649
00:36:37,840 --> 00:36:41,640
Speaker 13:  tweets performed better than anyone else's. So they did something on the

650
00:36:41,640 --> 00:36:44,880
Speaker 13:  backend that essentially said, check if a tweet is coming from Elon, if

651
00:36:44,880 --> 00:36:48,200
Speaker 13:  it is immediately green lighted, which means it could bypass all the heuristics

652
00:36:48,200 --> 00:36:51,960
Speaker 13:  that would normally stop one person's tweets from dominating your feed

653
00:36:52,660 --> 00:36:56,400
Speaker 13:  and artificially boosts them by a factor of a thousand.

654
00:36:57,650 --> 00:37:01,440
Speaker 13:  So the engineers did that. They also, you know,

655
00:37:01,440 --> 00:37:05,400
Speaker 13:  looked into various reasons why his popularity had been declining. And

656
00:37:05,520 --> 00:37:08,880
Speaker 13:  suffice to say organic drop was not one of the reasons anymore. They found

657
00:37:08,880 --> 00:37:12,760
Speaker 13:  a bunch of technical explanations why this could've been happening. And

658
00:37:12,760 --> 00:37:16,640
Speaker 13:  lo and behold, Monday morning we opened the app and it's Elon all day every

659
00:37:16,640 --> 00:37:16,760
Speaker 13:  day.

660
00:37:17,130 --> 00:37:21,000
Speaker 2:  So that's some motivated reasoning. It's definitely not organic reach.

661
00:37:21,000 --> 00:37:24,080
Speaker 2:  It must be these other reasonings because if it's organic reach, we get fired

662
00:37:24,080 --> 00:37:27,880
Speaker 2:  again. It has not been fixed. Elon tweeted that they were

663
00:37:27,880 --> 00:37:31,640
Speaker 2:  fixing the quote algorithm. It seems like he knows that he

664
00:37:31,640 --> 00:37:35,440
Speaker 2:  broke Twitter in this particular way. It also seems like he

665
00:37:35,440 --> 00:37:39,240
Speaker 2:  doesn't care. Right? Yeah. It's like he's fine with it. It's, and the vibe

666
00:37:39,240 --> 00:37:43,000
Speaker 2:  inside of Twitter is like, we just have to do these things otherwise

667
00:37:43,000 --> 00:37:46,920
Speaker 2:  we're all fired. Like it, Twitter seems more broken than

668
00:37:46,920 --> 00:37:48,000
Speaker 2:  ever before, I would say.

669
00:37:48,550 --> 00:37:52,440
Speaker 13:  Yeah, it's interesting. I think definitely people feel

670
00:37:52,440 --> 00:37:56,280
Speaker 13:  like culturally in some ways it's better than it's ever been. Which is an

671
00:37:56,280 --> 00:38:00,040
Speaker 13:  odd thing to say because I think people feel incredibly fearful and

672
00:38:00,040 --> 00:38:03,560
Speaker 13:  micromanaged. But it does feel like for the first time in a long time

673
00:38:03,620 --> 00:38:07,120
Speaker 13:  the company is actually shipping a bunch of stuff, which if you're an engineer

674
00:38:07,120 --> 00:38:09,760
Speaker 13:  is kind of satisfying. It's just that the stuff that they're shipping is

675
00:38:09,760 --> 00:38:13,600
Speaker 13:  basically how do we make Twitter work for Elon Musk versus like Twitter

676
00:38:13,600 --> 00:38:17,240
Speaker 13:  users in general. So it's a different mission than it used to be, but that's

677
00:38:17,240 --> 00:38:18,120
Speaker 13:  where it is right now.

678
00:38:18,270 --> 00:38:21,520
Speaker 2:  Yeah, I think I say this every time we talk about Twitter on the show

679
00:38:21,880 --> 00:38:25,600
Speaker 2:  criticism of the Musk, Twitter is in no

680
00:38:25,620 --> 00:38:29,160
Speaker 2:  way praise for the previous administration of Twitter

681
00:38:29,200 --> 00:38:32,680
Speaker 2:  super busted, which was a disaster and accomplished nothing.

682
00:38:33,080 --> 00:38:36,720
Speaker 2:  Right? Like they, they had a lot of ideas and they rolled

683
00:38:36,720 --> 00:38:40,440
Speaker 2:  approximately none of them out. But it does seem like a lot of the

684
00:38:40,440 --> 00:38:44,360
Speaker 2:  ideas they had and didn't roll out Elon's, like just do them

685
00:38:44,700 --> 00:38:48,400
Speaker 2:  and they're breaking the site in the way that the

686
00:38:48,600 --> 00:38:50,840
Speaker 2:  previous administration predicted it would break the site.

687
00:38:51,110 --> 00:38:55,000
Speaker 13:  Yeah. I think one thing our sources have told us repeatedly is, look, the

688
00:38:55,000 --> 00:38:58,240
Speaker 13:  reason we're able to do so much of this stuff so quickly is cuz the code

689
00:38:58,240 --> 00:39:01,960
Speaker 13:  was already there because we tried this five years ago and then we killed

690
00:39:01,960 --> 00:39:04,960
Speaker 13:  it for really understandable reasons. So yeah, when Elon's like roll out

691
00:39:04,960 --> 00:39:08,800
Speaker 13:  long form video, we just go to the code repo and like get it and then

692
00:39:08,800 --> 00:39:12,400
Speaker 13:  launch it. So yeah, it's not perhaps as

693
00:39:12,400 --> 00:39:16,040
Speaker 13:  impressive as you would think, but it's certainly more efficient than it

694
00:39:16,040 --> 00:39:16,480
Speaker 13:  used to be.

695
00:39:16,850 --> 00:39:20,760
Speaker 2:  Is there any word on whether they're gonna roll back the Boost

696
00:39:20,760 --> 00:39:22,320
Speaker 2:  Elon by a factor of a thousand?

697
00:39:22,630 --> 00:39:26,200
Speaker 13:  Yeah, it's already rolled back. So the, it's called a power user

698
00:39:26,200 --> 00:39:30,000
Speaker 13:  multiplier, but the power user is still just Elon right now. But,

699
00:39:31,180 --> 00:39:34,600
Speaker 13:  but it's, it's been rolled back from a thousand, we don't know what the

700
00:39:34,600 --> 00:39:38,560
Speaker 13:  current number is, but it's lower than that. So you're not seeing him as

701
00:39:38,560 --> 00:39:42,280
Speaker 13:  much hypothetically, but the other kind of green light

702
00:39:42,280 --> 00:39:46,160
Speaker 13:  heuristics are, are still in place. So he's not subject to kind of the filters

703
00:39:46,160 --> 00:39:47,160
Speaker 13:  that normal users are.

704
00:39:47,460 --> 00:39:51,160
Speaker 7:  Can anyone explain the one question that I, that I have about this?

705
00:39:51,420 --> 00:39:55,320
Speaker 7:  Why didn't I see it? And I don't know who else also had this, but

706
00:39:55,320 --> 00:39:57,400
Speaker 7:  my for you page has no Elon on it.

707
00:39:57,400 --> 00:39:58,080
Speaker 13:  Have you blocked

708
00:39:58,080 --> 00:40:01,600
Speaker 7:  Him? No, I follow him. I have not notifications on first tweets. I read every

709
00:40:01,600 --> 00:40:05,480
Speaker 7:  single one for years now. But for some reason my for You page is an

710
00:40:05,480 --> 00:40:06,360
Speaker 7:  Elon free space.

711
00:40:06,430 --> 00:40:10,360
Speaker 2:  It's because everyone knows Richard, you played a very important role

712
00:40:10,360 --> 00:40:14,040
Speaker 2:  in what I would call Bitcoin summer where you just

713
00:40:14,120 --> 00:40:17,920
Speaker 2:  reply to all the Bitcoin people all the time and you were like, this you,

714
00:40:17,920 --> 00:40:21,600
Speaker 2:  and it was like a chart going down. Like I was like, when does Richard

715
00:40:21,600 --> 00:40:25,320
Speaker 2:  find the time? And I think Elon might've blocked you

716
00:40:27,380 --> 00:40:29,040
Speaker 7:  See there. There you go.

717
00:40:29,040 --> 00:40:32,520
Speaker 13:  This is my mystery. We'll try and solve though. I'm actually fascinated

718
00:40:32,520 --> 00:40:33,160
Speaker 13:  now. So

719
00:40:33,500 --> 00:40:37,080
Speaker 2:  I'm just saying if there is, there was a class of

720
00:40:37,080 --> 00:40:40,640
Speaker 2:  characters during Bitcoin summer that found no peace from Richard

721
00:40:40,640 --> 00:40:41,120
Speaker 2:  Lawler.

722
00:40:41,470 --> 00:40:42,120
Speaker 7:  I have a list.

723
00:40:42,500 --> 00:40:46,200
Speaker 2:  Zoe Elon was on stage at like the World

724
00:40:46,200 --> 00:40:50,080
Speaker 2:  Global Forum with, I think it was in Dubai. It was a very strange

725
00:40:50,080 --> 00:40:52,640
Speaker 2:  interview. I encouraged people to watch it. He was on a giant screen. Yeah.

726
00:40:52,820 --> 00:40:56,440
Speaker 2:  And he said the future of Twitter is this X app and

727
00:40:56,720 --> 00:41:00,240
Speaker 2:  everything app. Sure. He said this several times, right? The reason I bring

728
00:41:00,240 --> 00:41:03,560
Speaker 2:  it up is, okay now there's a new engineering culture inside of Twitter.

729
00:41:03,970 --> 00:41:07,920
Speaker 2:  It is oriented around one person and his dreams. Is there any tangible

730
00:41:08,520 --> 00:41:12,360
Speaker 2:  progress towards this big vision that bubbles out

731
00:41:12,360 --> 00:41:13,360
Speaker 2:  every now and again? X

732
00:41:14,470 --> 00:41:18,320
Speaker 13:  I mean it seems like he's actively making moves against it.

733
00:41:18,320 --> 00:41:21,360
Speaker 13:  Like if you're gonna make the Everything app, you need an open API where

734
00:41:21,360 --> 00:41:25,240
Speaker 13:  people are building apps on Twitter. For Twitter, they've

735
00:41:25,240 --> 00:41:29,160
Speaker 13:  closed that up, they're gonna make developers pay for it. So I'm not like

736
00:41:29,160 --> 00:41:32,560
Speaker 13:  totally sure how that even happens. Are they expecting to build every component

737
00:41:32,560 --> 00:41:36,360
Speaker 13:  of it? It in-house with a staff of less than 500 engineers?

738
00:41:36,400 --> 00:41:40,240
Speaker 13:  Maybe. But it seems more like just one of those things that he

739
00:41:40,680 --> 00:41:44,200
Speaker 13:  talks about perpetually. You know, we know payments is moving forward. Like

740
00:41:44,320 --> 00:41:47,160
Speaker 13:  Esther Crawford is still running that team and that seems like a big initiative

741
00:41:47,160 --> 00:41:50,680
Speaker 13:  that he's interested in. But on everything app, like I don't see it.

742
00:41:51,020 --> 00:41:54,480
Speaker 2:  And then Esther Crawford is also very much in charge of Twitter Blue.

743
00:41:55,150 --> 00:41:59,040
Speaker 2:  That seems it's still there. It's still I every day I wait for

744
00:41:59,040 --> 00:41:59,600
Speaker 2:  my check mark to

745
00:41:59,600 --> 00:42:01,920
Speaker 5:  Go away. Same. Richard, yours is gonna go first.

746
00:42:02,240 --> 00:42:05,880
Speaker 2:  Richard is definitely going first. It does not seem like Blue is a success.

747
00:42:05,970 --> 00:42:09,280
Speaker 2:  It does also doesn't seem like the company is in dire financial trouble.

748
00:42:09,590 --> 00:42:12,960
Speaker 2:  It's like the advertisers are gone and people aren't signing for blue. Like

749
00:42:13,230 --> 00:42:15,560
Speaker 2:  h how are the finances the company staying afloat?

750
00:42:16,240 --> 00:42:20,130
Speaker 13:  I mean so far it's just cuz they've cut costs so drastically that

751
00:42:20,130 --> 00:42:24,050
Speaker 13:  like some of this is I guess netting out for the moment. But obviously

752
00:42:24,050 --> 00:42:27,130
Speaker 13:  stuff still needs to change. I think they are gonna try and make Twitter

753
00:42:27,130 --> 00:42:30,650
Speaker 13:  blue more attractive so that more people subscribe. Whether enough people

754
00:42:30,650 --> 00:42:34,570
Speaker 13:  will subscribe to compensate for the number of advertisers who've fled and

755
00:42:34,570 --> 00:42:38,090
Speaker 13:  not come back. Like, I don't think that math works out right now, but we'll

756
00:42:38,090 --> 00:42:38,210
Speaker 13:  see.

757
00:42:38,630 --> 00:42:42,490
Speaker 2:  All right. Well I am still trying to not use it. Yeah,

758
00:42:42,490 --> 00:42:46,450
Speaker 2:  I open it once a day. See all of Elon's tweets, all the tweets

759
00:42:46,450 --> 00:42:50,130
Speaker 2:  that you don't see. Richard. I definitely see, they double show them to me,

760
00:42:50,130 --> 00:42:50,970
Speaker 2:  which is a delight.

761
00:42:51,080 --> 00:42:54,850
Speaker 5:  I don't go in that tap anymore. The one that's like for you

762
00:42:54,850 --> 00:42:56,130
Speaker 5:  instead of the following tab.

763
00:42:56,130 --> 00:42:59,410
Speaker 7:  That was the interesting thing to me. I, I didn't know so many people used

764
00:42:59,410 --> 00:43:01,130
Speaker 7:  it. I, I only used following.

765
00:43:01,200 --> 00:43:04,330
Speaker 5:  I kept complaining about it because I didn't realize following was there

766
00:43:04,330 --> 00:43:07,130
Speaker 5:  and all these people were like, you dumbass following is there. And I was

767
00:43:07,130 --> 00:43:07,210
Speaker 5:  like,

768
00:43:07,210 --> 00:43:11,090
Speaker 2:  Oh. So my, my following tab has gotten a lot quieter cuz all the people I

769
00:43:11,090 --> 00:43:14,690
Speaker 2:  know stop using Twitter. Oh, so the four, the four you is where the action

770
00:43:14,690 --> 00:43:15,210
Speaker 2:  is. Does

771
00:43:15,210 --> 00:43:16,810
Speaker 5:  You need Richard over there and Elon.

772
00:43:17,440 --> 00:43:17,930
Speaker 2:  Elon?

773
00:43:17,930 --> 00:43:21,370
Speaker 13:  Are you just doing quick posts or are you on master on like, what's, what's

774
00:43:21,370 --> 00:43:22,330
Speaker 13:  your replacement right

775
00:43:22,330 --> 00:43:26,290
Speaker 2:  Now? I am trying to not have feed-based social media in my life for a minute.

776
00:43:26,290 --> 00:43:28,650
Speaker 2:  Love this. I was addicted to it for 10 years.

777
00:43:29,260 --> 00:43:30,210
Speaker 5:  So good. It

778
00:43:30,210 --> 00:43:33,650
Speaker 2:  Is challenging. I watch a lot of weird

779
00:43:33,930 --> 00:43:37,650
Speaker 2:  hustle, bro. TikTok now, if anyone wants to make quick money at chat,

780
00:43:37,650 --> 00:43:40,130
Speaker 2:  g p t Boy are the ideas in my brain

781
00:43:41,480 --> 00:43:44,970
Speaker 2:  just infected. But I, I spent so long like

782
00:43:44,970 --> 00:43:48,930
Speaker 2:  tweeting and knowing what minor Twitter war

783
00:43:48,930 --> 00:43:52,450
Speaker 2:  was happening and now I don't and I feel it at peace. Yeah. So I've not yet

784
00:43:52,450 --> 00:43:54,650
Speaker 2:  signed up from, and I eventually will have to, I think

785
00:43:54,700 --> 00:43:58,450
Speaker 5:  It feels like it's coming. Yeah. I feel like I need to be on Mastodon, but

786
00:43:58,450 --> 00:43:58,610
Speaker 5:  then I

787
00:43:58,610 --> 00:43:59,690
Speaker 2:  Don't So are you on Mastodon?

788
00:43:59,860 --> 00:44:03,850
Speaker 13:  No. Okay. So I tried, I did the, the tweet where I was like, it's really

789
00:44:03,850 --> 00:44:07,690
Speaker 13:  happening. Here's my handle. Follow me everyone. And every

790
00:44:07,690 --> 00:44:11,010
Speaker 13:  time I've tried to do what me and Casey agreed, which is we're gonna post

791
00:44:11,010 --> 00:44:14,890
Speaker 13:  News First and Discord and then on Mastodon and then on Twitter. Mastodon

792
00:44:15,030 --> 00:44:18,210
Speaker 13:  automatically will shut off as I'm writing the tweet. This has happened

793
00:44:18,420 --> 00:44:22,410
Speaker 13:  at least 10 times. And then I just said no. I'm like, I

794
00:44:22,410 --> 00:44:26,050
Speaker 13:  can't, I, I want you but not bad enough to do this. So now I just do

795
00:44:26,050 --> 00:44:29,610
Speaker 13:  discordant Twitter and I'm like, I give it up, it crashes. It's crazy.

796
00:44:30,080 --> 00:44:33,240
Speaker 2:  I feel like someone is gonna come out with like the server.

797
00:44:34,110 --> 00:44:35,480
Speaker 5:  It's Tumblr. I'm telling

798
00:44:35,480 --> 00:44:36,240
Speaker 2:  You it's Tumblr.

799
00:44:36,350 --> 00:44:40,200
Speaker 5:  It's like, like Tumblr and I think another very small social media

800
00:44:40,200 --> 00:44:43,960
Speaker 5:  site that only the Vergecast talks about is planning to like adopt

801
00:44:43,960 --> 00:44:47,000
Speaker 5:  that, that the API or whatever that Masta Ong uses

802
00:44:47,140 --> 00:44:49,720
Speaker 2:  AVS Forum. What small media site are you talking about?

803
00:44:49,720 --> 00:44:53,040
Speaker 5:  I'm, I'll remember at some point I'll, I'll tweet it. Everybody just followed

804
00:44:53,040 --> 00:44:55,360
Speaker 5:  me on Twitter. I'll tweet it. You'll know. I don't

805
00:44:55,440 --> 00:44:56,480
Speaker 2:  Remember who it is. This is so mysterious.

806
00:44:56,570 --> 00:45:00,480
Speaker 5:  It is mysterious. But Tumblr is definitely doing it and there's rumors of

807
00:45:00,480 --> 00:45:02,360
Speaker 5:  another social media platform doing it.

808
00:45:02,360 --> 00:45:06,040
Speaker 2:  Yeah, I know Tumblr's doing it because Matt Maga the CEO of

809
00:45:06,040 --> 00:45:09,240
Speaker 2:  WordPress and Tumblr automatic, they're building it into

810
00:45:09,360 --> 00:45:12,920
Speaker 2:  WordPress. Yeah. And they're moving Tumblr onto whatever WordPress

811
00:45:12,920 --> 00:45:16,360
Speaker 5:  Uses. And that's just like a smart move. I feel like across the board

812
00:45:16,650 --> 00:45:20,440
Speaker 2:  It would be amazing if all the people move from Twitter to Tumblr. I mean,

813
00:45:20,440 --> 00:45:23,960
Speaker 2:  being emo has been like a real theme of this first, cuz we talked about Bing

814
00:45:23,960 --> 00:45:27,120
Speaker 2:  for a long time today. It would be amazing if everyone just like got

815
00:45:27,470 --> 00:45:29,680
Speaker 2:  much more emotional on Tumblr. Get

816
00:45:29,680 --> 00:45:31,920
Speaker 5:  Your, your music lyrics ready

817
00:45:32,520 --> 00:45:36,480
Speaker 2:  At some point I'll break because I mist tweeting during events. Yeah.

818
00:45:36,480 --> 00:45:39,600
Speaker 2:  Like that's like the main, that's the only thing that I truly miss.

819
00:45:40,100 --> 00:45:43,800
Speaker 2:  The like constant low stakes bonfire of Twitter warfare is like not

820
00:45:44,200 --> 00:45:44,520
Speaker 2:  whatever. How

821
00:45:44,520 --> 00:45:48,240
Speaker 5:  Is the balloon discourse hit you then? Like if you're not,

822
00:45:48,290 --> 00:45:48,640
Speaker 5:  oh,

823
00:45:48,640 --> 00:45:51,400
Speaker 2:  Because the New York Times is like, fuck it. Have you seen the balloon balloon

824
00:45:51,400 --> 00:45:55,040
Speaker 2:  balloon? Another balloon alert the balloon's due in numbers. Tell people

825
00:45:55,040 --> 00:45:57,600
Speaker 2:  about the balloon. Like the control.

826
00:45:57,600 --> 00:45:59,080
Speaker 5:  You're still there. You're still getting

827
00:45:59,080 --> 00:46:02,720
Speaker 2:  There. It's like no one can escape the balloons if you are a subscriber to

828
00:46:02,720 --> 00:46:06,240
Speaker 2:  the New York Times. It's like outta control. Zoe, do you think there's like

829
00:46:06,240 --> 00:46:09,720
Speaker 2:  a, a next thing for Twitter? Is it just chaos and we should just expect more

830
00:46:09,720 --> 00:46:10,320
Speaker 2:  chaos every day?

831
00:46:11,080 --> 00:46:14,680
Speaker 13:  I think the chaos will continue for a while now. I mean,

832
00:46:15,110 --> 00:46:18,840
Speaker 13:  it's Elon like he says he'll maybe step down by the end of the year. So

833
00:46:18,840 --> 00:46:22,480
Speaker 13:  we've got many more months of turmoil, I would say. But

834
00:46:22,480 --> 00:46:26,360
Speaker 13:  yeah, I mean I think there's gonna be more weird stuff happening in the

835
00:46:26,360 --> 00:46:29,280
Speaker 13:  coming weeks that I'm expecting to write about and then hopefully the news

836
00:46:29,280 --> 00:46:31,280
Speaker 13:  cycle dies down and we can pay attention to something else.

837
00:46:31,750 --> 00:46:35,560
Speaker 2:  I love it. Well Zoe, is it Platformer? Platformer News? You can

838
00:46:35,880 --> 00:46:39,600
Speaker 2:  subscribe. Our friend Casey's there as well. We syndicate platformer super

839
00:46:39,600 --> 00:46:42,080
Speaker 2:  good. So I feel like I should tell the people where they can find you on

840
00:46:42,080 --> 00:46:44,760
Speaker 2:  Twitter. It's like, not what I want to do. Yeah.

841
00:46:46,710 --> 00:46:47,960
Speaker 5:  Find your own Mastodon.

842
00:46:48,090 --> 00:46:51,320
Speaker 13:  No, please don't. Please don't. Instead I checked it.

843
00:46:52,270 --> 00:46:54,360
Speaker 13:  I regret that tweet. Okay.

844
00:46:56,080 --> 00:46:59,680
Speaker 2:  Look, that's why I didn't do it. Yeah, right. Like ev there was that week

845
00:46:59,680 --> 00:47:02,240
Speaker 2:  where I was like, I'm leaving. Here's all the places you can find me. And

846
00:47:02,240 --> 00:47:04,760
Speaker 2:  it's like, motherfucker, I don't want you to find my Instagram.

847
00:47:04,760 --> 00:47:05,200
Speaker 5:  Yeah.

848
00:47:05,200 --> 00:47:06,840
Speaker 2:  Like don't, this isn't who I am.

849
00:47:07,220 --> 00:47:09,800
Speaker 5:  You don't need that. You don't need to see the food I ate last week

850
00:47:10,160 --> 00:47:14,120
Speaker 2:  At the end. I think he did me a favor. I think my, my, my heart in my head

851
00:47:14,120 --> 00:47:14,640
Speaker 2:  have healed.

852
00:47:14,830 --> 00:47:18,560
Speaker 5:  I've got a, I've got actually a bet about Elon right now with both Liz Lato

853
00:47:18,560 --> 00:47:19,720
Speaker 5:  and Monica Chen. What's

854
00:47:19,720 --> 00:47:20,040
Speaker 2:  That bet?

855
00:47:20,440 --> 00:47:24,200
Speaker 5:  Whether or not he's gonna post horrible pictures that we are all gonna

856
00:47:24,200 --> 00:47:26,640
Speaker 5:  have to see because he controls the algorithm right now.

857
00:47:26,670 --> 00:47:28,160
Speaker 13:  What do you mean horrible pictures?

858
00:47:28,470 --> 00:47:29,720
Speaker 2:  I mean, there's already the milk

859
00:47:29,720 --> 00:47:33,560
Speaker 5:  One. Yeah, the milk one, like 10 times worse than milk. I feel like it's

860
00:47:33,560 --> 00:47:33,760
Speaker 5:  coming.

861
00:47:33,790 --> 00:47:34,920
Speaker 2:  It's a porn tweets.

862
00:47:34,920 --> 00:47:37,520
Speaker 5:  Yeah. I I feel like we're gonna get a dick pick at some point. I don't know

863
00:47:37,520 --> 00:47:41,440
Speaker 5:  who's Oh, I'm sorry. I'm sorry I'm, but we gotta, I think

864
00:47:41,440 --> 00:47:44,840
Speaker 2:  It's gonna happen. It's been great here. Love having you. We gotta

865
00:47:45,570 --> 00:47:45,920
Speaker 2:  so

866
00:47:47,830 --> 00:47:49,720
Speaker 2:  well, you're right back with our lightning round.

867
00:47:54,790 --> 00:47:58,730
Speaker 14:  Hey there. I'm Josh Muo, host of the Pitch. A show where

868
00:47:58,730 --> 00:48:02,450
Speaker 14:  real entrepreneurs pitch real tech investors for real money.

869
00:48:02,830 --> 00:48:05,210
Speaker 14:  And this season gets a little saucy.

870
00:48:05,380 --> 00:48:09,250
Speaker 15:  We knew that we were not going to join the same old

871
00:48:09,250 --> 00:48:12,490
Speaker 15:  sauce game. We were gonna disrupt 130 billion

872
00:48:13,090 --> 00:48:14,330
Speaker 15:  industry. We're coming after

873
00:48:14,330 --> 00:48:18,130
Speaker 14:  Hines. We meet founders pitching everything from a better ketchup

874
00:48:18,660 --> 00:48:21,890
Speaker 14:  to a solution for one of the biggest problems today,

875
00:48:22,520 --> 00:48:23,130
Speaker 14:  energy

876
00:48:23,180 --> 00:48:26,890
Speaker 16:  In the future, the vehicles and garage. It's gonna be like a giant

877
00:48:26,890 --> 00:48:30,210
Speaker 16:  power plant that could be used to reinforce the power grid,

878
00:48:30,210 --> 00:48:33,850
Speaker 16:  substitute fossil fuel power plants, and allow more renewable energy on

879
00:48:33,850 --> 00:48:34,090
Speaker 16:  the network.

880
00:48:34,320 --> 00:48:37,770
Speaker 14:  This season, 14 founders with 12 ideas

881
00:48:37,950 --> 00:48:41,850
Speaker 14:  big enough to change the world. If the investors invest.

882
00:48:42,160 --> 00:48:46,090
Speaker 14:  I would like to invest in this round. $300,000 new episodes every

883
00:48:46,090 --> 00:48:47,930
Speaker 14:  Wednesday. See you in the pitch room.

884
00:48:53,970 --> 00:48:57,850
Speaker 2:  We're back. Yeah. Lightning round. This one I think merits more

885
00:48:57,850 --> 00:49:01,690
Speaker 2:  than a lightning round just because of the headline that it is. But

886
00:49:01,690 --> 00:49:05,650
Speaker 2:  then it does not YouTube CEO Susan Majeski stepping down after nine

887
00:49:05,650 --> 00:49:06,930
Speaker 2:  years. C o of YouTube.

888
00:49:06,930 --> 00:49:08,930
Speaker 5:  What a lap do, a victory lap. Susan,

889
00:49:09,030 --> 00:49:13,010
Speaker 2:  She is like a major force in this world. Yes. She took YouTube, which

890
00:49:13,010 --> 00:49:16,210
Speaker 2:  was like not a great business. She's one of the earliest Google employees.

891
00:49:16,390 --> 00:49:19,010
Speaker 2:  She took YouTube, which is not a great business, and turned it into this

892
00:49:19,490 --> 00:49:20,450
Speaker 2:  absolute powerhouse,

893
00:49:20,930 --> 00:49:21,450
Speaker 5:  Absolute Goliath.

894
00:49:21,540 --> 00:49:24,730
Speaker 2:  It is the, it remains, I think the gold standard for every creator.

895
00:49:25,290 --> 00:49:29,130
Speaker 2:  There's lots of creators on Twitter and other places today saying, you know,

896
00:49:29,130 --> 00:49:32,130
Speaker 2:  we have our complaints about YouTube and boy do they have their complaints

897
00:49:32,130 --> 00:49:35,610
Speaker 2:  about YouTube, but it is the platform that treats us the best. Right. I think

898
00:49:35,610 --> 00:49:39,170
Speaker 2:  Mark has had a tweet that's like, just look at compare. Like YouTube has

899
00:49:39,180 --> 00:49:42,850
Speaker 2:  creator plaques and events and monetization schedules and reps.

900
00:49:42,950 --> 00:49:46,570
Speaker 2:  And then he goes like, Instagram, nothing TikTok, nothing Twitter,

901
00:49:46,570 --> 00:49:50,490
Speaker 2:  nothing like, so YouTube is a powerhouse. So kind of

902
00:49:50,490 --> 00:49:53,610
Speaker 2:  amazing. She's stepping down. There's a little bit of color. Mark Bergen,

903
00:49:53,610 --> 00:49:56,450
Speaker 2:  who's a great reporter like Bloomberg, has been tweeting a little bit of

904
00:49:56,450 --> 00:50:00,290
Speaker 2:  color. She's dipped in the past few months. No one's really seen

905
00:50:00,290 --> 00:50:03,290
Speaker 2:  her. Yeah. No one's really understood this transition.

906
00:50:03,580 --> 00:50:04,690
Speaker 5:  Oh, that's not great.

907
00:50:04,750 --> 00:50:08,170
Speaker 2:  But it has been nine years and she's been replaced by her own handpicked

908
00:50:08,170 --> 00:50:12,050
Speaker 2:  deputy Neil Mohan, who is the chief product officer of YouTube. But now

909
00:50:12,070 --> 00:50:15,930
Speaker 2:  importantly, not becoming the CEO of YouTube, he's becoming the

910
00:50:15,930 --> 00:50:18,130
Speaker 2:  senior vice president of YouTube reporting atcha.

911
00:50:18,460 --> 00:50:21,370
Speaker 5:  Oh, interesting. So that's, that's an org chart.

912
00:50:21,470 --> 00:50:22,250
Speaker 2:  You know how I'm

913
00:50:22,250 --> 00:50:25,610
Speaker 5:  Wizard I was about to say this. Like, this like peaked you. Huh? You were

914
00:50:25,610 --> 00:50:25,850
Speaker 5:  like, oh

915
00:50:25,850 --> 00:50:29,440
Speaker 2:  Yeah, there's, there's obviously some drama here. Yeah. It's not a lot of

916
00:50:29,440 --> 00:50:33,240
Speaker 2:  drama because I know Neil, Neil has been on decoder many times. I just

917
00:50:33,480 --> 00:50:36,080
Speaker 2:  actually just talked to Neil after they did the Sunday ticket deal for YouTube

918
00:50:36,080 --> 00:50:40,000
Speaker 2:  tv. He was very happy he, he won. I don't

919
00:50:40,000 --> 00:50:42,360
Speaker 2:  think you're gonna see a lot of change between Susan and Neil. Like they

920
00:50:42,360 --> 00:50:46,040
Speaker 2:  were pretty lockstep. YouTube actually really interesting or chart thing.

921
00:50:46,310 --> 00:50:50,200
Speaker 2:  Neil head of product YouTube's content moderation and trust and

922
00:50:50,200 --> 00:50:53,800
Speaker 2:  safety reports to him is the head of product weird

923
00:50:53,800 --> 00:50:57,080
Speaker 2:  because they've built it as a product. They think it's a product cap. This

924
00:50:57,080 --> 00:51:00,280
Speaker 2:  is my whole thing about content moderation is the product. It's a built into

925
00:51:00,280 --> 00:51:03,720
Speaker 2:  YouTube. It is a product. Okay. It doesn't report to like legal or Mark

926
00:51:03,720 --> 00:51:05,480
Speaker 2:  Zuckerberg or whatever. It's Right.

927
00:51:05,830 --> 00:51:09,440
Speaker 5:  Just mark for some reason, just that part of YouTube

928
00:51:09,550 --> 00:51:10,560
Speaker 5:  Mark handles?

929
00:51:11,170 --> 00:51:14,280
Speaker 2:  No, it's just like at Facebook. Yeah. Like at the end of the day, like Mark

930
00:51:14,280 --> 00:51:15,760
Speaker 2:  made a bunch of decisions, right?

931
00:51:15,760 --> 00:51:17,680
Speaker 5:  Like everything goes up to Mark.

932
00:51:17,680 --> 00:51:20,920
Speaker 2:  Yeah. This one was like it's product. And Susan made a bunch of those decisions

933
00:51:20,920 --> 00:51:24,200
Speaker 2:  too. But it's, it's always just like one of the more unique parts of YouTube

934
00:51:24,360 --> 00:51:26,720
Speaker 2:  structure. And now he's in charge. Yeah.

935
00:51:27,170 --> 00:51:30,600
Speaker 5:  Do you think he, he didn't get the title because like she was an early

936
00:51:31,440 --> 00:51:35,040
Speaker 5:  employee of Google. She put a lot of time and energy into Google for years.

937
00:51:35,530 --> 00:51:39,280
Speaker 5:  It made sense to make her ceo and now the structures, everything's changed.

938
00:51:39,810 --> 00:51:43,280
Speaker 5:  It makes sense to just have that person report directly to Sundar and like,

939
00:51:43,280 --> 00:51:43,800
Speaker 5:  that's it.

940
00:51:43,800 --> 00:51:47,360
Speaker 2:  Yeah. I don't know. I mean, again, my, I love an hard shark

941
00:51:47,360 --> 00:51:51,280
Speaker 2:  conversation. Yeah. She was the CEO of YouTube. Okay. Before Sundar Pacha

942
00:51:51,280 --> 00:51:55,040
Speaker 2:  became CEO of Google. Right? So there's Larry and Sergei, they start Google

943
00:51:55,650 --> 00:51:59,320
Speaker 2:  in her garage. She's early, early, early.

944
00:52:00,160 --> 00:52:03,880
Speaker 2:  She becomes a CEO of YouTube. And then they're like, we're, Larry Page is

945
00:52:03,880 --> 00:52:07,520
Speaker 2:  like, we're dipping, screw this. We're just gonna do flying car

946
00:52:07,520 --> 00:52:10,880
Speaker 2:  startups or whatever it is. They do. Sun is the CEO of

947
00:52:11,070 --> 00:52:14,880
Speaker 2:  Google. So she predated Sundar as the CEO

948
00:52:14,880 --> 00:52:18,040
Speaker 2:  of YouTube before he became the CEO of Google. And then he became the CEO

949
00:52:18,040 --> 00:52:18,720
Speaker 2:  of Alphabet.

950
00:52:18,780 --> 00:52:21,240
Speaker 5:  And so she reported to him when he became CEO of

951
00:52:21,400 --> 00:52:25,280
Speaker 2:  Alphabet? No, C of Google. Okay. So YouTube is a weird thing. It's part

952
00:52:25,280 --> 00:52:28,920
Speaker 2:  of Google, which is part of Alphabet, but it's still part of Google, but

953
00:52:28,920 --> 00:52:29,240
Speaker 5:  It's still

954
00:52:29,240 --> 00:52:33,160
Speaker 2:  YouTube, but it's it, but it's its own little island. Okay. It's very confusing,

955
00:52:33,160 --> 00:52:36,760
Speaker 2:  super confusing. So now Neil is in charge again. You can go listen to

956
00:52:36,910 --> 00:52:40,840
Speaker 2:  many decoder episodes with, with Neil Moen. He's smart. He's got

957
00:52:40,840 --> 00:52:43,840
Speaker 2:  a lot of ideas he will not answer when YouTube TV will be in 4k.

958
00:52:44,450 --> 00:52:46,060
Speaker 2:  Just very good. Please

959
00:52:46,060 --> 00:52:46,380
Speaker 5:  Answer.

960
00:52:46,380 --> 00:52:50,260
Speaker 2:  Sometimes I hit the, the brick wall of someone's media training on

961
00:52:50,260 --> 00:52:51,340
Speaker 2:  decoder. Yeah. And that's the wall

962
00:52:53,580 --> 00:52:56,780
Speaker 2:  straight up. I found it. There it is. And I bounced right off it. And try,

963
00:52:56,780 --> 00:52:57,100
Speaker 2:  try

964
00:52:57,100 --> 00:53:00,500
Speaker 5:  Again. Anyone? You ask. What about YouTube, TV and 4k? They're like, no.

965
00:53:00,570 --> 00:53:03,260
Speaker 2:  Yeah. The just the shades come down. It's

966
00:53:03,260 --> 00:53:05,740
Speaker 5:  All decide. Yeah. He's like, I, I don't have an answer, but no.

967
00:53:05,740 --> 00:53:09,300
Speaker 2:  When we talked about when they, the Sunny Ticket deal, I was like, all right,

968
00:53:09,300 --> 00:53:11,820
Speaker 2:  you gonna do football in 4k? Like you had to know this conversation. You

969
00:53:11,820 --> 00:53:13,220
Speaker 2:  volunteered to talk to me. Yeah,

970
00:53:13,220 --> 00:53:14,100
Speaker 5:  It has to. He

971
00:53:14,100 --> 00:53:16,820
Speaker 2:  Had to, what do I talk about? And he was like, I can't ask that question.

972
00:53:16,870 --> 00:53:17,940
Speaker 5:  So he was prepped to say that

973
00:53:20,940 --> 00:53:24,220
Speaker 2:  One time, this is trip one time I just sent him an email watching the NBA

974
00:53:24,220 --> 00:53:26,680
Speaker 2:  Final just sponsored by, by YouTube tv. And I was like, why don't you just

975
00:53:26,680 --> 00:53:29,920
Speaker 2:  pay to do this in 4K and have it exclusive? And he was just like, yeah, it's

976
00:53:29,920 --> 00:53:30,360
Speaker 2:  a good idea.

977
00:53:30,390 --> 00:53:31,040
Speaker 5:  Left you

978
00:53:31,040 --> 00:53:34,520
Speaker 2:  On Lead. Goodbye absolute professor. You have been a bad Bing.

979
00:53:35,990 --> 00:53:39,760
Speaker 2:  Anyway, I'm hoping to talk to Neil soon about his plans. I, I said, I

980
00:53:39,760 --> 00:53:43,240
Speaker 2:  suspect things will be, let's say these are two people who worked in lockstep

981
00:53:43,520 --> 00:53:47,240
Speaker 2:  for many, many years. Whatever Neil's plans were were

982
00:53:47,240 --> 00:53:51,000
Speaker 2:  Susan's plans for a long time. But it is fascinating that if this,

983
00:53:51,310 --> 00:53:55,180
Speaker 2:  I think for Google, the threats are all over

984
00:53:55,180 --> 00:53:59,140
Speaker 2:  the place, right? There's the, whatever you think of the Bing

985
00:53:59,140 --> 00:54:03,060
Speaker 2:  threat, it is a real threat. It wiped a hundred billion dollars off its market

986
00:54:03,060 --> 00:54:06,420
Speaker 2:  cap last week. The idea that search will change,

987
00:54:06,800 --> 00:54:10,740
Speaker 2:  not just because people might switch to Ping cuz they wanna bang the

988
00:54:10,740 --> 00:54:11,420
Speaker 2:  robot, but

989
00:54:11,420 --> 00:54:11,860
Speaker 5:  TikTok

990
00:54:11,860 --> 00:54:14,860
Speaker 2:  And, but they're spending their time on TikTok. They're using search there.

991
00:54:15,160 --> 00:54:19,100
Speaker 2:  And then what Chat G p T has really done is the cost

992
00:54:19,100 --> 00:54:22,660
Speaker 2:  to generate an infinite amount of mediocre text is now zero.

993
00:54:23,030 --> 00:54:26,860
Speaker 2:  So Google's being flooded with garbage. Right. And

994
00:54:26,860 --> 00:54:30,140
Speaker 2:  they've gotta sort that out too. And so like that's a big problem for Google.

995
00:54:30,140 --> 00:54:33,980
Speaker 2:  And then next, and then YouTube is fine. It's still a gold standard for

996
00:54:33,980 --> 00:54:37,740
Speaker 2:  creators, but it has to fend off TikTok. It has to create new

997
00:54:37,740 --> 00:54:41,540
Speaker 2:  monetization models for creators. It has to, it, the

998
00:54:41,540 --> 00:54:43,700
Speaker 2:  most exciting part of YouTube is it's cable network.

999
00:54:44,170 --> 00:54:45,340
Speaker 5:  Yeah. It's

1000
00:54:45,340 --> 00:54:48,220
Speaker 2:  Weird, right? Weird table system like YouTube tv. That's weird.

1001
00:54:48,330 --> 00:54:52,260
Speaker 7:  It's an interesting timing for her to, to make this move. I think not only

1002
00:54:52,260 --> 00:54:55,780
Speaker 7:  because of the length of time that she's been with YouTube, with Google and

1003
00:54:55,780 --> 00:54:58,620
Speaker 7:  it has been doing all these things, these No. And now they're facing these

1004
00:54:58,620 --> 00:55:02,500
Speaker 7:  problems. I find it interesting and, and I don't think

1005
00:55:02,500 --> 00:55:05,180
Speaker 7:  that these things are directly related because it, it sound, it feels like

1006
00:55:05,180 --> 00:55:07,860
Speaker 7:  this is something that has been in the works for at least a little while.

1007
00:55:07,860 --> 00:55:11,180
Speaker 7:  And as you said, does she and Neil are kind of on the same page of terms

1008
00:55:11,180 --> 00:55:15,060
Speaker 7:  of what they're doing. But it's interesting that she's leaving as Sergey

1009
00:55:15,060 --> 00:55:18,980
Speaker 7:  and Larry are suddenly seemingly retaking active roles in the company.

1010
00:55:19,080 --> 00:55:22,700
Speaker 7:  And she's probably the person there who has known them the longest,

1011
00:55:22,790 --> 00:55:26,140
Speaker 7:  or I mean certainly the person there who has known them the longest. Yeah.

1012
00:55:26,140 --> 00:55:28,420
Speaker 7:  And now that they're back, suddenly she's out the door.

1013
00:55:28,770 --> 00:55:32,700
Speaker 2:  It's unclear how much they're back. So there was the report that Google

1014
00:55:32,700 --> 00:55:36,180
Speaker 2:  had called the Code Red about chatbots and all this stuff. And then

1015
00:55:36,650 --> 00:55:39,980
Speaker 2:  I, I think Alex Heath reported in Command Line, they were back because Google

1016
00:55:39,980 --> 00:55:43,580
Speaker 2:  has never had layoffs before. So if they were gonna be layoffs, they wanted

1017
00:55:43,580 --> 00:55:47,460
Speaker 2:  to be in the building part of that process. It's a big shock

1018
00:55:47,460 --> 00:55:51,180
Speaker 2:  to Google's culture. There was a blog post this week

1019
00:55:51,180 --> 00:55:54,620
Speaker 2:  from an ex Googler link at the show notes pretty

1020
00:55:54,980 --> 00:55:58,540
Speaker 2:  devastating take down of Google's culture overall, right. And how it's is

1021
00:55:58,540 --> 00:56:02,340
Speaker 2:  basically a culture that avoids risks and is buoyed

1022
00:56:02,340 --> 00:56:06,220
Speaker 2:  by the ad printing, the ad money printing machine of Google

1023
00:56:06,220 --> 00:56:09,900
Speaker 2:  search. And now the company needs to invent new things

1024
00:56:09,900 --> 00:56:13,620
Speaker 2:  and chart new paths and be what people think Google

1025
00:56:13,620 --> 00:56:15,380
Speaker 2:  is and they're struggling to do it.

1026
00:56:15,610 --> 00:56:19,460
Speaker 5:  I thought that was interesting cuz Tony Fidel his book last year, what was

1027
00:56:19,460 --> 00:56:22,500
Speaker 5:  it called? Build I think. Yeah. There's a big chunk of that book where he's

1028
00:56:22,500 --> 00:56:25,360
Speaker 5:  like, I got got Nest. And then I go over to Google and it's gonna be great

1029
00:56:25,360 --> 00:56:26,720
Speaker 5:  and then everything falls

1030
00:56:26,720 --> 00:56:29,560
Speaker 2:  Apart. Yeah. I don't know anybody who's matter at Google than Tony

1031
00:56:29,560 --> 00:56:32,920
Speaker 5:  Fidel. And it was like a really, like, it felt like kind of

1032
00:56:33,260 --> 00:56:36,040
Speaker 5:  foreshadowing for what we're starting to see this year where he was like,

1033
00:56:36,150 --> 00:56:40,080
Speaker 5:  I saw all of this dysfunction. It was constant. It's always the loudest person

1034
00:56:40,080 --> 00:56:44,000
Speaker 5:  in the room wins. It's always but only very specific

1035
00:56:44,000 --> 00:56:47,880
Speaker 5:  loud people. Nobody is ever talking to each other, especially in ways they

1036
00:56:47,880 --> 00:56:51,560
Speaker 5:  need to. And so like all of that dysfunction it feels like is starting to

1037
00:56:51,560 --> 00:56:55,440
Speaker 5:  come to a head. And it makes sense for Susan to be like, you know

1038
00:56:55,440 --> 00:56:59,320
Speaker 5:  what? I got my bag. I'm gonna retire. I'm gonna go do my own thing. I love

1039
00:56:59,320 --> 00:57:00,240
Speaker 5:  that for her. Yeah.

1040
00:57:00,550 --> 00:57:04,160
Speaker 2:  Like I said, incredible run. YouTube is the platform. It is

1041
00:57:04,750 --> 00:57:08,680
Speaker 2:  many reasons to criticize YouTube on any day. Some

1042
00:57:08,680 --> 00:57:12,400
Speaker 2:  YouTuber is getting their wings and making the video where they complain

1043
00:57:12,400 --> 00:57:16,280
Speaker 2:  about YouTube. Yes. And that is the beginning of the true, that's like

1044
00:57:16,280 --> 00:57:19,360
Speaker 2:  when you become a YouTuber is when you make the video complaining about being

1045
00:57:19,360 --> 00:57:22,320
Speaker 2:  demonetized or some other, or the algorithm banning or whatever it is.

1046
00:57:22,350 --> 00:57:23,720
Speaker 5:  I can't wait to do that for the

1047
00:57:23,720 --> 00:57:27,160
Speaker 2:  Ver that's when you know, you're a professional YouTube. And I'm like, I've

1048
00:57:27,160 --> 00:57:29,160
Speaker 2:  said this to Neil. I'm like, this is a thing. He's like, yeah, it's weird,

1049
00:57:29,160 --> 00:57:33,000
Speaker 2:  huh? Like they know that this is part of the cycle with YouTube. Yeah. And

1050
00:57:33,000 --> 00:57:35,520
Speaker 2:  they, they, they have this competition with TikTok. They have to fend off.

1051
00:57:35,930 --> 00:57:39,800
Speaker 2:  So we'll see. I I, I make, I'm gonna ask to have Neil come back on one of

1052
00:57:39,800 --> 00:57:43,400
Speaker 2:  our shows and talk about his plan soon, but I think it's,

1053
00:57:43,630 --> 00:57:47,480
Speaker 2:  it's worth noting that right now YouTube is the

1054
00:57:47,480 --> 00:57:51,400
Speaker 2:  strongest business I think at Google in terms of just what it represents

1055
00:57:51,400 --> 00:57:54,720
Speaker 2:  to people and how important it is to people. But it is still under threat.

1056
00:57:54,720 --> 00:57:58,600
Speaker 5:  Yeah. But I mean, to be clear it's under threat. But like TikTok

1057
00:57:58,600 --> 00:58:02,200
Speaker 5:  still has a smaller Yeah. Piece of the pie than YouTube. Like YouTube is

1058
00:58:02,200 --> 00:58:06,120
Speaker 5:  still big, it's still huge. TikTok is coming, but talk's

1059
00:58:06,120 --> 00:58:08,800
Speaker 5:  got a ways to go to like Surpass.

1060
00:58:08,830 --> 00:58:12,600
Speaker 2:  Yeah. And TikTok does not have, well we don't know. TikTok is a black

1061
00:58:12,600 --> 00:58:15,320
Speaker 2:  box. Right? It's owned by Bite. Dance by dance is not publicly.

1062
00:58:15,320 --> 00:58:16,680
Speaker 5:  Who knows what's gonna happen with it.

1063
00:58:17,140 --> 00:58:20,960
Speaker 2:  But we think that TikTok does not have the sort of

1064
00:58:20,960 --> 00:58:24,920
Speaker 2:  revenue engine that YouTube has. Right? Like YouTube

1065
00:58:24,920 --> 00:58:28,440
Speaker 2:  makes a lot of money, it shares a lot of money with creators. It is building

1066
00:58:28,440 --> 00:58:32,040
Speaker 2:  this new creator fund for shorts. It's, it's doing all the stuff that TikTok

1067
00:58:32,040 --> 00:58:35,480
Speaker 2:  has not yet done. Yeah. So YouTube is like a complete business that makes

1068
00:58:35,480 --> 00:58:38,920
Speaker 2:  a lot of money. Which again, this is credit to Susan

1069
00:58:39,070 --> 00:58:42,680
Speaker 2:  Wosk, right? It did not make money for a long time.

1070
00:58:42,680 --> 00:58:46,440
Speaker 2:  Like yeah, mark Bergen has a great book about the history of YouTube.

1071
00:58:46,440 --> 00:58:50,000
Speaker 2:  Mark was on decoder, you can go listen to that episode. But like YouTube

1072
00:58:50,000 --> 00:58:53,120
Speaker 2:  was so bad for a while that Google was like, we're gonna shut it down. Like

1073
00:58:53,120 --> 00:58:56,960
Speaker 2:  this doesn't make any sense. The cost of hosting distributing video are high

1074
00:58:56,960 --> 00:59:00,920
Speaker 2:  earth and the whatever we might ever return on it. Right. And then it

1075
00:59:00,920 --> 00:59:04,680
Speaker 2:  clicked for them. And the YouTube has gone through many arrows, but it, they're

1076
00:59:04,680 --> 00:59:08,600
Speaker 2:  like, watch time is the thing we're gonna incentivize like 10

1077
00:59:08,600 --> 00:59:12,360
Speaker 2:  minute videos, like three pre-roll breaks or whatever it is. And that's when

1078
00:59:12,620 --> 00:59:16,440
Speaker 2:  the Charlie bit my fingers of YouTube, like people stop making those

1079
00:59:16,440 --> 00:59:18,920
Speaker 2:  videos. They start making these really long YouTube, like a YouTube video.

1080
00:59:18,920 --> 00:59:22,880
Speaker 5:  Right. And then Charlie bit, my, my finger would now be absolute banger

1081
00:59:22,880 --> 00:59:26,080
Speaker 5:  on TikTok. Exactly. Oh my god. The remixes for TikTok would be so good.

1082
00:59:26,080 --> 00:59:28,360
Speaker 2:  The stitches of other people biting other fingers. Oh

1083
00:59:28,360 --> 00:59:28,480
Speaker 5:  Yeah.

1084
00:59:28,810 --> 00:59:32,000
Speaker 2:  So great. So there's just like a, there's a lot there. But she

1085
00:59:32,480 --> 00:59:36,400
Speaker 2:  shepherded that entire creator platform into existence and in

1086
00:59:36,400 --> 00:59:40,200
Speaker 2:  doing so, like created a new class of

1087
00:59:40,480 --> 00:59:43,440
Speaker 2:  internet personality, a new style of video.

1088
00:59:44,030 --> 00:59:47,680
Speaker 2:  Like there's just a lot here that Susan oversaw, which I think is worthwhile.

1089
00:59:47,680 --> 00:59:51,480
Speaker 2:  There's a lot of criticism too. Like she did not handle a bunch of harassment

1090
00:59:51,480 --> 00:59:54,760
Speaker 2:  stuff well across the platform. Her

1091
00:59:54,760 --> 00:59:58,520
Speaker 2:  moderation decisions. I think anybody would tell you very deliberate,

1092
00:59:58,520 --> 00:59:59,320
Speaker 2:  very slow.

1093
00:59:59,920 --> 01:00:03,360
Speaker 5:  There were a lot of times you would groan when you'd see like, she'd be like,

1094
01:00:03,420 --> 01:00:06,840
Speaker 5:  I'm gonna speak out about harassment on YouTube. And you'd be like, oh no.

1095
01:00:06,840 --> 01:00:10,600
Speaker 2:  Yeah. This is like the most interesting criticism because

1096
01:00:10,790 --> 01:00:13,920
Speaker 2:  I think from her perspective, this is not a criticism, but from any normal

1097
01:00:13,920 --> 01:00:17,160
Speaker 2:  person's perspective, this is like what he doing. Google's like a totally

1098
01:00:17,160 --> 01:00:21,040
Speaker 2:  data oriented company. So the users of YouTube would

1099
01:00:21,060 --> 01:00:24,840
Speaker 2:  see something happening on YouTube. YouTubers would see

1100
01:00:24,840 --> 01:00:28,440
Speaker 2:  something happening good or bad on YouTube. But until there was

1101
01:00:28,440 --> 01:00:32,360
Speaker 2:  data, YouTube couldn't see it. Yeah. Susan couldn't see it.

1102
01:00:32,490 --> 01:00:36,440
Speaker 2:  So the YouTubers like screaming their heads off about something good or

1103
01:00:36,440 --> 01:00:39,760
Speaker 2:  bad happening and like there'd be this massive lag

1104
01:00:40,430 --> 01:00:43,280
Speaker 2:  before they could see what was happening on their own platform. Cuz they're,

1105
01:00:43,280 --> 01:00:45,760
Speaker 2:  they're just like, they had to figure out how to quantify whatever it was,

1106
01:00:45,760 --> 01:00:49,680
Speaker 2:  harassment, people dancing, whatever it is. And then they would see it

1107
01:00:49,680 --> 01:00:53,560
Speaker 2:  and then they would react to it and it would like be over or the stakes would

1108
01:00:53,560 --> 01:00:57,440
Speaker 2:  be lower or some bad thing had already happened. And like, this

1109
01:00:57,440 --> 01:01:00,800
Speaker 2:  is like, I think a constant kind of dynamic inside of YouTube

1110
01:01:00,880 --> 01:01:04,760
Speaker 2:  where because Google's just so data driven, they, they can't see what is

1111
01:01:04,760 --> 01:01:08,640
Speaker 2:  happening on their own platform in real time. The, my my favorite example

1112
01:01:08,640 --> 01:01:12,520
Speaker 2:  of this, like this is one of those moments where I keep listening to

1113
01:01:12,520 --> 01:01:14,880
Speaker 2:  executive, you're like, what planet do you live on? I think it was at the

1114
01:01:14,880 --> 01:01:17,320
Speaker 2:  code conference. She's like, well obviously you know, YouTube is like mostly

1115
01:01:17,320 --> 01:01:20,440
Speaker 2:  a music service. Okay. All of us are like, Walter keeps talking

1116
01:01:21,020 --> 01:01:24,840
Speaker 2:  and like the data inside of YouTube is people

1117
01:01:24,990 --> 01:01:28,880
Speaker 2:  just put music on in the background. They watch music videos, like

1118
01:01:28,880 --> 01:01:29,360
Speaker 2:  do all the stuff.

1119
01:01:29,360 --> 01:01:31,000
Speaker 5:  It's not the lofi streaming.

1120
01:01:31,000 --> 01:01:34,520
Speaker 2:  Right. So if you are just looking at YouTube from this like aggregate data

1121
01:01:34,520 --> 01:01:38,360
Speaker 2:  perspective, you're like, people come to us to listen to music. But if you

1122
01:01:38,360 --> 01:01:42,000
Speaker 2:  talk to a normal person, that is absolutely not what they're doing.

1123
01:01:42,000 --> 01:01:45,360
Speaker 2:  Right. And there's just, there's a gap there. And I like, like I said, is

1124
01:01:45,360 --> 01:01:49,160
Speaker 2:  that a criticism of Susan's regime? Is it her being smart and saying here's

1125
01:01:49,160 --> 01:01:52,480
Speaker 2:  what's actually happening. I think cuts both ways. Yeah. But wherever she

1126
01:01:52,480 --> 01:01:55,720
Speaker 2:  got in trouble, you, that's kind of that little dynamic is always at the

1127
01:01:55,720 --> 01:01:56,000
Speaker 2:  root of

1128
01:01:56,000 --> 01:01:59,080
Speaker 5:  It. Yeah. And then TikTok is smart cuz it just doesn't say anything at all.

1129
01:01:59,080 --> 01:01:59,520
Speaker 5:  Ever.

1130
01:02:00,070 --> 01:02:00,560
Speaker 2:  Yeah.

1131
01:02:00,590 --> 01:02:01,440
Speaker 5:  Always quiet.

1132
01:02:01,440 --> 01:02:05,240
Speaker 7:  It's that connection to the individual that that seems to have always

1133
01:02:05,240 --> 01:02:08,200
Speaker 7:  lacked at YouTube. Whether it's like something that we talked about in a

1134
01:02:08,320 --> 01:02:12,200
Speaker 7:  previous episode of Kyrie Irving going down a YouTube rabbit hole and

1135
01:02:12,200 --> 01:02:16,080
Speaker 7:  getting convinced of very strange things or other, other even more, more

1136
01:02:16,280 --> 01:02:19,120
Speaker 7:  devastating events. We we, there was the time where the one woman who who

1137
01:02:19,120 --> 01:02:22,960
Speaker 7:  posted videos came and, and fired shots at the campus because she was so

1138
01:02:22,960 --> 01:02:26,800
Speaker 7:  angry about what she felt like the algorithm was doing. And YouTube just

1139
01:02:26,800 --> 01:02:29,720
Speaker 7:  kind of doesn't see that effect that it, that it has on people.

1140
01:02:30,030 --> 01:02:33,200
Speaker 2:  I think after that they really started seeing that effect on people. I think

1141
01:02:33,200 --> 01:02:36,560
Speaker 2:  that if that happened to any workplace, it would change the dynamic of the

1142
01:02:36,560 --> 01:02:38,520
Speaker 2:  workplace. And that absolutely changed the dynamic of that

1143
01:02:38,520 --> 01:02:42,120
Speaker 7:  Workplace. There. There was a notable difference and, and I think the thing

1144
01:02:42,120 --> 01:02:45,640
Speaker 7:  that, that jumps out to me, just something, every time there's an executive

1145
01:02:45,640 --> 01:02:48,880
Speaker 7:  leaving, I'm wondering who is the next ex executive who is going to leave?

1146
01:02:48,920 --> 01:02:52,600
Speaker 7:  We've seen Reed Hastings step down and now Susan, Susan

1147
01:02:52,610 --> 01:02:54,880
Speaker 7:  is stepping down. What? Within a month. So

1148
01:02:54,880 --> 01:02:56,960
Speaker 5:  Are you saying they come in threes, they

1149
01:02:56,960 --> 01:02:58,040
Speaker 7:  Come in like fives,

1150
01:02:58,110 --> 01:02:58,920
Speaker 5:  It's them.

1151
01:02:58,920 --> 01:03:02,840
Speaker 2:  No, someone go ask Bing for a spooky ghost story about the rule

1152
01:03:02,840 --> 01:03:03,680
Speaker 2:  of threes and CEO

1153
01:03:04,080 --> 01:03:06,440
Speaker 7:  Stepping down Daniel Eck. That's my, that's my guess.

1154
01:03:06,550 --> 01:03:10,280
Speaker 2:  That's who you think he just did a big reorg. He got rid of the person who

1155
01:03:10,280 --> 01:03:13,880
Speaker 2:  did our big podcast push. I don't know. We'll see. That's, that's

1156
01:03:13,880 --> 01:03:16,440
Speaker 2:  Richard's. I have no prediction. I no, I don't know what's going on. I just

1157
01:03:16,440 --> 01:03:19,560
Speaker 2:  know that it's org chart season baby. I'd just like to say and that's my

1158
01:03:19,560 --> 01:03:22,880
Speaker 2:  time to shine Tim Cook. No way.

1159
01:03:23,600 --> 01:03:27,480
Speaker 2:  A little bit more lightning round stuff. iOS 16.4 beta hit.

1160
01:03:27,660 --> 01:03:31,560
Speaker 2:  I'm only bringing this up because there's now a new hook that lets web

1161
01:03:31,560 --> 01:03:33,280
Speaker 2:  apps send push notifications.

1162
01:03:33,730 --> 01:03:34,320
Speaker 5:  Oh boy.

1163
01:03:34,450 --> 01:03:38,280
Speaker 2:  Amazing. This is fully an EU compliance move. Like this is

1164
01:03:38,280 --> 01:03:42,080
Speaker 2:  so Apple can go to European regulators and see the web is just as

1165
01:03:42,080 --> 01:03:45,320
Speaker 2:  good as our native app platform. They can even send push notifications.

1166
01:03:45,320 --> 01:03:46,040
Speaker 5:  Oh I love that.

1167
01:03:46,470 --> 01:03:50,320
Speaker 2:  I promise you that's what's going on here. Our S 23 ultra review went up

1168
01:03:50,320 --> 01:03:54,080
Speaker 2:  Sweater alert. Allison loved it. It's it's a great phone. Showed

1169
01:03:54,180 --> 01:03:57,040
Speaker 2:  she was like the most interesting thing to do with the 200 megapixel camera.

1170
01:03:57,040 --> 01:04:00,880
Speaker 2:  Not take 200 megapixel photos. Which is a great line but go watch it. Really

1171
01:04:00,880 --> 01:04:01,120
Speaker 2:  cool.

1172
01:04:01,120 --> 01:04:04,680
Speaker 5:  She's been having a lo lovely time with the Zoom. She's super, super

1173
01:04:04,880 --> 01:04:05,360
Speaker 5:  enamored with

1174
01:04:05,360 --> 01:04:08,920
Speaker 2:  It. Yeah, she gave it a nine razor blade. 18 review is up.

1175
01:04:08,920 --> 01:04:10,080
Speaker 2:  Monica get add an

1176
01:04:10,080 --> 01:04:13,160
Speaker 5:  Eight. Did you know how much the razor blade 18 costs? How

1177
01:04:13,160 --> 01:04:13,640
Speaker 2:  Much does it cost?

1178
01:04:14,010 --> 01:04:15,760
Speaker 5:  So much money. Hold on. I just closed it.

1179
01:04:15,870 --> 01:04:17,880
Speaker 2:  $3,800. I got it. Okay.

1180
01:04:17,930 --> 01:04:21,760
Speaker 5:  It costs $3,800. Yeah that's, that's a lot of buddy

1181
01:04:21,760 --> 01:04:22,080
Speaker 5:  for that

1182
01:04:22,080 --> 01:04:23,760
Speaker 2:  Reason. But look at all the, all the

1183
01:04:23,760 --> 01:04:26,920
Speaker 5:  Lights. It's, yeah it's got lights, it's got a nice screen. She didn't hate

1184
01:04:26,920 --> 01:04:28,000
Speaker 5:  it for that price.

1185
01:04:28,000 --> 01:04:31,760
Speaker 2:  And then I, I wanna end on just like a little bit. Three pieces of car

1186
01:04:31,760 --> 01:04:35,600
Speaker 2:  news. Little news, little news. Tesla we're called like

1187
01:04:35,600 --> 01:04:39,080
Speaker 2:  362,000 vehicles because full self driving

1188
01:04:39,150 --> 01:04:43,120
Speaker 2:  beta has a crash risk. They're not the Tesla people are

1189
01:04:43,120 --> 01:04:45,280
Speaker 2:  gonna come for me. It's not a recall. They're gonna ship in over the air

1190
01:04:45,280 --> 01:04:49,160
Speaker 2:  update. Yeah. But like the government was like yo this is bad.

1191
01:04:49,160 --> 01:04:53,000
Speaker 2:  And Tesla's like we agree. We will push the button marked recall on the

1192
01:04:53,000 --> 01:04:56,040
Speaker 2:  dashboard. Yeah. And tell everyone that they shouldn't use this until they

1193
01:04:56,040 --> 01:04:56,440
Speaker 2:  update their

1194
01:04:56,600 --> 01:04:59,440
Speaker 5:  Software. Do they have to like get to wifi to do it?

1195
01:05:00,390 --> 01:05:01,840
Speaker 2:  A lot of Teslas have some modems.

1196
01:05:01,840 --> 01:05:05,200
Speaker 5:  They have. Okay. I've never, I don't have a Tesla. I don't know how they

1197
01:05:05,200 --> 01:05:05,320
Speaker 5:  work.

1198
01:05:05,320 --> 01:05:05,800
Speaker 2:  What are you doing?

1199
01:05:06,100 --> 01:05:07,320
Speaker 5:  I'm driving my little Mazda.

1200
01:05:07,960 --> 01:05:11,440
Speaker 2:  Well does your Mazda have more than 90 miles of electric range?

1201
01:05:11,440 --> 01:05:15,000
Speaker 2:  Because we reviewed the Mazda NX 30 this week. The world's

1202
01:05:15,000 --> 01:05:15,760
Speaker 2:  shittiest car.

1203
01:05:16,190 --> 01:05:19,960
Speaker 5:  I was so sad about that too. That was such a, I'm like

1204
01:05:19,960 --> 01:05:22,600
Speaker 5:  rooting for Mazda cuz I love my CX five. Yes.

1205
01:05:22,600 --> 01:05:26,200
Speaker 7:  It's such a sad life for the rotary engine. Like, like this is how the rotary

1206
01:05:26,250 --> 01:05:26,840
Speaker 7:  is back.

1207
01:05:28,890 --> 01:05:32,800
Speaker 2:  So we read the MX 30. I just wanna bring this up. We're in the midst of what

1208
01:05:32,800 --> 01:05:35,840
Speaker 2:  we think is an EV transition. Yeah. We could go either way

1209
01:05:36,720 --> 01:05:37,120
Speaker 2:  honestly at this

1210
01:05:37,120 --> 01:05:38,880
Speaker 5:  Point. Not with Mazda making the

1211
01:05:38,880 --> 01:05:40,920
Speaker 2:  Carts not right. That's what I mean. Like

1212
01:05:40,930 --> 01:05:42,680
Speaker 5:  We know which way it's going. If Mazda's

1213
01:05:42,680 --> 01:05:45,520
Speaker 2:  In charge, it's like you just like bring your gas tank to the Mazda dealership.

1214
01:05:45,610 --> 01:05:49,520
Speaker 2:  So the MX 30 s out, it's a little compact SUV and there's just

1215
01:05:49,520 --> 01:05:52,840
Speaker 2:  like a lot of debate over how much range you really need in a car. Yeah.

1216
01:05:52,840 --> 01:05:56,200
Speaker 2:  And like most Americans are like 700 miles in a shotgun. Like whatever,

1217
01:05:56,920 --> 01:06:00,200
Speaker 2:  whatever Americans think the number most manufacturers are settling on is

1218
01:06:00,200 --> 01:06:04,000
Speaker 2:  like 300 and some are lower. Like there's a lot of 270 mile EVs

1219
01:06:04,200 --> 01:06:04,280
Speaker 2:  floating

1220
01:06:04,280 --> 01:06:08,000
Speaker 5:  Around. That's because those car designers are on the east coast. Yeah. Like

1221
01:06:08,000 --> 01:06:10,960
Speaker 5:  you get to that middle part of the country and you're like no I need a thousand

1222
01:06:10,960 --> 01:06:11,280
Speaker 5:  miles.

1223
01:06:11,280 --> 01:06:15,040
Speaker 2:  Yeah. In a shotgun. Like can I get the rack on the back

1224
01:06:15,040 --> 01:06:18,760
Speaker 2:  outbound gas tanks on my F three 50 but whatever. So

1225
01:06:18,760 --> 01:06:22,200
Speaker 2:  two 70 is like ah it's a little weird. Yeah. You know and then you've got

1226
01:06:22,200 --> 01:06:24,800
Speaker 2:  kind of the new battery chemistries and like the lucid stuff that are way

1227
01:06:24,800 --> 01:06:28,600
Speaker 2:  higher. Mazda's like 92, straight up

1228
01:06:28,600 --> 01:06:32,040
Speaker 2:  92 miles range in this $20,000 car

1229
01:06:32,460 --> 01:06:36,440
Speaker 2:  and the interior surface is made of cork. Oh. Which

1230
01:06:36,870 --> 01:06:40,680
Speaker 2:  easily stains. And they're like yes we know but

1231
01:06:40,680 --> 01:06:44,560
Speaker 2:  we're, we're honoring our heritage as a cork manufacturer.

1232
01:06:44,670 --> 01:06:46,520
Speaker 2:  This is a real thing. They said

1233
01:06:47,620 --> 01:06:50,720
Speaker 5:  The patina, you love that pat, that cork patina.

1234
01:06:50,720 --> 01:06:54,480
Speaker 2:  I shit I've never been more like I understand the

1235
01:06:54,480 --> 01:06:58,320
Speaker 2:  cyber truck. Yeah. And it's wiper situation much

1236
01:06:58,320 --> 01:07:02,160
Speaker 2:  more than Mazda fully shipping an EV with 92 miles a range.

1237
01:07:02,160 --> 01:07:02,440
Speaker 2:  And

1238
01:07:02,440 --> 01:07:05,960
Speaker 5:  Like their hybrids are pretty nice. Yeah. People Is it the, the hybrid

1239
01:07:06,570 --> 01:07:07,160
Speaker 5:  CX 50

1240
01:07:07,160 --> 01:07:09,840
Speaker 2:  People like Mazdas. I'm just saying this car

1241
01:07:09,840 --> 01:07:12,280
Speaker 5:  Sucks. This car, ah Mazda do better.

1242
01:07:12,500 --> 01:07:16,160
Speaker 2:  The cork thing just got me were the, the answers but we have to honor our

1243
01:07:16,400 --> 01:07:17,480
Speaker 2:  heritages whatever.

1244
01:07:17,940 --> 01:07:21,040
Speaker 5:  Was it like, was it a cork? Is that how they started?

1245
01:07:21,040 --> 01:07:22,080
Speaker 2:  Well that's what they said.

1246
01:07:22,310 --> 01:07:23,240
Speaker 5:  I don't believe them.

1247
01:07:23,590 --> 01:07:26,800
Speaker 2:  I don't know man. They're only selling in California and if you ever been

1248
01:07:26,800 --> 01:07:29,280
Speaker 2:  in California you know that this is the least California compatible vehicle

1249
01:07:29,280 --> 01:07:29,720
Speaker 2:  in the entire

1250
01:07:29,720 --> 01:07:33,400
Speaker 5:  World. It began as Toyo Cork. See Koo Company

1251
01:07:33,400 --> 01:07:33,760
Speaker 5:  Limited.

1252
01:07:34,110 --> 01:07:36,600
Speaker 2:  Yeah. And after all this time they hadn't figured out how to make the cork

1253
01:07:36,600 --> 01:07:37,600
Speaker 2:  stain resistant Alex

1254
01:07:38,650 --> 01:07:40,560
Speaker 5:  Because cause they, they pivoted the cars.

1255
01:07:40,870 --> 01:07:44,480
Speaker 2:  Okay. So that's the Mazda. And then lastly the most hilarious car

1256
01:07:44,480 --> 01:07:48,200
Speaker 2:  update of them all. Hyundai and Kia have been forced to

1257
01:07:48,200 --> 01:07:52,000
Speaker 2:  update software on millions of their cars because a group of

1258
01:07:52,000 --> 01:07:55,760
Speaker 2:  TikTok ERs called the Kia Boys have done the Kia

1259
01:07:55,760 --> 01:07:58,800
Speaker 2:  Challenge where they teach people how to steal the cars

1260
01:07:59,250 --> 01:08:03,040
Speaker 2:  because they don't have lockouts on the ignition system. The

1261
01:08:03,040 --> 01:08:04,960
Speaker 2:  key, the boys by the way spelled it the

1262
01:08:04,960 --> 01:08:08,120
Speaker 5:  Z. Oh yeah. I mean you can't spell it any other way when you're called the

1263
01:08:08,120 --> 01:08:10,360
Speaker 5:  Kia boys, you're not

1264
01:08:10,360 --> 01:08:14,280
Speaker 2:  Allowed many 2015 to 2019 Hyundai and Kia vehicles lack electronic. Im

1265
01:08:14,280 --> 01:08:17,400
Speaker 2:  mobilizers to prevent these from simply breaking in and bypassing the igni

1266
01:08:17,400 --> 01:08:20,360
Speaker 2:  ignition. The Kia boys obviously

1267
01:08:21,200 --> 01:08:25,040
Speaker 2:  discovered this as the Kia boys are want to do. They're

1268
01:08:25,160 --> 01:08:28,800
Speaker 2:  stealing the cars. Here is how they're fixing it. They are updating the theft

1269
01:08:28,840 --> 01:08:32,520
Speaker 2:  alarm software logic to extend the length of the

1270
01:08:32,520 --> 01:08:35,560
Speaker 2:  alarm sound from 30 seconds to one minute.

1271
01:08:36,050 --> 01:08:38,160
Speaker 5:  That'll that, that that's gonna

1272
01:08:38,160 --> 01:08:42,000
Speaker 2:  Do. I dunno if you've ever tried to deter the Kia Boys, but if you go

1273
01:08:42,000 --> 01:08:44,320
Speaker 2:  over 30 seconds of alarm sounds they run,

1274
01:08:44,410 --> 01:08:48,360
Speaker 5:  Do they not just have some bows they can put on like some gear

1275
01:08:48,360 --> 01:08:49,560
Speaker 5:  plugs? I think it'll

1276
01:08:49,560 --> 01:08:53,200
Speaker 2:  Be fine. And if you have a Hyundai or a Kia that has a standard turn

1277
01:08:53,200 --> 01:08:55,960
Speaker 2:  the key to start, then they will lock

1278
01:08:55,960 --> 01:08:59,720
Speaker 5:  That up. So should you just get like a club instead? Was that the old one?

1279
01:08:59,720 --> 01:09:03,680
Speaker 5:  The the red bar Put on your, your steering wheel. That's what you need to

1280
01:09:03,680 --> 01:09:03,880
Speaker 5:  get in

1281
01:09:03,880 --> 01:09:07,560
Speaker 2:  Kia Boys. They've got a club. Let's get outta here. I mean

1282
01:09:07,560 --> 01:09:11,400
Speaker 2:  easily the best. There's like certain, we've run a couple of headlines this

1283
01:09:11,400 --> 01:09:15,280
Speaker 2:  week where I'm like this is what the Verge was designed for. Yeah. Bing

1284
01:09:15,280 --> 01:09:18,880
Speaker 2:  is an emotionally manipulative liar that people love. I was like, I'd said

1285
01:09:18,880 --> 01:09:22,560
Speaker 2:  to James, I am pretty sure like we, we were

1286
01:09:22,560 --> 01:09:24,680
Speaker 2:  made for this headline.

1287
01:09:24,680 --> 01:09:26,120
Speaker 5:  Yeah. A hundred percent.

1288
01:09:26,120 --> 01:09:29,880
Speaker 2:  Like this is why we exist. And the second one is the Kia

1289
01:09:29,880 --> 01:09:33,520
Speaker 2:  boys have forced Kia to issue a recall

1290
01:09:33,850 --> 01:09:37,200
Speaker 2:  because they're on TikTok teaching people how to sell cars.

1291
01:09:37,460 --> 01:09:41,360
Speaker 2:  The we were, we were destined. Yeah. In 2011

1292
01:09:41,360 --> 01:09:45,200
Speaker 2:  we're like we should all quit our jobs and start the verge in the back of

1293
01:09:45,200 --> 01:09:46,000
Speaker 2:  my mind just

1294
01:09:46,000 --> 01:09:46,840
Speaker 5:  Kia boys. The

1295
01:09:46,840 --> 01:09:47,480
Speaker 2:  Kia boys

1296
01:09:47,920 --> 01:09:48,920
Speaker 5:  Revving their engines.

1297
01:09:49,270 --> 01:09:53,080
Speaker 2:  It's like a viral TikTok challenge. We'll teach

1298
01:09:53,080 --> 01:09:56,640
Speaker 2:  people how to steal cars and a major car

1299
01:09:56,640 --> 01:10:00,120
Speaker 2:  manufacturer will have to deal with it. That's what we meant by technology

1300
01:10:00,120 --> 01:10:03,920
Speaker 2:  and culture. This is the cyberpunk future we were promised. And also the

1301
01:10:03,920 --> 01:10:07,400
Speaker 2:  cyberpunk future we were promised was like a very emotional robot will be

1302
01:10:07,400 --> 01:10:08,960
Speaker 2:  like, you're a bad bing.

1303
01:10:09,150 --> 01:10:10,520
Speaker 5:  I mean Yeah, that's true

1304
01:10:10,520 --> 01:10:14,040
Speaker 2:  Though. No, I'm a good bing. All right. I think you're all great Bings,

1305
01:10:14,230 --> 01:10:17,880
Speaker 2:  just for the record, you're all good chatbots and I love you all very much.

1306
01:10:18,150 --> 01:10:20,000
Speaker 2:  Please do not smooch your laptops.

1307
01:10:20,000 --> 01:10:21,840
Speaker 5:  Please don't. You don't know where it's been.

1308
01:10:22,750 --> 01:10:26,440
Speaker 2:  That would be a problem. Alright. Fear not the Kia boys

1309
01:10:26,930 --> 01:10:30,800
Speaker 2:  go off in the good night. There's other stuff. Read Liz this week on

1310
01:10:30,800 --> 01:10:34,200
Speaker 2:  like the, how the interest rate environment is like radically changing the

1311
01:10:34,200 --> 01:10:38,160
Speaker 2:  tech industry. It's classic Liz Le Patto. It's very good. Wednesday

1312
01:10:38,380 --> 01:10:41,920
Speaker 2:  you guys are gonna talk about the PSVR review with Addie Sean Hollister.

1313
01:10:41,950 --> 01:10:45,000
Speaker 5:  Yeah. Very exciting. So Sean and Addie are gonna come on. We're gonna talk

1314
01:10:45,000 --> 01:10:48,880
Speaker 5:  about the psvr. I've got Jen and Chris person are gonna

1315
01:10:48,880 --> 01:10:52,680
Speaker 5:  talk about multi room audio because Chris is super into not doing

1316
01:10:52,680 --> 01:10:56,160
Speaker 5:  the typical stuff. Ooh. He's got a whole hacked out system that he loves.

1317
01:10:56,420 --> 01:11:00,200
Speaker 5:  And then we're gonna be talking with Ariel about podcast misinformation

1318
01:11:00,340 --> 01:11:03,360
Speaker 5:  and by we, I mean she's gonna be talking to some great people about podcast

1319
01:11:03,360 --> 01:11:06,480
Speaker 5:  misinformation and I'm gonna be listening attentively.

1320
01:11:06,590 --> 01:11:10,480
Speaker 2:  Very good. Dakota. This week I talked to the chair of the

1321
01:11:10,480 --> 01:11:13,560
Speaker 2:  Mozilla Foundation, also the CEO of Mozilla whole org chart situation in

1322
01:11:13,560 --> 01:11:17,280
Speaker 2:  there. Here's something, she told me, people more likely to switch browsers

1323
01:11:17,280 --> 01:11:21,120
Speaker 2:  on their phones than on their laptops. I believe it. People are

1324
01:11:21,120 --> 01:11:24,880
Speaker 2:  used to downloading apps on their phones and just that little bit of behavioral

1325
01:11:24,880 --> 01:11:28,520
Speaker 2:  conditioning means that it's possible. I love it. So listen to that

1326
01:11:28,570 --> 01:11:31,400
Speaker 2:  decoder is just like lit up the next couple weeks. Yeah, just check it out.

1327
01:11:31,400 --> 01:11:34,040
Speaker 2:  Very good. Liam hates it when I say it, but he's busy switching the video

1328
01:11:34,040 --> 01:11:37,480
Speaker 2:  so he can't coming off. All right. That's it. Thanks to our guests. Thanks

1329
01:11:37,480 --> 01:11:41,320
Speaker 2:  to Zoe. She's at Zoe's Schiffer on Twitter. James is at JJ

1330
01:11:41,320 --> 01:11:45,120
Speaker 2:  Vincent. Addie is at the Dex. I'm at Reckless. Alex is

1331
01:11:45,120 --> 01:11:48,960
Speaker 2:  Alex h Cranz. Richard as always is at RJ cc. God only knows his

1332
01:11:48,960 --> 01:11:49,440
Speaker 2:  Twitter is up.

1333
01:11:50,190 --> 01:11:50,880
Speaker 1:  He's the

1334
01:11:50,880 --> 01:11:54,680
Speaker 2:  First to be banned. Someone boost me by a thousand. That's it.

1335
01:11:54,680 --> 01:11:55,240
Speaker 2:  Rock and roll.

1336
01:12:00,940 --> 01:12:04,560
Speaker 1:  And that's a wrap for Vergecast this week. We'd love to hear from you. Shoot

1337
01:12:04,560 --> 01:12:08,360
Speaker 1:  us an email@vergecasttheverge.com. The Vergecast is a

1338
01:12:08,360 --> 01:12:11,840
Speaker 1:  production of the Verge and the Box Media Podcast network. The show is produced

1339
01:12:11,840 --> 01:12:15,600
Speaker 1:  by me, Liam James, and our senior audio director, Andrew Marino.

1340
01:12:15,660 --> 01:12:19,480
Speaker 1:  Our editorial director is Brooke Min. That's it. We'll see you next

1341
01:12:19,480 --> 01:12:19,680
Speaker 1:  week.

