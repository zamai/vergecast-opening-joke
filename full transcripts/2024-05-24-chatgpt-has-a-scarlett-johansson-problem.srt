1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: f90c4cb2-6f0e-478e-a557-9d0308e39935
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/7867437718327820157/1869656059317631369/s93290-US-5690s-1716767986.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and David Pierce discuss announcements from Microsoft Build, the OpenAI's trouble with Scarlett Johansson, new Sonos headphones, and more.

2
00:01:14,525 --> 00:01:16,825
Speaker 2:  no matter what. It's

3
00:01:56,895 --> 00:01:58,675
Speaker 2:  and then we're gonna get your products

4
00:02:00,455 --> 00:02:04,315
Speaker 2:  and the truth is gonna come out. Right? Like that's the cycle that we're

5
00:02:04,315 --> 00:02:08,195
Speaker 2:  in is it's great. It's a we are super great video. I'm glad you

6
00:02:08,195 --> 00:02:09,395
Speaker 2:  paid all the influencers.

7
00:02:11,095 --> 00:02:15,035
Speaker 2:  I'm gonna put your face computer on my face man. And then we're gonna know.

8
00:02:15,035 --> 00:02:18,395
Speaker 2:  Yeah. I'm gonna hold your, I'm gonna hold your teenage engineering box

9
00:02:18,975 --> 00:02:19,835
Speaker 2:  and then we're gonna know

10
00:02:20,395 --> 00:02:24,275
Speaker 5:  I like it. 'cause it's one timeline darker than the, is this

11
00:02:24,555 --> 00:02:27,155
Speaker 5:  anything timeline? Yeah. And I feel like we were in the, is this anything

12
00:02:27,435 --> 00:02:30,835
Speaker 5:  timeline for a while and now it's, it's getting slightly Bleecker.

13
00:02:31,075 --> 00:02:33,635
Speaker 2:  Yeah. It's probably broken and I don't believe you. Yeah. We're gonna put

14
00:02:33,775 --> 00:02:36,915
Speaker 2:  AI in Google search and Google search is gonna tell you to put glue in your

15
00:02:36,915 --> 00:02:40,315
Speaker 2:  pizza, which is a real headline. Yeah. By the time you are listening to this,

16
00:02:40,835 --> 00:02:44,195
Speaker 2:  I think Kylie will have that headline up, which is AI previews and Google

17
00:02:44,195 --> 00:02:47,115
Speaker 2:  search are overwhelmingly stupid.

18
00:02:48,145 --> 00:02:51,715
Speaker 2:  Yeah. Mostly 'cause they're reading Reddit and Reddit is full of jokes

19
00:02:52,535 --> 00:02:56,115
Speaker 2:  and people are, are now like willfully

20
00:02:56,215 --> 00:02:59,725
Speaker 2:  trying to like Google bombing is back. Yeah.

21
00:03:00,395 --> 00:03:04,125
Speaker 2:  Because you ask Google like what mammal has the most

22
00:03:04,175 --> 00:03:07,565
Speaker 2:  bones, which is the real thing people are doing. And it goes and finds a

23
00:03:07,565 --> 00:03:10,005
Speaker 2:  Reddit joke from five years ago that's like, it's snakes.

24
00:03:12,505 --> 00:03:16,445
Speaker 5:  And it makes you realize like the the running joke on the internet is that

25
00:03:16,875 --> 00:03:20,805
Speaker 5:  sarcasm and jokes are very hard to understand. It turns out that like people

26
00:03:20,895 --> 00:03:24,685
Speaker 5:  still better at it than Google, which is real, real bad at it.

27
00:03:25,195 --> 00:03:28,085
Speaker 2:  It's probably broken and I don't believe you again, the theme of the week,

28
00:03:28,435 --> 00:03:32,405
Speaker 2:  there's some exciting news this week, like legitimately cool news this week.

29
00:03:33,215 --> 00:03:36,805
Speaker 2:  David, you talk to Tom about everything that happened at Microsoft Build

30
00:03:36,805 --> 00:03:40,765
Speaker 2:  and the Surface event. Legitimately cool news if that stuff works, the

31
00:03:40,865 --> 00:03:44,445
Speaker 2:  Mac versus PC race is fully back on. Yep. The processor

32
00:03:44,675 --> 00:03:48,645
Speaker 2:  wars are back on with a new metric. Right? Like they have, Microsoft

33
00:03:48,645 --> 00:03:52,165
Speaker 2:  has NPUs now with the Qualcomm chips doing ai. Apple has neural engines.

34
00:03:52,785 --> 00:03:54,485
Speaker 2:  That's all really exciting.

35
00:03:56,205 --> 00:04:00,025
Speaker 2:  Boy, do I want that stuff to work, right? Like it's

36
00:04:00,025 --> 00:04:03,985
Speaker 2:  just a we do you not feel like Open Eye week, we're gonna talk about Open

37
00:04:03,985 --> 00:04:07,745
Speaker 2:  Eye and Carl Johansen and it's like, yeah, I don't believe you.

38
00:04:08,735 --> 00:04:11,505
Speaker 2:  Like what a weird place to be.

39
00:04:12,365 --> 00:04:15,905
Speaker 5:  It is. And it also, you end up in this place of like questioning

40
00:04:15,905 --> 00:04:19,745
Speaker 5:  everybody's intentions all the time too. Like with, with Microsoft it's like,

41
00:04:19,805 --> 00:04:22,825
Speaker 5:  oh, windows on arm. You did it. That's so exciting. Do you remember all the

42
00:04:22,825 --> 00:04:25,545
Speaker 5:  other times that you told me that you did it? That that's really interesting.

43
00:04:25,685 --> 00:04:29,545
Speaker 5:  And then with all this OpenAI is stuff, the, the timelines are weird.

44
00:04:29,545 --> 00:04:33,465
Speaker 5:  Everybody is saying things that don't all make sense simultaneously

45
00:04:34,045 --> 00:04:37,825
Speaker 5:  and it's just, you get to the point where it's like, does anyone know what

46
00:04:37,825 --> 00:04:38,505
Speaker 5:  they're talking about?

47
00:04:40,605 --> 00:04:43,105
Speaker 5:  And it's, it's just very strange. But I, I agree with you on the Microsoft

48
00:04:43,105 --> 00:04:46,385
Speaker 5:  stuff in particular I thought was both

49
00:04:46,385 --> 00:04:50,345
Speaker 5:  substantially nerdier and substantially more exciting than I expected it

50
00:04:50,345 --> 00:04:53,785
Speaker 5:  to be. Like it was a wonky build in like the best way.

51
00:04:54,055 --> 00:04:54,345
Speaker 5:  Yeah.

52
00:04:54,345 --> 00:04:57,505
Speaker 2:  Let's start there. We we are gonna talk about OpenAI and scrolls your sen

53
00:04:57,505 --> 00:05:01,025
Speaker 2:  later. We're gonna have a lightning round unsponsored

54
00:05:02,365 --> 00:05:05,065
Speaker 2:  for now. We gotta start a shoe company that's called Unsponsored and then

55
00:05:05,065 --> 00:05:07,505
Speaker 2:  have the Lightning Round sponsored by Unsponsored.

56
00:05:07,785 --> 00:05:08,745
Speaker 5:  That's pretty good. If, you're

57
00:05:08,745 --> 00:05:11,145
Speaker 2:  A shoe investor. Call me. Is it

58
00:05:11,405 --> 00:05:15,065
Speaker 5:  All the logos or is it No logos. No logo on the shoe. I think it might be

59
00:05:15,065 --> 00:05:16,145
Speaker 5:  all the logos. Oh,

60
00:05:16,205 --> 00:05:18,345
Speaker 2:  Oh right. You like bring it all the way back around. Yeah.

61
00:05:18,415 --> 00:05:22,305
Speaker 5:  It's every shoe logo on it. And it's called Unsponsored Please. Like

62
00:05:22,545 --> 00:05:24,945
Speaker 5:  somebody at mischief is building that right now. They just heard me. You

63
00:05:24,945 --> 00:05:26,065
Speaker 5:  can have that mischief. Enjoy

64
00:05:26,725 --> 00:05:28,385
Speaker 2:  If. you sponsor logo If,

65
00:05:28,385 --> 00:05:29,425
Speaker 5:  You sponsor the around.

66
00:05:30,095 --> 00:05:32,985
Speaker 2:  Look, I'm not saying we're a bunch of IP experts on this show. I'm just saying

67
00:05:32,985 --> 00:05:36,745
Speaker 2:  If, you give us cash, we'll give you that idea. Okay. We,

68
00:05:36,745 --> 00:05:39,505
Speaker 2:  let's start with Build. 'cause it, it was an nerdy build and I think you

69
00:05:39,505 --> 00:05:42,585
Speaker 2:  guys only really scratched the surface with Tom who was there. Great job

70
00:05:42,905 --> 00:05:46,825
Speaker 2:  covering it. The big news is obviously the, the co-pilot PCs, right. Co-pilot

71
00:05:46,985 --> 00:05:50,745
Speaker 2:  plus PCs, which have the NPU, which are running on

72
00:05:51,245 --> 00:05:54,825
Speaker 2:  the new Snapdragon X processors. It should be noted that

73
00:05:55,045 --> 00:05:58,565
Speaker 2:  the team that built those processors at Qualcomm is

74
00:05:58,875 --> 00:06:02,765
Speaker 2:  from a company called Nuvia, which Qualcomm acquired. Nuvia

75
00:06:02,785 --> 00:06:06,445
Speaker 2:  is a bunch of ex Apple chip designers. You can see how they caught up.

76
00:06:07,065 --> 00:06:10,885
Speaker 2:  Yep. It's a pretty straight line. Right? You you, you're losing the apple.

77
00:06:11,225 --> 00:06:15,165
Speaker 2:  The Apple chip designers leave, you buy them. So you're competitive with

78
00:06:15,165 --> 00:06:19,085
Speaker 2:  Apple. Again, we gotta run the benchmarks, we gotta get the

79
00:06:19,085 --> 00:06:21,885
Speaker 2:  things. But Kranz, I mean you're, you're a chip nerd. Do you, do you buy

80
00:06:21,885 --> 00:06:22,285
Speaker 2:  it right now?

81
00:06:23,485 --> 00:06:27,245
Speaker 6:  I mean I I think it's the nuvia part of it that makes me kind of cautiously

82
00:06:27,335 --> 00:06:31,165
Speaker 6:  optimistic. And also the fact that laptop makers actually want us

83
00:06:31,165 --> 00:06:33,405
Speaker 6:  to try the laptops, generally

84
00:06:33,725 --> 00:06:34,605
Speaker 2:  Speaking. Oh yeah. That's a big one.

85
00:06:34,915 --> 00:06:38,685
Speaker 6:  They do not when it sucks. They're like, you, you, are you sure you want

86
00:06:38,685 --> 00:06:41,525
Speaker 6:  this? and they slow roll you, right? Yeah. Like they, they wait a while to

87
00:06:41,525 --> 00:06:43,885
Speaker 6:  get it to you and then you end up having to buy it and review it yourself.

88
00:06:44,145 --> 00:06:46,765
Speaker 6:  And this time they're like, when do you want it? We'll get it to you next

89
00:06:46,765 --> 00:06:50,365
Speaker 6:  week. They're, they're really eager about it. And, and in a way that like

90
00:06:51,235 --> 00:06:54,405
Speaker 6:  surprised me. I took a few briefings on the laptops and I was like, oh, you

91
00:06:54,405 --> 00:06:58,245
Speaker 6:  guys are like pumped. I'm not used to that. And it reminds me a lot of when

92
00:06:58,685 --> 00:07:02,285
Speaker 6:  a MD was like coming back when it was making its big comeback what, in like

93
00:07:02,285 --> 00:07:06,005
Speaker 6:  2017 where suddenly people were like, do you care about a MD? Like get

94
00:07:06,005 --> 00:07:08,965
Speaker 6:  excited. And I was like, oh, oh you, this is weird. But

95
00:07:09,875 --> 00:07:13,565
Speaker 6:  yeah, the the, the NPUs is the big, is the big deal here. I think

96
00:07:13,795 --> 00:07:17,485
Speaker 6:  like Microsoft said, If, you wanna take

97
00:07:17,485 --> 00:07:21,285
Speaker 6:  advantage of the, the most stuff that's gonna be in Windows now you

98
00:07:21,525 --> 00:07:24,885
Speaker 6:  probably want an NPU in that computer. And NPUs have been around for a while.

99
00:07:24,885 --> 00:07:27,805
Speaker 6:  Like Intel's had them. They're, they're called neural processing units. Everybody's

100
00:07:27,805 --> 00:07:31,605
Speaker 6:  been doing it. You know, apple calls it a neural engine, whatever. But

101
00:07:31,605 --> 00:07:35,285
Speaker 6:  they've all been doing it and it's basically just another processor to offload

102
00:07:35,455 --> 00:07:38,965
Speaker 6:  those like tasks that generally you would've put on your GPU

103
00:07:39,465 --> 00:07:43,125
Speaker 6:  or like made your CPU kind of brute force. And now it's like, okay,

104
00:07:43,475 --> 00:07:47,365
Speaker 6:  just for this. And it's like

105
00:07:47,365 --> 00:07:50,245
Speaker 6:  when Intel introduced it a couple of years ago, it was one of those things

106
00:07:50,245 --> 00:07:53,565
Speaker 6:  where you're like, okay, do I care about this? Like I'm not an AI researcher,

107
00:07:53,565 --> 00:07:57,445
Speaker 6:  why should I care about this? And that's still kind of the

108
00:07:57,765 --> 00:08:00,925
Speaker 6:  question unless now I think the big change is that like

109
00:08:01,595 --> 00:08:05,045
Speaker 6:  Adobe's gonna be using it and a lot of these like film processing companies

110
00:08:05,045 --> 00:08:08,605
Speaker 6:  and stuff are doing it. So it's like, okay, I guess it makes a little more

111
00:08:08,605 --> 00:08:12,285
Speaker 6:  sense now than it did, you know, six years ago. What do you guys feel about

112
00:08:12,285 --> 00:08:12,445
Speaker 6:  that?

113
00:08:13,435 --> 00:08:17,165
Speaker 2:  Well, I'm curious. I mean a lot of what they announced in

114
00:08:17,165 --> 00:08:21,005
Speaker 2:  Windows is Yeah, fairly interesting. Some of it

115
00:08:21,045 --> 00:08:24,405
Speaker 2:  I think people have had a really strong reaction to like recall, but some

116
00:08:24,405 --> 00:08:27,605
Speaker 2:  of it is just straightforwardly cool. Right? Like you can draw stick figures

117
00:08:27,605 --> 00:08:31,485
Speaker 2:  in Microsoft Paint and it will generate more

118
00:08:31,485 --> 00:08:35,365
Speaker 2:  photorealistic images as you draw. That's cool. Yeah.

119
00:08:35,365 --> 00:08:39,005
Speaker 2:  That's neat. And that's one of the first examples of like

120
00:08:39,765 --> 00:08:43,645
Speaker 2:  a cool local AI workload beyond the light rooms

121
00:08:44,025 --> 00:08:47,205
Speaker 2:  and premieres of the world that I can think of

122
00:08:47,795 --> 00:08:51,725
Speaker 2:  because most of the other AI workloads like happen in the cloud. Yeah. Right.

123
00:08:51,725 --> 00:08:54,605
Speaker 2:  You like, you talk to a chat bot and they happen in the cloud or you're coding

124
00:08:54,605 --> 00:08:58,365
Speaker 2:  and GitHub and co-pilot and GitHub is happening in the cloud and now you've

125
00:08:58,365 --> 00:09:01,605
Speaker 2:  got some cool stuff that's happening locally. We should talk about recall,

126
00:09:01,605 --> 00:09:05,525
Speaker 2:  which is the other big thing that's happening on the NPU. But they're not

127
00:09:05,555 --> 00:09:09,195
Speaker 2:  more of those that, I mean like on like they're not more of those.

128
00:09:09,195 --> 00:09:12,315
Speaker 2:  There's the other stuff that's like you're playing Minecraft and the computers

129
00:09:12,555 --> 00:09:15,595
Speaker 2:  watching with you and talking you through how to play Minecraft,

130
00:09:16,415 --> 00:09:20,355
Speaker 2:  but I suspect there's a little bit of hybrid local

131
00:09:20,525 --> 00:09:24,155
Speaker 2:  cloud happening there. I know it's happening with paint because

132
00:09:25,245 --> 00:09:29,195
Speaker 2:  there are things paint won't let you generate because Microsoft's

133
00:09:29,195 --> 00:09:32,915
Speaker 2:  commitment to AI safety. Yeah. Which is weird. 'cause If, you turn off the

134
00:09:33,015 --> 00:09:35,915
Speaker 2:  ai you can draw any kind of dongs you want and paint.

135
00:09:37,225 --> 00:09:40,995
Speaker 2:  Yeah. Yeah. So there there is like a little bit of push and pull

136
00:09:40,995 --> 00:09:43,875
Speaker 2:  between the cloud and local, right? Yeah. In in just in terms of safety,

137
00:09:44,085 --> 00:09:47,795
Speaker 2:  which is really interesting to think about. Like Google Docs is a cloud application.

138
00:09:47,795 --> 00:09:51,675
Speaker 2:  You can type anything you want in Google Docs. Microsoft Word 365 can

139
00:09:51,675 --> 00:09:55,235
Speaker 2:  be expressed as a cloud application. You can type anything you want in

140
00:09:55,515 --> 00:09:59,355
Speaker 2:  Microsoft Word 365, you turn on AI features in Microsoft

141
00:09:59,355 --> 00:10:03,315
Speaker 2:  Paint. It's like you can't draw some stuff weird. That's just weird.

142
00:10:03,345 --> 00:10:06,595
Speaker 2:  Yeah. You know, and like I suspect we're gonna have a a long conversation

143
00:10:06,595 --> 00:10:10,395
Speaker 2:  about those kinds of lines for years to come. But that part, just

144
00:10:10,395 --> 00:10:14,275
Speaker 2:  the other part of it where it's like, where's the AI happening? Is it in

145
00:10:14,275 --> 00:10:18,235
Speaker 2:  the cloud or is it here on the machine? It's kind of interesting that Microsoft's

146
00:10:18,235 --> 00:10:20,035
Speaker 2:  answer is very much like both. Yeah.

147
00:10:20,345 --> 00:10:23,875
Speaker 5:  Well, and I think that's the right answer, right? A again, the the where

148
00:10:23,875 --> 00:10:27,755
Speaker 5:  one ends and the other begins is I think forever going to be

149
00:10:27,755 --> 00:10:31,675
Speaker 5:  complicated, but everyone I talk to says that the place we have to

150
00:10:31,675 --> 00:10:35,555
Speaker 5:  get to is both because the stuff that you do on device is

151
00:10:35,705 --> 00:10:39,435
Speaker 5:  private and fast in a way that anything that happens off

152
00:10:39,435 --> 00:10:43,155
Speaker 5:  device just can't be. And I think about like the, the translation stuff

153
00:10:43,435 --> 00:10:46,235
Speaker 5:  Microsoft is trying to do. Like there's a chunk of that that If, you don't

154
00:10:46,235 --> 00:10:49,555
Speaker 5:  do it locally, it's going to suck because real time

155
00:10:49,555 --> 00:10:52,995
Speaker 5:  translation that has to go to the cloud is by definition not real time

156
00:10:52,995 --> 00:10:56,635
Speaker 5:  translation. It just breaks. And so the thing that they launched where like

157
00:10:56,695 --> 00:11:00,555
Speaker 5:  it you can be in edge and it can translate a

158
00:11:00,555 --> 00:11:04,355
Speaker 5:  YouTube video while you're watching the YouTube video. Very

159
00:11:04,355 --> 00:11:08,075
Speaker 5:  cool. In theory. Awful with a half second delay.

160
00:11:08,755 --> 00:11:12,715
Speaker 5:  Like it's the sort of thing that I think is is that tuning that they're gonna

161
00:11:12,715 --> 00:11:15,595
Speaker 5:  have to do to get those two pieces right, is actually like the whole ball

162
00:11:15,595 --> 00:11:18,835
Speaker 5:  game. Yeah. But I think it's very cool that Microsoft is is deep in this

163
00:11:18,835 --> 00:11:22,275
Speaker 5:  idea of we're going to start on your device. And I think that's

164
00:11:22,565 --> 00:11:23,395
Speaker 5:  smart and right.

165
00:11:23,655 --> 00:11:26,835
Speaker 2:  And I think the other interesting part there is obviously we're, you know,

166
00:11:26,835 --> 00:11:30,355
Speaker 2:  we're barreling towards wwc where we expect Apple to announce many, many

167
00:11:30,355 --> 00:11:34,275
Speaker 2:  similar kinds of things, or at least a similar kind of focus

168
00:11:34,275 --> 00:11:38,235
Speaker 2:  on ai. Microsoft has the big AI

169
00:11:38,345 --> 00:11:42,325
Speaker 2:  part, they've got Azure and the big OpenAI didn

170
00:11:42,325 --> 00:11:45,365
Speaker 2:  relationship and they are integrating all this stuff and they have their

171
00:11:45,365 --> 00:11:48,765
Speaker 2:  own models that they're running at all sorts of scales. And so they're, I

172
00:11:48,765 --> 00:11:52,365
Speaker 2:  think they're able to just like take more shots across Windows and

173
00:11:52,365 --> 00:11:56,285
Speaker 2:  devices and, and their suite of applications that runs everywhere. And that

174
00:11:56,285 --> 00:11:58,965
Speaker 2:  to me, that to me is like the interesting part. Like there isn't another

175
00:12:00,115 --> 00:12:02,805
Speaker 2:  company that kind of has all the pieces,

176
00:12:03,635 --> 00:12:07,465
Speaker 2:  whether Microsoft can execute, whether it can deliver finished

177
00:12:07,465 --> 00:12:10,945
Speaker 2:  applications that people like to use, whether it can make people interested

178
00:12:10,945 --> 00:12:14,185
Speaker 2:  in Windows, like consumers interested in Windows outside of a gaming context.

179
00:12:14,275 --> 00:12:14,625
Speaker 2:  Again,

180
00:12:16,715 --> 00:12:19,155
Speaker 2:  I think it's broken and I don't believe you like, that's that's where I'm

181
00:12:19,155 --> 00:12:22,315
Speaker 2:  at. You know, it's like you gotta, we have to get the machines. It's interesting

182
00:12:22,315 --> 00:12:25,555
Speaker 2:  Alex that you're saying that the companies are excited, but there's a lot

183
00:12:25,555 --> 00:12:29,155
Speaker 2:  of investment in like Intel chips in the world, the next 86 applications.

184
00:12:29,615 --> 00:12:33,555
Speaker 2:  And that transition's gonna be slow, not fast. I I think they did say they

185
00:12:33,555 --> 00:12:34,915
Speaker 2:  did say stuff about emulation, right?

186
00:12:35,085 --> 00:12:38,875
Speaker 6:  Intel chips have had NPU in them for a very long time. They, they introduced

187
00:12:38,955 --> 00:12:42,395
Speaker 6:  a couple of years ago. So the big difference is now co-pilot

188
00:12:42,855 --> 00:12:46,155
Speaker 6:  has like kind of a minimum amount of what your NPU should be able to process.

189
00:12:46,695 --> 00:12:50,555
Speaker 6:  So the deal is that before, you know, an NPU would do

190
00:12:50,555 --> 00:12:54,475
Speaker 6:  it, like what, 18 tops or something, which is

191
00:12:54,475 --> 00:12:58,235
Speaker 6:  like trillion operations per second. So, so it would do like 18 and now they're

192
00:12:58,235 --> 00:13:01,275
Speaker 6:  saying, okay, it has to be 40, it has to be 45. And and that was one of the

193
00:13:01,275 --> 00:13:04,995
Speaker 6:  things like Apple was bringing up tops in their M four presentation

194
00:13:05,675 --> 00:13:09,435
Speaker 6:  back with the iPad where they were like, we can do 30, 38

195
00:13:09,535 --> 00:13:13,315
Speaker 6:  tops. And you're like, cool, what does that mean? This is great. And, and

196
00:13:13,675 --> 00:13:15,715
Speaker 6:  then Microsoft was just like, you gotta do 40. And we're like,

197
00:13:16,395 --> 00:13:17,035
Speaker 2:  I. love it. Cool.

198
00:13:17,335 --> 00:13:21,075
Speaker 5:  Tom used to do megapixels in my favorite possible way. Yes. This rule.

199
00:13:21,215 --> 00:13:23,355
Speaker 2:  Yes. We're back to like the megahertz war or whatever.

200
00:13:24,145 --> 00:13:26,755
Speaker 6:  Yeah. It's gonna be super goofy, but they're like

201
00:13:26,755 --> 00:13:27,795
Speaker 2:  Polar are here on

202
00:13:27,795 --> 00:13:31,155
Speaker 6:  The top. They're really leaning on that. 'cause I guess you'd need, they,

203
00:13:31,155 --> 00:13:34,075
Speaker 6:  like they're saying, you know, a lot of the stuff that's that's coming to

204
00:13:34,075 --> 00:13:37,675
Speaker 6:  Windows will work Okay. On older Windows

205
00:13:38,095 --> 00:13:41,955
Speaker 6:  PCs, but like you're really gonna need those, the the neuro processors. Well,

206
00:13:41,955 --> 00:13:42,835
Speaker 6:  but also and really good

207
00:13:42,835 --> 00:13:46,155
Speaker 5:  Ones. Importantly also your computer will be good, right.

208
00:13:47,205 --> 00:13:51,195
Speaker 5:  Which is not a small thing. Right. The idea of all

209
00:13:51,195 --> 00:13:54,115
Speaker 5:  of these things, like Alex, you said this before, like if you're an AI researcher,

210
00:13:54,145 --> 00:13:57,635
Speaker 5:  it's a, it's a nice to have, but it is going to in other ways

211
00:13:58,025 --> 00:14:01,835
Speaker 5:  make your computer worse. And the promise that Microsoft came out with,

212
00:14:01,835 --> 00:14:05,435
Speaker 5:  with the copilot plus PCs is these computers are going to be fast. They're

213
00:14:05,435 --> 00:14:08,795
Speaker 5:  going to do AI stuff, they're going to have long battery life and they're

214
00:14:08,795 --> 00:14:12,755
Speaker 5:  going to be good computers. And that is not a small thing in

215
00:14:12,755 --> 00:14:16,595
Speaker 5:  the Windows world right now, in which you've kind of had to choose one or

216
00:14:16,595 --> 00:14:20,555
Speaker 5:  two of those three things for a really long time. And especially

217
00:14:20,555 --> 00:14:24,515
Speaker 5:  with this Qualcomm stuff, if they can get the battery life there to the

218
00:14:24,515 --> 00:14:28,435
Speaker 5:  point where AI is not like, you don't have to buy an AI computer the

219
00:14:28,435 --> 00:14:31,715
Speaker 5:  way you buy a gaming computer and you like make a bunch of sacrifices in

220
00:14:31,875 --> 00:14:34,955
Speaker 5:  exchange for this one thing. You want to work really well if they can actually

221
00:14:34,985 --> 00:14:38,875
Speaker 5:  have solved these three things together. That to me is where the

222
00:14:38,875 --> 00:14:42,755
Speaker 5:  stuff gets really powerful. And then when, and then you get the whole scale

223
00:14:42,775 --> 00:14:46,395
Speaker 5:  of Microsoft and the fact that it has the app store and the browser

224
00:14:46,615 --> 00:14:50,595
Speaker 5:  and the os and the models and the cloud service. Like that's

225
00:14:50,595 --> 00:14:54,245
Speaker 5:  when that becomes really powerful. If and only if the devices don't suck

226
00:14:55,065 --> 00:14:57,045
Speaker 5:  and maybe possibly the devices don't suck.

227
00:14:57,235 --> 00:15:00,005
Speaker 6:  Well if they don't suck and they're, they're affordable enough, right? Yeah.

228
00:15:00,005 --> 00:15:03,845
Speaker 6:  Like most people are spending $700 on a laptop and most of these laptops

229
00:15:03,845 --> 00:15:07,165
Speaker 6:  are in the $1,200 range, which is intentional. They're, they're meant to

230
00:15:07,165 --> 00:15:10,965
Speaker 6:  go after the MacBook Air because MacBook airs sell a lot better

231
00:15:11,075 --> 00:15:13,555
Speaker 6:  than everybody else's laptops.

232
00:15:13,805 --> 00:15:17,115
Speaker 2:  Right? Right. And what's interesting is, you know, Sasha and CEO of Microsoft

233
00:15:17,115 --> 00:15:21,035
Speaker 2:  talked to Joanna Stern at the Wall Street Journal, notable for expat, Joanna

234
00:15:21,035 --> 00:15:21,715
Speaker 2:  Stern I might add,

235
00:15:23,535 --> 00:15:26,395
Speaker 2:  and basically was like, yeah, we're gonna beat the MacBook Air, Yusef Medi,

236
00:15:26,405 --> 00:15:30,355
Speaker 2:  who's executive vice president of Microsoft basically is

237
00:15:30,355 --> 00:15:33,835
Speaker 2:  like a MacBook error with an M three. These computers will beat those by

238
00:15:33,835 --> 00:15:37,675
Speaker 2:  50% on cbe. So they're making huge performance claims

239
00:15:37,815 --> 00:15:41,115
Speaker 2:  on these PCs. Right. They're also thin and light. There's some question I

240
00:15:41,115 --> 00:15:43,915
Speaker 2:  think John Gruber raised about whether they have fans as though people care,

241
00:15:44,335 --> 00:15:44,555
Speaker 2:  but

242
00:15:44,785 --> 00:15:46,275
Speaker 6:  They do have, some of them have fans,

243
00:15:46,275 --> 00:15:49,675
Speaker 2:  Some of them are gonna have fans. Yeah, fine. But MacBooks have thermal

244
00:15:49,675 --> 00:15:52,635
Speaker 2:  throttling issues, particularly the error. Right. They slow, they do slow

245
00:15:52,635 --> 00:15:56,555
Speaker 2:  down. So If, you wanna run these computers really hard with AI

246
00:15:56,755 --> 00:16:00,235
Speaker 2:  workloads, which are essentially on a GPU like piece of the processor.

247
00:16:00,505 --> 00:16:03,715
Speaker 2:  They make it hot, but we're expecting good battery life.

248
00:16:04,855 --> 00:16:06,575
Speaker 2:  I, and then the list of

249
00:16:08,215 --> 00:16:11,755
Speaker 2:  the list of manufacturers here is Dell, Lenovo, Samsung, hp, ASER as

250
00:16:12,545 --> 00:16:16,515
Speaker 2:  it's everybody. Microsoft obviously has a surface pro and the surface that

251
00:16:16,535 --> 00:16:18,435
Speaker 2:  that's the suite, right? Yeah. There isn't

252
00:16:19,995 --> 00:16:23,375
Speaker 2:  how Alex you taken the briefings. How are they differentiating these is just

253
00:16:23,375 --> 00:16:27,335
Speaker 2:  like, are you a doll fan boy or a Seuss fan boy and they still the same

254
00:16:27,335 --> 00:16:27,655
Speaker 2:  concern.

255
00:16:27,725 --> 00:16:30,935
Speaker 6:  Yeah. Kind of like everybody's, everybody's bringing up their own thing.

256
00:16:31,015 --> 00:16:34,695
Speaker 6:  I took an HP briefing and they rebranded their whole thing. Right?

257
00:16:34,895 --> 00:16:38,615
Speaker 6:  'cause you had like the HP Specter and the HP Pavilion and the

258
00:16:38,875 --> 00:16:42,815
Speaker 6:  HP something you've never heard of. Like, like it was just a mess. and they

259
00:16:43,015 --> 00:16:46,815
Speaker 6:  redid their whole portfolio into the elite book and the, the

260
00:16:46,875 --> 00:16:50,615
Speaker 6:  pro book for, for corporates or corporate stuff.

261
00:16:50,795 --> 00:16:54,495
Speaker 6:  And then the omni book, which is for the rest of us. And the omni book is

262
00:16:54,495 --> 00:16:57,495
Speaker 6:  like, I, I got to play around with one. It was really nice. Didn't get to

263
00:16:57,495 --> 00:17:00,615
Speaker 6:  benchmark or anything like that. So again, we're gonna have to wait and see,

264
00:17:00,795 --> 00:17:03,455
Speaker 6:  but it was like, it was a nice laptop. It felt like

265
00:17:05,555 --> 00:17:09,355
Speaker 6:  exciting and good in a way. It felt kind of like my iPad Pro with the,

266
00:17:09,815 --> 00:17:12,915
Speaker 6:  the thing on it, the the the magic keyboard only

267
00:17:13,665 --> 00:17:14,875
Speaker 6:  lighter and better.

268
00:17:14,975 --> 00:17:17,835
Speaker 2:  Oh, the the surface keyboard looks awesome. Right? 'cause it's totally wireless.

269
00:17:18,105 --> 00:17:18,395
Speaker 2:  Yeah.

270
00:17:18,395 --> 00:17:21,715
Speaker 6:  Like that's always kind of the way, right? Like Microsoft has done this before

271
00:17:21,715 --> 00:17:25,435
Speaker 6:  where they go and they say, all your OEMs, you're gonna have to like swarm

272
00:17:25,435 --> 00:17:28,635
Speaker 6:  around this. It's gonna be the netbook or, or touch computers or whatever.

273
00:17:28,635 --> 00:17:31,235
Speaker 6:  But you're gonna have to start making these touch computers. They're just

274
00:17:31,235 --> 00:17:34,915
Speaker 6:  like, yeah, you gotta, you gotta touch the screen. Right? And, and, and this

275
00:17:34,915 --> 00:17:37,955
Speaker 6:  case, this is the swarm around the dumbest name alive.

276
00:17:38,895 --> 00:17:42,875
Speaker 6:  But, but they recognize apple's coming for us. You know,

277
00:17:42,875 --> 00:17:46,155
Speaker 6:  everybody to your, your point Neela, you say this all the time, the computer

278
00:17:46,215 --> 00:17:48,835
Speaker 6:  is no longer about the rest of the computer. It's about the, the browser.

279
00:17:49,585 --> 00:17:53,525
Speaker 6:  And, and Microsoft has to compete with that. Like that is a very, that

280
00:17:53,525 --> 00:17:57,005
Speaker 6:  is an existential crisis for Microsoft, right? If everybody just says, okay,

281
00:17:57,005 --> 00:18:00,965
Speaker 6:  I just need a Chromebook or whatever my phone, that's bad for them. Yeah.

282
00:18:00,965 --> 00:18:03,525
Speaker 6:  So that, that, that's why they've put all this AI in. That's why they're

283
00:18:03,525 --> 00:18:07,125
Speaker 6:  really like glomming onto this trend at the moment.

284
00:18:07,665 --> 00:18:10,165
Speaker 6:  And I don't know the, these things will be really interesting. I'm really

285
00:18:10,195 --> 00:18:14,165
Speaker 6:  curious to see how fast they are because Qualcomm's made these

286
00:18:14,165 --> 00:18:17,645
Speaker 6:  promises before, but this time they've got like this whole new team behind

287
00:18:17,645 --> 00:18:20,805
Speaker 6:  it, which like, HP was like, well, you know, 'cause I was like, what's different

288
00:18:20,805 --> 00:18:22,645
Speaker 6:  this time you guys have released some real dog shit?

289
00:18:23,195 --> 00:18:24,685
Speaker 2:  Like it's probably broken and I don't believe you.

290
00:18:25,195 --> 00:18:29,005
Speaker 6:  Yeah. And HP was like, well it's a new team over there. Yeah. So like, we're

291
00:18:29,005 --> 00:18:32,325
Speaker 6:  feeling confident. And I was like, okay, well that's, that's different. I

292
00:18:32,325 --> 00:18:35,605
Speaker 6:  mean, but we'll see. Right. You just kind of have to, to hold your breath

293
00:18:35,985 --> 00:18:36,205
Speaker 6:  and

294
00:18:36,205 --> 00:18:39,605
Speaker 2:  Yeah. Look, this is the thing about tech coverage and I, I feel like we have

295
00:18:39,605 --> 00:18:43,525
Speaker 2:  said this many, many times, but this is like a new era and we

296
00:18:43,525 --> 00:18:46,805
Speaker 2:  just need to repeat ourselves. Eventually the products ship

297
00:18:47,545 --> 00:18:51,005
Speaker 2:  and then there's no hiding and there's only two kinds of

298
00:18:51,485 --> 00:18:54,645
Speaker 2:  coverage in the world where the truth just outs it's sports

299
00:18:55,175 --> 00:18:59,005
Speaker 2:  where one team wins and another team loses. And

300
00:18:59,005 --> 00:19:02,925
Speaker 2:  it's tech where you can hold the thing in your hand and it's,

301
00:19:03,265 --> 00:19:07,045
Speaker 2:  it works or it doesn't. Yep. And like a lot

302
00:19:07,065 --> 00:19:09,085
Speaker 2:  of things have not worked recently.

303
00:19:10,745 --> 00:19:14,445
Speaker 2:  We are on an all time run of things not working. Yes. And a lot of the reason

304
00:19:14,445 --> 00:19:18,005
Speaker 2:  that things don't work is the underlying AI models are not

305
00:19:18,445 --> 00:19:21,605
Speaker 2:  reliable, which is something Alex you have been pointing out now for a couple

306
00:19:21,605 --> 00:19:25,445
Speaker 2:  weeks. It, you we're making these huge bets and like,

307
00:19:26,185 --> 00:19:29,835
Speaker 2:  it's like betting on a toddler. Like, like, are you gonna,

308
00:19:30,015 --> 00:19:32,875
Speaker 2:  are you gonna stand up for a long time? Are you just gonna tip over like

309
00:19:32,925 --> 00:19:34,355
Speaker 2:  let's build a business around you?

310
00:19:35,305 --> 00:19:38,275
Speaker 6:  What was interesting was like some of the AI stuff that Microsoft was doing

311
00:19:38,375 --> 00:19:41,515
Speaker 6:  is stuff that we've already seen before, right? Like, like HP and a lot of

312
00:19:41,515 --> 00:19:45,155
Speaker 6:  these PE companies said, okay, you're gonna buy one of these IPCs,

313
00:19:45,425 --> 00:19:48,995
Speaker 6:  what the hell does that actually mean practically? And in

314
00:19:49,225 --> 00:19:52,795
Speaker 6:  HP's case, they were like, we've slapped it, we've like thrown in the chat

315
00:19:52,955 --> 00:19:56,235
Speaker 6:  GPT prompt window so you can talk to chat GPT-3 0.5

316
00:19:56,735 --> 00:19:59,515
Speaker 6:  on an HP computer. And I was like, cool, I don't, I don't care

317
00:19:59,515 --> 00:20:02,035
Speaker 5:  About that. That's not the answer. That's the answer. It is definitively

318
00:20:02,035 --> 00:20:02,635
Speaker 5:  the, not

319
00:20:02,635 --> 00:20:04,675
Speaker 6:  The answer. But then the other thing they were doing was they were like,

320
00:20:04,675 --> 00:20:08,635
Speaker 6:  okay, well you can also, like we, we can learn more about the computer to

321
00:20:08,635 --> 00:20:11,515
Speaker 6:  better optimize it. And that's something that we've seen from NVIDIA's been

322
00:20:11,515 --> 00:20:15,115
Speaker 6:  doing that for years now with DLSS. And so it's like, okay, well that's actually

323
00:20:16,015 --> 00:20:19,875
Speaker 6:  useful and like a use of AI that I've seen

324
00:20:19,895 --> 00:20:23,755
Speaker 6:  in practice and seen at work. Yeah. So I could almost get excited about that.

325
00:20:24,155 --> 00:20:27,155
Speaker 5:  I also think that's where recall comes in Yeah. And is actually really important

326
00:20:27,295 --> 00:20:30,675
Speaker 5:  to this whole thing because what I think Microsoft did well,

327
00:20:31,065 --> 00:20:34,515
Speaker 5:  both at the surface event on Monday and at Build on Tuesday, is

328
00:20:34,945 --> 00:20:38,555
Speaker 5:  give people examples of what AI can do on their computer.

329
00:20:38,895 --> 00:20:42,835
Speaker 5:  Yes. And, and the answer is not a chat bot that you

330
00:20:42,835 --> 00:20:45,955
Speaker 5:  could find at a website like that isn't, that's nothing. You have accomplished

331
00:20:45,955 --> 00:20:49,925
Speaker 5:  nothing if that's what you've done. But what Microsoft said is they, and

332
00:20:49,925 --> 00:20:52,805
Speaker 5:  it was a lot of little things in spots, right? Where they're like AI copy

333
00:20:52,805 --> 00:20:56,525
Speaker 5:  and paste, copy in one language, paste in another. Like it's you, you kind

334
00:20:56,525 --> 00:21:00,485
Speaker 5:  of have to do a million of those in order to make this case. But I think

335
00:21:00,505 --> 00:21:04,445
Speaker 5:  recall was the big swing where it was like, okay, here is one genuinely

336
00:21:05,065 --> 00:21:08,805
Speaker 5:  new thing you can do on an AI PC that you couldn't before. That is better

337
00:21:08,805 --> 00:21:12,205
Speaker 5:  because of ai. Yes. And like in the whole AI world,

338
00:21:13,105 --> 00:21:17,005
Speaker 5:  we are severely lacking in that type of thing. Yes.

339
00:21:17,065 --> 00:21:20,565
Speaker 5:  And I feel like the, whether recall is any good, whether people want it,

340
00:21:20,675 --> 00:21:24,405
Speaker 5:  what we do with the privacy implications, all that aside, just the fact that

341
00:21:24,645 --> 00:21:28,165
Speaker 5:  Microsoft is like, we, we found a new thing we can do now because of this

342
00:21:29,115 --> 00:21:32,085
Speaker 5:  felt really good to me that I was like, okay, you understand what this is

343
00:21:32,245 --> 00:21:36,125
Speaker 5:  actually for? Which is things like it's it's for doing things.

344
00:21:36,125 --> 00:21:39,925
Speaker 5:  Yeah. It's not for talking to a chatbot I can talk to in a web browser.

345
00:21:40,155 --> 00:21:44,005
Speaker 5:  It's not for a bunch of random nonsense that doesn't help

346
00:21:44,005 --> 00:21:45,565
Speaker 5:  anybody. Like it's it's for things.

347
00:21:46,115 --> 00:21:49,685
Speaker 2:  Yeah. I think Ryan Broderick, who writes the Garbage Day newsletter has been

348
00:21:49,685 --> 00:21:53,485
Speaker 2:  consistently pointing out that the AI industry has to talk

349
00:21:53,485 --> 00:21:56,965
Speaker 2:  about a GI in destroying the world to make their

350
00:21:57,035 --> 00:21:59,125
Speaker 2:  extremely boring demos have stakes.

351
00:22:01,185 --> 00:22:01,605
Speaker 2:  That's

352
00:22:01,805 --> 00:22:02,605
Speaker 5:  Very true. Yeah. Like

353
00:22:02,605 --> 00:22:06,285
Speaker 2:  You, you have to be like, it can see an orange, this will lead to the end

354
00:22:06,285 --> 00:22:10,165
Speaker 2:  of the world. Like you just fine. You know?

355
00:22:10,235 --> 00:22:14,085
Speaker 2:  Yeah. But you're right that Microsoft laid out a particular vision

356
00:22:14,085 --> 00:22:17,365
Speaker 2:  of computing, which is you're using a computer all day.

357
00:22:18,115 --> 00:22:21,525
Speaker 2:  What If you could then interact with a computer in a way that

358
00:22:21,965 --> 00:22:25,925
Speaker 2:  remembered all of the things you were doing and that became your

359
00:22:25,925 --> 00:22:29,725
Speaker 2:  companion. And we should get into the reaction and the reality of that

360
00:22:29,745 --> 00:22:32,805
Speaker 2:  and how it might work and the privacy implications. I think all those things

361
00:22:32,805 --> 00:22:35,845
Speaker 2:  are really important. I just wanna st spend one second on Nadela saying this

362
00:22:35,845 --> 00:22:39,165
Speaker 2:  is one of the two dreams people have had for a long time in the keynote.

363
00:22:39,385 --> 00:22:43,205
Speaker 2:  Mm. Because he is not wrong. Right. The idea that you live your

364
00:22:43,235 --> 00:22:46,765
Speaker 2:  life on the computer and all the things you do on your computer

365
00:22:46,965 --> 00:22:50,445
Speaker 2:  comprise your experiences and it would be cool

366
00:22:51,025 --> 00:22:55,005
Speaker 2:  if the computer could help you sort those out is in fact a dream.

367
00:22:55,435 --> 00:22:55,725
Speaker 2:  Yeah.

368
00:22:55,945 --> 00:22:56,165
Speaker 5:  Yes.

369
00:22:56,545 --> 00:23:00,525
Speaker 2:  And it requires like one bit of a philosophical shift, which is I think

370
00:23:00,635 --> 00:23:03,485
Speaker 2:  most people think about that dream in terms of just like walking around the

371
00:23:03,485 --> 00:23:07,405
Speaker 2:  world. Like what If you had an ambient computer or the ar ar glasses

372
00:23:08,065 --> 00:23:11,485
Speaker 2:  and you were just like asking questions and your, your sort of IRL experiences

373
00:23:11,515 --> 00:23:15,125
Speaker 2:  were that thing. And Microsoft has been like, all right, we're

374
00:23:15,395 --> 00:23:19,325
Speaker 2:  like, we HoloLens aside, like actually you live your life on the screen

375
00:23:20,105 --> 00:23:23,125
Speaker 2:  so we can just build a thing on this, on the screen and then we'll get to

376
00:23:23,125 --> 00:23:26,805
Speaker 2:  your eyes later. You know, we'll put the computer on your face at a later

377
00:23:26,835 --> 00:23:29,805
Speaker 2:  time. What about the computer that's in front of your face? What if we made

378
00:23:29,805 --> 00:23:33,485
Speaker 2:  that do the thing that everyone wants? And that is a leap.

379
00:23:33,725 --> 00:23:37,085
Speaker 2:  I I, it's actually kind of an interesting leap that Microsoft took first

380
00:23:37,945 --> 00:23:41,125
Speaker 2:  in this way that where whereas everyone else has been chasing these classes

381
00:23:42,085 --> 00:23:45,925
Speaker 2:  and I, I just like, it should be, I I think noting that he called it

382
00:23:45,925 --> 00:23:48,045
Speaker 2:  one of the two dreams everyone had and I think people were like, is that

383
00:23:48,045 --> 00:23:51,125
Speaker 2:  the dream? And it's like, oh actually we've been talking to air glasses for

384
00:23:51,125 --> 00:23:54,605
Speaker 2:  like a decade and this is just that, but just

385
00:23:54,605 --> 00:23:55,765
Speaker 2:  digitally instead

386
00:23:55,765 --> 00:23:58,285
Speaker 5:  Of how Yeah. Once Microsoft is the company that actually like lives in the

387
00:23:58,285 --> 00:23:59,645
Speaker 5:  real world unlike everybody else.

388
00:24:00,045 --> 00:24:02,485
Speaker 2:  Yeah. They're not just showing some weird concept videos of like the Microsoft

389
00:24:02,685 --> 00:24:03,285
Speaker 2:  kin, like

390
00:24:03,635 --> 00:24:04,125
Speaker 5:  Exactly

391
00:24:05,165 --> 00:24:08,605
Speaker 2:  Whatever that is or the, the, what was it, the papyrus, what was that thing

392
00:24:08,605 --> 00:24:10,245
Speaker 2:  called? The Courier? Oh,

393
00:24:10,245 --> 00:24:14,165
Speaker 5:  The Coru. Coer. RIP The Courier. They probably made a thing called the

394
00:24:14,165 --> 00:24:15,045
Speaker 5:  Papyrus at one point.

395
00:24:15,465 --> 00:24:15,845
Speaker 2:  I'm sure

396
00:24:16,515 --> 00:24:17,365
Speaker 6:  They will send if

397
00:24:17,365 --> 00:24:20,925
Speaker 2:  They haven't. Can I tell you a, an early a story about Neo Patels

398
00:24:21,195 --> 00:24:25,125
Speaker 2:  experiences of copyright infringement? You don't, nobody remembers the kin.

399
00:24:25,125 --> 00:24:27,885
Speaker 2:  This is like a thing that happened to a small number of us on the internet.

400
00:24:28,445 --> 00:24:32,245
Speaker 2:  But Microsoft in the early iPhone days, instead of releasing a smartphone

401
00:24:32,885 --> 00:24:36,685
Speaker 2:  released a teen focused feature phone called the Microsoft Kin. This is real.

402
00:24:36,685 --> 00:24:40,445
Speaker 2:  There was a kin one and a kin two. They were bonnet like, I dunno

403
00:24:40,565 --> 00:24:43,645
Speaker 2:  why they did this. and they were not smartphones, they were

404
00:24:43,765 --> 00:24:47,245
Speaker 2:  legitimately feature phones that could like send MSN messages.

405
00:24:47,825 --> 00:24:51,365
Speaker 2:  And their marketing campaign was like beautiful young people in hip art

406
00:24:51,645 --> 00:24:55,165
Speaker 2:  districts of cities like skateboarding talking on their feature phones. And

407
00:24:55,205 --> 00:24:59,005
Speaker 2:  I remixed their ads to the song Two Drunk to Fuck

408
00:24:59,085 --> 00:24:59,925
Speaker 2:  by the Dead Kennedys

409
00:25:01,985 --> 00:25:05,125
Speaker 2:  and Microsoft immediately had YouTube take them down. Amazing.

410
00:25:06,105 --> 00:25:09,405
Speaker 2:  And I'm like, I'm like, that would just be a great TikTok now. Like no one

411
00:25:09,405 --> 00:25:12,565
Speaker 2:  would even ask this question. Now anyone, if you're out there, do your thing.

412
00:25:12,565 --> 00:25:16,405
Speaker 2:  TikTok the Kin, by the way, we gave them horrible reviews before they

413
00:25:16,405 --> 00:25:20,045
Speaker 2:  even came out. Microsoft canceled them. This is a real victory of the eng

414
00:25:20,045 --> 00:25:23,925
Speaker 2:  Gadget days. Well, I dunno what we're talking about anyway. No, the products

415
00:25:23,925 --> 00:25:27,125
Speaker 2:  are real. That's what we're talking about. Not the Microsoft can, the products

416
00:25:27,125 --> 00:25:30,965
Speaker 2:  are real. You can like use it and then the reaction to them is,

417
00:25:31,365 --> 00:25:33,405
Speaker 2:  I don't trust this at all. It's probably broken. I don't, I don't believe

418
00:25:33,405 --> 00:25:33,525
Speaker 2:  you.

419
00:25:34,075 --> 00:25:37,725
Speaker 6:  Yeah. Yeah. The reaction to recall in

420
00:25:37,725 --> 00:25:41,365
Speaker 6:  particular was surprising to me because like I I edited our story on it and

421
00:25:41,365 --> 00:25:44,445
Speaker 6:  I was like, this is cool. I'm really excited about this. Like, didn't think

422
00:25:44,445 --> 00:25:48,045
Speaker 6:  of the privacy stuff at all. 'cause I operate under the assumption if someone

423
00:25:48,045 --> 00:25:51,565
Speaker 6:  physically has your computer, then you're boned. Like,

424
00:25:52,155 --> 00:25:54,645
Speaker 6:  like you're screwed if someone has your computer, you're done.

425
00:25:54,685 --> 00:25:55,365
Speaker 2:  That's the same word.

426
00:25:55,675 --> 00:25:59,565
Speaker 6:  Yeah. They're the same. Just in case people didn't understand what

427
00:25:59,565 --> 00:26:03,485
Speaker 6:  bone, what means. I, I wanna make sure our entire audience understands,

428
00:26:05,105 --> 00:26:08,645
Speaker 6:  but I, I just didn't ca I was like, okay, cool. This is, this is really neat

429
00:26:08,745 --> 00:26:12,125
Speaker 6:  and and I'm excited for my computer to be a better archive of things because

430
00:26:12,125 --> 00:26:14,965
Speaker 6:  that's what I use a computer for. Yeah. And then a friend was like, have

431
00:26:14,965 --> 00:26:18,125
Speaker 6:  you heard about how pissed people are about recall? And I was like, why?

432
00:26:18,125 --> 00:26:20,525
Speaker 6:  And they're like, because it records everything you do. And I was like, that's

433
00:26:20,525 --> 00:26:22,285
Speaker 6:  literally what a computer does. Yeah.

434
00:26:22,285 --> 00:26:23,445
Speaker 5:  That's like the point of

435
00:26:23,635 --> 00:26:25,765
Speaker 2:  Them. Middle of the history tab of your browser. I

436
00:26:25,765 --> 00:26:28,005
Speaker 5:  Was just about to say, this is one of my favorite things to do to people

437
00:26:28,005 --> 00:26:31,845
Speaker 5:  is like, people don't understand the extent to which this

438
00:26:31,925 --> 00:26:35,765
Speaker 5:  stuff is already being recorded. Like go look at the cash files on

439
00:26:35,765 --> 00:26:39,685
Speaker 5:  your computer for what happens in all the apps on your computer. Like they,

440
00:26:39,755 --> 00:26:43,405
Speaker 5:  they already know. But I think Kranz to your point, the,

441
00:26:43,865 --> 00:26:47,405
Speaker 5:  the thing about privacy, there are real interesting privacy implications

442
00:26:47,405 --> 00:26:50,085
Speaker 5:  here about like, what happens if somebody gets hold of this? What if there

443
00:26:50,145 --> 00:26:52,765
Speaker 5:  are there gonna be new kinds of malware that can get onto my computer and

444
00:26:52,765 --> 00:26:55,685
Speaker 5:  chase this stuff down? Like the stakes that go up slightly. But I, I do think

445
00:26:55,685 --> 00:26:59,285
Speaker 5:  part of the reaction was just reminding

446
00:26:59,285 --> 00:27:03,125
Speaker 5:  people of how much these devices already know.

447
00:27:03,355 --> 00:27:06,845
Speaker 5:  Yeah. And the the just the thing where it's like I am,

448
00:27:07,825 --> 00:27:11,565
Speaker 5:  I'm doing all of this stuff on here. Like this is in a very real

449
00:27:11,665 --> 00:27:15,445
Speaker 5:  way my life. And just the fact that that's being

450
00:27:15,445 --> 00:27:19,365
Speaker 5:  collected somewhere feels gross to be reminded of. Like

451
00:27:19,435 --> 00:27:23,045
Speaker 5:  even though sort of intellectually I understand that that's already the case.

452
00:27:23,795 --> 00:27:27,645
Speaker 5:  It's, it's like when you realize that your ISP has access

453
00:27:27,645 --> 00:27:31,285
Speaker 5:  to every website that you go to on your phone, even if you're on like in

454
00:27:31,395 --> 00:27:34,405
Speaker 5:  incognito, like no, like Verizon still knows hi Verizon. It's still there.

455
00:27:34,515 --> 00:27:38,245
Speaker 5:  Like it just, people hate being reminded of that and it is just such

456
00:27:38,445 --> 00:27:42,125
Speaker 5:  an unavoidable fact of being alive right now that it is how it is that I

457
00:27:42,125 --> 00:27:45,805
Speaker 5:  think it every time it comes up again, you're made to feel sort of powerless

458
00:27:46,065 --> 00:27:49,965
Speaker 5:  and helpless against this like, incredible tide of all these

459
00:27:50,395 --> 00:27:52,285
Speaker 5:  electronics that know too much about me. Well I think

460
00:27:52,285 --> 00:27:55,725
Speaker 6:  It's also the fact that it's got, it's, it's using ai, right? Like, like

461
00:27:55,785 --> 00:27:59,285
Speaker 6:  that's a big part of it. And people are having this weird moment,

462
00:27:59,285 --> 00:28:01,965
Speaker 6:  especially this week and we're gonna talk about it more with like what's

463
00:28:01,965 --> 00:28:05,725
Speaker 6:  going on with Scarlet Johansson later. But people are just having this sudden

464
00:28:06,045 --> 00:28:09,805
Speaker 6:  visceral reaction to ai. And so this thing was, she was like, yes,

465
00:28:10,245 --> 00:28:12,605
Speaker 6:  computers archive all your data. They have done that since the beginning

466
00:28:12,605 --> 00:28:16,485
Speaker 6:  of time. This is just more efficient at it. They were like,

467
00:28:16,835 --> 00:28:19,925
Speaker 6:  that might have gone over a little easier if they hadn't also said, and we're

468
00:28:19,925 --> 00:28:23,525
Speaker 6:  gonna use AI to do it all. 'cause even though the ai, everything is happening

469
00:28:23,525 --> 00:28:26,885
Speaker 6:  according to Microsoft, you know, security researchers will do their thing.

470
00:28:27,575 --> 00:28:30,605
Speaker 6:  Everything is happening on the computer. It is not going into the cloud.

471
00:28:30,675 --> 00:28:34,485
Speaker 6:  None of this is being shared anywhere. The, it's using the NPU to like

472
00:28:34,485 --> 00:28:38,365
Speaker 6:  process this stuff. All of it's happening there. So theoretically it is no

473
00:28:38,365 --> 00:28:42,205
Speaker 6:  different than just using your computer as normal. Only now if a

474
00:28:42,205 --> 00:28:45,805
Speaker 6:  bad person gets ahold of your computer, they're gonna have a little easier

475
00:28:45,955 --> 00:28:49,795
Speaker 6:  time recovering everything you do. That's it. But

476
00:28:49,795 --> 00:28:53,275
Speaker 6:  that you put AI in there and people are like, burn it to the ground. Well

477
00:28:53,355 --> 00:28:56,915
Speaker 5:  The whole road of that is so fascinating to me. 'cause I was talking to Dan

478
00:28:57,075 --> 00:29:00,635
Speaker 5:  Roker, the CEO of this company called Rewind, which has been doing a third

479
00:29:00,635 --> 00:29:04,035
Speaker 5:  party version of this on max for the last, I think year or so now.

480
00:29:04,465 --> 00:29:07,955
Speaker 5:  It's another one that it uses basically like screen recording and your

481
00:29:08,175 --> 00:29:11,795
Speaker 5:  device audio to pull out a lot of this same information. and

482
00:29:12,545 --> 00:29:16,435
Speaker 5:  they actually just launched a new version that sinks in the cloud

483
00:29:16,585 --> 00:29:19,635
Speaker 5:  because the overwhelming feedback from people was either

484
00:29:21,025 --> 00:29:24,515
Speaker 5:  hell no, or hell yes, the hell no people you're never gonna get, and what

485
00:29:24,515 --> 00:29:28,395
Speaker 5:  the hell yes people wanted was for all of this to be available to them

486
00:29:28,395 --> 00:29:32,155
Speaker 5:  in more places. And so it's like we're actually in this bonkers

487
00:29:32,155 --> 00:29:34,915
Speaker 5:  divide where there's a set of people who like as soon as you present them

488
00:29:34,915 --> 00:29:38,355
Speaker 5:  the beginning of the road are like absolutely not. And just like, nope. Their

489
00:29:38,355 --> 00:29:41,725
Speaker 5:  way out of this whole technological transition. And then there are a bunch

490
00:29:41,725 --> 00:29:45,605
Speaker 5:  of people for whom the privacy stuff is actually going

491
00:29:45,605 --> 00:29:49,005
Speaker 5:  to be a hindrance. Yeah. More than anything else. And there are gonna be

492
00:29:49,005 --> 00:29:51,925
Speaker 5:  people who are like, well why can't I access everything that I did on my

493
00:29:51,925 --> 00:29:55,365
Speaker 5:  other computer from my work computer? And then Microsoft is gonna be in this

494
00:29:55,565 --> 00:29:58,045
Speaker 5:  horrible position of being like, well, we have to do the less private thing

495
00:29:58,115 --> 00:30:01,965
Speaker 5:  because it's actually what our users want. And it is just, it just

496
00:30:02,235 --> 00:30:06,085
Speaker 5:  like exacerbates that divide further and further and it just, I, I

497
00:30:06,085 --> 00:30:08,565
Speaker 5:  don't know, it just feels crazy to me. Like the rewind folks were like, I

498
00:30:08,605 --> 00:30:11,405
Speaker 5:  I don't know what you want from us. We tried to do it as private as we could

499
00:30:11,405 --> 00:30:13,085
Speaker 5:  and the people who used it didn't want that anymore.

500
00:30:13,365 --> 00:30:16,285
Speaker 2:  So there, there's two things about that that I think are fascinating and

501
00:30:16,285 --> 00:30:20,225
Speaker 2:  they are expressed sort of in very tangible ways than

502
00:30:20,225 --> 00:30:24,145
Speaker 2:  I think in very esoteric ways. So the very tangible way I did

503
00:30:24,145 --> 00:30:27,585
Speaker 2:  the thing when I interviewed Sundar, where I handed him my phone to look

504
00:30:27,585 --> 00:30:30,785
Speaker 2:  at Google search results and we ran that clip. The clip is like doing bonkers

505
00:30:30,785 --> 00:30:31,625
Speaker 2:  on TikTok. And

506
00:30:33,455 --> 00:30:37,275
Speaker 2:  in the clip as a joke, I say to Sundar, phai,

507
00:30:37,365 --> 00:30:40,675
Speaker 2:  don't dig through my phone. Like of all the things you were ever gonna say

508
00:30:40,735 --> 00:30:44,195
Speaker 2:  to a CEO e like a billionaire, CEO of one of the largest, you just pull it

509
00:30:44,195 --> 00:30:48,115
Speaker 2:  up kicks and Max don't literally like, who am I? Of course I said like, I

510
00:30:48,115 --> 00:30:51,675
Speaker 2:  couldn't resist it, you know, but it was just a joke. It's just like a

511
00:30:52,135 --> 00:30:55,555
Speaker 2:  way to be human when you hand someone your phone and the

512
00:30:55,815 --> 00:30:58,515
Speaker 2:  people in the comments are like, he already knows everything.

513
00:31:00,305 --> 00:31:02,675
Speaker 2:  Like why would you tell him not to dig through your phone? Like he's, he's

514
00:31:02,675 --> 00:31:05,395
Speaker 2:  the CEO of Google, like If you want, he can just, he can already dig through

515
00:31:05,395 --> 00:31:09,275
Speaker 2:  your phone. And I think that's nihilism, right? Like

516
00:31:09,275 --> 00:31:11,995
Speaker 2:  there's an element of nihilism there, which is like, it's already over. Like

517
00:31:11,995 --> 00:31:15,235
Speaker 2:  you might as well embrace it and be excited about it. And then there's an

518
00:31:15,235 --> 00:31:18,435
Speaker 2:  element of nihilism that's like, well I can't stop it. I'm gonna live off

519
00:31:18,435 --> 00:31:21,915
Speaker 2:  the grid for the rest of my life. And that's one thing, the, the more esoteric

520
00:31:21,915 --> 00:31:25,675
Speaker 2:  thing that I think about all the time is as more and more of these

521
00:31:26,035 --> 00:31:29,355
Speaker 2:  companies get more and more into AI content generation,

522
00:31:29,975 --> 00:31:33,715
Speaker 2:  the thing that they are promising, the people who pay the money, which is

523
00:31:33,775 --> 00:31:37,755
Speaker 2:  in large part advertisers, is that they will make autogenerated

524
00:31:38,055 --> 00:31:41,635
Speaker 2:  ads on their platforms. Yeah. So like Google will make autogenerated

525
00:31:41,935 --> 00:31:45,835
Speaker 2:  AI created ads on YouTube. Meta will do it on Instagram. TikTok

526
00:31:45,835 --> 00:31:49,395
Speaker 2:  will do it on TikTok. TikTok actually announced AI generated ads this week

527
00:31:50,055 --> 00:31:51,875
Speaker 2:  and Mia Sato is writing with them.

528
00:31:53,695 --> 00:31:57,555
Speaker 2:  You're gonna get to a place where you get extremely well targeted custom

529
00:31:57,555 --> 00:32:01,355
Speaker 2:  content and the people who are like Facebook is listening to me are

530
00:32:01,355 --> 00:32:02,555
Speaker 2:  gonna lose their minds.

531
00:32:02,935 --> 00:32:03,155
Speaker 5:  Yes.

532
00:32:04,085 --> 00:32:07,995
Speaker 2:  Right? Like the idea that you have privacy then is like out

533
00:32:07,995 --> 00:32:11,635
Speaker 2:  the window. Like right now people think Facebook is listening to them because

534
00:32:11,635 --> 00:32:15,495
Speaker 2:  it can target you based on your wifi networks, which is

535
00:32:15,515 --> 00:32:18,095
Speaker 2:  the simplest thing. It just like knows what IP address you're on. It knows

536
00:32:18,095 --> 00:32:21,895
Speaker 2:  that your friends have been on that IP address and it was like serve some

537
00:32:21,955 --> 00:32:25,935
Speaker 2:  ads across based on interest targeting. It's not smart, it's

538
00:32:25,935 --> 00:32:29,295
Speaker 2:  not stupid, it's like somewhere in the middle, you know? Yeah. Like it's

539
00:32:29,295 --> 00:32:32,935
Speaker 2:  a pretty blunt instrument, but people are convinced that

540
00:32:33,135 --> 00:32:37,055
Speaker 2:  Facebook is listening to them. You're gonna get to a place where it knows

541
00:32:37,135 --> 00:32:40,205
Speaker 2:  a whole lot about you and then it knows that you're on the same wifi network

542
00:32:40,225 --> 00:32:43,845
Speaker 2:  as your friend that you talked to about vacations in Mexico and then it delivers

543
00:32:43,845 --> 00:32:45,165
Speaker 2:  you a video of you in Mexico.

544
00:32:48,555 --> 00:32:52,485
Speaker 2:  What? Yeah. Like people are lose their minds and then

545
00:32:52,565 --> 00:32:54,925
Speaker 2:  I think that, well I don't have any privacy anyway. I might as well just

546
00:32:54,955 --> 00:32:58,365
Speaker 2:  give the cloud provider of recall or rewind

547
00:32:58,375 --> 00:33:02,165
Speaker 2:  everything. I think that might actually change in a dramatic way very quickly

548
00:33:02,225 --> 00:33:02,725
Speaker 2:  at that point.

549
00:33:04,575 --> 00:33:08,365
Speaker 5:  Maybe it like, it really does seem like either, either the

550
00:33:08,365 --> 00:33:12,125
Speaker 5:  pendulum is going to swing aggressively at some point soon or the pendulum

551
00:33:12,125 --> 00:33:16,005
Speaker 5:  is just going to like burst through the side. No,

552
00:33:16,005 --> 00:33:17,045
Speaker 6:  No. You know what's gonna happen

553
00:33:17,205 --> 00:33:17,765
Speaker 2:  Explodes.

554
00:33:17,765 --> 00:33:20,565
Speaker 5:  Yeah. Like Charlie in the chocolate factory style, just like launch through

555
00:33:20,565 --> 00:33:21,525
Speaker 5:  the ceiling and be gone.

556
00:33:23,665 --> 00:33:24,045
Speaker 2:  One thing

557
00:33:28,075 --> 00:33:31,165
Speaker 2:  like, like what, what's gonna happen to the privacy pendulum? They, it's

558
00:33:31,165 --> 00:33:31,725
Speaker 2:  go into space.

559
00:33:35,225 --> 00:33:39,085
Speaker 2:  One thing I'll note you mentioned that the ISPs can see everything. Like

560
00:33:39,085 --> 00:33:41,765
Speaker 2:  you would think that there would be a federal privacy bill. I'm, there is

561
00:33:41,765 --> 00:33:45,325
Speaker 2:  not one of the first things that the Republican congress in

562
00:33:45,325 --> 00:33:49,285
Speaker 2:  Trump's first term did. Literally one of the first things was

563
00:33:49,285 --> 00:33:52,925
Speaker 2:  they threw out privacy bill that would've kept your data private from

564
00:33:53,115 --> 00:33:56,925
Speaker 2:  ISPs. Yep. It was just like one of, Hey, we're here. What should we do? We

565
00:33:56,925 --> 00:34:00,445
Speaker 2:  should make it so at and t can read all your shit. Like immediate.

566
00:34:01,305 --> 00:34:04,485
Speaker 2:  It was an outrage. Yeah. And we, we have just not fixed that

567
00:34:05,105 --> 00:34:05,525
Speaker 2:  in years.

568
00:34:05,885 --> 00:34:09,325
Speaker 6:  I I I tend to feel like that that tension is only gonna be resolved

569
00:34:10,675 --> 00:34:14,525
Speaker 6:  when congress actually knows what a computer is, how

570
00:34:14,525 --> 00:34:17,765
Speaker 6:  it works, how to like use it like when they finally, well

571
00:34:17,765 --> 00:34:21,605
Speaker 2:  Here's what a computer is. Yeah, it is. Its an AI powered

572
00:34:21,605 --> 00:34:25,125
Speaker 2:  device. But If you say the right words, we'll bang you. Yeah.

573
00:34:25,465 --> 00:34:28,325
Speaker 2:  And I think Congress will react to that idea. Very powerful.

574
00:34:30,065 --> 00:34:33,845
Speaker 6:  But it'll also If you say the wrong words, share all the photos of you

575
00:34:33,845 --> 00:34:34,805
Speaker 6:  banging it to

576
00:34:34,995 --> 00:34:38,005
Speaker 2:  Look If. you wanna get something done in United that'll get united that'll

577
00:34:38,005 --> 00:34:40,605
Speaker 2:  states congress, it's blackmail and booty. those are your choices.

578
00:34:41,105 --> 00:34:42,085
Speaker 5:  That's real. That's

579
00:34:43,305 --> 00:34:47,165
Speaker 6:  So honestly, this is a great thing, right? Like yeah. Like I think this tension

580
00:34:47,305 --> 00:34:50,245
Speaker 6:  has been there for a very long time and, and I think you are right that we

581
00:34:50,245 --> 00:34:53,605
Speaker 6:  are coming to a point where like something's gonna have to give

582
00:34:53,895 --> 00:34:57,765
Speaker 6:  these companies like Microsoft are really pushing for,

583
00:34:57,825 --> 00:35:01,045
Speaker 6:  for stuff that is just completely counter to what we understand of privacy

584
00:35:01,185 --> 00:35:04,445
Speaker 6:  and security and what we want. And the only way that's gonna change is if

585
00:35:05,775 --> 00:35:07,665
Speaker 6:  someone else says, Hey, you can't do that.

586
00:35:08,135 --> 00:35:11,345
Speaker 2:  Yeah. I look, I think the recall stuff is really interesting. I think it's,

587
00:35:11,885 --> 00:35:14,225
Speaker 2:  to your point, David, it can't see what you're doing on your phone, right?

588
00:35:14,225 --> 00:35:17,465
Speaker 2:  So this promise that you have this companion and this sort of living your

589
00:35:17,465 --> 00:35:20,225
Speaker 2:  digital life with you except for what you do on your phone,

590
00:35:21,405 --> 00:35:24,545
Speaker 2:  that's weird, right? And then you get into, should this be a cloud service?

591
00:35:24,605 --> 00:35:28,105
Speaker 2:  Are you're running a background app on an Android? Like Apple's not gonna

592
00:35:28,105 --> 00:35:31,665
Speaker 2:  let them do it, right? No. Maybe Apple does it on its own in iOS in a couple

593
00:35:31,665 --> 00:35:35,545
Speaker 2:  weeks. Like we're, they're gonna have to bridge that gap I think for

594
00:35:35,545 --> 00:35:39,405
Speaker 2:  this to be truly successful. But then there's the piece where

595
00:35:40,555 --> 00:35:43,285
Speaker 2:  yeah, we're gonna have to hold them accountable to this thing being totally

596
00:35:43,295 --> 00:35:47,285
Speaker 2:  local and a bunch of other AI stuff on this device is hybridized, right?

597
00:35:47,605 --> 00:35:50,925
Speaker 2:  Like the paint example is hybridized. Like, I don't know. And that's why

598
00:35:51,165 --> 00:35:55,125
Speaker 2:  I don't wanna seem overly skeptical. I I I really, I can see the jump

599
00:35:55,245 --> 00:35:58,885
Speaker 2:  that Microsoft made from ar glasses are too hard. We have a bunch of

600
00:35:59,025 --> 00:36:02,725
Speaker 2:  ai, we can basically do ar ar on windows. Like

601
00:36:03,345 --> 00:36:07,205
Speaker 2:  ar is not the right word, but like this sort of augmented experience

602
00:36:07,205 --> 00:36:08,965
Speaker 2:  in Windows for your digital life.

603
00:36:09,455 --> 00:36:11,125
Speaker 6:  Multimodal you mean? Yeah.

604
00:36:11,595 --> 00:36:11,885
Speaker 5:  Yeah.

605
00:36:11,885 --> 00:36:14,925
Speaker 2:  It's, it's, it's, there's something there. Co-pilot's like almost exactly

606
00:36:14,945 --> 00:36:18,845
Speaker 2:  the right word, right? Like you're just doing stuff on a

607
00:36:18,965 --> 00:36:21,085
Speaker 2:  computer and you're asking questions about the experiences you're having.

608
00:36:22,005 --> 00:36:24,845
Speaker 2:  A lot of people have thought that would happen first in glasses on your face

609
00:36:24,845 --> 00:36:27,205
Speaker 2:  and Microsoft is like, what if it happens on the screen in front of you?

610
00:36:27,205 --> 00:36:28,125
Speaker 2:  Like, great, I

611
00:36:28,125 --> 00:36:31,005
Speaker 6:  Like this. I just wanna point out, you said co-pilot is the right word. Where

612
00:36:31,005 --> 00:36:33,125
Speaker 6:  do you stand on copilot plus PCs?

613
00:36:33,325 --> 00:36:34,725
Speaker 5:  Awful. Awful,

614
00:36:35,135 --> 00:36:36,005
Speaker 2:  Awful. Awful. I

615
00:36:36,005 --> 00:36:39,245
Speaker 5:  Know what that is. Awful. Even, even Microsoft

616
00:36:39,865 --> 00:36:43,845
Speaker 5:  has trouble saying the phrase copilot plus PCs. Like If,

617
00:36:43,845 --> 00:36:46,885
Speaker 5:  you go back and watch, there are a bunch of people who clearly practiced

618
00:36:46,995 --> 00:36:50,965
Speaker 5:  very hard who still occasionally struggle to say the phrase copilot plus

619
00:36:51,265 --> 00:36:54,925
Speaker 5:  PCs. It's terrible. Yeah. Cool product. Bad name.

620
00:36:55,595 --> 00:36:57,245
Speaker 2:  They, I just wanna figure out, figure how we're gonna get it right.

621
00:36:57,665 --> 00:37:00,205
Speaker 6:  Why didn't they just call 'em AI PCs? Like did h HP just, just

622
00:37:00,285 --> 00:37:04,005
Speaker 5:  'em copilot PCs their first, the hell is the plus doing there Copilot.

623
00:37:04,035 --> 00:37:04,965
Speaker 5:  It's not a streaming service.

624
00:37:06,555 --> 00:37:07,645
Speaker 5:  What are we doing here?

625
00:37:08,405 --> 00:37:10,245
Speaker 2:  Do you have these features on Windows have a monthly fee?

626
00:37:11,575 --> 00:37:12,795
Speaker 6:  No, I don't think so yet.

627
00:37:12,975 --> 00:37:16,915
Speaker 5:  Not as far as we know. I don't think some of it requires an

628
00:37:17,335 --> 00:37:21,155
Speaker 5:  OpenAI API key, which you have to pay for

629
00:37:21,945 --> 00:37:25,275
Speaker 5:  like some of the, some of the copy and paste stuff. You can only do If. you

630
00:37:25,275 --> 00:37:28,955
Speaker 5:  have the, I think the like GPT-4 key or something. But ah,

631
00:37:29,505 --> 00:37:32,795
Speaker 5:  most of this stuff seems like it's gonna come baked into Windows, which itself

632
00:37:32,935 --> 00:37:36,915
Speaker 5:  is rapidly becoming a subscription service. So I think that's where you end

633
00:37:36,915 --> 00:37:40,635
Speaker 5:  up getting charged for it is like as a Microsoft's 365 thing rather

634
00:37:40,635 --> 00:37:43,155
Speaker 5:  than a specific AI thing over time. I

635
00:37:43,155 --> 00:37:46,755
Speaker 2:  Do love the mental image of like three to five years from now the extremely

636
00:37:46,755 --> 00:37:50,595
Speaker 2:  cheap Acce R PC in a Walmart with like the dented cardboard sign that's like

637
00:37:50,945 --> 00:37:54,035
Speaker 2:  copy and paste in multiple languages and it's just like

638
00:37:54,585 --> 00:37:58,435
Speaker 2:  sitting in Racine, Wisconsin. You know, like everyone's like, why?

639
00:37:59,095 --> 00:38:01,435
Speaker 2:  And that costs a monthly fee and they're like, why aren't these computers

640
00:38:01,435 --> 00:38:04,755
Speaker 2:  selling? Like, that's gonna be great for everybody. We take a break.

641
00:38:05,095 --> 00:38:08,595
Speaker 2:  That's just build, there's more,

642
00:38:08,905 --> 00:38:12,595
Speaker 2:  there's like yet more behind use this week. Yeah, we'll come back. We'll

643
00:38:12,595 --> 00:38:14,995
Speaker 2:  talk about Scarlet Show Hansen. It's really what we're gonna do here on The

644
00:38:15,075 --> 00:38:15,875
Speaker 2:  Vergecast. We'll be right back.

645
00:41:30,225 --> 00:41:32,345
Speaker 2:  out in, in the scheme of

646
00:42:19,715 --> 00:42:23,645
Speaker 2:  many of the traditional PC companies announced copilot plus

647
00:42:23,985 --> 00:42:27,565
Speaker 2:  PCs including Samsung and If. you have been tracking

648
00:42:27,595 --> 00:42:31,445
Speaker 2:  Samsung, you will know that Samsung recently instituted

649
00:42:31,705 --> 00:42:35,525
Speaker 2:  six day work weeks for its executives to quote, inject a sense of

650
00:42:35,525 --> 00:42:38,965
Speaker 2:  crisis into the company. They did not

651
00:42:39,115 --> 00:42:42,925
Speaker 2:  institute six day work weeks for their engineers or products for the executives.

652
00:42:43,065 --> 00:42:46,965
Speaker 2:  You get a bunch of executives working on Saturdays, you

653
00:42:46,965 --> 00:42:47,845
Speaker 2:  get crazy ideas,

654
00:42:49,615 --> 00:42:51,525
Speaker 2:  which I have taken to calling Saturday Samsung.

655
00:42:53,345 --> 00:42:57,125
Speaker 2:  So Samsung's copilot pc, this is a real thing. So If, you buy

656
00:42:57,185 --> 00:43:00,485
Speaker 2:  the new Galaxy Book four Edge, by the way, there's no space between book

657
00:43:00,485 --> 00:43:04,285
Speaker 2:  and four, but a space between four and Edge. All right.

658
00:43:04,355 --> 00:43:08,165
Speaker 5:  Marketing you is that the name of that is the Samsung Galaxy

659
00:43:08,165 --> 00:43:11,885
Speaker 5:  Book four Edge copilot plus pc. Yeah, that makes me wanna throw things.

660
00:43:12,465 --> 00:43:14,925
Speaker 2:  I'm saying you had the extra day on Saturday, you could have gotten rid of

661
00:43:14,965 --> 00:43:17,205
Speaker 2:  a couple words. Alright.

662
00:43:18,785 --> 00:43:22,205
Speaker 2:  But the, the people hooked Samsung in their quest to

663
00:43:22,705 --> 00:43:26,365
Speaker 2:  inject a sense of crisis into the company have decided that their,

664
00:43:26,375 --> 00:43:30,045
Speaker 2:  their single best idea is to just give away

665
00:43:30,385 --> 00:43:34,365
Speaker 2:  TVs. Yeah. So the previous one of these we

666
00:43:34,365 --> 00:43:37,405
Speaker 2:  covered the previous iteration of Saturday Samsung was that If, you bought

667
00:43:37,445 --> 00:43:39,965
Speaker 2:  A Frame tv, you got another tv, which is perfect

668
00:43:41,345 --> 00:43:45,285
Speaker 2:  If. you buy a Galaxy Book four Edge, again, there's no space between

669
00:43:45,315 --> 00:43:49,045
Speaker 2:  book and four, but a space between four and Edge. A Galaxy Book four Edge

670
00:43:49,045 --> 00:43:53,005
Speaker 2:  copilot plus vc. You do receive a 50 inch crystal

671
00:43:53,285 --> 00:43:54,325
Speaker 2:  UHD 4K tv

672
00:43:55,395 --> 00:43:55,685
Speaker 5:  Sick.

673
00:43:55,955 --> 00:43:59,925
Speaker 2:  This is, this is the plan. I know. What's the SRP on that? It's

674
00:43:59,925 --> 00:44:03,565
Speaker 2:  $379. They're, they're, they're just throwing you a $380 tv.

675
00:44:03,825 --> 00:44:07,045
Speaker 2:  I'm sure it looks like absolute trash. I'm sure it's the same panel as a

676
00:44:07,045 --> 00:44:10,965
Speaker 2:  frame tv. I gonna say just a single LED backlight

677
00:44:10,965 --> 00:44:12,365
Speaker 2:  just shining gray in the dark.

678
00:44:14,225 --> 00:44:17,485
Speaker 2:  But I just love that Samsung, they, they've got all the executives that are

679
00:44:17,485 --> 00:44:20,965
Speaker 2:  on Saturday. Just how are we gonna, it's a crisis you guys.

680
00:44:21,545 --> 00:44:24,485
Speaker 2:  How are we gonna, how are we gonna bring the stock price back up? And they're

681
00:44:24,485 --> 00:44:27,805
Speaker 2:  like, here's what we're gonna do. Everybody gets a 50 inch tv. No.

682
00:44:29,235 --> 00:44:33,165
Speaker 5:  Well you know that before that somebody was like, what if we made

683
00:44:33,165 --> 00:44:35,605
Speaker 5:  them cheaper? Someone was like, no, no, no, no, no, no, no.

684
00:44:35,605 --> 00:44:37,005
Speaker 6:  Just, just get rid of them. We're

685
00:44:37,005 --> 00:44:40,205
Speaker 5:  Gonna go more, we're gonna go the other way. Yeah, but you also get a television.

686
00:44:40,295 --> 00:44:43,565
Speaker 2:  We're gonna find the $380 of margin in every other product.

687
00:44:44,385 --> 00:44:48,125
Speaker 2:  And you know what everybody loves is a free garbage 50 inch

688
00:44:48,185 --> 00:44:51,925
Speaker 2:  TV If. you buy a tv, you get another tv, you buy

689
00:44:51,925 --> 00:44:55,285
Speaker 2:  Chromebook, you get a tv, you buy and you buy our brand new

690
00:44:55,285 --> 00:44:59,245
Speaker 2:  state-of-the-art copilot plus laptop free tv. That's,

691
00:44:59,245 --> 00:45:02,885
Speaker 2:  that'll do it. So you're considering a surface. Does a surface come with

692
00:45:02,925 --> 00:45:03,485
Speaker 2:  a free tv?

693
00:45:06,265 --> 00:45:10,195
Speaker 2:  This I'm, it's great. I cannot, this year of Saturdays is gonna be amazing.

694
00:45:11,065 --> 00:45:14,955
Speaker 2:  Okay. Sorry, I just literally, I said to David and Alex before we

695
00:45:14,955 --> 00:45:17,195
Speaker 2:  started the show, the only thing I wanted to talk about was Saturday Samsung

696
00:45:17,575 --> 00:45:20,635
Speaker 2:  and we forgot it's my fault. Just excited.

697
00:45:21,475 --> 00:45:24,275
Speaker 2:  I it's, I could keep going another hour on Saturday. Samsung. Alright. Right.

698
00:45:24,275 --> 00:45:28,195
Speaker 2:  We should talk about OpenAI is who I would say did not

699
00:45:28,265 --> 00:45:32,115
Speaker 2:  have as successful of a week in AI as Microsoft

700
00:45:33,895 --> 00:45:36,435
Speaker 2:  is. That's fair. Right? Fair. Even though Microsoft Fair, even big partner

701
00:45:36,535 --> 00:45:40,515
Speaker 2:  Sam Altman was on stage at build. But I, I would,

702
00:45:40,755 --> 00:45:43,395
Speaker 2:  I would characterize OpenAI didn as weak as definite.

703
00:45:44,625 --> 00:45:48,515
Speaker 5:  Yeah. Yeah. Own goal was a phrase I heard used a lot.

704
00:45:48,745 --> 00:45:52,475
Speaker 5:  Yeah. And it really is like a record week for

705
00:45:52,485 --> 00:45:55,355
Speaker 5:  don't tweet as advice to give on the internet.

706
00:45:56,695 --> 00:46:00,595
Speaker 5:  Really one of the all time what If you just didn't tweet moments

707
00:46:00,785 --> 00:46:01,915
Speaker 5:  from Sam Waltman this week.

708
00:46:02,975 --> 00:46:06,355
Speaker 2:  All right, let's run through it. It's, it's not a complicated story. It is

709
00:46:06,355 --> 00:46:09,875
Speaker 2:  getting more and more complicated as more more people talk, which is bad.

710
00:46:09,905 --> 00:46:13,235
Speaker 2:  That is a bad sign. If a seemingly simple story

711
00:46:13,535 --> 00:46:17,515
Speaker 2:  starts to get more and more complicated, lawyers are gonna get

712
00:46:17,515 --> 00:46:20,605
Speaker 2:  money. I will remind you what, what my

713
00:46:21,285 --> 00:46:24,405
Speaker 2:  contracts professor in law school used to say, Stuart McCauley, why did this

714
00:46:24,405 --> 00:46:28,015
Speaker 2:  case get filed? And we'd all say some idealistic first year law school

715
00:46:28,295 --> 00:46:30,775
Speaker 2:  bullshit. And he would say no, the lawyers wanted A BMW.

716
00:46:32,425 --> 00:46:34,745
Speaker 2:  A lot of lawyers are getting BMWs at the end of this rainbow.

717
00:46:35,185 --> 00:46:35,465
Speaker 5:  Yeah.

718
00:46:35,465 --> 00:46:38,385
Speaker 6:  Charlotte Johansen's lawyer is gonna be driving.

719
00:46:39,005 --> 00:46:40,585
Speaker 2:  He already bought, he's like, I'm good. He bought

720
00:46:40,585 --> 00:46:40,905
Speaker 6:  It from Disney.

721
00:46:41,255 --> 00:46:44,625
Speaker 2:  Yeah. He like pre he's getting another one to bmw. He like, he's in a rivian

722
00:46:44,625 --> 00:46:47,905
Speaker 2:  this time. He is like, just put a standing tab at the BM BMW

723
00:46:48,035 --> 00:46:52,025
Speaker 2:  dealership. Just whatever m is coming out next. Just

724
00:46:52,025 --> 00:46:55,905
Speaker 2:  gimme one of those. Although I've heard the XM is not great. Anyway. Okay,

725
00:46:55,905 --> 00:46:59,865
Speaker 2:  so here's the, the basic rundown. They announced GPT-4

726
00:47:00,065 --> 00:47:03,745
Speaker 2:  Oh, which in classic, I think it's broken. I don't believe you

727
00:47:03,745 --> 00:47:06,665
Speaker 2:  appears to hallucinate more than GPT-4. Great.

728
00:47:07,805 --> 00:47:11,305
Speaker 2:  But the, the headline feature of 4.0 is, it's

729
00:47:11,305 --> 00:47:14,345
Speaker 2:  multimodal. O is for Omni. It can look at stuff, it can talk to you, you

730
00:47:14,345 --> 00:47:17,985
Speaker 2:  can talk to it. And it has these, because it's multimodal.

731
00:47:18,425 --> 00:47:22,355
Speaker 2:  A lot of the demos are people chatting to it. And then

732
00:47:22,585 --> 00:47:25,715
Speaker 2:  open. I had these voices, they already had them.

733
00:47:26,455 --> 00:47:29,240
Speaker 2:  One of them was called Sky, which was a, a woman.

734
00:47:30,535 --> 00:47:34,515
Speaker 2:  But because of these demos, which were very voice heavy and

735
00:47:34,515 --> 00:47:37,795
Speaker 2:  very personality driven, because that's the other big feature of 4.0,

736
00:47:39,155 --> 00:47:42,955
Speaker 2:  a lot of people noticed that the Sky voice sounded a lot like Scarlet

737
00:47:43,275 --> 00:47:46,995
Speaker 2:  Hansen. And that sky in particular made a lot of people

738
00:47:46,995 --> 00:47:50,355
Speaker 2:  feel flirtatious. Right. Alex. I, I would say that's like the, I I

739
00:47:50,355 --> 00:47:53,155
Speaker 6:  Think, I think that's an accurate assessment of things people got a little

740
00:47:53,315 --> 00:47:54,715
Speaker 6:  horny for for Sky there. Yeah.

741
00:47:54,795 --> 00:47:57,115
Speaker 2:  A lot of people thought they could bang an iPad last week. Yeah.

742
00:47:57,115 --> 00:47:58,115
Speaker 6:  It was uncomfortable for

743
00:47:58,235 --> 00:48:00,875
Speaker 2:  Everyone. That's just, I mean I, I I don't know how else to characterize

744
00:48:00,895 --> 00:48:04,315
Speaker 2:  the way people feel about AI right now except an awful horny, awful lot of

745
00:48:04,315 --> 00:48:07,915
Speaker 2:  people wanna bang an iPad. It's fine. You know,

746
00:48:08,025 --> 00:48:10,595
Speaker 2:  it's like some people wanna give all their privacy away to a cloud. Other

747
00:48:10,595 --> 00:48:14,035
Speaker 2:  people, they have a particular dream. Okay, that's fine. You can, that

748
00:48:14,035 --> 00:48:16,035
Speaker 5:  Was the other dream. Satya Anella was talking about

749
00:48:16,385 --> 00:48:19,115
Speaker 2:  It's dream number two. Yeah. What If you could tag an iPad.

750
00:48:20,305 --> 00:48:24,275
Speaker 2:  Alright, so there's a lot of conversation about

751
00:48:24,305 --> 00:48:28,125
Speaker 2:  this voice, whether it sounds like Farhan or

752
00:48:28,125 --> 00:48:30,885
Speaker 2:  not. And there's like a reasonable debate. Some people are like, absolutely

753
00:48:30,885 --> 00:48:34,005
Speaker 2:  not. Some people are like, it does, people have asked

754
00:48:34,715 --> 00:48:38,405
Speaker 2:  open eye executives very directly. Our own Kylie Robinson asked Mira

755
00:48:38,965 --> 00:48:42,725
Speaker 2:  directly and she said, no, it was not our intention. No thought of this whatsoever.

756
00:48:42,985 --> 00:48:46,965
Speaker 2:  And then Sam Altman tweets the word her, which

757
00:48:46,965 --> 00:48:49,885
Speaker 2:  is the movie from Spike Jones at SARS Scarlet Hansen, which

758
00:48:49,885 --> 00:48:52,805
Speaker 6:  He says is one of his favorite movies of all time in an inspiration.

759
00:48:53,475 --> 00:48:57,045
Speaker 2:  Yeah. Great. So now he's undone his poor

760
00:48:57,675 --> 00:49:00,965
Speaker 2:  executive who was saying this was not the point. And then a very strange

761
00:49:00,965 --> 00:49:04,565
Speaker 2:  thing happens, opening eyes sort of out of nowhere publishes a blog post.

762
00:49:04,565 --> 00:49:06,005
Speaker 2:  It's like, here's how we selected our voices.

763
00:49:07,645 --> 00:49:11,585
Speaker 2:  That's a weird time to do it. and they, it's just a very odine corporate

764
00:49:11,615 --> 00:49:15,145
Speaker 2:  blog post. It's like we had a process. We did casting directors, blah, blah,

765
00:49:15,145 --> 00:49:19,105
Speaker 2:  blah blah. Then they had our Mia David, our AI reporter, Mia David

766
00:49:19,495 --> 00:49:23,385
Speaker 2:  talk to one of their executives to go through it. They would not

767
00:49:23,385 --> 00:49:25,265
Speaker 2:  say that. No. They pulled it first.

768
00:49:27,115 --> 00:49:28,445
Speaker 5:  They pulled it before they explained it.

769
00:49:28,755 --> 00:49:32,735
Speaker 2:  Yeah. Okay. So it's like a very normal corporate and I

770
00:49:32,765 --> 00:49:35,535
Speaker 2:  blog post, then they pull the voice

771
00:49:36,495 --> 00:49:40,345
Speaker 2:  like almost immediately after that they pull the voice sky. Sky, yeah. And

772
00:49:40,345 --> 00:49:44,145
Speaker 2:  say, based on concerns we've heard, we're pulling this

773
00:49:44,145 --> 00:49:47,745
Speaker 2:  voice. Which is interesting 'cause there's only one person's concern

774
00:49:48,095 --> 00:49:51,945
Speaker 2:  that matters. and she is a very powerful actress in Hollywood

775
00:49:51,945 --> 00:49:53,945
Speaker 2:  who was married to the head writer Saturday Night Life.

776
00:49:54,565 --> 00:49:58,425
Speaker 5:  So can I, can I, can I just pause you on that moment? I, I, I'm curious If,

777
00:49:58,565 --> 00:50:02,465
Speaker 5:  you guys had the same reaction that I did in that moment, which is

778
00:50:02,575 --> 00:50:06,225
Speaker 5:  obviously Scarlett Johansen is mad. Yep. Yeah, right there. I just couldn't

779
00:50:06,225 --> 00:50:09,865
Speaker 5:  think of any other, 'cause this happened on Sunday night or like overnight

780
00:50:09,865 --> 00:50:13,825
Speaker 5:  into Monday morning. I, I could not imagine a world in

781
00:50:13,825 --> 00:50:17,745
Speaker 5:  which they pulled this voice and put up this long blog

782
00:50:17,745 --> 00:50:21,705
Speaker 5:  post, which essentially amounted to one of those. Like my, we didn't

783
00:50:21,705 --> 00:50:25,305
Speaker 5:  copy Scarlet Johansen t-shirt is raising a lot of questions answered by my,

784
00:50:25,305 --> 00:50:29,285
Speaker 5:  we didn't copy Scarlet Johansen t-shirt, one of those moments. And we

785
00:50:29,285 --> 00:50:32,765
Speaker 5:  were, we were covering this thing like, okay, they were, they say they're

786
00:50:32,765 --> 00:50:36,085
Speaker 5:  just trying to allay confusion. Like, I couldn't think of another plausible

787
00:50:36,085 --> 00:50:39,845
Speaker 5:  reason other than Scarlet is yelling Yes.

788
00:50:39,845 --> 00:50:40,925
Speaker 5:  Right? Like, was that your reaction?

789
00:50:41,315 --> 00:50:45,205
Speaker 2:  It's the only thing be happening here. Yeah. Okay. Then they called

790
00:50:45,205 --> 00:50:48,165
Speaker 2:  me a David, one of our reporters and said, we have an interview with our

791
00:50:48,275 --> 00:50:52,045
Speaker 2:  executive. They would not tell Mia the timeline.

792
00:50:52,985 --> 00:50:56,365
Speaker 2:  The only question is, did Scarlet call you? And then you pulled the voice,

793
00:50:56,415 --> 00:50:59,565
Speaker 2:  right? Legitimately the only question here. And they're like, we heard these

794
00:50:59,565 --> 00:51:02,045
Speaker 2:  concerns, we want to get out. We pulled the voice and they just like, wouldn't

795
00:51:02,045 --> 00:51:05,805
Speaker 2:  say the timeline weird. So we run this story, they said, here's a

796
00:51:05,925 --> 00:51:06,125
Speaker 2:  headline.

797
00:51:07,705 --> 00:51:10,925
Speaker 2:  Our headline is like, pretty anodyne, right? It's also just like more OpenAI

798
00:51:11,065 --> 00:51:14,875
Speaker 2:  is talking. We pulled this voice out of concerns. Here's,

799
00:51:14,875 --> 00:51:18,435
Speaker 2:  here's some more color on that, except they won't say what the timeline is.

800
00:51:19,905 --> 00:51:23,895
Speaker 2:  Hours later, Scarlet Hansen has a statement out first to Bobby Allen at NPR

801
00:51:23,895 --> 00:51:27,735
Speaker 2:  and then Wily to everyone that says basically like, Sam

802
00:51:27,735 --> 00:51:31,655
Speaker 2:  Altman called me to be the voice of chat, GBT. And I

803
00:51:31,655 --> 00:51:35,445
Speaker 2:  said, no. And then, you know, several weeks later

804
00:51:35,625 --> 00:51:39,045
Speaker 2:  I'm listening to these demos and it sounds like my voice, I lo I lawyered

805
00:51:39,045 --> 00:51:41,205
Speaker 2:  up and I want to know exactly how this happened.

806
00:51:42,115 --> 00:51:46,005
Speaker 6:  Well, he called her like two days before the launch. Like he, he

807
00:51:46,005 --> 00:51:49,965
Speaker 6:  called her initially his, her timeline that she outlines is like, they talk,

808
00:51:49,965 --> 00:51:53,245
Speaker 6:  they chatted a while ago about it and she thought about it and then she said,

809
00:51:53,265 --> 00:51:55,845
Speaker 6:  no, I don't wanna do the voice. And he's like, cool. Didn't hear from him

810
00:51:55,845 --> 00:51:59,725
Speaker 6:  for a while. Two days before the demo, Sam Altman calls her

811
00:51:59,725 --> 00:52:03,125
Speaker 6:  and is like, Hey, you reconsidered. And she's like, no, I don't wanna do

812
00:52:03,125 --> 00:52:03,885
Speaker 6:  it. And he's like, cool.

813
00:52:04,425 --> 00:52:06,165
Speaker 2:  And then she hears a voice That sounds like her. Yep.

814
00:52:06,515 --> 00:52:09,325
Speaker 5:  That was a perfect Sam Altman impression by the way. Alex, like I know you

815
00:52:09,325 --> 00:52:11,125
Speaker 5:  were working on that. I worked really hard, worked well was really good.

816
00:52:11,175 --> 00:52:11,605
Speaker 5:  Thank you.

817
00:52:12,145 --> 00:52:13,605
Speaker 2:  It sounded like it was an all lowercase.

818
00:52:16,915 --> 00:52:20,605
Speaker 2:  Then there's just one last piece of reporting from Natasha Teko, another

819
00:52:20,605 --> 00:52:23,925
Speaker 2:  former version reporter at the Washington Post. She has talked to the agent

820
00:52:23,945 --> 00:52:27,805
Speaker 2:  of the voice actress that OpenAI cast for the Sky Voice. Both the actress

821
00:52:27,945 --> 00:52:31,485
Speaker 2:  and the agent wished to remain anonymous. I have a lot of feelings about

822
00:52:31,485 --> 00:52:34,405
Speaker 2:  that, but they wish to remain anonymous. And she's seen some documents that

823
00:52:34,405 --> 00:52:38,285
Speaker 2:  lay out a version of the timeline that says OpenAI actually recorded all

824
00:52:38,285 --> 00:52:42,085
Speaker 2:  this and never said to this actress, sound like Sol Jansen. So maybe

825
00:52:42,085 --> 00:52:45,845
Speaker 2:  there's some blurriness here. There's some old case law in the mix

826
00:52:45,845 --> 00:52:49,325
Speaker 2:  here. We, you know, we did a story, we talked to some lawyers. There's a

827
00:52:49,325 --> 00:52:52,605
Speaker 2:  very famous case with Tom Waits who has a very distinctive voice.

828
00:52:53,745 --> 00:52:57,045
Speaker 2:  By the way, Heath led was basically doing Tom Wait's impression in the dark

829
00:52:57,045 --> 00:52:59,805
Speaker 2:  night, which is very funny. But that's the voice like If. you think about

830
00:52:59,805 --> 00:53:02,245
Speaker 2:  Heath Ledger in the dark night, like Tom Waits has this very distinctive

831
00:53:02,245 --> 00:53:05,965
Speaker 2:  voice. There's a very famous commercial where they asked the singer to sound

832
00:53:05,965 --> 00:53:09,925
Speaker 2:  like Tom Waits, Tom Waits sued one. There are other tastes

833
00:53:09,925 --> 00:53:13,845
Speaker 2:  like this where people's likeness get used without their permission. My my

834
00:53:14,005 --> 00:53:17,085
Speaker 2:  favorite one of this is a Vana White v Samsung,

835
00:53:17,905 --> 00:53:21,845
Speaker 2:  yes. Maybe the best of these cases that exists. Samsung ran

836
00:53:21,885 --> 00:53:25,485
Speaker 2:  a commercial that looked like Wheel of Fortune where a robot was turning

837
00:53:25,485 --> 00:53:28,285
Speaker 2:  the letters and Vanna White said, that robot is me

838
00:53:29,425 --> 00:53:30,805
Speaker 2:  and one. Wow.

839
00:53:31,035 --> 00:53:33,245
Speaker 6:  Yeah, it seems very good. She felt like a monopoly.

840
00:53:33,825 --> 00:53:37,085
Speaker 2:  It is. I, with all of the cases I ever read in law school, I was like, this

841
00:53:37,085 --> 00:53:40,205
Speaker 2:  is why I came here. Vanna White v Sam Electronics.

842
00:53:41,185 --> 00:53:44,845
Speaker 2:  So there's like a lot of these weird cases, like you are not allowed to trade,

843
00:53:45,305 --> 00:53:49,005
Speaker 2:  in particular in New York and California, which is where most of the celebrities

844
00:53:49,705 --> 00:53:53,485
Speaker 2:  are. The state law does not allow you to trade on the license of celebrities

845
00:53:53,485 --> 00:53:56,925
Speaker 2:  to sell your products. That's just a thing we have not gotten

846
00:53:57,625 --> 00:54:01,485
Speaker 2:  to. We cloned a voice using another actress and everyone

847
00:54:01,685 --> 00:54:05,405
Speaker 2:  got confused. That's new. That's a new problem for our court system that

848
00:54:05,405 --> 00:54:09,325
Speaker 2:  I'm very much looking forward to seeing litigated. But, but we're

849
00:54:09,325 --> 00:54:12,925
Speaker 2:  in this place where we've expanded the boundary of what you can take.

850
00:54:13,725 --> 00:54:17,315
Speaker 2:  Right. And the AI companies are really just taking a lot of stuff without

851
00:54:17,315 --> 00:54:21,235
Speaker 2:  asking for a lot of permission. And I think this one with Scarlet

852
00:54:21,235 --> 00:54:24,595
Speaker 2:  Jen Hansen and Sam Altman in this completely insane timeline that is getting

853
00:54:24,595 --> 00:54:28,275
Speaker 2:  complicated, the more anyone talks is, I think really,

854
00:54:28,575 --> 00:54:31,795
Speaker 2:  you know, we talked about it in the earlier segment. People have really,

855
00:54:32,095 --> 00:54:36,035
Speaker 2:  really an like, antagonistic feelings towards AI because it feels like

856
00:54:36,035 --> 00:54:39,955
Speaker 2:  it's taking stuff from us. And a famous actress

857
00:54:40,175 --> 00:54:43,995
Speaker 2:  who was in a bunch of Marvel movies, she's not the

858
00:54:43,995 --> 00:54:45,995
Speaker 2:  right adversary for Opening Eyes.

859
00:54:46,015 --> 00:54:46,915
Speaker 6:  Who sued Disney,

860
00:54:47,255 --> 00:54:49,915
Speaker 2:  Who sued Disney, right? Like, I don't know, man,

861
00:54:50,535 --> 00:54:54,395
Speaker 6:  That's not the person you wanna like mess with. Yeah. I, it was just

862
00:54:54,395 --> 00:54:57,835
Speaker 6:  kind of gobsmacked by this because, you know, I think

863
00:54:58,105 --> 00:55:01,955
Speaker 6:  like Lake Bell plays does a, a really good scar, Scarlet

864
00:55:01,955 --> 00:55:05,475
Speaker 6:  Johansson impression on an animated show where she's playing Black Widow

865
00:55:05,815 --> 00:55:09,395
Speaker 6:  and, and she sounds a lot like her. It's great. But also she's playing Black

866
00:55:09,395 --> 00:55:12,795
Speaker 6:  Widow and, and, and there's, there's a whole, there's like agreements in

867
00:55:12,795 --> 00:55:16,755
Speaker 6:  place and that's why it's allowed this is she, this

868
00:55:16,755 --> 00:55:20,395
Speaker 6:  person sounded a lot like Scarlett Johanssen and then Sam

869
00:55:20,675 --> 00:55:24,355
Speaker 6:  Altman tweeted her clearly referencing this.

870
00:55:24,465 --> 00:55:28,195
Speaker 6:  He's talked about how much affection he has for that voice, for that

871
00:55:28,195 --> 00:55:31,925
Speaker 6:  concept. It's just like, oh, you really just kept walking into

872
00:55:31,975 --> 00:55:35,885
Speaker 6:  rakes on this one dude. Like, just, just stop, stay

873
00:55:36,015 --> 00:55:39,845
Speaker 6:  stand still for a moment because yeah, they probably did hire this

874
00:55:39,845 --> 00:55:43,285
Speaker 6:  woman and she probably didn't know that she sounded a lot like

875
00:55:43,355 --> 00:55:46,285
Speaker 6:  Scarlet Johansson or that that's why they hired her, right? Like

876
00:55:46,535 --> 00:55:49,165
Speaker 5:  There are so many weird twists and turns of all of this, right? 'cause then

877
00:55:49,165 --> 00:55:52,525
Speaker 5:  there's the question of was this voice

878
00:55:52,915 --> 00:55:56,765
Speaker 5:  made to sound like Samantha from her or does it sound like

879
00:55:56,765 --> 00:56:00,485
Speaker 5:  Scarlett Johansen? And is there a distinction between those two things and

880
00:56:00,485 --> 00:56:03,805
Speaker 5:  is that meaningful? And how much do people associate the voice of Samantha

881
00:56:03,825 --> 00:56:07,085
Speaker 5:  and the role of Samantha with Scarlett Johanssen? That's just like, all of

882
00:56:07,085 --> 00:56:10,685
Speaker 5:  this is unprecedented in such bizarre ways. But the thing that keeps jumping

883
00:56:10,685 --> 00:56:14,445
Speaker 5:  out to me is that the immediate

884
00:56:15,125 --> 00:56:18,685
Speaker 5:  response was that of course OpenAI is,

885
00:56:19,145 --> 00:56:23,085
Speaker 5:  is wrong. Yes. 'cause of course OpenAI would've just gone

886
00:56:23,305 --> 00:56:27,285
Speaker 5:  and ingested the movie and used that. I mean, we've seen

887
00:56:27,285 --> 00:56:29,845
Speaker 5:  the technology now that you can use five minutes of somebody's voice and

888
00:56:29,845 --> 00:56:33,205
Speaker 5:  spit out a pretty passable version of that voice. Like

889
00:56:33,555 --> 00:56:37,085
Speaker 5:  just the immediate assumption that that's what OpenAI didn was. Either

890
00:56:37,835 --> 00:56:41,525
Speaker 5:  find a person and tell them to do a Scarlet Johansen impression or screw

891
00:56:41,645 --> 00:56:45,525
Speaker 5:  all that. Just train your model on Samantha and her

892
00:56:45,585 --> 00:56:49,365
Speaker 5:  and call it a day that like this company has so aggressively lost the benefit

893
00:56:49,425 --> 00:56:52,885
Speaker 5:  of the doubt. It made me think of the, the iPad Crush commercial in the same

894
00:56:52,885 --> 00:56:55,845
Speaker 5:  way. Yeah. Yeah. That it's just like this is a company that people do not

895
00:56:55,855 --> 00:56:59,485
Speaker 5:  trust and people think the worst of now and

896
00:56:59,485 --> 00:57:03,405
Speaker 5:  believe that their intentions are bad. And I think the, the thing with

897
00:57:03,405 --> 00:57:07,125
Speaker 5:  the OpenAI thing is, I think the answer is probably some weird

898
00:57:07,125 --> 00:57:10,325
Speaker 5:  middle sketchy thing. And it seems like the OpenAI version of the story is

899
00:57:10,325 --> 00:57:14,285
Speaker 5:  that OpenAI didn ran a pretty normal process. And then Sam Altman, who is

900
00:57:14,285 --> 00:57:18,045
Speaker 5:  the CEO of OpenAI, just like weird, went rogue and started calling

901
00:57:18,045 --> 00:57:21,365
Speaker 5:  Scarlett Johansen. And now there are all these things out there about how

902
00:57:21,665 --> 00:57:24,565
Speaker 5:  Sam just desperately wants to be famous and it's making him make mistakes.

903
00:57:24,865 --> 00:57:27,965
Speaker 5:  And like we've gone in all of these weird directions. Wait,

904
00:57:27,965 --> 00:57:31,845
Speaker 2:  Hold on. Can I just say I hate it when companies pretend

905
00:57:31,845 --> 00:57:34,485
Speaker 2:  they're CEOs are idiots? I hate

906
00:57:34,485 --> 00:57:35,085
Speaker 5:  Totally fair.

907
00:57:35,385 --> 00:57:39,325
Speaker 2:  Sam Alman is a billionaire. He is the founder of

908
00:57:39,385 --> 00:57:43,325
Speaker 2:  OpenAI. He is so much in charge of OpenAI is that when the board of

909
00:57:43,545 --> 00:57:47,205
Speaker 2:  OpenAI fired him for being reckless and manipulative, the

910
00:57:47,365 --> 00:57:51,285
Speaker 2:  employees all staged a reverse coup and brought him back. That's

911
00:57:51,285 --> 00:57:55,205
Speaker 2:  real. And it's like, yeah, that's a, a real thing that happened just a little

912
00:57:55,405 --> 00:57:58,725
Speaker 2:  bit ago. Yeah. A couple months ago because the board of directors was like,

913
00:57:58,785 --> 00:58:02,605
Speaker 2:  we don't trust Sam Altman and fired him and he

914
00:58:02,705 --> 00:58:06,245
Speaker 2:  got his way back in the company. He has a new board of directors and now

915
00:58:06,405 --> 00:58:09,805
Speaker 2:  a bunch of employees are quitting. 'cause opening eye is generally abandoning

916
00:58:09,805 --> 00:58:13,445
Speaker 2:  It's like safety culture. Yeah. And he's doing this reckless stuff and I

917
00:58:13,445 --> 00:58:16,525
Speaker 2:  just don't buy it that the company is like, yeah, we are running responsibly.

918
00:58:16,675 --> 00:58:20,565
Speaker 2:  There's just this hot head that we reverse cooed back into the CEO role

919
00:58:20,675 --> 00:58:22,165
Speaker 2:  that we can't control. Like you,

920
00:58:22,165 --> 00:58:26,045
Speaker 6:  You can't like, like it's just a full like lie on their part

921
00:58:26,045 --> 00:58:28,645
Speaker 6:  to be like, we have no control over him. It's like, well he runs the company.

922
00:58:29,185 --> 00:58:29,405
Speaker 6:  So

923
00:58:29,405 --> 00:58:32,565
Speaker 5:  Like I agree with that, that, and yet I also find it

924
00:58:33,005 --> 00:58:36,925
Speaker 5:  absolutely plausible that the billionaire CEO who is kind of

925
00:58:36,925 --> 00:58:39,445
Speaker 5:  feeling himself would call Scarlett Johansen on the side

926
00:58:39,545 --> 00:58:41,045
Speaker 2:  And try to get it true. Oh, that's, but yeah, no, if I had a billion dollars

927
00:58:41,425 --> 00:58:44,565
Speaker 2:  and I was like, my robot can plausibly make Kevin Roos think that it's in

928
00:58:44,565 --> 00:58:47,205
Speaker 2:  love with him. Like I would call Scarlett Janssen. you

929
00:58:47,205 --> 00:58:50,245
Speaker 5:  Don see what would happen. You don't tell these people who are doing the

930
00:58:50,245 --> 00:58:54,125
Speaker 5:  casting process who then you are happy to throw to the wolves after the

931
00:58:54,125 --> 00:58:57,965
Speaker 5:  fact. But like the, the bones of that

932
00:58:57,965 --> 00:59:01,205
Speaker 5:  version of the story do not seem implausible to me at all.

933
00:59:02,025 --> 00:59:05,325
Speaker 5:  But it also doesn't matter. He's the CEO of the company. It doesn't matter.

934
00:59:05,375 --> 00:59:09,085
Speaker 2:  Right. The part where they ultimately shipped the voice that sounded like

935
00:59:09,505 --> 00:59:13,285
Speaker 2:  her in every sense of that word. And he knew

936
00:59:13,285 --> 00:59:16,645
Speaker 2:  about it and those were the demos. And I'm confident that he watched every

937
00:59:16,645 --> 00:59:17,925
Speaker 2:  second of those demos. He was

938
00:59:17,925 --> 00:59:18,725
Speaker 5:  In the, and he tweeted

939
00:59:18,865 --> 00:59:22,765
Speaker 2:  In the audience the word her. Yeah. Those are his choices. Yeah. He

940
00:59:22,765 --> 00:59:22,885
Speaker 2:  knew,

941
00:59:23,025 --> 00:59:23,245
Speaker 5:  He

942
00:59:23,245 --> 00:59:24,005
Speaker 2:  Tweeted the word her.

943
00:59:24,785 --> 00:59:25,565
Speaker 5:  He knew the whole thing. It's

944
00:59:25,565 --> 00:59:27,325
Speaker 2:  Hard to undo that thing. By the way, your question about whether it's the

945
00:59:27,325 --> 00:59:30,445
Speaker 2:  movie Her or Raja Hansen, I will offer you the

946
00:59:30,685 --> 00:59:34,445
Speaker 2:  1995 case Metro Goldwin Meyer, the American Honda

947
00:59:34,445 --> 00:59:38,285
Speaker 2:  Company where Honda made an ad for the Civic del Soul that

948
00:59:38,285 --> 00:59:42,205
Speaker 2:  looked a little bit too much like a James Bond movie and lost

949
00:59:42,325 --> 00:59:46,285
Speaker 2:  to MGM. Yes. Really? It was, it was not James James Bond. They

950
00:59:46,285 --> 00:59:49,685
Speaker 2:  made too good a commercial. It was a guy in a suit jumping out the, you know,

951
00:59:49,805 --> 00:59:53,445
Speaker 2:  the, I don't remember the civic tell soul. That's not a

952
00:59:53,655 --> 00:59:57,565
Speaker 2:  high watermark and civic design, but it was a civic with a, a top that you

953
00:59:57,725 --> 01:00:01,485
Speaker 2:  could like take off. And so the ad was like

954
01:00:01,515 --> 01:00:05,365
Speaker 2:  he's driving James A. James Bond like character is driving a Honda

955
01:00:05,365 --> 01:00:08,645
Speaker 2:  Civic, very plausible. And he blows the top off and jumps outta the car and

956
01:00:08,405 --> 01:00:11,805
Speaker 2:  does some James Bond stuff. And MGM said that that is James Bond. You made

957
01:00:11,945 --> 01:00:14,765
Speaker 2:  James Bond. Wow. You can't have it. and they

958
01:00:16,225 --> 01:00:20,005
Speaker 2:  won. Yes. That's so dumb. I'm excited for us to go

959
01:00:20,475 --> 01:00:24,445
Speaker 2:  to court against OpenAI and say, your robot, the

960
01:00:24,445 --> 01:00:28,325
Speaker 2:  precedent by which we are suing you is MGM versus American. Have

961
01:00:28,325 --> 01:00:32,245
Speaker 2:  you seen the Civic del Soul judge? MGM by the way, also

962
01:00:32,245 --> 01:00:36,105
Speaker 2:  now owned by Amazon. Just one of those

963
01:00:36,185 --> 01:00:39,985
Speaker 2:  things. Just a weird moment in time

964
01:00:39,985 --> 01:00:43,545
Speaker 2:  because again, most of these cases took place in New York and

965
01:00:44,095 --> 01:00:47,815
Speaker 2:  California ages ago. Most of them are about pretty

966
01:00:47,815 --> 01:00:50,415
Speaker 2:  normal ads, right? Like that's why you appropriate a celebrity's likeness

967
01:00:50,415 --> 01:00:54,135
Speaker 2:  to try to sell something. Those days are over. Like TikTok is

968
01:00:54,135 --> 01:00:57,855
Speaker 2:  full of weird, deep fake celebrities and just like

969
01:00:57,855 --> 01:01:00,935
Speaker 2:  pure copyright infringement. The number of companies right now If, you run

970
01:01:01,015 --> 01:01:04,855
Speaker 2:  a regular company and you wanna buy a regular ad and you're like, I'd like

971
01:01:04,855 --> 01:01:08,815
Speaker 2:  to use Taylor Swift to sell my towels. You can't. But that's

972
01:01:08,815 --> 01:01:12,415
Speaker 2:  too expensive. If you're a super shady distributor of

973
01:01:12,565 --> 01:01:16,415
Speaker 2:  Alibaba towels and you're on TikTok, it's all Taylor Swift all

974
01:01:16,415 --> 01:01:20,375
Speaker 2:  day long deep fakes baby. It's crazy. And like there's a

975
01:01:20,395 --> 01:01:23,855
Speaker 2:  gap here that's happening where the law has absolutely not caught up to reality.

976
01:01:24,255 --> 01:01:27,055
Speaker 2:  'cause most of these cases are about big ad agencies and big companies with

977
01:01:27,055 --> 01:01:30,815
Speaker 2:  big budgets running ads. Right? I don't know

978
01:01:31,015 --> 01:01:34,295
Speaker 2:  that you we're just gonna, we're just in for a moment of chaos

979
01:01:34,565 --> 01:01:38,175
Speaker 2:  because the reality of what's happening on the ground is people do not care

980
01:01:38,175 --> 01:01:42,055
Speaker 2:  about the law as particularly IP law and the big companies

981
01:01:42,055 --> 01:01:45,655
Speaker 2:  Google OpenAI, they are the next generation of their

982
01:01:45,655 --> 01:01:49,335
Speaker 2:  technology is just founded on taking stuff,

983
01:01:49,705 --> 01:01:52,735
Speaker 2:  which is why OpenAI does not have the benefit of the doubt. Everyone believes

984
01:01:52,735 --> 01:01:56,575
Speaker 2:  they took whatever they want and if they wanted to, they, they would

985
01:01:56,965 --> 01:02:00,135
Speaker 2:  just make a clone of Scarlet Hanson's voice. Whether or not they cast an

986
01:02:00,135 --> 01:02:03,375
Speaker 2:  actress who sounded just like her. It doesn't matter. Because if the technology

987
01:02:03,375 --> 01:02:06,135
Speaker 2:  exists for them to just do it and no one thinks they have the control to

988
01:02:06,135 --> 01:02:09,615
Speaker 2:  not do it. Weird. Yeah. A weird moment for them.

989
01:02:09,925 --> 01:02:10,215
Speaker 2:  Yeah.

990
01:02:10,595 --> 01:02:14,575
Speaker 5:  And I, at least from what I've heard so far the last couple of days, I'm

991
01:02:14,575 --> 01:02:18,495
Speaker 5:  curious if y'all have heard anything different, is that this, there's

992
01:02:18,495 --> 01:02:22,245
Speaker 5:  no indication so far that Scarlett Johanssen is

993
01:02:22,485 --> 01:02:25,045
Speaker 5:  actually going to like, take real legal action here. She put out that one

994
01:02:25,045 --> 01:02:28,765
Speaker 5:  statement and we don't know what's gonna happen next, but that if she

995
01:02:28,825 --> 01:02:31,565
Speaker 5:  did this would have a real chance of

996
01:02:32,535 --> 01:02:36,085
Speaker 5:  going somewhere and being pretty important and precedent setting

997
01:02:37,065 --> 01:02:40,605
Speaker 5:  in terms of like, it just feels like in so many ways we're itching for like

998
01:02:40,625 --> 01:02:44,485
Speaker 5:  the one weird AI lawsuit and like, maybe it's the New

999
01:02:44,485 --> 01:02:48,125
Speaker 5:  York Times one, but it would be frankly a lot like Wilder

1000
01:02:48,125 --> 01:02:50,645
Speaker 5:  and weirder and more fun if it was Scarlett Johanssen.

1001
01:02:50,955 --> 01:02:54,405
Speaker 6:  Well, I think they're probably kind of different cases. Like they're, they're

1002
01:02:54,405 --> 01:02:57,245
Speaker 6:  targeting different things, but they get the same thrust of things, which

1003
01:02:57,245 --> 01:02:59,245
Speaker 6:  is can you just take shit?

1004
01:02:59,765 --> 01:03:02,445
Speaker 5:  Yeah. Like what are you allowed to do in the name of That's very

1005
01:03:02,445 --> 01:03:02,525
Speaker 2:  Nice.

1006
01:03:03,525 --> 01:03:04,965
Speaker 6:  Ultimately it's can you just do stuff?

1007
01:03:06,235 --> 01:03:09,205
Speaker 2:  Yeah. Yeah. Right. I mean like, if, if I do it with a computer, it's fine.

1008
01:03:09,205 --> 01:03:12,525
Speaker 2:  Right? It's like that's the answer. And there look, there are very meaningful

1009
01:03:12,525 --> 01:03:14,965
Speaker 2:  differences between what you can do at a computer. I can do it real life.

1010
01:03:14,965 --> 01:03:18,165
Speaker 2:  Like for example, Alex, if I came to your house and took one of your Blu-rays

1011
01:03:18,165 --> 01:03:21,885
Speaker 2:  away, I that would be a crime. Yeah. For many reasons. One, I

1012
01:03:21,955 --> 01:03:24,765
Speaker 2:  just a crime against our friendship, you know? Right. I'd very obsessed,

1013
01:03:24,885 --> 01:03:28,445
Speaker 2:  but importantly you would not have the Blu-ray anymore. Whereas if I came

1014
01:03:28,445 --> 01:03:31,205
Speaker 2:  to your house and copied a file off your PL server and left, I think you'd

1015
01:03:31,205 --> 01:03:33,325
Speaker 2:  be like, well, and our friendship has strengthened. Yeah.

1016
01:03:33,705 --> 01:03:33,925
Speaker 6:  And

1017
01:03:35,025 --> 01:03:38,885
Speaker 2:  You would also have the original copy, your very legal copy

1018
01:03:38,885 --> 01:03:39,525
Speaker 2:  of whatever content. Yeah, totally.

1019
01:03:39,925 --> 01:03:43,045
Speaker 6:  I would have the Blu-ray still in my closet. Yes, that's correct.

1020
01:03:43,865 --> 01:03:47,725
Speaker 2:  Th this is like a meaningful thing in copyright law. Whereas, you know, it

1021
01:03:47,725 --> 01:03:50,965
Speaker 2:  was all, it's all founded on physical scarcity. It has not translated to

1022
01:03:50,965 --> 01:03:54,525
Speaker 2:  the world of like digital very well. On the other hand,

1023
01:03:55,765 --> 01:03:58,845
Speaker 2:  I can just do whatever I want. 'cause it's a computer is not a workable

1024
01:04:00,125 --> 01:04:03,605
Speaker 2:  strategy either. Right. You know, like there's some middle ground here that

1025
01:04:03,625 --> 01:04:06,965
Speaker 2:  no one has ever really thought through. And I think that AI conversations,

1026
01:04:06,985 --> 01:04:10,805
Speaker 2:  you know, this is a conversation I had with Sundar, like, do you feel great

1027
01:04:10,805 --> 01:04:14,645
Speaker 2:  about OpenAI training on YouTube? Okay. The web doesn't feel that way about

1028
01:04:14,645 --> 01:04:18,405
Speaker 2:  you. And he was like, that's weird. You know, like everyone has to wrestle

1029
01:04:18,475 --> 01:04:22,245
Speaker 2:  with those two ideas at the same time while we're saying, Hey,

1030
01:04:22,245 --> 01:04:25,525
Speaker 2:  some of this technology is like really cool. Like, you want some of this

1031
01:04:25,525 --> 01:04:29,245
Speaker 2:  technology, some of this stuff is really great for people who have, who are

1032
01:04:29,245 --> 01:04:33,125
Speaker 2:  differently abled. Right? Like who, how are we gonna solve these problems?

1033
01:04:33,585 --> 01:04:37,405
Speaker 2:  And I Alex I think you're definitely right that like the training law, the

1034
01:04:37,605 --> 01:04:41,285
Speaker 2:  training lawsuit is a copyright lawsuit. And this other

1035
01:04:41,285 --> 01:04:45,165
Speaker 2:  stuff is like a likeness lawsuit, a right of publicity lawsuit. They're different

1036
01:04:45,165 --> 01:04:48,765
Speaker 2:  bodies of law that are very hard to distinguish for normal people.

1037
01:04:49,185 --> 01:04:51,765
Speaker 2:  Normal people can't tell you the difference between a copyright and a trademark

1038
01:04:51,765 --> 01:04:55,245
Speaker 2:  and a patent. Right. Like this is even more esoteric than that.

1039
01:04:56,065 --> 01:04:59,405
Speaker 2:  And on top of it, sort of the inherent nature of it is that these are celebrities.

1040
01:05:00,675 --> 01:05:04,165
Speaker 2:  Like you can't, you don't have a right of publicity. You don't have publicity.

1041
01:05:04,855 --> 01:05:08,565
Speaker 2:  Right. And like all of the law is built around celebrity. And so

1042
01:05:08,705 --> 01:05:12,645
Speaker 2:  you're just gonna get a sequence of people, of plaintiffs

1043
01:05:12,705 --> 01:05:16,605
Speaker 2:  who are just the most sympathetic because they are gonna be celebrities.

1044
01:05:16,705 --> 01:05:20,445
Speaker 2:  Yep. And at some point, you know, like we did an

1045
01:05:20,445 --> 01:05:23,125
Speaker 2:  episode, Sarah John and I did an episode of Decoder where we talked about

1046
01:05:23,125 --> 01:05:25,885
Speaker 2:  the copyright law. And like that feels like a time bomb waiting to go off.

1047
01:05:26,155 --> 01:05:29,925
Speaker 2:  Something's gonna happen with copyright law and training data. It, it is

1048
01:05:29,925 --> 01:05:32,325
Speaker 2:  inevitable that one of those cases goes the wrong way. 'cause there's so

1049
01:05:32,325 --> 01:05:36,145
Speaker 2:  many of them now. It feels inevitable that particularly is

1050
01:05:36,955 --> 01:05:40,775
Speaker 2:  these companies wanna trade on famous voices even

1051
01:05:40,775 --> 01:05:44,745
Speaker 2:  if it's not this case that, I don't know man, like Kevin

1052
01:05:44,745 --> 01:05:47,785
Speaker 2:  Hart is gonna be like, that sounds too much like me, just like sue Amazon.

1053
01:05:47,785 --> 01:05:50,785
Speaker 2:  Like it's like one of those kinds of things is gonna happen. I mean

1054
01:05:50,785 --> 01:05:53,945
Speaker 5:  It's this, it's the music thing, right? Yeah. Like this, it's, it's all of

1055
01:05:54,285 --> 01:05:58,265
Speaker 5:  the, the like copyright trolls coming after anyone who writes

1056
01:05:58,265 --> 01:06:02,145
Speaker 5:  a song with power chords in guitars now. Yeah. Like, we're, we're gonna do

1057
01:06:02,145 --> 01:06:04,105
Speaker 5:  that again at a crazy scale. I

1058
01:06:04,105 --> 01:06:07,905
Speaker 6:  Feel it's a little almost simpler than that because it, it,

1059
01:06:07,905 --> 01:06:11,665
Speaker 6:  it's kind of straightforward that if Amazon had gone and released an

1060
01:06:11,665 --> 01:06:15,505
Speaker 6:  Alexa voice that was Kevin Hart and it wasn't actually Kevin Hart, he would've

1061
01:06:15,565 --> 01:06:19,545
Speaker 6:  sued, but Amazon went and said, Hey Kevin Hart or Samuel Hall Jackson or

1062
01:06:19,545 --> 01:06:21,185
Speaker 6:  whoever, do you wanna be a voice? No. Do you wanna, but

1063
01:06:21,185 --> 01:06:23,505
Speaker 2:  They have done that Bunch Money Jackson is a voice on on

1064
01:06:23,595 --> 01:06:27,225
Speaker 6:  Alexa. Yeah. They paid him. Yeah. And, and so like, like I think in this

1065
01:06:27,225 --> 01:06:30,145
Speaker 6:  case there's a lot of precedent of like everybody else knew how to do this

1066
01:06:30,505 --> 01:06:33,385
Speaker 6:  business and OpenAI is like

1067
01:06:34,335 --> 01:06:34,625
Speaker 6:  Yolo.

1068
01:06:35,055 --> 01:06:38,145
Speaker 2:  Yeah. By the way, I dunno what's wrong with me. That I had to immediately

1069
01:06:38,145 --> 01:06:41,145
Speaker 2:  think of it. That I had to quickly think of a very distinctive voice and

1070
01:06:41,145 --> 01:06:42,985
Speaker 2:  my mind went immediately to Kevin Hart.

1071
01:06:43,185 --> 01:06:45,345
Speaker 6:  I wasn't gonna comment on it, but that's just what I

1072
01:06:45,345 --> 01:06:45,425
Speaker 2:  Had

1073
01:06:45,935 --> 01:06:47,465
Speaker 6:  Comment on it to you later in Slack.

1074
01:06:47,575 --> 01:06:50,905
Speaker 2:  It's a very distinctive voice. I just don't, I just dunno.

1075
01:06:51,465 --> 01:06:53,785
Speaker 5:  I often think of you as the Kevin Hart of The Vergecast so that

1076
01:06:54,055 --> 01:06:55,305
Speaker 2:  It's time to take a break David.

1077
01:06:58,925 --> 01:07:02,385
Speaker 2:  And we'll see if you're back when we come back for the lightning round.

1078
01:07:04,675 --> 01:07:07,945
Speaker 2:  Weird time. It's kind of broken and I dunno if I believe you, we'll be right

1079
01:07:08,185 --> 01:07:08,425
Speaker 2:  back.

1080
01:10:11,335 --> 01:10:11,945
Speaker 5:  It's beautiful.

1081
01:10:11,945 --> 01:10:15,905
Speaker 2:  Dreams. Dreams do come true. Alright, lightning

1082
01:10:15,905 --> 01:10:18,545
Speaker 2:  round. We gotta start with this one. It's maybe the most important lightning

1083
01:10:18,545 --> 01:10:22,065
Speaker 2:  round item. The United States government, the Department of Justice has

1084
01:10:22,065 --> 01:10:25,625
Speaker 2:  sued Ticketmaster Live Nation. I'm shown Ticketmaster. They want to break

1085
01:10:25,625 --> 01:10:28,785
Speaker 2:  it up. This has been a long time coming. I would

1086
01:10:30,335 --> 01:10:33,705
Speaker 2:  say this is Taylor Swift lawsuit. It feels fully like Taylor Swift lawsuit.

1087
01:10:34,135 --> 01:10:38,105
Speaker 5:  Yeah. Yeah. It really is. And this has been I

1088
01:10:38,105 --> 01:10:41,705
Speaker 5:  think kind of in the wind ever since the merger

1089
01:10:42,065 --> 01:10:45,985
Speaker 5:  happened. I mean in 2010 right? Was when the merger was, and even

1090
01:10:45,985 --> 01:10:49,425
Speaker 5:  then, if I remember right, there was real, this should not be allowed

1091
01:10:49,565 --> 01:10:49,985
Speaker 5:  energy.

1092
01:10:50,805 --> 01:10:54,585
Speaker 2:  Oh this was fully If. you wanna ever make the case that Brock Obama was kinda

1093
01:10:54,585 --> 01:10:58,545
Speaker 2:  like a bad president? His competition policy where he just like,

1094
01:10:58,545 --> 01:11:02,345
Speaker 2:  let this stuff slide like his DOJ and his FTC just let this stuff slide

1095
01:11:02,345 --> 01:11:06,265
Speaker 2:  all over the place. Like full Reagan. Like this is the

1096
01:11:06,265 --> 01:11:10,105
Speaker 2:  core of that argument. Yeah. Like when, when people make it like Obama

1097
01:11:10,145 --> 01:11:13,845
Speaker 2:  presided over the great recession and all this other stuff in recovery. Part

1098
01:11:13,845 --> 01:11:17,725
Speaker 2:  of it was we are just gonna let these mergers slide to bring the

1099
01:11:17,725 --> 01:11:21,405
Speaker 2:  economy back. And now we're on the other side of it. You can also blame Ronald

1100
01:11:21,605 --> 01:11:24,645
Speaker 2:  Reagan. I I prefer to blame Ronald Reagan when we made an entire decoder

1101
01:11:24,645 --> 01:11:28,485
Speaker 2:  video blaming Ronald Reagan for Ticketmaster. You can watch

1102
01:11:28,485 --> 01:11:32,285
Speaker 2:  that video. But basically the heart of the argument is Ticketmaster

1103
01:11:32,745 --> 01:11:36,205
Speaker 2:  Live Nation owns ticket sales, obviously promotion artist management. They

1104
01:11:36,205 --> 01:11:39,805
Speaker 2:  own all the venues. They own 60 of the top 100 venues in the United States.

1105
01:11:39,805 --> 01:11:43,205
Speaker 2:  And so you just have this vertically integrated monopoly and if an

1106
01:11:43,615 --> 01:11:47,485
Speaker 2:  artist tries to screw with it, ticket master crushes them, which means

1107
01:11:47,585 --> 01:11:48,765
Speaker 2:  prices go up. 'cause there's no competition.

1108
01:11:49,235 --> 01:11:53,125
Speaker 5:  It's actually, in a very funny way, it is sort of perfect

1109
01:11:54,045 --> 01:11:57,765
Speaker 5:  textbook monopoly stuff. Like the allegations are so simple

1110
01:11:57,825 --> 01:12:00,405
Speaker 5:  to understand. I actually encourage people to go read the lawsuit because

1111
01:12:00,425 --> 01:12:03,885
Speaker 5:  it really is like If, you want to understand what a monopoly looks like.

1112
01:12:03,945 --> 01:12:07,885
Speaker 5:  It is what these allegations say Live Nation and Ticketmaster

1113
01:12:07,885 --> 01:12:11,805
Speaker 5:  does, which is own everything and just use that to beat

1114
01:12:11,805 --> 01:12:14,925
Speaker 5:  the hell out of anybody who tries to own anything and come at you. Yeah.

1115
01:12:14,985 --> 01:12:18,405
Speaker 5:  And like the, the threats that they are alleged to make against

1116
01:12:18,515 --> 01:12:21,885
Speaker 5:  artists and venues who don't work with them, the ways that they like raise

1117
01:12:21,885 --> 01:12:25,405
Speaker 5:  fees to keep everybody out and like self deal to each other. Like it's, it

1118
01:12:25,405 --> 01:12:28,845
Speaker 5:  is pure monopoly stuff. If all of these allegations are true, it's,

1119
01:12:28,905 --> 01:12:32,325
Speaker 2:  And at the end of it, at the very bottom of it, yep. Prices are high, but

1120
01:12:32,475 --> 01:12:36,405
Speaker 2:  service quality is low. Right. Like they don't have to invest in the ticketing

1121
01:12:36,685 --> 01:12:40,405
Speaker 2:  platform or the app or the website because who cares. Right.

1122
01:12:40,455 --> 01:12:41,845
Speaker 5:  Which, which is why Ticketmaster broke.

1123
01:12:42,115 --> 01:12:45,085
Speaker 2:  Yeah. Which is why there's a crush of demand for Taylor Swift tickets. Even

1124
01:12:45,085 --> 01:12:48,605
Speaker 2:  after she told them there will be a crush of demand, can you handle it? And

1125
01:12:48,605 --> 01:12:51,595
Speaker 2:  it still crashed and they didn't have to do anything. 'cause there's where,

1126
01:12:51,595 --> 01:12:55,075
Speaker 2:  where are you gonna go Taylor? And I think once the United States government

1127
01:12:55,075 --> 01:12:58,675
Speaker 2:  is like, where, where are you gonna go Taylor? She doesn't have choices.

1128
01:13:00,415 --> 01:13:03,595
Speaker 5:  If Taylor Swift doesn't have options, there's a monopoly happening here.

1129
01:13:03,665 --> 01:13:03,955
Speaker 5:  Yeah.

1130
01:13:04,105 --> 01:13:07,835
Speaker 2:  Then, then maybe we'll do something But. anyway, you can go read the lawsuit

1131
01:13:07,855 --> 01:13:10,675
Speaker 2:  and like I said, we'll, we'll drop the link. We made a long decoder about

1132
01:13:10,675 --> 01:13:13,795
Speaker 2:  the whole buildup of this with a lot of great interviews in this. One of

1133
01:13:13,795 --> 01:13:16,915
Speaker 2:  our, when it was a, it was a pi you know, we do two episodes now. It was

1134
01:13:16,915 --> 01:13:20,555
Speaker 2:  a pilot of our explainer episode. So it was a fun one. All that was my, I

1135
01:13:20,555 --> 01:13:22,275
Speaker 2:  we gotta say that one. Alex, what's your lightning round?

1136
01:13:22,625 --> 01:13:23,275
Speaker 6:  Salt Spoon?

1137
01:13:24,705 --> 01:13:25,955
Speaker 2:  Totally different move. Sorry.

1138
01:13:25,955 --> 01:13:26,115
Speaker 5:  What?

1139
01:13:26,355 --> 01:13:29,795
Speaker 6:  Totally, totally different. I actually have two. Yeah. And I feel this one

1140
01:13:29,795 --> 01:13:31,275
Speaker 6:  is almost more important. That's not true.

1141
01:13:31,275 --> 01:13:33,955
Speaker 5:  Salt Spoon is the new Mad Max movie, right? Yeah. Is that, that's what that's

1142
01:13:33,955 --> 01:13:34,315
Speaker 5:  called. Okay.

1143
01:13:34,315 --> 01:13:37,435
Speaker 6:  That it's exactly what it's called. But I I know we, we've done, we've talked

1144
01:13:37,455 --> 01:13:41,075
Speaker 6:  before on The Vergecast about how you can like use

1145
01:13:41,475 --> 01:13:44,995
Speaker 6:  electricity to make people taste things a little differently. Andrew Marino

1146
01:13:44,995 --> 01:13:46,635
Speaker 6:  did a really good podcast, Vergecast on it.

1147
01:13:48,355 --> 01:13:51,715
Speaker 6:  Somebody else was like, what if we put those electrodes into a spoon and

1148
01:13:51,715 --> 01:13:54,595
Speaker 6:  so we could take things that aren't salty and make them more salty. And as

1149
01:13:54,595 --> 01:13:58,355
Speaker 6:  someone who grew up low sodium because of their mom, hell yes.

1150
01:13:59,185 --> 01:13:59,475
Speaker 5:  Yeah.

1151
01:13:59,715 --> 01:14:03,555
Speaker 6:  I am so excited for this. I didn't know what salt tasted like until I was

1152
01:14:03,555 --> 01:14:07,395
Speaker 6:  like 16. Like, this is incredible. I'm so excited. But

1153
01:14:07,395 --> 01:14:09,355
Speaker 6:  they're only, they're only selling like 200 of 'em

1154
01:14:11,265 --> 01:14:14,885
Speaker 6:  and it doesn't work. If, you have like heart problems, which

1155
01:14:15,465 --> 01:14:19,125
Speaker 6:  If you most people on low sodium diets. That's why they're on them. It doesn't

1156
01:14:19,125 --> 01:14:23,045
Speaker 6:  work if you're like pregnant. So a lot of the people who might

1157
01:14:23,135 --> 01:14:26,405
Speaker 6:  wanna be on low sodium diets this won't actually work for, but

1158
01:14:26,915 --> 01:14:28,165
Speaker 6:  it's a salt spoon and that's cool.

1159
01:14:28,795 --> 01:14:32,685
Speaker 2:  Yeah. When I read this, I thought it was a spoon that just dumped salt.

1160
01:14:34,865 --> 01:14:37,205
Speaker 6:  No, that's, I've got one of those in my, my kitchen right now

1161
01:14:37,205 --> 01:14:38,685
Speaker 5:  That's just called a spoon. Yeah,

1162
01:14:38,805 --> 01:14:42,765
Speaker 2:  Yeah. No, but that's just a real CES local news gadget, you know? Yeah. Like

1163
01:14:42,765 --> 01:14:46,165
Speaker 2:  it's spoon that precisely delivers that. No, this one just shocks you into

1164
01:14:46,165 --> 01:14:48,605
Speaker 2:  believing there's salt. A different approach.

1165
01:14:49,365 --> 01:14:52,125
Speaker 6:  I love that for them. And it's from the people who do like soy sauce, it's

1166
01:14:52,175 --> 01:14:53,045
Speaker 6:  Kiran. Oh,

1167
01:14:53,045 --> 01:14:53,325
Speaker 2:  Nice.

1168
01:14:53,585 --> 01:14:55,165
Speaker 6:  So very good. They they know salt.

1169
01:14:55,165 --> 01:14:56,525
Speaker 2:  They should make a sriracha spoon.

1170
01:14:57,025 --> 01:14:57,485
Speaker 6:  Oh my God.

1171
01:14:58,325 --> 01:15:01,485
Speaker 2:  I want, they should just continue electrocuting my brain into, into flavor.

1172
01:15:02,355 --> 01:15:05,325
Speaker 6:  Just trick my brain constantly. Please. Yeah.

1173
01:15:05,625 --> 01:15:06,765
Speaker 2:  All right, David. Like

1174
01:15:08,565 --> 01:15:12,405
Speaker 5:  I, we have to talk about humane, right? Oh gosh. Everyone's everyone's

1175
01:15:12,525 --> 01:15:16,405
Speaker 5:  favorite startup. There was a Bloomberg report this week that Humane

1176
01:15:16,665 --> 01:15:20,245
Speaker 5:  is now looking for a buyer. And I believe the price it's looking for is somewhere

1177
01:15:20,245 --> 01:15:24,085
Speaker 5:  between 750 million. And a billion dollars

1178
01:15:24,905 --> 01:15:28,565
Speaker 5:  also was in that story that the company has raised

1179
01:15:28,825 --> 01:15:29,045
Speaker 5:  $230 million. So

1180
01:15:29,045 --> 01:15:32,365
Speaker 2:  It wants a four x valuation Yeah. At the top end. Okay. And

1181
01:15:32,365 --> 01:15:36,325
Speaker 5:  I would remind you that it is, it's, it's that humane. It is. It's

1182
01:15:36,325 --> 01:15:39,885
Speaker 5:  the one, it's the one we've talked about. It's not a different, like B2B

1183
01:15:39,885 --> 01:15:41,245
Speaker 5:  Humane that is very successful,

1184
01:15:43,015 --> 01:15:46,645
Speaker 5:  truly bold strategy to come out and say, I'm not surprised this company's

1185
01:15:46,645 --> 01:15:50,485
Speaker 5:  looking for a buyer. Right. Like, I think the, the thing you hope for with

1186
01:15:50,485 --> 01:15:54,165
Speaker 5:  your first product is that it gets you money to get to the second product.

1187
01:15:54,265 --> 01:15:56,765
Speaker 5:  And then the second product is the one where you start to win. And that's

1188
01:15:56,765 --> 01:16:00,325
Speaker 5:  where you go, like, that's the strategy. But If, you flop on the first one

1189
01:16:00,945 --> 01:16:03,925
Speaker 5:  and holy God, does it look like they flopped on the first one?

1190
01:16:05,235 --> 01:16:08,005
Speaker 5:  It's very hard to get out of that. So I'm not shocked that Humane is looking

1191
01:16:08,005 --> 01:16:11,765
Speaker 5:  for a way out. I think a, it's going to have a lot of trouble

1192
01:16:11,765 --> 01:16:15,725
Speaker 5:  finding a buyer because this company is not full of people who are like beloved

1193
01:16:15,745 --> 01:16:17,005
Speaker 5:  in the tech industry, I would say.

1194
01:16:18,785 --> 01:16:19,765
Speaker 2:  And at the top,

1195
01:16:20,315 --> 01:16:24,045
Speaker 5:  Yeah, at the top. Like Imran Chari in particular, I would say

1196
01:16:24,115 --> 01:16:28,085
Speaker 5:  left Apple not beloved by people still

1197
01:16:28,305 --> 01:16:32,245
Speaker 5:  at Apple. That's about as much as I can report

1198
01:16:32,265 --> 01:16:34,285
Speaker 5:  on that. But that I, I feel pretty good saying that

1199
01:16:35,825 --> 01:16:36,045
Speaker 2:  The

1200
01:16:36,435 --> 01:16:39,045
Speaker 5:  Rest of it is just, it is truly

1201
01:16:40,235 --> 01:16:43,805
Speaker 5:  wild for this company to come out and say, we think we should get four x

1202
01:16:43,805 --> 01:16:44,485
Speaker 5:  what we phrased.

1203
01:16:44,745 --> 01:16:45,485
Speaker 2:  And everyone who works

1204
01:16:45,485 --> 01:16:48,365
Speaker 5:  Here should get rich because of this terrible product that we've made.

1205
01:16:48,515 --> 01:16:52,405
Speaker 2:  Okay. So two things. One, it's probably broken. And I

1206
01:16:52,405 --> 01:16:52,925
Speaker 2:  don't believe you

1207
01:16:54,875 --> 01:16:57,165
Speaker 2:  real theme, I'm just telling you.

1208
01:16:57,165 --> 01:16:58,805
Speaker 5:  Well, that one's definitely broken. Yeah.

1209
01:16:59,185 --> 01:17:02,805
Speaker 2:  We know that it's super broken and I don't believe, like I'm just saying,

1210
01:17:02,805 --> 01:17:05,925
Speaker 2:  we caught a lot of heat for being mean at the TED Talk.

1211
01:17:07,195 --> 01:17:10,965
Speaker 2:  Here we are. Second, let's go through the list. Right. So Apple

1212
01:17:11,065 --> 01:17:14,605
Speaker 2:  is out. I don't, they're not gonna pay them the money. No. Nope. And I, I,

1213
01:17:14,765 --> 01:17:17,965
Speaker 2:  I concur with your reporting and that is also about as much as I can say.

1214
01:17:18,165 --> 01:17:22,045
Speaker 2:  Yeah. But like, if you're out in the mix, it's just out there.

1215
01:17:22,045 --> 01:17:25,125
Speaker 2:  That's just information. Yeah. Apple's not gonna buy this company especially

1216
01:17:25,245 --> 01:17:28,125
Speaker 2:  'cause they're entire pitch was, we're gonna obsolete the iPhone. Right.

1217
01:17:28,155 --> 01:17:30,725
Speaker 2:  Doesn't seem like the sort of thing Apple's gonna invest in. No. Apple has

1218
01:17:30,725 --> 01:17:34,525
Speaker 2:  the Apple Watch, they're gonna be fine. They have

1219
01:17:34,555 --> 01:17:36,765
Speaker 2:  Siri, which is a little shaky, but they got the Apple watch

1220
01:17:38,385 --> 01:17:41,285
Speaker 2:  Google. No. Right. That feels like a no. They're, they're still struggling

1221
01:17:41,285 --> 01:17:44,805
Speaker 2:  to integrate Fitbit and don't need Google has predict projector, more

1222
01:17:45,165 --> 01:17:48,965
Speaker 5:  Hardware, both like more hardware chops and more hardware issues

1223
01:17:49,595 --> 01:17:50,965
Speaker 5:  than it can deal with already.

1224
01:17:50,965 --> 01:17:54,685
Speaker 2:  Right. Google for example, I'll just do this. They just merged Android and

1225
01:17:54,685 --> 01:17:58,045
Speaker 2:  the Pixel team under Rick Oslow who formerly ran the Pixel team

1226
01:17:59,305 --> 01:18:02,965
Speaker 2:  and all of hardware. And now he runs all the platforms too. So Rick has to

1227
01:18:03,155 --> 01:18:06,875
Speaker 2:  like keep Samsung happy, right? Like

1228
01:18:06,875 --> 01:18:10,395
Speaker 2:  he's, he now manages the whole ecosystem, which is really interesting. Right?

1229
01:18:11,535 --> 01:18:15,315
Speaker 2:  But can you, and, and he is gotta figure out the rest of Fitbit, which

1230
01:18:15,495 --> 01:18:19,315
Speaker 2:  is still messy from what I understand from V You can't just like throw a

1231
01:18:19,315 --> 01:18:22,755
Speaker 2:  bunch of X apple designer to not their right. Running

1232
01:18:23,095 --> 01:18:24,395
Speaker 2:  not your AI platform.

1233
01:18:24,965 --> 01:18:28,435
Speaker 5:  Right. And a big part of what Humane, I suspect is trying to sell

1234
01:18:29,135 --> 01:18:33,075
Speaker 5:  is the operating system. Right? Like they've always said, we

1235
01:18:33,075 --> 01:18:36,555
Speaker 5:  are not just about the AI pin. Like they're, they wanna be a platform. And

1236
01:18:36,555 --> 01:18:39,715
Speaker 5:  this is a pitch you hear from everybody, but I, I suspect If, you're gonna

1237
01:18:39,715 --> 01:18:43,555
Speaker 5:  pay anything like the price that they want. You think you're buying what

1238
01:18:43,555 --> 01:18:47,355
Speaker 5:  has the potential to be a winning platform? Not a bunch of hardware engineers.

1239
01:18:47,355 --> 01:18:49,875
Speaker 5:  Like Google doesn't actually need more hardware engineers at this moment

1240
01:18:49,875 --> 01:18:50,195
Speaker 5:  in time.

1241
01:18:50,195 --> 01:18:54,075
Speaker 6:  No. Is it not just them trying to be like, everybody's really hot

1242
01:18:54,075 --> 01:18:57,195
Speaker 6:  for AI right now. We shipped an AI thing

1243
01:18:58,295 --> 01:18:59,275
Speaker 6:  by our AI thing,

1244
01:18:59,575 --> 01:19:01,635
Speaker 2:  But their AI thing is OpenAI.

1245
01:19:01,905 --> 01:19:04,795
Speaker 6:  Yeah. But Samsung's got Saturdays

1246
01:19:06,175 --> 01:19:06,515
Speaker 2:  If they

1247
01:19:06,515 --> 01:19:10,355
Speaker 5:  Wanted to do that, they should have sold four months ago. Right. Like

1248
01:19:10,575 --> 01:19:14,435
Speaker 5:  if you're just trying to capitalize, you sell before you ship and not after.

1249
01:19:14,625 --> 01:19:14,915
Speaker 5:  Yeah.

1250
01:19:14,985 --> 01:19:15,275
Speaker 2:  Well,

1251
01:19:15,275 --> 01:19:17,875
Speaker 6:  Unless you're greedy and you think you actually like,

1252
01:19:18,575 --> 01:19:19,835
Speaker 2:  Or they thought they were gonna win. Yes.

1253
01:19:19,835 --> 01:19:20,955
Speaker 6:  Yeah. and they thought they were gonna win. Yeah.

1254
01:19:21,475 --> 01:19:25,235
Speaker 2:  Which is fine. Companies are allowed to work harder. Like sure. There's,

1255
01:19:26,455 --> 01:19:30,075
Speaker 2:  how do I put this? There's no lack of sincerity in the humane

1256
01:19:30,075 --> 01:19:33,435
Speaker 2:  ecosystem, right? Like no, that's, it might be some confusion,

1257
01:19:34,145 --> 01:19:36,755
Speaker 2:  some delusion, but not But they're not

1258
01:19:36,755 --> 01:19:37,075
Speaker 5:  Kidding.

1259
01:19:37,075 --> 01:19:40,995
Speaker 2:  They're not insincere. Right? Yeah. Right. Okay. So Google's

1260
01:19:40,995 --> 01:19:44,155
Speaker 2:  out, right? I think Google's out just 'cause the, the stuff runs on OpenAI

1261
01:19:44,205 --> 01:19:47,795
Speaker 2:  stack and Google doesn't run that and they don't need more complication.

1262
01:19:47,795 --> 01:19:51,275
Speaker 2:  Google, if anything is trying to streamlined, right? Yeah. Microsoft is interesting.

1263
01:19:51,435 --> 01:19:54,595
Speaker 2:  Microsoft runs a bunch of OpenAI didn stuff. They don't have mobile. Right.

1264
01:19:54,595 --> 01:19:58,235
Speaker 2:  There's lots of rumors. I think Saachi Nadella is smarter

1265
01:19:58,305 --> 01:20:02,275
Speaker 2:  than this. That's frankly just my, my belief

1266
01:20:02,275 --> 01:20:04,515
Speaker 2:  there. I don't think he, I don't think he's like, yeah, we'll take a flyer

1267
01:20:04,515 --> 01:20:08,325
Speaker 2:  in this nonsense. He's they're winning. He's like,

1268
01:20:08,325 --> 01:20:09,565
Speaker 2:  I'm gonna take on the MacBook Air.

1269
01:20:10,035 --> 01:20:10,325
Speaker 5:  Yeah.

1270
01:20:10,755 --> 01:20:14,405
Speaker 2:  Yeah. That I would put if of all the possibles Microsoft at the top. I just

1271
01:20:14,405 --> 01:20:17,285
Speaker 2:  think Nadella is smarter than this distraction. Samsung

1272
01:20:18,375 --> 01:20:21,045
Speaker 2:  loves bad unfocused ideas. Yeah.

1273
01:20:22,075 --> 01:20:25,685
Speaker 2:  They're, they're in the mix. Love goofy hardware. A bunch of

1274
01:20:25,685 --> 01:20:28,525
Speaker 2:  executives sitting around on Saturdays with nothing to do except do have

1275
01:20:28,605 --> 01:20:28,805
Speaker 2:  a deals.

1276
01:20:28,805 --> 01:20:31,525
Speaker 6:  Yeah. They're, they're gonna see that, that, that story on Saturday and be

1277
01:20:31,525 --> 01:20:31,645
Speaker 6:  like,

1278
01:20:32,305 --> 01:20:32,805
Speaker 2:  Got it. Yep.

1279
01:20:33,185 --> 01:20:35,805
Speaker 5:  Can you imagine the humane team just being given Bali?

1280
01:20:36,125 --> 01:20:37,245
Speaker 2:  Yeah. Right. Bali. Yeah.

1281
01:20:37,245 --> 01:20:37,605
Speaker 5:  Go figure

1282
01:20:37,605 --> 01:20:40,245
Speaker 2:  Out Bali. Yeah. Just this now the laser projector rolls around.

1283
01:20:42,305 --> 01:20:43,725
Speaker 2:  Is that, is that something

1284
01:20:46,405 --> 01:20:49,685
Speaker 5:  I could actually see it and I, and Samsung, like smart things is always an

1285
01:20:49,765 --> 01:20:51,245
Speaker 5:  interesting version of that. Samsung actually a lot

1286
01:20:51,625 --> 01:20:55,405
Speaker 2:  Is the Humane band running smart things is like a

1287
01:20:55,475 --> 01:20:59,165
Speaker 2:  pure nightmare. Just no, it's gonna run absolute nightmare.

1288
01:20:59,835 --> 01:21:00,125
Speaker 2:  Yeah.

1289
01:21:00,265 --> 01:21:01,685
Speaker 5:  Agreed. But Samsung is,

1290
01:21:03,225 --> 01:21:04,325
Speaker 5:  has a history of

1291
01:21:05,925 --> 01:21:09,885
Speaker 5:  actually following through on pretty big acquisitions in a way that I think

1292
01:21:09,885 --> 01:21:13,685
Speaker 5:  could be interesting. I suspect the humane team would not be psyched

1293
01:21:13,685 --> 01:21:17,485
Speaker 5:  about working for Samsung. I don't, but that is the funniest possible

1294
01:21:17,485 --> 01:21:17,805
Speaker 5:  outcome.

1295
01:21:19,935 --> 01:21:22,725
Speaker 2:  Don't, don't, I'm stuck on Samsung as a history of following through app.

1296
01:21:22,915 --> 01:21:24,645
Speaker 2:  Have you used smart things, sir?

1297
01:21:25,235 --> 01:21:27,405
Speaker 5:  It's it's still there. That's that's

1298
01:21:27,405 --> 01:21:28,205
Speaker 2:  True. It's there. It's,

1299
01:21:28,275 --> 01:21:28,565
Speaker 5:  It's

1300
01:21:28,565 --> 01:21:32,245
Speaker 2:  Around, I would say for as much, as much as I talk about the Frame tv, the,

1301
01:21:33,185 --> 01:21:36,725
Speaker 2:  the people who run the frame, the services are the boxy people. 'cause Samsung

1302
01:21:36,725 --> 01:21:38,085
Speaker 2:  bought them a while back. Oh yeah.

1303
01:21:38,665 --> 01:21:39,605
Speaker 5:  Oh, boxy.

1304
01:21:42,145 --> 01:21:43,645
Speaker 2:  I'm not saying this is great.

1305
01:21:45,665 --> 01:21:49,125
Speaker 2:  I'm just saying it's, it's true that Samsung buys things and then the people

1306
01:21:49,445 --> 01:21:53,285
Speaker 2:  continue working there. Yeah. All right. So Sam, LG I think is the

1307
01:21:53,285 --> 01:21:55,325
Speaker 2:  same, right? If, if LG gets with, but they're not doing

1308
01:21:55,685 --> 01:21:55,925
Speaker 6:  Saturdays.

1309
01:21:56,095 --> 01:21:59,445
Speaker 2:  Right. But yeah, Samsung's ahead. 'cause they're in the office on Saturday.

1310
01:21:59,825 --> 01:22:03,765
Speaker 2:  But if LG gets a whiff that Samsung might do it, LG might buy it. LG famously

1311
01:22:03,765 --> 01:22:07,285
Speaker 2:  bought Web os So a history of betting on doomed

1312
01:22:07,645 --> 01:22:10,885
Speaker 2:  platforms and then turning them into television operating systems.

1313
01:22:11,465 --> 01:22:12,485
Speaker 2:  You could see the Humane

1314
01:22:13,185 --> 01:22:13,605
Speaker 6:  Cosmos.

1315
01:22:13,635 --> 01:22:17,045
Speaker 2:  What if your TV had a laser projector in it that projected under your hand

1316
01:22:18,625 --> 01:22:21,445
Speaker 2:  Pretty good. Amazon feels like a hard no.

1317
01:22:22,735 --> 01:22:26,415
Speaker 2:  Right? Yeah. That company is is bringing it's,

1318
01:22:26,525 --> 01:22:27,975
Speaker 2:  it's like getting smaller, not bigger.

1319
01:22:28,305 --> 01:22:30,055
Speaker 6:  Panos is running all of hardware now.

1320
01:22:30,645 --> 01:22:32,735
Speaker 2:  Yeah. Oh yeah, yeah. Panos is there. Yeah.

1321
01:22:32,905 --> 01:22:33,575
Speaker 6:  Panos is there. Like

1322
01:22:33,785 --> 01:22:36,415
Speaker 2:  Panos Pane is now Yeah. In charge of Amazon devices and services.

1323
01:22:37,585 --> 01:22:41,205
Speaker 6:  He, he's not gonna go and be like, yeah, let's, let's buy that and

1324
01:22:41,205 --> 01:22:43,605
Speaker 6:  integrate that into Amazon's tech stack.

1325
01:22:43,825 --> 01:22:47,125
Speaker 5:  If your question was, which of these companies is most likely to

1326
01:22:47,685 --> 01:22:51,045
Speaker 5:  ship something that resembles the AI pin in the next three years? I think

1327
01:22:51,045 --> 01:22:52,845
Speaker 5:  it might be Amazon. I would

1328
01:22:52,845 --> 01:22:53,125
Speaker 6:  Agree.

1329
01:22:53,585 --> 01:22:57,485
Speaker 5:  But I don't think Amazon would get to that point by

1330
01:22:57,485 --> 01:22:58,565
Speaker 5:  acquiring Humane.

1331
01:22:58,905 --> 01:23:02,485
Speaker 2:  No, my question is, who will spend a billion dollars on

1332
01:23:02,745 --> 01:23:03,165
Speaker 2:  Humane?

1333
01:23:03,305 --> 01:23:05,925
Speaker 5:  The answer to that is like Steve Mnuchin or something like

1334
01:23:06,755 --> 01:23:10,725
Speaker 2:  Some social now runs the ai. Exactly. I'm just saying

1335
01:23:10,745 --> 01:23:14,645
Speaker 2:  my list is Samsung, lg. If Samsung,

1336
01:23:14,865 --> 01:23:18,725
Speaker 2:  if they get a whiff of Saturday Samsung and then like

1337
01:23:18,725 --> 01:23:21,605
Speaker 2:  distant 5,000 place third

1338
01:23:23,005 --> 01:23:23,925
Speaker 2:  Microsoft. Yeah.

1339
01:23:24,085 --> 01:23:24,805
Speaker 5:  I think that's probably right.

1340
01:23:24,935 --> 01:23:28,605
Speaker 2:  Agree. Who else has a billion dollars? Costco. It's Walmart.

1341
01:23:29,145 --> 01:23:32,645
Speaker 2:  I'm just thinking like the people that work at Oracle, who, who, who thought

1342
01:23:32,645 --> 01:23:34,725
Speaker 2:  about buying TikTok but would buy Humane instead.

1343
01:23:34,815 --> 01:23:38,765
Speaker 5:  It'll it'll end up being like Verizon or Comcast who's just like really

1344
01:23:38,765 --> 01:23:42,245
Speaker 5:  excited about a new they're they're gonna be like, this is your TV

1345
01:23:42,345 --> 01:23:45,685
Speaker 5:  remote. Yeah, yeah. On, on your lapel. And everybody's gonna be like, no,

1346
01:23:45,685 --> 01:23:45,805
Speaker 5:  thank

1347
01:23:45,805 --> 01:23:49,765
Speaker 2:  You. We're gonna, the idea of the humane design team

1348
01:23:50,285 --> 01:23:53,685
Speaker 2:  bringing that energy to launching like the next Fios remote.

1349
01:23:56,995 --> 01:23:57,565
Speaker 5:  It's beautiful.

1350
01:23:57,565 --> 01:24:01,405
Speaker 2:  It's very good. That's compelling. Just like, it's like on stage at

1351
01:24:01,565 --> 01:24:05,005
Speaker 2:  Ted being like, you can turn the volume up and down. Yeah.

1352
01:24:05,605 --> 01:24:08,205
Speaker 5:  I do wanna know though, by the way, if, if you're listening and you have

1353
01:24:08,205 --> 01:24:11,565
Speaker 5:  an idea of a company that either might or should buy Humane,

1354
01:24:12,275 --> 01:24:12,725
Speaker 5:  tell us

1355
01:24:13,095 --> 01:24:13,445
Speaker 2:  Exxon

1356
01:24:13,725 --> 01:24:17,565
Speaker 5:  Vergecast at The Verge dot com. Call the hotline. Eight six six Virgin

1357
01:24:17,885 --> 01:24:21,725
Speaker 5:  one one. I genuinely wanna know. 'cause I bet there is an interesting

1358
01:24:21,725 --> 01:24:24,845
Speaker 5:  match or two that we're not thinking of, and I'd love to know what they are.

1359
01:24:24,895 --> 01:24:28,885
Speaker 5:  Ford. Ford. There you go. Drive your car with your

1360
01:24:28,885 --> 01:24:32,085
Speaker 5:  Oh no, it's gonna be, it's gonna be Elon Musk and it's gonna be Grok and

1361
01:24:32,085 --> 01:24:32,485
Speaker 5:  it's just gonna be the

1362
01:24:32,485 --> 01:24:33,645
Speaker 2:  Brock Hardware. Don't,

1363
01:24:33,645 --> 01:24:34,285
Speaker 6:  Don't summon that

1364
01:24:34,575 --> 01:24:38,365
Speaker 2:  Rough. All right. Rock bush. We'll see what happens. And

1365
01:24:38,365 --> 01:24:41,725
Speaker 2:  again, it's just a Bloomberg report. Humane itself has not said anything,

1366
01:24:42,305 --> 01:24:45,925
Speaker 2:  but I strongly suspect they need to get out the game. I also suspect Rabbit's

1367
01:24:45,925 --> 01:24:47,645
Speaker 2:  gonna end up in the same spot very soon.

1368
01:24:47,825 --> 01:24:50,285
Speaker 6:  All right. I I, I gotta do one more. I gotta do one more. Okay.

1369
01:24:51,515 --> 01:24:51,865
Speaker 6:  Apple,

1370
01:24:53,445 --> 01:24:56,145
Speaker 6:  you sometimes you get photos on your phone and you're like, that's that's

1371
01:24:56,145 --> 01:24:59,225
Speaker 6:  a terrible photo. You delete it or you're like, Ooh, that's too spicy to

1372
01:24:59,225 --> 01:25:03,025
Speaker 6:  keep here in case my mom sees my photos and you delete it. And there was

1373
01:25:03,025 --> 01:25:06,665
Speaker 6:  an unfortunate bug where those photos just popped back up

1374
01:25:06,975 --> 01:25:10,425
Speaker 6:  into people's things, into their phones and Apple said

1375
01:25:11,255 --> 01:25:15,105
Speaker 6:  nothing. But they corrected it just with the latest

1376
01:25:15,325 --> 01:25:19,145
Speaker 6:  os update. So you should no longer have reappearing nudes or

1377
01:25:19,305 --> 01:25:22,745
Speaker 6:  whatever else. And Apple has said nothing about that too.

1378
01:25:23,175 --> 01:25:26,705
Speaker 2:  Yeah. It's weird to confirm a bug like this by

1379
01:25:26,705 --> 01:25:30,505
Speaker 2:  issuing an iOS update and then saying nothing else. Yeah.

1380
01:25:30,505 --> 01:25:34,385
Speaker 6:  Yeah. and they were like old. The, the, my understanding

1381
01:25:34,405 --> 01:25:37,785
Speaker 6:  is it's like not just, oh, a photo you deleted last night. It's like,

1382
01:25:37,805 --> 01:25:40,105
Speaker 5:  So people are getting like years old photos, right? Yeah.

1383
01:25:40,525 --> 01:25:43,865
Speaker 6:  So, so that's, that's terrifying. But at the same time

1384
01:25:44,655 --> 01:25:48,545
Speaker 6:  with recall, I would've loved that. So I, I don't know where I stand on this,

1385
01:25:48,605 --> 01:25:51,985
Speaker 6:  but I do stand on like yeah, if I delete something, I generally want it

1386
01:25:52,535 --> 01:25:53,425
Speaker 6:  deleted. Do

1387
01:25:53,425 --> 01:25:55,305
Speaker 2:  You believe it's deleted, right? Yeah. Is it,

1388
01:25:55,405 --> 01:25:57,545
Speaker 6:  Is it actually gone? Yeah. Don't believe it's deleted. I want to know where

1389
01:25:57,545 --> 01:26:01,225
Speaker 6:  that data goes when I make a choice about it. And I think that's the difference

1390
01:26:01,225 --> 01:26:04,290
Speaker 6:  between recall and this. Like, recall, you will know where the da theoretically

1391
01:26:04,335 --> 01:26:07,645
Speaker 6:  where the data goes once the security researchers actually figure that out.

1392
01:26:08,835 --> 01:26:09,685
Speaker 6:  This we don't

1393
01:26:09,685 --> 01:26:10,085
Speaker 2:  Know. Big caveat.

1394
01:26:10,865 --> 01:26:11,285
Speaker 6:  Big caveat.

1395
01:26:12,395 --> 01:26:16,325
Speaker 2:  Yeah. This is weird. I I, I think, you know, we, we are part of our job

1396
01:26:16,345 --> 01:26:19,325
Speaker 2:  is holding big companies accountable and it is truly

1397
01:26:20,115 --> 01:26:23,845
Speaker 2:  irresponsible for Apple to not say why this happened and what they did

1398
01:26:23,905 --> 01:26:27,205
Speaker 2:  to fix it. There's a bunch of people on Reddit who love to argue with me

1399
01:26:27,205 --> 01:26:30,205
Speaker 2:  on threads who are like, we figured it out and you're overhyping this and

1400
01:26:30,205 --> 01:26:34,045
Speaker 2:  it doesn't matter because we didn't cause the bug. We didn't fix the

1401
01:26:34,045 --> 01:26:38,025
Speaker 2:  bug. It's great that you have a theory about why it happened. The

1402
01:26:38,025 --> 01:26:41,945
Speaker 2:  theory send wonderfully plausible I love it. I don't know what happened.

1403
01:26:42,035 --> 01:26:45,545
Speaker 2:  Right? There's, there's one party that knows what happened and they, they

1404
01:26:45,545 --> 01:26:48,305
Speaker 2:  should be accountable for their mistake. 'cause this is a big mistake and

1405
01:26:48,305 --> 01:26:52,225
Speaker 2:  it's weird that we give them a pass. Yep. And so, you know, we

1406
01:26:52,225 --> 01:26:56,145
Speaker 2:  can't force them to, all we can do is send emails. That's, that's the

1407
01:26:56,615 --> 01:27:00,505
Speaker 2:  sheriff Neli has got Mime Stream and he pushes that button

1408
01:27:00,505 --> 01:27:04,425
Speaker 2:  every day. Yep. That's all we can do. And we just can tell

1409
01:27:04,425 --> 01:27:07,345
Speaker 2:  you they have not responded to any of our emails or any of our phone calls

1410
01:27:07,345 --> 01:27:09,985
Speaker 2:  about this. And I think that's fundamentally irresponsible. Yeah. Because

1411
01:27:09,985 --> 01:27:13,705
Speaker 2:  you're, you're, you're playing with people's, like some of the most personally

1412
01:27:13,705 --> 01:27:14,185
Speaker 2:  that exists

1413
01:27:15,805 --> 01:27:18,705
Speaker 2:  anyway. Hopefully they say something soon and if not, I'll just keep sending

1414
01:27:18,705 --> 01:27:22,625
Speaker 2:  emails. What else is there to do? Yeah. In my

1415
01:27:22,625 --> 01:27:24,985
Speaker 2:  administration they would go straight to jail. Vote for tell.

1416
01:27:27,015 --> 01:27:30,225
Speaker 2:  Last one. I'm actually excited about this. Sonos, ACE headphones are out.

1417
01:27:30,585 --> 01:27:34,545
Speaker 2:  A shout out to Chris Welch who covers every square inch of the

1418
01:27:34,715 --> 01:27:38,505
Speaker 2:  Sonos beat to the point where I think Sonos like the Sonos,

1419
01:27:38,525 --> 01:27:41,865
Speaker 2:  Reddit like waits for Chris to come around and then like, Sonos itself is

1420
01:27:41,865 --> 01:27:45,745
Speaker 2:  just like, oh no, Chris is here. It's great. He's, he scooped the

1421
01:27:45,745 --> 01:27:49,625
Speaker 2:  headphones, obviously they look great. They're like a cross between

1422
01:27:49,655 --> 01:27:53,185
Speaker 2:  like the Sony vibe and an AirPods Max vibe. The four

1423
01:27:53,185 --> 01:27:57,065
Speaker 2:  $50 is expensive. The headline feature is, if you're watching TV

1424
01:27:57,085 --> 01:27:59,785
Speaker 2:  on a sono soundbar, you can push a button in the audio, come to your headphones.

1425
01:28:02,865 --> 01:28:06,665
Speaker 2:  I, I want this to be compelling to me. And I just, it's not my use case

1426
01:28:06,685 --> 01:28:07,225
Speaker 2:  for these things.

1427
01:28:07,665 --> 01:28:11,385
Speaker 6:  I think that's a really compelling If. you are married and you live in a

1428
01:28:11,385 --> 01:28:15,065
Speaker 6:  small home and you wanna stay up late playing video

1429
01:28:15,115 --> 01:28:17,465
Speaker 6:  games or you wanna stay up late watching tv.

1430
01:28:18,055 --> 01:28:21,065
Speaker 2:  Yeah. Neli in his cavernous mansion wouldn't,

1431
01:28:21,145 --> 01:28:24,545
Speaker 6:  Wouldn't understand. Yeah. Neli, your house is just too big to fully appreciate

1432
01:28:24,545 --> 01:28:24,705
Speaker 6:  that

1433
01:28:25,285 --> 01:28:28,265
Speaker 2:  In his home theater. How often? No, it's not. It's the real time switching

1434
01:28:28,265 --> 01:28:31,745
Speaker 2:  that. I understand that sometimes you wanna watch TV and listen to headphones.

1435
01:28:31,825 --> 01:28:35,105
Speaker 2:  I, I got you. Yeah. But the part where you're like, well, I started with

1436
01:28:35,105 --> 01:28:35,705
Speaker 2:  the soundbar

1437
01:28:36,575 --> 01:28:39,905
Speaker 6:  Because they walk out of the room, they say, I'm going to bed and now you're

1438
01:28:39,905 --> 01:28:40,985
Speaker 6:  just like, boop. Yeah.

1439
01:28:41,645 --> 01:28:44,705
Speaker 2:  Ah, yeah, I can see it. But that's a headline feature by the way. That feature

1440
01:28:44,955 --> 01:28:47,905
Speaker 2:  oddly is iOS only right now, soon coming to Android. Weird

1441
01:28:48,695 --> 01:28:52,425
Speaker 2:  special audio. A PTX If. you have Android as well?

1442
01:28:52,485 --> 01:28:55,785
Speaker 2:  No high res anywhere else? No. Why? It's Bluetooth

1443
01:28:55,835 --> 01:28:59,665
Speaker 2:  everywhere. Except if you're doing the weird TV

1444
01:28:59,665 --> 01:29:03,565
Speaker 2:  thing, probably for battery life. And then obviously

1445
01:29:03,565 --> 01:29:06,925
Speaker 2:  this all comes on the heels of the disastrous app launch,

1446
01:29:07,255 --> 01:29:10,765
Speaker 2:  which Sonos won't come out and say, but it feels very

1447
01:29:10,795 --> 01:29:14,605
Speaker 2:  obvious that they set the timing of the headphone launch. They needed the

1448
01:29:14,605 --> 01:29:17,885
Speaker 2:  app for the headphones. And so then the app came out before it was ready.

1449
01:29:20,025 --> 01:29:23,965
Speaker 2:  If any other sequence of events occurred to make the app come out before

1450
01:29:23,965 --> 01:29:27,765
Speaker 2:  it was ready, I would be shocked. I wish Sonos would just say it like Pat

1451
01:29:28,135 --> 01:29:32,005
Speaker 2:  Chris talked to Patrick Spence, CEO, Sonos, and he was like, well, the app

1452
01:29:32,005 --> 01:29:35,205
Speaker 2:  was just ready. And it's like, why don't you just say we put out the app

1453
01:29:35,205 --> 01:29:38,205
Speaker 2:  because the headphones are coming out. That would be fine. Or give people

1454
01:29:38,205 --> 01:29:39,765
Speaker 2:  a choice. Don't update the app yet,

1455
01:29:40,615 --> 01:29:44,365
Speaker 5:  Right? Yeah. Ship it as If, you buy the headphones, here's the new version

1456
01:29:44,385 --> 01:29:47,645
Speaker 5:  of our app. It's going to be rolled out to everybody before long. Like that's,

1457
01:29:47,955 --> 01:29:51,765
Speaker 5:  it's not a hard sequence of events to do. But

1458
01:29:51,765 --> 01:29:55,045
Speaker 5:  instead Sonos was like, oh, this doesn't have Q Management, which is what

1459
01:29:55,565 --> 01:29:57,245
Speaker 5:  everyone wants. Let's ship it Anyway.

1460
01:29:57,585 --> 01:30:00,445
Speaker 2:  So I don't actually, I have no problems with this app. I'm the only person

1461
01:30:00,445 --> 01:30:03,725
Speaker 2:  in America who has no problems with this app because we don't use the queue.

1462
01:30:04,545 --> 01:30:08,285
Speaker 2:  We use playlists. Our playlists are in Spotify and Apple Music.

1463
01:30:08,905 --> 01:30:10,405
Speaker 2:  And so like, whatever.

1464
01:30:11,445 --> 01:30:13,445
Speaker 5:  I just don't use the Sonos app. Same.

1465
01:30:13,545 --> 01:30:17,165
Speaker 2:  That's what I mean. Like Becky just uses Airplay Yeah. For our

1466
01:30:17,255 --> 01:30:21,125
Speaker 2:  Sonos. But it's fine. It is a little bit faster. I know

1467
01:30:21,125 --> 01:30:24,285
Speaker 2:  other people have wild problems. Like Casey Newton is like, I can't even

1468
01:30:24,285 --> 01:30:25,685
Speaker 2:  set the volume on my speakers anymore.

1469
01:30:27,195 --> 01:30:30,565
Speaker 5:  It's really bad. If, you have a local music collection that you care about

1470
01:30:30,605 --> 01:30:32,845
Speaker 5:  a lot too, which is like, and, and Chris and I talked about this a couple

1471
01:30:32,845 --> 01:30:36,645
Speaker 5:  weeks ago that like, those are the core Sonos people and have

1472
01:30:36,645 --> 01:30:40,325
Speaker 5:  been for two decades. Yeah. And Sonos just continues to try to

1473
01:30:40,335 --> 01:30:43,925
Speaker 5:  drive them away in very strange ways. Yeah. Headphones look sick though,

1474
01:30:43,925 --> 01:30:44,405
Speaker 5:  because

1475
01:30:44,405 --> 01:30:45,045
Speaker 6:  There's six of them.

1476
01:30:46,115 --> 01:30:49,365
Speaker 2:  Yeah, right. This is the classic story. The company is chasing the bigger

1477
01:30:49,685 --> 01:30:53,205
Speaker 2:  consumer market where the money is and the growth is. And then you've got

1478
01:30:53,285 --> 01:30:57,005
Speaker 2:  these like very passionate users who are like, my entire life is one

1479
01:30:57,005 --> 01:31:00,845
Speaker 2:  button an app from 2004 and we are the people in the red.

1480
01:31:01,235 --> 01:31:04,645
Speaker 2:  Yeah. Well, like you, you, there's, there's a dynamic there that I am

1481
01:31:05,225 --> 01:31:07,765
Speaker 2:  as a person who surprised, designed our website. I'm aware of the people

1482
01:31:07,765 --> 01:31:08,765
Speaker 2:  who were mad. Yeah.

1483
01:31:09,395 --> 01:31:09,885
Speaker 5:  They've

1484
01:31:09,965 --> 01:31:13,005
Speaker 6:  Released apps before. Concurrently. They had, they had the two versions of

1485
01:31:13,005 --> 01:31:13,125
Speaker 6:  the

1486
01:31:13,235 --> 01:31:14,205
Speaker 2:  Lass of Death. No, that was a disaster.

1487
01:31:14,465 --> 01:31:16,045
Speaker 6:  It was horrible. But they had two.

1488
01:31:16,705 --> 01:31:19,965
Speaker 2:  No, that's a disa like Sonos. It's all bad now. Sonos is just bad. Well,

1489
01:31:19,965 --> 01:31:23,045
Speaker 2:  Patrick has been on the show a bunch of times. We've talked to him about

1490
01:31:23,045 --> 01:31:26,925
Speaker 2:  a lot of things. Everything from spatial audio and like how

1491
01:31:26,925 --> 01:31:29,485
Speaker 2:  they design speakers all the way to like antitrust. Like,

1492
01:31:30,845 --> 01:31:34,685
Speaker 2:  like he's been on the show. They have mismanaged in particular

1493
01:31:35,095 --> 01:31:39,085
Speaker 2:  these apps and what the apps do and how the apps work. Like the S

1494
01:31:39,085 --> 01:31:42,605
Speaker 2:  one to S two transition disaster. The Sonos, like,

1495
01:31:43,175 --> 01:31:46,765
Speaker 2:  we're gonna trade in your old speakers ever. And then like, we're gonna break

1496
01:31:46,765 --> 01:31:50,725
Speaker 2:  the old ones and they undid it. That was bad. Like there's this thing

1497
01:31:50,995 --> 01:31:54,965
Speaker 2:  that they're in particular not good at, which is wild. 'cause that's, and

1498
01:31:54,965 --> 01:31:55,685
Speaker 2:  they should get better at it.

1499
01:31:55,945 --> 01:31:59,565
Speaker 6:  The magic of Sonos is, is how it works with all your stuff. And

1500
01:32:00,465 --> 01:32:03,045
Speaker 6:  it just talks to each other. It's supposed to be super smart. And yet the

1501
01:32:03,045 --> 01:32:04,445
Speaker 6:  app continues to be kind of dumb.

1502
01:32:04,795 --> 01:32:08,685
Speaker 2:  Yeah. And the, the, again, I I, I have not had I, because I don't

1503
01:32:08,685 --> 01:32:11,925
Speaker 2:  use some of these features, like, what was it, timers and alarm clocks are

1504
01:32:11,925 --> 01:32:15,205
Speaker 2:  broken in this app. And the idea that I would set my alarm clock using the

1505
01:32:15,255 --> 01:32:19,165
Speaker 2:  Sonos app is just what, but like If, you are the person who, but

1506
01:32:19,165 --> 01:32:20,005
Speaker 5:  A lot of people do. Yeah, yeah.

1507
01:32:20,005 --> 01:32:23,485
Speaker 2:  Yeah. If you're the person whose alarm clock broke, like you are mad. Yeah.

1508
01:32:23,485 --> 01:32:27,325
Speaker 2:  Yeah. That's a problem. Right? Like, you should find a way

1509
01:32:27,325 --> 01:32:30,525
Speaker 2:  to be like, okay, these core features, there's some of these core features

1510
01:32:30,525 --> 01:32:33,765
Speaker 2:  where people just like build their lives. Like alarm clocks are, people build

1511
01:32:33,765 --> 01:32:37,525
Speaker 2:  their lives around it. You, you have to respect it. And like in

1512
01:32:38,355 --> 01:32:41,165
Speaker 2:  this case, I think Sonos says they've gotten more and more consumer,

1513
01:32:41,955 --> 01:32:45,125
Speaker 2:  they've sort of like gotten away from it and they should just recalibrate

1514
01:32:45,125 --> 01:32:48,245
Speaker 2:  that a little bit. That said, I have like 9,000 Sonos speakers. Like I'm

1515
01:32:48,245 --> 01:32:51,045
Speaker 2:  trapped in this ecosystem. What you can do, it's fine.

1516
01:32:52,745 --> 01:32:56,525
Speaker 2:  All right. I think that's it. That's it. David, do you have another one?

1517
01:32:56,675 --> 01:32:58,525
Speaker 5:  Nope, that's it. We're done.

1518
01:32:58,545 --> 01:32:59,285
Speaker 2:  All right. We gotta

1519
01:32:59,285 --> 01:33:00,645
Speaker 5:  Wrap this thing. Spotify has a font,

1520
01:33:01,315 --> 01:33:02,405
Speaker 2:  Spotify has a font.

1521
01:33:02,405 --> 01:33:05,245
Speaker 5:  That's all that's I have. I have, that's all the information I have for you.

1522
01:33:05,955 --> 01:33:06,245
Speaker 5:  Yeah.

1523
01:33:06,835 --> 01:33:09,845
Speaker 2:  They might, Amazon might make you pay for Alexa once they add AI too. It's

1524
01:33:09,845 --> 01:33:13,245
Speaker 2:  like, that's kinda stuff. Yeah. The big thing is that I'm gonna go try to

1525
01:33:13,245 --> 01:33:17,125
Speaker 2:  call Scarlet Johanson now and see if she will voice

1526
01:33:17,605 --> 01:33:19,325
Speaker 2:  verges ai. Yeah. As revenge.

1527
01:33:19,745 --> 01:33:22,005
Speaker 6:  Do like AI calling Jos to call her.

1528
01:33:22,395 --> 01:33:26,125
Speaker 2:  Yeah. I will say a friend of mine was like, Google should just

1529
01:33:26,155 --> 01:33:29,045
Speaker 2:  take this voice now. They should just pay her whatever amount of money she

1530
01:33:29,045 --> 01:33:32,725
Speaker 2:  wants now. Yeah. If there's like an arms race for

1531
01:33:32,725 --> 01:33:36,325
Speaker 2:  celebrity AI voices, I would be, that would be very entertaining. Alright,

1532
01:33:36,325 --> 01:33:40,125
Speaker 2:  that's it. Also, my voice is available for cheaper than Scarlett Johansen's.

1533
01:33:40,665 --> 01:33:41,805
Speaker 2:  In case you're wondering, it's hard to

1534
01:33:41,885 --> 01:33:45,205
Speaker 5:  Reproduce. It's the most expensive. Like who do you, is it, is it like, I

1535
01:33:45,205 --> 01:33:48,525
Speaker 5:  feel like it's either like Scarlett Johansen or Ryan Reynolds. This is whole

1536
01:33:48,945 --> 01:33:52,285
Speaker 5:  Jones. We could do no way in terms of like, no, James Earl Jones.

1537
01:33:52,415 --> 01:33:54,885
Speaker 6:  James Earl Jones, Mufasa, and Darcy.

1538
01:33:54,955 --> 01:33:58,205
Speaker 5:  It's important to everyone that I remind you that Alex Kranz is 66 years

1539
01:33:58,205 --> 01:33:58,405
Speaker 5:  old.

1540
01:34:00,025 --> 01:34:02,805
Speaker 5:  And if you've never heard of any of those words that she just said, don't

1541
01:34:02,805 --> 01:34:03,645
Speaker 5:  worry. They all

1542
01:34:03,675 --> 01:34:04,765
Speaker 6:  Know who that is.

1543
01:34:05,125 --> 01:34:09,005
Speaker 2:  I mean, aren't there, there's like Amazon, isn't Amazon set the mark? I guess

1544
01:34:09,005 --> 01:34:12,165
Speaker 2:  the rates are gone higher, but yeah, this is a thing that exists. You can

1545
01:34:12,165 --> 01:34:13,805
Speaker 2:  just buy other voices from Amazon, right?

1546
01:34:14,065 --> 01:34:15,685
Speaker 6:  Yep. You can.

1547
01:34:16,435 --> 01:34:19,525
Speaker 2:  Alright. Send us your notes with who you think the most expensive voice would

1548
01:34:19,525 --> 01:34:23,045
Speaker 2:  be. I don't want to, I don't. We will read. We will read your ideas.

1549
01:34:23,805 --> 01:34:24,845
Speaker 6:  Everyone's gonna agree with me. It's fine.

1550
01:34:24,845 --> 01:34:27,165
Speaker 2:  It's David at The Verge dot com everybody. All right. That's it. That's The

1551
01:34:27,245 --> 01:34:28,285
Speaker 2:  Verge test, rock and roll.

1552
01:34:32,225 --> 01:34:35,445
Speaker 3:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1553
01:34:35,445 --> 01:34:39,165
Speaker 3:  Give us a call at eight six six VERGE one. One. The Vergecast

1554
01:34:39,165 --> 01:34:42,805
Speaker 3:  is the production of The Verge and Vox Media Podcast Network. Our show is

1555
01:34:42,805 --> 01:34:46,405
Speaker 3:  produced by Andrew Marino and Liam James. That's it. We'll see you next week.

