1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 58e71880-58ad-11ed-bd19-d1e4a3a9b905
Status: Done
Stage: Done
Title: Twitter is now an Elon Musk company
Audio URL: https://jfe93e.s3.amazonaws.com/2441121861087490009/-6365024637408409357/s93290-US-2867s-1667173647.mp3
Description: The Verge's Nilay Patel, Liz Lopatto, David Pierce, and Alex Cranz discuss Elon Musk officially becoming the owner of Twitter, and what that means for the future of the company.

2
00:01:17,070 --> 00:01:20,790
Speaker 5:  Hello and welcome to The Road Chest, the flagship podcast of

3
00:01:20,790 --> 00:01:24,710
Speaker 5:  impulse buying things you shouldn't have bought. I just bought a car. That's

4
00:01:24,710 --> 00:01:28,150
Speaker 5:  a real thing that happened on the show. We'll talk about it later. I'm just

5
00:01:28,150 --> 00:01:31,950
Speaker 5:  gonna be, I'm your friend of my hi. I'm just gonna be honest with you. We

6
00:01:31,950 --> 00:01:35,630
Speaker 5:  thought we had today to prepare for this emergency. Elon

7
00:01:35,630 --> 00:01:38,550
Speaker 5:  buys Twitter podcast. We thought it was gonna happen at like

8
00:01:38,740 --> 00:01:41,750
Speaker 5:  4:20 PM today.

9
00:01:42,270 --> 00:01:46,030
Speaker 5:  Nice. Right? See lot like if, if he could have

10
00:01:46,030 --> 00:01:49,590
Speaker 5:  bought Twitter at 69 o'clock, he would've bought Twitter at 69

11
00:01:52,000 --> 00:01:55,910
Speaker 5:  o'clock. So we thought we had like a day to prep and, but we didn't

12
00:01:56,360 --> 00:01:59,430
Speaker 5:  because in classic Elon fashion, he appears to have bought it at

13
00:01:59,610 --> 00:02:03,270
Speaker 5:  like 10:00 PM last night and then he fired

14
00:02:03,270 --> 00:02:07,110
Speaker 5:  all the executives and then he just like wandered the halls. Like, I don't

15
00:02:07,110 --> 00:02:10,950
Speaker 5:  know. I don't know man. Elon, So we're doing this, It's early, we're just

16
00:02:10,950 --> 00:02:14,870
Speaker 5:  gonna get through it. I just got an email saying my recent article

17
00:02:14,870 --> 00:02:17,310
Speaker 5:  sucks. That article is entitled Welcome to Hell Elon.

18
00:02:18,900 --> 00:02:22,470
Speaker 5:  I forgot to turn off my notifications. This is how unprepared we are. My

19
00:02:22,470 --> 00:02:26,270
Speaker 5:  notifications are Shalon. Anyway, I'm here. I'm Eli. David's here. Hi,

20
00:02:26,300 --> 00:02:27,190
Speaker 5:  Alex is here.

21
00:02:27,370 --> 00:02:31,110
Speaker 6:  Hey, I'm your friend who also impulse bought something this week, but it

22
00:02:31,110 --> 00:02:31,950
Speaker 6:  was a fruit bowl so

23
00:02:31,950 --> 00:02:34,830
Speaker 5:  It's fine. And Liz La Plato's here who has been writing this week in Elon

24
00:02:34,830 --> 00:02:38,430
Speaker 5:  and she just very happily said, My part of this story is over

25
00:02:38,430 --> 00:02:39,110
Speaker 5:  Welcome, Liz.

26
00:02:41,130 --> 00:02:44,750
Speaker 7:  Hey, thanks for having me. I got about three hours of sleep last night, so

27
00:02:44,750 --> 00:02:45,870
Speaker 7:  let's see how coherent I am.

28
00:02:46,340 --> 00:02:50,270
Speaker 5:  It's, it's a lot. I mean we really did think that he was

29
00:02:50,270 --> 00:02:54,110
Speaker 5:  gonna do it at like 4:20 PM today. Like the amount of confidence I

30
00:02:54,110 --> 00:02:55,590
Speaker 5:  had in that outcome was very high.

31
00:02:55,850 --> 00:02:58,590
Speaker 3:  The thing that this process has made me realize, like I've realized this

32
00:02:58,590 --> 00:03:02,040
Speaker 3:  a hundred different times, but even this like last bit has made me realize

33
00:03:02,040 --> 00:03:05,640
Speaker 3:  is that there's just no rules to anything. Yeah. Like you would think there's

34
00:03:05,640 --> 00:03:09,600
Speaker 3:  like a thing that's supposed to happen where like a, like a person with authority

35
00:03:09,600 --> 00:03:13,560
Speaker 3:  says like you now own Twitter. It's like the the like Michael

36
00:03:13,560 --> 00:03:17,200
Speaker 3:  Scott, I declare bankruptcy, right? And it's just like, but

37
00:03:17,370 --> 00:03:20,960
Speaker 3:  instead, instead it just like happened. There was just a minute where like

38
00:03:20,960 --> 00:03:23,600
Speaker 3:  Elon Musk had a sink and then it was it.

39
00:03:23,870 --> 00:03:27,760
Speaker 5:  That part is ridiculous. So I am pretty sure, I don't know if you've,

40
00:03:28,250 --> 00:03:32,080
Speaker 5:  if you ever bought anything large, right? You actually show up to

41
00:03:32,080 --> 00:03:36,000
Speaker 5:  a ceremony. Like even if you buy, like you buy a house, you're to like

42
00:03:36,000 --> 00:03:39,600
Speaker 5:  go to a lawyer's office and then there's like lots of

43
00:03:39,600 --> 00:03:43,400
Speaker 5:  randos there and then you have to sign 9,000 documents, get keys.

44
00:03:43,400 --> 00:03:46,800
Speaker 5:  For some reason their entire lawyers like have an entire business just having

45
00:03:46,800 --> 00:03:49,120
Speaker 5:  these meetings where you close the house and you leave and you're holding

46
00:03:49,120 --> 00:03:52,120
Speaker 5:  the keys to the house and you're like, Crap, I hope I bought the right house,

47
00:03:52,120 --> 00:03:55,280
Speaker 5:  right? Like yep, that's a thing when you buy a car, there's like lots of

48
00:03:55,280 --> 00:03:58,880
Speaker 5:  stuff. There's a bank involved. I'm pretty sure Elon had that moment

49
00:03:58,880 --> 00:04:02,720
Speaker 5:  yesterday he just did it with a sink that he had brought

50
00:04:02,720 --> 00:04:06,680
Speaker 5:  to the meeting, like a physical bathroom sink that he had walked into

51
00:04:06,680 --> 00:04:09,680
Speaker 5:  Twitter with to post a meme where he got to say let that sink in, which is

52
00:04:09,680 --> 00:04:11,640
Speaker 5:  v very funny. Like,

53
00:04:11,640 --> 00:04:12,640
Speaker 3:  Oh I loved it. He,

54
00:04:12,640 --> 00:04:14,840
Speaker 6:  So he did that. He brought the sink in on, he did that Wednesday.

55
00:04:15,210 --> 00:04:16,040
Speaker 5:  He went, okay,

56
00:04:16,040 --> 00:04:19,560
Speaker 6:  But I do have a question. Did he bring that sink in,

57
00:04:19,850 --> 00:04:23,600
Speaker 6:  go straight into Prague's office, drop it on the table and say

58
00:04:23,800 --> 00:04:24,600
Speaker 6:  tomorrow you're fired.

59
00:04:24,930 --> 00:04:28,640
Speaker 5:  So the sink was on Wednesday. I think he left the sink at Twitter. I don't

60
00:04:28,640 --> 00:04:32,560
Speaker 5:  think he took the sink back out. So I think the sink just ominously sat in

61
00:04:32,560 --> 00:04:36,480
Speaker 5:  the executive board in the Twitter and then he had his

62
00:04:36,480 --> 00:04:40,440
Speaker 5:  closing meeting where he and his lawyers showed up yesterday. Twitter's

63
00:04:40,440 --> 00:04:44,160
Speaker 5:  executives and board were there because they have to sign the documents,

64
00:04:44,250 --> 00:04:47,920
Speaker 5:  turning all the stuff over to Elon. And then when that was done,

65
00:04:48,170 --> 00:04:51,480
Speaker 5:  he just looked at them all and said, Most of you are fired.

66
00:04:51,570 --> 00:04:55,200
Speaker 5:  Because the first report, if you recall the first report, I think it was

67
00:04:55,200 --> 00:04:58,840
Speaker 5:  David favorite CNBC said Twitter CEO and CFO

68
00:04:58,840 --> 00:05:02,640
Speaker 5:  have left the building and they are not expected to return. Which is an

69
00:05:02,640 --> 00:05:05,760
Speaker 5:  incredible, like if you just think about what that he fired them right after

70
00:05:05,760 --> 00:05:09,440
Speaker 5:  they signed the deal. He's like, you're gone. And then he fired

71
00:05:09,670 --> 00:05:12,680
Speaker 5:  Vigo who's their head of policy, which is a big decision that we should talk

72
00:05:12,680 --> 00:05:16,240
Speaker 5:  about. And I think he fired the general council, general council by the way,

73
00:05:16,240 --> 00:05:19,960
Speaker 5:  who led the legal team that defeated Elon Musk. That

74
00:05:19,960 --> 00:05:23,800
Speaker 5:  made Eon by Twitter, which he definitely tried to get out of Liz. That's

75
00:05:23,800 --> 00:05:24,800
Speaker 5:  the part I want to start with.

76
00:05:24,800 --> 00:05:28,720
Speaker 7:  The general counsel was escorted out by security. Like there were a couple

77
00:05:28,720 --> 00:05:32,240
Speaker 7:  of people where it was, they weren't just like fired, They were fired in

78
00:05:32,240 --> 00:05:35,760
Speaker 7:  humiliating fashion. So Sean t is the general counsel

79
00:05:35,820 --> 00:05:39,720
Speaker 7:  and Bloomberg says security escorted him out. Paraag Aggarwal, the

80
00:05:39,720 --> 00:05:43,520
Speaker 7:  former CEO and Neg Siegel who was the chief financial

81
00:05:43,520 --> 00:05:46,920
Speaker 7:  officer, were also escorted out by security according to Reuters. Like can

82
00:05:46,920 --> 00:05:50,840
Speaker 7:  you imagine like you've just done a deal with Elon Musk, you've signed the

83
00:05:50,840 --> 00:05:54,720
Speaker 7:  thing and then two burley security guards like strong

84
00:05:54,720 --> 00:05:55,600
Speaker 7:  arm you out of the building.

85
00:05:55,600 --> 00:05:59,320
Speaker 5:  Oh they, I mean I can absolutely imagine that in this context.

86
00:05:59,740 --> 00:06:03,440
Speaker 3:  I'm guessing if I'm parag aural, that's like the least

87
00:06:03,540 --> 00:06:07,520
Speaker 3:  bad thing I thought might happen to me when all of this went down. Like that

88
00:06:07,520 --> 00:06:10,960
Speaker 3:  dude has gotta have spent the last six months wondering what

89
00:06:11,600 --> 00:06:13,880
Speaker 3:  horrible thing Elon was gonna do to him when this deal closed.

90
00:06:13,910 --> 00:06:15,960
Speaker 7:  Well he is getting a nice payout.

91
00:06:15,960 --> 00:06:17,880
Speaker 5:  Well he's gonna pay him $30 million.

92
00:06:18,150 --> 00:06:19,000
Speaker 3:  Well there's that,

93
00:06:19,000 --> 00:06:22,600
Speaker 5:  Right? So these, some of these executives got huge payouts. So like, if you

94
00:06:22,600 --> 00:06:26,360
Speaker 5:  wanna give me 30 million and escort me out of a building, you pick

95
00:06:26,360 --> 00:06:28,360
Speaker 5:  the to not run Twitter, I'm here

96
00:06:30,060 --> 00:06:33,560
Speaker 5:  tomorrow. You just tell the building to show up at. I'll be escorted in the

97
00:06:33,560 --> 00:06:36,080
Speaker 5:  building out of the building. As long as there's a check you, you just let

98
00:06:36,280 --> 00:06:36,440
Speaker 5:  me know.

99
00:06:36,440 --> 00:06:37,400
Speaker 7:  You hear that Elon?

100
00:06:40,450 --> 00:06:44,360
Speaker 5:  So Liz, I wanted to start there, right there. This has been a long and winding

101
00:06:44,560 --> 00:06:48,440
Speaker 5:  m and a journey. And I think that he escorted the general counsel of

102
00:06:48,440 --> 00:06:52,200
Speaker 5:  Twitter outta the building by security as revenge because he got owned

103
00:06:52,250 --> 00:06:56,040
Speaker 5:  in this process. Like he bought it for the number, he said he was gonna

104
00:06:56,240 --> 00:06:59,840
Speaker 5:  buy it to the to the dime. He didn't even get a

105
00:07:00,080 --> 00:07:00,520
Speaker 5:  discount after

106
00:07:00,520 --> 00:07:04,440
Speaker 7:  All this. No, I actually would've kept that lawyer because he is a nightmare.

107
00:07:04,950 --> 00:07:08,400
Speaker 7:  I I like full respect to all of the legal teams.

108
00:07:09,160 --> 00:07:12,440
Speaker 7:  But Twitter's legal team in particular was quite good. And you may remember

109
00:07:12,440 --> 00:07:16,120
Speaker 7:  that they were, the outside team was brought on and they were like the inventors

110
00:07:16,120 --> 00:07:19,880
Speaker 7:  of the poison pill, which is a deal provision that that prevents,

111
00:07:19,880 --> 00:07:23,520
Speaker 7:  prevents hostile takeovers that Twitter implemented before

112
00:07:23,580 --> 00:07:27,240
Speaker 7:  the agreement to buy Twitter happened. And then Elon

113
00:07:27,240 --> 00:07:30,520
Speaker 7:  canceled the deal and they canceled the deal some more And and, and then

114
00:07:30,520 --> 00:07:32,880
Speaker 7:  we all were like, Oh I guess we're going to court. And then we didn't go

115
00:07:32,880 --> 00:07:36,640
Speaker 7:  to court. So this was a really, really good agreement that was

116
00:07:36,640 --> 00:07:40,440
Speaker 7:  negotiated. Like this was a really strong agreement. And like, I

117
00:07:40,440 --> 00:07:44,320
Speaker 7:  think we've talked about this before, but I was of the opinion that Elon

118
00:07:44,320 --> 00:07:47,440
Speaker 7:  Musk was gonna lose a trial and it looks like Elon Musk was also of the opinion

119
00:07:47,440 --> 00:07:50,920
Speaker 7:  that Elon Musk was going to lose a trial. And so here we all are on Elon

120
00:07:50,920 --> 00:07:54,520
Speaker 7:  musk's twitter.com or quitting elon musk's twitter.com

121
00:07:54,520 --> 00:07:56,800
Speaker 7:  specifically because of all of these shenanigans.

122
00:07:57,070 --> 00:07:58,360
Speaker 5:  Yeah, I mean that's the,

123
00:07:58,360 --> 00:07:59,480
Speaker 7:  No one's gonna quit.

124
00:07:59,810 --> 00:08:03,720
Speaker 5:  So here's what I've, okay, first of all, I quit smoking while cold

125
00:08:03,720 --> 00:08:07,520
Speaker 5:  Turkey while studying for the lsat. My faith that I can quit anything is

126
00:08:07,520 --> 00:08:11,200
Speaker 5:  very high. It wasn't easy and I blew bubbles. I like took like kids

127
00:08:11,200 --> 00:08:14,840
Speaker 5:  bubbles and I blew them outside of the law school in between class breaks.

128
00:08:14,840 --> 00:08:17,680
Speaker 5:  Cause I needed to do something on my hands. But I'm willing to do it. You

129
00:08:17,680 --> 00:08:21,120
Speaker 5:  can, if you find the bubbles equivalent of Twitter, I'll quit Twitter. It's

130
00:08:21,120 --> 00:08:25,000
Speaker 5:  still there. I think we should see what happens to it. I just think a

131
00:08:25,000 --> 00:08:27,600
Speaker 5:  lot of people are gonna stop using it. I think a lot of people are gonna

132
00:08:27,600 --> 00:08:31,280
Speaker 5:  stop using it the way they had been using it. There is that

133
00:08:31,280 --> 00:08:34,680
Speaker 5:  report in Reuters last week

134
00:08:34,870 --> 00:08:38,480
Speaker 5:  that Twitter is already seeing a massive decline in quote

135
00:08:38,680 --> 00:08:42,560
Speaker 5:  heavy tweeters and they kind of don't know why, but

136
00:08:42,560 --> 00:08:45,360
Speaker 5:  the answers are obvious. It's that Twitter is an

137
00:08:45,620 --> 00:08:46,920
Speaker 5:  inhospitable place.

138
00:08:47,070 --> 00:08:50,360
Speaker 7:  Yeah, I mean like the thing about that report is that the decline started

139
00:08:50,360 --> 00:08:53,600
Speaker 7:  well before Elon Musk, right? Like it starts during the pandemic. And I think

140
00:08:54,160 --> 00:08:57,720
Speaker 7:  Ryan Broderick had the best explanation. He writes a newsletter called Garbage

141
00:08:57,720 --> 00:09:01,000
Speaker 7:  Day. And I've just been thinking about this over and over where like Twitter

142
00:09:01,000 --> 00:09:03,920
Speaker 7:  is a place where the users all hate each other and also the product itself.

143
00:09:03,920 --> 00:09:07,440
Speaker 7:  Yeah. And like that's Twitter. It's just like we're all here at hell

144
00:09:07,540 --> 00:09:10,880
Speaker 7:  and like if you turn up the heat a little more on hell, more people leave.

145
00:09:10,910 --> 00:09:14,000
Speaker 5:  Yeah. And I, this is I think the fundamental problem

146
00:09:14,780 --> 00:09:18,680
Speaker 5:  for Elon. This is what I, I promised I would write this piece, Welcome

147
00:09:18,680 --> 00:09:21,440
Speaker 5:  to hell Elon. It was supposed to be had something else and then it was just,

148
00:09:21,440 --> 00:09:25,240
Speaker 5:  I just wrote it. But the problem for him is that to

149
00:09:25,240 --> 00:09:29,160
Speaker 5:  make any money at all, he actually can't do

150
00:09:29,160 --> 00:09:33,120
Speaker 5:  the things that he's promised his ardent fans that he would do. So

151
00:09:33,120 --> 00:09:37,000
Speaker 5:  if you wanna make money and you don't just want to have ads

152
00:09:37,000 --> 00:09:40,960
Speaker 5:  for supplements in like racist pillows, you have to promise

153
00:09:41,360 --> 00:09:45,240
Speaker 5:  advertisers brand safety, right? You have to say your ads are gonna go in

154
00:09:45,240 --> 00:09:48,760
Speaker 5:  a place that is, that won't make your brand look bad. Taco Bell

155
00:09:48,990 --> 00:09:52,640
Speaker 5:  like it, it's just the, it's the basic. If you just read

156
00:09:52,640 --> 00:09:56,400
Speaker 5:  about internet advertising, they are always constantly talking

157
00:09:56,400 --> 00:09:59,920
Speaker 5:  about brand safety. It is the thing. And so Twitter

158
00:10:00,210 --> 00:10:04,000
Speaker 5:  is a real problem with that, as in general it has

159
00:10:04,920 --> 00:10:08,520
Speaker 5:  somewhat managed to find ways to overcome it. But it is the

160
00:10:08,520 --> 00:10:12,400
Speaker 5:  smallest to the social networks because it is inhospitable, it does not

161
00:10:12,630 --> 00:10:16,400
Speaker 5:  make you feel good to use it. Like if you're a new Twitter user,

162
00:10:16,830 --> 00:10:20,560
Speaker 5:  your desire to tweet is pretty low because

163
00:10:20,560 --> 00:10:24,320
Speaker 5:  the things that will happen to you after you tweet are mostly

164
00:10:24,720 --> 00:10:28,520
Speaker 5:  negative, right? Like bots will try to sell you crypto or some Randa

165
00:10:28,520 --> 00:10:32,360
Speaker 5:  will pop up and talk shit to you. Like those are the, or no one

166
00:10:32,360 --> 00:10:35,600
Speaker 5:  or nothing will happen. Which is actually the worst outcome for a social

167
00:10:35,600 --> 00:10:39,200
Speaker 5:  network. Like if you're trying to incentivize content creation

168
00:10:39,610 --> 00:10:43,600
Speaker 5:  because a social network without lots of people working for free is

169
00:10:43,600 --> 00:10:47,240
Speaker 5:  a bad idea. So you're, you're trying to get people to work for free

170
00:10:47,300 --> 00:10:51,280
Speaker 5:  and if the thing that happens most often is nothing, like no one pays attention

171
00:10:51,280 --> 00:10:54,640
Speaker 5:  to you. That's bad. If it's bots try to sell you crypto, that's bad. Maybe

172
00:10:54,640 --> 00:10:58,440
Speaker 5:  Elon can solve that problem and he's focused on it or it's you get

173
00:10:58,440 --> 00:11:02,320
Speaker 5:  harassed all this is bad. Like none of these are, this is why the

174
00:11:02,320 --> 00:11:05,000
Speaker 5:  only people who tweet are like hopelessly addicted online people.

175
00:11:05,000 --> 00:11:08,520
Speaker 3:  Well, and this is why you see Elon like talking out of both sides of his

176
00:11:08,520 --> 00:11:11,840
Speaker 3:  mouth throughout this whole process. On the one hand he's, he's talking to

177
00:11:11,840 --> 00:11:15,120
Speaker 3:  people about, you know, scaling down content moderation and bringing back

178
00:11:15,120 --> 00:11:18,280
Speaker 3:  Donald Trump and, and open sourcing the algorithm whatever the hell that

179
00:11:18,340 --> 00:11:21,320
Speaker 3:  means. And this idea that like it should be a freer place that follows the

180
00:11:21,340 --> 00:11:24,560
Speaker 3:  laws. And then he's sitting with advertisers saying, No, no, no, it's still

181
00:11:24,560 --> 00:11:28,160
Speaker 3:  gonna be fine. I won't like free for all. Hellscape is the phrase that he

182
00:11:28,160 --> 00:11:31,040
Speaker 3:  used that I think is going to haunt him for a long time because it is now

183
00:11:31,040 --> 00:11:34,720
Speaker 3:  very stuck in my brain. And like, and and so

184
00:11:34,720 --> 00:11:38,480
Speaker 3:  he's, he's trying to do a thing that everyone has proved over and over

185
00:11:38,480 --> 00:11:42,280
Speaker 3:  is impossible. Which is to let chaos reign in

186
00:11:42,480 --> 00:11:44,960
Speaker 3:  an advertising friendly way and like you just can't do

187
00:11:44,960 --> 00:11:48,800
Speaker 5:  That. Right? And so the pro like Elon does not like being told what to

188
00:11:48,800 --> 00:11:51,800
Speaker 5:  do. Liz, I feel like that's a Liz is shaking your head. Okay. I feel like

189
00:11:51,800 --> 00:11:55,400
Speaker 5:  that's a safe statement. Elon Musk does not like being told what to do. Yeah.

190
00:11:55,400 --> 00:11:59,280
Speaker 5:  When you own a social network, all kinds of people get to

191
00:11:59,280 --> 00:12:02,840
Speaker 5:  tell you what to do. So the advertisers say we need brand safety. You need

192
00:12:02,840 --> 00:12:05,600
Speaker 5:  to moderate away the racism, the sexism, the transphobia, all the bad stuff

193
00:12:05,890 --> 00:12:09,760
Speaker 5:  so that we can sell more shoes. That's just the basics of the thing. They

194
00:12:09,760 --> 00:12:13,520
Speaker 5:  want their ads near stuff. That's nice. Then you gotta incentivize people

195
00:12:13,810 --> 00:12:17,800
Speaker 5:  to make nice things, which is hard, just like flatly hard. Like

196
00:12:17,800 --> 00:12:21,720
Speaker 5:  YouTube can't do it. YouTube is maybe the most successful of

197
00:12:21,720 --> 00:12:24,480
Speaker 5:  these companies, right? We don't think of it as a social network in that

198
00:12:24,480 --> 00:12:28,440
Speaker 5:  way, but YouTube gets like most kids in America want to be

199
00:12:28,440 --> 00:12:32,000
Speaker 5:  YouTubers when they grow up. Like YouTube has created the incentive

200
00:12:32,280 --> 00:12:35,640
Speaker 5:  structure where people wanna show up and make nice things for other people

201
00:12:35,640 --> 00:12:39,280
Speaker 5:  on YouTube and advertisers want pour money into that and that's fine

202
00:12:39,480 --> 00:12:43,120
Speaker 5:  great. YouTube has moderation problems every single day at this scale that

203
00:12:43,120 --> 00:12:46,920
Speaker 5:  you could not possibly imagine. That's just the basics. Like you

204
00:12:46,920 --> 00:12:49,760
Speaker 5:  need people to show up, you need to get money from advertisers, advertisers

205
00:12:49,760 --> 00:12:52,840
Speaker 5:  that want nice things. Now you have to listen to the advertisers, which is

206
00:12:52,840 --> 00:12:56,320
Speaker 5:  why Elon Musk wrote an open letter on Twitter promising advertisers it was

207
00:12:56,320 --> 00:12:59,440
Speaker 5:  gonna be fine cuz he can't kill his revenue right away.

208
00:13:00,270 --> 00:13:04,000
Speaker 5:  Then there are governments around the world which love

209
00:13:04,000 --> 00:13:07,880
Speaker 5:  passing speech regulations, love it. Can't get enough of it. It seems like

210
00:13:07,880 --> 00:13:10,720
Speaker 5:  if you become a politician, the first thing you wanna do is tell people what

211
00:13:10,720 --> 00:13:13,440
Speaker 5:  they can say. This is true in our country.

212
00:13:14,410 --> 00:13:17,880
Speaker 5:  So in Texas and Florida there are these social media laws that are effectively

213
00:13:17,880 --> 00:13:20,920
Speaker 5:  impossible to comply with because they ban

214
00:13:21,850 --> 00:13:25,750
Speaker 5:  any moderation that is based on viewpoint without really defining it.

215
00:13:25,900 --> 00:13:29,270
Speaker 5:  That's going to the Supreme Court. Elon is out here saying I'm gonna support

216
00:13:29,300 --> 00:13:32,750
Speaker 5:  DeSantis for president and he is massively

217
00:13:32,750 --> 00:13:36,430
Speaker 5:  invested in Texas and he's gotta fight those governments,

218
00:13:36,550 --> 00:13:40,430
Speaker 5:  those governors, those are their laws. He's gotta go tell the Supreme

219
00:13:40,430 --> 00:13:44,310
Speaker 5:  Court these laws are unconstitutional. That's just like I that that's a

220
00:13:44,310 --> 00:13:45,230
Speaker 5:  tight rope he has to

221
00:13:45,230 --> 00:13:49,150
Speaker 6:  Launch. Does he have to tell them that? Like I think we

222
00:13:49,150 --> 00:13:53,070
Speaker 6:  are assuming that he is going to run this company like

223
00:13:53,070 --> 00:13:56,870
Speaker 6:  every other social media network and that he is going to

224
00:13:56,970 --> 00:14:00,550
Speaker 6:  incentivize advertisers like every other social media network and I just

225
00:14:00,550 --> 00:14:00,990
Speaker 6:  don't think that's

226
00:14:00,990 --> 00:14:03,950
Speaker 5:  True. We can use the advertiser assumption because he just told

227
00:14:03,950 --> 00:14:07,910
Speaker 6:  And then he fired the person who does advertising at Twitter. Like I

228
00:14:07,910 --> 00:14:11,510
Speaker 6:  genuinely do not. I think we are in a whole new

229
00:14:11,510 --> 00:14:15,470
Speaker 6:  space with him because he is the richest man in the world and he

230
00:14:15,470 --> 00:14:18,310
Speaker 6:  has shown again and again and again he can do what he wants.

231
00:14:19,290 --> 00:14:23,230
Speaker 7:  And I, so yes, I agree with you Alex.

232
00:14:23,870 --> 00:14:27,790
Speaker 7:  100% I think we were in for like a big reign of chaos and you all know how

233
00:14:27,790 --> 00:14:30,950
Speaker 7:  I feel about chaos. I love it very much. But I'm just thinking about like

234
00:14:31,250 --> 00:14:35,110
Speaker 7:  all of the moderation problems that every larger company has had, right?

235
00:14:35,110 --> 00:14:38,750
Speaker 7:  And like I'm not even like for like leave aside like the most

236
00:14:38,750 --> 00:14:42,150
Speaker 7:  nasty part of it. Like politics, like livestream,

237
00:14:42,630 --> 00:14:46,590
Speaker 7:  shootings, all of those things like leave aside the nastiest parts

238
00:14:46,590 --> 00:14:50,510
Speaker 7:  of moderation. Like if you think about Google

239
00:14:50,510 --> 00:14:54,200
Speaker 7:  for instance, their text snippets have at times told users

240
00:14:54,200 --> 00:14:57,880
Speaker 7:  that Snoopy assassinated Abraham Lincoln and

241
00:14:58,610 --> 00:15:02,080
Speaker 7:  at Facebook they actually lost a piece of their moderation policy for three

242
00:15:02,080 --> 00:15:05,120
Speaker 7:  years. So in addition to like the big stuff, there are all these little things

243
00:15:05,120 --> 00:15:08,840
Speaker 7:  that can also go wrong. But on top of that, this is the

244
00:15:08,840 --> 00:15:12,680
Speaker 7:  cherry on top for me working with people is not like

245
00:15:13,040 --> 00:15:16,240
Speaker 7:  engineering. Yeah. Because building a community is really different from

246
00:15:16,400 --> 00:15:18,360
Speaker 7:  building a rocket cuz rockets don't talk back.

247
00:15:18,510 --> 00:15:21,320
Speaker 5:  Yeah. I mean this to, Well they tend to explode

248
00:15:23,030 --> 00:15:24,320
Speaker 5:  that in that way. They're a lot

249
00:15:24,320 --> 00:15:24,640
Speaker 6:  Like Twitter.

250
00:15:27,610 --> 00:15:31,480
Speaker 5:  No, I mean I agree with you. I mean this is to me that I, we see this

251
00:15:31,480 --> 00:15:35,160
Speaker 5:  repeated over and over again, right? People approach these community problems

252
00:15:35,160 --> 00:15:39,120
Speaker 5:  like engineering problems and they say things I'll give you,

253
00:15:39,120 --> 00:15:42,960
Speaker 5:  I'll give you a totally unrelated example, but it it's the same example because

254
00:15:42,960 --> 00:15:46,560
Speaker 5:  it's the same approach. You talk to self-driving car CEOs,

255
00:15:46,560 --> 00:15:48,920
Speaker 5:  you're like, how are you gonna build the cars? Who's gonna write the rules?

256
00:15:48,920 --> 00:15:51,200
Speaker 5:  And they're like, the rules are already written down, it's just the rules

257
00:15:51,200 --> 00:15:54,760
Speaker 5:  of the road. And it's like, so you, you think you can just like ingest the

258
00:15:54,760 --> 00:15:58,680
Speaker 5:  state DMV into your robot car and then it's gonna be fine. And

259
00:15:58,680 --> 00:16:01,960
Speaker 5:  it turns out like cars are totally unpredictable. Like this is not how it

260
00:16:01,960 --> 00:16:05,920
Speaker 5:  works. Like other people in cars tend to make decisions that have nothing

261
00:16:05,920 --> 00:16:09,560
Speaker 5:  to do with the what the DMV thinks you should do. And they're stuck because

262
00:16:09,560 --> 00:16:12,560
Speaker 5:  they thought there was a list of rules they could just follow. And you can

263
00:16:12,560 --> 00:16:16,160
Speaker 5:  apply that exact same thinking to Elon has tweeted this,

264
00:16:16,290 --> 00:16:20,160
Speaker 5:  If people want the speech to be legal or if people want a ban speech,

265
00:16:20,160 --> 00:16:23,440
Speaker 5:  they can write a law. He said this, so we're just gonna take the first amendment

266
00:16:23,860 --> 00:16:27,480
Speaker 5:  and glue it into some AI and then that will run

267
00:16:27,480 --> 00:16:30,720
Speaker 5:  moderation for us against the principles of the First Amendment. Well the

268
00:16:30,730 --> 00:16:34,320
Speaker 5:  problem is, the thing the First amendment actually does is it prevents

269
00:16:34,380 --> 00:16:38,160
Speaker 5:  the people of America from making a law that bans some

270
00:16:38,160 --> 00:16:42,040
Speaker 5:  speech, right? It prevents government speech regulation. It

271
00:16:42,040 --> 00:16:45,690
Speaker 5:  allows for private speech regulation, it

272
00:16:45,690 --> 00:16:49,530
Speaker 5:  allows for the it for private companies, for private citizens to speak

273
00:16:49,530 --> 00:16:53,450
Speaker 5:  however they want to publish however they want and it keeps the government

274
00:16:53,450 --> 00:16:57,130
Speaker 5:  out of it. At least that's how it should work. So you you

275
00:16:57,200 --> 00:17:00,490
Speaker 5:  like just from like an end, like a systems engineering perspective,

276
00:17:00,840 --> 00:17:04,210
Speaker 5:  you're saying okay, we're gonna grab the first amendment,

277
00:17:04,330 --> 00:17:08,290
Speaker 5:  which prevents our system from doing much of anything. And if

278
00:17:08,290 --> 00:17:11,730
Speaker 5:  people wanna change it, they wanna change how Twitter works,

279
00:17:11,960 --> 00:17:15,930
Speaker 5:  they can somehow overrule the first amendment, which is the, the

280
00:17:15,930 --> 00:17:19,650
Speaker 5:  main thing and it's like you are trapped in a doom loop. No,

281
00:17:19,940 --> 00:17:22,370
Speaker 5:  He is the first thing that's gonna happen. No, cuz the first thing that's

282
00:17:22,370 --> 00:17:25,730
Speaker 5:  gonna happen to him is he's like every other social network that promises

283
00:17:25,730 --> 00:17:29,530
Speaker 5:  the same thing, he's gonna start banning people that do horrible

284
00:17:29,530 --> 00:17:32,970
Speaker 5:  things. Par this happened to Parlor happened to Getter happened to Truth

285
00:17:32,970 --> 00:17:36,570
Speaker 5:  Social. Yeah, truth social has like the most restrictive mother age.

286
00:17:37,390 --> 00:17:41,290
Speaker 6:  But I wonder like, is he actually going to do that? That's

287
00:17:41,290 --> 00:17:43,890
Speaker 6:  that's the thing is like we, we keep assuming that he's going to be like,

288
00:17:43,890 --> 00:17:47,330
Speaker 6:  Oh this person said something horrible online. They they go,

289
00:17:47,670 --> 00:17:51,280
Speaker 6:  but he has repeatedly said he wants every viewpoint on

290
00:17:51,280 --> 00:17:54,760
Speaker 6:  Twitter. He wants to make this a big open ecosystem.

291
00:17:54,930 --> 00:17:58,480
Speaker 6:  He wants to minimize moderation. So I like

292
00:17:59,080 --> 00:18:00,640
Speaker 5:  Don't think he knows what that means.

293
00:18:01,130 --> 00:18:02,680
Speaker 6:  No, I agree. But that's why I

294
00:18:02,930 --> 00:18:06,360
Speaker 5:  We have reasonable here's some like reasonable debates you could have, right?

295
00:18:06,360 --> 00:18:10,200
Speaker 5:  Yeah. That every viewpoint on Twitter, you can have

296
00:18:10,200 --> 00:18:13,760
Speaker 5:  a reasonable debate about the Fed setting interest rates

297
00:18:14,180 --> 00:18:17,560
Speaker 5:  and people will scream at each other and wanna murder each other

298
00:18:17,830 --> 00:18:21,760
Speaker 5:  over half a percentage point of the Fed setting interest rate. You can

299
00:18:21,760 --> 00:18:24,480
Speaker 5:  have a reasonable debate about whether the government should provide healthcare,

300
00:18:24,640 --> 00:18:28,560
Speaker 5:  right? I think it should. A lot of people don't. That has a reasonable

301
00:18:28,560 --> 00:18:31,560
Speaker 5:  debate, right? There are numbers on both sides of that argument. Yeah. You

302
00:18:31,560 --> 00:18:35,520
Speaker 5:  can have a reasonable debate on education policy. You

303
00:18:35,520 --> 00:18:39,200
Speaker 5:  can, you really can. Lots of ways to educate your kids, but you can't have

304
00:18:39,200 --> 00:18:43,080
Speaker 5:  a reasonable debate on like whether brown people are inferior to

305
00:18:43,080 --> 00:18:46,920
Speaker 5:  British people. Like yeah, that's not a choice. And that is the actually

306
00:18:46,920 --> 00:18:50,720
Speaker 5:  the aperture that everybody wants to open. Yep. Right?

307
00:18:50,720 --> 00:18:54,080
Speaker 5:  The app, the thing that everybody wants to open is Kanye West saying Jewish

308
00:18:54,080 --> 00:18:57,880
Speaker 5:  people control the media and the reason that Twitter doesn't allow

309
00:18:57,880 --> 00:19:01,800
Speaker 5:  it is well one that's like dangerous. Yeah, it

310
00:19:01,800 --> 00:19:05,240
Speaker 5:  causes actual political violence in the world, but Twitter does not wanna

311
00:19:05,240 --> 00:19:08,560
Speaker 5:  be associated with, and two, it makes people fucking

312
00:19:08,840 --> 00:19:12,640
Speaker 5:  leave the platform. Like the most cynical motivation that anyone could have

313
00:19:12,770 --> 00:19:16,760
Speaker 5:  is, oh, this retards user growth like this makes people go away.

314
00:19:16,760 --> 00:19:20,520
Speaker 5:  It makes the number go down. And like all of that is

315
00:19:20,520 --> 00:19:24,480
Speaker 5:  the people who want to open the aperture of free speech on the

316
00:19:24,480 --> 00:19:28,000
Speaker 5:  margin are not people with more conservative economic policy.

317
00:19:28,390 --> 00:19:31,840
Speaker 5:  It's basically people who want to like reintroduce racism.

318
00:19:32,080 --> 00:19:36,000
Speaker 5:  Right. Reintroduce sexism. Right. And what they are complaining about

319
00:19:36,000 --> 00:19:39,480
Speaker 5:  is not even that they can't do it. Cause have you been on Twitter? No,

320
00:19:39,480 --> 00:19:41,280
Speaker 6:  No, no. I'm, I'm very aware they

321
00:19:41,280 --> 00:19:43,000
Speaker 5:  Can do it. What they're complaining about is they don't get the reach

322
00:19:43,600 --> 00:19:44,160
Speaker 6:  Right.

323
00:19:44,330 --> 00:19:48,320
Speaker 5:  They're like their shadow banned. Right. And like, is Elon gonna increase

324
00:19:48,320 --> 00:19:51,040
Speaker 5:  their reach? Is he gonna solve that problem? Like I just don't, I don't think

325
00:19:51,040 --> 00:19:52,200
Speaker 5:  that that problem is tractable.

326
00:19:52,470 --> 00:19:55,520
Speaker 6:  Yeah. I I just don't think that Elon

327
00:19:56,440 --> 00:20:00,280
Speaker 6:  cares like, like I think he is going to go after

328
00:20:00,600 --> 00:20:04,080
Speaker 6:  whatever gets him the biggest numbers and I think if he sees, Oh I brought

329
00:20:04,080 --> 00:20:07,080
Speaker 6:  Trunk back, I'm gonna get bigger numbers. Oh if I bring Kanye back, I'm gonna

330
00:20:07,080 --> 00:20:11,040
Speaker 6:  get bigger numbers. He's going to focus on the numbers, not necessarily on

331
00:20:11,040 --> 00:20:15,000
Speaker 6:  the shit show. Those numbers bring because he cares about the

332
00:20:15,000 --> 00:20:17,640
Speaker 6:  numbers. He's like a reality TV producer

333
00:20:17,820 --> 00:20:21,520
Speaker 7:  And he likes shit shows. Let's be real. I mean like think about Tesla,

334
00:20:22,040 --> 00:20:25,840
Speaker 7:  remember like the the robots that were gonna build the cars and he was like,

335
00:20:25,840 --> 00:20:29,080
Speaker 7:  I'm re-engineering from first principles how to build cars. And everybody

336
00:20:29,080 --> 00:20:32,480
Speaker 7:  was like, Toyota knows how to build cars, like just do that. And he was like,

337
00:20:32,480 --> 00:20:36,160
Speaker 7:  No, no, it's going to be an alien dread knot. And then that didn't work out

338
00:20:36,160 --> 00:20:40,040
Speaker 7:  and he put a an extra card building tent in the parking lot

339
00:20:40,040 --> 00:20:44,000
Speaker 7:  of the Fremont factory. I mean like I don't know that he necessarily

340
00:20:44,000 --> 00:20:46,360
Speaker 7:  is thinking about any of this stuff because as we've discovered from his

341
00:20:46,360 --> 00:20:50,320
Speaker 7:  text messages where he quits the Twitter board two minutes after Agro

342
00:20:50,320 --> 00:20:53,760
Speaker 7:  Wall asks him not to say mean things about Twitter,

343
00:20:53,990 --> 00:20:56,480
Speaker 7:  like this man has no impulse control. Yeah.

344
00:20:56,670 --> 00:21:00,400
Speaker 3:  Well and I think part of this is also like a lot of the stuff that we're

345
00:21:00,400 --> 00:21:04,120
Speaker 3:  talking about assumes on some base level that Elon Musk

346
00:21:04,120 --> 00:21:07,920
Speaker 3:  tends to keep Twitter at like a very fundamental level the

347
00:21:07,920 --> 00:21:11,080
Speaker 3:  same. And I'm not sure there's actually like a lot of evidence that that's

348
00:21:11,080 --> 00:21:15,040
Speaker 3:  the case. I think like you look at what Elon Musk has wanted to do and

349
00:21:15,040 --> 00:21:17,440
Speaker 3:  he talks about, he talks about TikTok and he talks about payments and he

350
00:21:17,440 --> 00:21:20,960
Speaker 3:  talks about WeChat and it's like this idea of like, I wanna make

351
00:21:21,070 --> 00:21:24,880
Speaker 3:  this sort of beautiful network where people can talk to each other

352
00:21:24,880 --> 00:21:27,880
Speaker 3:  that like Jack Dorsey used to talk about, I don't know the dealer Musk has

353
00:21:27,880 --> 00:21:31,440
Speaker 3:  any particular interest in that. So like this idea that like he's just gonna

354
00:21:31,440 --> 00:21:35,000
Speaker 3:  sort of shunt that off to the side and like let it burn down while he tries

355
00:21:35,000 --> 00:21:38,440
Speaker 3:  to build a payment network on top of it and also tries to like retain his

356
00:21:38,760 --> 00:21:41,960
Speaker 3:  own audience where he can use it to sell Tesla's. That seems perfectly plausible

357
00:21:41,960 --> 00:21:45,440
Speaker 3:  to me that he's like, Oh no, is twitter.com or moderation nightmare and everybody's

358
00:21:45,440 --> 00:21:49,320
Speaker 3:  gonna stop using it. That's really not a problem. I'm gonna go like

359
00:21:49,320 --> 00:21:53,000
Speaker 3:  try to do something else on top of this network and make money at when people

360
00:21:53,000 --> 00:21:54,080
Speaker 3:  like buy subscriptions.

361
00:21:54,310 --> 00:21:57,960
Speaker 7:  Well I also think about airlines here because we're talking about a hellscape,

362
00:21:57,960 --> 00:22:01,720
Speaker 7:  right? And one of the things that Elon Musk has promised

363
00:22:01,720 --> 00:22:05,640
Speaker 7:  to do is he's, he wants to increase subscription revenue. Most of Twitter's

364
00:22:05,640 --> 00:22:09,080
Speaker 7:  revenue is advertising, but if you can make a subscriptions, you know, service

365
00:22:09,080 --> 00:22:12,360
Speaker 7:  really work for you, then it doesn't matter if you're brand safe because

366
00:22:12,360 --> 00:22:14,680
Speaker 7:  that's not where the bulk of your money is coming from, it's coming from

367
00:22:14,680 --> 00:22:17,840
Speaker 7:  your users. And so like one possible thing that you can do if you're evil

368
00:22:18,330 --> 00:22:21,800
Speaker 7:  is like you can be like, yeah you can have the hellscape or you can pay,

369
00:22:21,830 --> 00:22:24,240
Speaker 7:  I don't know, $10 a month and not have a hellscape.

370
00:22:24,410 --> 00:22:27,360
Speaker 5:  So this is the most interesting idea to me for

371
00:22:27,600 --> 00:22:29,240
Speaker 3:  The Spirit Airlines version of Twitter.

372
00:22:29,270 --> 00:22:33,200
Speaker 5:  Yeah, I just watched an incredible to talk with

373
00:22:33,200 --> 00:22:37,040
Speaker 5:  the Sea of Ryanair where he admitted that for a while he

374
00:22:37,040 --> 00:22:40,920
Speaker 5:  wanted to create a standing room only part of

375
00:22:40,920 --> 00:22:41,280
Speaker 5:  the plane.

376
00:22:41,830 --> 00:22:45,760
Speaker 6:  They tested it with like seats that where you just like stood up

377
00:22:45,760 --> 00:22:46,680
Speaker 6:  and kinda leaned.

378
00:22:46,790 --> 00:22:50,320
Speaker 5:  Yeah, yeah. And he was like, here's the thing, here's what I know. We would've

379
00:22:50,320 --> 00:22:54,000
Speaker 5:  filled that part of the plane first, but like it's morally outrageous so

380
00:22:54,000 --> 00:22:57,680
Speaker 5:  we didn't do it. I'm fine with this, but the reason I'm, I'm,

381
00:22:57,790 --> 00:23:01,400
Speaker 5:  I mean I don't, I refuse to fly Spirit Airlines. I'm the sucker is like,

382
00:23:01,400 --> 00:23:04,600
Speaker 5:  no I need, I need comfort plus at least come on. But

383
00:23:05,060 --> 00:23:08,720
Speaker 5:  the thing that is super interesting about this is there isn't a market

384
00:23:08,720 --> 00:23:12,480
Speaker 5:  for moderation. Like the thing the airlines have proven is that there's actually

385
00:23:12,480 --> 00:23:16,240
Speaker 5:  a market for fair classes, right? You can, you can set, you

386
00:23:16,240 --> 00:23:20,160
Speaker 5:  can segment the plane all the way to at the back, you are

387
00:23:20,360 --> 00:23:24,120
Speaker 5:  standing up and at the front you are lying down and people will pay various

388
00:23:24,120 --> 00:23:28,080
Speaker 5:  amounts of money to be in all those tiers of the plane. That's weird. You

389
00:23:28,080 --> 00:23:31,040
Speaker 5:  know, like maybe that's a little too capitalistic, but they've proven that

390
00:23:31,040 --> 00:23:34,400
Speaker 5:  there's a effective functional market for all those different kinds of service

391
00:23:34,400 --> 00:23:38,320
Speaker 5:  to get from point A to point B. We have never seen any

392
00:23:38,320 --> 00:23:42,280
Speaker 5:  social network really try to experiment with is there a market for levels

393
00:23:42,280 --> 00:23:46,040
Speaker 5:  of moderation even between the social networks because of all of the

394
00:23:46,280 --> 00:23:49,520
Speaker 5:  external pressures they all face from advertisers, from governments, from

395
00:23:49,520 --> 00:23:53,320
Speaker 5:  their own users and service of growth. They all kind of land

396
00:23:53,320 --> 00:23:56,920
Speaker 5:  at the same places with moderation, which I think is deeply

397
00:23:56,920 --> 00:24:00,080
Speaker 5:  frustrating to a lot of people. We've never seen any social networks say

398
00:24:00,080 --> 00:24:03,120
Speaker 5:  with a single user base. Okay, you can, you can pick

399
00:24:03,780 --> 00:24:07,240
Speaker 5:  all like all the way moderation, happy place, Disney World

400
00:24:07,570 --> 00:24:10,800
Speaker 5:  or Hellscape. And this is like one of those

401
00:24:11,920 --> 00:24:15,480
Speaker 5:  academic ideas that we like hear about all the time, like Addie and Casey

402
00:24:15,480 --> 00:24:19,240
Speaker 5:  and I have just like heard this idea a million times that maybe moderation

403
00:24:19,240 --> 00:24:21,400
Speaker 5:  should be outsourced and you should be able to bring your own moderation

404
00:24:21,680 --> 00:24:25,240
Speaker 5:  provider and your own algorithm to your social graph and you should buy the

405
00:24:25,240 --> 00:24:28,120
Speaker 5:  one you want. That sounds great. Except when you play it out, it's like,

406
00:24:28,370 --> 00:24:32,200
Speaker 5:  no, people don't want to think that hard about any software in their lives.

407
00:24:32,310 --> 00:24:36,160
Speaker 5:  Like they just want the one that comes in the box like, and

408
00:24:36,160 --> 00:24:40,040
Speaker 5:  so we'll just see if the default Twitter algorithm is the one that's

409
00:24:40,040 --> 00:24:43,840
Speaker 5:  safest for advertisers and the one that will create more growth, that's the

410
00:24:43,840 --> 00:24:47,480
Speaker 5:  one people are gonna use. And if your choice is I can turn it down and see

411
00:24:47,480 --> 00:24:51,360
Speaker 5:  more racism, no one's gonna turn it down or we are all gonna be

412
00:24:51,360 --> 00:24:54,640
Speaker 5:  very surprised at the market dynamics of what turning it down looks like.

413
00:24:54,950 --> 00:24:58,840
Speaker 7:  I also, while we're talking money, one thing that I am really curious about

414
00:24:58,850 --> 00:25:02,480
Speaker 7:  is not Twitter directly, but Twitter's effects on Elon's other companies.

415
00:25:02,490 --> 00:25:05,840
Speaker 7:  So like Twitter for instance, you know, if you have a strong free speech

416
00:25:06,440 --> 00:25:10,240
Speaker 7:  stance on Twitter and you're trying to sell cars in China, you have a very

417
00:25:10,240 --> 00:25:14,040
Speaker 7:  large attack surface whenever anything happens on Twitter that the Chinese

418
00:25:14,040 --> 00:25:17,400
Speaker 7:  government doesn't like. And one of the things that's true about Tesla is

419
00:25:17,400 --> 00:25:21,160
Speaker 7:  that it's not in India yet, but India is a really big market and

420
00:25:21,160 --> 00:25:25,120
Speaker 7:  Twitter is extremely controversial in India. And like you can imagine

421
00:25:25,120 --> 00:25:28,800
Speaker 7:  if you're taking a strong free speech stance where you don't remove Nazis,

422
00:25:28,910 --> 00:25:32,680
Speaker 7:  well you have a Tesla factory in Germany where a Nazi speech is banned.

423
00:25:33,210 --> 00:25:36,960
Speaker 7:  So there are, there's like, one of the things that I think is really interesting

424
00:25:36,960 --> 00:25:40,360
Speaker 7:  here is that Eli Musk has a much larger attack surface than anybody like

425
00:25:40,360 --> 00:25:43,800
Speaker 7:  Mark Zuckerberg or you know, anybody running any of these other

426
00:25:43,920 --> 00:25:47,760
Speaker 7:  companies. Like you can actually punish him through the other things that

427
00:25:47,760 --> 00:25:51,320
Speaker 7:  he does. And so I think that's going to be really fascinating to see how

428
00:25:51,320 --> 00:25:55,000
Speaker 7:  that plays out both for Twitter and for everything else. Like

429
00:25:55,230 --> 00:25:59,160
Speaker 7:  what happens to SpaceX? Like if, you know

430
00:25:59,490 --> 00:26:03,400
Speaker 7:  he does something that the government doesn't like, does, does SpaceX

431
00:26:03,400 --> 00:26:03,680
Speaker 7:  suffer?

432
00:26:04,070 --> 00:26:07,480
Speaker 5:  Yeah, I mean that one is like deeply

433
00:26:08,140 --> 00:26:12,080
Speaker 5:  contentious, right? Because SpaceX is in Texas, if he fights

434
00:26:12,080 --> 00:26:15,840
Speaker 5:  Abbott, that's just a big problem. But the government

435
00:26:15,840 --> 00:26:19,800
Speaker 5:  needs SpaceX, like it doesn't have the capability to launch anything by itself.

436
00:26:20,330 --> 00:26:23,880
Speaker 5:  So that's a good spot. I think Tesla is the

437
00:26:23,880 --> 00:26:27,600
Speaker 5:  scariest one, right? If you're Elon, you've now,

438
00:26:27,790 --> 00:26:31,360
Speaker 5:  you've got a big business in, in Germany, like you said, which has very

439
00:26:31,440 --> 00:26:35,320
Speaker 5:  stringent speech laws. You have a huge business in China where you can't

440
00:26:35,320 --> 00:26:39,080
Speaker 5:  even operate Twitter. Like your, the game you are now

441
00:26:39,080 --> 00:26:42,880
Speaker 5:  playing with Tesla in China is, if there's a lot of anti-China

442
00:26:43,040 --> 00:26:46,840
Speaker 5:  stuff on Twitter, they can, they, they're the Chinese

443
00:26:46,840 --> 00:26:50,720
Speaker 5:  government has no compunction about just destroying companies left and

444
00:26:50,720 --> 00:26:53,640
Speaker 5:  right. It destroys companies. Yeah. Especially the companies seem too powerful

445
00:26:53,640 --> 00:26:57,400
Speaker 5:  or the executives of those companies seem too powerful. Elon is now in a

446
00:26:57,400 --> 00:27:00,960
Speaker 5:  place where the pressure he will feel to make Twitter

447
00:27:00,960 --> 00:27:04,840
Speaker 5:  more China friendly is actually real because

448
00:27:04,840 --> 00:27:07,520
Speaker 5:  Tesla needs to expand in China because that is the market for

449
00:27:07,520 --> 00:27:10,880
Speaker 3:  Those cars. Yeah, and I, well, and I think e even more sort of abstractly

450
00:27:10,880 --> 00:27:14,160
Speaker 3:  like Liz, you've been, you've written about this a bunch in the last six

451
00:27:14,160 --> 00:27:18,080
Speaker 3:  months. There's like this force of personality that Elon Musk has that

452
00:27:18,080 --> 00:27:22,040
Speaker 3:  has gotten him a lot of what he has gotten over time. And it

453
00:27:22,040 --> 00:27:25,560
Speaker 3:  has like been his bet that just by virtue of the fact that I am Elon Musk,

454
00:27:25,830 --> 00:27:29,400
Speaker 3:  I can do whatever I want at any time and it'll probably work out because

455
00:27:29,400 --> 00:27:33,360
Speaker 3:  I'm Elon Musk and the rules don't apply to me. And historically speaking

456
00:27:33,360 --> 00:27:36,920
Speaker 3:  that is true. But it also seems like that like with

457
00:27:36,920 --> 00:27:40,760
Speaker 3:  this trial that didn't end up happening in Delaware, there

458
00:27:40,760 --> 00:27:44,640
Speaker 3:  was this real tide turning against him and you could feel that

459
00:27:44,640 --> 00:27:48,320
Speaker 3:  like the, the sort of cult of Elon Musk in which like the richest man in

460
00:27:48,320 --> 00:27:51,840
Speaker 3:  the world can do no wrong by virtue of who he is seems to be going away.

461
00:27:51,840 --> 00:27:55,600
Speaker 3:  And now he's like tied this incredible anvil around his neck

462
00:27:55,600 --> 00:27:59,240
Speaker 3:  called Twitter that feels like, like, like if

463
00:27:59,480 --> 00:28:03,360
Speaker 3:  anything is going to make everyone sour on the whole idea of Elon Musk,

464
00:28:03,360 --> 00:28:07,320
Speaker 3:  it's it, boy it seems like it would be Twitter. And it does feel

465
00:28:07,320 --> 00:28:11,080
Speaker 3:  like it's gonna have all kinds of ramifications

466
00:28:11,080 --> 00:28:13,840
Speaker 3:  across these other companies. Like the, the Tesla investors have been worried

467
00:28:13,840 --> 00:28:16,760
Speaker 3:  about this crazy distraction that is Twitter that is not going to get less

468
00:28:16,960 --> 00:28:20,760
Speaker 3:  distracting now that he owns the place and it just, yeah, it seems like if

469
00:28:20,760 --> 00:28:24,680
Speaker 3:  I'm interested in any of his other companies, Twitter feels like

470
00:28:24,680 --> 00:28:28,480
Speaker 3:  it's gonna just drag everything around him down into the

471
00:28:28,520 --> 00:28:29,480
Speaker 3:  muck forever and ever.

472
00:28:29,560 --> 00:28:32,960
Speaker 7:  David, I wouldn't say Anvil. I would say Albatross.

473
00:28:33,350 --> 00:28:33,840
Speaker 3:  Sure.

474
00:28:33,950 --> 00:28:37,160
Speaker 5:  Good at it from Liz. All right, let's take a break. I think we've made it

475
00:28:37,320 --> 00:28:39,720
Speaker 5:  abundantly clear what we think is going to happen to Twitter. Let's take

476
00:28:39,720 --> 00:28:43,520
Speaker 5:  a break though. Come back and let's do a little bit on what the upside might

477
00:28:43,800 --> 00:28:46,280
Speaker 5:  be. It's gonna be a short segment, but we ran back.

478
00:28:50,660 --> 00:28:54,630
Speaker 1:  This episode is brought to you by Blazing Trails and original podcasts. When

479
00:28:54,630 --> 00:28:58,310
Speaker 1:  the mind's over at Salesforce, their new six part series Meet the Customer

480
00:28:58,310 --> 00:29:01,950
Speaker 1:  hones in on the relationships between businesses and the people who buy their

481
00:29:01,950 --> 00:29:05,910
Speaker 1:  products. I had a chance to get an advanced listen to their first episode.

482
00:29:06,020 --> 00:29:09,630
Speaker 1:  It's all about how King Arthur Flower listens to their customers to know

483
00:29:09,630 --> 00:29:13,150
Speaker 1:  where they're going. And if you like pizza, I mean of course you like pizza,

484
00:29:13,300 --> 00:29:17,190
Speaker 1:  you'll like this episode to hear, meet the customer, search for Blazing

485
00:29:17,190 --> 00:29:20,990
Speaker 1:  Trails from Salesforce in your podcast player and thanks to Salesforce for

486
00:29:20,990 --> 00:29:21,430
Speaker 1:  their support.

487
00:29:24,260 --> 00:29:28,010
Speaker 3:  Today's episode is sponsored by npr, the podcast Planet Money

488
00:29:28,010 --> 00:29:31,490
Speaker 3:  imagines a world where the complex economy actually makes sense,

489
00:29:31,490 --> 00:29:35,330
Speaker 3:  where human stories supersede complex theories and where listeners

490
00:29:35,330 --> 00:29:38,930
Speaker 3:  can learn, laugh and be entertained. It's econ down to earth

491
00:29:39,000 --> 00:29:42,760
Speaker 3:  from How the Perfect Apple was created to new ways to pay for college, to

492
00:29:42,760 --> 00:29:46,040
Speaker 3:  breaking down the price of gasoline. The Planet Money Team lives to break

493
00:29:46,040 --> 00:29:49,840
Speaker 3:  down an interesting topic and tell a good story all in a 30 minute episode.

494
00:29:49,840 --> 00:29:53,640
Speaker 3:  I have to tell you, I've been listening to Planet Money for years. It's one

495
00:29:53,640 --> 00:29:57,520
Speaker 3:  of the best podcasts about just like the weird side of how the

496
00:29:57,520 --> 00:30:01,320
Speaker 3:  economy works. They find deeply strange things like that. Apple episode

497
00:30:01,320 --> 00:30:05,160
Speaker 3:  was about how we went from really crappy apples to really great apples

498
00:30:05,160 --> 00:30:08,920
Speaker 3:  and how the business world made that happen. It's super interesting.

499
00:30:08,920 --> 00:30:11,560
Speaker 3:  You should listen to it. The episodes are pretty short. The hosts are really

500
00:30:11,560 --> 00:30:15,000
Speaker 3:  fun to listen to. It's a very good podcast. I cannot recommend it enough.

501
00:30:15,000 --> 00:30:17,760
Speaker 3:  Planet Money comes out every week and you can tune in to hear entertaining

502
00:30:17,760 --> 00:30:21,400
Speaker 3:  stories and insights about how money shapes our world stories that can't

503
00:30:21,400 --> 00:30:24,720
Speaker 3:  be found anywhere else. Check out Planet Money from NPR wherever you get

504
00:30:24,720 --> 00:30:25,400
Speaker 3:  your podcasts.

505
00:30:32,190 --> 00:30:35,760
Speaker 5:  Okay, we're back. Let's, I think it's clear

506
00:30:36,210 --> 00:30:39,800
Speaker 5:  we don't think this is gonna go well, but let's, let's do the thought exercise

507
00:30:39,980 --> 00:30:43,760
Speaker 5:  and let's stipulate, but let's do the thought exercise of what might go

508
00:30:43,760 --> 00:30:47,280
Speaker 5:  well here, what might work. And we should stipulate like I will happily concede,

509
00:30:47,410 --> 00:30:51,240
Speaker 5:  Twitter is not a well run company. It has not had good

510
00:30:51,240 --> 00:30:54,960
Speaker 5:  executives running it or even like executives who are paying

511
00:30:54,960 --> 00:30:58,520
Speaker 5:  attention or use the service. Like it's just not,

512
00:30:58,790 --> 00:31:02,080
Speaker 5:  it's it, it probably has too many people. I, you know, it has

513
00:31:02,840 --> 00:31:06,600
Speaker 5:  thousands of people doing nothing in Prag Auger roll the the

514
00:31:06,760 --> 00:31:10,520
Speaker 5:  outgoing ceo, like his greatest accomplishment was forcing Elon to buy the

515
00:31:10,520 --> 00:31:14,440
Speaker 5:  company. The only person who has ever stood up to Elon Musk is Prag aural.

516
00:31:14,440 --> 00:31:18,040
Speaker 5:  Congratulations, you shipped no software but yet you

517
00:31:18,040 --> 00:31:21,000
Speaker 5:  executed this. You had the stones to, to stand up to Elon. Great

518
00:31:21,030 --> 00:31:24,720
Speaker 7:  Incredible man, Not incredible deal

519
00:31:24,720 --> 00:31:28,600
Speaker 7:  by the way. Like incredible moves. Like this is like CEO hall of fame

520
00:31:28,600 --> 00:31:32,320
Speaker 7:  shit and he wasn't even CEO for a year. So like hats off

521
00:31:32,320 --> 00:31:32,880
Speaker 7:  man. Good

522
00:31:32,880 --> 00:31:36,720
Speaker 5:  For you. I think that, Who's the Salesforce guy? Benioff? No, the other

523
00:31:36,720 --> 00:31:40,680
Speaker 5:  guy who's on the board. Brett Taylor. I think Brett Taylor who's the, the

524
00:31:40,880 --> 00:31:43,480
Speaker 5:  chairman of Twitter's board and the co CEO Salesforce might have helped.

525
00:31:43,670 --> 00:31:47,160
Speaker 5:  I just wanna put that out. But congrats to Prague.

526
00:31:47,430 --> 00:31:51,360
Speaker 5:  It's not like Twitter is well run, right? So Elon's showing up, maybe he's

527
00:31:51,360 --> 00:31:53,600
Speaker 5:  gonna run it. Well what are the good things that could happen?

528
00:31:53,660 --> 00:31:57,600
Speaker 3:  The first thing that jumps out to me is that by virtue of being private and

529
00:31:57,600 --> 00:32:01,400
Speaker 3:  also by being owned by someone like Elon Musk, Twitter can

530
00:32:01,400 --> 00:32:05,080
Speaker 3:  in theory start to do some of these things that don't

531
00:32:05,080 --> 00:32:08,840
Speaker 3:  obviously scale, which we've never really seen in social before. Like this

532
00:32:08,840 --> 00:32:12,360
Speaker 3:  idea that you know, you could start charging money for Facebook and some

533
00:32:12,360 --> 00:32:14,720
Speaker 3:  percentage of people would sign up for Facebook, like Facebook was never

534
00:32:14,720 --> 00:32:18,400
Speaker 3:  gonna invest in that because you have quarterly results and you can't

535
00:32:18,400 --> 00:32:21,760
Speaker 3:  make a dent in your business in order for like, there just is no such thing

536
00:32:21,760 --> 00:32:25,640
Speaker 3:  as like a long term vision for something like that, for these like big public

537
00:32:25,640 --> 00:32:27,960
Speaker 3:  companies because everything has to look like growth and everything has to

538
00:32:27,960 --> 00:32:31,840
Speaker 3:  look like scale. And Elon has even talked about this that like the

539
00:32:31,840 --> 00:32:34,880
Speaker 3:  only way to reset Twitter is to basically go through some really, really,

540
00:32:34,880 --> 00:32:38,840
Speaker 3:  really like messy times on the platform. And if he actually has

541
00:32:38,840 --> 00:32:42,760
Speaker 3:  the patience to do that, we can like get this incubator of

542
00:32:43,000 --> 00:32:46,880
Speaker 3:  interesting social ideas that I think could in theory be very cool.

543
00:32:47,440 --> 00:32:51,280
Speaker 3:  And some of the stuff he's talked about like trying to experiment with subscriptions

544
00:32:51,280 --> 00:32:55,120
Speaker 3:  and trying to like build new ways of thinking about algorithms and like

545
00:32:55,510 --> 00:32:58,880
Speaker 3:  will any of that work? I don't know, but at least there's like an interesting

546
00:32:58,880 --> 00:33:02,640
Speaker 3:  test bed with a lot of people inside of it that could actually lead to some

547
00:33:02,840 --> 00:33:03,480
Speaker 3:  interesting stuff.

548
00:33:04,030 --> 00:33:07,920
Speaker 7:  I am going to say one thing that's close to reality and one

549
00:33:07,920 --> 00:33:11,280
Speaker 7:  thing that I think is pretty far fetched but I think is also interesting,

550
00:33:11,900 --> 00:33:15,480
Speaker 7:  the thing that's close to reality is I personally would love to see encrypted

551
00:33:15,540 --> 00:33:19,480
Speaker 7:  dms and I understand that there are probably downsides to doing that

552
00:33:19,480 --> 00:33:23,320
Speaker 7:  kind of encryption specifically around like reporting spam in your

553
00:33:23,320 --> 00:33:26,720
Speaker 7:  dms and so on. But the upside is that you have secure dms now, which has

554
00:33:26,720 --> 00:33:30,200
Speaker 7:  been a long time problem for Twitter and for anybody using Twitter. And that

555
00:33:30,200 --> 00:33:32,720
Speaker 7:  seems like it could be a really good thing and it's something that we've

556
00:33:32,720 --> 00:33:36,600
Speaker 7:  heard Musk talking about. So like I certainly hope that he executes on that

557
00:33:36,600 --> 00:33:39,720
Speaker 7:  at least. But the other thing that I think about that's really like more

558
00:33:39,720 --> 00:33:43,560
Speaker 7:  far-fetched is we know he has an interest in AI and we know he has an interest

559
00:33:43,560 --> 00:33:46,560
Speaker 7:  in neurolink, which is a brain machine interface that's like

560
00:33:48,010 --> 00:33:51,920
Speaker 7:  he Musk's ambitions for it are well beyond what brain

561
00:33:51,920 --> 00:33:55,880
Speaker 7:  machine interfaces probably can do. But he wants to like have

562
00:33:55,880 --> 00:33:58,680
Speaker 7:  it be like, you know, your way to communicate with a computer, you can be

563
00:33:58,880 --> 00:34:02,600
Speaker 7:  a cyborg if you want. So like there's like this wild idea that has been

564
00:34:02,600 --> 00:34:06,080
Speaker 7:  circulating that like I now have as an intrusive thought where it's like

565
00:34:06,080 --> 00:34:09,560
Speaker 7:  oh he's going to use all of our language inputs to train like

566
00:34:09,890 --> 00:34:13,520
Speaker 7:  an AI of some kind and it's gonna be like the meanest,

567
00:34:13,550 --> 00:34:17,240
Speaker 7:  nastiest AI you've ever seen. But

568
00:34:17,420 --> 00:34:21,320
Speaker 7:  you know it's gonna be trained with all of this real world input from

569
00:34:21,320 --> 00:34:25,120
Speaker 7:  all of these real people who are really all on Twitter and like as a database

570
00:34:25,120 --> 00:34:28,560
Speaker 7:  set, if you're a person who likes ai, that's potentially interesting.

571
00:34:31,140 --> 00:34:34,760
Speaker 5:  I'm just, sorry, I'm just imagining these AI that are just doing Twitter

572
00:34:34,760 --> 00:34:36,320
Speaker 5:  memes all day like this,

573
00:34:41,270 --> 00:34:41,760
Speaker 9:  Just

574
00:34:41,960 --> 00:34:43,800
Speaker 6:  Dolly pictures, boom,

575
00:34:45,870 --> 00:34:46,840
Speaker 6:  it's gonna be great.

576
00:34:49,020 --> 00:34:52,000
Speaker 3:  You say like, hello. It just responds with the thread icons.

577
00:34:52,820 --> 00:34:56,080
Speaker 5:  The most sophisticated AI in the world is all like I did a thing.

578
00:35:01,070 --> 00:35:01,560
Speaker 8:  Oops.

579
00:35:01,900 --> 00:35:04,040
Speaker 7:  All I'm saying is he brought in a sink. You never know.

580
00:35:05,190 --> 00:35:08,880
Speaker 5:  Just, all right, so David's on interesting

581
00:35:08,880 --> 00:35:12,600
Speaker 5:  tested, let's experiment on the population of celebrities

582
00:35:12,600 --> 00:35:16,520
Speaker 5:  and world leaders to see what happens. Internalists Liz is, I

583
00:35:16,520 --> 00:35:20,000
Speaker 5:  want brains and vats basically brains and vats.

584
00:35:20,070 --> 00:35:21,200
Speaker 5:  Alex, what do you got?

585
00:35:21,470 --> 00:35:25,240
Speaker 6:  I think like this guy is going to try to use AI

586
00:35:25,240 --> 00:35:29,160
Speaker 6:  to solve the moderation problem, right? And he's going to put a

587
00:35:29,160 --> 00:35:32,480
Speaker 6:  ton, he's already putting resources towards it. He was like, all right, I'll

588
00:35:32,480 --> 00:35:36,320
Speaker 6:  use Tesla self-driving car engineers get on over to Twitter

589
00:35:36,320 --> 00:35:40,280
Speaker 6:  and figure this out. And so I think we are pretty rapidly going

590
00:35:40,280 --> 00:35:43,760
Speaker 6:  to have a very conclusive answer to can AI solve

591
00:35:43,760 --> 00:35:47,480
Speaker 6:  moderation? I think the answer's gonna be no. But I think we're going to

592
00:35:47,480 --> 00:35:50,880
Speaker 6:  see so many resources put towards it in a way that we just at a level we

593
00:35:50,880 --> 00:35:54,720
Speaker 6:  just have never seen before that like we're gonna be able to say he did

594
00:35:55,120 --> 00:35:58,770
Speaker 6:  everything and still couldn't do it. And we'll finally be able to

595
00:35:58,790 --> 00:36:01,210
Speaker 6:  put that like put that baby in bed.

596
00:36:02,370 --> 00:36:03,650
Speaker 6:  That's it. We have the,

597
00:36:03,650 --> 00:36:06,290
Speaker 5:  Here you go, dumb baby, you're asleep now We've

598
00:36:06,290 --> 00:36:09,850
Speaker 6:  Seen it. We've seen it, We know we can't do it. Move on.

599
00:36:09,960 --> 00:36:13,930
Speaker 5:  Yeah. So the one thing I'll say about that I agree, I again, I

600
00:36:13,930 --> 00:36:17,850
Speaker 5:  think a lot of what we're gonna see here is ideas that are abstract and

601
00:36:18,050 --> 00:36:21,810
Speaker 5:  academic getting run to ground. Yep. Right? Marketplace of

602
00:36:21,810 --> 00:36:25,490
Speaker 5:  moderation systems that ideas gonna get run to ground. Can we scale

603
00:36:25,500 --> 00:36:29,090
Speaker 5:  AI to do it? That idea is gonna get run to ground because if he wants to

604
00:36:29,090 --> 00:36:32,090
Speaker 5:  fire as many people as he has said he wants to fire, Twitter itself said

605
00:36:32,090 --> 00:36:35,970
Speaker 5:  it wanted to fire 25% of its employees this year before Elon showed up

606
00:36:36,250 --> 00:36:39,410
Speaker 5:  with a sink. So if he wants to get rid of all the people he wants to get

607
00:36:39,410 --> 00:36:42,610
Speaker 5:  rid of you, you have to automate some of it. Yeah.

608
00:36:42,850 --> 00:36:46,410
Speaker 5:  Right? Like or you realize like you just have a bunch of engineers who are

609
00:36:46,490 --> 00:36:50,250
Speaker 5:  drinking coffee all day, not doing anything, tune us. But to me the

610
00:36:50,590 --> 00:36:54,570
Speaker 5:  the thing that he's, because he's the king of Twitter, every moderation

611
00:36:54,810 --> 00:36:58,650
Speaker 5:  decision is gonna be, it's his reputation. Yeah. And the thing that

612
00:36:58,650 --> 00:37:02,570
Speaker 5:  is true of every automated system anywhere in the

613
00:37:02,570 --> 00:37:06,530
Speaker 5:  world doing anything is that they false a lot, right? They

614
00:37:06,530 --> 00:37:10,450
Speaker 5:  just like, they have an error rate. It's inherent to everything. So you

615
00:37:10,450 --> 00:37:14,170
Speaker 5:  have an, I'll give you another benign example. Content ID on YouTube

616
00:37:14,980 --> 00:37:18,770
Speaker 5:  is theoretically a benign system. I don't think it's Ayn system,

617
00:37:18,770 --> 00:37:21,690
Speaker 5:  but theoretically it's a benign system, right? Where

618
00:37:22,670 --> 00:37:26,610
Speaker 5:  the music industry doesn't wants to get paid for its music, which

619
00:37:26,610 --> 00:37:30,090
Speaker 5:  is also theoretically a benign motivation. And so they can take stuff down

620
00:37:30,090 --> 00:37:33,970
Speaker 5:  if it matches the copyrights of songs. This thing falses

621
00:37:33,980 --> 00:37:37,570
Speaker 5:  so much all the time. I think

622
00:37:37,770 --> 00:37:41,490
Speaker 5:  most people have a better sense of the copyright

623
00:37:41,490 --> 00:37:45,290
Speaker 5:  policy of YouTube than like the speed limit

624
00:37:45,290 --> 00:37:46,770
Speaker 5:  two miles away from their house,

625
00:37:48,290 --> 00:37:51,810
Speaker 5:  Right? Cuz you are just confronted with this automated system

626
00:37:51,810 --> 00:37:55,770
Speaker 5:  fucking with people all the time and all the YouTubers are constantly

627
00:37:55,770 --> 00:37:59,450
Speaker 5:  talking about it. And that's like a inherently benign system,

628
00:37:59,570 --> 00:38:03,330
Speaker 5:  right? That's a system that's designed that has the motivations like

629
00:38:03,330 --> 00:38:06,290
Speaker 5:  basically everybody can agree on, which is artists should get paid for their

630
00:38:06,290 --> 00:38:10,170
Speaker 5:  music. And yet it's like a in practice, a maligned

631
00:38:10,170 --> 00:38:13,530
Speaker 5:  force, right? It like makes the experience worse for lots of people. You

632
00:38:13,530 --> 00:38:17,250
Speaker 5:  expand that to like semantic analysis of whether is this joke

633
00:38:17,250 --> 00:38:21,210
Speaker 5:  racist or is it in context? Justine Sacco, Like who

634
00:38:21,210 --> 00:38:25,010
Speaker 5:  knows, right? That when that system starts fing whatever,

635
00:38:25,010 --> 00:38:28,690
Speaker 5:  error raid, all of it gets blamed on Elon. And that is like a

636
00:38:28,970 --> 00:38:29,570
Speaker 5:  disaster zone.

637
00:38:29,680 --> 00:38:33,330
Speaker 3:  I think you might be vastly underestimating Elon Musk's ability

638
00:38:33,340 --> 00:38:37,240
Speaker 3:  to not give one single shit about whether

639
00:38:37,240 --> 00:38:40,120
Speaker 3:  people are mad at him about moderation decisions like this has all made me

640
00:38:40,120 --> 00:38:43,240
Speaker 3:  think about Mark Zuckerberg a lot because there was this like, this like

641
00:38:43,240 --> 00:38:46,520
Speaker 3:  joker origin story from Mark Zuckerberg where he like, I think

642
00:38:46,560 --> 00:38:50,160
Speaker 3:  genuinely felt bad about some of the mistakes that Facebook was making and

643
00:38:50,220 --> 00:38:53,920
Speaker 3:  and sort of, you know, apologized over and over and over in public. And then

644
00:38:53,920 --> 00:38:56,560
Speaker 3:  you could just see over the course of the last few years went to a point

645
00:38:56,560 --> 00:38:59,120
Speaker 3:  where he's just like, I'm not gonna talk about this anymore. Like, I'm just

646
00:38:59,120 --> 00:39:02,160
Speaker 3:  not interested in this. I'm gonna talk about the metaverse and if you wanna

647
00:39:02,160 --> 00:39:05,800
Speaker 3:  be mad about moderation problems, go be mad at somebody else. This is no

648
00:39:05,800 --> 00:39:09,000
Speaker 3:  longer my problem. I pay nickle a lot of money to be the one who apologizes

649
00:39:09,000 --> 00:39:10,880
Speaker 3:  for all the dumb stuff that happens on Facebook. I'm

650
00:39:10,880 --> 00:39:13,120
Speaker 5:  So excited for the first Elon Congressional hearing.

651
00:39:13,880 --> 00:39:17,810
Speaker 3:  Yeah. Elon is like a 40 out of 10 on that scale of just like,

652
00:39:22,320 --> 00:39:25,770
Speaker 3:  he's like, Oh, you're mad. Congratulations. Enjoy being mad

653
00:39:26,020 --> 00:39:27,810
Speaker 3:  on Elon Musk. Goodbye. A

654
00:39:27,810 --> 00:39:28,490
Speaker 5:  69 out

655
00:39:28,490 --> 00:39:31,250
Speaker 3:  Of 10, he's a 60, he's a four 20.69.

656
00:39:34,020 --> 00:39:37,530
Speaker 5:  Alright, so what do I, the one good thing that I think will happen out of

657
00:39:37,530 --> 00:39:41,290
Speaker 5:  all of this is that this is such a backwards

658
00:39:41,290 --> 00:39:45,010
Speaker 5:  compliment. All right, here's my backwards compliment. I

659
00:39:45,010 --> 00:39:48,690
Speaker 5:  think people are tired of Twitter, I think they're tired of social

660
00:39:48,690 --> 00:39:52,410
Speaker 5:  networks like this. I feel tired of them and I, I

661
00:39:52,410 --> 00:39:56,130
Speaker 5:  think that all this noise is just like a shock

662
00:39:56,130 --> 00:39:59,650
Speaker 5:  to the system that will make people reconsider how they spend their time.

663
00:40:00,120 --> 00:40:03,580
Speaker 5:  And I, I, we talked about it on the vercast earlier today, like

664
00:40:03,910 --> 00:40:07,140
Speaker 5:  we are surrounded by tech giants that are pretty complacent.

665
00:40:07,620 --> 00:40:11,190
Speaker 5:  Like, and their products are getting weird and bad. Like Google search is

666
00:40:11,190 --> 00:40:14,900
Speaker 5:  bad. Like whatever is going on with the

667
00:40:14,900 --> 00:40:18,740
Speaker 5:  iPad is not the force of focused competition,

668
00:40:20,050 --> 00:40:23,100
Speaker 5:  it's just the stuff is happening and they're trying to extract more money

669
00:40:23,100 --> 00:40:26,740
Speaker 5:  from everything. Twitter is in this unique position where everyone's just

670
00:40:26,740 --> 00:40:30,060
Speaker 5:  sort of, it's the default for a bunch of journalists and politicians. It's

671
00:40:30,060 --> 00:40:32,940
Speaker 5:  where we're all mean to each other. It's like we issue dunks and if this

672
00:40:32,940 --> 00:40:36,900
Speaker 5:  just creates a moment for some people to reconsider their relationship with

673
00:40:36,900 --> 00:40:40,420
Speaker 5:  that product cause they don't wanna work for Elon for free, right? And if

674
00:40:40,420 --> 00:40:44,340
Speaker 5:  you are creating content for Twitter, you are now working for Elon

675
00:40:44,340 --> 00:40:47,180
Speaker 5:  Musk for free. I don't think you should work for the richest man in the world

676
00:40:47,180 --> 00:40:51,140
Speaker 5:  for free. That's just my my take on it. That might actually shake a

677
00:40:51,140 --> 00:40:55,060
Speaker 5:  bunch of competition loose. And I think that is just a huge benefit for everybody

678
00:40:55,060 --> 00:40:58,740
Speaker 5:  right now cuz they're so in the sort of core internet services,

679
00:40:58,870 --> 00:41:02,500
Speaker 5:  there's such vanishingly little competition

680
00:41:03,070 --> 00:41:07,060
Speaker 5:  in like, everyone's afraid of TikTok. That's it. That's what you got.

681
00:41:07,060 --> 00:41:10,740
Speaker 5:  That's the evidence of competition on the internet. Today's TikTok and they're

682
00:41:10,740 --> 00:41:13,220
Speaker 5:  all competing with it by just slavishly copying it.

683
00:41:14,540 --> 00:41:18,400
Speaker 5:  I'm just, I'm just hopeful that this is, you can't just tell people to be

684
00:41:18,400 --> 00:41:21,560
Speaker 5:  different. You have to like provide some shock to the system and this is

685
00:41:21,560 --> 00:41:23,480
Speaker 5:  like the biggest possible shock to the system.

686
00:41:23,840 --> 00:41:27,120
Speaker 6:  Tumblr's gonna come back. That's, that's my prediction

687
00:41:27,390 --> 00:41:30,240
Speaker 3:  That that might be real. But I've been thinking, I've been thinking a lot

688
00:41:30,240 --> 00:41:33,640
Speaker 3:  about this cuz I think like one of the big questions coming out of this is

689
00:41:33,640 --> 00:41:36,360
Speaker 3:  like, okay, if you wanna leave Twitter, like where do you go? Tell me if

690
00:41:36,360 --> 00:41:39,360
Speaker 3:  there just isn't something else like Twitter. But then I think the question

691
00:41:39,450 --> 00:41:42,440
Speaker 3:  is morphing into something much more interesting, which is like, and this

692
00:41:42,440 --> 00:41:44,840
Speaker 3:  is what's happening to a lot of people with social in general. It's like,

693
00:41:44,930 --> 00:41:48,760
Speaker 3:  is this a thing that I want, Like, like is is, is is

694
00:41:48,760 --> 00:41:52,680
Speaker 3:  the premise of Twitter a good idea? Is a thing

695
00:41:52,680 --> 00:41:56,200
Speaker 3:  that like, I I have not, I've spent a lot of time wondering why Twitter is

696
00:41:56,200 --> 00:41:59,640
Speaker 3:  such a bad platform, but like, is Twitter a bad idea? Is like not a question

697
00:41:59,640 --> 00:42:02,960
Speaker 3:  I think a lot of people have thought about and I think a lot of people are

698
00:42:02,960 --> 00:42:06,400
Speaker 3:  going through this now with like what Facebook has become and what Instagram

699
00:42:06,400 --> 00:42:10,360
Speaker 3:  has become and now what, what Twitter has become is gonna spark that in a

700
00:42:10,360 --> 00:42:13,080
Speaker 3:  lot of people. So I think you're right. And it's like, for me at least, it

701
00:42:13,080 --> 00:42:16,320
Speaker 3:  has become this like weirdly philosophical thing where it's like, I'm not

702
00:42:16,320 --> 00:42:19,080
Speaker 3:  trying to like replace Twitter with Mastodon. I'm like, maybe I should just

703
00:42:19,290 --> 00:42:23,080
Speaker 3:  be done with this thing that I do and consume all day in

704
00:42:23,080 --> 00:42:23,800
Speaker 3:  general. Yeah.

705
00:42:24,430 --> 00:42:27,840
Speaker 7:  I mean, I quit Instagram this year. Like I still have an Instagram account,

706
00:42:27,840 --> 00:42:31,280
Speaker 7:  I just don't open the app anymore. And I've been thinking

707
00:42:31,280 --> 00:42:35,160
Speaker 7:  seriously about like, my relationship with Twitter and much like

708
00:42:35,160 --> 00:42:39,040
Speaker 7:  the people in that Reuters report, unless I'm live tweeting a

709
00:42:39,040 --> 00:42:42,880
Speaker 7:  trial or something, I, I have seriously decreased my Twitter

710
00:42:42,900 --> 00:42:46,800
Speaker 7:  use partially because it's like, I open it up and it's like

711
00:42:47,310 --> 00:42:50,920
Speaker 7:  a lot of bad feelings and people being mean to each other and like, I don't

712
00:42:50,920 --> 00:42:54,760
Speaker 7:  know about you guys, but like, I felt bad enough during the pandemic.

713
00:42:54,830 --> 00:42:58,800
Speaker 7:  Like I didn't, I didn't need the bad feelings app that said, I do

714
00:42:58,800 --> 00:43:02,480
Speaker 7:  think there is a very funny outcome where like some also ran social network,

715
00:43:02,550 --> 00:43:05,960
Speaker 7:  like Tumblr or LinkedIn gets an influx

716
00:43:06,290 --> 00:43:10,080
Speaker 7:  of like weird garbage users who

717
00:43:10,080 --> 00:43:12,520
Speaker 7:  like go full Twitter on the platform. Oh my

718
00:43:12,520 --> 00:43:15,320
Speaker 5:  God, that would be amazing.

719
00:43:15,800 --> 00:43:19,520
Speaker 6:  Tumblr's already seen it. Tumblr's seen like between TikTok and Twitter,

720
00:43:19,920 --> 00:43:23,720
Speaker 6:  Tumblr's has seen like this influx of users and they're young users. That's

721
00:43:23,720 --> 00:43:27,000
Speaker 6:  the thing is like everybody in this room is like, I've grown tired of social

722
00:43:27,000 --> 00:43:30,720
Speaker 6:  media and that's because everybody in this room remembers a time before social

723
00:43:30,720 --> 00:43:34,320
Speaker 6:  media and we have a whole generation of people who don't remember that, who

724
00:43:34,320 --> 00:43:38,040
Speaker 6:  social media is just a part of their lives. They assume that it's,

725
00:43:38,070 --> 00:43:41,960
Speaker 6:  that it's like universal. Everybody has it. And that's the group that I

726
00:43:41,960 --> 00:43:45,640
Speaker 6:  don't necessarily think is going to have these big moments of is social

727
00:43:45,640 --> 00:43:49,520
Speaker 6:  media for me, I think that's just a question that never enters

728
00:43:49,520 --> 00:43:52,280
Speaker 6:  their head. And they are going, if they choose to migrate, they're going

729
00:43:52,280 --> 00:43:55,840
Speaker 6:  to migrate. They're not just gonna like delete the app and

730
00:43:56,080 --> 00:43:56,960
Speaker 6:  touch grass.

731
00:43:57,070 --> 00:44:00,800
Speaker 5:  Well, I mean Tumblr is, there's a PhD thesis

732
00:44:00,860 --> 00:44:02,440
Speaker 5:  why Tumblr persists. Like

733
00:44:04,570 --> 00:44:07,920
Speaker 5:  despite the best efforts of everyone around Tumblr, including

734
00:44:07,920 --> 00:44:09,920
Speaker 6:  The users of Tumblr, right?

735
00:44:09,920 --> 00:44:10,240
Speaker 5:  Like

736
00:44:10,920 --> 00:44:11,920
Speaker 6:  Everyone has tried to destroy,

737
00:44:11,920 --> 00:44:15,840
Speaker 5:  Here are some things that have tried to kill Tumblr. Apple, straight up trying

738
00:44:15,840 --> 00:44:19,800
Speaker 5:  to murder Tumblr, Verizon, the user base

739
00:44:19,800 --> 00:44:20,640
Speaker 5:  of Tumblr.

740
00:44:22,470 --> 00:44:24,200
Speaker 5:  Okay, Yahoo. Yahoo.

741
00:44:26,830 --> 00:44:30,680
Speaker 5:  Like through the years, like a new challenger emerges and Tumblr's like,

742
00:44:30,680 --> 00:44:33,840
Speaker 5:  I stand best stride you. So

743
00:44:34,200 --> 00:44:38,000
Speaker 5:  Steve Tumblr, if you're in the Tumblr meeting this morning

744
00:44:38,000 --> 00:44:41,960
Speaker 5:  where they're like, fucking, it's on, it's on. Sneak

745
00:44:41,960 --> 00:44:45,200
Speaker 5:  that shit to us. We will write about it without even checking the sources

746
00:44:45,200 --> 00:44:45,720
Speaker 5:  out. All

747
00:44:45,720 --> 00:44:48,840
Speaker 7:  12 people that work there, they're like, Oh man, it's our time.

748
00:44:49,400 --> 00:44:52,600
Speaker 5:  Right? That's the best outcome. Tumblr resurgence.

749
00:44:53,440 --> 00:44:54,000
Speaker 3:  Thank

750
00:44:54,000 --> 00:44:57,960
Speaker 5:  You Elon. It's back. We, we, we have carried on for too long about this.

751
00:44:57,970 --> 00:45:00,640
Speaker 5:  We, who knows what happens next. The deal is closed, He's taking it over.

752
00:45:00,680 --> 00:45:04,280
Speaker 5:  We'll see what happens next is like the only logical

753
00:45:04,840 --> 00:45:08,280
Speaker 5:  thing to end on, because honestly, none of us know.

754
00:45:08,280 --> 00:45:12,080
Speaker 5:  There's, there's no prediction here. How many people are gonna quit Twitter

755
00:45:12,080 --> 00:45:15,280
Speaker 5:  today? Right? It's just a, who knows. So yeah,

756
00:45:15,280 --> 00:45:18,480
Speaker 3:  The, the leadership question seems like the most immediate one, right? Like

757
00:45:18,480 --> 00:45:19,080
Speaker 3:  who is going

758
00:45:19,080 --> 00:45:22,240
Speaker 5:  To, He said he's gonna be the ceo that was a report in Bloomberg yesterday.

759
00:45:22,240 --> 00:45:25,920
Speaker 3:  Right? But like forever, like at some point there are going to have to be

760
00:45:25,920 --> 00:45:27,520
Speaker 3:  people who like run this company professionally.

761
00:45:27,810 --> 00:45:31,600
Speaker 7:  So he's previously said he's an interim CEO and he will be bringing somebody

762
00:45:31,600 --> 00:45:35,560
Speaker 7:  in. And if you remember from the text messages, Jason Callons, Jason,

763
00:45:35,700 --> 00:45:39,600
Speaker 7:  the, the VC and entrepreneur has said that Twitter

764
00:45:39,600 --> 00:45:42,920
Speaker 7:  CEO is his dream job. So like maybe we have that to look forward to.

765
00:45:42,920 --> 00:45:44,760
Speaker 5:  Yes, everything is hustling. The

766
00:45:44,760 --> 00:45:48,080
Speaker 3:  Only good outcome is if it's Jack Dorsey. Just that's just, it's

767
00:45:48,470 --> 00:45:50,800
Speaker 3:  just full chaos mode. Bring Jack

768
00:45:51,920 --> 00:45:53,240
Speaker 3:  back. Let's just do this all again.

769
00:45:53,540 --> 00:45:57,400
Speaker 5:  All right, well that's our last prediction, Jack Dorsey. It's,

770
00:45:57,400 --> 00:45:59,880
Speaker 5:  it's, he's not gonna look like he came outta the woods. He's looked like

771
00:45:59,880 --> 00:46:01,360
Speaker 5:  he came outta the woods for like five years

772
00:46:01,360 --> 00:46:02,600
Speaker 7:  Now. Just one more tw in his

773
00:46:02,600 --> 00:46:05,480
Speaker 3:  Beer. I mean, what's F Williams doing now? Bring him back. He's outta medium.

774
00:46:05,480 --> 00:46:07,440
Speaker 5:  Yeah, Bi stone, Just all the founders. Just,

775
00:46:07,440 --> 00:46:09,040
Speaker 3:  Just start Twitter at the beginning again. Yeah.

776
00:46:09,610 --> 00:46:12,880
Speaker 5:  Oh man. All right. That's it. That's the verest. We've done enough thanks

777
00:46:12,880 --> 00:46:15,480
Speaker 5:  to Liz for joining us. It's super, it's the west coast. She's on the west

778
00:46:15,480 --> 00:46:18,640
Speaker 5:  coast. It's very early for hers. Thank you so much, Liz. We are gonna have

779
00:46:18,640 --> 00:46:22,320
Speaker 5:  a weekend. Everybody have it. Don't tweet, just have a weekend, take your

780
00:46:22,320 --> 00:46:25,000
Speaker 5:  foot off the gas. It's gonna be great. We'll see you next week.

781
00:46:29,580 --> 00:46:33,360
Speaker 1:  And that's a wrap for Verge Cast this week. Thanks for listening. If you

782
00:46:33,360 --> 00:46:36,760
Speaker 1:  enjoy the show, subscribe in the podcast app of your choice or tell a friend,

783
00:46:36,900 --> 00:46:40,880
Speaker 1:  you can send us feedback at vergecast@theverge.com. This show is

784
00:46:41,080 --> 00:46:44,680
Speaker 1:  produced by me, Liam James, and our senior audio director, Andrew Marino.

785
00:46:44,680 --> 00:46:48,320
Speaker 1:  This episode was edited and mixed by Amanda Rose

786
00:46:48,320 --> 00:46:52,120
Speaker 1:  Smith. Our editorial director is Brooke Mins and our executive producer

787
00:46:52,320 --> 00:46:56,000
Speaker 1:  is Eleanor Donovan. The Vergecast is a production of the Verge and

788
00:46:56,000 --> 00:46:59,120
Speaker 1:  Box Media Podcast network. And that's it. We'll see you next week

