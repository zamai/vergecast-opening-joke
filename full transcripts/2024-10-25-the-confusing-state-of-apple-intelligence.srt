1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 022af8bd-f7aa-4e91-8f76-fe11895784bd
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/2840723305610264508/5539584907600292/s93290-US-5458s-1729854825.mp3
Description: Nilay, David, and Richard Lawler talk about all of the coming Apple gadgets and software, from the new iPad Mini to the upcoming week of Mac announcements to the many flavors of iOS and Apple Intelligence heading to a device near you soon. Then they talk about the other news in AI, from Anthropic's new computer-using model to the growing set of lawsuits against AI companies. In the lightning round, they discuss the Boox Palma 2, T-Mobile's "lifetime" deals, and the battle over FTC's click-to-cancel rule.

2
00:00:45,335 --> 00:00:48,685
Speaker 3:  Hello and welcome to Vergecast the flagship podcast. I'm jolly scrolling.

3
00:00:49,775 --> 00:00:52,685
Speaker 3:  We're sorry. It's still there. All right, use your eyes.

4
00:00:53,585 --> 00:00:55,165
Speaker 3:  I'm your friend. Neli David Pierce is here.

5
00:00:55,505 --> 00:00:58,885
Speaker 4:  Hi. I hate that I know what jelly scrolling is. You

6
00:00:58,885 --> 00:01:00,165
Speaker 3:  Got in a lot of trouble this week, buddy.

7
00:01:00,885 --> 00:01:03,845
Speaker 4:  I did, and then I was immediately vindicated and it's been, it's been a fun

8
00:01:03,845 --> 00:01:05,805
Speaker 4:  week. I would like to, I'm excited to talk about

9
00:01:05,805 --> 00:01:08,605
Speaker 3:  This. We'll, we'll come back to this in a moment. Richard Lawler is here.

10
00:01:09,295 --> 00:01:12,885
Speaker 5:  Hello. I'm somehow unaware of the massive jelly scrolling scandal.

11
00:01:13,685 --> 00:01:16,845
Speaker 4:  Richard doesn't jelly scroll. That's one important thing to know about Richard.

12
00:01:17,185 --> 00:01:21,125
Speaker 3:  How do you know Richard doesn't pay attention? Not once to stakes free Apple

13
00:01:21,175 --> 00:01:23,765
Speaker 3:  drama. He doesn't know about Jelly scrolling.

14
00:01:25,985 --> 00:01:29,155
Speaker 3:  Welcome back Richard. By way, fan favorite Richard Lawler.

15
00:01:30,655 --> 00:01:33,235
Speaker 5:  My people, they, they made the request and we came through

16
00:01:34,685 --> 00:01:38,315
Speaker 5:  there. There will be crypto in all of your wallets. It's coming.

17
00:01:39,745 --> 00:01:42,955
Speaker 3:  Richard will give you a Bitcoin If. you vir vote for The. Vergecast

18
00:01:43,455 --> 00:01:46,755
Speaker 5:  It. It's quantum lock. So it's already there and it's coming all at the same

19
00:01:46,755 --> 00:01:48,115
Speaker 5:  time. It's, it's a, yeah, it's a thing.

20
00:01:49,195 --> 00:01:52,195
Speaker 3:  I believe it was Richard who today pointed out what was Starbucks is failing

21
00:01:52,215 --> 00:01:54,795
Speaker 3:  and some other companies failing. And you're like, these are the two companies.

22
00:01:54,795 --> 00:01:58,715
Speaker 3:  Nike, Nike, Starbucks, and Nike. Tell your theory. Say your

23
00:01:58,715 --> 00:01:59,635
Speaker 3:  theory about Starbucks and Nike.

24
00:02:00,115 --> 00:02:03,555
Speaker 5:  Starbucks and Nike. They were both out there just promoting NFTs. There were

25
00:02:03,555 --> 00:02:07,235
Speaker 5:  lots of companies and lots of brands doing NFT things a few years ago that

26
00:02:07,255 --> 00:02:10,855
Speaker 5:  now have, are acting like they didn't, but they were out there and they,

27
00:02:10,915 --> 00:02:14,775
Speaker 5:  and they were doing it big. They were really, really pushing it. And I don't

28
00:02:14,775 --> 00:02:17,455
Speaker 5:  think that they are going down now or that they're having the struggles they're

29
00:02:17,455 --> 00:02:20,815
Speaker 5:  having because they did NFTs. But I think that If, you look at the companies

30
00:02:20,815 --> 00:02:24,655
Speaker 5:  that were really into it, where the executives looked at NFTs

31
00:02:24,655 --> 00:02:28,535
Speaker 5:  and didn't immediately say that is nothing and move on. That they

32
00:02:28,535 --> 00:02:31,415
Speaker 5:  had something wrong in their boardroom that, that their executives, they,

33
00:02:31,415 --> 00:02:33,695
Speaker 5:  they weren't focused on what was going on and what their customers really

34
00:02:33,695 --> 00:02:33,935
Speaker 5:  wanted.

35
00:02:34,145 --> 00:02:37,655
Speaker 4:  Right? The NFTs aren't the, aren't the cause, but they're, they're a symptom

36
00:02:37,875 --> 00:02:40,855
Speaker 4:  of the problem that is causing all these other problems. Right?

37
00:02:40,875 --> 00:02:44,015
Speaker 3:  And the problem is you have no idea what you make or sell and you think selling

38
00:02:44,015 --> 00:02:46,135
Speaker 3:  nothing is a, is a functional business. You

39
00:02:46,135 --> 00:02:49,975
Speaker 4:  Think NFT shoes are the same as shoes? Is is, yeah.

40
00:02:50,195 --> 00:02:53,175
Speaker 3:  Should we make a coffee or the idea of coffee

41
00:02:54,275 --> 00:02:57,885
Speaker 5:  If we will sell experiences behind digital tokens?

42
00:02:58,305 --> 00:03:02,105
Speaker 3:  Hey, hey, hey, don't knock selling experiences behind digital tokens.

43
00:03:02,335 --> 00:03:05,865
Speaker 3:  What is a podcast in our RSS s feed except an

44
00:03:05,865 --> 00:03:09,185
Speaker 3:  experience behind a deeply fungible digital token. But

45
00:03:09,855 --> 00:03:12,785
Speaker 3:  nevermind, there's a lot to talk about this week.

46
00:03:14,065 --> 00:03:18,025
Speaker 3:  Quite a lot to talk about. There's David reviewed an iPad. Mini I would

47
00:03:18,025 --> 00:03:21,985
Speaker 3:  say that as someone who edited that review felt like a hostage situation

48
00:03:21,985 --> 00:03:25,505
Speaker 3:  for David. We'll talk about that. There's a bunch of other Apple

49
00:03:25,695 --> 00:03:29,425
Speaker 3:  News iOS 18.1 is coming out next week with Apple

50
00:03:29,425 --> 00:03:33,265
Speaker 3:  Intelligence, which means Apple is already started seeding iOS 18.2,

51
00:03:33,835 --> 00:03:37,625
Speaker 3:  which has the actual features in it. Very confusing. What worked through

52
00:03:37,625 --> 00:03:41,545
Speaker 3:  all of that. Then there's a week of Mac announcements to come. Greg Jozwiak

53
00:03:41,545 --> 00:03:45,465
Speaker 3:  is posting at that right now. There's a bunch of AI news as always,

54
00:03:45,465 --> 00:03:49,345
Speaker 3:  including some deeply funny humane news and then we got a lightning round.

55
00:03:49,345 --> 00:03:53,305
Speaker 3:  Alright, but let's start with the drama. Look,

56
00:03:53,305 --> 00:03:56,985
Speaker 3:  every now and again, The Verge gets itself mixed up in a bunch of high

57
00:03:57,085 --> 00:04:01,025
Speaker 3:  stakes drama, journalism ethics, what

58
00:04:01,025 --> 00:04:04,185
Speaker 3:  are we doing here? and we did that this week. And then David also pissed

59
00:04:04,185 --> 00:04:06,065
Speaker 3:  off a bunch of people by talking about the iPad.

60
00:04:07,685 --> 00:04:11,425
Speaker 3:  So the iPad Mini came out, they updated the chip. It supports Apple Intelligence

61
00:04:11,425 --> 00:04:15,305
Speaker 3:  at eight gigs of Ram. David, you reviewed it, you were disappointed.

62
00:04:15,465 --> 00:04:16,665
Speaker 3:  I There's no other way to put this.

63
00:04:16,925 --> 00:04:20,905
Speaker 4:  No, that's, that's right. Disappointed is fair. So I, I have been

64
00:04:20,905 --> 00:04:24,625
Speaker 4:  an iPad mini person forever. I think I've owned two

65
00:04:24,625 --> 00:04:28,305
Speaker 4:  different generations. I bought my wife and my dad

66
00:04:29,015 --> 00:04:32,785
Speaker 4:  iPad minis over the years. Like I, this thing is awesome and I love it and

67
00:04:32,785 --> 00:04:35,665
Speaker 4:  believe in it and have used it many times or many for many years.

68
00:04:36,875 --> 00:04:40,865
Speaker 4:  Apple shipped this one and like I, I don't know how else to

69
00:04:40,865 --> 00:04:44,505
Speaker 4:  frame it except it did the literal least it possibly could

70
00:04:45,045 --> 00:04:48,745
Speaker 4:  in a way that felt rude. Like this was not

71
00:04:49,225 --> 00:04:53,025
Speaker 4:  a case of, of just sort of chip upgrades that

72
00:04:53,025 --> 00:04:56,105
Speaker 4:  make the thing not necessarily brand new, but like

73
00:04:56,305 --> 00:05:00,265
Speaker 4:  commensurate with the times. Apple gave this a worst version of

74
00:05:00,265 --> 00:05:04,065
Speaker 4:  last year's chip and changed nothing else. And and

75
00:05:04,125 --> 00:05:07,865
Speaker 4:  you're supposed to be like, oh, thank God a new iPad mini. And the thing

76
00:05:07,865 --> 00:05:11,745
Speaker 4:  that Apple knows, and Nathan Edwards who worked with me on this

77
00:05:11,745 --> 00:05:14,785
Speaker 4:  piece a lot, the thing he kept pointing out is like Apple knows that If,

78
00:05:14,785 --> 00:05:18,065
Speaker 4:  you want an iPad mini, you're gonna buy an iPad mini because it's the iPad

79
00:05:18,065 --> 00:05:21,905
Speaker 4:  mini and you want an iPad mini and that is how Apple has

80
00:05:21,905 --> 00:05:25,705
Speaker 4:  you, it is, it is the most like self-selecting device Apple makes.

81
00:05:26,405 --> 00:05:30,385
Speaker 4:  And so why would Apple try if it doesn't have to try? It's just

82
00:05:30,385 --> 00:05:33,545
Speaker 4:  gonna do the absolute literal bare minimum.

83
00:05:34,975 --> 00:05:38,675
Speaker 4:  And just, here you go. Do you, do you want it?

84
00:05:38,675 --> 00:05:42,595
Speaker 4:  You're gonna have it if not like, and, and So, I just got to this point

85
00:05:42,595 --> 00:05:46,315
Speaker 4:  where I was like, this thing is, it's not that it's not good, it's just that

86
00:05:46,315 --> 00:05:50,235
Speaker 4:  it's not nearly as good as it could be. And Apple only had to

87
00:05:50,295 --> 00:05:54,275
Speaker 4:  try a tiny bit to make it way better. And it just

88
00:05:54,335 --> 00:05:57,205
Speaker 4:  didn't. And the thing I wrote was like, this feels like an iPad designed

89
00:05:57,265 --> 00:06:01,125
Speaker 4:  by a supply chain, not by a designer. And it,

90
00:06:01,155 --> 00:06:05,125
Speaker 4:  it's, it's just true. I cannot shake that feeling. And so it is both

91
00:06:05,245 --> 00:06:08,405
Speaker 4:  a good tablet and a deeply disappointing one, which is a very strange,

92
00:06:09,075 --> 00:06:11,965
Speaker 4:  it's a thing we deal with a lot with the iPad, honestly, frankly is like

93
00:06:12,585 --> 00:06:16,485
Speaker 4:  we want the iPad to be more than it is, but what it is is very

94
00:06:16,485 --> 00:06:19,885
Speaker 4:  good for what it is in a lot of ways. And so we, we felt this tension

95
00:06:20,195 --> 00:06:22,645
Speaker 4:  more and more. And I feel like with this one, it just tipped the other way

96
00:06:22,645 --> 00:06:25,765
Speaker 4:  for me where I was like, this is just not, it's just not a good buy. Lots

97
00:06:25,765 --> 00:06:28,605
Speaker 4:  of people are gonna buy it. Lots of people are gonna like it, but it is not

98
00:06:29,555 --> 00:06:29,845
Speaker 4:  good.

99
00:06:30,275 --> 00:06:33,245
Speaker 3:  Yeah. And it's funny 'cause you know, the old one will be around on sale

100
00:06:33,345 --> 00:06:37,215
Speaker 3:  for quite some, like in true Apple fashion you'll be able to get the old

101
00:06:37,215 --> 00:06:40,055
Speaker 3:  one for a minute right on, on some discount. Yeah.

102
00:06:40,055 --> 00:06:42,655
Speaker 4:  And the only meaningful difference is it doesn't run Apple Intelligence.

103
00:06:42,655 --> 00:06:45,375
Speaker 4:  And like, is that a meaningful difference to anyone and will it be anytime

104
00:06:45,375 --> 00:06:46,055
Speaker 4:  soon? Well,

105
00:06:46,055 --> 00:06:50,015
Speaker 3:  Especially 'cause the iPad Mini in particular is a secondary

106
00:06:50,015 --> 00:06:53,895
Speaker 3:  device. Like almost definitionally you have another thing If, you have an

107
00:06:53,895 --> 00:06:57,575
Speaker 3:  iPad mini, right? Like the iPad maybe

108
00:06:57,675 --> 00:07:01,655
Speaker 3:  can replace your laptop and maybe you want Apple Intelligence there.

109
00:07:02,815 --> 00:07:06,715
Speaker 3:  I'm, I'm struggling to make the case, but you know, maybe you do the,

110
00:07:06,715 --> 00:07:10,595
Speaker 3:  there's no one who's like, I need my notifications on my iPad mini

111
00:07:10,835 --> 00:07:11,075
Speaker 3:  summarized,

112
00:07:11,075 --> 00:07:14,715
Speaker 4:  Right? My travel laptop is my iPad Mini is not a sentence.

113
00:07:14,715 --> 00:07:18,555
Speaker 4:  People say, yeah, it's just not. And so it's like, yeah, all this stuff

114
00:07:18,555 --> 00:07:21,835
Speaker 4:  that Apple does where it's like, help me write this email is not an iPad

115
00:07:21,835 --> 00:07:25,595
Speaker 4:  mini thing. They're like really good. Siri might someday

116
00:07:25,775 --> 00:07:27,915
Speaker 4:  be, but, but again, like I,

117
00:07:28,655 --> 00:07:29,995
Speaker 3:  But you have your phone. I don't

118
00:07:30,315 --> 00:07:33,555
Speaker 4:  Think anyone should buy any Apple product on the basis of Apple Intelligence

119
00:07:33,555 --> 00:07:37,075
Speaker 4:  right now. And the iPad Mini might be the one that needs it the least

120
00:07:37,575 --> 00:07:40,075
Speaker 4:  at this moment. And so, yeah, I just got to this point where I was like,

121
00:07:40,195 --> 00:07:44,155
Speaker 4:  I, unless your iPad mini is broken and you are desperate

122
00:07:44,175 --> 00:07:47,915
Speaker 4:  for a new iPad mini, there is no reason this thing is

123
00:07:47,915 --> 00:07:51,795
Speaker 4:  compelling. And it turns out a lot of people disagreed with me about that.

124
00:07:53,015 --> 00:07:53,235
Speaker 3:  Or

125
00:07:53,785 --> 00:07:56,995
Speaker 4:  What is actually true is there are a lot of iPad mini people out there. So,

126
00:07:57,015 --> 00:08:00,275
Speaker 5:  So what's their, their case and, and I guess my question is what's the problem

127
00:08:00,275 --> 00:08:03,635
Speaker 5:  with this one? So it's using the a 17 processor from the iPhone and not the

128
00:08:03,835 --> 00:08:06,475
Speaker 5:  M four that the other tablets and the laptops use

129
00:08:06,755 --> 00:08:10,635
Speaker 3:  A 17 pro from the iPhone 15 pro, but it appears to be the, like the

130
00:08:10,635 --> 00:08:11,195
Speaker 3:  Bend version,

131
00:08:11,565 --> 00:08:14,635
Speaker 4:  Right? So it, it benchmarks slightly below

132
00:08:15,575 --> 00:08:19,195
Speaker 4:  the iPhone 15 pro and it also has one

133
00:08:19,365 --> 00:08:23,035
Speaker 4:  fewer GPU core, which in both cases suggests that it's

134
00:08:23,035 --> 00:08:26,915
Speaker 4:  basically a chip that came off the manufacturing line for whatever

135
00:08:26,915 --> 00:08:30,715
Speaker 4:  reason, not up to full capacity. And that's like a normal thing. A lot of

136
00:08:30,875 --> 00:08:34,515
Speaker 4:  companies do this, right? Like when they sell you a, a middle range

137
00:08:34,585 --> 00:08:38,275
Speaker 4:  processor, often it is a a, a sort of imperfectly

138
00:08:38,275 --> 00:08:41,075
Speaker 4:  manufactured version of a better processor. Like that's a super normal thing.

139
00:08:41,075 --> 00:08:44,915
Speaker 4:  People do it, it's fine, but it's just like this, this brand

140
00:08:44,975 --> 00:08:48,675
Speaker 4:  new product is giving you a worse version of last year's chip

141
00:08:48,975 --> 00:08:52,595
Speaker 4:  in a time when Apple is like a telling you that you need all the performance

142
00:08:52,595 --> 00:08:56,205
Speaker 4:  you can possibly get in order to run Apple Intelligence has massively upgraded

143
00:08:56,205 --> 00:08:59,325
Speaker 4:  a bunch of its other iPads. There's just no way to look at this other than

144
00:08:59,385 --> 00:09:01,765
Speaker 4:  it got the worst of the worst.

145
00:09:02,555 --> 00:09:06,445
Speaker 3:  Like literally the part spin. Yeah. Like it's a Chevy with a Cadillac logo

146
00:09:06,465 --> 00:09:06,805
Speaker 3:  on it.

147
00:09:07,205 --> 00:09:08,205
Speaker 4:  Exactly. The

148
00:09:08,365 --> 00:09:12,085
Speaker 3:  Pontiac iPad is just fine. Yeah. So we should talk about the jelly

149
00:09:12,085 --> 00:09:15,685
Speaker 3:  scrolling because I think if people understand the iPad mini got a chip bump

150
00:09:15,685 --> 00:09:19,125
Speaker 3:  and nothing is very compelling and that's why And the iPad Mini has effectively

151
00:09:19,125 --> 00:09:22,925
Speaker 3:  no competition. There's not some like eight inch Android tablet that's

152
00:09:23,005 --> 00:09:26,965
Speaker 3:  crushing it. Right. And the iPad mini has for whatever it's worth

153
00:09:26,965 --> 00:09:30,885
Speaker 3:  the iPad app library, which doesn't exist on the

154
00:09:30,885 --> 00:09:31,565
Speaker 3:  Android side. Correct.

155
00:09:31,675 --> 00:09:33,725
Speaker 4:  Also, pilots use it. I dunno, If you knew that

156
00:09:33,865 --> 00:09:35,205
Speaker 3:  The pilots are outta control

157
00:09:35,825 --> 00:09:39,805
Speaker 4:  The pilots and apparently there are like hundreds of millions of pilots out

158
00:09:39,805 --> 00:09:42,525
Speaker 3:  There flying planes directly to Apple stores. Yeah,

159
00:09:42,555 --> 00:09:43,045
Speaker 4:  Exactly.

160
00:09:43,715 --> 00:09:46,525
Speaker 3:  Like landing right on Michigan Avenue and go

161
00:09:46,755 --> 00:09:49,805
Speaker 4:  Outside of the Fifth Avenue store instead of people. It's just a bunch of

162
00:09:49,805 --> 00:09:52,325
Speaker 4:  tiny planes lining fifth Avenue to get to

163
00:09:52,325 --> 00:09:55,645
Speaker 3:  The store. Right? Right. In Lake Michigan in the pontoons

164
00:09:56,365 --> 00:09:57,725
Speaker 3:  swimming up to the Apple store.

165
00:09:59,515 --> 00:10:03,485
Speaker 3:  Okay, don't do that. If you're listening to this and you're like, I should

166
00:10:03,825 --> 00:10:07,125
Speaker 3:  fly my float plane to the Chicago Apple store, I'm just

167
00:10:07,725 --> 00:10:08,845
Speaker 3:  encouraging you not to do that If,

168
00:10:08,845 --> 00:10:11,605
Speaker 4:  You do send us a video because we will put it on our website.

169
00:10:12,585 --> 00:10:16,565
Speaker 3:  That's, that's the opposite of not showing the person running under the field

170
00:10:16,565 --> 00:10:17,565
Speaker 3:  during the football game.

171
00:10:19,965 --> 00:10:23,165
Speaker 3:  Opposite incentive. All right, so there's whatever, this is a spec bump.

172
00:10:23,305 --> 00:10:26,885
Speaker 3:  The thing that got people really lit up is not you dare to criticize an Apple

173
00:10:26,885 --> 00:10:30,805
Speaker 3:  product. It's that the screen on

174
00:10:30,805 --> 00:10:34,485
Speaker 3:  the iPad mini is notorious for jelly scrolling. Yes. Which is when you

175
00:10:34,485 --> 00:10:38,445
Speaker 3:  scroll, one side of the screen moves faster than the other, which

176
00:10:38,575 --> 00:10:41,325
Speaker 3:  warps everything on the screen. This is one of those things where some people

177
00:10:41,325 --> 00:10:45,205
Speaker 3:  can see it and some people can't. But unlike 120 hertz

178
00:10:45,205 --> 00:10:49,125
Speaker 3:  refresh, like once you see it, you're done. You're hooped a

179
00:10:49,125 --> 00:10:51,525
Speaker 3:  hundred percent. Like it's, it's never, it's like motion smoothing. It's

180
00:10:51,525 --> 00:10:53,885
Speaker 3:  like you can ruin someone's life by pointing this out.

181
00:10:54,105 --> 00:10:57,965
Speaker 4:  And do you know who ruined my life? Dieter Bone. Thank you Dieter

182
00:10:57,995 --> 00:11:01,205
Speaker 4:  with his 2021 review of the iPad Mini

183
00:11:01,845 --> 00:11:04,525
Speaker 4:  in which he spent a long time talking about how bad the jelly scrolling is.

184
00:11:05,105 --> 00:11:08,885
Speaker 4:  And I think he, he either there or somewhere else linked to a test

185
00:11:08,905 --> 00:11:11,965
Speaker 4:  you can do to test for jelly scrolling that actually it just basically moves

186
00:11:12,565 --> 00:11:16,405
Speaker 4:  a thing across the screen so you can see it and it's a line. And the

187
00:11:16,405 --> 00:11:19,885
Speaker 4:  idea is if the line appears to bend, it means the screen is not refreshing

188
00:11:20,065 --> 00:11:23,805
Speaker 4:  in a uniform way. And once you see it,

189
00:11:24,275 --> 00:11:28,045
Speaker 4:  holy Lord, can you not unsee it? Yeah. And you can, you can super see it

190
00:11:28,425 --> 00:11:32,365
Speaker 4:  on the, the 2021 mini. So yeah, the here are several things that I

191
00:11:32,365 --> 00:11:36,245
Speaker 4:  know. I know that jelly scrolling was a big problem on the 2021

192
00:11:36,245 --> 00:11:40,165
Speaker 4:  mini. I know that Apple said in 2021, it's not a real

193
00:11:40,165 --> 00:11:44,115
Speaker 4:  problem, don't worry about it. Everything's fine. That's you. You can go

194
00:11:44,115 --> 00:11:46,595
Speaker 4:  back and find that those are statements Apple gave that was like, yeah, this

195
00:11:46,595 --> 00:11:50,395
Speaker 4:  is a normal thing that happens on screens, don't worry about it. Like

196
00:11:51,335 --> 00:11:55,325
Speaker 4:  bad response. But that was the response. I also know that Apple this

197
00:11:55,325 --> 00:11:59,245
Speaker 4:  year tried to fix it. They changed the way the display controller works

198
00:11:59,385 --> 00:12:02,445
Speaker 4:  in order to ostensibly refresh the screen more uniformly

199
00:12:03,395 --> 00:12:06,725
Speaker 4:  this one is better. It, it is certainly less

200
00:12:07,095 --> 00:12:10,965
Speaker 4:  jelly than the 2021 model I sat. I mean

201
00:12:11,005 --> 00:12:14,805
Speaker 4:  I like, so our, our reviews all went up at nine and at like

202
00:12:14,805 --> 00:12:18,605
Speaker 4:  nine 30, a bunch of like the, the Apple blogs and stuff do the

203
00:12:18,605 --> 00:12:22,445
Speaker 4:  roundups of reviews and because the jelly scrolling was a tiny low

204
00:12:22,445 --> 00:12:26,045
Speaker 4:  stakes, you know, controversy in 2021.

205
00:12:26,795 --> 00:12:30,685
Speaker 4:  They all are talking about the jelly scrolling. I was the only reviewer of

206
00:12:30,685 --> 00:12:33,765
Speaker 4:  that early batch that said jelly scrolling still existed.

207
00:12:34,785 --> 00:12:37,725
Speaker 4:  And these, it's a bunch of other like good smart reviewers, people whose

208
00:12:37,725 --> 00:12:40,045
Speaker 4:  work that I like. So I was like, oh my god, did I get this wrong? So I went

209
00:12:40,245 --> 00:12:42,645
Speaker 4:  upstairs and got Anna's 2021 mini

210
00:12:43,795 --> 00:12:47,325
Speaker 4:  grabbed the 2024 mini and sat here for, I don't know, 90 minutes just

211
00:12:47,325 --> 00:12:50,645
Speaker 4:  scrolling, scrolling everything I could find up and down. Do you believe

212
00:12:50,645 --> 00:12:51,125
Speaker 3:  Your own eyes?

213
00:12:51,385 --> 00:12:54,885
Speaker 4:  That's the like I didn't for like an hour and a half. And then even our own

214
00:12:54,885 --> 00:12:58,245
Speaker 4:  staff, I think it was Chris Welch who posted a link and was like, everybody

215
00:12:58,245 --> 00:13:00,525
Speaker 4:  else disagrees with you, David, are you sure you're right? And I was like,

216
00:13:00,545 --> 00:13:01,765
Speaker 4:  no I'm not.

217
00:13:03,345 --> 00:13:06,125
Speaker 3:  That's that's one of the most in character Chris Welch things I've ever

218
00:13:06,125 --> 00:13:09,685
Speaker 4:  Heard. Yeah. Oh, for sure. Yeah. Yeah. And it's like typically when I disagree

219
00:13:09,685 --> 00:13:13,205
Speaker 4:  with like Federico vii, it's like I should internally really think through

220
00:13:13,275 --> 00:13:17,165
Speaker 4:  life, but I went and looked and

221
00:13:17,165 --> 00:13:21,045
Speaker 4:  like it is, it is better than it was in 2021. Like you can really

222
00:13:21,785 --> 00:13:25,645
Speaker 4:  see it's, it's like the, the 2021 screen is like

223
00:13:25,765 --> 00:13:29,325
Speaker 4:  a ship on the ocean. Yes. Right. It's like rolling waves as you scroll. The

224
00:13:29,325 --> 00:13:32,965
Speaker 4:  new one is better. It is, it is meaningfully better. It's still Jelly

225
00:13:32,995 --> 00:13:36,405
Speaker 4:  Scrolls. Like, and, and the way I think about it is like the, the easiest

226
00:13:36,585 --> 00:13:40,445
Speaker 4:  one to see for me was you go to the Amazon homepage and it's a bunch of

227
00:13:40,815 --> 00:13:44,645
Speaker 4:  white boxes with like images and texts and seven side

228
00:13:44,825 --> 00:13:48,565
Speaker 4:  and If. you scroll quickly, the right side of the box appears to pull down

229
00:13:48,585 --> 00:13:52,565
Speaker 4:  and then up faster than the left side. So it just, it just every,

230
00:13:52,745 --> 00:13:56,405
Speaker 4:  and then it catches up and it, but it's like you're, you're turning a rectangle

231
00:13:56,515 --> 00:13:59,445
Speaker 4:  into a parallelogram and then back into a rectangle as you scroll. And it's

232
00:13:59,445 --> 00:14:03,205
Speaker 4:  not horrible. Like on the 20 21 1, Anna has

233
00:14:03,325 --> 00:14:05,885
Speaker 4:  never looked at me and been like, is this scrolling weird? Like it's not

234
00:14:05,975 --> 00:14:06,325
Speaker 4:  never

235
00:14:06,325 --> 00:14:06,605
Speaker 3:  Tell

236
00:14:06,605 --> 00:14:09,925
Speaker 4:  Her I'm not going to. So thi this is the thing, right? It's like it's not.

237
00:14:10,305 --> 00:14:12,605
Speaker 4:  And even a lot of the reviewers were like, I didn't really notice it then

238
00:14:12,605 --> 00:14:16,525
Speaker 4:  and I definitely don't notice it now. And that's a fine and BI think representative

239
00:14:16,585 --> 00:14:20,125
Speaker 4:  of just about everybody's experience. But once you see it, you can't unsee

240
00:14:20,125 --> 00:14:23,925
Speaker 4:  it and it is there. And the great thing for me was our reviews ran

241
00:14:23,925 --> 00:14:27,805
Speaker 4:  on Tuesday. The things started, shit, I, I got 24 hours of people

242
00:14:27,805 --> 00:14:31,525
Speaker 4:  being like, you're a monster who has an agenda against Apple or

243
00:14:31,665 --> 00:14:34,805
Speaker 4:  for jelly scrolling or something. Yes. And then 24 hours later a bunch of

244
00:14:34,805 --> 00:14:37,165
Speaker 4:  people who reluctantly were like, oh no, he's right. There's some jelly scrolling.

245
00:14:37,635 --> 00:14:41,085
Speaker 4:  Some people still don't notice it, some people see it. I am

246
00:14:41,485 --> 00:14:45,405
Speaker 4:  absolutely confident it's there. But it's also fine. Like don't not

247
00:14:45,465 --> 00:14:47,445
Speaker 4:  buy it because of the jelly scrolling. Well

248
00:14:47,445 --> 00:14:47,925
Speaker 3:  You have no choice.

249
00:14:48,265 --> 00:14:50,405
Speaker 4:  It doesn't make it better or worse. But also you have no choice.

250
00:14:50,685 --> 00:14:53,245
Speaker 3:  If you're a pilot, you have no choice. You're like, this is what I, this

251
00:14:53,245 --> 00:14:55,445
Speaker 3:  is who I am now I have to see this parallelogram.

252
00:14:55,725 --> 00:14:59,485
Speaker 4:  I think If, you have the thing strapped to your thigh jelly scrolling is

253
00:14:59,485 --> 00:15:02,325
Speaker 4:  not going to be a huge issue. It's just a thought.

254
00:15:02,705 --> 00:15:06,445
Speaker 3:  That's a sentence that only makes sense in the context of aviation and almost

255
00:15:06,445 --> 00:15:07,285
Speaker 3:  in no other context.

256
00:15:09,315 --> 00:15:11,845
Speaker 3:  Yeah. Like as you walk about the subway in New York City,

257
00:15:13,735 --> 00:15:16,525
Speaker 3:  you'd be like, jelly scrolling is an issue. Would you like to scroll up my

258
00:15:16,525 --> 00:15:16,725
Speaker 3:  thigh?

259
00:15:17,115 --> 00:15:19,605
Speaker 4:  Yeah. It's too big for a pocket. So you just kind of wrap it around your

260
00:15:19,605 --> 00:15:19,805
Speaker 4:  thigh.

261
00:15:20,035 --> 00:15:22,605
Speaker 3:  That experience of writing the review that everyone disagrees with and then

262
00:15:22,605 --> 00:15:26,485
Speaker 3:  being fully vindicated by it much later. The longer it

263
00:15:26,485 --> 00:15:30,045
Speaker 3:  takes, the better it is. I'm just telling you like you had 24 hours,

264
00:15:30,665 --> 00:15:34,205
Speaker 3:  I'm, I'm reading they cut Vision Pro production this week and I'm like

265
00:15:34,595 --> 00:15:35,245
Speaker 3:  told you so

266
00:15:36,835 --> 00:15:40,805
Speaker 3:  yeah. So that's the iPad mini. You get one scroll away. Let us know

267
00:15:40,805 --> 00:15:44,005
Speaker 3:  what you think if, if like motion smoothing, it's the thing you start to

268
00:15:44,005 --> 00:15:46,405
Speaker 3:  see and then you blame us forever. That's not our fault.

269
00:15:46,865 --> 00:15:50,485
Speaker 4:  If, you don't see jelly scrolling. Don't run the test and

270
00:15:50,595 --> 00:15:54,285
Speaker 4:  just believe that I'm wrong. Like truly your life will be better If. you

271
00:15:54,285 --> 00:15:57,245
Speaker 4:  just send me a mean message about how I'm wrong and then move on with your

272
00:15:57,245 --> 00:15:57,685
Speaker 4:  life. Yeah.

273
00:15:57,685 --> 00:16:00,965
Speaker 3:  There's nothing worse you can do for yourself than learn how to evaluate

274
00:16:00,995 --> 00:16:01,485
Speaker 3:  screens.

275
00:16:01,705 --> 00:16:04,245
Speaker 4:  That's really true. This is why Neli has been so ruined.

276
00:16:05,265 --> 00:16:06,285
Speaker 3:  Why am I a cranky monster?

277
00:16:06,425 --> 00:16:08,765
Speaker 4:  And Richard, for that matter, both of you are pixel, like pixel true pixel

278
00:16:08,765 --> 00:16:09,485
Speaker 4:  monster. You

279
00:16:09,485 --> 00:16:13,045
Speaker 3:  Should not do it. Yeah, yeah. It's why are Richard and I trauma

280
00:16:13,145 --> 00:16:16,965
Speaker 3:  bonded for life? We can look at screens.

281
00:16:16,965 --> 00:16:20,325
Speaker 3:  Okay, so that's the, I've had many, we can set that aside. Although please

282
00:16:20,605 --> 00:16:23,085
Speaker 3:  continue sending David notes about the words jelly scrolling. That's very

283
00:16:23,085 --> 00:16:27,005
Speaker 3:  good. Then there's what's going on with like this next week of

284
00:16:27,165 --> 00:16:31,005
Speaker 3:  announcements to come, which it seems like Apple has

285
00:16:31,005 --> 00:16:34,845
Speaker 3:  very purposefully lined up a bunch of things in a way

286
00:16:34,845 --> 00:16:38,485
Speaker 3:  that I, I don't wanna say it's confusing, it's just noisy,

287
00:16:39,415 --> 00:16:43,285
Speaker 3:  right? Like there's just a lot of Apple noise coming next week. There's Apple

288
00:16:43,285 --> 00:16:46,845
Speaker 3:  intelligence launching, there's a week of Mac announcements.

289
00:16:47,845 --> 00:16:51,805
Speaker 3:  18.2 is in developer beta. I If you

290
00:16:51,805 --> 00:16:54,805
Speaker 3:  were a casual, I don't know that you would understand what shipping, what's

291
00:16:54,805 --> 00:16:58,125
Speaker 3:  not, what works with what, right? Because there's just gonna be this like

292
00:16:58,215 --> 00:17:02,205
Speaker 3:  flood of stuff and in particular 18.1 is coming

293
00:17:02,205 --> 00:17:06,005
Speaker 3:  to the phone. So now Apple intelligence really is here. But then

294
00:17:06,005 --> 00:17:09,765
Speaker 3:  you're gonna see a bunch of demos of like Gen Moji, which is not only

295
00:17:09,785 --> 00:17:13,445
Speaker 3:  in the developer beta for 18.2, which most people can't get, but on a wait

296
00:17:13,445 --> 00:17:16,525
Speaker 3:  list inside that developer beta. Can you peel this apart for me David?

297
00:17:17,865 --> 00:17:18,085
Speaker 4:  No.

298
00:17:19,945 --> 00:17:23,805
Speaker 4:  And I, I honestly I have, I I think that's on purpose, right? Like

299
00:17:23,805 --> 00:17:27,525
Speaker 4:  every time I watch an ad for the iPhone 16

300
00:17:27,665 --> 00:17:30,565
Speaker 4:  and it's running Apple intelligence, I wanna like throw things at my tv.

301
00:17:30,795 --> 00:17:34,365
Speaker 4:  It's so just disingenuous at this point to sell this stuff

302
00:17:34,575 --> 00:17:38,365
Speaker 4:  based on, look, here's a Siri feature you can't use. But anyway,

303
00:17:38,465 --> 00:17:42,295
Speaker 4:  So I think just to like run down the list of

304
00:17:42,295 --> 00:17:44,855
Speaker 4:  stuff. And Richard, you should tell me what I'm missing because I'm, I'm

305
00:17:44,855 --> 00:17:48,695
Speaker 4:  pretty sure I'm missing stuff. iOS 18.1 is

306
00:17:48,695 --> 00:17:52,565
Speaker 4:  launching for real like out of beta and onto people's phones

307
00:17:52,905 --> 00:17:56,525
Speaker 4:  on Monday. Yep. That's the very first run of Apple

308
00:17:56,525 --> 00:18:00,205
Speaker 4:  intelligence stuff. Some of the like help me write stuff, some of the

309
00:18:00,355 --> 00:18:03,285
Speaker 4:  message summaries which appear to mostly just be funny for

310
00:18:03,795 --> 00:18:07,725
Speaker 4:  hilarious reasons and not actually useful in anyone's life. Did

311
00:18:07,845 --> 00:18:10,565
Speaker 3:  You see the one that's ring notifications and it was summarized as five to

312
00:18:10,565 --> 00:18:14,005
Speaker 3:  10 people are at your door easily The best one.

313
00:18:14,465 --> 00:18:15,245
Speaker 4:  That's very good.

314
00:18:15,905 --> 00:18:16,645
Speaker 3:  That's very good

315
00:18:16,935 --> 00:18:19,405
Speaker 4:  There. Yeah, there's one running around that I think is like, I think it's

316
00:18:19,405 --> 00:18:23,045
Speaker 4:  a, it's a joke but it's like it's a text from somebody that's like very busy

317
00:18:23,045 --> 00:18:26,885
Speaker 4:  right now. Semicolon not ready for a relationship and it's like well I

318
00:18:26,885 --> 00:18:30,765
Speaker 4:  got dumped I guess but so 18.1 is coming

319
00:18:31,105 --> 00:18:35,085
Speaker 4:  but 18.2, which as you mentioned I think is gonna be a much bigger change

320
00:18:35,145 --> 00:18:39,005
Speaker 4:  for some reasons we should get into is in developer beta now.

321
00:18:39,005 --> 00:18:41,925
Speaker 4:  Which typically means we're somewhere between a few weeks and a couple of

322
00:18:41,925 --> 00:18:45,885
Speaker 4:  months from shipping for real. I don't know if Apple has promised when that's

323
00:18:45,965 --> 00:18:48,725
Speaker 4:  supposed to be coming. I've lost track of that timeline but it's basically

324
00:18:48,725 --> 00:18:52,245
Speaker 4:  all supposed to happen somewhere between right now and like spring. So

325
00:18:53,075 --> 00:18:53,895
Speaker 4:  do it that which you will

326
00:18:54,115 --> 00:18:58,095
Speaker 3:  But 18.2 seems like the real iOS 18.

327
00:18:58,515 --> 00:19:01,575
Speaker 3:  That's what I that that's what I'm getting about by how noisy next week is

328
00:19:01,575 --> 00:19:05,335
Speaker 3:  going to be. We don't know what week of Mac news is coming.

329
00:19:05,395 --> 00:19:08,735
Speaker 3:  We can guess right? There's been a lot of leaks. I think we're gonna get

330
00:19:09,455 --> 00:19:13,375
Speaker 3:  M four MacBook Pros. I think we're gonna get this new Mac mini that's

331
00:19:13,375 --> 00:19:16,255
Speaker 3:  in a much smaller case that looks like an Apple TV

332
00:19:17,255 --> 00:19:21,105
Speaker 3:  that maybe some other Macs but like maybe the Smac the people

333
00:19:21,105 --> 00:19:24,225
Speaker 3:  have been talking about, I don't know but like so there's a week of Mac news

334
00:19:24,225 --> 00:19:27,225
Speaker 3:  to come. So a bunch of Apple noise

335
00:19:27,995 --> 00:19:31,225
Speaker 3:  which is great. I'm actually, if they do a Apple, if you're listening If

336
00:19:31,225 --> 00:19:34,385
Speaker 3:  you do a 32 and chime mac, I'll buy that thing tomorrow. Just saying jaws.

337
00:19:34,565 --> 00:19:38,105
Speaker 3:  So that's great. So like dead ahead Mac News there are New Macs, they look

338
00:19:38,105 --> 00:19:41,585
Speaker 3:  different, at least in the case of Mac Mini we'll generate some headlines

339
00:19:41,905 --> 00:19:45,745
Speaker 3:  then on Monday you get iOS 18.1 is here, apple intelligence

340
00:19:45,765 --> 00:19:49,745
Speaker 3:  is here and then what actually people will have on their phones

341
00:19:50,165 --> 00:19:54,125
Speaker 3:  is almost nothing, right? Like truly almost nothing.

342
00:19:55,075 --> 00:19:58,565
Speaker 3:  It's message summaries. A little bit of help me write, it's a new

343
00:19:58,795 --> 00:20:02,725
Speaker 3:  Siri animation that isn't connected to an actual new Siri. Weird.

344
00:20:03,195 --> 00:20:07,005
Speaker 3:  Yeah that's just weird all the way around. Yes. But then what people

345
00:20:07,115 --> 00:20:10,805
Speaker 3:  also see if they go to any tech website including our own, is a bunch of

346
00:20:11,005 --> 00:20:14,805
Speaker 3:  coverage of 18.2 which is now out which has the features people want and

347
00:20:14,805 --> 00:20:18,605
Speaker 3:  not just the AI features. People want like big features, people want like

348
00:20:18,755 --> 00:20:22,685
Speaker 3:  alternative browsers set as default alternative message clients

349
00:20:22,785 --> 00:20:26,085
Speaker 3:  set as default, other browser engines being able to set

350
00:20:26,785 --> 00:20:30,605
Speaker 3:  web apps on the home screen in the eu like big sweeping changes

351
00:20:30,745 --> 00:20:34,645
Speaker 3:  to iOS 18. And then also the the good

352
00:20:35,095 --> 00:20:38,845
Speaker 3:  Apple intelligence features like visual intelligence in Gen

353
00:20:39,035 --> 00:20:42,885
Speaker 3:  Moji like things at chat GPT integration,

354
00:20:43,545 --> 00:20:47,165
Speaker 3:  the stuff that people thought would be there from the beginning. So I. That

355
00:20:47,165 --> 00:20:49,765
Speaker 3:  just feels like a lot of stuff to parse out at once.

356
00:20:50,505 --> 00:20:54,485
Speaker 4:  My ongoing theory about Apple intelligence is that the only

357
00:20:54,485 --> 00:20:58,445
Speaker 4:  three features most people will like think are

358
00:20:58,445 --> 00:21:02,285
Speaker 4:  cool and get excited about are gen moji. Which I

359
00:21:02,405 --> 00:21:06,045
Speaker 4:  actually think is gonna be a big deal. I think like it might be

360
00:21:06,425 --> 00:21:10,165
Speaker 4:  sticky but I think a lot of people are going use it in the way that like,

361
00:21:10,525 --> 00:21:14,405
Speaker 4:  I dunno if this has been y'all's experience but like tap back emoji now are

362
00:21:14,405 --> 00:21:17,605
Speaker 4:  like completely normalized in my life. Like the every, everyone I text with

363
00:21:17,755 --> 00:21:21,445
Speaker 4:  uses the tap backs now, which I think is awesome. And like that is

364
00:21:21,545 --> 00:21:25,445
Speaker 4:  the gen moji is like the next step to that, which I think is

365
00:21:25,445 --> 00:21:26,045
Speaker 4:  gonna be cool. By

366
00:21:26,045 --> 00:21:28,605
Speaker 3:  The way, can I just say I desperately wish email had tap backs?

367
00:21:28,905 --> 00:21:32,605
Speaker 4:  Yes it is. It is such good messaging

368
00:21:32,745 --> 00:21:36,725
Speaker 4:  ui like bring it everywhere. Let me like an email like and let them

369
00:21:36,785 --> 00:21:39,965
Speaker 4:  see that I liked the email So I can just be like, yep, I read it. Please

370
00:21:39,965 --> 00:21:41,565
Speaker 4:  don't follow up again. Like we're

371
00:21:41,565 --> 00:21:42,965
Speaker 3:  Good when If you ever liked an email?

372
00:21:44,465 --> 00:21:45,005
Speaker 4:  That's fair.

373
00:21:45,075 --> 00:21:45,445
Speaker 3:  Well no

374
00:21:45,865 --> 00:21:46,805
Speaker 4:  Thumbs down an email.

375
00:21:47,385 --> 00:21:50,085
Speaker 3:  You know everybody watched the show The Bear and then everyone went through

376
00:21:50,085 --> 00:21:53,365
Speaker 3:  the phase where they talked like a Chicago line cook for like a summer and

377
00:21:53,775 --> 00:21:57,365
Speaker 3:  heard the word heard is just a IRL tap back.

378
00:21:57,545 --> 00:22:01,445
Speaker 3:  That's all it is. Yeah. And that's all I want do with most of my email

379
00:22:02,305 --> 00:22:06,245
Speaker 3:  is just signify receipt. Yep. Yeah, I I see what you

380
00:22:06,245 --> 00:22:08,125
Speaker 3:  said here. I have no further reactions.

381
00:22:09,985 --> 00:22:13,805
Speaker 3:  That's it. That's nothing. You are getting nothing else from me.

382
00:22:13,875 --> 00:22:16,725
Speaker 4:  What is the emoji for that? Is it the, is it the face that's not smiling

383
00:22:16,785 --> 00:22:18,245
Speaker 4:  or frowning? It's just the, the

384
00:22:18,245 --> 00:22:20,245
Speaker 3:  Co neutral face That's that face is theater. Mm.

385
00:22:22,825 --> 00:22:25,565
Speaker 4:  So then what's the, what's the, is it just a, is it just a here by

386
00:22:25,565 --> 00:22:28,605
Speaker 3:  The way that's true that he sent me that emoji more than anyone else in my

387
00:22:28,605 --> 00:22:29,205
Speaker 3:  entire life.

388
00:22:31,895 --> 00:22:34,715
Speaker 3:  No, I don't think there is one. You just need a like a check.

389
00:22:35,815 --> 00:22:36,635
Speaker 3:  That's the one I'm

390
00:22:36,635 --> 00:22:39,275
Speaker 4:  Check is good. I use the raise hands emoji for that a lot.

391
00:22:39,695 --> 00:22:42,995
Speaker 3:  That's horrible. That's like a celebration.

392
00:22:43,655 --> 00:22:47,075
Speaker 3:  No, no, no that's good. That's what it's for. Yeah, the raised hands.

393
00:22:47,075 --> 00:22:49,995
Speaker 4:  Hands raised hands can be like oh thank you. Or it can just be like, got

394
00:22:49,995 --> 00:22:51,795
Speaker 4:  it. Or it can just be like sick.

395
00:22:52,175 --> 00:22:52,915
Speaker 3:  I'm under arrest.

396
00:22:53,225 --> 00:22:56,715
Speaker 4:  It's just a, it's a sort of, I'm under arrest. It's just, it's just

397
00:22:56,715 --> 00:22:58,275
Speaker 4:  acknowledgement. That's all it is.

398
00:22:58,735 --> 00:23:02,715
Speaker 3:  If, you think raised hands is, I'm so confused. A lot of our conver,

399
00:23:02,795 --> 00:23:04,355
Speaker 3:  I have to rethink a lot of our conversations.

400
00:23:05,215 --> 00:23:07,355
Speaker 4:  Do you think? I'm telling you, I'm being arrested every time.

401
00:23:10,475 --> 00:23:14,135
Speaker 3:  All right so Jen Moji tap backs of being arrested are coming to iOS 18.2.

402
00:23:16,785 --> 00:23:17,525
Speaker 3:  That's where we're getting.

403
00:23:17,785 --> 00:23:21,045
Speaker 4:  But yeah, anyway, so Gen Moji is, is the first thing I think people are gonna

404
00:23:21,045 --> 00:23:25,005
Speaker 4:  care about. That is actually like a net new thing. The second one is chat

405
00:23:25,125 --> 00:23:29,005
Speaker 4:  GPT, which I think it remains to be seen if that's actually going

406
00:23:29,005 --> 00:23:32,925
Speaker 4:  to really be baked into iOS in a way that people care about right now.

407
00:23:32,925 --> 00:23:36,725
Speaker 4:  It seems kind of clunky but it is a thing people will like go find and do

408
00:23:36,725 --> 00:23:39,605
Speaker 4:  on purpose. Yeah. And then the third one is visual intelligence, especially

409
00:23:39,605 --> 00:23:43,285
Speaker 4:  because of the camera control. So I, I totally agree with you that like the

410
00:23:43,285 --> 00:23:46,945
Speaker 4:  first sincerely new Apple intelligence things

411
00:23:47,485 --> 00:23:51,105
Speaker 4:  are all in 18.2 and they're also

412
00:23:51,325 --> 00:23:55,265
Speaker 4:  pretty big complicated things. So I would not be shocked to see this in a

413
00:23:55,285 --> 00:23:59,145
Speaker 4:  dev beta longer than some features we've seen from iOS before.

414
00:23:59,325 --> 00:24:02,825
Speaker 3:  But also the part where 18.2 everywhere

415
00:24:03,175 --> 00:24:07,145
Speaker 3:  including the United States, lets you set default phone and messaging apps

416
00:24:07,645 --> 00:24:11,505
Speaker 3:  is a big change to iOS. And that's 18.2, the part of

417
00:24:11,665 --> 00:24:15,385
Speaker 3:  18.2 where the beginnings of different browser

418
00:24:15,495 --> 00:24:19,385
Speaker 3:  engines in the EU huge like earth shattering change

419
00:24:19,405 --> 00:24:21,825
Speaker 3:  to iOS 18.2. Richard,

420
00:24:21,855 --> 00:24:24,505
Speaker 4:  What do you know about how this is actually gonna work? Because I'm skeptical

421
00:24:24,525 --> 00:24:25,865
Speaker 4:  of this being actually a big deal

422
00:24:26,445 --> 00:24:28,625
Speaker 5:  In terms of the browser changes and the default changing.

423
00:24:28,855 --> 00:24:32,585
Speaker 4:  Yeah. Like what does that mean when you do that? Do we know anything? The

424
00:24:32,585 --> 00:24:35,745
Speaker 5:  Way that I am reading it is that you will actually be able to have a different

425
00:24:35,745 --> 00:24:39,345
Speaker 5:  browser and engine and because what what we've had so far is people, you

426
00:24:39,345 --> 00:24:41,945
Speaker 5:  can have Chrome on your iPhone, right? But it's still safari underneath,

427
00:24:41,945 --> 00:24:45,145
Speaker 5:  right? It just looks a little different. But you can a If, you can actually

428
00:24:45,255 --> 00:24:48,545
Speaker 5:  have your own your own browsing engine. You can have a lot more features.

429
00:24:48,545 --> 00:24:52,305
Speaker 5:  You can enable a lot more things than they, they are able to right now. Right

430
00:24:52,305 --> 00:24:55,265
Speaker 5:  now they're limited by what Apple wants to do. I think that those are things

431
00:24:55,265 --> 00:24:57,545
Speaker 5:  that, that they will be massive as they're saying they lot. But

432
00:24:59,115 --> 00:25:02,375
Speaker 5:  the things that people might notice, I know at least in our newsroom, Jen

433
00:25:02,375 --> 00:25:05,735
Speaker 5:  was excited seeing this announcement about the mail app having categories

434
00:25:06,045 --> 00:25:07,135
Speaker 5:  like Gmail has had forever.

435
00:25:07,535 --> 00:25:07,735
Speaker 3:  Yeah

436
00:25:08,125 --> 00:25:11,015
Speaker 5:  Like there's on, we've seen the on device AI scanning, they talked about

437
00:25:11,015 --> 00:25:13,935
Speaker 5:  how that's gonna do it. Child safety thing and iMessage. I think there are

438
00:25:13,935 --> 00:25:17,335
Speaker 5:  gonna be a lot more like kind of small features of things that it does on

439
00:25:17,335 --> 00:25:21,215
Speaker 5:  the device now that just haven't been there. And those may be bigger

440
00:25:21,245 --> 00:25:24,535
Speaker 5:  than any of the Apple intelligence features right away.

441
00:25:24,885 --> 00:25:28,775
Speaker 3:  Yeah, I think upgrading the apps people use, yeah that's always

442
00:25:28,775 --> 00:25:32,655
Speaker 3:  better than use a new thing, right? Like If, you use mail every day

443
00:25:32,655 --> 00:25:36,455
Speaker 3:  and now mail has new features, you'll be happy. I'm just getting at the idea

444
00:25:36,455 --> 00:25:40,355
Speaker 3:  that iOS 18.2 is gonna fast follow

445
00:25:40,635 --> 00:25:44,435
Speaker 3:  18.1 when it is actually full of huge changes. Yeah. To how

446
00:25:44,615 --> 00:25:48,195
Speaker 3:  iOS works and Apple Intelligence, the the first

447
00:25:48,425 --> 00:25:51,955
Speaker 3:  real set of Apple intelligence features, I just suspect that one's gonna

448
00:25:51,955 --> 00:25:55,115
Speaker 3:  be longer in the future. And then we're gonna talk about AI agents and stuff

449
00:25:55,115 --> 00:25:59,075
Speaker 3:  in the second section 'cause there's a bunch of that news. But you

450
00:25:59,075 --> 00:26:02,515
Speaker 3:  know, the real promise of Apple intelligence is a sir that is actually smart

451
00:26:02,515 --> 00:26:06,075
Speaker 3:  and can do things for you. And that is a million years away from what I can

452
00:26:06,075 --> 00:26:06,235
Speaker 3:  tell.

453
00:26:07,095 --> 00:26:10,635
Speaker 4:  Yes it is not. It is not here. We can say that with great

454
00:26:10,635 --> 00:26:14,595
Speaker 4:  confidence but the thing on the defaults that I can't figure out is

455
00:26:15,835 --> 00:26:19,405
Speaker 4:  I think a, the browser thing is very cool

456
00:26:19,745 --> 00:26:23,645
Speaker 4:  in theory but Thomas Ricker pointed out this morning that it's been

457
00:26:23,955 --> 00:26:27,765
Speaker 4:  nine months and no one has built the browser. So like

458
00:26:28,825 --> 00:26:32,805
Speaker 4:  Google and Mozilla like where you at friends build us, build

459
00:26:32,805 --> 00:26:36,645
Speaker 4:  us the better browser prove to us this will actually work. And the second

460
00:26:36,665 --> 00:26:38,965
Speaker 4:  one I really wanna know is about messaging apps. 'cause I actually think

461
00:26:38,965 --> 00:26:42,525
Speaker 4:  being able to change your default messaging app on iOS could be like

462
00:26:42,555 --> 00:26:45,845
Speaker 4:  massively consequential but only If.

463
00:26:46,465 --> 00:26:50,325
Speaker 4:  you can do SMS and RRCs in the US on those

464
00:26:50,325 --> 00:26:52,925
Speaker 4:  things. Yep. Like what this will mean for a lot of people, especially in

465
00:26:52,925 --> 00:26:55,565
Speaker 4:  the EU is that when you tap a phone number it'll go to WhatsApp, right? Like

466
00:26:55,565 --> 00:26:59,325
Speaker 4:  that's a, that's a big victory for all those people and I think is like a

467
00:26:59,505 --> 00:27:02,885
Speaker 4:  pretty neat sort of one-to-one mapping of

468
00:27:03,365 --> 00:27:06,365
Speaker 4:  features that is just, it just makes more sense, right? Like that is instead

469
00:27:06,365 --> 00:27:10,125
Speaker 4:  of having to copy it and then paste it, you just tap it and it'll go to

470
00:27:10,285 --> 00:27:13,885
Speaker 4:  WhatsApp. That's great for me personally and for people in the US the question

471
00:27:13,905 --> 00:27:17,165
Speaker 4:  is can ISMS from another app and I'm assuming the answer is no

472
00:27:17,985 --> 00:27:21,605
Speaker 4:  and until you can, the default is sort of

473
00:27:21,605 --> 00:27:25,525
Speaker 4:  meaningless. So my, my theory as to why Apple is willing

474
00:27:25,545 --> 00:27:29,245
Speaker 4:  to do this everywhere specifically with messaging is that it actually

475
00:27:29,335 --> 00:27:31,245
Speaker 4:  isn't gonna do very much.

476
00:27:31,245 --> 00:27:34,885
Speaker 3:  Yeah. Apple always gives up the ones that it's like fine. Yeah, right. You

477
00:27:34,885 --> 00:27:36,285
Speaker 3:  wanna leave on, you can leave on us. Yeah.

478
00:27:36,285 --> 00:27:40,165
Speaker 4:  Do you wanna switch to a browser engine that doesn't exist or like go use

479
00:27:40,165 --> 00:27:42,365
Speaker 4:  a messaging app that's worse, knock yourself out

480
00:27:44,465 --> 00:27:47,445
Speaker 3:  The browser one in particular is interesting because

481
00:27:48,265 --> 00:27:51,965
Speaker 3:  you know the Safari team will be upset that I say this because I say it a

482
00:27:51,965 --> 00:27:55,765
Speaker 3:  lot and they're always upset. But Apple has intentionally limited

483
00:27:55,875 --> 00:27:59,405
Speaker 3:  what Safari can do on the phone to keep web apps from

484
00:27:59,405 --> 00:28:03,195
Speaker 3:  competing with app store apps. That is real like yeah

485
00:28:03,745 --> 00:28:07,515
Speaker 3:  that is in the heart of the Department of Justice's

486
00:28:07,545 --> 00:28:11,515
Speaker 3:  antitrust case against Apple. Evidence of that is presented and

487
00:28:11,515 --> 00:28:15,475
Speaker 3:  every browser maker will tell you we would love to do more things

488
00:28:15,975 --> 00:28:19,235
Speaker 3:  on the, on the phone browser and let web apps flourish like they have on

489
00:28:19,235 --> 00:28:22,275
Speaker 3:  the desktop. But there's only Safari and Apple has

490
00:28:22,825 --> 00:28:25,835
Speaker 3:  carefully calibrated Safari at exactly the level they need to

491
00:28:26,615 --> 00:28:30,355
Speaker 3:  to keep everyone away until, I dunno the DOJ woke up and filed a lawsuit.

492
00:28:30,975 --> 00:28:34,835
Speaker 3:  So things like game streaming, Microsoft really

493
00:28:34,875 --> 00:28:37,435
Speaker 3:  wanted to do that. They tried to do it in the web browser, If you will recall.

494
00:28:37,435 --> 00:28:40,675
Speaker 3:  And they just kinda like couldn't get there. Like it's not compelling. The

495
00:28:40,675 --> 00:28:44,595
Speaker 3:  apps don't feel like real Apps Safari will just like be like, you know what,

496
00:28:44,615 --> 00:28:48,395
Speaker 3:  I'm outta memory shut everything down. Like there's a million things that

497
00:28:48,395 --> 00:28:51,915
Speaker 3:  are a problem there. So the idea that Chrome can show up

498
00:28:52,695 --> 00:28:56,515
Speaker 3:  and build, let you build real web apps inside of Chrome or

499
00:28:56,515 --> 00:29:00,315
Speaker 3:  Chrome will be more powerful. All of that is great. But then the reality

500
00:29:00,375 --> 00:29:03,715
Speaker 3:  of it's still a mobile phone, it still has a battery, it still has a limited

501
00:29:03,715 --> 00:29:07,595
Speaker 3:  amount of Ram, it's still often on a cellular connection. I think

502
00:29:07,595 --> 00:29:10,995
Speaker 3:  those, those things are real, you know, there's meaningfully real

503
00:29:11,535 --> 00:29:14,475
Speaker 3:  and I think that a lot of people are taking a lot of time with it. And then

504
00:29:14,475 --> 00:29:18,235
Speaker 3:  on top of it, who do you really want here? You want Google to show up with

505
00:29:18,235 --> 00:29:21,835
Speaker 3:  Chrome and Google and Apple are currently in a

506
00:29:22,265 --> 00:29:25,955
Speaker 3:  international regulatory dog fight about how much Google

507
00:29:26,025 --> 00:29:29,955
Speaker 3:  pays Apple to be the default. And then opening up a

508
00:29:30,015 --> 00:29:33,475
Speaker 3:  new sphere of competition for applications in

509
00:29:34,375 --> 00:29:38,155
Speaker 3:  the app store versus WebKit versus Blink web apps. I mean I,

510
00:29:38,785 --> 00:29:42,485
Speaker 3:  that seems like Google would have to give up

511
00:29:42,515 --> 00:29:46,245
Speaker 3:  negotiating with Apple in the future, right? In like a very real way. Do

512
00:29:46,245 --> 00:29:50,005
Speaker 3:  you know what I mean? Like Google's like screw it, here's Chrome

513
00:29:50,025 --> 00:29:53,445
Speaker 3:  on the iPhone. You can run web abstinent that are great,

514
00:29:54,155 --> 00:29:58,085
Speaker 3:  this will disrupt the iOS app store. It feels like

515
00:29:58,085 --> 00:30:02,005
Speaker 3:  that is a move that's nuclear enough to foreclose any amount of future

516
00:30:02,005 --> 00:30:05,845
Speaker 3:  Google Apple negotiating, which they need to do because Google is

517
00:30:05,845 --> 00:30:08,925
Speaker 3:  about to be forced to stop paying Apple billions of dollars a year to be

518
00:30:08,925 --> 00:30:11,965
Speaker 3:  the default search. All very complicated.

519
00:30:12,115 --> 00:30:15,565
Speaker 4:  It's essentially like a declaration of war if Google does that. And, and

520
00:30:15,565 --> 00:30:19,525
Speaker 4:  the thing I keep thinking about is from the search trial, one of the things

521
00:30:19,525 --> 00:30:23,485
Speaker 4:  that Apple was really worried about with Google and their

522
00:30:23,485 --> 00:30:27,245
Speaker 4:  deal was that if Apple didn't set Google as the

523
00:30:27,245 --> 00:30:31,085
Speaker 4:  default, Google had a bunch of apps on the iPhone that were

524
00:30:31,085 --> 00:30:34,725
Speaker 4:  super popular and it was really worried that Google would just start getting

525
00:30:34,725 --> 00:30:38,485
Speaker 4:  people to use the Google app and that because it had such a

526
00:30:38,485 --> 00:30:42,165
Speaker 4:  wide surface. Like if Google put a banner at the top of YouTube and Gmail

527
00:30:42,345 --> 00:30:45,605
Speaker 4:  and Docs and everything else that they have on the iPhone

528
00:30:46,305 --> 00:30:49,565
Speaker 4:  saying download the Google app and do all of your searching there, it could

529
00:30:49,565 --> 00:30:52,885
Speaker 4:  do that. Like Google could engineer scale in that way

530
00:30:53,665 --> 00:30:56,765
Speaker 4:  unlike almost any other company. And Apple was really worried about that

531
00:30:56,765 --> 00:31:00,085
Speaker 4:  happening if they took Google out of the default position in Safari.

532
00:31:00,955 --> 00:31:04,245
Speaker 4:  This is the, this is the inverse of that, right? What Google could say is

533
00:31:04,705 --> 00:31:08,325
Speaker 4:  now you can run full featured Chrome

534
00:31:08,585 --> 00:31:12,045
Speaker 4:  web apps, which have become very powerful in the last few years

535
00:31:13,745 --> 00:31:17,645
Speaker 4:  as progressive web apps on your phone like Google. It could essentially like

536
00:31:18,355 --> 00:31:21,805
Speaker 4:  undo the whole app store model under this new system.

537
00:31:22,345 --> 00:31:26,245
Speaker 4:  And if in a purely competitive world that is obviously what Google would

538
00:31:26,245 --> 00:31:29,445
Speaker 4:  try to do right? Is say don't don't download apps from the app store and

539
00:31:29,445 --> 00:31:33,125
Speaker 4:  give up 30%, just install the app running on Blink and chromium

540
00:31:33,355 --> 00:31:37,325
Speaker 4:  because now you can on your iPhone and that would,

541
00:31:37,355 --> 00:31:40,445
Speaker 4:  that would put these two companies at like incredible odds with each other.

542
00:31:40,585 --> 00:31:44,045
Speaker 4:  But is also if there weren't many billions of dollars between the two of

543
00:31:44,045 --> 00:31:45,525
Speaker 4:  them, exactly what Google would do.

544
00:31:45,625 --> 00:31:49,445
Speaker 3:  And that is the heart of the DOJ case against Apple. Right? Totally. Like

545
00:31:49,445 --> 00:31:52,445
Speaker 3:  inside of that case that the thing that that lawsuit

546
00:31:53,145 --> 00:31:56,925
Speaker 3:  claims like at its core is, well web apps did this

547
00:31:56,945 --> 00:32:00,685
Speaker 3:  to Windows, there's a reason they haven't done it to phones, right? And it's

548
00:32:00,685 --> 00:32:04,365
Speaker 3:  not battery life, it's because Apple has kept blah blah blah blah blah. We'll

549
00:32:04,365 --> 00:32:07,485
Speaker 3:  see it. It's like, it's very funny to go from, you can install native web

550
00:32:07,485 --> 00:32:11,125
Speaker 3:  apps in iOS 18.2 to look at this thicket of

551
00:32:11,125 --> 00:32:14,685
Speaker 3:  lawsuits. But that's really what's happening here. Like that thicket of

552
00:32:14,685 --> 00:32:18,645
Speaker 3:  lawsuits is directly impacting how these products are signed. Yeah, I think

553
00:32:18,645 --> 00:32:22,485
Speaker 3:  it is also, we should just talk about this Tim Cook, big

554
00:32:22,485 --> 00:32:25,565
Speaker 3:  Tim Cook interview in the Wall Street Journal magazine. 'cause it's a big

555
00:32:25,805 --> 00:32:29,685
Speaker 3:  argument for being, I think his, his quote is

556
00:32:30,665 --> 00:32:31,475
Speaker 3:  best not first.

557
00:32:31,865 --> 00:32:32,155
Speaker 4:  Yeah,

558
00:32:32,485 --> 00:32:35,355
Speaker 3:  Right. Like he, he's like, we'll make the best products, not the first products.

559
00:32:35,415 --> 00:32:38,595
Speaker 3:  And that is in reference to the Vision Pro, which we should talk about for

560
00:32:38,595 --> 00:32:41,725
Speaker 3:  one second. And definitely in reference to ai,

561
00:32:42,245 --> 00:32:44,155
Speaker 3:  right? He is like, we're not first outta the gate but we're gonna do the

562
00:32:44,155 --> 00:32:47,715
Speaker 3:  best with these over time. These are our big bets with the Vision Pro. He

563
00:32:47,715 --> 00:32:50,435
Speaker 3:  basically is like, here's how I use it. I lie flat on my back on the couch

564
00:32:50,435 --> 00:32:53,675
Speaker 3:  and I watch tv. It's a real quote. Fun

565
00:32:53,675 --> 00:32:54,995
Speaker 4:  Fact. That's how Richard watches TV too.

566
00:32:55,655 --> 00:32:58,995
Speaker 3:  And he is like at 3,500 not Division Pro. Yeah. At

567
00:32:58,995 --> 00:33:02,955
Speaker 3:  $3,500. It's not a mainstream device. Fine. Right? Like they took a shot

568
00:33:02,955 --> 00:33:06,555
Speaker 3:  on a weird product and whatever and they did admit there was a VR headset,

569
00:33:06,585 --> 00:33:09,475
Speaker 3:  fine, they're gonna make a cheaper one with ai. It's like,

570
00:33:10,515 --> 00:33:13,795
Speaker 3:  I get that you're saying, you know, you wanna be the best thing and you certainly

571
00:33:13,795 --> 00:33:17,435
Speaker 3:  have the distribution advantage 'cause you, you have the iPhone, you can

572
00:33:17,435 --> 00:33:20,435
Speaker 3:  just get it in front of more people fastest. But If, you looked around

573
00:33:21,305 --> 00:33:21,595
Speaker 5:  Like

574
00:33:22,225 --> 00:33:26,155
Speaker 3:  This week philanthropic is saying Claude can now just

575
00:33:26,215 --> 00:33:30,155
Speaker 3:  use computers for you. Like there's just a, there's

576
00:33:30,935 --> 00:33:34,795
Speaker 3:  the level of innovation in the industry is like out of control. Like

577
00:33:34,995 --> 00:33:38,115
Speaker 3:  everyone is racing as fast as they can. Whether or not that's gonna come

578
00:33:38,115 --> 00:33:41,675
Speaker 3:  to anything we should talk about. But it's this thing that where

579
00:33:42,535 --> 00:33:46,515
Speaker 3:  Tim Cook is saying, these glimmers of Apple intelligence are evidence that

580
00:33:46,515 --> 00:33:49,795
Speaker 3:  will make the best thing. I'm not, I'm just not, I'm not so sure about it.

581
00:33:50,265 --> 00:33:54,155
Speaker 5:  It's evidence that they're making something now. They are sort at least in

582
00:33:54,155 --> 00:33:57,915
Speaker 5:  this kind of artificial intelligence race that Google and Microsoft and everyone

583
00:33:57,915 --> 00:34:01,515
Speaker 5:  have been just driving forward over the last, I guess year, 18 months now.

584
00:34:02,285 --> 00:34:05,985
Speaker 5:  But it, I I think that's the question. Is it any good? Is it what people

585
00:34:05,985 --> 00:34:09,585
Speaker 5:  want? Is it going to do something other than generate a new emoji

586
00:34:10,335 --> 00:34:13,305
Speaker 5:  that you'll send to only other people with iPhones?

587
00:34:13,695 --> 00:34:14,585
Speaker 3:  Yeah. I dunno.

588
00:34:14,685 --> 00:34:16,945
Speaker 5:  Is it gonna kill your battery? We don't know that

589
00:34:16,945 --> 00:34:19,785
Speaker 3:  Part actually. We, we definitely do not know. Like If, you make Gen Moji

590
00:34:19,785 --> 00:34:22,425
Speaker 3:  all day, are you gonna destroy a battery? David will soon find out.

591
00:34:22,565 --> 00:34:26,465
Speaker 4:  And also potentially the world. Yeah, I I don't know. I I

592
00:34:26,465 --> 00:34:30,345
Speaker 4:  read a lot of this Wall Street Journal piece is like, what else is Tim Cook

593
00:34:30,545 --> 00:34:33,905
Speaker 4:  supposed to say? Right? Like seriously, I i, I don't even, I don't even blame

594
00:34:33,905 --> 00:34:35,025
Speaker 4:  the guy like this is

595
00:34:36,795 --> 00:34:40,585
Speaker 4:  seven months ago he was telling everybody how he liked to do everything in

596
00:34:40,785 --> 00:34:44,585
Speaker 4:  addition Pro. Like this is just what you do. And I think Apple

597
00:34:44,765 --> 00:34:48,505
Speaker 4:  is in a position now where it is forced

598
00:34:48,765 --> 00:34:52,465
Speaker 4:  to try to make the case that this stuff is really great. So Tim Cook is waking

599
00:34:52,525 --> 00:34:55,345
Speaker 4:  is saying he like wakes up in the morning and checks his email and having

600
00:34:55,345 --> 00:34:58,705
Speaker 4:  summaries of his emails as a game changer. Like, I just don't believe that

601
00:34:58,855 --> 00:35:02,745
Speaker 4:  like maybe having a bunch of emails

602
00:35:03,185 --> 00:35:06,185
Speaker 4:  slightly better triaged for you is a game changer in your life.

603
00:35:08,205 --> 00:35:09,925
Speaker 4:  I don't think that's true for very many people.

604
00:35:10,305 --> 00:35:14,215
Speaker 3:  Can you imagine If, you were an Apple executive and you're like, this

605
00:35:14,215 --> 00:35:17,575
Speaker 3:  email got summarized. Yeah, Tim Cook didn't read my shit.

606
00:35:18,095 --> 00:35:21,575
Speaker 5:  I think, I think Tim Cook can afford an assistant. Like if, if that were

607
00:35:21,575 --> 00:35:25,145
Speaker 5:  the game, if that were truly the game changer that he needed, what has he

608
00:35:25,145 --> 00:35:29,025
Speaker 5:  been doing until now and why has someone not been summarizing Tim Cook's

609
00:35:29,025 --> 00:35:29,465
Speaker 5:  emails for

610
00:35:31,735 --> 00:35:35,665
Speaker 5:  like, like how much value have Apple shareholders lost? Because Tim

611
00:35:35,665 --> 00:35:39,605
Speaker 5:  Cook insisted on reading every word of every email for the last 10 years.

612
00:35:39,635 --> 00:35:39,925
Speaker 5:  Well,

613
00:35:39,925 --> 00:35:42,805
Speaker 3:  No, he's famous. He's he's one of the CEOs who's famous for reading customer

614
00:35:42,805 --> 00:35:46,205
Speaker 3:  emails and like forwarding 'em off. But you emailed Tim Cook in like

615
00:35:46,875 --> 00:35:50,805
Speaker 3:  some Apple SVP three layers down the chain. We'll write back to you

616
00:35:51,035 --> 00:35:54,805
Speaker 3:  because he'll, he'll do that. I don't know if he does it the way that Jeff Bezos

617
00:35:54,805 --> 00:35:58,245
Speaker 3:  used to do it, which is forward the email to a deputy with just a question

618
00:35:58,245 --> 00:36:00,035
Speaker 3:  mark. Like just

619
00:36:00,095 --> 00:36:02,075
Speaker 4:  At, at like all hours of the day at night. Yeah.

620
00:36:02,105 --> 00:36:04,515
Speaker 3:  What is this? Which is truly incredible. Why am I getting this email? Yeah.

621
00:36:05,175 --> 00:36:07,155
Speaker 3:  Do you think Tim cooks his enterprise software at work?

622
00:36:09,105 --> 00:36:12,815
Speaker 3:  No. I'm just asking like, has this man ever logged into Con Concur? No.

623
00:36:13,115 --> 00:36:15,535
Speaker 3:  Do you think he knows his frequent flyer number? He has a plane.

624
00:36:16,135 --> 00:36:19,095
Speaker 4:  I was gonna say bet he does. I bet he knows his United number.

625
00:36:19,835 --> 00:36:20,575
Speaker 3:  No way. He has a,

626
00:36:20,575 --> 00:36:22,575
Speaker 4:  Because Apple famously has like a big thing with United

627
00:36:22,835 --> 00:36:26,535
Speaker 3:  For people who are not Tim Cook Apple's famous thing with United is they

628
00:36:26,535 --> 00:36:29,655
Speaker 3:  own a plane and sometimes they say the word united on it.

629
00:36:30,285 --> 00:36:30,575
Speaker 3:  Yeah,

630
00:36:30,575 --> 00:36:32,135
Speaker 4:  That's, that's reasonable.

631
00:36:32,635 --> 00:36:36,455
Speaker 3:  I'm reading this profile and I'm looking at the two big product

632
00:36:36,695 --> 00:36:40,335
Speaker 3:  launches this year. And the profile is very much about Tim Cook making big

633
00:36:40,485 --> 00:36:44,345
Speaker 3:  bets, right? And it's okay, the Vision Pro whatever you, you

634
00:36:44,345 --> 00:36:47,945
Speaker 3:  gotta put one foot in front of the other and yes, the meta glasses exist

635
00:36:47,945 --> 00:36:50,865
Speaker 3:  and yes, the Quest three s is even cheaper than before. Whatever.

636
00:36:52,735 --> 00:36:56,475
Speaker 3:  It's much more like, his argument is we will

637
00:36:56,475 --> 00:37:00,155
Speaker 3:  eventually do this best and that will win. And I actually don't know if that

638
00:37:00,155 --> 00:37:02,755
Speaker 3:  is the case right now for in these two categories.

639
00:37:02,865 --> 00:37:05,075
Speaker 5:  Well, his argument for the Vision Pro is that they're getting this started

640
00:37:05,075 --> 00:37:08,795
Speaker 5:  for the developers. They're getting the ecosystem kickstarted

641
00:37:09,015 --> 00:37:11,995
Speaker 5:  and that, you know, the, these early adopters, the people who are going to

642
00:37:11,995 --> 00:37:15,805
Speaker 5:  make things are getting into it. But I'm not seeing that yet. I'm not seeing

643
00:37:16,025 --> 00:37:19,325
Speaker 5:  the, the people who are making and using these tools and that, that that's

644
00:37:19,325 --> 00:37:22,365
Speaker 5:  kicking a flywheel of innovation that's gonna help them make something that

645
00:37:22,365 --> 00:37:25,725
Speaker 5:  someone else will use years from now. Have we actually gotten anywhere with

646
00:37:25,725 --> 00:37:26,885
Speaker 5:  this $3,500 device?

647
00:37:27,175 --> 00:37:31,005
Speaker 4:  Isn't that also the opposite of the best not first strategy?

648
00:37:31,235 --> 00:37:31,885
Speaker 4:  Like yes.

649
00:37:31,975 --> 00:37:32,325
Speaker 3:  There

650
00:37:32,325 --> 00:37:35,765
Speaker 4:  Nothing, nothing about the Vision Pro suggests. Best not first.

651
00:37:35,875 --> 00:37:36,365
Speaker 4:  Nothing.

652
00:37:36,365 --> 00:37:37,365
Speaker 5:  $3,500.

653
00:37:37,705 --> 00:37:38,645
Speaker 3:  3,500.

654
00:37:38,755 --> 00:37:40,005
Speaker 4:  Yeah. Seven outta 10,

655
00:37:41,385 --> 00:37:45,125
Speaker 3:  God dammit. All right. He said one other thing in this profile that I just

656
00:37:45,125 --> 00:37:48,865
Speaker 3:  want, I want to sit with for a second. He said he

657
00:37:48,945 --> 00:37:52,925
Speaker 3:  uses every Apple product every day, every product, every day. That's a

658
00:37:52,925 --> 00:37:56,885
Speaker 3:  quote Our own West Davis tried to figure out how you would do

659
00:37:56,885 --> 00:38:00,765
Speaker 3:  that. Apple makes a lot of products. Wes owns a lot of them, including

660
00:38:01,045 --> 00:38:04,885
Speaker 3:  a 3,500 vision pro. And I would say that he, he,

661
00:38:05,065 --> 00:38:09,055
Speaker 3:  he made a valiant effort and failed because there

662
00:38:09,055 --> 00:38:12,895
Speaker 3:  are too many Apple products. Like using every kind of iPad in a

663
00:38:12,895 --> 00:38:14,855
Speaker 3:  single day is not a good idea.

664
00:38:15,205 --> 00:38:17,775
Speaker 4:  It's not just that there are too many Apple products, it's that they overlap

665
00:38:17,995 --> 00:38:21,655
Speaker 4:  in complicated ways, right? So that, like, even even

666
00:38:21,755 --> 00:38:25,535
Speaker 4:  Cook's thing is he, he claims to have an a

667
00:38:25,535 --> 00:38:29,015
Speaker 4:  MacBook Air, a MacBook Pro and an iMac at the office and

668
00:38:29,245 --> 00:38:33,135
Speaker 4:  like God help you If you try to use all three of those devices during a

669
00:38:33,135 --> 00:38:35,725
Speaker 4:  single workday. Like how, what?

670
00:38:35,875 --> 00:38:38,325
Speaker 3:  Well, so that's what kicked me off. 'cause I was like, Wes, you should do

671
00:38:38,325 --> 00:38:41,965
Speaker 3:  this story. 'cause it's, it's one thing If, you say, I use

672
00:38:42,055 --> 00:38:45,845
Speaker 3:  every one of every kind of product we make every day, right? I use one Mac,

673
00:38:45,865 --> 00:38:49,085
Speaker 3:  one iPad, one phone, one AirPods. I think that's what you use every product

674
00:38:49,085 --> 00:38:52,165
Speaker 3:  every day. And then specifically calling out three Macs is like,

675
00:38:52,865 --> 00:38:56,845
Speaker 3:  oh, you mean every product, right? The every, does

676
00:38:56,845 --> 00:38:58,685
Speaker 4:  That that include both sizes of the Apple watch?

677
00:38:58,875 --> 00:39:02,805
Speaker 3:  Both sizes, apple watch all 15 skews of the Apple

678
00:39:02,805 --> 00:39:06,485
Speaker 3:  watch. Do you think he's like, well time to switch the Apple watch with cellular.

679
00:39:06,715 --> 00:39:08,245
Speaker 3:  Like that's a

680
00:39:11,105 --> 00:39:14,445
Speaker 3:  and I actually think the evidence is that he doesn't use most of the products

681
00:39:14,475 --> 00:39:15,285
Speaker 3:  most of the time.

682
00:39:16,145 --> 00:39:19,045
Speaker 5:  The number one thing that came up in the comments was this guy does not use

683
00:39:19,045 --> 00:39:20,205
Speaker 5:  Apple Home. There's no one,

684
00:39:20,615 --> 00:39:20,965
Speaker 3:  Right?

685
00:39:21,385 --> 00:39:22,125
Speaker 5:  No one believes it.

686
00:39:22,585 --> 00:39:25,445
Speaker 4:  No. Tim Cook's a Sonos guy. Let's be real with each other. He's definitely

687
00:39:25,485 --> 00:39:26,125
Speaker 4:  a Sonos guy.

688
00:39:26,765 --> 00:39:28,165
Speaker 3:  He'd buy it. Well, Sonos supports airplay.

689
00:39:30,465 --> 00:39:33,645
Speaker 3:  Joanna CERN interviewed Craig Federighi at Wall Street Journal Tech Live

690
00:39:33,645 --> 00:39:36,765
Speaker 3:  this week. You can watch that. She has some Apple intelligence and two things

691
00:39:36,765 --> 00:39:40,485
Speaker 3:  she said of note related to that, Richard, she asked him about Siri

692
00:39:40,625 --> 00:39:44,365
Speaker 3:  and he said, I use Siri every day. Siri serves

693
00:39:44,365 --> 00:39:47,365
Speaker 3:  1.5 billion queries a day, which is an interesting number we should come

694
00:39:47,365 --> 00:39:51,205
Speaker 3:  back to. And he said, I use it to open my garage. Which

695
00:39:51,225 --> 00:39:55,205
Speaker 3:  is fine, implies that Craig Feder has a home

696
00:39:55,205 --> 00:39:59,045
Speaker 3:  kit garage door opener, which is challenging actually

697
00:39:59,365 --> 00:40:02,965
Speaker 3:  for a variety of ways. And that he does not,

698
00:40:03,225 --> 00:40:07,125
Speaker 3:  he chooses to say, Siri open the garage every single day

699
00:40:07,755 --> 00:40:09,405
Speaker 3:  instead of just pressing a button

700
00:40:11,065 --> 00:40:14,725
Speaker 3:  as somebody with two different kinds of home kit garage door openers. I love

701
00:40:14,825 --> 00:40:18,525
Speaker 3:  the idea of Apple's SVP of all software

702
00:40:19,135 --> 00:40:22,925
Speaker 3:  going on Amazon and ordering a $30 Mi Ross garage door opener and wiring

703
00:40:22,925 --> 00:40:26,815
Speaker 3:  it into a thing like I love it with all of

704
00:40:26,815 --> 00:40:30,695
Speaker 3:  my heart. And I, I'm absolutely certain that did not happen. I don't

705
00:40:30,695 --> 00:40:34,095
Speaker 3:  know how he's opening his garage door home kit, but that's the way you do

706
00:40:34,095 --> 00:40:37,895
Speaker 3:  it. And I just, I love the idea of him being like, my ecosystem is

707
00:40:37,895 --> 00:40:41,535
Speaker 3:  working while he's like screwing the thing into the terminals.

708
00:40:42,215 --> 00:40:45,495
Speaker 4:  I wonder if it Apple, when you get like promoted to SVP

709
00:40:45,885 --> 00:40:49,815
Speaker 4:  instead of getting, you know, a a a Rolex or a company car

710
00:40:49,815 --> 00:40:53,455
Speaker 4:  or whatever, they just hand you a giant box full of Apple

711
00:40:53,455 --> 00:40:56,375
Speaker 4:  connected gear and they're like, you have to have this in your house now.

712
00:40:56,565 --> 00:40:59,695
Speaker 4:  Like this is, this is the setup. Yeah. Welcome.

713
00:41:00,595 --> 00:41:04,095
Speaker 3:  You're now in charge, you're in home kit. Now you're actually, I, you know,

714
00:41:04,335 --> 00:41:08,295
Speaker 3:  as somebody who runs HomeBridge to bridge a lot of stuff into

715
00:41:08,365 --> 00:41:12,255
Speaker 3:  HomeKit, I know that a lot of the people who work on HomeKit at

716
00:41:12,255 --> 00:41:15,795
Speaker 3:  Apple also run HomeBridge. This is a real thing, huh?

717
00:41:16,345 --> 00:41:19,875
Speaker 3:  Like a lot of HomeKit engineers are like, yep, we're gonna build the plugins

718
00:41:19,875 --> 00:41:23,755
Speaker 3:  to integrate rings. It's a real thing. And If, you are those

719
00:41:23,755 --> 00:41:25,795
Speaker 3:  people. Let us know 'cause I have a number of feature requests for you. And

720
00:41:25,795 --> 00:41:26,635
Speaker 3:  also I just wanna say thank you.

721
00:41:28,635 --> 00:41:31,715
Speaker 3:  I don't know, I don't know that you can use every Apple product every day.

722
00:41:31,715 --> 00:41:35,405
Speaker 3:  But If you are listening to this and you, you can figure it out. Send us

723
00:41:35,405 --> 00:41:37,805
Speaker 3:  your chart. We'll read the chart next week. That's all I really wanna say.

724
00:41:37,865 --> 00:41:38,085
Speaker 3:  We

725
00:41:38,085 --> 00:41:41,845
Speaker 4:  Think Tim is mostly an iPad guy, right? Tim? Tim in my head, Tim is an

726
00:41:41,845 --> 00:41:42,285
Speaker 4:  iPad guy.

727
00:41:42,545 --> 00:41:45,685
Speaker 3:  Tim has got it figured out. He's circling things in red and sending PDFs

728
00:41:45,685 --> 00:41:49,245
Speaker 3:  back to people. Okay. All right. We gotta take a break. If you again. If.

729
00:41:49,245 --> 00:41:51,005
Speaker 3:  you can figure it out. Let me know. We're

730
00:43:08,755 --> 00:43:11,645
Speaker 3:  This is gonna be incredible. Get ready Richard. We'll be right back.

731
00:43:16,345 --> 00:43:20,065
Speaker 3:  All right, we're back. We're gonna let AI drive this segment. It's gonna

732
00:43:20,065 --> 00:43:22,105
Speaker 3:  be great. Take it away. Claude.

733
00:43:23,475 --> 00:43:25,625
Speaker 4:  Hello. Welcome to just,

734
00:43:26,765 --> 00:43:30,145
Speaker 3:  We should let Notebook LM do a segment that should, that's a pretty good

735
00:43:30,145 --> 00:43:30,505
Speaker 3:  gimmick

736
00:43:30,665 --> 00:43:33,305
Speaker 4:  Actually. That is actually fun. Maybe I'll do that for the next one. I'll

737
00:43:33,305 --> 00:43:37,105
Speaker 4:  just upload all the stuff in our rundown to Notebook lm and let it make the

738
00:43:37,105 --> 00:43:39,745
Speaker 4:  podcast and then we'll run it. 'cause it makes like a 12 minute podcast.

739
00:43:40,115 --> 00:43:42,865
Speaker 4:  We'll run it at the end of an episode and people can decide which one they

740
00:43:42,865 --> 00:43:43,345
Speaker 4:  like better.

741
00:43:43,525 --> 00:43:46,745
Speaker 3:  That's actually pretty good. We'll do that. I will say that the chances that

742
00:43:46,745 --> 00:43:49,315
Speaker 3:  that is a more evenly

743
00:43:50,645 --> 00:43:52,475
Speaker 3:  paced Vergecast is very high.

744
00:43:53,055 --> 00:43:53,275
Speaker 4:  Yes.

745
00:43:53,705 --> 00:43:55,155
Speaker 3:  More focused. Anyway. Do

746
00:43:55,155 --> 00:43:58,155
Speaker 4:  They have more catchphrases than Eli is really the question That's

747
00:43:58,595 --> 00:44:02,555
Speaker 3:  Absolutely not. So there's a lot of AI news this week. The

748
00:44:02,835 --> 00:44:06,795
Speaker 3:  big theme is that everyone figured out that chatbots aren't

749
00:44:06,795 --> 00:44:10,725
Speaker 3:  gonna make them enough money. And so now we have to build robots that can

750
00:44:10,725 --> 00:44:12,565
Speaker 3:  take actions on our computer. Is that about right David?

751
00:44:13,275 --> 00:44:17,045
Speaker 4:  Yeah, I think there's, there's basically like two big pieces

752
00:44:17,145 --> 00:44:20,805
Speaker 4:  of AI news this week, which is

753
00:44:21,745 --> 00:44:25,605
Speaker 4:  we have to give AI more stuff to do and then, oh

754
00:44:25,665 --> 00:44:29,655
Speaker 4:  God, what if we give AI more stuff to do? Let's sue the AI

755
00:44:29,935 --> 00:44:32,605
Speaker 4:  companies. So I'd say those are the two things. So let's, let's just take

756
00:44:32,605 --> 00:44:36,565
Speaker 4:  them one at a time. The biggest news

757
00:44:36,845 --> 00:44:40,765
Speaker 4:  I would say in terms of like things that got the AI people excited

758
00:44:41,825 --> 00:44:44,965
Speaker 4:  was the new update to Claude, which is Anthropics

759
00:44:45,595 --> 00:44:49,525
Speaker 4:  chatbot. It's, it's it's competitor to GPT-4 Oh and Llama and whatever

760
00:44:49,525 --> 00:44:52,365
Speaker 4:  else it can now use a computer for you.

761
00:44:53,235 --> 00:44:56,885
Speaker 4:  They, they released a very cool demo video, which is just one of the

762
00:44:57,045 --> 00:45:00,165
Speaker 4:  researchers sitting there and he basically opens up a, a spreadsheet and

763
00:45:00,185 --> 00:45:04,085
Speaker 4:  is like, Hey, I need you to pull data out of this and put it into an email

764
00:45:04,105 --> 00:45:07,445
Speaker 4:  for me. And it goes through and it clicks around and it says, oh, I can't

765
00:45:07,445 --> 00:45:10,925
Speaker 4:  find this data in here. So it opens, I think it's like a vendor software

766
00:45:11,695 --> 00:45:14,765
Speaker 4:  thing, searches through that, finds the information, puts it where it's,

767
00:45:16,305 --> 00:45:20,085
Speaker 4:  the idea is that it can actually click around and look at and use your computer

768
00:45:20,185 --> 00:45:23,685
Speaker 4:  and pull data from it. It was actually tracking what it was doing as it was

769
00:45:23,685 --> 00:45:27,005
Speaker 4:  going. So it was saying like, I hit the page down button and I put the cursor

770
00:45:27,005 --> 00:45:30,765
Speaker 4:  over here and I typed this into the, seems to work very well. It is

771
00:45:30,985 --> 00:45:34,885
Speaker 4:  one demo and it's not a product that Anthropic is shipping. It's a

772
00:45:34,885 --> 00:45:36,805
Speaker 4:  thing in the API that other people can build on top of.

773
00:45:36,875 --> 00:45:39,685
Speaker 3:  I've already seen someone build an app that can sort of do this on a Mac.

774
00:45:39,705 --> 00:45:39,925
Speaker 3:  Oh

775
00:45:39,925 --> 00:45:40,405
Speaker 4:  Really? Yep.

776
00:45:40,715 --> 00:45:41,125
Speaker 3:  Sort of.

777
00:45:41,645 --> 00:45:45,605
Speaker 4:  I I will say Anthropic is productizing,

778
00:45:45,865 --> 00:45:49,845
Speaker 4:  its AI better and faster than any of its competitors right now in a way

779
00:45:49,845 --> 00:45:53,685
Speaker 4:  that I think is really interesting. But that is, that is the thing,

780
00:45:53,685 --> 00:45:57,005
Speaker 4:  right? That's what everyone is out there trying to build that it, it's a

781
00:45:57,005 --> 00:46:00,965
Speaker 4:  more elegant version essentially of what Rabbit is doing with the large action

782
00:46:00,965 --> 00:46:03,565
Speaker 4:  model. I mean, it's the same thing. It just clicks around a webpage for you.

783
00:46:04,205 --> 00:46:07,365
Speaker 5:  I mean, is Rabbit doing it? They they said they would do it and they have

784
00:46:07,365 --> 00:46:10,965
Speaker 5:  built something that does something and some people have tried it and

785
00:46:11,625 --> 00:46:15,325
Speaker 5:  it does mostly nothing but some things.

786
00:46:15,945 --> 00:46:16,165
Speaker 3:  I'm

787
00:46:16,165 --> 00:46:18,685
Speaker 4:  Sorry, they've built something that does something and so people have tried

788
00:46:18,685 --> 00:46:20,125
Speaker 4:  it is the AI industry,

789
00:46:22,255 --> 00:46:22,605
Speaker 3:  Right?

790
00:46:22,745 --> 00:46:25,445
Speaker 5:  Can you actually do anything useful with what they built? No,

791
00:46:25,765 --> 00:46:29,645
Speaker 3:  I, yes. I, we again, we, we, the CEO of Rabbit on decoder, you can

792
00:46:29,645 --> 00:46:32,965
Speaker 3:  listen to it. They're at a very different spot than

793
00:46:33,475 --> 00:46:36,525
Speaker 3:  Anthropic. and we had Mike Krieger, who's the head of product at Philanthropic.

794
00:46:36,545 --> 00:46:40,125
Speaker 3:  And your point about they're better at productizing is because they have

795
00:46:40,125 --> 00:46:44,045
Speaker 3:  a great product person. Yeah. And all the AI companies are trying

796
00:46:44,045 --> 00:46:45,165
Speaker 3:  to hire product people.

797
00:46:46,705 --> 00:46:49,225
Speaker 4:  I hire Kevin Wheel, who's done a bunch of really interesting product stuff

798
00:46:49,245 --> 00:46:52,865
Speaker 4:  for social companies over the years. Like this is the playbook now everybody's

799
00:46:52,865 --> 00:46:53,185
Speaker 4:  after this.

800
00:46:53,325 --> 00:46:56,745
Speaker 3:  And so the, and the, the first product everyone's thinking of is beyond a

801
00:46:56,745 --> 00:47:00,105
Speaker 3:  chatbot, what can we do? And it appears to be

802
00:47:00,995 --> 00:47:04,545
Speaker 3:  super Siri, right? Like the thing that everyone is

803
00:47:04,845 --> 00:47:08,665
Speaker 3:  trying to build is the ultimate Google assistant or super Siri or this

804
00:47:08,665 --> 00:47:12,505
Speaker 3:  vision that everybody had when Alexa first hit the scene ages ago,

805
00:47:12,505 --> 00:47:15,265
Speaker 3:  which is you're gonna talk to the computer and the computer's gonna do stuff,

806
00:47:15,965 --> 00:47:19,945
Speaker 3:  and then the sort of how underneath this user story,

807
00:47:20,725 --> 00:47:24,665
Speaker 3:  no one could do forever, right? Like, well, here's what Alexa can do.

808
00:47:24,665 --> 00:47:27,265
Speaker 3:  You can play music and set timers and not much else. Here's what Siri can

809
00:47:27,265 --> 00:47:31,165
Speaker 3:  do, apparently open Craig Federer's garage door. That's what

810
00:47:31,165 --> 00:47:34,885
Speaker 3:  it can do. But then you, because it can't understand actual language,

811
00:47:35,185 --> 00:47:39,045
Speaker 3:  you're stuck it, you have to write every query

812
00:47:39,185 --> 00:47:42,725
Speaker 3:  and match every query. Now you can understand actual language. Now you gotta

813
00:47:42,725 --> 00:47:46,645
Speaker 3:  do the next thing. And then the solution seems to be we'll just have it

814
00:47:46,645 --> 00:47:50,405
Speaker 3:  use a computer to do that for you. And then AI will

815
00:47:50,405 --> 00:47:51,165
Speaker 3:  solve that problem too.

816
00:47:51,355 --> 00:47:54,445
Speaker 4:  Yeah. And it's actually the, the, I encourage everyone to watch the Anthropic

817
00:47:54,445 --> 00:47:58,225
Speaker 4:  video because it's actually sort of inadvertently a really

818
00:47:58,425 --> 00:48:01,465
Speaker 4:  interesting breakdown of how this tech works. Like it essentially just goes

819
00:48:01,465 --> 00:48:05,065
Speaker 4:  through and takes a series of screenshots and OCR is the information

820
00:48:05,245 --> 00:48:08,625
Speaker 4:  out of the screenshots and then says, oh, I don't have what I need from this

821
00:48:08,625 --> 00:48:11,665
Speaker 4:  screenshot, but I see, I, I see from the scroll bar, I can page down or I

822
00:48:11,665 --> 00:48:15,505
Speaker 4:  see there's a thing over there. Like the, the key here is

823
00:48:15,575 --> 00:48:19,465
Speaker 4:  basically taking this thing that people know how to do,

824
00:48:19,525 --> 00:48:23,305
Speaker 4:  but is very complicated to teach computers and just

825
00:48:23,505 --> 00:48:27,185
Speaker 4:  breaking it down to a bunch of very simple things, which is like, look for

826
00:48:27,335 --> 00:48:31,265
Speaker 4:  this number next to this name. And If, you don't see the name search

827
00:48:31,285 --> 00:48:34,985
Speaker 4:  for it in this, like, it's actually a bunch of pretty discreet steps.

828
00:48:35,135 --> 00:48:38,785
Speaker 4:  It's just very hard to explain to a computer in an efficient way. But like

829
00:48:38,785 --> 00:48:40,905
Speaker 4:  you said, now that a computer can understand

830
00:48:42,545 --> 00:48:45,585
Speaker 4:  language and images, essentially all you have to do is figure out a way to

831
00:48:45,585 --> 00:48:48,985
Speaker 4:  keep feeding it language and images and it can do something with it. And

832
00:48:48,985 --> 00:48:52,865
Speaker 4:  I think like the Anthropic video was the most convinced I've been that

833
00:48:52,865 --> 00:48:56,185
Speaker 4:  this is doable because it's like, here's just a bunch of screenshots. And

834
00:48:56,185 --> 00:49:00,105
Speaker 4:  it's like, it's the same thing that Microsoft is pitching with recall. And

835
00:49:00,105 --> 00:49:03,705
Speaker 4:  we've seen from some startups and stuff that, like If, you just turn

836
00:49:04,165 --> 00:49:07,865
Speaker 4:  the universe into a series of screenshots. AI can

837
00:49:08,145 --> 00:49:10,425
Speaker 4:  actually make a lot out of that for you.

838
00:49:10,655 --> 00:49:14,465
Speaker 5:  Well, it's not that far off from the way that autonomous vehicles work. I

839
00:49:14,465 --> 00:49:17,705
Speaker 5:  mean, they, they're not necessarily looking at things the way that a driver

840
00:49:17,705 --> 00:49:21,385
Speaker 5:  does, but they're taking these, these pictures many, many, many times a second.

841
00:49:21,385 --> 00:49:25,345
Speaker 5:  Totally. And reacting based on that. And it's just doing this on a computer,

842
00:49:25,345 --> 00:49:29,185
Speaker 5:  which to some extent more and less complicated because of the way things

843
00:49:29,185 --> 00:49:29,785
Speaker 5:  are already labeled.

844
00:49:30,135 --> 00:49:30,425
Speaker 3:  Yeah.

845
00:49:30,685 --> 00:49:34,425
Speaker 4:  It, it is amazing to me how many people building all kinds of

846
00:49:34,485 --> 00:49:38,385
Speaker 4:  AI products I talk to who just want to use the self-driving

847
00:49:38,385 --> 00:49:41,225
Speaker 4:  car metaphor. It's very funny. And I think you're right, it is, it is a really

848
00:49:41,225 --> 00:49:45,025
Speaker 4:  similar problem. It's also a really similar like challenge over

849
00:49:45,055 --> 00:49:48,945
Speaker 4:  time to get from like very good to perfect. But it is

850
00:49:48,945 --> 00:49:51,625
Speaker 4:  like I, you're totally right. I think that's exactly the right comparison.

851
00:49:52,165 --> 00:49:54,305
Speaker 3:  The really interesting thing on the computer side

852
00:49:56,525 --> 00:50:00,505
Speaker 3:  is self-driving cars can't like be recursive in that particular way.

853
00:50:00,765 --> 00:50:04,665
Speaker 3:  The most interesting Claude demo I saw was Claude asking itself

854
00:50:04,665 --> 00:50:06,345
Speaker 3:  to generate code to solve a problem.

855
00:50:06,975 --> 00:50:08,785
Speaker 4:  Whoa. You can Claude with Claude.

856
00:50:08,925 --> 00:50:11,465
Speaker 3:  So it's like, you're like, do you solve this problem for me? And it's like,

857
00:50:11,465 --> 00:50:14,545
Speaker 3:  okay, the best thing I know how to do is open a web browser, open Claude,

858
00:50:15,245 --> 00:50:18,945
Speaker 3:  ask prompt myself, get an answer. Wow. And like,

859
00:50:19,255 --> 00:50:23,185
Speaker 3:  that is really clever. You know, there's all these benchmarks that

860
00:50:24,185 --> 00:50:27,885
Speaker 3:  candidly, I, I, I think are too early in the world of

861
00:50:28,065 --> 00:50:31,645
Speaker 3:  AI benchmarks to mean anything. But they're like, here's a test we ran that,

862
00:50:31,875 --> 00:50:35,685
Speaker 3:  that humans score 70% on Claude is at like seven. You know,

863
00:50:35,685 --> 00:50:39,605
Speaker 3:  like yeah, it's nowhere, but it's, it's, it's

864
00:50:39,605 --> 00:50:42,485
Speaker 3:  twice as good as the next nearest thing. And then Rabbit is like,

865
00:50:43,545 --> 00:50:45,165
Speaker 3:  we we're not doing a good job with Spotify.

866
00:50:47,685 --> 00:50:50,835
Speaker 3:  Sorry, I'm gonna get another angry email. I can, who is doing a good job

867
00:50:50,835 --> 00:50:54,195
Speaker 3:  with Spotify? Can you use Spotify? Well, I think we were all older, about

868
00:50:54,195 --> 00:50:57,675
Speaker 3:  7% using Spotify. Spotify is like, it's a podcast.

869
00:50:58,105 --> 00:51:01,875
Speaker 3:  Like that's not what I wanted. But the, the idea that the

870
00:51:02,035 --> 00:51:05,995
Speaker 3:  computer will figure out how to use itself is powerful. The idea that the,

871
00:51:06,055 --> 00:51:09,915
Speaker 3:  the computer can generically use any computer and reason its way

872
00:51:09,915 --> 00:51:13,635
Speaker 3:  to, I should just use Claude to solve this powerful. And then underneath

873
00:51:13,635 --> 00:51:14,675
Speaker 3:  it is,

874
00:51:16,365 --> 00:51:20,205
Speaker 3:  I think just a, like the industry has to come to grips with the idea that

875
00:51:20,205 --> 00:51:23,645
Speaker 3:  we're, we're not programming computers anymore. Like

876
00:51:24,585 --> 00:51:28,525
Speaker 3:  an API is the best way to use Spotify. Yeah. Right. Like there's

877
00:51:28,525 --> 00:51:31,565
Speaker 3:  a lot of tasks where just telling the computer what to do and having it do

878
00:51:31,565 --> 00:51:35,365
Speaker 3:  it deterministically is the correct solution. And we're headed towards a

879
00:51:35,365 --> 00:51:39,165
Speaker 3:  place where a robot taking an infinite

880
00:51:39,165 --> 00:51:43,085
Speaker 3:  number of screenshots of a Windows desktop clicking on stuff

881
00:51:43,625 --> 00:51:47,405
Speaker 3:  is the, is gonna support the billion dollar valuations of the AI

882
00:51:47,645 --> 00:51:50,485
Speaker 3:  companies. And it's like, I don't actually know about, I don't, I don't know

883
00:51:50,485 --> 00:51:54,405
Speaker 3:  about that. I, I don't know. It's probably broken and I don't believe you.

884
00:51:55,595 --> 00:51:59,485
Speaker 4:  Well this is, this is a good pivot into the humane stuff, which

885
00:51:59,505 --> 00:52:03,405
Speaker 4:  is the other kind of, I don't know, agent might

886
00:52:03,405 --> 00:52:05,525
Speaker 4:  even be too kind to what Humane is trying to do.

887
00:52:05,605 --> 00:52:08,405
Speaker 3:  I mean every company's doing Microsoft announced some agency stuff this week.

888
00:52:08,405 --> 00:52:12,085
Speaker 3:  Yeah. In response to the Anthropic News and some of the Microsoft News

889
00:52:12,865 --> 00:52:16,845
Speaker 3:  OpenAI leaked out that it's working on some agent stuff. Like again,

890
00:52:16,905 --> 00:52:20,445
Speaker 3:  the future of Siri that they have described is a,

891
00:52:20,745 --> 00:52:24,325
Speaker 3:  the, the, I'm so sorry for this word. Age agentic. Yep.

892
00:52:24,685 --> 00:52:27,445
Speaker 3:  Whatever I keeps saying. So the future of Sirius age, agentic

893
00:52:28,375 --> 00:52:31,005
Speaker 3:  Apple has a different way of building it. 'cause they actually have the apps

894
00:52:31,005 --> 00:52:33,085
Speaker 3:  running locally in their operating system and they control the operating

895
00:52:33,085 --> 00:52:37,045
Speaker 3:  system so they can, they can do more API layer stuff, but everyone's

896
00:52:37,245 --> 00:52:41,085
Speaker 3:  headed here. And then Humane sort of like here they are.

897
00:52:41,785 --> 00:52:44,965
Speaker 4:  You skipped right past that by the way. But that is Apple's single biggest

898
00:52:44,965 --> 00:52:48,605
Speaker 4:  advantage in all of this. I'm increasingly convinced is that Apple

899
00:52:48,745 --> 00:52:52,645
Speaker 4:  is going to, with the new Siri, just launch a thing. I think

900
00:52:52,645 --> 00:52:56,325
Speaker 4:  it's either called Siri intense or app intense. And basically it, it

901
00:52:56,505 --> 00:53:00,485
Speaker 4:  allows developers to open up things that Siri can go do inside

902
00:53:00,485 --> 00:53:04,165
Speaker 4:  of their apps. That's the answer, right? Like the, the only

903
00:53:04,165 --> 00:53:08,125
Speaker 4:  reason to go do this much more complicated deterministic thing is If.

904
00:53:08,125 --> 00:53:11,845
Speaker 4:  you can't convince the world to open up a APIs to you

905
00:53:12,185 --> 00:53:16,045
Speaker 4:  and Apple is maybe the only company that can just

906
00:53:16,075 --> 00:53:20,005
Speaker 4:  tell developers to give it access to stuff. And most of them will do it.

907
00:53:20,505 --> 00:53:21,285
Speaker 4:  And I think

908
00:53:21,285 --> 00:53:22,805
Speaker 3:  It's, it's we'll see about that. We

909
00:53:22,805 --> 00:53:25,845
Speaker 4:  Will, it's, it's possible that I'm wrong, but historically speaking,

910
00:53:26,455 --> 00:53:30,125
Speaker 4:  Apple is the only company on earth that tells developers to jump and they

911
00:53:30,125 --> 00:53:31,325
Speaker 4:  jump And Netflix

912
00:53:31,615 --> 00:53:31,965
Speaker 3:  Still

913
00:53:31,965 --> 00:53:35,765
Speaker 5:  Not on Apple TV or Vision Pro or or integrated into the Apple TV app or

914
00:53:35,825 --> 00:53:36,525
Speaker 5:  on Vision Pro.

915
00:53:36,915 --> 00:53:40,845
Speaker 4:  True. That is true. There are not many companies as powerful as Netflix in

916
00:53:40,845 --> 00:53:44,805
Speaker 4:  those negotiations. That's true. But So I think it, it's, if Apple

917
00:53:44,945 --> 00:53:48,445
Speaker 4:  can do that, it puts Apple way ahead because you don't have to solve really

918
00:53:48,445 --> 00:53:52,205
Speaker 4:  complicated a AI problems If, you just are handed the structured data,

919
00:53:52,555 --> 00:53:56,325
Speaker 4:  then Siri has to do the beginning and end. Right. Which is understand what

920
00:53:56,325 --> 00:53:59,965
Speaker 4:  you want and figure out how to go do it. Make sense of the universe is not

921
00:53:59,965 --> 00:54:03,845
Speaker 4:  part of it. And that, that is the hard part. But the humane

922
00:54:03,845 --> 00:54:07,445
Speaker 4:  thing is, so there were two bits of Humane news this week that I very much

923
00:54:07,445 --> 00:54:11,325
Speaker 4:  enjoyed. One is that Humane drastically

924
00:54:11,345 --> 00:54:15,165
Speaker 4:  cut the price of the AI pin from $699 to

925
00:54:15,165 --> 00:54:18,805
Speaker 4:  $499. And if you're saying David, that sounds too expensive, is there's still

926
00:54:18,845 --> 00:54:22,645
Speaker 4:  a $24 a month subscription. The answer is yes it is. There is,

927
00:54:24,385 --> 00:54:28,045
Speaker 4:  but they're still here. They're still trying to ship stuff and do stuff.

928
00:54:28,265 --> 00:54:31,925
Speaker 4:  And the other bit of news that has been,

929
00:54:32,235 --> 00:54:36,005
Speaker 4:  it's like kind of news but kind of not news. Oh, Malick went

930
00:54:36,025 --> 00:54:39,805
Speaker 4:  and met with Bethany and Iran, the co-founders and

931
00:54:40,425 --> 00:54:44,325
Speaker 4:  saw their operating system, which they call Cosmos, but it's os

932
00:54:44,665 --> 00:54:47,645
Speaker 4:  at the end capitalized 'cause it's an operating system. It's very good

933
00:54:48,395 --> 00:54:52,285
Speaker 4:  running on a, some kind of car dashboard. It was not exactly clear,

934
00:54:52,305 --> 00:54:56,285
Speaker 4:  but the idea was Humane now wants to license

935
00:54:56,705 --> 00:55:00,085
Speaker 4:  its AI operating system to other

936
00:55:00,605 --> 00:55:00,765
Speaker 4:  companies.

937
00:55:00,905 --> 00:55:04,805
Speaker 3:  The web os story. Yes, exactly. That's what I said. This is Blackberry, it's

938
00:55:04,805 --> 00:55:06,365
Speaker 3:  Palm, it's web os, you name it.

939
00:55:06,715 --> 00:55:08,205
Speaker 4:  Yeah. Yeah. And so

940
00:55:09,885 --> 00:55:13,605
Speaker 4:  a perfectly reasonable idea. I think there are a lot of

941
00:55:13,755 --> 00:55:17,245
Speaker 4:  bets right now about who the AI operating system is going to be.

942
00:55:18,175 --> 00:55:19,365
Speaker 4:  Every Google would

943
00:55:19,365 --> 00:55:22,445
Speaker 3:  Like it to be generated. Wait, is there, is that a bet? Is that a it's, I

944
00:55:22,445 --> 00:55:22,845
Speaker 3:  don't get it.

945
00:55:23,305 --> 00:55:27,165
Speaker 4:  Amazon is about to spend an awful lot of money and try to

946
00:55:27,325 --> 00:55:30,445
Speaker 4:  convince you to spend some money on the idea that it might be Alexa that

947
00:55:30,585 --> 00:55:34,445
Speaker 4:  can underpin all of your AI devices. Google. That's what Gemini

948
00:55:34,445 --> 00:55:38,245
Speaker 4:  is trying to be. I mean that's what, that's what GPT-4 oh is like, all these

949
00:55:38,525 --> 00:55:42,405
Speaker 4:  companies are going to make most of their money baking their stuff into other

950
00:55:42,405 --> 00:55:46,325
Speaker 4:  products. Like it it that is, that is the win here. If. you can pull it off.

951
00:55:46,325 --> 00:55:49,925
Speaker 3:  That's what I mean. But that's not an os like I'm looking at this

952
00:55:49,985 --> 00:55:53,045
Speaker 3:  humane image of what cause most comos,

953
00:55:55,065 --> 00:55:57,455
Speaker 3:  Cosmo os, whatever it's called Cosmos.

954
00:55:57,455 --> 00:55:58,255
Speaker 4:  It's called Cosmos.

955
00:55:58,365 --> 00:56:01,215
Speaker 3:  Well let's, but the, the the os are is capitals. You

956
00:56:01,215 --> 00:56:03,415
Speaker 4:  Just have to kind of hit it hard. You hit the last cosmos,

957
00:56:05,565 --> 00:56:06,935
Speaker 4:  it's ovn like,

958
00:56:07,405 --> 00:56:07,695
Speaker 3:  Like

959
00:56:10,155 --> 00:56:13,795
Speaker 3:  I don't wanna talk about Ovn at this time, but so you look at their block

960
00:56:13,795 --> 00:56:17,035
Speaker 3:  diagram of how Cosmos works and all they've really done is they've sort of

961
00:56:17,235 --> 00:56:21,075
Speaker 3:  replaced like applications with the word agent and then added like

962
00:56:21,075 --> 00:56:24,885
Speaker 3:  15 more blocks on top of that that are basically like, we

963
00:56:24,915 --> 00:56:27,885
Speaker 3:  hear you speak and then we understand what you want. and we go to one of

964
00:56:27,885 --> 00:56:31,685
Speaker 3:  these agents and it, the part where you replace the application layer

965
00:56:31,685 --> 00:56:35,485
Speaker 3:  with agents is a, is a big deal. If that's a thing

966
00:56:35,485 --> 00:56:39,125
Speaker 3:  that's gonna happen that is like a 50 year industrywide project,

967
00:56:39,985 --> 00:56:43,565
Speaker 3:  not a thing. You can ship to HP to put in a car,

968
00:56:44,295 --> 00:56:47,085
Speaker 3:  which is kind of what the article says is gonna

969
00:56:48,315 --> 00:56:51,925
Speaker 3:  like, it's very confusing, easy. You still need

970
00:56:51,925 --> 00:56:55,285
Speaker 3:  applications. Sure. Like you still need a Spotify

971
00:56:56,065 --> 00:56:58,285
Speaker 3:  to exist so you can like click on it.

972
00:56:59,145 --> 00:56:59,365
Speaker 4:  Yes.

973
00:56:59,665 --> 00:57:03,605
Speaker 3:  As long as you need Spotify to exist. The easiest way to play the music

974
00:57:03,945 --> 00:57:07,685
Speaker 3:  is to pay Spotify money to use the API to just play the music.

975
00:57:08,305 --> 00:57:10,485
Speaker 3:  Rivian ISS gonna get Apple Music. And you're like, cool. And I was like,

976
00:57:10,485 --> 00:57:14,165
Speaker 3:  here's how they're gonna do it. You're gonna ask Alexa to open a web browser

977
00:57:15,145 --> 00:57:19,045
Speaker 3:  and open Apple Music and click on Apple Music and App. You'd be like, what?

978
00:57:19,105 --> 00:57:23,045
Speaker 3:  And I'd be like, or you can sign a deal with Apple to put the Apple Music

979
00:57:23,185 --> 00:57:27,165
Speaker 3:  app running on the Rivian. Right. Which is what they did. Because that is

980
00:57:27,485 --> 00:57:31,165
Speaker 3:  sane. And like I, there's no reason that

981
00:57:31,665 --> 00:57:34,645
Speaker 3:  you have to give up on the way that we've run computers up until now

982
00:57:35,605 --> 00:57:39,175
Speaker 3:  just because you need to license your It's I'm deeply confused by this.

983
00:57:39,845 --> 00:57:43,815
Speaker 4:  Yeah, I I it's, it's, I think the same argument we've had a

984
00:57:43,815 --> 00:57:47,495
Speaker 4:  lot about all of these AI devices is like, isn't this just another app on

985
00:57:47,495 --> 00:57:51,095
Speaker 4:  your phone? Like, is could, could an AI system

986
00:57:51,245 --> 00:57:55,135
Speaker 4:  like credibly replaceable a lot of what you do in Android Auto or CarPlay

987
00:57:55,135 --> 00:57:58,935
Speaker 4:  over time, like Yes, absolutely. Is is good Siri, the solution to

988
00:57:58,935 --> 00:58:02,495
Speaker 4:  CarPlay a hundred percent. That is not

989
00:58:02,795 --> 00:58:04,295
Speaker 4:  the future of iOS.

990
00:58:05,875 --> 00:58:09,375
Speaker 4:  It, it's part of it. Right. And I think there are, there are a lot of instances

991
00:58:09,475 --> 00:58:13,215
Speaker 4:  in which agents are going to be useful and powerful and valuable,

992
00:58:13,995 --> 00:58:17,375
Speaker 4:  but the idea that they're going to like immediately overthrow

993
00:58:18,115 --> 00:58:22,095
Speaker 4:  the whole ecosystem we currently have is just wrong.

994
00:58:22,475 --> 00:58:26,335
Speaker 4:  And So, I think again, if you're, if you're an OS maker and,

995
00:58:26,435 --> 00:58:29,775
Speaker 4:  and if you're humane and you're desperate to have a reason to stay alive,

996
00:58:30,955 --> 00:58:34,735
Speaker 4:  you have to make this bet, right? Like the, the big swing is we are

997
00:58:34,735 --> 00:58:38,255
Speaker 4:  going to be the one who controls all the agents. And If, you're the one who

998
00:58:38,255 --> 00:58:41,455
Speaker 4:  controls all the agents. You, you become very powerful. The problem is there

999
00:58:41,455 --> 00:58:45,295
Speaker 4:  aren't any agents. The ones that there are, aren't very good. And also no

1000
00:58:45,295 --> 00:58:49,135
Speaker 4:  one needs them yet. Yeah. So like, it's a, it's a tough sled.

1001
00:58:49,295 --> 00:58:49,415
Speaker 4:  I

1002
00:58:49,415 --> 00:58:52,455
Speaker 5:  Think the thing I struggle with every time we we get into this discussion

1003
00:58:52,455 --> 00:58:55,935
Speaker 5:  or we're talking about these things is let's say that it works perfectly.

1004
00:58:55,935 --> 00:58:59,655
Speaker 5:  That they get all the bugs out. That it, it's smart, it's fast, it's

1005
00:58:59,815 --> 00:59:03,415
Speaker 5:  efficient, it's still like clicking around and using a thing. How many times

1006
00:59:03,575 --> 00:59:06,095
Speaker 5:  a day do you click on something and like the page doesn't load?

1007
00:59:07,565 --> 00:59:11,135
Speaker 5:  Like, and, and maybe they can make it smart enough to, okay, now I need to

1008
00:59:11,135 --> 00:59:13,935
Speaker 5:  refresh, or I got an error and I need to go back and I need to do it again.

1009
00:59:14,075 --> 00:59:17,935
Speaker 5:  But like, if I'm just sitting there saying, Hey man, I told you to do the

1010
00:59:17,935 --> 00:59:20,375
Speaker 5:  thing. And it's like, yeah, the page isn't loading. Sorry bro.

1011
00:59:21,405 --> 00:59:25,245
Speaker 3:  Honestly, if my computer would be more honest with me about why it's not

1012
00:59:25,245 --> 00:59:29,165
Speaker 3:  working in that specific way, in that specific tone that would rule.

1013
00:59:30,035 --> 00:59:33,205
Speaker 3:  Like it was like, I'm sorry bro, like this connection's shit,

1014
00:59:33,685 --> 00:59:37,205
Speaker 4:  That's what Siri should do whenever instead of being like, here's some stuff

1015
00:59:37,285 --> 00:59:40,165
Speaker 4:  I found on the web, it should just be like, sorry bro. I don't know. Yeah.

1016
00:59:40,165 --> 00:59:40,525
Speaker 3:  Like I,

1017
00:59:40,725 --> 00:59:41,085
Speaker 4:  I would like

1018
00:59:41,085 --> 00:59:44,365
Speaker 3:  It better. Do you have Verizon? 'cause at and t's sucking right now. Yeah.

1019
00:59:44,365 --> 00:59:46,365
Speaker 3:  That would be, I'm incredible.

1020
00:59:46,835 --> 00:59:47,525
Speaker 4:  Yeah. Or just

1021
00:59:47,665 --> 00:59:48,565
Speaker 3:  That's what AI is for.

1022
00:59:48,615 --> 00:59:52,285
Speaker 4:  Think for a second. And then it's just like, I don't know, does it matter?

1023
00:59:52,385 --> 00:59:52,605
Speaker 4:  Who

1024
00:59:52,605 --> 00:59:55,685
Speaker 3:  Cares? The vibes are off. The vibes are off. Vibes are off. I just like,

1025
00:59:55,745 --> 00:59:58,885
Speaker 3:  I'm very curious about all these like AI agents, everyone's pitching them

1026
00:59:58,945 --> 01:00:02,775
Speaker 3:  now because they have to like, I, I really think

1027
01:00:02,775 --> 01:00:06,735
Speaker 3:  the business imperative for why are we spending all this money?

1028
01:00:06,835 --> 01:00:10,775
Speaker 3:  Why are we running all these GPUs all day and all night is not chatbots,

1029
01:00:11,005 --> 01:00:14,745
Speaker 3:  it's not image generators, it's not disinformation at scale.

1030
01:00:15,655 --> 01:00:17,785
Speaker 4:  Like it's a little bit disinformation at scale.

1031
01:00:17,925 --> 01:00:20,505
Speaker 3:  That's like a, that's like fun, you know, it's like a, it's like a fun little,

1032
01:00:20,695 --> 01:00:24,625
Speaker 3:  it's, it's like the garnish like of the business. You know, like

1033
01:00:25,045 --> 01:00:27,905
Speaker 3:  do you wanna see a picture of Elon Musk jumping up and down and turning into

1034
01:00:27,905 --> 01:00:30,865
Speaker 3:  a monkey and then exploding? We can do that for you. Is that a business?

1035
01:00:32,165 --> 01:00:35,805
Speaker 3:  I don't know, like it does not appear to be paying anyone's bills,

1036
01:00:36,465 --> 01:00:36,685
Speaker 3:  but

1037
01:00:38,495 --> 01:00:41,935
Speaker 3:  I can make you an all powerful robot that can use a computer for you and

1038
01:00:42,045 --> 01:00:45,295
Speaker 3:  fill out and do this spreadsheet work that you don't wanna do.

1039
01:00:45,825 --> 01:00:48,655
Speaker 3:  Maybe maybe someone will pay a lot of money for that. And so you just like

1040
01:00:48,655 --> 01:00:52,055
Speaker 3:  see them charging towards, well hopefully we can pull this off.

1041
01:00:52,165 --> 01:00:55,375
Speaker 3:  Hopefully we can get so good at this to the business basis for it. And I

1042
01:00:55,795 --> 01:00:58,935
Speaker 3:  you are gonna, we're gonna cover it a lot. 'cause it, it is the dream, right?

1043
01:00:58,935 --> 01:01:01,735
Speaker 3:  You just talk to a computer and goes off and does something for you. I'm

1044
01:01:01,735 --> 01:01:04,215
Speaker 3:  just not sure any of this technology can actually pull it off yet. And then

1045
01:01:04,215 --> 01:01:06,855
Speaker 3:  underneath it, this is the next thing we should talk about is the fact that

1046
01:01:06,875 --> 01:01:10,735
Speaker 3:  all of this is built on a bunch of models that

1047
01:01:10,735 --> 01:01:14,455
Speaker 3:  are trained in a much of copyrighted information that none of the

1048
01:01:14,455 --> 01:01:18,295
Speaker 3:  lawsuits have been resolved yet and more of them show up every day. So

1049
01:01:18,295 --> 01:01:21,175
Speaker 3:  just this week, news Corp, which owns the Wall Street Journal and the New York

1050
01:01:21,175 --> 01:01:24,815
Speaker 3:  Post, it's Murdoch Empire Company,

1051
01:01:25,375 --> 01:01:29,175
Speaker 3:  they sued perplexity for infringing on all of its content. Perplexity gets

1052
01:01:29,175 --> 01:01:32,695
Speaker 3:  to their paywall. Perplexity is sad. Perplexity is sort of like

1053
01:01:32,695 --> 01:01:34,895
Speaker 3:  perpetually sad that it's getting sued for copyright infringement.

1054
01:01:35,065 --> 01:01:38,615
Speaker 4:  Perplexity is the one that Wired and Forbes and others have

1055
01:01:39,375 --> 01:01:43,255
Speaker 4:  repeatedly also pointed out is I think the wired headline was Perplexity

1056
01:01:43,255 --> 01:01:46,495
Speaker 4:  is a Bullshit Machine, which is very good. Yeah. And then wrote another story

1057
01:01:46,495 --> 01:01:50,295
Speaker 4:  about how Perplexity stole the story about

1058
01:01:50,295 --> 01:01:52,495
Speaker 4:  perplexity being a bullshit. And it was just very good.

1059
01:01:54,285 --> 01:01:58,095
Speaker 4:  This and Perplexity so far has basically been like, ah, it's the

1060
01:01:58,215 --> 01:02:01,935
Speaker 4:  internet, everything's probably fine. We can just have it. Yep. And

1061
01:02:02,285 --> 01:02:05,775
Speaker 4:  it's really only been a matter of time until this came for perplexity in

1062
01:02:05,775 --> 01:02:08,815
Speaker 4:  a pretty big way. I think it was, was it this week or last week that the

1063
01:02:08,815 --> 01:02:12,735
Speaker 4:  New York Times sent a cease and desist to perplexity essentially alleging

1064
01:02:12,735 --> 01:02:15,255
Speaker 4:  the same thing. Yep. Is that last week? It was last week. It

1065
01:02:15,255 --> 01:02:18,895
Speaker 3:  Was last week. and we should know the Times also sued Open ai. That's an

1066
01:02:18,925 --> 01:02:22,445
Speaker 3:  ongoing lawsuit. Our company, Vox Media, the business side of the company

1067
01:02:22,445 --> 01:02:26,365
Speaker 3:  signed a deal with OpenAI, as did Conde Nast, as did a bunch of

1068
01:02:26,365 --> 01:02:29,525
Speaker 3:  other publishers. Like this is in the air, like

1069
01:02:30,185 --> 01:02:32,485
Speaker 3:  that's a disclosure by the way. You have nothing to do with that deal. But

1070
01:02:32,485 --> 01:02:34,845
Speaker 3:  this is just like in the air. The Atlantic signed a deal like that. And they

1071
01:02:35,045 --> 01:02:39,005
Speaker 3:  actually have a cool set of open AI powered

1072
01:02:39,005 --> 01:02:42,965
Speaker 3:  tools on their website, one of which is just a Chrome plugin that

1073
01:02:43,145 --> 01:02:46,805
Speaker 3:  as you click, like as you browse the web, it shows you

1074
01:02:47,125 --> 01:02:51,115
Speaker 3:  relevant stories from like the 150 year history of the Atlantic. Which

1075
01:02:51,115 --> 01:02:54,755
Speaker 3:  is neat. Like yeah, I wouldn't read Old Atlantic stories if not for an ai.

1076
01:02:54,755 --> 01:02:58,675
Speaker 3:  Like it does, it works like c plus let's not get ahead of

1077
01:02:58,675 --> 01:03:02,315
Speaker 3:  ourselves, but like they're building stuff with it. Yeah. But this is the,

1078
01:03:02,975 --> 01:03:06,475
Speaker 3:  the foundation of this entire industry is like copyright infringement or

1079
01:03:06,475 --> 01:03:09,955
Speaker 3:  alleged copyright infringement just this week as well. Kevin Bacon, Kate

1080
01:03:09,955 --> 01:03:13,155
Speaker 3:  McKinnon, 11,500 other creators

1081
01:03:14,865 --> 01:03:18,755
Speaker 3:  just like issued a letter saying this is an unjust value

1082
01:03:19,115 --> 01:03:22,675
Speaker 3:  transfer. We made all this work. We, this is our life's work.

1083
01:03:22,895 --> 01:03:26,675
Speaker 3:  You've taken it all and now you're supporting these valuations without giving

1084
01:03:26,675 --> 01:03:30,405
Speaker 3:  us anything in return. You can feel about that any way you want. I know a

1085
01:03:30,405 --> 01:03:34,165
Speaker 3:  lot of people who feel like this is history's greatest crime and Sam Altman

1086
01:03:34,165 --> 01:03:37,725
Speaker 3:  should be in jail. I know those people exist because they write to us every

1087
01:03:37,725 --> 01:03:40,765
Speaker 3:  time we talk about this on The Vergecast, we, we welcome you here. Then there's

1088
01:03:40,765 --> 01:03:42,885
Speaker 3:  another group of people who say, well look, it's just a computer reading

1089
01:03:42,885 --> 01:03:45,365
Speaker 3:  everything in the world. The same way that If, you could read everything

1090
01:03:45,365 --> 01:03:48,405
Speaker 3:  in the world and then summarize iMessage notifications. You would do the

1091
01:03:48,405 --> 01:03:52,325
Speaker 3:  same thing, which is a stretch but also an argument you can make.

1092
01:03:52,425 --> 01:03:55,725
Speaker 3:  And I have no, I, this is a coin flip, pure coin flip. I have no idea how

1093
01:03:55,725 --> 01:03:59,565
Speaker 3:  this is gonna take off. But If, you believe that the future of Alexa and

1094
01:03:59,635 --> 01:04:02,845
Speaker 3:  Siri and Google Assistant and Humane and Anthropic

1095
01:04:03,465 --> 01:04:07,105
Speaker 3:  is do like taking a million screenshots of your computers and clicking around

1096
01:04:07,105 --> 01:04:10,825
Speaker 3:  for you. All of that is built on the foundation of copyright lawsuits that

1097
01:04:11,105 --> 01:04:14,145
Speaker 3:  have not yet been resolved in any way, shape or form. And it's kind of like,

1098
01:04:14,805 --> 01:04:18,345
Speaker 3:  you know, this industry is headed towards the cliff, like full cliff,

1099
01:04:18,865 --> 01:04:19,025
Speaker 3:  like

1100
01:04:19,245 --> 01:04:19,905
Speaker 4:  Yes. One way

1101
01:04:19,905 --> 01:04:20,145
Speaker 3:  Or another.

1102
01:04:20,325 --> 01:04:23,305
Speaker 4:  The one other part of this backdrop that we should talk about is

1103
01:04:24,125 --> 01:04:27,545
Speaker 4:  all of these companies right now are trying to raise gigantic,

1104
01:04:28,665 --> 01:04:32,505
Speaker 4:  astonishing amounts of money. OpenAI just raised the biggest round of

1105
01:04:32,505 --> 01:04:36,425
Speaker 4:  funding ever. Anthropic is out trying to raise a ton

1106
01:04:36,425 --> 01:04:39,385
Speaker 4:  of money. Perplexity is out trying to raise a ton of money. Like there, there's

1107
01:04:39,385 --> 01:04:42,905
Speaker 4:  a real sense from all of these companies that they are going to need just

1108
01:04:43,215 --> 01:04:46,905
Speaker 4:  vast oceans of cash to do some combination of build

1109
01:04:47,005 --> 01:04:50,985
Speaker 4:  God, find a way to sell this stuff to enterprise customers

1110
01:04:51,485 --> 01:04:55,225
Speaker 4:  and protect themselves from lawsuits. Like I, I think I

1111
01:04:55,225 --> 01:04:59,105
Speaker 4:  every one of these companies is trying to make the case that this

1112
01:04:59,105 --> 01:05:02,945
Speaker 4:  stuff is going to be so big and so lucrative in order to be

1113
01:05:02,975 --> 01:05:06,745
Speaker 4:  able to fund the fight to get there. And it's such a

1114
01:05:06,745 --> 01:05:10,505
Speaker 4:  bizarre dynamic where it's like they're simultaneously promising

1115
01:05:10,605 --> 01:05:14,105
Speaker 4:  how great the world is gonna be and like battening down the hatches, right?

1116
01:05:14,105 --> 01:05:17,130
Speaker 4:  And they're like, Microsoft and open AI this week was another piece of, of

1117
01:05:17,130 --> 01:05:21,045
Speaker 4:  news that they're giving a bunch of money to newsrooms

1118
01:05:21,665 --> 01:05:25,645
Speaker 4:  to make them make more AI stuff. And it's just like, what,

1119
01:05:25,875 --> 01:05:29,565
Speaker 4:  what a bizarre inversion of this same thing where they're like,

1120
01:05:30,105 --> 01:05:34,045
Speaker 4:  we are gonna give you money to take your stuff or we're just gonna

1121
01:05:34,045 --> 01:05:36,485
Speaker 4:  take it for free, but here's some money to make AI stuff.

1122
01:05:37,275 --> 01:05:40,645
Speaker 5:  Yeah, well they're, they're giving some money. They announced $10 million

1123
01:05:40,705 --> 01:05:43,645
Speaker 5:  and 5 million of it is in credits for the software.

1124
01:05:44,305 --> 01:05:46,285
Speaker 4:  Oh, is it really? That's annoying. Which

1125
01:05:46,285 --> 01:05:48,485
Speaker 5:  Is not the same thing as money. Exactly.

1126
01:05:48,545 --> 01:05:52,285
Speaker 4:  No, it's not. It is super not, but all of this stuff is like, it's,

1127
01:05:53,015 --> 01:05:56,885
Speaker 4:  everything is running as fast as possible in these

1128
01:05:56,885 --> 01:06:00,285
Speaker 4:  like mutually exclusive ways

1129
01:06:01,025 --> 01:06:04,765
Speaker 4:  except that it sort of feels like if the money gets big enough,

1130
01:06:04,935 --> 01:06:06,405
Speaker 4:  it'll just be okay. That,

1131
01:06:06,585 --> 01:06:10,445
Speaker 3:  So this is, I I haven't thought this all the way through and

1132
01:06:10,465 --> 01:06:12,285
Speaker 3:  I'm curious for reader feedback on this. What

1133
01:06:12,285 --> 01:06:13,045
Speaker 4:  The first cast is for,

1134
01:06:13,625 --> 01:06:17,005
Speaker 3:  If the money gets big enough, I think the judges and the copyright cases

1135
01:06:17,185 --> 01:06:19,965
Speaker 3:  are gonna be like, look at all of your money.

1136
01:06:21,025 --> 01:06:24,955
Speaker 3:  Pay it to them. Like at some point you're like, I made

1137
01:06:24,995 --> 01:06:28,635
Speaker 3:  a thing so valuable by taking all of this stuff without

1138
01:06:28,635 --> 01:06:32,475
Speaker 3:  permission. A judge is gonna say, well, that, that is actually a

1139
01:06:32,475 --> 01:06:36,425
Speaker 3:  value transfer That doesn't make sense, right? If, you needed all of

1140
01:06:36,425 --> 01:06:39,505
Speaker 3:  this to raise your $6.9 billion

1141
01:06:40,565 --> 01:06:44,145
Speaker 3:  OpenAI. Then it stands to reason that that stuff is

1142
01:06:44,425 --> 01:06:47,505
Speaker 3:  valuable and you should take some of the money you raised and pay it to the

1143
01:06:47,505 --> 01:06:50,745
Speaker 3:  people whose work you need to make your product that supports the valuation.

1144
01:06:51,285 --> 01:06:51,505
Speaker 3:  But

1145
01:06:51,505 --> 01:06:54,225
Speaker 4:  That's kind of my point, right? I think if you're open ai, you say, okay,

1146
01:06:54,365 --> 01:06:57,505
Speaker 4:  listen judge, fine us fine us

1147
01:06:57,685 --> 01:07:01,625
Speaker 4:  $1 billion. That's so much money. One whole billion

1148
01:07:01,625 --> 01:07:05,545
Speaker 4:  dollars. Oh it's cool. We got five and a half other and

1149
01:07:05,545 --> 01:07:08,465
Speaker 4:  this, this is what I mean. Like I think there is a sense among these companies

1150
01:07:08,465 --> 01:07:12,345
Speaker 4:  that if they can just build a big enough war chest, they can either buy

1151
01:07:12,345 --> 01:07:15,585
Speaker 4:  out or wait out all of their problems and that those are tech problems. Those

1152
01:07:15,585 --> 01:07:19,145
Speaker 4:  are regulatory problems. Those are fights with publishers. Like that just

1153
01:07:19,145 --> 01:07:22,825
Speaker 4:  gives them the money to wait. And I I I,

1154
01:07:22,825 --> 01:07:26,345
Speaker 4:  there's a reasonable evidence that they're gonna be right about that.

1155
01:07:26,805 --> 01:07:30,545
Speaker 4:  The the wild card is, are we due for some

1156
01:07:30,615 --> 01:07:31,065
Speaker 4:  kind of

1157
01:07:32,585 --> 01:07:36,065
Speaker 4:  judicial end to this that just knife through the whole industry And that

1158
01:07:36,225 --> 01:07:40,065
Speaker 4:  I think everybody is lying to you if they think they know the

1159
01:07:40,065 --> 01:07:40,225
Speaker 4:  answer.

1160
01:07:40,455 --> 01:07:41,865
Speaker 3:  Yeah, that's definitely a coin flip.

1161
01:07:41,925 --> 01:07:44,105
Speaker 5:  The the other thing that's happening is that they're also getting attacked

1162
01:07:44,105 --> 01:07:48,065
Speaker 5:  from the other side on the, the output side. Not just the copyright

1163
01:07:48,065 --> 01:07:51,745
Speaker 5:  infringement accusations of what they've done, but who is responsible for

1164
01:07:51,745 --> 01:07:55,505
Speaker 5:  what these tools do? There's a lawsuit that has been publicized written by

1165
01:07:55,505 --> 01:07:58,825
Speaker 5:  the New York Times and other outlets we wrote about it where character, AI

1166
01:07:58,825 --> 01:08:02,625
Speaker 5:  and Google are being sued after this, this teen died. And

1167
01:08:02,885 --> 01:08:06,795
Speaker 5:  one of the main questions about this is this. So if I interact with

1168
01:08:06,795 --> 01:08:10,555
Speaker 5:  your generative AI tool, and maybe I put in a prompt and it says something

1169
01:08:10,555 --> 01:08:14,315
Speaker 5:  back to me, is anyone legally responsible for what happens next? Who, how

1170
01:08:14,315 --> 01:08:18,215
Speaker 5:  much, what, what are you supposed to do? What, what should be the rules there?

1171
01:08:18,215 --> 01:08:21,375
Speaker 5:  What should, what safety should you implement? Should these things be usable

1172
01:08:21,375 --> 01:08:24,665
Speaker 5:  by children? None of these questions have been answered yet and they're,

1173
01:08:24,855 --> 01:08:26,025
Speaker 5:  that could be another problem.

1174
01:08:26,535 --> 01:08:30,025
Speaker 3:  Yeah. And that is the realist problem, especially for

1175
01:08:30,485 --> 01:08:33,985
Speaker 3:  the character as of the world where there's sort of like advertising that

1176
01:08:33,985 --> 01:08:37,965
Speaker 3:  these are therapy tools and it's like, well If,

1177
01:08:37,965 --> 01:08:40,325
Speaker 3:  you lead people to these outcomes, you're probably responsible for them.

1178
01:08:40,785 --> 01:08:44,685
Speaker 3:  And no one has thought any of this through. I'm just saying it's neat to

1179
01:08:44,685 --> 01:08:48,605
Speaker 3:  talk about agents and Siri or Alexa that

1180
01:08:48,605 --> 01:08:51,205
Speaker 3:  can just do stuff for you. And then there's like, oh shit. It's doing stuff

1181
01:08:51,225 --> 01:08:55,125
Speaker 3:  for you. Also, the foundation of this might be Kevin. Bacon

1182
01:08:55,155 --> 01:08:58,045
Speaker 3:  gets a billion dollars like weird,

1183
01:08:58,935 --> 01:09:00,325
Speaker 3:  weird outcome. Can I

1184
01:09:00,325 --> 01:09:03,805
Speaker 4:  Just read you the one sentence thing that they all signed onto?

1185
01:09:04,155 --> 01:09:07,405
Speaker 4:  Yeah. It says the unlicensed use of creative works for training generative

1186
01:09:07,465 --> 01:09:10,725
Speaker 4:  AI is a major unjust threat to the livelihoods of the people behind those

1187
01:09:10,965 --> 01:09:12,565
Speaker 4:  works and must not be permitted.

1188
01:09:14,415 --> 01:09:18,025
Speaker 4:  Okay. Like I'm just, I'm amazed they got

1189
01:09:18,025 --> 01:09:21,305
Speaker 4:  11,500 people to care about that. Enough to sign.

1190
01:09:21,485 --> 01:09:24,265
Speaker 3:  Oh no. A piece of paper. No, we could get 11,000 people to sign a statement

1191
01:09:24,265 --> 01:09:27,985
Speaker 3:  like tomorrow. The Verge commenters alone would sign that statement.

1192
01:09:28,335 --> 01:09:28,625
Speaker 3:  It's

1193
01:09:28,625 --> 01:09:32,545
Speaker 4:  Just such a like, I don't know, it's just such a milk toast way to

1194
01:09:32,545 --> 01:09:34,905
Speaker 4:  be like, I am gently mad at the AI industry.

1195
01:09:35,865 --> 01:09:39,785
Speaker 3:  I don't think that's gentle. It's a major unjust threat that must not be

1196
01:09:39,785 --> 01:09:43,135
Speaker 3:  permitted. What do you want them to say? Sam Altman should be arrested

1197
01:09:43,155 --> 01:09:44,735
Speaker 3:  11,000 people. It's

1198
01:09:44,735 --> 01:09:45,855
Speaker 4:  Just like money. Please.

1199
01:09:46,835 --> 01:09:48,575
Speaker 3:  No, it says must not be permitted.

1200
01:09:49,075 --> 01:09:53,015
Speaker 4:  It says livelihoods. You know what that is? It's, this is not, it's

1201
01:09:53,015 --> 01:09:55,415
Speaker 4:  not. It's money. Money.

1202
01:09:56,235 --> 01:09:59,415
Speaker 5:  The unfortunate part is that it already happened. Yeah. So

1203
01:09:59,415 --> 01:10:02,815
Speaker 3:  Yeah. Cat's outta the bag. Kevin Bacon Bacon. No, Dan in this town, bro.

1204
01:10:03,165 --> 01:10:07,135
Speaker 4:  Yeah. Open AI raised $7 billion so that it can give Kevin Bacon one of

1205
01:10:07,135 --> 01:10:08,455
Speaker 4:  them and then it can move on.

1206
01:10:10,315 --> 01:10:12,855
Speaker 3:  The other thing that's going on in the AI world, which we just skim over

1207
01:10:13,095 --> 01:10:16,295
Speaker 3:  'cause it is an election year and everyone can see the weird AI election

1208
01:10:16,295 --> 01:10:19,615
Speaker 3:  misinformation for their own eyes, is that the, the companies that make these

1209
01:10:19,615 --> 01:10:23,535
Speaker 3:  tools and have promised labeling and metadata for years and

1210
01:10:23,535 --> 01:10:27,175
Speaker 3:  have done nothing while all of this is getting fully outta control,

1211
01:10:27,595 --> 01:10:31,335
Speaker 3:  are slowly starting to add labels and metadata to their own work.

1212
01:10:31,995 --> 01:10:35,775
Speaker 3:  So Google is gonna open source its watermarking tool for AI

1213
01:10:35,775 --> 01:10:38,935
Speaker 3:  generated text. We'll see how that goes. That one's is, that's the hardest

1214
01:10:39,035 --> 01:10:42,775
Speaker 3:  one. But Google Photos is now gonna show AI

1215
01:10:42,775 --> 01:10:46,755
Speaker 3:  generated metadata in the photos. Like there'll be a label if we

1216
01:10:46,755 --> 01:10:49,955
Speaker 3:  can detect the metadata. We're just, it's a slow,

1217
01:10:51,215 --> 01:10:55,025
Speaker 3:  slow burn. And then, oh, apple Craig Feder in that same interview, Joanna,

1218
01:10:55,205 --> 01:10:58,705
Speaker 3:  so that he wants photos to be photos. He doesn't want real photos to be fantasy

1219
01:10:59,205 --> 01:11:03,025
Speaker 3:  and that they are also showing some information in the photos. This is

1220
01:11:03,045 --> 01:11:06,905
Speaker 3:  not enough flatly. None of these companies are doing enough. They've all

1221
01:11:07,285 --> 01:11:11,215
Speaker 3:  talked a huge game about things like C two

1222
01:11:11,295 --> 01:11:15,205
Speaker 3:  PA and content authenticity. And we're kind of at like, we're

1223
01:11:15,205 --> 01:11:19,125
Speaker 3:  gonna do exif data and labels that people can

1224
01:11:19,125 --> 01:11:22,925
Speaker 3:  change while the misinformation is like running rampant everywhere. Has

1225
01:11:22,925 --> 01:11:23,125
Speaker 3:  Craig

1226
01:11:23,125 --> 01:11:25,925
Speaker 5:  Federighi seen that, seen that photo The bridesmaid with like three arms?

1227
01:11:26,565 --> 01:11:28,925
Speaker 5:  Because somebody should tell him about that. Yeah,

1228
01:11:29,145 --> 01:11:33,045
Speaker 3:  But that was like a panorama mode thing, right? It wasn't even ai.

1229
01:11:33,185 --> 01:11:34,405
Speaker 3:  It wasn't even ai. It's

1230
01:11:34,405 --> 01:11:38,005
Speaker 4:  Just the, the pace of all of that is so backwards to me because what the,

1231
01:11:38,065 --> 01:11:41,445
Speaker 4:  the logic for going slow on all of this from all of these companies is like,

1232
01:11:41,445 --> 01:11:45,125
Speaker 4:  we have to really test this stuff and harden it and make sure it works

1233
01:11:45,545 --> 01:11:47,685
Speaker 4:  and make sure it can't be,

1234
01:12:18,915 --> 01:12:20,005
Speaker 3:  Fire, fire

1235
01:13:49,145 --> 01:13:52,365
Speaker 3:  If assigned to Richard. I'm gonna just gonna be very honest with you, I'm

1236
01:13:52,365 --> 01:13:53,685
Speaker 3:  very excited. We'll be right back.

1237
01:13:58,185 --> 01:14:01,865
Speaker 3:  All right, we're back. The lightning round sponsored by no one,

1238
01:14:02,765 --> 01:14:05,345
Speaker 3:  no one for all of your not not money needs.

1239
01:14:06,595 --> 01:14:10,455
Speaker 3:  See, see what I yet that's what it could look like. If, you paid

1240
01:14:10,455 --> 01:14:14,445
Speaker 3:  us money. Someone money, not me. We'll get there one

1241
01:14:14,445 --> 01:14:15,685
Speaker 3:  day. All right. It's lightning around.

1242
01:14:17,385 --> 01:14:21,375
Speaker 3:  David, I think you need to start with honestly the news that

1243
01:14:21,375 --> 01:14:22,575
Speaker 3:  should have led the entire vergecast.

1244
01:14:23,215 --> 01:14:26,815
Speaker 4:  I just want you both to be very proud of me that I made it o over an hour

1245
01:14:26,885 --> 01:14:30,295
Speaker 4:  into The Vergecast. And I have not mentioned the books Palmer two once. I

1246
01:14:30,295 --> 01:14:32,175
Speaker 4:  haven't even, I haven't even teased it.

1247
01:14:32,435 --> 01:14:36,095
Speaker 3:  The best thing about the BWA two is that we've entered the stage of covering

1248
01:14:36,605 --> 01:14:40,575
Speaker 3:  this thing where we, there was a leak of it and we covered the leak

1249
01:14:40,675 --> 01:14:43,935
Speaker 3:  and then the announcement of the tiny e-reader

1250
01:14:44,805 --> 01:14:46,595
Speaker 5:  We're, we're deep in the weeds. The Verge

1251
01:14:46,595 --> 01:14:50,515
Speaker 4:  Is the official news source of the

1252
01:14:50,515 --> 01:14:52,715
Speaker 4:  book's Palmer and I feel very good about this. What I

1253
01:14:52,715 --> 01:14:56,355
Speaker 5:  Like is that they got the sequel out fast enough that my prescheduled six

1254
01:14:56,355 --> 01:14:59,515
Speaker 5:  month out tweet asking people if they know where their books Palmer is,

1255
01:14:59,935 --> 01:15:03,075
Speaker 5:  hasn't even gone up yet. And they're already selling the sequel. They, I

1256
01:15:03,075 --> 01:15:03,995
Speaker 5:  got, they got me on this one.

1257
01:15:04,515 --> 01:15:08,395
Speaker 4:  Somebody posted on Threads today accusing me of being in

1258
01:15:08,395 --> 01:15:12,275
Speaker 4:  league with books to get rid of all their inventory on the, on the one

1259
01:15:12,275 --> 01:15:16,075
Speaker 4:  before the two came out. And they're like, you, you timed this on Burs,

1260
01:15:16,075 --> 01:15:16,355
Speaker 4:  didn't you?

1261
01:15:16,495 --> 01:15:17,155
Speaker 5:  Big books is

1262
01:15:17,815 --> 01:15:21,555
Speaker 4:  Big books, yeah. That's, that's, that's me. They, they fund everything.

1263
01:15:21,605 --> 01:15:25,235
Speaker 4:  Everything in the basketball hoop you see behind me. So the books

1264
01:15:25,255 --> 01:15:28,715
Speaker 4:  Palmer two still $280, which is still too expensive,

1265
01:15:29,335 --> 01:15:33,235
Speaker 4:  but it's, it's still, it's a smartphone sized e-reader. To answer

1266
01:15:33,235 --> 01:15:36,475
Speaker 4:  your question, Richard, I use mine every single day and I love it

1267
01:15:37,105 --> 01:15:40,635
Speaker 4:  very much. It fixes a couple of the things that were wrong with the last

1268
01:15:40,655 --> 01:15:44,195
Speaker 4:  one. Mostly it was a little slow and it ran a really

1269
01:15:44,555 --> 01:15:48,435
Speaker 4:  outdated version of Android. Now I suspect it will be less slow and it runs

1270
01:15:48,475 --> 01:15:51,795
Speaker 4:  a less outdated version of Android. It doesn't run a current version of Android,

1271
01:15:52,095 --> 01:15:53,235
Speaker 4:  but it's better.

1272
01:15:55,025 --> 01:15:58,555
Speaker 4:  It's still, it's the same size, it's the same screen.

1273
01:15:58,895 --> 01:16:02,235
Speaker 4:  It has, I believe, the same amount of ram and the same amount of storage.

1274
01:16:02,505 --> 01:16:06,395
Speaker 4:  It's just like a gentle upgrade of the same

1275
01:16:06,695 --> 01:16:07,115
Speaker 4:  device,

1276
01:16:08,975 --> 01:16:12,845
Speaker 4:  which I think is fine. I, to me, honestly, the fact that books just made

1277
01:16:12,845 --> 01:16:16,445
Speaker 4:  another one of these is the thing that is most exciting and

1278
01:16:16,765 --> 01:16:20,325
Speaker 4:  interesting books is basically like a spaghetti at the wall kind of company.

1279
01:16:20,325 --> 01:16:24,045
Speaker 4:  They just make every single product size

1280
01:16:24,425 --> 01:16:25,285
Speaker 4:  nsq, you can imagine

1281
01:16:25,355 --> 01:16:28,365
Speaker 3:  They're Dyson, but for e ink screens. Yeah.

1282
01:16:28,465 --> 01:16:31,885
Speaker 4:  And, and that's fine, right? Like you can go on their website and buy just

1283
01:16:31,885 --> 01:16:35,565
Speaker 4:  about any dimension of e ink device that you want. And I think just the fact

1284
01:16:35,565 --> 01:16:39,325
Speaker 4:  that this one is getting a second rev suggests

1285
01:16:40,035 --> 01:16:42,965
Speaker 4:  that it is successful and and meaningful to people.

1286
01:16:43,035 --> 01:16:45,605
Speaker 3:  Just by the way, just in case people are wondering, what I mean by Dyson

1287
01:16:45,605 --> 01:16:49,325
Speaker 3:  is If, you want something with a fan in it, Dyson will sell it to you. Oh

1288
01:16:49,325 --> 01:16:50,765
Speaker 3:  yeah. Suck, blow anything with a fan,

1289
01:16:50,765 --> 01:16:54,365
Speaker 4:  However tall you'd like your vacuum to be. Dyson has you, they got you so

1290
01:16:54,365 --> 01:16:58,085
Speaker 4:  good. There was, there's one I believe that

1291
01:16:58,325 --> 01:17:02,165
Speaker 4:  launched in just China that also doesn't have a

1292
01:17:02,165 --> 01:17:05,805
Speaker 4:  camera. Yeah. Which I think is pretty cool and is a thing. There were a bunch

1293
01:17:05,805 --> 01:17:09,765
Speaker 4:  of people when I wrote about the poem in the first place, we're like, I would

1294
01:17:09,765 --> 01:17:13,645
Speaker 4:  love for one of these without a camera for just a variety

1295
01:17:13,705 --> 01:17:17,565
Speaker 4:  of, you know, security and privacy and family reasons. But the one

1296
01:17:17,565 --> 01:17:19,445
Speaker 4:  that's shipping around the world still has a camera.

1297
01:17:20,425 --> 01:17:20,645
Speaker 3:  Why

1298
01:17:21,285 --> 01:17:22,365
Speaker 4:  I they say say,

1299
01:17:22,365 --> 01:17:24,885
Speaker 3:  Have you ever used the camera on your books? Poi No.

1300
01:17:24,885 --> 01:17:28,765
Speaker 4:  Literally not once. When I wrote about it, I forgot it existed until

1301
01:17:28,965 --> 01:17:31,925
Speaker 4:  somebody I was talking to asked me if I had ever used the camera and I said,

1302
01:17:31,925 --> 01:17:33,125
Speaker 4:  no, I forgot it had a camera.

1303
01:17:34,785 --> 01:17:38,725
Speaker 4:  But their thesis is that you might use it for

1304
01:17:38,725 --> 01:17:42,405
Speaker 4:  like scanning and digitizing stuff, which like, sure. But to me it's like,

1305
01:17:42,405 --> 01:17:44,725
Speaker 4:  get rid of the camera and knock the price down. 40 bucks. Everybody wins.

1306
01:17:44,945 --> 01:17:48,285
Speaker 4:  Yes. Way better. But yeah, I, I think this thing is,

1307
01:17:49,135 --> 01:17:52,395
Speaker 4:  I'm excited about it. I'm hopeful that they'll ship me one and I can review

1308
01:17:52,395 --> 01:17:56,115
Speaker 4:  it. I, I think the thing books is

1309
01:17:56,115 --> 01:17:59,475
Speaker 4:  trying to do it got kind of right the first time. Like that combination of

1310
01:17:59,975 --> 01:18:02,395
Speaker 4:  the size of the thing and the fact that it's e inc and the fact that it runs

1311
01:18:02,395 --> 01:18:03,755
Speaker 4:  Android apps is awesome.

1312
01:18:05,495 --> 01:18:09,435
Speaker 4:  We tried to get Anos Pinene to look at one. He refused, he refused to do

1313
01:18:09,435 --> 01:18:10,635
Speaker 4:  it on camera. It was very upsetting.

1314
01:18:12,215 --> 01:18:15,915
Speaker 4:  And yeah. We'll, we'll we'll see. I got a lot of people asking me,

1315
01:18:15,915 --> 01:18:19,635
Speaker 4:  should I buy this one or the new Kindle paper? White people want E Ink devices,

1316
01:18:19,815 --> 01:18:20,555
Speaker 4:  man. Yeah,

1317
01:18:21,155 --> 01:18:24,795
Speaker 3:  I will say I encourage everyone to pull over in your car and then look at

1318
01:18:24,855 --> 01:18:28,715
Speaker 3:  our article with BMA two, click on it and scroll down and look

1319
01:18:28,715 --> 01:18:32,235
Speaker 3:  at B'S own press photo of a guy holding The Boox Palma two.

1320
01:18:32,945 --> 01:18:35,635
Speaker 3:  He's very much like, what am I doing here? Yeah,

1321
01:18:35,635 --> 01:18:36,755
Speaker 4:  He's, he's like, what is this thing?

1322
01:18:37,185 --> 01:18:41,115
Speaker 3:  He's like, how did I get here? He is like, yep, I'm looking at a little phone

1323
01:18:41,115 --> 01:18:42,035
Speaker 3:  with me and scream.

1324
01:18:43,185 --> 01:18:46,035
Speaker 4:  It's true. It's just a guy in a beige sweater who's like, why does this have

1325
01:18:46,035 --> 01:18:46,395
Speaker 4:  no colors?

1326
01:18:47,385 --> 01:18:51,195
Speaker 3:  It's like his face is like, I I hate this book. I don't want to be reading

1327
01:18:51,255 --> 01:18:55,075
Speaker 3:  it. It's very, it's very much the face. I imagine Richard made

1328
01:18:55,095 --> 01:18:56,955
Speaker 3:  the first time he had the books spot.

1329
01:18:58,875 --> 01:19:02,685
Speaker 3:  Like this is the, the face that that made you pre-schedule the tweet like

1330
01:19:02,685 --> 01:19:05,305
Speaker 3:  right here. It's good. And,

1331
01:19:05,305 --> 01:19:08,265
Speaker 5:  And I'm, I'm glad that these things exist. Like someone should make like

1332
01:19:08,265 --> 01:19:11,385
Speaker 5:  an Android thing with an eating screen that runs apps.

1333
01:19:12,145 --> 01:19:15,805
Speaker 5:  It's great. I just think that people will probably put them down

1334
01:19:16,865 --> 01:19:18,205
Speaker 5:  and forget that they bought them. I

1335
01:19:18,515 --> 01:19:21,245
Speaker 3:  Just as I believe this gentleman put this one down.

1336
01:19:21,915 --> 01:19:22,765
Speaker 4:  It's why, it's

1337
01:19:22,765 --> 01:19:24,965
Speaker 5:  Why the If you If, you asked him about it right now. He wouldn't be able

1338
01:19:24,965 --> 01:19:25,565
Speaker 5:  to tell you what a book

1339
01:19:25,565 --> 01:19:26,765
Speaker 3:  Home. He's like, whatcha talking about

1340
01:19:26,905 --> 01:19:27,485
Speaker 4:  The the what

1341
01:19:27,485 --> 01:19:30,125
Speaker 3:  Now? That was a job. Like that's his face. Yeah.

1342
01:19:30,195 --> 01:19:33,965
Speaker 4:  This is why the problem is the price with Yeah, really with all of these

1343
01:19:33,965 --> 01:19:37,085
Speaker 4:  devices like the Palmer or something like it at

1344
01:19:37,345 --> 01:19:41,165
Speaker 4:  99 bucks, like sold, right? Like the, the Kindle

1345
01:19:41,285 --> 01:19:44,845
Speaker 4:  succeeds for that same reason. It's like If. you buy a Kindle and use it

1346
01:19:44,845 --> 01:19:46,765
Speaker 4:  a bunch and then forget about it for a while and then use it a bunch and

1347
01:19:46,765 --> 01:19:49,645
Speaker 4:  then forget about it for a while. You've paid the correct amount for it.

1348
01:19:50,745 --> 01:19:54,005
Speaker 4:  280 bucks is too much for that use case. Yeah,

1349
01:19:55,925 --> 01:19:57,945
Speaker 4:  that's said I'm gonna, I'll be purchasing one.

1350
01:19:58,425 --> 01:20:01,665
Speaker 3:  I gotta go use my books Pomo one at least one more time to make it worth

1351
01:20:01,665 --> 01:20:04,745
Speaker 3:  it apparently. All right, Richard, you've got the best one at all.

1352
01:20:06,445 --> 01:20:10,335
Speaker 5:  Yes. Senior citizens, very, very upset at

1353
01:20:10,615 --> 01:20:14,255
Speaker 5:  T-Mobile of all companies because it won't honor its lifetime price

1354
01:20:14,255 --> 01:20:18,095
Speaker 5:  guarantee. Turns out they lived, I guess longer than T-Mobile

1355
01:20:18,095 --> 01:20:21,255
Speaker 5:  thought. When you, when you tell people that they're going to, that you're

1356
01:20:21,255 --> 01:20:24,735
Speaker 5:  gonna give them a lifetime price lock. You figure how long could that last

1357
01:20:26,155 --> 01:20:30,135
Speaker 5:  longer than you think T-Mobile, they, they probably figure that out the

1358
01:20:30,135 --> 01:20:31,015
Speaker 5:  same way they do security.

1359
01:20:32,625 --> 01:20:34,045
Speaker 3:  That's rough. Which

1360
01:20:34,045 --> 01:20:34,685
Speaker 5:  Is why everyone

1361
01:20:34,685 --> 01:20:36,765
Speaker 3:  Has security number now That's a stab right there.

1362
01:20:39,325 --> 01:20:40,525
Speaker 3:  Thousands of FCC complaints

1363
01:20:42,115 --> 01:20:46,005
Speaker 3:  that from people saying they, they signed up for price lock in what,

1364
01:20:46,005 --> 01:20:49,765
Speaker 3:  2015. So 10 years ago. Yes. So T-Mobile assumed everyone would

1365
01:20:49,825 --> 01:20:50,965
Speaker 3:  die within one decade.

1366
01:20:51,475 --> 01:20:54,845
Speaker 5:  They had a senior plan that was marketed specifically to people 55 and up

1367
01:20:55,655 --> 01:20:56,865
Speaker 3:  With lifetime price lock.

1368
01:20:57,245 --> 01:21:00,625
Speaker 5:  Yes. And it included this. And those are not people who you want to disappoint

1369
01:21:00,625 --> 01:21:01,425
Speaker 5:  on a lifetime offer.

1370
01:21:03,135 --> 01:21:04,745
Speaker 3:  Like they will remember.

1371
01:21:05,525 --> 01:21:09,345
Speaker 4:  Has anyone ever not lived to regret the lifetime

1372
01:21:10,035 --> 01:21:12,825
Speaker 4:  price guarantee? Like this always makes me think of back in the day when

1373
01:21:12,825 --> 01:21:15,385
Speaker 4:  the airlines would sell the people like the

1374
01:21:15,385 --> 01:21:19,145
Speaker 4:  $250,000 lifetime fly wherever you want first

1375
01:21:19,145 --> 01:21:22,745
Speaker 4:  class thing. And then there were the people who were like, all right, challenge

1376
01:21:23,065 --> 01:21:26,545
Speaker 4:  accepted. I live in the sky now. Yeah. And eventually all the airlines are

1377
01:21:26,545 --> 01:21:30,165
Speaker 4:  like, well you can't do that. And they're like, remember when you said

1378
01:21:30,365 --> 01:21:34,325
Speaker 4:  Lifetime and like the everyone is j

1379
01:21:34,515 --> 01:21:38,485
Speaker 4:  just don't do this If. you can't do it. Just don't do it. And then

1380
01:21:38,485 --> 01:21:42,085
Speaker 4:  there are the companies that offer a lifetime deal and then go out of business

1381
01:21:42,385 --> 01:21:44,885
Speaker 4:  and it's like, what are, what are we doing? Y'all like, well

1382
01:21:44,885 --> 01:21:46,525
Speaker 3:  That's a win. That means you outlived the,

1383
01:21:47,355 --> 01:21:49,805
Speaker 5:  It's the lifetime of the executive who approved it,

1384
01:21:50,125 --> 01:21:51,925
Speaker 4:  Right? Yeah, a hundred percent

1385
01:21:52,225 --> 01:21:55,605
Speaker 3:  In the FCC complaints. There are some, some choice quotes. AZ Technica dug

1386
01:21:55,605 --> 01:21:59,485
Speaker 3:  them all up. One of them, I am not dead yet. A customer

1387
01:21:59,485 --> 01:22:00,685
Speaker 3:  in New York wrote bluntly.

1388
01:22:04,225 --> 01:22:05,845
Speaker 3:  That's, that's very good.

1389
01:22:06,135 --> 01:22:09,525
Speaker 4:  There was one person who complained that their price is going up $50 a month

1390
01:22:09,525 --> 01:22:12,685
Speaker 4:  because they had 10 lines. I'm like, wow, 10 lines.

1391
01:22:13,025 --> 01:22:15,685
Speaker 3:  That's good. Well if you've got that lifetime guarantee, that's,

1392
01:22:15,725 --> 01:22:19,085
Speaker 4:  I mean that's fair. I'd get everybody on that plan. You guys wanna join my

1393
01:22:19,085 --> 01:22:19,325
Speaker 4:  plan?

1394
01:22:19,925 --> 01:22:23,445
Speaker 3:  I will take this opportunity to remind the Vergecast audience that T-Mobile

1395
01:22:23,445 --> 01:22:26,605
Speaker 3:  exists in its current state. Because the government allowed it to buy Sprint

1396
01:22:26,865 --> 01:22:29,965
Speaker 3:  in a deal which took our nation's number of viable

1397
01:22:30,655 --> 01:22:34,245
Speaker 3:  nationwide wireless carriers from four to three. And to solve that problem,

1398
01:22:34,585 --> 01:22:38,505
Speaker 3:  the government made dish network make

1399
01:22:38,545 --> 01:22:41,745
Speaker 3:  a network. That pro network is called Project Genesis or Gen five sis.

1400
01:22:42,815 --> 01:22:46,745
Speaker 3:  That network does not exist. Just, I don't know what to tell you. You can

1401
01:22:46,745 --> 01:22:50,665
Speaker 3:  go to the website. The website sometimes doesn't load when you go to the

1402
01:22:50,665 --> 01:22:53,425
Speaker 3:  Dish Network project. It's sometimes just isn't there.

1403
01:22:54,935 --> 01:22:58,785
Speaker 3:  Like just physically doesn't load. And then If,

1404
01:22:58,785 --> 01:23:02,665
Speaker 3:  you do get it to load. You can buy a, what is it, a Motorola Edge plus

1405
01:23:03,055 --> 01:23:06,845
Speaker 3:  like a 2023 Motorola Edge plus Plus. That's the phone they sell and that

1406
01:23:06,975 --> 01:23:10,725
Speaker 3:  phone If you go look at the subreddit will mostly roam onto at

1407
01:23:10,965 --> 01:23:11,285
Speaker 3:  t's network.

1408
01:23:11,715 --> 01:23:14,725
Speaker 4:  Perfect. I don't see the pro That's competition baby.

1409
01:23:15,395 --> 01:23:17,965
Speaker 3:  What I, I'm just saying, you know, I know it's election season, I'm, that

1410
01:23:17,965 --> 01:23:21,765
Speaker 3:  was the deal negotiated by Donald Trump's antitrust

1411
01:23:22,445 --> 01:23:25,045
Speaker 3:  division to preserve competition in the wireless market.

1412
01:23:25,695 --> 01:23:26,435
Speaker 4:  We did it y'all.

1413
01:23:26,665 --> 01:23:30,275
Speaker 3:  When I think of at and t, Verizon and T-Mobile and the biggest threats they

1414
01:23:30,275 --> 01:23:30,715
Speaker 3:  face every

1415
01:23:30,715 --> 01:23:34,195
Speaker 4:  Day is, Zach, can I just read you one paragraph from their story? Yeah.

1416
01:23:34,425 --> 01:23:37,115
Speaker 4:  It's like an aside in the middle of the story, but it is, it is the best

1417
01:23:37,115 --> 01:23:41,035
Speaker 4:  knife in the whole story. Last year, T-Mobile notified some customers that

1418
01:23:41,035 --> 01:23:44,475
Speaker 4:  it would automatically switch them to newer more expensive plans unless the

1419
01:23:44,635 --> 01:23:47,795
Speaker 4:  customers called the company to opt outta the change. T-Mobile customer service

1420
01:23:47,865 --> 01:23:51,315
Speaker 4:  reps were instructed to tell users we are not raising the price of any of

1421
01:23:51,315 --> 01:23:55,235
Speaker 4:  our plans. We are moving you to a newer plan with more benefits at a different

1422
01:23:55,265 --> 01:23:59,035
Speaker 4:  cost. It's not whether the price is more or less, you guys,

1423
01:23:59,105 --> 01:23:59,795
Speaker 4:  it's just different.

1424
01:24:02,145 --> 01:24:02,875
Speaker 4:  It's just different.

1425
01:24:03,495 --> 01:24:07,275
Speaker 3:  I'm sorry and I'm just reading the the project Genesis subreddit, none of

1426
01:24:07,275 --> 01:24:10,475
Speaker 3:  it's good man. Anyhow. Wireless carriers and ISP your favorite.

1427
01:24:11,175 --> 01:24:14,235
Speaker 3:  Now to further enrage you about our nation's wireless carriers

1428
01:24:15,215 --> 01:24:18,835
Speaker 3:  and ISPs, I will inform you of my lightning round item,

1429
01:24:19,725 --> 01:24:23,395
Speaker 3:  which is that the Federal Trade Commission passed what's called the click

1430
01:24:23,395 --> 01:24:27,035
Speaker 3:  to cancel rule. This is the rule that says If,

1431
01:24:27,575 --> 01:24:31,475
Speaker 3:  you sign up for something canceling, it has to be as easy as signing up,

1432
01:24:31,805 --> 01:24:35,035
Speaker 3:  right? So you can click on something to sign up for a subscription or a yearly

1433
01:24:35,125 --> 01:24:38,675
Speaker 3:  membership or whatever to get out of it. You gotta be able to click one button.

1434
01:24:38,685 --> 01:24:41,475
Speaker 3:  These are called negative option contracts. There's a whole thing. There's

1435
01:24:41,475 --> 01:24:45,355
Speaker 3:  like three or four different laws to try to solve this problem. And

1436
01:24:45,355 --> 01:24:49,275
Speaker 3:  the FT C put up a rule a while ago everyone complained about the rule and

1437
01:24:49,275 --> 01:24:52,435
Speaker 3:  now they pass the rule. This is like Lena con's FT C saying this is bad for

1438
01:24:52,635 --> 01:24:55,995
Speaker 3:  consumers to make people dance through cancel

1439
01:24:56,365 --> 01:25:00,035
Speaker 3:  flows or to hide, you know, options where the price

1440
01:25:00,395 --> 01:25:02,915
Speaker 3:  escalates. It's gotta be as easy to get out as it was to get in.

1441
01:25:02,985 --> 01:25:05,275
Speaker 4:  This is an objectively correct stance, right?

1442
01:25:05,795 --> 01:25:06,475
Speaker 3:  Everyone loves this rule.

1443
01:25:06,585 --> 01:25:09,755
Speaker 4:  There's not like a secret thing that I'm missing about why this might suck

1444
01:25:09,755 --> 01:25:11,035
Speaker 4:  for consumers. This is just great.

1445
01:25:11,575 --> 01:25:13,355
Speaker 3:  No, it's totally, this is, this is

1446
01:25:13,355 --> 01:25:16,755
Speaker 7:  What happens when you make millennials call the New York Times to

1447
01:25:16,755 --> 01:25:17,755
Speaker 3:  Cancel their subscriptions.

1448
01:25:18,355 --> 01:25:19,395
Speaker 7:  Millennials hate phone calls

1449
01:25:19,675 --> 01:25:20,195
Speaker 4:  A hundred percent.

1450
01:25:20,335 --> 01:25:22,875
Speaker 3:  And so, you know, the lean con FCC is like doing its thing.

1451
01:25:24,165 --> 01:25:27,115
Speaker 3:  Guess immediately. Who sued to stop this rule?

1452
01:25:28,495 --> 01:25:31,435
Speaker 3:  If, you guessed our nation's ISPs and wireless carriers. You would be correct.

1453
01:25:33,855 --> 01:25:34,315
Speaker 3:  So I

1454
01:25:34,475 --> 01:25:36,395
Speaker 4:  Was gonna guess gyms that was gonna be my other

1455
01:25:36,665 --> 01:25:39,635
Speaker 3:  Gyms is in there. A bunch of advertisers are in there.

1456
01:25:40,115 --> 01:25:43,875
Speaker 3:  A-D-T-A-D-T is in there. So it's, it's they're trade groups,

1457
01:25:44,325 --> 01:25:47,995
Speaker 3:  right? So the nash, the NCTA, which is the, the

1458
01:25:48,035 --> 01:25:51,315
Speaker 3:  ISP trade group that represents Comcast Charter Cox,

1459
01:25:53,015 --> 01:25:56,355
Speaker 3:  it represents streaming services like Disney, A MC, paramount, Warner Brothers,

1460
01:25:56,355 --> 01:26:00,235
Speaker 3:  discovery Disclosure, Comcast is an investor, our

1461
01:26:00,235 --> 01:26:00,795
Speaker 3:  parent company,

1462
01:26:02,485 --> 01:26:05,345
Speaker 3:  I'd take that from what it's worth, I think they suck in this case,

1463
01:26:06,645 --> 01:26:10,185
Speaker 3:  the IAB, which is the advertising, the internet advertising bureau,

1464
01:26:10,685 --> 01:26:14,145
Speaker 3:  that's Google, Netflix, Amazon Meta Vio and the NFL,

1465
01:26:14,845 --> 01:26:18,825
Speaker 3:  the ESAs, A DT. So all of these huge companies are through their

1466
01:26:18,955 --> 01:26:22,845
Speaker 3:  trade organizations basically suing the

1467
01:26:22,885 --> 01:26:26,845
Speaker 3:  FTC and saying, no, we do not want you to

1468
01:26:27,115 --> 01:26:30,765
Speaker 3:  make click to cancel mandatory. And the argument is

1469
01:26:31,025 --> 01:26:34,205
Speaker 3:  the FTC is trying to regulate consumer contracts for all companies in all

1470
01:26:34,205 --> 01:26:37,965
Speaker 3:  industries across all sectors of the economy, from forbidding businesses

1471
01:26:38,265 --> 01:26:42,205
Speaker 3:  for making customers cancel services using a method that differs from

1472
01:26:42,205 --> 01:26:42,885
Speaker 3:  how they signed up.

1473
01:26:43,345 --> 01:26:45,445
Speaker 4:  Yes. That is what they're trying. That's correct.

1474
01:26:45,625 --> 01:26:49,085
Speaker 3:  And so the, the argument here is freedom in America

1475
01:26:49,525 --> 01:26:51,885
Speaker 3:  requires us to be able to fleece our customers.

1476
01:26:53,755 --> 01:26:57,485
Speaker 3:  Like get the nanny state outta here. It's wild

1477
01:26:57,895 --> 01:27:01,885
Speaker 3:  underneath that this is some real wonky legal stuff

1478
01:27:02,945 --> 01:27:06,725
Speaker 3:  is how they will probably attempt to go after this. So the

1479
01:27:06,765 --> 01:27:10,685
Speaker 3:  FTC is a, you know, executive agency of the government. It has some

1480
01:27:10,685 --> 01:27:14,525
Speaker 3:  powers or some powers. It doesn't have the main power is like

1481
01:27:14,525 --> 01:27:18,125
Speaker 3:  its rulemaking authority. It's like ability to like make decisions

1482
01:27:18,125 --> 01:27:21,965
Speaker 3:  basically. And that was basically

1483
01:27:21,965 --> 01:27:24,525
Speaker 3:  just overturned by this Supreme Court.

1484
01:27:26,405 --> 01:27:30,395
Speaker 3:  We'll link to it. But there was this concept called Chevron deference where

1485
01:27:30,935 --> 01:27:34,635
Speaker 3:  the courts would defer to the agency like, you know best, like you're staffed

1486
01:27:34,635 --> 01:27:38,355
Speaker 3:  by experts. You made this decision, we will defer to your rulemaking

1487
01:27:38,355 --> 01:27:42,315
Speaker 3:  process. If you get sued. But this Supreme Court threw that

1488
01:27:42,375 --> 01:27:45,915
Speaker 3:  out. So now the courts get to decide if these rules make sense,

1489
01:27:46,565 --> 01:27:49,955
Speaker 3:  which is basically just like an explosion of how our government works Anyway.

1490
01:27:49,955 --> 01:27:53,835
Speaker 3:  All that means is Comcast gets to Sue Lehan for

1491
01:27:53,835 --> 01:27:57,075
Speaker 3:  making easier sort of get outta your ISP agreement. And that is stupid.

1492
01:27:57,895 --> 01:28:01,795
Speaker 3:  So that's good. This is going to, it's amazing that these companies are buying

1493
01:28:01,795 --> 01:28:05,135
Speaker 3:  this amount of bad press. They should just make better products.

1494
01:28:05,935 --> 01:28:06,555
Speaker 3:  But here we are.

1495
01:28:06,825 --> 01:28:10,555
Speaker 4:  This always just makes me think of the, the place we've gotten to with streaming

1496
01:28:10,555 --> 01:28:14,475
Speaker 4:  TV where basically what we've done is just very slowly reinvent

1497
01:28:14,475 --> 01:28:18,395
Speaker 4:  cable, but it's easier to cancel now and in, in a

1498
01:28:18,395 --> 01:28:22,115
Speaker 4:  very real way. That's a giant victory that like I pay as much,

1499
01:28:22,585 --> 01:28:25,915
Speaker 4:  it's a worst user experience in a lot of ways, but at least I can cancel

1500
01:28:25,975 --> 01:28:29,345
Speaker 4:  it success. And it is just

1501
01:28:29,935 --> 01:28:33,865
Speaker 4:  bonkers how few things have gone that way. Yeah, like it's, I,

1502
01:28:33,985 --> 01:28:37,425
Speaker 4:  there are so many things I pay for because I just can't figure out how to

1503
01:28:37,425 --> 01:28:41,385
Speaker 4:  cancel it. Like literally the the just to name one, the Wall Street

1504
01:28:41,385 --> 01:28:45,265
Speaker 4:  Journal, which I'm not important enough to need

1505
01:28:45,325 --> 01:28:45,545
Speaker 4:  to

1506
01:29:20,685 --> 01:29:24,405
Speaker 3:  care of your health. So sign up for another gym membership and then never

1507
01:29:24,405 --> 01:29:26,925
Speaker 3:  get outta it. Thanks.

1508
01:29:28,225 --> 01:29:31,965
Speaker 3:  That's it. We we're way over. I'm furious that I can't

1509
01:29:32,025 --> 01:29:35,965
Speaker 3:  cancel more things more easily. I don't even know my Hulu

1510
01:29:36,125 --> 01:29:37,245
Speaker 3:  Password, but I'd cancel it. That's what

1511
01:29:37,345 --> 01:29:40,005
Speaker 4:  If you want to cancel The? Vergecast. You have to call Eli and we'll never

1512
01:29:40,005 --> 01:29:40,645
Speaker 4:  tell you his number.

1513
01:29:41,565 --> 01:29:44,245
Speaker 3:  That's it. Call The Vergecast line and explain to us why you need to cancel

1514
01:29:44,245 --> 01:29:48,125
Speaker 3:  the show. That is a dangerous prompt. Think twice. Make it

1515
01:29:48,125 --> 01:29:50,365
Speaker 3:  funny. That's it. That's The Vergecast. Rock and roll.

1516
01:29:54,505 --> 01:29:57,765
Speaker 1:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1517
01:29:57,915 --> 01:30:01,805
Speaker 1:  Give us a call at eight six six VERGE one. One The Vergecast is a

1518
01:30:01,805 --> 01:30:05,445
Speaker 1:  production of The Verge and Vox Media Podcast Network. Our show is produced

1519
01:30:05,445 --> 01:30:09,405
Speaker 1:  by Liam James Wil Pour and Eric Gomez. And that's it. We'll see

1520
01:30:09,405 --> 01:30:09,845
Speaker 1:  you next week.

