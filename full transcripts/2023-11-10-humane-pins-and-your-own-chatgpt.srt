1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 9c4f3215-3067-4577-ad90-05e6bc6f7d9f
Status: Done
Stage: Done
Title: Humane Pins and your own ChatGPT
Audio URL: https://jfe93e.s3.amazonaws.com/-8040525363765735058/5250667281620810394/s93290-US-6479s-1699626140.mp3
Description: The Verge's Nilay Patel, David Pierce, Alex Cranz, and Alex Heath discuss the debut of Humane's AI Pin, OpenAI's DevDay, GPT-4 updates, and more.

2
00:01:21,205 --> 00:01:21,555
Speaker 2:  Hello

3
00:01:21,555 --> 00:01:24,795
Speaker 3:  And Welcome To, the Vergecast flagship podcast of Fox Media.

4
00:01:25,895 --> 00:01:27,435
Speaker 3:  I'm just taking it back baby. Yeah,

5
00:01:27,915 --> 00:01:28,315
Speaker 4:  I love that.

6
00:01:28,375 --> 00:01:31,435
Speaker 3:  That's 2024. Yeah, just open mutiny.

7
00:01:32,645 --> 00:01:36,595
Speaker 3:  Bring it on Sports website. I'm your friend Neil Alex.

8
00:01:36,685 --> 00:01:37,275
Speaker 3:  Cranz is here.

9
00:01:37,335 --> 00:01:40,355
Speaker 4:  I'm your friend who, I just gotta say it. My brother won a Michelin Star.

10
00:01:40,425 --> 00:01:43,755
Speaker 4:  It's pretty good. And I didn't even want to try to, to put that in neatly.

11
00:01:43,835 --> 00:01:47,035
Speaker 4:  I just wanna brag. Yeah. So that was me bragging. And that's, that's it.

12
00:01:47,035 --> 00:01:48,155
Speaker 4:  It's, it's here. It happened.

13
00:01:48,215 --> 00:01:50,835
Speaker 3:  The flagship podcast is being related to Michelin Stars. Yeah,

14
00:01:51,045 --> 00:01:51,555
Speaker 4:  There we go.

15
00:01:51,785 --> 00:01:52,995
Speaker 3:  He's my brother too. Yeah,

16
00:01:53,225 --> 00:01:53,755
Speaker 4:  He's all our

17
00:01:53,755 --> 00:01:54,955
Speaker 3:  Brothers. He's everyone's family.

18
00:01:55,435 --> 00:01:56,595
Speaker 4:  Everyone. Everyone earned this with

19
00:01:56,595 --> 00:01:59,035
Speaker 3:  Him. If you're listening to this show, you can tell people You know a Michelin

20
00:01:59,035 --> 00:01:59,235
Speaker 3:  Star.

21
00:01:59,385 --> 00:02:00,195
Speaker 4:  Yeah, you can.

22
00:02:00,735 --> 00:02:01,555
Speaker 3:  The star, not the person,

23
00:02:01,555 --> 00:02:02,675
Speaker 4:  Just the star. Just the star.

24
00:02:02,925 --> 00:02:04,675
Speaker 3:  David Pierce is in San Francisco.

25
00:02:04,725 --> 00:02:07,795
Speaker 5:  We're like 85 seconds in and I'm like, this is gonna be a three and a half

26
00:02:07,795 --> 00:02:11,715
Speaker 5:  hour long broadcast. It's not gonna make any sense to anyone. And this

27
00:02:11,715 --> 00:02:15,115
Speaker 5:  might be the end of The Vergecast. Yeah. Like this for a lot of reasons.

28
00:02:15,245 --> 00:02:18,715
Speaker 5:  Today might be the last Vergecast ever. I think it's gonna be, it's gonna

29
00:02:18,715 --> 00:02:19,075
Speaker 5:  be a day.

30
00:02:20,255 --> 00:02:24,235
Speaker 3:  If you're gonna go go with a bang. Alex Heath is here

31
00:02:24,235 --> 00:02:27,235
Speaker 3:  from Los Angeles. Hi. I am really just a AI agent, but I'm here.

32
00:02:29,625 --> 00:02:32,355
Speaker 3:  I'll say that some people in the YouTube comments

33
00:02:33,255 --> 00:02:36,875
Speaker 3:  openly wondered if I was interviewing an AI Barack Obama. They're like, that

34
00:02:36,875 --> 00:02:40,635
Speaker 3:  didn't happen. Aw, you got trolled by AI Barack Obama. And maybe I did

35
00:02:41,495 --> 00:02:44,995
Speaker 3:  All. right? So David. You're in San Francisco. Yep. Because you were supposed

36
00:02:44,995 --> 00:02:48,635
Speaker 3:  to attend the launch of the Humane AI Pin. That's correct.

37
00:02:48,805 --> 00:02:51,835
Speaker 3:  Which when David says the energy of this podcast is a little,

38
00:02:53,685 --> 00:02:57,565
Speaker 3:  I would say that that is the thing that we are gonna talk about. It is. It's

39
00:02:57,565 --> 00:02:59,800
Speaker 3:  a lot. Yes. There's a lot to unpack with the, with the fact that David is

40
00:02:59,800 --> 00:03:03,005
Speaker 3:  in San Francisco and did not go to that event.

41
00:03:03,865 --> 00:03:07,845
Speaker 3:  And then earlier this week, Alex, you did attend the OpenAI s DevDay.

42
00:03:07,945 --> 00:03:08,485
Speaker 6:  That's right.

43
00:03:09,215 --> 00:03:13,005
Speaker 3:  Which is probably like a pivotal moment in the history of that

44
00:03:13,005 --> 00:03:16,445
Speaker 3:  company because they're, they're headed towards app stores,

45
00:03:17,185 --> 00:03:20,405
Speaker 3:  so there's like quite a lot of news. And then we got a lightning round.

46
00:03:21,265 --> 00:03:23,925
Speaker 3:  But we should start with the fact that I'm using Samsung decks.

47
00:03:24,075 --> 00:03:26,845
Speaker 5:  Well first I should say, we should actually start with an apology, which

48
00:03:26,845 --> 00:03:30,805
Speaker 5:  is that I made certain promises on this show two weeks ago to hold you to

49
00:03:30,845 --> 00:03:34,525
Speaker 5:  a standard of using decks on the podcast. And I did not do that.

50
00:03:34,925 --> 00:03:38,445
Speaker 5:  I abdicated my responsibility in bullying you into using this

51
00:03:38,545 --> 00:03:42,485
Speaker 5:  insane setup that you have going on and on YouTube, there

52
00:03:42,485 --> 00:03:46,285
Speaker 5:  were several people who were very upset with us. And so I just wanna

53
00:03:46,285 --> 00:03:49,925
Speaker 5:  preface whatever Neli ISS about to say as the 30 minutes

54
00:03:50,185 --> 00:03:53,925
Speaker 5:  before we started recording this podcast, were spent with Neli and

55
00:03:54,315 --> 00:03:58,245
Speaker 5:  Liam, our producer, attempting to make this thing that is happening

56
00:03:58,305 --> 00:03:58,965
Speaker 5:  now work.

57
00:04:01,345 --> 00:04:04,965
Speaker 3:  So if you're listening, if you're in your car, close your eyes, pull over,

58
00:04:04,965 --> 00:04:08,365
Speaker 3:  close your eyes. I want you to just imagine my setup. Dell Monitor. Pretty

59
00:04:08,365 --> 00:04:11,925
Speaker 3:  nice. Still monitor actually that we, we just stole it from a desk. Liam

60
00:04:11,925 --> 00:04:15,405
Speaker 3:  just wandered the office and took the nicest one he could find is connected

61
00:04:15,405 --> 00:04:18,685
Speaker 3:  to A-U-S-B-C hub, which is connected to

62
00:04:19,325 --> 00:04:23,085
Speaker 3:  a dead Bluetooth keyboard that is now charging by being plugged into the

63
00:04:23,085 --> 00:04:27,045
Speaker 3:  hub and a, a wired mouse. And the tracking speed of this

64
00:04:27,045 --> 00:04:30,565
Speaker 3:  mouse is out of control. This computer is almost unusable because the tracking

65
00:04:30,565 --> 00:04:34,285
Speaker 3:  speed of this mouse is set at Warp 20. It's out of control.

66
00:04:34,865 --> 00:04:38,545
Speaker 3:  And then that's all plugged into a Galaxy fold five with AD brand skin on

67
00:04:38,545 --> 00:04:40,865
Speaker 3:  it. It's our custom D brandand skin. You can go to store out The Verge dot

68
00:04:40,865 --> 00:04:44,785
Speaker 3:  com. Put that plug in there and I'm, I'm in it man. I'm just running

69
00:04:44,795 --> 00:04:48,785
Speaker 3:  multi windows. Yeah. My dream Alex doesn't know any of this. My dream

70
00:04:48,785 --> 00:04:52,465
Speaker 3:  Alex is that I would commute with nothing. No

71
00:04:52,605 --> 00:04:56,465
Speaker 3:  bag, no laptop. I would just have the fold. I would do phone stuff

72
00:04:56,465 --> 00:04:59,905
Speaker 3:  on the train. I would tell David that his Humane leak was clunky

73
00:05:00,615 --> 00:05:04,505
Speaker 3:  just for my, my folding tablet. Then I would get to the office and I would

74
00:05:04,565 --> 00:05:07,225
Speaker 3:  use decks and this would be my computer at work.

75
00:05:08,485 --> 00:05:12,265
Speaker 3:  And Alex, I just want to you to tell the people what you said

76
00:05:12,335 --> 00:05:14,305
Speaker 3:  immediately upon seeing this setup.

77
00:05:15,495 --> 00:05:19,345
Speaker 6:  Just that looks stupid. I, I actually don't, I don't remember exactly what

78
00:05:19,345 --> 00:05:23,265
Speaker 6:  I said. That was it. Yeah. But yeah, the only

79
00:05:23,265 --> 00:05:27,225
Speaker 6:  thing that could possibly be stupider is like a screenless thing

80
00:05:27,345 --> 00:05:31,225
Speaker 6:  on your lapel I think that you're using but

81
00:05:31,325 --> 00:05:32,505
Speaker 6:  you're not using it. So

82
00:05:33,105 --> 00:05:37,025
Speaker 3:  I. Wish I had one, but we never will. Nope. That seems clear. All, right?

83
00:05:37,025 --> 00:05:39,505
Speaker 3:  So I'm using Dex. We're gonna get through the show notes with Dex.

84
00:05:40,845 --> 00:05:44,815
Speaker 3:  It's not unusable. I would say. Well the monitor's

85
00:05:45,055 --> 00:05:48,935
Speaker 3:  gorgeous. The Dell display that we again just boosted

86
00:05:49,085 --> 00:05:52,495
Speaker 3:  from the working space in this office with, without asking anyone.

87
00:05:53,315 --> 00:05:56,775
Speaker 3:  That's great. And it's basically just a Chromebook that sucks.

88
00:05:57,035 --> 00:06:00,845
Speaker 5:  Neli has the exact energy of the person who brings an iMac to Starbucks

89
00:06:00,985 --> 00:06:04,165
Speaker 5:  and plugs it in. Like that's, that's the vibe that I'm getting from you in

90
00:06:04,165 --> 00:06:04,845
Speaker 5:  the studio right now.

91
00:06:05,555 --> 00:06:09,245
Speaker 4:  Yeah. You gotta bring this whole setup to Starbucks. That's when we'll really

92
00:06:09,245 --> 00:06:09,925
Speaker 4:  appreciate it

93
00:06:10,185 --> 00:06:12,165
Speaker 3:  All. right. We're gonna try to get through the rundown with this. So let's

94
00:06:12,165 --> 00:06:15,985
Speaker 3:  start with Humane. There's, we're gonna take the first ad break in the show.

95
00:06:16,045 --> 00:06:18,145
Speaker 3:  My laptop is 100% coming back.

96
00:06:19,945 --> 00:06:23,745
Speaker 3:  I just want you all to know this also, just to remind everyone I

97
00:06:23,755 --> 00:06:27,225
Speaker 3:  truly dislike using this phone because of the jelly scrolling. Nothing about

98
00:06:27,225 --> 00:06:28,545
Speaker 3:  this plan was a good idea.

99
00:06:29,065 --> 00:06:32,065
Speaker 4:  I like that. I don't need my laptop open because this screen is so large.

100
00:06:32,145 --> 00:06:33,105
Speaker 4:  I see the whole rundown.

101
00:06:33,105 --> 00:06:36,785
Speaker 3:  Yeah. This is a good case for having a big screen. Yeah. Not a

102
00:06:36,785 --> 00:06:40,465
Speaker 3:  folding phone that runs text All. right. Let's start with Humane. There's

103
00:06:40,465 --> 00:06:43,345
Speaker 3:  a lot of drama with Humane right now.

104
00:06:44,405 --> 00:06:47,705
Speaker 3:  We do a lot of big splashy launch features.

105
00:06:48,195 --> 00:06:51,465
Speaker 3:  David has written a billion of them. I have written half a billion of them.

106
00:06:52,095 --> 00:06:56,065
Speaker 3:  They're fun to do. I appreciate that people do them. It's great. They're,

107
00:06:56,065 --> 00:06:57,985
Speaker 3:  they're like great. This company

108
00:06:59,605 --> 00:07:03,055
Speaker 3:  just from the jump has communicated about itself

109
00:07:03,595 --> 00:07:07,525
Speaker 3:  in the weirdest ways possible. And then they,

110
00:07:08,025 --> 00:07:12,005
Speaker 3:  I'm just gonna say it David, they invited you to California and then ghosted

111
00:07:12,005 --> 00:07:15,725
Speaker 3:  you and you are not at the That's correct. They disinvited you from the

112
00:07:15,725 --> 00:07:19,605
Speaker 3:  event, which is crazy. And I, we know a lot of other reporters and

113
00:07:19,605 --> 00:07:20,965
Speaker 3:  reviewers who are not there either.

114
00:07:21,355 --> 00:07:25,285
Speaker 5:  Yeah. This is a weird situation that I still am trying to wrap my head

115
00:07:25,285 --> 00:07:29,245
Speaker 5:  around because on the one hand, like I think a majority of

116
00:07:29,245 --> 00:07:33,125
Speaker 5:  people who work at Humane used to work at Apple. So there's a

117
00:07:33,125 --> 00:07:37,045
Speaker 5:  real apple like aesthetic and ethos and way of

118
00:07:37,045 --> 00:07:40,525
Speaker 5:  thinking about the world. And Apple is a famously secretive company that

119
00:07:40,525 --> 00:07:43,565
Speaker 5:  has weird ideas about how to sort of bring people in on what it wants to

120
00:07:43,565 --> 00:07:47,165
Speaker 5:  do. And that's all fine and good, right? Like Apple,

121
00:07:47,215 --> 00:07:50,565
Speaker 5:  apple does things its own way and that works fine. But then also

122
00:07:51,145 --> 00:07:54,925
Speaker 5:  Humane has been doing this thing where I think for eight months now, it has

123
00:07:54,925 --> 00:07:58,525
Speaker 5:  been sort of slowly trickling out information about the device. And one of

124
00:07:58,525 --> 00:08:01,725
Speaker 5:  the things we're gonna get into this, but one of the most fascinating things

125
00:08:01,725 --> 00:08:05,445
Speaker 5:  about this launch to me was that it's functionally the same

126
00:08:05,455 --> 00:08:09,325
Speaker 5:  thing that Iran Chowdry the co-founder did

127
00:08:09,345 --> 00:08:13,245
Speaker 5:  on the TED stage like six months ago. Like the,

128
00:08:13,265 --> 00:08:16,525
Speaker 5:  the features are the same, the device is the same. Like my guy just launched

129
00:08:16,545 --> 00:08:19,485
Speaker 5:  it then and it has really not changed all that much since then, which is

130
00:08:19,485 --> 00:08:23,085
Speaker 5:  really fascinating. But yeah, so there was supposed to be a

131
00:08:23,275 --> 00:08:27,085
Speaker 5:  kind of launch thing ahead of the event and then that got

132
00:08:27,085 --> 00:08:29,765
Speaker 5:  changed to being after the event and then a bunch of people got disinvited

133
00:08:29,765 --> 00:08:33,285
Speaker 5:  from that And. what seems very clear is that Humane is not interested

134
00:08:33,585 --> 00:08:37,285
Speaker 5:  in product reviewers getting

135
00:08:37,285 --> 00:08:40,485
Speaker 5:  their hands on this thing at this time. Even the people, they gave these

136
00:08:40,485 --> 00:08:44,365
Speaker 5:  sort of big splashy stories to like Wired had a good story. The Times

137
00:08:44,625 --> 00:08:48,205
Speaker 5:  had a big giant profiley thing. The Wall Street Journal got a story ahead

138
00:08:48,205 --> 00:08:51,485
Speaker 5:  of time. They weren't allowed to photograph the thing. They weren't allowed

139
00:08:51,625 --> 00:08:54,925
Speaker 5:  to like talk that much about how to use it. No one is really trying this

140
00:08:55,045 --> 00:08:59,005
Speaker 5:  thing. It's a strange thing that is happening right now

141
00:08:59,005 --> 00:09:01,645
Speaker 5:  where it is both a launched product that they've been talking about for many

142
00:09:01,645 --> 00:09:05,605
Speaker 5:  months and also seems to be not close to being finished

143
00:09:05,665 --> 00:09:08,325
Speaker 5:  or ready for public consumption. It's very strange. Yeah.

144
00:09:08,365 --> 00:09:11,725
Speaker 4:  A lot of the features were just, they'd be like, they're coming soon. They're

145
00:09:11,725 --> 00:09:12,485
Speaker 4:  coming soon. Yeah.

146
00:09:12,485 --> 00:09:16,445
Speaker 5:  Like recording video is coming in a software update. It's like that. That's

147
00:09:16,445 --> 00:09:18,285
Speaker 5:  a pretty basic thing to get right on your camera.

148
00:09:18,745 --> 00:09:22,595
Speaker 3:  So we have obviously talked a lot about Humane. David was invited

149
00:09:22,595 --> 00:09:25,915
Speaker 3:  to the event, got disinvited and then last night totally separate from all

150
00:09:25,915 --> 00:09:29,435
Speaker 3:  of that. And the other reviewers we know who

151
00:09:29,855 --> 00:09:33,755
Speaker 3:  didn't get to cover the thing, it was all leak to us. Yeah.

152
00:09:34,085 --> 00:09:36,995
Speaker 3:  Which is what happens. That's, that's life. And I'll take the leak every

153
00:09:36,995 --> 00:09:40,555
Speaker 3:  time. So we know a lot about this thing and Yep. There were no

154
00:09:40,835 --> 00:09:43,875
Speaker 3:  surprises. I think most of the leaks we got, all of the leaks we got were

155
00:09:43,875 --> 00:09:47,155
Speaker 3:  dead on accurate today. Yep. So the basics are pretty,

156
00:09:47,505 --> 00:09:50,915
Speaker 3:  they're understandable. And then what's not understandable is this device

157
00:09:50,915 --> 00:09:54,355
Speaker 3:  at all. So the basics are, it's a smartphone, right?

158
00:09:54,745 --> 00:09:58,075
Speaker 3:  It's a Qualcomm eight core. Yep. Qualcomm processor. They didn't say which

159
00:09:58,075 --> 00:10:01,915
Speaker 3:  one but whatever. It's got a camera, it has a person

160
00:10:02,025 --> 00:10:05,915
Speaker 3:  speaker that creates a bubble of sound, which no one has experienced.

161
00:10:06,375 --> 00:10:09,975
Speaker 3:  It has the laser projector, which we've seen it needs

162
00:10:10,295 --> 00:10:13,375
Speaker 3:  wireless service. Obviously they've partnered with T-Mobile, which is really

163
00:10:13,615 --> 00:10:17,445
Speaker 3:  interesting. And it's not just T-Mobile. It's like

164
00:10:17,445 --> 00:10:20,205
Speaker 3:  they have their own wireless network like an MVNO.

165
00:10:21,375 --> 00:10:23,995
Speaker 3:  So You know how like visible is Verizon. Yeah, they've got that. But it's

166
00:10:23,995 --> 00:10:27,555
Speaker 3:  Humane. And so you get a phone number, you can get

167
00:10:27,685 --> 00:10:31,635
Speaker 3:  texts, you're a green bubble. We should talk about that. You get

168
00:10:31,635 --> 00:10:35,515
Speaker 3:  data and you get cloud storage and you get access to OpenAI

169
00:10:35,515 --> 00:10:38,835
Speaker 3:  and Microsoft models and then their slides say had Google

170
00:10:39,735 --> 00:10:42,735
Speaker 3:  and Slack and importantly title

171
00:10:43,195 --> 00:10:45,575
Speaker 4:  And it's weighs about the same as the tennis ball.

172
00:10:46,765 --> 00:10:47,295
Speaker 3:  Okay. Which

173
00:10:47,565 --> 00:10:51,015
Speaker 4:  That, that's the part that's really I'm hanging up on because like

174
00:10:51,555 --> 00:10:54,575
Speaker 4:  my mom gave me this beautiful pen earlier this year that also weighs about

175
00:10:54,575 --> 00:10:57,295
Speaker 4:  the same as a tennis ball. And I'm like, oh I wanna wear this really pretty

176
00:10:57,315 --> 00:11:00,695
Speaker 4:  pen. It's this big gold pen. It's cool. And I'm like, I wanna wear it. I

177
00:11:00,695 --> 00:11:04,255
Speaker 4:  can't wear it with anything because fabrics like

178
00:11:04,405 --> 00:11:07,815
Speaker 4:  just crumple under it And you can see it. What's wild is you can see it in

179
00:11:07,815 --> 00:11:11,615
Speaker 4:  the video, you can see it in the New York Times piece too, where it's just

180
00:11:11,615 --> 00:11:15,495
Speaker 4:  like the, the fabric around the Pin is just crumpling around the weight

181
00:11:15,495 --> 00:11:18,575
Speaker 4:  of the Pin. And I'm like, okay, so this won't work with the T-shirt 'cause

182
00:11:18,575 --> 00:11:21,535
Speaker 4:  it's gonna drag the whole side down. Like you can watch him and he's wearing

183
00:11:21,615 --> 00:11:25,575
Speaker 4:  a shirt, his whole side's like one side of the collar's dragging down from

184
00:11:25,575 --> 00:11:29,535
Speaker 4:  this thing. Yeah. I'm like, why would you? No. Like who wants that?

185
00:11:30,255 --> 00:11:30,855
Speaker 4:  I don't want that.

186
00:11:31,085 --> 00:11:33,215
Speaker 3:  Well you haven't gotten to the battery boosters yet.

187
00:11:33,275 --> 00:11:33,775
Speaker 4:  Oh, sorry.

188
00:11:33,775 --> 00:11:36,415
Speaker 3:  Sorry. I think it's, it's a tennis ball without the batteries.

189
00:11:37,715 --> 00:11:40,575
Speaker 3:  So I would say a huge part of the Humane story

190
00:11:42,545 --> 00:11:46,045
Speaker 3:  is what you might call self seriousness. There are other words you could

191
00:11:46,045 --> 00:11:49,725
Speaker 3:  use, but self seriousness I think really

192
00:11:49,725 --> 00:11:53,605
Speaker 3:  captures it. 'cause I think they're sincere. Yes. And I think that means

193
00:11:53,605 --> 00:11:57,125
Speaker 3:  they're blinded by some of the inherent

194
00:11:57,195 --> 00:12:01,125
Speaker 3:  absurdities of the product. And I'll just give you this example which has

195
00:12:01,275 --> 00:12:05,205
Speaker 3:  made me giggle all day. And this is very clever. Again,

196
00:12:05,285 --> 00:12:07,085
Speaker 3:  I think they're sincere. I think they came up with the solutions. It's very

197
00:12:07,085 --> 00:12:10,845
Speaker 3:  clever. The thing obviously choose through battery. Yeah.

198
00:12:11,245 --> 00:12:11,965
Speaker 3:  'cause it's small

199
00:12:13,565 --> 00:12:17,105
Speaker 3:  and it has an always on wireless connection and it's running GPT and it's

200
00:12:17,105 --> 00:12:20,385
Speaker 3:  got around the mics and it's, it has a laser projector in it.

201
00:12:21,455 --> 00:12:25,315
Speaker 3:  So instead of making it big, they're doing an inductive

202
00:12:25,315 --> 00:12:29,135
Speaker 3:  battery that is the clip. Yeah. So the back of the

203
00:12:29,135 --> 00:12:32,745
Speaker 3:  magnetized clip contains the battery or more battery.

204
00:12:32,965 --> 00:12:36,425
Speaker 3:  So, and you can swap them so that the computer part has a battery in it.

205
00:12:36,645 --> 00:12:40,225
Speaker 3:  And then the back of the thing is called the battery booster and you can

206
00:12:40,225 --> 00:12:44,065
Speaker 3:  hot swap them because the computer part has a battery. Yeah. This is

207
00:12:44,065 --> 00:12:47,225
Speaker 3:  very clever. I don't want to take one ounce. If you were to solve this problem,

208
00:12:47,225 --> 00:12:50,385
Speaker 3:  this is very clever. They're doing this cool inductive thing. Many questions.

209
00:12:50,445 --> 00:12:54,385
Speaker 3:  If the fabric is really thick, does that lower the transfer power? Like all

210
00:12:54,385 --> 00:12:58,265
Speaker 3:  this stuff, but whatever. It's very clever Humane, I

211
00:12:58,265 --> 00:13:02,105
Speaker 3:  just watch this video. They're like, this is a perpetual power

212
00:13:02,125 --> 00:13:02,545
Speaker 3:  system.

213
00:13:04,205 --> 00:13:07,665
Speaker 3:  And it's like my guys, it's a battery. Like

214
00:13:08,245 --> 00:13:11,945
Speaker 3:  is my anchor USBC battery pack a perpetual power

215
00:13:11,945 --> 00:13:15,465
Speaker 3:  system? Yeah. And I think the answer is yes. The apple

216
00:13:15,695 --> 00:13:19,545
Speaker 3:  MagSafe battery attachment is a perpetual bat. Like what are we doing?

217
00:13:20,375 --> 00:13:23,625
Speaker 3:  Like the, the overtalk of the product is out of control. Well didn't David

218
00:13:23,685 --> 00:13:27,425
Speaker 6:  Say that most of the people at Humane used to work at Apple? So doesn't that

219
00:13:27,455 --> 00:13:29,585
Speaker 6:  explain the branding? Yeah.

220
00:13:29,765 --> 00:13:33,745
Speaker 4:  It kind of reminded me of a lot of times you'll, you'll get a really good

221
00:13:33,745 --> 00:13:37,305
Speaker 4:  writer out there in the world like writing like a TV show and they go and

222
00:13:37,305 --> 00:13:39,865
Speaker 4:  they write this really cool TV show and you're like, oh man, I wish they

223
00:13:39,885 --> 00:13:42,785
Speaker 4:  ran that show. That would be so cool. Instead of just being like a writer,

224
00:13:43,105 --> 00:13:47,065
Speaker 4:  I want them to run it. And then they go and they run it and no one's

225
00:13:47,065 --> 00:13:50,065
Speaker 4:  there to say, Hey, that's a bad idea. And so you just get

226
00:13:50,555 --> 00:13:53,505
Speaker 4:  unfiltered writer and it's, that's the sa it felt the same way.

227
00:13:53,505 --> 00:13:56,065
Speaker 3:  In fact Alex, I would suggest that the exact opposite happened. Oh

228
00:13:56,325 --> 00:13:56,545
Speaker 4:  You

229
00:13:56,545 --> 00:13:57,105
Speaker 3:  Think, which

230
00:13:57,105 --> 00:13:59,145
Speaker 4:  You mean. Okay. You think these are the bad writers?

231
00:13:59,605 --> 00:14:03,545
Speaker 3:  No, I think that, I mean Iran and, and Bethany, the two leaders of the

232
00:14:03,665 --> 00:14:06,985
Speaker 3:  company worked at Apple that they have a long history. They've built some

233
00:14:06,985 --> 00:14:10,785
Speaker 3:  amazing things. Their names are on some very important technologies. Lemme

234
00:14:10,785 --> 00:14:14,545
Speaker 3:  just read you this section from the Times profile, which I

235
00:14:14,545 --> 00:14:16,665
Speaker 3:  encourage everyone to read. And again, I began

236
00:14:17,125 --> 00:14:18,425
Speaker 4:  The podcast. I mean I can already see it

237
00:14:18,685 --> 00:14:22,305
Speaker 3:  By saying we have written a lot of access based

238
00:14:22,425 --> 00:14:25,985
Speaker 3:  launch features. I love 'em. I love doing 'em. I love reading 'em.

239
00:14:26,495 --> 00:14:29,865
Speaker 3:  They're, they're some of the most fun things you can do. I have no qualms

240
00:14:29,865 --> 00:14:33,555
Speaker 3:  against it, but you add the self seriousness of

241
00:14:33,555 --> 00:14:36,235
Speaker 3:  Humane to the institutional self seriousness of the times.

242
00:14:37,575 --> 00:14:40,715
Speaker 3:  And it's, it's a gold, like every line of this is gold. So

243
00:14:41,505 --> 00:14:44,995
Speaker 3:  when like the writer, no one told them to stop. In fact the exact

244
00:14:45,315 --> 00:14:49,155
Speaker 3:  opposite happened. A Buddhist monk named Brother Spirit led

245
00:14:49,155 --> 00:14:49,955
Speaker 3:  them to Humane.

246
00:14:51,505 --> 00:14:55,445
Speaker 3:  Mr. Chadri and Ms. Angiano had developed concepts for two AI products,

247
00:14:55,765 --> 00:14:58,925
Speaker 3:  a women's health device in the Pin mm. Brother spirit

248
00:15:00,195 --> 00:15:02,285
Speaker 3:  whom they had met through their acupuncturist

249
00:15:04,315 --> 00:15:08,005
Speaker 3:  recommended that they share these ideas with his friend Mark

250
00:15:08,115 --> 00:15:08,605
Speaker 3:  Benioff,

251
00:15:10,225 --> 00:15:14,085
Speaker 3:  the founder of Salesforce, sitting beneath a palm

252
00:15:14,085 --> 00:15:17,885
Speaker 3:  tree on a cliff above the ocean at Mr. Benioff's Hawaiian home.

253
00:15:18,275 --> 00:15:22,125
Speaker 3:  They explained both devices. This one, Mr. Benioff said

254
00:15:22,125 --> 00:15:25,845
Speaker 3:  pointing at the Pin as Dolphins breached to the surf below

255
00:15:26,465 --> 00:15:30,365
Speaker 3:  is huge. It's going to be a massive company. He said

256
00:15:30,825 --> 00:15:33,005
Speaker 4:  He was right about one thing. It is huge.

257
00:15:33,565 --> 00:15:36,805
Speaker 3:  I just wanna know if the Dolphins thing passed the Times fact check. Yeah.

258
00:15:37,505 --> 00:15:41,005
Speaker 3:  Do they have three sources on the Dolphins? Incredible.

259
00:15:41,725 --> 00:15:44,565
Speaker 3:  The best part about that whole thing by the way, is that Benioff owns Time

260
00:15:44,845 --> 00:15:48,645
Speaker 3:  Magazine and then Time Magazine just like immediately named the AI Pin best

261
00:15:48,645 --> 00:15:52,525
Speaker 3:  invention of 2023. Like before it'd ever been announced. Yeah, it's

262
00:15:52,525 --> 00:15:55,725
Speaker 3:  good. Yeah. All that's great. That's what I mean by self seriousness.

263
00:15:56,355 --> 00:16:00,065
Speaker 3:  Like maybe this thing's really cool. Maybe having a voice assistant that

264
00:16:00,065 --> 00:16:03,945
Speaker 3:  talks to ChatGPT BT is a really good idea. Mm.

265
00:16:05,215 --> 00:16:07,305
Speaker 3:  It's $700, it costs $24 a month.

266
00:16:08,915 --> 00:16:09,955
Speaker 3:  I don't know about that. I, I

267
00:16:09,955 --> 00:16:13,875
Speaker 5:  Think the, the like very basic thing, like if you describe it the way you

268
00:16:13,875 --> 00:16:17,195
Speaker 5:  just described it, which is a, a voice assistant for talking to chat GPT,

269
00:16:17,435 --> 00:16:20,395
Speaker 5:  I think that is like a real thing that a lot of people are going to have

270
00:16:20,455 --> 00:16:24,075
Speaker 5:  at some point in the very near future. That makes perfect sense to me. Like

271
00:16:24,435 --> 00:16:27,755
Speaker 5:  I have talked to a lot of people who think that there is a really interesting

272
00:16:28,375 --> 00:16:31,715
Speaker 5:  set of hardware to be made around some of these AI things, right? We heard

273
00:16:31,715 --> 00:16:35,635
Speaker 5:  about Sam Altman and Johnny, ive working on some of this stuff

274
00:16:35,795 --> 00:16:39,115
Speaker 5:  together. Like how do you access these language models from an interface

275
00:16:39,495 --> 00:16:43,395
Speaker 5:  is super interesting. What this feels like to

276
00:16:43,395 --> 00:16:47,355
Speaker 5:  me is like, do you remember that period of Samsung where they never

277
00:16:47,385 --> 00:16:51,075
Speaker 5:  said no to anything that it was like any engineer and product manager who

278
00:16:51,075 --> 00:16:53,795
Speaker 5:  was like, I have an idea about how something should work and where a button

279
00:16:53,795 --> 00:16:57,635
Speaker 5:  should go. They just said yes. So there were 40,000 ways to do everything

280
00:16:58,135 --> 00:17:01,515
Speaker 5:  and nothing. None of it actually made any sort of coherent sense.

281
00:17:02,135 --> 00:17:05,155
Speaker 5:  That's what this seems like to me is they like built a bunch of tech demos

282
00:17:05,655 --> 00:17:08,995
Speaker 5:  and then were just like, yes, this is it.

283
00:17:10,465 --> 00:17:14,435
Speaker 3:  Well no, it's yes to anything but a screen. Right, right. Because there's

284
00:17:14,435 --> 00:17:17,555
Speaker 3:  one really easy way to interact with AI chatbots

285
00:17:18,335 --> 00:17:21,955
Speaker 3:  and it's the chat GPT app on the iPhone.

286
00:17:22,415 --> 00:17:23,275
Speaker 3:  You can just get it.

287
00:17:23,925 --> 00:17:27,695
Speaker 5:  Yeah. Which is It works pretty well. Also has really good voice assistant

288
00:17:27,695 --> 00:17:27,895
Speaker 5:  stuff

289
00:17:27,895 --> 00:17:30,415
Speaker 4:  Now. And I was gonna say Alex, do you and I have both used the Ray bands?

290
00:17:30,475 --> 00:17:34,175
Speaker 4:  Yes. From from from Meta. I, I like 'em. I was wearing 'em on the way to

291
00:17:34,175 --> 00:17:37,975
Speaker 4:  work today. They were great. I infinitely prefer that to

292
00:17:38,135 --> 00:17:41,535
Speaker 4:  this. What about you as a way for accessing voice,

293
00:17:42,075 --> 00:17:45,815
Speaker 6:  For accessing voice? It helps to have something that can sit on your body

294
00:17:46,205 --> 00:17:50,005
Speaker 6:  that isn't dependent on the clothes you wear, I would say. Yeah.

295
00:17:50,345 --> 00:17:54,205
Speaker 6:  And You know, speaking of Sam Altman Neli, what did Sam Altman say about

296
00:17:54,265 --> 00:17:56,285
Speaker 6:  Humane in that Times piece?

297
00:17:57,125 --> 00:17:58,165
Speaker 3:  The time piece is so good.

298
00:17:59,545 --> 00:18:02,645
Speaker 6:  So Sam Altman is a, is a large investor in Humane

299
00:18:03,705 --> 00:18:04,725
Speaker 6:  and he's and de

300
00:18:04,885 --> 00:18:08,085
Speaker 5:  Seemingly every other AI thing happening right now, right? Like anybody who's

301
00:18:08,085 --> 00:18:10,525
Speaker 5:  making a hardware, it's like, yep, Sam's there,

302
00:18:10,725 --> 00:18:10,925
Speaker 3:  Right?

303
00:18:10,925 --> 00:18:11,525
Speaker 6:  But yeah, So

304
00:18:11,605 --> 00:18:14,125
Speaker 3:  I, I was talking to our friend Joanna Stern at the journal.

305
00:18:15,665 --> 00:18:19,285
Speaker 3:  She also did not get to interact with the AI Pin.

306
00:18:19,465 --> 00:18:21,965
Speaker 3:  But when I say like they didn't want reviewers there, people who were gonna

307
00:18:21,965 --> 00:18:25,205
Speaker 3:  ask technical questions about how things work. They got reporters there.

308
00:18:25,275 --> 00:18:28,445
Speaker 3:  They in very good reporters went to this So I don't mean to, yeah, it's just

309
00:18:28,485 --> 00:18:32,325
Speaker 3:  a different skill set, a different mindset, whatever. No reviewers have seen

310
00:18:32,325 --> 00:18:35,405
Speaker 3:  this thing. Tech reporters have seen it and they wrote that's great. And

311
00:18:35,635 --> 00:18:38,845
Speaker 3:  many newsrooms have a split between news and reviews. We kind of don't. It's

312
00:18:39,015 --> 00:18:41,525
Speaker 3:  where a tech publication is different, but Joanna didn't get to go and see

313
00:18:41,525 --> 00:18:44,965
Speaker 3:  it. And so she and I were just talking about all of this and she pointed

314
00:18:44,985 --> 00:18:48,885
Speaker 3:  out that Sam Altman, an investor in Humane, was on stage with her

315
00:18:48,885 --> 00:18:52,765
Speaker 3:  at her conference last month. And he, she asked him if he was gonna

316
00:18:52,765 --> 00:18:56,645
Speaker 3:  make a phone and he said, no, phones are great. There's no point in competing

317
00:18:56,645 --> 00:18:59,325
Speaker 3:  with a phone. And then there's these rumors he's working with Johnny Ive,

318
00:18:59,985 --> 00:19:03,685
Speaker 3:  and he said, ah, You know like Demurred. And he said, I think there's something

319
00:19:03,685 --> 00:19:06,965
Speaker 3:  great to do in AI hardware, but I don't know what it is yet.

320
00:19:08,375 --> 00:19:09,085
Speaker 3:  Which is like

321
00:19:09,605 --> 00:19:10,565
Speaker 4:  A month ago. A month.

322
00:19:11,305 --> 00:19:14,965
Speaker 3:  Do you think it's the company you invested in? Maybe. Maybe he's just taking

323
00:19:14,965 --> 00:19:18,845
Speaker 3:  shots. And then today in the Times he's an investor. So the Times

324
00:19:18,845 --> 00:19:22,765
Speaker 3:  talked to him, Sam, I would say waffled,

325
00:19:23,825 --> 00:19:26,805
Speaker 3:  no guarantee of success. That will be up to customers to decide. Maybe it's

326
00:19:26,805 --> 00:19:29,205
Speaker 3:  a bridge too far. Or maybe people are like, this is better than my phone.

327
00:19:29,465 --> 00:19:32,525
Speaker 3:  And then he says to the times, plenty of technology, it looks like a Sher

328
00:19:32,525 --> 00:19:35,125
Speaker 3:  bet ends up selling for 90% off at Best Buy.

329
00:19:36,655 --> 00:19:37,565
Speaker 3:  Which is brutal.

330
00:19:37,565 --> 00:19:38,565
Speaker 4:  Brutal. The Zune

331
00:19:40,155 --> 00:19:41,805
Speaker 3:  Here, the Zune look like a sherbet.

332
00:19:42,215 --> 00:19:46,165
Speaker 6:  We'll we'll talk more about this later, but I was with Sam on

333
00:19:46,165 --> 00:19:50,085
Speaker 6:  Monday at OpenAI. He talked about consumer hardware. He didn't

334
00:19:50,085 --> 00:19:53,765
Speaker 6:  mention Humane and this was days ahead of the

335
00:19:53,765 --> 00:19:56,605
Speaker 6:  public. Everyone knew Humane was about to launch. He didn't acknowledge it,

336
00:19:56,905 --> 00:19:59,605
Speaker 6:  but he got asked about this. We can talk more about this later, but yeah,

337
00:19:59,655 --> 00:20:03,525
Speaker 6:  there is this very interesting pressure happening where people are

338
00:20:03,525 --> 00:20:07,205
Speaker 6:  talking about how can OpenAI intersect with

339
00:20:07,445 --> 00:20:11,285
Speaker 6:  a consumer device. And Sam does not seem to think Humane is

340
00:20:11,285 --> 00:20:11,845
Speaker 6:  that device

341
00:20:12,165 --> 00:20:12,605
Speaker 3:  Because

342
00:20:12,675 --> 00:20:16,365
Speaker 4:  He's invested in this. And also like there's a a necklace too. Like an

343
00:20:16,545 --> 00:20:18,845
Speaker 4:  AI powered necklace. Yeah. And

344
00:20:20,095 --> 00:20:21,655
Speaker 3:  Re it's like the Rewinder Rewind. Yeah,

345
00:20:21,655 --> 00:20:22,335
Speaker 4:  The rewind or something.

346
00:20:22,805 --> 00:20:24,295
Speaker 5:  Yeah, the rewind pendant it's

347
00:20:24,295 --> 00:20:27,055
Speaker 3:  Called. Yeah. All this is great. And. what they're all trying to run away

348
00:20:27,055 --> 00:20:30,535
Speaker 3:  from is the phone. Yeah. Because the second you put a screen on it, this

349
00:20:30,535 --> 00:20:33,855
Speaker 3:  you've made a little phone. And then once you've made a little phone, and

350
00:20:33,855 --> 00:20:37,655
Speaker 3:  we've seen this with every other little phone, you've gotta have Instagram,

351
00:20:38,315 --> 00:20:42,295
Speaker 3:  you've right. Bank apps have to be there. All the stuff that people expect

352
00:20:42,295 --> 00:20:46,135
Speaker 3:  from phones just come for the package. You have to deal with

353
00:20:46,455 --> 00:20:50,365
Speaker 3:  whatever Epic and Google are doing in their trial this week. Right. You

354
00:20:50,415 --> 00:20:54,165
Speaker 3:  can't like, it's too hard. The mountain is too high to climb. So you

355
00:20:54,235 --> 00:20:58,005
Speaker 3:  take the screen off it and you're like, and again, in the Times Fe and in

356
00:20:58,005 --> 00:21:01,765
Speaker 3:  all of the marketing for the Pin, they're like, this is to solve your

357
00:21:01,765 --> 00:21:05,245
Speaker 3:  relationship with your phone. And they are very clear

358
00:21:05,795 --> 00:21:09,525
Speaker 3:  that you should not need a phone. Right. That you're

359
00:21:09,655 --> 00:21:13,485
Speaker 3:  $700, that's the cost of a phone. $24 a month.

360
00:21:13,485 --> 00:21:17,365
Speaker 3:  That's a cost of a data plan. If you've got this and your phone in your pocket,

361
00:21:17,365 --> 00:21:18,365
Speaker 3:  you're doing something wrong.

362
00:21:18,515 --> 00:21:21,685
Speaker 4:  They sent mixed messages there because in the New York Times piece, they

363
00:21:21,685 --> 00:21:25,645
Speaker 4:  end it with saying, oh we we're still using our phones,

364
00:21:26,425 --> 00:21:29,605
Speaker 4:  we just, we use 'em differently. Yeah, but we're still using them. I mean

365
00:21:29,645 --> 00:21:31,965
Speaker 4:  I guess you, you were also still using a phone just differently.

366
00:21:32,025 --> 00:21:36,005
Speaker 3:  So they did answer some questions in the presentation today that I've had

367
00:21:36,005 --> 00:21:38,605
Speaker 3:  for a long time. How does it know your contacts, if you wanna make a call,

368
00:21:38,735 --> 00:21:42,725
Speaker 3:  where do your photos go? All this stuff. When you set up, when you go to

369
00:21:42,725 --> 00:21:46,685
Speaker 3:  order one, you import all your contacts and then it

370
00:21:46,685 --> 00:21:50,205
Speaker 3:  arrives with your contacts preloaded. This is, this is what they want you

371
00:21:50,205 --> 00:21:53,405
Speaker 3:  to do then. Then there's a website called Humane Center

372
00:21:53,975 --> 00:21:57,285
Speaker 3:  where your photos are and your videos are, well your videos when video recording

373
00:21:57,285 --> 00:21:58,525
Speaker 3:  is released, they haven't, what

374
00:21:58,525 --> 00:22:02,405
Speaker 6:  Happens if I meet someone and I add their contact? How do I do

375
00:22:02,405 --> 00:22:04,285
Speaker 6:  that? How does that then get on the Pin

376
00:22:04,815 --> 00:22:05,445
Speaker 4:  Chest bump?

377
00:22:08,085 --> 00:22:10,765
Speaker 3:  I don't know man. These are all, these are all questions that like phones

378
00:22:10,765 --> 00:22:13,885
Speaker 3:  have solved really well and I think this is why when people ask Altman about

379
00:22:13,885 --> 00:22:17,485
Speaker 3:  it, he knows that whatever AI device

380
00:22:17,795 --> 00:22:21,005
Speaker 3:  arrives is gonna sit right next to your phone.

381
00:22:21,035 --> 00:22:24,885
Speaker 6:  Yeah. And also what is a phone? This thing comes with a phone number.

382
00:22:25,145 --> 00:22:28,885
Speaker 6:  It makes calls. It is a phone. What is a phone? It's a phone. It's a

383
00:22:28,885 --> 00:22:31,925
Speaker 6:  Screenless phone. They made a Screenless phone. So let's stop calling it

384
00:22:32,125 --> 00:22:33,125
Speaker 6:  anything else but that, that's

385
00:22:33,125 --> 00:22:33,205
Speaker 4:  Fair.

386
00:22:33,525 --> 00:22:37,045
Speaker 5:  I don't think the idea that you have to have this and your phone is like

387
00:22:37,165 --> 00:22:40,725
Speaker 5:  a huge strike against Humane. I think we're, we're in this

388
00:22:41,455 --> 00:22:45,405
Speaker 5:  phase now where I think every gadget maker is out there trying to sort

389
00:22:45,405 --> 00:22:48,525
Speaker 5:  of disintermediate the phone with the recognition that like, you're gonna

390
00:22:48,525 --> 00:22:52,245
Speaker 5:  keep having your phone in your pocket. What can we give you that is better

391
00:22:52,305 --> 00:22:55,885
Speaker 5:  and faster? And like it's certainly true that if you want to

392
00:22:56,105 --> 00:22:59,965
Speaker 5:  ask chat GPT a question, the Pin is going to be a faster way to do it

393
00:23:00,075 --> 00:23:03,365
Speaker 5:  than your phone. And that's meaningful. That's real. You don't have to fish

394
00:23:03,405 --> 00:23:05,765
Speaker 5:  a thing outta your pocket. You don't have to unlock a thing. You don't have

395
00:23:05,765 --> 00:23:08,245
Speaker 5:  to open an app. Like that's a better system.

396
00:23:08,575 --> 00:23:11,005
Speaker 3:  Right, right. Until that thing is built into your AirPods,

397
00:23:11,065 --> 00:23:14,725
Speaker 6:  It already is. You can already, with shortcuts on iOS, you can already ask

398
00:23:14,995 --> 00:23:16,805
Speaker 6:  chat GPT things through your AirPods.

399
00:23:18,305 --> 00:23:22,285
Speaker 5:  No Alex. The minute a sentence starts with shortcuts on iOS, like I'm

400
00:23:22,285 --> 00:23:22,365
Speaker 5:  out.

401
00:23:22,565 --> 00:23:23,085
Speaker 3:  Sure. Like,

402
00:23:23,085 --> 00:23:26,885
Speaker 6:  Oh so, so that's harder for you than buying an $800 Pin that you have to

403
00:23:26,915 --> 00:23:29,885
Speaker 6:  sync on the web and pay a monthly. No

404
00:23:29,885 --> 00:23:31,885
Speaker 3:  Alex, you can listen to title on it.

405
00:23:33,005 --> 00:23:34,125
Speaker 4:  I forgot about title.

406
00:23:34,915 --> 00:23:38,565
Speaker 5:  Yeah. The persona speakers. But no, but I think the, the problem

407
00:23:38,715 --> 00:23:42,565
Speaker 5:  with a thing like this, and a problem with a lot of these LLMs is

408
00:23:42,565 --> 00:23:46,515
Speaker 5:  they don't do anything. They're really good like information

409
00:23:46,755 --> 00:23:50,035
Speaker 5:  retrieval systems and they're useful for generating text and images. But

410
00:23:50,035 --> 00:23:53,955
Speaker 5:  like I, I want to, I want to add to my contacts,

411
00:23:54,025 --> 00:23:57,835
Speaker 5:  that is not a thing Chat GPT is capable of and Humane is gonna

412
00:23:57,895 --> 00:24:00,955
Speaker 5:  try to be the platform for all of that in the same way that OpenAI is trying

413
00:24:00,955 --> 00:24:04,395
Speaker 5:  to be the platform for that the like the real race underlying all of this

414
00:24:04,415 --> 00:24:08,155
Speaker 5:  is to be the AI app store. Like that is, that is the fight here and

415
00:24:08,155 --> 00:24:12,075
Speaker 5:  everybody wants to do it. And until someone wins

416
00:24:12,105 --> 00:24:15,675
Speaker 5:  that battle, the list of things you can actually do by

417
00:24:15,995 --> 00:24:17,755
Speaker 5:  interacting with an LLM is pretty small.

418
00:24:17,845 --> 00:24:20,835
Speaker 3:  We're gonna get that because that's the OpenAI story, right?

419
00:24:21,135 --> 00:24:24,635
Speaker 6:  Yes. But D David, what David is hitting on is very important here, which

420
00:24:24,635 --> 00:24:28,155
Speaker 6:  is that You know you saying ChatGPT BT is a good information

421
00:24:28,355 --> 00:24:32,155
Speaker 6:  retrieval system. I would argue that it's not, and the problem

422
00:24:32,155 --> 00:24:35,995
Speaker 6:  with this Humane product is that it's core tech, which is

423
00:24:35,995 --> 00:24:39,555
Speaker 6:  like talk to OpenAI, guess what? ChatGPT BT makes

424
00:24:39,745 --> 00:24:43,555
Speaker 6:  shit up all the time. Like and they know this and

425
00:24:43,615 --> 00:24:46,755
Speaker 6:  Sam talked about this with me and other reporters on Monday and he doesn't

426
00:24:46,755 --> 00:24:49,835
Speaker 6:  think this problem will be solved for like three years.

427
00:24:50,575 --> 00:24:54,515
Speaker 6:  So this is a at least, so this is a device you're buying where the

428
00:24:54,515 --> 00:24:58,195
Speaker 6:  primary interface, information retrieval system, the people who designed

429
00:24:58,195 --> 00:25:02,155
Speaker 6:  it know that it has still a huge accuracy problem. I was using

430
00:25:02,345 --> 00:25:05,755
Speaker 6:  chat GPT to try to do some earnings coverage. It just made the financials

431
00:25:05,755 --> 00:25:09,715
Speaker 6:  up for the company. I was so, so imagine walking around and this

432
00:25:09,715 --> 00:25:13,275
Speaker 6:  is your only device on you and you ask it something and it's like, yeah,

433
00:25:13,275 --> 00:25:16,955
Speaker 6:  take a left to go here and you end up like walking into oncoming

434
00:25:17,155 --> 00:25:18,795
Speaker 6:  traffic. Right. So

435
00:25:18,795 --> 00:25:22,755
Speaker 3:  This raises another important point, which is we still dunno how It

436
00:25:22,755 --> 00:25:25,555
Speaker 3:  works. Like even in all the coverage you can read and in their demos,

437
00:25:26,695 --> 00:25:30,545
Speaker 3:  we don't know like is it OpenAI underneath

438
00:25:30,545 --> 00:25:34,265
Speaker 3:  it in the press release? It just says Microsoft and OpenAI models

439
00:25:34,735 --> 00:25:36,385
Speaker 3:  doesn't say which ones. They said

440
00:25:36,385 --> 00:25:39,825
Speaker 5:  In a couple of the stories that it's, it's chat GPT primarily So I think,

441
00:25:39,865 --> 00:25:41,745
Speaker 5:  I think we can fairly safely assume it's a

442
00:25:42,295 --> 00:25:45,105
Speaker 6:  ChatGPT they're they're using open AI's. API. Yeah.

443
00:25:45,115 --> 00:25:48,985
Speaker 3:  Right. But then the, there's the Google logo sitting right in their

444
00:25:48,985 --> 00:25:52,225
Speaker 3:  press release and I don't know why. Then they claim they have their own platform

445
00:25:52,225 --> 00:25:55,865
Speaker 3:  called my AI and maybe it is all just chat GBT

446
00:25:56,125 --> 00:25:57,905
Speaker 4:  And it's all cloud-based, right?

447
00:25:58,295 --> 00:26:00,825
Speaker 3:  Yeah. Which I think is also why it burns through batteries because

448
00:26:00,825 --> 00:26:04,785
Speaker 4:  Yeah, well it burns through batteries and it also, that just cuts off a

449
00:26:04,785 --> 00:26:08,025
Speaker 4:  huge user base. There's a ton of people who can't have ready access to the

450
00:26:08,025 --> 00:26:11,945
Speaker 4:  cloud all the time. Right. Like internet is inconsistent. They weren't gonna

451
00:26:11,945 --> 00:26:12,465
Speaker 4:  buy it anyway.

452
00:26:12,465 --> 00:26:14,265
Speaker 3:  Yeah. You think the people in rural America, or

453
00:26:14,445 --> 00:26:18,345
Speaker 4:  Are you saying they were not watching the CAPI show's to see it like, oh

454
00:26:18,345 --> 00:26:18,505
Speaker 4:  my

455
00:26:18,505 --> 00:26:21,425
Speaker 3:  God it's it's running on the T-Mobile network. Yeah. Those people were already

456
00:26:21,425 --> 00:26:21,825
Speaker 3:  left out.

457
00:26:25,655 --> 00:26:29,065
Speaker 3:  It's just like even that stuff, right? It primarily the ChatGPT

458
00:26:29,065 --> 00:26:32,545
Speaker 3:  B-T-A-P-I, that's what we think it is. We don't know.

459
00:26:32,845 --> 00:26:35,825
Speaker 3:  No one's actually used it to figure out its limits. No one's actually used

460
00:26:35,825 --> 00:26:39,145
Speaker 3:  it to figure out if when you ask it for directions, it falls back to Google

461
00:26:39,295 --> 00:26:42,225
Speaker 3:  Maps or something like, or Bing Maps or whatever

462
00:26:43,335 --> 00:26:46,865
Speaker 3:  acquisition. No, like Microsoft made through the Nokia deal 500 years ago

463
00:26:46,865 --> 00:26:50,585
Speaker 3:  that they forgot about that still runs and somehow makes $5 million. Which

464
00:26:50,585 --> 00:26:52,385
Speaker 3:  is the true story of many Microsoft acquisitions.

465
00:26:53,925 --> 00:26:57,905
Speaker 3:  And because it doesn't have a screen, you, the user will never

466
00:26:57,905 --> 00:27:01,895
Speaker 3:  know. Like you'll ask it a question and it can never tell you how it arrived

467
00:27:01,895 --> 00:27:05,415
Speaker 3:  at the answer. Which has been a problem with voice assistance in general

468
00:27:05,555 --> 00:27:06,335
Speaker 3:  for a very long time.

469
00:27:06,475 --> 00:27:09,975
Speaker 6:  It could list sources, but still, like I don't, we'll talk more about this

470
00:27:09,975 --> 00:27:13,855
Speaker 6:  later. I don't trust chat GPT for like information because it

471
00:27:13,855 --> 00:27:17,415
Speaker 6:  hallucinates. Yeah. And so if you're thinking date like tactically this is

472
00:27:17,415 --> 00:27:20,575
Speaker 6:  my new phone, this is how I'm gonna interact with the internet.

473
00:27:21,355 --> 00:27:25,255
Speaker 6:  The main way that you're doing that, you cannot inherently trust

474
00:27:25,405 --> 00:27:29,335
Speaker 6:  what is being said back to you. How is that a replacement for the

475
00:27:29,335 --> 00:27:31,135
Speaker 6:  devices we already have? It doesn't make sense.

476
00:27:31,605 --> 00:27:35,175
Speaker 5:  Yeah. There's this interesting moment in the, the demo video that they put

477
00:27:35,175 --> 00:27:38,455
Speaker 5:  out, which everyone should watch. It's, it's Wild. Neli has suggestion to

478
00:27:38,455 --> 00:27:41,135
Speaker 5:  me it was to watch it at two x speed, which it seems like normal speed, which

479
00:27:41,135 --> 00:27:45,095
Speaker 5:  is good advice. But there's this one moment in it where he holds up I

480
00:27:45,095 --> 00:27:48,935
Speaker 5:  think a book to the Pin and says, how much does this

481
00:27:48,935 --> 00:27:52,335
Speaker 5:  cost online? He's, he's like demonstrating the shopping features and he's

482
00:27:52,335 --> 00:27:56,055
Speaker 5:  like, how much does this cost online? And it responds, it costs $28 online.

483
00:27:56,315 --> 00:28:00,195
Speaker 5:  And he says Buy it. And that's the end of the demo. And it's like, I have

484
00:28:00,315 --> 00:28:03,915
Speaker 5:  a million questions between here and there. Like

485
00:28:03,915 --> 00:28:06,955
Speaker 5:  there's only one price on the whole internet for this thing. Where did that

486
00:28:06,955 --> 00:28:09,715
Speaker 5:  price come from? Where am I buying it from? How fast is it gonna ship? Where's

487
00:28:09,715 --> 00:28:12,995
Speaker 5:  it gonna ship? Which credit card did it use? Like Amazon has gone through

488
00:28:12,995 --> 00:28:16,555
Speaker 5:  this problem where if you just say to your echo speaker

489
00:28:17,535 --> 00:28:20,755
Speaker 5:  by toilet paper, it turns out that actually takes you through a flow that

490
00:28:20,755 --> 00:28:24,115
Speaker 5:  you don't understand and can go wrong in a thousand ways. And people don't

491
00:28:24,115 --> 00:28:27,955
Speaker 5:  do it as a result. And the idea that Humane

492
00:28:27,955 --> 00:28:31,795
Speaker 5:  is going for is to abstract even more of that away and to just

493
00:28:31,855 --> 00:28:35,515
Speaker 5:  say, all you have to do is just describe to this thing what you wanna accomplish

494
00:28:35,515 --> 00:28:38,355
Speaker 5:  in the world and it will do it for you. And I think that's a super interesting

495
00:28:38,355 --> 00:28:40,875
Speaker 5:  and completely impossible. Yeah. The thesis,

496
00:28:41,515 --> 00:28:45,325
Speaker 3:  The price online is $28. It's like, is it Amazon?

497
00:28:45,505 --> 00:28:47,565
Speaker 3:  Do you think Amazon is the, the only store?

498
00:28:48,145 --> 00:28:49,325
Speaker 6:  Is it, who knows eBay?

499
00:28:49,325 --> 00:28:50,845
Speaker 3:  Like right. Who knows? Maybe

500
00:28:51,035 --> 00:28:54,925
Speaker 5:  It's an artist and it's like, by doing this, am I signing up for

501
00:28:54,955 --> 00:28:58,845
Speaker 5:  whoever Humane has decided to like Rev share partner with in order to be

502
00:28:58,845 --> 00:29:02,725
Speaker 5:  able to do this? Should I be able to choose? And at that point, if I'm choosing,

503
00:29:02,795 --> 00:29:05,925
Speaker 5:  what are we accomplishing by abstracting all of this away? There's just no

504
00:29:06,435 --> 00:29:09,765
Speaker 5:  version of this that feels good in the way that you kind of want it to.

505
00:29:09,905 --> 00:29:13,645
Speaker 3:  And by the way, the book is a easy example in the sort of medium

506
00:29:13,785 --> 00:29:17,765
Speaker 3:  scheme of things. Because the book has a barcode and maybe it has

507
00:29:17,765 --> 00:29:21,725
Speaker 3:  a barcode. Maybe he can see the title. Yeah. And right there's

508
00:29:22,325 --> 00:29:25,285
Speaker 3:  structured data. Yeah. On and around the book.

509
00:29:25,915 --> 00:29:29,805
Speaker 3:  There's another part of that demo where he just holds up a handful of almonds

510
00:29:30,625 --> 00:29:34,565
Speaker 3:  and he says how many, how much protein is in these almonds? And it just

511
00:29:34,655 --> 00:29:38,605
Speaker 3:  tells him it's 15 grams. Mm. And it's like, did it count

512
00:29:39,505 --> 00:29:43,445
Speaker 3:  the almonds? And I pointed out it must be kind of almonds. And Liam

513
00:29:43,445 --> 00:29:45,765
Speaker 3:  said, well technically you'd wanna weigh the almonds,

514
00:29:47,615 --> 00:29:50,845
Speaker 3:  right? Because you're inside of that Chad GBT is just making up

515
00:29:51,845 --> 00:29:55,725
Speaker 3:  grams per almond. Yes. Average. And then spitting out

516
00:29:55,805 --> 00:29:56,845
Speaker 3:  a number amount of protein

517
00:29:56,975 --> 00:30:00,885
Speaker 6:  Using a dubiously data collected source

518
00:30:01,195 --> 00:30:03,845
Speaker 6:  that could be accurate or could not be accurate. Yeah. Who knows that you

519
00:30:03,845 --> 00:30:04,565
Speaker 6:  would not know. Yeah.

520
00:30:04,705 --> 00:30:08,325
Speaker 3:  And it's like all you've made is like a very complicated lose it. And you

521
00:30:08,325 --> 00:30:11,525
Speaker 3:  don't know if it's counting the almonds accurately. That's weird.

522
00:30:12,275 --> 00:30:14,765
Speaker 6:  Yeah, I mean it's, I mean it's not good. There's

523
00:30:14,765 --> 00:30:16,245
Speaker 3:  No structured data about the almonds.

524
00:30:16,855 --> 00:30:20,245
Speaker 6:  There are a mil, I mean there are a million holes you can poke in this thing.

525
00:30:20,245 --> 00:30:23,765
Speaker 6:  And I think we're doing that appropriately. I think the direction Humane

526
00:30:23,765 --> 00:30:27,405
Speaker 6:  is trying to go here is really interesting. And I want to have competing

527
00:30:27,405 --> 00:30:30,125
Speaker 6:  visions for where computing is going in the world because everyone else,

528
00:30:30,715 --> 00:30:34,205
Speaker 6:  meta, et cetera, apple, they think the future of computing is

529
00:30:34,995 --> 00:30:38,285
Speaker 6:  more visuals in your eyes, right? Like literally

530
00:30:38,415 --> 00:30:42,165
Speaker 6:  augmenting over the world. And Humane is saying no, we think actually

531
00:30:42,195 --> 00:30:46,045
Speaker 6:  computing should be recessed more into the background and you should

532
00:30:46,365 --> 00:30:49,805
Speaker 6:  interact through voice. And it's more of that Spike Jones her future. And

533
00:30:49,805 --> 00:30:52,525
Speaker 6:  I think those different directions need to exist. People need to be building

534
00:30:52,525 --> 00:30:56,285
Speaker 6:  those different directions. It just feels very early. This feels like General

535
00:30:56,415 --> 00:31:00,045
Speaker 6:  Magic or Magic Leap. You know, there's a ton of examples of

536
00:31:00,285 --> 00:31:03,525
Speaker 6:  companies that were just too early to the tech. And I think this is the case

537
00:31:03,525 --> 00:31:03,765
Speaker 6:  here.

538
00:31:04,505 --> 00:31:07,325
Speaker 3:  No, I'll make a, I'll make a big distinction between those things because

539
00:31:07,425 --> 00:31:10,765
Speaker 3:  we, oh, now we're old and we know a bunch of external magic people. They

540
00:31:10,765 --> 00:31:14,245
Speaker 3:  were very sincere. Tony Fidel's friend of the show, he was part of the general

541
00:31:14,245 --> 00:31:17,925
Speaker 3:  magic story. He financed the documentary, a self-serving documentary.

542
00:31:18,305 --> 00:31:21,325
Speaker 3:  But you will take, 'cause they all come out as heroes like in the end.

543
00:31:22,505 --> 00:31:24,925
Speaker 3:  But it's, it's a fun watch. And I encourage you to go watch the General Magic

544
00:31:25,205 --> 00:31:29,165
Speaker 3:  documentary. They were nerds, like huge nerds. And

545
00:31:29,165 --> 00:31:32,365
Speaker 3:  they did not think they were cool and they thought they were inventing the

546
00:31:32,365 --> 00:31:35,605
Speaker 3:  future fine. But at the end of the day, they were like

547
00:31:36,095 --> 00:31:37,005
Speaker 3:  silly nerds.

548
00:31:37,525 --> 00:31:41,445
Speaker 6:  They didn't put their Pin on like fashion models in Paris fashion. They,

549
00:31:41,445 --> 00:31:43,445
Speaker 3:  They weren't, none of that stuff was happening. Right? Yeah. No, Capri show

550
00:31:44,175 --> 00:31:47,925
Speaker 3:  Magic Leap. Huge Nerds obviously took

551
00:31:47,925 --> 00:31:50,605
Speaker 3:  themselves so seriously. Well they

552
00:31:50,765 --> 00:31:51,805
Speaker 4:  Blew Beyonce away,

553
00:31:52,095 --> 00:31:55,925
Speaker 3:  Right? They had other people down there for demos. I think

554
00:31:55,925 --> 00:31:59,485
Speaker 3:  David got a demo once. Didn't you go down there at some point, David? I

555
00:31:59,485 --> 00:32:02,885
Speaker 5:  Got a demo. It wasn't, I didn't get the full treatment, but I did get to

556
00:32:02,885 --> 00:32:03,845
Speaker 5:  try it at one point. Yeah, yeah.

557
00:32:04,245 --> 00:32:07,565
Speaker 3:  They said things like we're hacking the GPU of the mind, which is,

558
00:32:07,955 --> 00:32:11,085
Speaker 3:  it's just an incredible phrase. They promised to reinvent

559
00:32:11,825 --> 00:32:14,885
Speaker 3:  all of entertainment. Yeah. All of technology

560
00:32:15,765 --> 00:32:19,535
Speaker 3:  that like this thing was, and they got so far ahead of the

561
00:32:19,535 --> 00:32:23,475
Speaker 3:  technology with the hype and the seriousness that

562
00:32:23,475 --> 00:32:26,595
Speaker 3:  there was no way the product could have lived up to their own ideas. General

563
00:32:26,595 --> 00:32:30,515
Speaker 3:  Magic was early, but they weren't, they knew that

564
00:32:30,515 --> 00:32:34,435
Speaker 3:  there was a thing that they were trying to get to that was far away

565
00:32:34,435 --> 00:32:37,795
Speaker 3:  from where they were. And they were honest about it, I think.

566
00:32:38,375 --> 00:32:42,145
Speaker 3:  And if that's the spectrum of companies that are too early, like sincere

567
00:32:42,155 --> 00:32:45,345
Speaker 3:  nerds trying to build the future and they're aware of their limitations and

568
00:32:45,575 --> 00:32:49,185
Speaker 3:  kind of silly and honest about it. And we've hyped ourselves so much

569
00:32:49,975 --> 00:32:53,825
Speaker 3:  that nothing we invent can meet the standard. Like Humane is

570
00:32:54,045 --> 00:32:57,625
Speaker 3:  beyond that. You think it's beyond Magic Leap? I think it's

571
00:32:57,865 --> 00:32:59,105
Speaker 3:  beyond magically. Wow. Wow.

572
00:32:59,165 --> 00:33:00,065
Speaker 4:  But it actually is

573
00:33:00,065 --> 00:33:03,945
Speaker 3:  Because they, they're out there. They have their employees ex Apple people

574
00:33:04,685 --> 00:33:08,425
Speaker 3:  in the press today saying they, they had retired from Apple

575
00:33:08,485 --> 00:33:12,185
Speaker 3:  and they joined Humane because they were disgusted

576
00:33:12,495 --> 00:33:16,385
Speaker 3:  with having invented the iPhone and destroyed society. And Humane

577
00:33:16,545 --> 00:33:20,505
Speaker 3:  is their chance to like reset. And it's like, you guys, I don't even

578
00:33:20,505 --> 00:33:24,385
Speaker 3:  know if it's counting the almonds You know. It's like imagine what

579
00:33:24,385 --> 00:33:28,345
Speaker 3:  bookstore You know. It's like that's crazy. And if you are, if you are

580
00:33:29,465 --> 00:33:32,945
Speaker 3:  positioning is that this will rescue us from the phone, but you've made a

581
00:33:32,945 --> 00:33:33,225
Speaker 3:  phone

582
00:33:35,525 --> 00:33:39,385
Speaker 3:  no nothing. There's, there's no product that can fill the hole

583
00:33:39,385 --> 00:33:42,505
Speaker 3:  that they are drawing. Right? And that is the

584
00:33:43,115 --> 00:33:47,105
Speaker 3:  issue I think with the, the product itself in many big ways, but

585
00:33:47,215 --> 00:33:50,985
Speaker 3:  also like how they're putting it out into market. But this

586
00:33:50,985 --> 00:33:54,225
Speaker 5:  Is also, this is just the world we live in now. I mean, I think about like

587
00:33:54,775 --> 00:33:58,425
Speaker 5:  Carl Pay and nothing, like he's running around being like, there are no interesting

588
00:33:58,625 --> 00:34:01,465
Speaker 5:  consumer electronics companies left. And it's like, well that just isn't

589
00:34:01,465 --> 00:34:05,345
Speaker 5:  true. But you have to like, you have to tell this big grand

590
00:34:05,435 --> 00:34:08,385
Speaker 5:  story about yourself and how you're changing the world. And it's like we

591
00:34:08,385 --> 00:34:11,785
Speaker 5:  still live in the, we're making the world a better place thing, which you

592
00:34:11,785 --> 00:34:15,585
Speaker 5:  would think Silicon Valley would've killed like by just

593
00:34:15,585 --> 00:34:19,105
Speaker 5:  being on that TV show. But it didn't. And So I think

594
00:34:19,625 --> 00:34:22,785
Speaker 5:  honestly like the, the single worst thing that Apple has done to tech culture

595
00:34:23,245 --> 00:34:26,265
Speaker 5:  is convince everybody that this is the right way to talk about your products.

596
00:34:26,375 --> 00:34:29,585
Speaker 5:  That everything has to be finished. Everything has to appear as if it was

597
00:34:29,585 --> 00:34:33,185
Speaker 5:  inevitable and just sort of rose out of the ground. This perfect finished

598
00:34:33,185 --> 00:34:36,745
Speaker 5:  product that we have solved every problem that has ever existed in the world.

599
00:34:37,165 --> 00:34:40,985
Speaker 5:  And there are no companies anymore who are making things being like, we

600
00:34:40,985 --> 00:34:44,305
Speaker 5:  made this. We don't know if it's any good. Do you want it? Ironically, OpenAI

601
00:34:44,325 --> 00:34:47,705
Speaker 5:  is probably the closest thing to that right now. Yeah. But it's, it's just

602
00:34:47,705 --> 00:34:51,025
Speaker 5:  not, it's just not how this works anymore. And the playbook

603
00:34:51,525 --> 00:34:54,665
Speaker 5:  worked so well for Apple that if you're a company who raised a lot of money

604
00:34:54,665 --> 00:34:57,785
Speaker 5:  and would like to make a lot of money, you just follow the Apple playbook.

605
00:34:57,845 --> 00:35:00,185
Speaker 5:  And I think the thing we've learned over and over is that it doesn't work

606
00:35:00,405 --> 00:35:02,305
Speaker 5:  for anybody other than Apple. I

607
00:35:02,305 --> 00:35:05,585
Speaker 4:  Don't think this is fair because there are a lot of interesting consumer

608
00:35:05,615 --> 00:35:09,025
Speaker 4:  tech companies that aren't doing this. They're not based in Silicon Valley.

609
00:35:09,025 --> 00:35:12,505
Speaker 4:  They're not, they're not in this like weird investor

610
00:35:13,015 --> 00:35:16,025
Speaker 4:  ecosystem, right? Where, where they're there there, there's almost this demand

611
00:35:16,025 --> 00:35:19,585
Speaker 4:  from their investors to ship something and to change the world at the same

612
00:35:19,585 --> 00:35:23,425
Speaker 4:  time. Right? Like there's plenty of weird tablets in China.

613
00:35:23,535 --> 00:35:27,145
Speaker 4:  China's doing a lot of cool consumer tech gadgets and not promising to change

614
00:35:27,145 --> 00:35:29,585
Speaker 4:  the world with their Steam Deck knockoffs.

615
00:35:29,585 --> 00:35:32,825
Speaker 3:  It would be amazing if Apple was like, we made these gamer lights. Yeah,

616
00:35:33,015 --> 00:35:33,705
Speaker 3:  they're awesome.

617
00:35:35,375 --> 00:35:38,045
Speaker 4:  They're just cool. Yeah, they're never gonna do that.

618
00:35:38,305 --> 00:35:41,485
Speaker 6:  But if you go back to like the launch of the original iPhone, Steve Dobbs,

619
00:35:41,585 --> 00:35:44,685
Speaker 6:  he didn't say it's gonna change the world. He was like, here are three things

620
00:35:44,745 --> 00:35:46,525
Speaker 6:  it can do at once. Well,

621
00:35:46,545 --> 00:35:49,205
Speaker 3:  No, he did start with today Apple reinvents the phone.

622
00:35:49,345 --> 00:35:50,645
Speaker 6:  But that's not, but that's

623
00:35:50,645 --> 00:35:52,485
Speaker 3:  Different with his arms outstretched. I would

624
00:35:52,485 --> 00:35:56,205
Speaker 6:  Argue that's different than what Humane's doing than what Magic Leap did.

625
00:35:56,205 --> 00:35:59,925
Speaker 6:  Which is like, it's the hubris, right? And Apple has hubris

626
00:36:00,425 --> 00:36:03,565
Speaker 6:  on You know they have tons of it and Steve Jobs was the king of hubris. But

627
00:36:03,565 --> 00:36:06,365
Speaker 6:  like yeah, you go back to that original iPhone, there was like an almost

628
00:36:06,365 --> 00:36:09,925
Speaker 6:  weird humility with the hubris, right? Where it was like we actually were

629
00:36:09,925 --> 00:36:13,725
Speaker 6:  just focusing on what it does. And what it did was such a aha moment

630
00:36:13,725 --> 00:36:16,565
Speaker 6:  where everybody in the room was like, oh my God, these three things together

631
00:36:16,625 --> 00:36:20,445
Speaker 6:  in a mobile form factor. Yes. You didn't have to overexplain it. Whereas

632
00:36:20,445 --> 00:36:24,085
Speaker 6:  like Humane, they, they can't explain it. Like, because it's,

633
00:36:24,155 --> 00:36:28,085
Speaker 6:  it's incomprehensible. There's, it's a, it's an idea in search of a

634
00:36:28,085 --> 00:36:31,325
Speaker 6:  product. Right? And and I think that was, that's the key difference here

635
00:36:31,325 --> 00:36:35,285
Speaker 6:  between like the early Apple where Apple is now and Humane I think.

636
00:36:35,385 --> 00:36:35,605
Speaker 6:  So

637
00:36:35,605 --> 00:36:39,285
Speaker 3:  Here's a question I have for you, Alex, just thinking about the, the cycle

638
00:36:39,545 --> 00:36:43,445
Speaker 3:  OpenAI is in. Yeah. Brother Spirit, the Buddhist monk introduced

639
00:36:43,445 --> 00:36:46,285
Speaker 3:  them to Mark Benioff in

640
00:36:46,785 --> 00:36:50,165
Speaker 3:  2018. The connection by the way, made through their acupuncturists

641
00:36:50,955 --> 00:36:54,765
Speaker 3:  very, this is all very important detail. 2018. That's a

642
00:36:54,765 --> 00:36:58,645
Speaker 3:  long time ago. Yeah. To, to start working on this. Pin OpenAI was

643
00:36:58,645 --> 00:37:01,765
Speaker 3:  nowhere in 2018, right? The

644
00:37:02,505 --> 00:37:05,605
Speaker 3:  the first glimmers of the thing were happening. They

645
00:37:05,605 --> 00:37:09,285
Speaker 6:  Were a research project. Yeah. They were, they were, they were maybe 60 people

646
00:37:09,565 --> 00:37:10,365
Speaker 6:  at that point. Yeah.

647
00:37:10,465 --> 00:37:13,325
Speaker 3:  And they were still kind of focused on not being a commercial company.

648
00:37:14,345 --> 00:37:17,365
Speaker 3:  So if you're making this product in 2018 and you have this vision,

649
00:37:19,355 --> 00:37:23,085
Speaker 3:  what is the technology that you're basing this on? What have they been

650
00:37:23,085 --> 00:37:24,165
Speaker 3:  working on? Oh,

651
00:37:24,425 --> 00:37:25,405
Speaker 4:  Oh, I, I know it. What

652
00:37:25,565 --> 00:37:28,285
Speaker 3:  Happened, right? There's, there's a pivot that's that's contained in the

653
00:37:28,285 --> 00:37:31,645
Speaker 3:  story of this product where they were like, you're gonna talk to chat GPT

654
00:37:31,645 --> 00:37:34,125
Speaker 3:  and that's the thing that's gonna sell it. Right? Because that's what we're

655
00:37:34,125 --> 00:37:38,005
Speaker 3:  talking about today. In 2018, you could not begin work on that

656
00:37:38,005 --> 00:37:38,525
Speaker 3:  product. Well

657
00:37:38,525 --> 00:37:42,165
Speaker 6:  Chat It didn't exist. Chat GPT. That's what I mean. It barely existed a year

658
00:37:42,165 --> 00:37:42,845
Speaker 6:  ago. Yeah.

659
00:37:42,845 --> 00:37:45,765
Speaker 4:  This was totally gonna be like an Android product until

660
00:37:46,585 --> 00:37:50,205
Speaker 4:  OpenAI popped off. Right. Like that that had to have been what happened.

661
00:37:50,235 --> 00:37:53,685
Speaker 4:  They were, they were building their own little android dumb like dumb phone.

662
00:37:53,885 --> 00:37:57,765
Speaker 4:  'cause it's effectively a dumb phone. Right? And, and then OpenAI

663
00:37:57,765 --> 00:38:01,045
Speaker 4:  popped off and they were like, oh, finally we have something cool to attach

664
00:38:01,045 --> 00:38:04,885
Speaker 4:  this to, to ship it so we're not just like we did a phone, but it's a Pin

665
00:38:05,345 --> 00:38:06,685
Speaker 4:  and not really a good phone.

666
00:38:06,685 --> 00:38:09,685
Speaker 3:  Yeah. I don't know. I there's just, I'm, I'm like thinking about this chronology

667
00:38:09,825 --> 00:38:12,965
Speaker 3:  and it's like in 2018 when you set off to make this product,

668
00:38:13,705 --> 00:38:17,565
Speaker 3:  you cannot assume that there will be an LLM explosion

669
00:38:17,785 --> 00:38:19,125
Speaker 3:  in mid 2023.

670
00:38:19,505 --> 00:38:20,245
Speaker 4:  No, but you could assume

671
00:38:20,245 --> 00:38:21,605
Speaker 3:  That recontextualizes this whole product,

672
00:38:21,865 --> 00:38:25,565
Speaker 4:  But you could assume Google was doing, Google was doing a lot at that point,

673
00:38:25,565 --> 00:38:28,605
Speaker 4:  right? They were doing a lot about visual identity, like, like being able

674
00:38:28,605 --> 00:38:32,045
Speaker 4:  to visually ID things with a camera. They were doing a lot about being able

675
00:38:32,045 --> 00:38:35,120
Speaker 4:  to just talk and, and have those natural conversations with Google, Google

676
00:38:35,120 --> 00:38:38,925
Speaker 4:  Assistant. Like all of that was happening in 2018. So I, I,

677
00:38:39,145 --> 00:38:42,965
Speaker 4:  that's why I'm, I'm fully convinced that at some point we're gonna find out

678
00:38:42,965 --> 00:38:44,565
Speaker 4:  that this was originally an Android product.

679
00:38:44,965 --> 00:38:48,045
Speaker 5:  I mean it's an Android product now. Like the thing, the thing runs Android.

680
00:38:49,365 --> 00:38:52,325
Speaker 5:  I I think you're exactly right. I think if you, if you rewind five years,

681
00:38:52,715 --> 00:38:56,685
Speaker 5:  they're looking at voice assistant and Google

682
00:38:56,835 --> 00:39:00,485
Speaker 5:  Lens and some of these other, like what can we do without

683
00:39:00,485 --> 00:39:03,765
Speaker 5:  putting a screen and a bunch of apps on your smartphone? Which

684
00:39:04,345 --> 00:39:08,245
Speaker 5:  in the abstract, like Heath has been saying is a really interesting and valuable

685
00:39:08,365 --> 00:39:11,085
Speaker 5:  question. Like, this is stuff we should be talking about. Because the idea

686
00:39:11,085 --> 00:39:14,885
Speaker 5:  that the like platonic ideal of how we interact with technology

687
00:39:14,905 --> 00:39:18,765
Speaker 5:  is like a grid of icons on a screen is just not true. And

688
00:39:19,625 --> 00:39:23,205
Speaker 5:  we, we should be exploring all these things. But I do think it's very clear

689
00:39:23,635 --> 00:39:27,525
Speaker 5:  that at some point along the way they went from, we're gonna

690
00:39:27,525 --> 00:39:30,845
Speaker 5:  have to cobble together a bunch of different things and probably invent a

691
00:39:30,845 --> 00:39:34,645
Speaker 5:  bunch of technology ourselves to just, this is now we are an interface

692
00:39:34,645 --> 00:39:38,485
Speaker 5:  company on top of chat GPT, which is functionally what this is, this

693
00:39:38,485 --> 00:39:42,205
Speaker 5:  is a, this is a, it's a revolutionary input device on par

694
00:39:42,235 --> 00:39:46,125
Speaker 5:  with the, with the keyboard and mouse for chat GPT.

695
00:39:46,125 --> 00:39:48,365
Speaker 5:  Like that's what they're trying to build. They, they make a big song and

696
00:39:48,365 --> 00:39:51,645
Speaker 5:  dance about all their stuff they can do. But this is functionally a way to

697
00:39:52,335 --> 00:39:56,125
Speaker 5:  input things to chat GPT. That is what this device exists to do. It's

698
00:39:56,125 --> 00:39:57,605
Speaker 4:  A real digital crown moment.

699
00:39:59,145 --> 00:40:02,805
Speaker 3:  You know any minute Apple's going to do a partnership with one of these

700
00:40:03,045 --> 00:40:05,005
Speaker 3:  companies 'cause they can't build it by themselves 'cause of all the privacy

701
00:40:05,005 --> 00:40:08,445
Speaker 3:  reasons. And they will say, if you want to use chat GBT,

702
00:40:09,665 --> 00:40:12,435
Speaker 3:  your Apple Watch can do it. And that will be the end of the AI Pin.

703
00:40:12,615 --> 00:40:15,875
Speaker 6:  Oh, I will take the opposite side of that bet. Yeah. I I'll bet money that

704
00:40:15,875 --> 00:40:19,755
Speaker 6:  they will not. Yeah, because because of, because of the privacy reasons

705
00:40:19,815 --> 00:40:22,395
Speaker 6:  you cited, they will not do this as an external partnership. No, but we're

706
00:40:22,595 --> 00:40:26,555
Speaker 3:  Watching this in the trial right now where Tim Cook and Sundar

707
00:40:26,635 --> 00:40:30,155
Speaker 3:  Petya are in meetings discussing their search deal and

708
00:40:30,155 --> 00:40:33,595
Speaker 3:  apple's like your services can begin where our

709
00:40:33,595 --> 00:40:37,355
Speaker 3:  limitations end. So you can see they could

710
00:40:37,375 --> 00:40:37,875
Speaker 3:  say, well,

711
00:40:37,875 --> 00:40:41,355
Speaker 6:  Well that there's an important fact there, which is that Google is paying

712
00:40:41,355 --> 00:40:45,155
Speaker 6:  Apple on the order of $20 billion a year. So yes, if someone

713
00:40:45,335 --> 00:40:48,915
Speaker 6:  is paying you You know 20% of your profit margin. Yeah,

714
00:40:49,345 --> 00:40:51,275
Speaker 6:  sure. You can have deep partnerships.

715
00:40:51,575 --> 00:40:54,635
Speaker 3:  You don't think OpenAI and Microsoft are gonna, they're gonna throw that

716
00:40:54,635 --> 00:40:58,315
Speaker 3:  money down. Like that's what I mean, the second Apple gets over it and there's

717
00:40:58,315 --> 00:41:02,255
Speaker 3:  a few ways for 'em to get over it. They can invent a privacy focused LLM

718
00:41:03,085 --> 00:41:06,815
Speaker 3:  very hard inside your values. You could take the money.

719
00:41:08,285 --> 00:41:11,655
Speaker 3:  It's true. Or you can do shortcuts with David's favorite

720
00:41:13,765 --> 00:41:14,575
Speaker 5:  Love shortcuts.

721
00:41:14,915 --> 00:41:17,655
Speaker 3:  But like, all I'm saying is there's a big action button on the side of my

722
00:41:17,655 --> 00:41:21,295
Speaker 3:  watch and there's a lot of different ways for that thing to trigger. Talk

723
00:41:21,295 --> 00:41:25,215
Speaker 3:  to ChatGPT GPT. Yeah. Right. There's, there's many methods by

724
00:41:25,215 --> 00:41:28,335
Speaker 3:  which that button can, can do the thing. And at that moment,

725
00:41:29,705 --> 00:41:33,605
Speaker 3:  the value of $700 plus another $24 of

726
00:41:33,605 --> 00:41:36,125
Speaker 3:  service fees a month, like begins to slide.

727
00:41:36,625 --> 00:41:40,285
Speaker 4:  If I hold my phone just right, I can do the shortcuts method

728
00:41:40,355 --> 00:41:42,525
Speaker 4:  with chat GPT and the Raybans.

729
00:41:43,275 --> 00:41:47,205
Speaker 6:  Yeah. Like it. And, and they're adding, so Cranz, they're adding early next

730
00:41:47,205 --> 00:41:50,805
Speaker 6:  year. Zuckerberg was talking to me about this when I interviewed him ahead

731
00:41:50,805 --> 00:41:54,045
Speaker 6:  of Connect. They're gonna do a software update for the Raybans where it's

732
00:41:54,045 --> 00:41:57,045
Speaker 6:  visual recognition. So in the interview he was like, you'll just look at

733
00:41:57,045 --> 00:42:00,485
Speaker 6:  the camera and say, what is that camera? Where can I buy it? And he didn't

734
00:42:00,485 --> 00:42:04,245
Speaker 6:  say buy it. He said, where can I buy it? And because so, so that's,

735
00:42:04,245 --> 00:42:08,165
Speaker 6:  that's a much more, I think, compelling because it's on you. Right. And

736
00:42:08,165 --> 00:42:09,685
Speaker 4:  Cheaper. And

737
00:42:09,685 --> 00:42:10,965
Speaker 6:  Cheaper. No subscription. And

738
00:42:10,965 --> 00:42:14,525
Speaker 4:  Like if you go, if you go to the beach, you don't have to worry about

739
00:42:14,735 --> 00:42:16,125
Speaker 4:  right. Where you're gonna Pin it.

740
00:42:16,125 --> 00:42:18,925
Speaker 6:  And it turns out And what kind of shots you're gonna get your head is on

741
00:42:18,925 --> 00:42:22,805
Speaker 6:  you all the time. Who knew? And you can have stuff on your head

742
00:42:23,025 --> 00:42:25,885
Speaker 6:  all the time no matter what you're wearing. It's actually a really cool human

743
00:42:25,885 --> 00:42:26,805
Speaker 6:  feature. Okay. This

744
00:42:26,805 --> 00:42:30,485
Speaker 3:  Brings me to the last thing I want to talk about in this sequence, which

745
00:42:30,485 --> 00:42:33,045
Speaker 3:  is a graph that's in my head at all times. Oh,

746
00:42:33,145 --> 00:42:34,045
Speaker 6:  Is there AY axis?

747
00:42:34,045 --> 00:42:36,485
Speaker 3:  There's AY and I'm, I'm gonna label the graph for you. So pull over in your

748
00:42:36,485 --> 00:42:39,925
Speaker 3:  car and I want you to imagine X in Y web axis. Okay. And I've worked this

749
00:42:39,925 --> 00:42:42,165
Speaker 3:  out 'cause I was on threads last night and people really helped me figure

750
00:42:42,165 --> 00:42:45,915
Speaker 3:  this out. This is the formula of

751
00:42:45,915 --> 00:42:47,355
Speaker 3:  wearable product success. Mm.

752
00:42:48,305 --> 00:42:50,035
Speaker 4:  Yeah. I, I, I saw this on threads.

753
00:42:50,265 --> 00:42:51,715
Speaker 3:  Okay, so it's X and y axis.

754
00:42:53,395 --> 00:42:57,335
Speaker 3:  It doesn't matter which side, which the Y axis, let's call

755
00:42:57,335 --> 00:43:00,135
Speaker 3:  it. Okay. Is amount. I have to care about this shit

756
00:43:00,635 --> 00:43:00,935
Speaker 4:  All, right.

757
00:43:01,785 --> 00:43:02,135
Speaker 3:  Times

758
00:43:03,845 --> 00:43:04,615
Speaker 3:  face multiple.

759
00:43:06,265 --> 00:43:08,135
Speaker 6:  Which is what's a what's a face?

760
00:43:08,475 --> 00:43:12,015
Speaker 3:  Is it on my face? Okay. Oh, okay. Okay. And is it a VR headset? So like,

761
00:43:12,015 --> 00:43:15,055
Speaker 3:  is it on my face and it's like a cool pair of sunglasses, low face, multiple

762
00:43:15,055 --> 00:43:18,975
Speaker 3:  mm. Is it on my face? And it's the medic quest pro high, enormous

763
00:43:19,085 --> 00:43:22,575
Speaker 3:  face. Multiple. Yeah. If it's not on your face, on your wrist, no face. Multiple.

764
00:43:23,765 --> 00:43:24,055
Speaker 4:  Okay,

765
00:43:24,315 --> 00:43:24,535
Speaker 3:  You

766
00:43:24,535 --> 00:43:26,055
Speaker 4:  Got me. Got it. I

767
00:43:26,055 --> 00:43:28,815
Speaker 3:  See it. Okay. So that's like, that's just one variable. Yeah, it's either

768
00:43:28,925 --> 00:43:31,975
Speaker 3:  it's, and you gotta decide for yourself in the glasses, how much do I have

769
00:43:31,975 --> 00:43:35,895
Speaker 3:  to care about it? Is, is this a tiny little computer? Do I

770
00:43:35,895 --> 00:43:39,735
Speaker 3:  got a software update? Plug it into battery charge. It does it Beep

771
00:43:41,535 --> 00:43:45,295
Speaker 3:  computer stuff. Yeah. And you gotta decide. Is it weird to get the photos

772
00:43:45,395 --> 00:43:48,575
Speaker 3:  off it? Yeah. All in the first factor. Yeah. So

773
00:43:49,245 --> 00:43:53,135
Speaker 3:  fids times, is it face? That's one axis.

774
00:43:53,245 --> 00:43:57,095
Speaker 3:  Okay. And then the X axis is how useful is it? Okay.

775
00:43:57,845 --> 00:44:01,815
Speaker 3:  Okay. So if X is greater than or equal to Y, so there's a

776
00:44:01,975 --> 00:44:03,455
Speaker 3:  line at 45 degrees.

777
00:44:03,555 --> 00:44:04,015
Speaker 6:  Oh God.

778
00:44:04,325 --> 00:44:08,055
Speaker 3:  Then it's, it's good. And if it's lower, you're

779
00:44:08,185 --> 00:44:12,015
Speaker 3:  duped. So like regular glasses, right? Alex, you're wearing

780
00:44:12,015 --> 00:44:12,855
Speaker 3:  some glasses. Yeah.

781
00:44:13,275 --> 00:44:13,495
Speaker 6:  You

782
00:44:13,495 --> 00:44:17,415
Speaker 3:  Gotta think about them. Never. So low number times on your

783
00:44:17,415 --> 00:44:21,255
Speaker 3:  face, low number. 'cause you have cool glasses value very high.

784
00:44:21,475 --> 00:44:23,135
Speaker 3:  So you're like historic success.

785
00:44:24,155 --> 00:44:27,735
Speaker 6:  Oh, right. Okay. See I didn't do well at math in school, but I, I think I'm

786
00:44:27,855 --> 00:44:28,335
Speaker 6:  tracking with you.

787
00:44:28,955 --> 00:44:32,725
Speaker 3:  You see what I'm saying? No. Neli also de is describing a graph that has

788
00:44:32,725 --> 00:44:35,725
Speaker 3:  like six or seven different s It's just one, it's just two numbers. So, I

789
00:44:35,725 --> 00:44:38,005
Speaker 4:  Wouldn't, there's there's algebra involved.

790
00:44:38,005 --> 00:44:41,365
Speaker 3:  Isly bullshit. Greater than or less than.

791
00:44:41,905 --> 00:44:42,125
Speaker 3:  How

792
00:44:42,125 --> 00:44:44,445
Speaker 6:  Useful is this? I'm gonna make a GPT for this. Where you can just,

793
00:44:44,905 --> 00:44:47,085
Speaker 3:  And I'm just saying the only real variable here is face.

794
00:44:47,635 --> 00:44:48,085
Speaker 4:  Yeah. Okay.

795
00:44:48,635 --> 00:44:51,925
Speaker 3:  It's, it's really just fake. Is it on your face? And then it's gotta be super

796
00:44:52,285 --> 00:44:56,005
Speaker 3:  valuable. Right? Right, right. Apple watch a lot of fiddly bits at the beginning.

797
00:44:56,065 --> 00:44:56,565
Speaker 3:  Low value.

798
00:44:57,545 --> 00:44:58,005
Speaker 4:  Not on your

799
00:44:58,005 --> 00:45:01,085
Speaker 3:  Face. Not on your face. But it still didn't do a lot at the beginning.

800
00:45:01,925 --> 00:45:04,605
Speaker 3:  You could like send flirty heartbeats to people who maybe didn't want them.

801
00:45:04,605 --> 00:45:08,085
Speaker 3:  Right. And they figured it out and they brought it up above the curve. And

802
00:45:08,085 --> 00:45:11,965
Speaker 3:  now it's actually kind of less fiddly too. Yeah. Success. Okay. Me, quest

803
00:45:12,065 --> 00:45:15,685
Speaker 3:  pro fid likeliest product in the world, gigantic

804
00:45:16,035 --> 00:45:18,725
Speaker 3:  face, multiple, no value. No

805
00:45:18,725 --> 00:45:18,965
Speaker 4:  Value,

806
00:45:19,425 --> 00:45:22,525
Speaker 3:  Not a success. They won't even talk about it anymore. Hmm. You see what I'm

807
00:45:22,525 --> 00:45:25,525
Speaker 3:  saying? So Humane ai Pin, very fiddly,

808
00:45:26,585 --> 00:45:30,425
Speaker 3:  no face. Multiple unclear value. Big question

809
00:45:30,425 --> 00:45:34,015
Speaker 3:  mark. Yeah. Vision pro. Enormous

810
00:45:34,165 --> 00:45:37,935
Speaker 3:  face multiple on the order of, they put another face on your face, right?

811
00:45:37,935 --> 00:45:41,535
Speaker 3:  Yeah. Huge problem there. Very

812
00:45:41,675 --> 00:45:45,455
Speaker 3:  fiddly, we think. Or perhaps not because it's kind of standalone,

813
00:45:45,685 --> 00:45:48,535
Speaker 3:  unclear. Fids, unclear value

814
00:45:49,175 --> 00:45:51,175
Speaker 4:  X, it's an X, the value is X. Who

815
00:45:51,175 --> 00:45:52,855
Speaker 3:  Knows? I'm just telling you, this works every time.

816
00:45:53,475 --> 00:45:53,695
Speaker 6:  Wow.

817
00:45:54,165 --> 00:45:56,975
Speaker 3:  Someone's gonna animate this graph and it's gonna be amazing. But I promise

818
00:45:57,035 --> 00:46:00,855
Speaker 3:  you, if you just work this out for any product, you get there. So the the

819
00:46:01,045 --> 00:46:05,015
Speaker 3:  meta glasses, people like 'em, right? Yeah. So you do

820
00:46:05,015 --> 00:46:08,575
Speaker 3:  it, they look cool. Low face, multiple, eh, fiddly.

821
00:46:08,765 --> 00:46:11,215
Speaker 4:  Liam complimented mine today before he saw the, the, the

822
00:46:11,215 --> 00:46:14,775
Speaker 3:  Lenses before he knew. Yeah. So medium face multiple, because people are

823
00:46:14,775 --> 00:46:18,615
Speaker 3:  like, oh, you've been, you've been watching me creep. That's a problem. But

824
00:46:18,615 --> 00:46:22,545
Speaker 3:  they look cool. Yeah. And then they're fun, right? Because they

825
00:46:22,705 --> 00:46:25,145
Speaker 3:  have good music, they've got great call mics. You should watch that video

826
00:46:25,145 --> 00:46:25,705
Speaker 3:  with Becca.

827
00:46:26,095 --> 00:46:26,945
Speaker 4:  They're useful.

828
00:46:26,945 --> 00:46:29,865
Speaker 3:  They're useful. Right on the line of success.

829
00:46:30,255 --> 00:46:34,065
Speaker 6:  Okay. I, my challenge to any Vergecast listener right now who's playing with

830
00:46:34,095 --> 00:46:37,985
Speaker 6:  Chad GT's, GPT builder, is to make a GPT based on what

831
00:46:38,135 --> 00:46:41,985
Speaker 6:  Neli just said, send it to us and it's gonna be

832
00:46:41,985 --> 00:46:42,265
Speaker 6:  amazing.

833
00:46:43,495 --> 00:46:47,425
Speaker 3:  Fids Times face, multiple has to be less than or equal

834
00:46:47,485 --> 00:46:48,825
Speaker 3:  to. How useful is it? What

835
00:46:48,825 --> 00:46:52,625
Speaker 6:  Is this bot called? Eli's. Eli's Gadget Or Eli's, yeah.

836
00:46:52,625 --> 00:46:53,145
Speaker 6:  Wearable

837
00:46:53,155 --> 00:46:53,985
Speaker 3:  Value. It needs a

838
00:46:53,985 --> 00:46:56,065
Speaker 6:  Good name. Yeah. Indicator. This is a horrible name.

839
00:46:56,205 --> 00:46:58,145
Speaker 3:  The theory of wearable bullshit.

840
00:46:59,055 --> 00:47:02,265
Speaker 6:  Okay, there we go. That's the episode title.

841
00:47:03,055 --> 00:47:05,905
Speaker 3:  Yeah. It needs a name. I, I am just telling you, I've been working this out

842
00:47:05,905 --> 00:47:08,545
Speaker 3:  on the show forever and last night it finally clicked when I looked at the

843
00:47:08,545 --> 00:47:12,465
Speaker 3:  Humane Pin where I was like, oh, it's not on your face. So that changes

844
00:47:12,485 --> 00:47:15,745
Speaker 3:  the whole equation. But then it is all the other things that it is,

845
00:47:17,865 --> 00:47:21,785
Speaker 3:  I was disgusted by my work on the iPhone, so instead I made a little laser

846
00:47:21,785 --> 00:47:24,445
Speaker 3:  projector that you have to look at your hand.

847
00:47:24,645 --> 00:47:27,965
Speaker 4:  There's no Instagram, can't do Instagram with lasers. Just

848
00:47:27,965 --> 00:47:30,660
Speaker 3:  Doesn't work. This thing is real silly. They should've let us, they should've

849
00:47:30,660 --> 00:47:34,405
Speaker 3:  let us and other reviewers actually into it and they should have been less

850
00:47:34,405 --> 00:47:37,365
Speaker 3:  self serious about it, is what I'll say. I'm excited to play with one. I

851
00:47:37,365 --> 00:47:39,005
Speaker 3:  think we're all excited about new gadgets all the time.

852
00:47:39,845 --> 00:47:41,445
Speaker 4:  I wanna see how the speakers work. I wanna

853
00:47:41,625 --> 00:47:42,445
Speaker 3:  The person speaker.

854
00:47:42,445 --> 00:47:45,125
Speaker 4:  Yeah. I wanna see, I want you and I both to have one and see if we can hear

855
00:47:45,125 --> 00:47:46,405
Speaker 4:  each other's. That's

856
00:47:46,405 --> 00:47:47,565
Speaker 3:  Gonna be the worst episode of the show.

857
00:47:47,665 --> 00:47:48,605
Speaker 4:  The worst episode. I'm sorry, just

858
00:47:48,695 --> 00:47:50,525
Speaker 3:  Sorry. People posting a title four feet apart.

859
00:47:51,965 --> 00:47:55,245
Speaker 1:  I mean, I will say again, to go back to the meta things on that one, like

860
00:47:55,245 --> 00:47:58,405
Speaker 1:  I've seen a bunch of people be like, oh, the person speaker, that's, this

861
00:47:58,405 --> 00:48:01,525
Speaker 1:  is the thing that the smart glasses have been doing for a while, right? This

862
00:48:01,525 --> 00:48:05,485
Speaker 1:  like personal directed audio that you can hear and other people

863
00:48:05,695 --> 00:48:09,525
Speaker 1:  don't is like relatively proven technology.

864
00:48:09,705 --> 00:48:10,645
Speaker 6:  The Ray bands do that.

865
00:48:11,035 --> 00:48:14,805
Speaker 4:  Yeah, I do this and I hear so well with the Ray bands.

866
00:48:14,805 --> 00:48:17,085
Speaker 4:  Yeah. It's the projector. I sit on the train like this. Yeah.

867
00:48:17,105 --> 00:48:20,165
Speaker 6:  The projector is what I want to try. I want to, I'm going to a concert Friday.

868
00:48:20,205 --> 00:48:23,205
Speaker 6:  I wish I had this so that during the concert I could just hold my hand out

869
00:48:23,585 --> 00:48:26,325
Speaker 6:  and it just like matrix displays. Are

870
00:48:26,325 --> 00:48:28,485
Speaker 3:  You gonna a concert that sucks. Is anyone else gonna be there?

871
00:48:28,625 --> 00:48:31,485
Speaker 6:  No, I'm just, I just wanna do it to get the reaction around me. You know,

872
00:48:31,725 --> 00:48:32,005
Speaker 6:  like,

873
00:48:32,005 --> 00:48:34,885
Speaker 3:  It's like you're, you're Alex is in the pitch. Just like

874
00:48:38,205 --> 00:48:39,365
Speaker 4:  I just assume it's very bright.

875
00:48:40,985 --> 00:48:42,885
Speaker 3:  Try not. I brought my own laser show.

876
00:48:43,285 --> 00:48:44,205
Speaker 6:  Yeah, there you go.

877
00:48:44,675 --> 00:48:47,485
Speaker 3:  That would be actually be sick. Okay, here's the killer app for the Humane

878
00:48:47,545 --> 00:48:49,485
Speaker 3:  ai Pin personal laser show.

879
00:48:49,925 --> 00:48:53,365
Speaker 6:  Oh yeah. Imagine Coldplay. And instead of the the bracelets they give you,

880
00:48:53,395 --> 00:48:55,445
Speaker 6:  it's just the Pins. They all, they

881
00:48:55,445 --> 00:48:57,165
Speaker 3:  All, yeah. This is a good time to know. Synced

882
00:48:57,225 --> 00:48:57,565
Speaker 4:  To your

883
00:48:57,565 --> 00:49:01,245
Speaker 3:  Title account Labs had it bored ape NFT party and they bought the

884
00:49:01,485 --> 00:49:05,365
Speaker 3:  wrong EV lights and they burned everyone's eyes. I love that. Maybe don't

885
00:49:05,365 --> 00:49:08,165
Speaker 3:  do the laser show thing. Just an idea. All, right? We gotta take a break.

886
00:49:08,385 --> 00:49:11,445
Speaker 3:  That's a true story by the way. It's heartbreaking. Check your EV lights.

887
00:49:12,185 --> 00:49:14,565
Speaker 3:  We gotta take a break. We'll be right back to talk about OpenAI Day.

888
00:51:19,425 --> 00:51:22,405
Speaker 3:  All right we're back. Less hardware this time, more software, but the future

889
00:51:22,465 --> 00:51:26,405
Speaker 3:  of software in a very exciting way. Alex, you went to OpenAI s DevDay

890
00:51:26,405 --> 00:51:29,045
Speaker 3:  this week. They announced a bunch of stuff. Take us through

891
00:51:29,045 --> 00:51:32,605
Speaker 6:  It. Yeah, so this was in downtown San Francisco Monday

892
00:51:33,065 --> 00:51:36,805
Speaker 6:  and this was Open AI's first big conference. And

893
00:51:37,195 --> 00:51:41,085
Speaker 6:  when I was going to it like realizing that ChatGPT PT is not even

894
00:51:41,285 --> 00:51:45,125
Speaker 6:  a year old at this point, like blew my mind. And then you get

895
00:51:45,125 --> 00:51:49,005
Speaker 6:  there and they have about a thousand developers there and it

896
00:51:49,005 --> 00:51:52,925
Speaker 6:  just had this energy of I, I'll get into the announcements, but I think it's

897
00:51:52,925 --> 00:51:56,125
Speaker 6:  important to like contextualize the energy. It felt like

898
00:51:56,705 --> 00:52:00,685
Speaker 6:  the early Apple events, it felt like this buzz that You know we had that

899
00:52:00,725 --> 00:52:04,445
Speaker 6:  a little bit. I think You know David and Eli, you were there too for the

900
00:52:04,465 --> 00:52:08,325
Speaker 6:  Vision Pro unveiling that had a an a special kind of feel

901
00:52:08,605 --> 00:52:12,485
Speaker 6:  'cause You know people had high hopes for Apple finally doing a big new thing.

902
00:52:13,065 --> 00:52:16,365
Speaker 6:  But like this just felt like something I haven't experienced since like early

903
00:52:16,505 --> 00:52:20,365
Speaker 6:  Mac Worlds and you get in

904
00:52:20,625 --> 00:52:24,485
Speaker 6:  and I mean they did a really nice keynote. It was about an hour and a

905
00:52:24,485 --> 00:52:28,325
Speaker 6:  half's worth of content packed into like a tight 45 minutes with

906
00:52:28,325 --> 00:52:32,165
Speaker 6:  like real live demos. Like there was a moment where they were showing off

907
00:52:32,165 --> 00:52:35,685
Speaker 6:  like a Zapier integration where you could like have a bot, you make You know

908
00:52:35,795 --> 00:52:39,645
Speaker 6:  text someone or send them a slack based on what you tell it. And like they're

909
00:52:39,645 --> 00:52:43,005
Speaker 6:  doing it on stage live. They're building the thing in like a minute and then

910
00:52:43,005 --> 00:52:45,525
Speaker 6:  like it sends the pinging to Sam and he's on the other side of the stage

911
00:52:45,525 --> 00:52:49,325
Speaker 6:  and he just holds his phone up and is like, yep, got it. Like, like I,

912
00:52:49,485 --> 00:52:53,045
Speaker 6:  I really don't like the direction the industry's going thanks to Apple where

913
00:52:53,045 --> 00:52:56,885
Speaker 6:  it's these highly produced pre-recorded keynotes that are

914
00:52:56,885 --> 00:52:59,885
Speaker 6:  really just like becoming marketing videos. I think we saw that with the

915
00:52:59,885 --> 00:53:01,125
Speaker 6:  last Mac event especially.

916
00:53:02,705 --> 00:53:06,045
Speaker 6:  So open a, it felt like this return to form that was like, wow, this feels

917
00:53:06,045 --> 00:53:09,805
Speaker 6:  good. In terms of what they announced, it was a lot.

918
00:53:09,925 --> 00:53:13,205
Speaker 6:  I mean I think like the most immediate thing that the developers were like

919
00:53:13,485 --> 00:53:16,285
Speaker 6:  freaking out about and like Sam had to tell everyone to like stop clapping

920
00:53:16,605 --> 00:53:20,365
Speaker 6:  'cause they kept clapping over and over was just like making GPT

921
00:53:20,365 --> 00:53:23,685
Speaker 6:  cheaper to use as a developer. So it's like three times cheaper.

922
00:53:23,955 --> 00:53:27,885
Speaker 6:  They've got GTP four Turbo, which apparently

923
00:53:27,885 --> 00:53:31,525
Speaker 6:  people really want larger context windows, which is the ability to upload

924
00:53:31,635 --> 00:53:35,445
Speaker 6:  more text tokens to these models and do more with them. So I think

925
00:53:35,445 --> 00:53:39,365
Speaker 6:  the limit was around like 20 ish K max before and now it's

926
00:53:39,365 --> 00:53:42,885
Speaker 6:  like 128 plus K context window.

927
00:53:43,265 --> 00:53:45,845
Speaker 6:  So you could put a book in this thing and have it do all kinds of things.

928
00:53:45,845 --> 00:53:49,725
Speaker 6:  People got really excited about that. And then a bunch of like kind of

929
00:53:49,725 --> 00:53:53,405
Speaker 6:  more developer focused stuff, right? It's a developer conference. The biggest

930
00:53:53,405 --> 00:53:57,205
Speaker 6:  thing though was this new AI platform. I mean really

931
00:53:57,205 --> 00:54:01,085
Speaker 6:  what OpenAI wants to become, and I wrote about this in command

932
00:54:01,085 --> 00:54:04,925
Speaker 6:  line this week, is the app store of ai. And so they've got this

933
00:54:05,265 --> 00:54:08,645
Speaker 6:  new GPT platform where with no coding required,

934
00:54:09,145 --> 00:54:12,965
Speaker 6:  you can build your own GPT, upload custom knowledge

935
00:54:13,625 --> 00:54:17,205
Speaker 6:  and do a bunch of what is essentially like abstracting away prompt engineering,

936
00:54:17,255 --> 00:54:20,965
Speaker 6:  right? And making it very like tailored to a specific use case

937
00:54:21,665 --> 00:54:25,045
Speaker 6:  and do all that in like honestly a matter of minutes. I got a live demo of

938
00:54:25,065 --> 00:54:28,205
Speaker 6:  one of these things being made and you can do it in under five minutes and

939
00:54:28,205 --> 00:54:32,045
Speaker 6:  then just publish it to the web eventually. And

940
00:54:32,045 --> 00:54:34,965
Speaker 6:  when I say eventually, I think just in like a few weeks, OpenAI is gonna

941
00:54:34,965 --> 00:54:38,565
Speaker 6:  have this store, the GPT store. And that's where things get really interesting

942
00:54:38,645 --> 00:54:42,485
Speaker 6:  I think because you're seeing OpenAI, not only is it becoming a a

943
00:54:42,515 --> 00:54:45,925
Speaker 6:  huge developer platform, an external platform

944
00:54:46,275 --> 00:54:49,685
Speaker 6:  that powers other things. They have over 2 million developers in under a

945
00:54:49,685 --> 00:54:52,925
Speaker 6:  year, which is just like wild. But they've got this massive

946
00:54:53,275 --> 00:54:57,125
Speaker 6:  growing consumer business. They have a hundred million weekly users of

947
00:54:57,235 --> 00:55:01,005
Speaker 6:  chat GPT, which is like a a for a product that is less than a year old

948
00:55:01,345 --> 00:55:04,445
Speaker 6:  is just absolutely insane. That means their monthlies are probably on the

949
00:55:04,445 --> 00:55:08,045
Speaker 6:  order of half a billion. And so what they're trying to do is like

950
00:55:08,995 --> 00:55:12,725
Speaker 6:  aggregate that audience and also become a consumer internet

951
00:55:13,045 --> 00:55:16,845
Speaker 6:  platform and, and have people making GPTs no

952
00:55:16,845 --> 00:55:20,525
Speaker 6:  coding required that they then surface to people through

953
00:55:20,755 --> 00:55:24,565
Speaker 6:  chat GPT and through the store. And so the store is coming very soon but

954
00:55:24,565 --> 00:55:27,605
Speaker 6:  they, they showed a preview of it and I think that's where things get really

955
00:55:27,845 --> 00:55:29,125
Speaker 6:  interesting. Is that a hundred million,

956
00:55:29,145 --> 00:55:30,965
Speaker 3:  Is that GPT plus the

957
00:55:31,115 --> 00:55:35,045
Speaker 6:  Paid thing or just ChatGPT that chat GPT users, they didn't give a plus number.

958
00:55:35,235 --> 00:55:39,125
Speaker 6:  Very important for the the custom GPT. You have to be a plus subscriber

959
00:55:39,265 --> 00:55:43,245
Speaker 6:  to access and build the custom GPT. So this is all feeding

960
00:55:43,245 --> 00:55:47,005
Speaker 6:  that loop of like OpenAI, I think I the information reported, they're already

961
00:55:47,025 --> 00:55:50,645
Speaker 6:  on like a 1 billion a year revenue run rate with the consumer

962
00:55:50,965 --> 00:55:54,845
Speaker 6:  subscription business. They really want to have more reasons to pay

963
00:55:54,845 --> 00:55:58,605
Speaker 6:  for chat GPT and, and an app store and custom GPT and

964
00:55:58,695 --> 00:56:02,365
Speaker 6:  again, abstracting way all that prompt engineering they think is gonna be

965
00:56:02,525 --> 00:56:06,365
Speaker 6:  a, a key unlock there. So like some of the examples were like, and it's multimodal

966
00:56:06,365 --> 00:56:09,765
Speaker 6:  so you can use Dolly. So it's like an interior design GPT that's like

967
00:56:10,085 --> 00:56:13,285
Speaker 6:  specifically for helping you figure out how to design your home or

968
00:56:13,965 --> 00:56:17,645
Speaker 6:  a cooking one or You know. Theoretically here at The Verge

969
00:56:17,905 --> 00:56:21,605
Speaker 6:  we could upload all Verge content ever made into

970
00:56:21,805 --> 00:56:25,405
Speaker 6:  a GPT and it could replace search on our website.

971
00:56:25,505 --> 00:56:29,005
Speaker 6:  That's an idea, right? And OpenAI, which we should get into this

972
00:56:29,995 --> 00:56:33,845
Speaker 6:  next OpenAI, will be compensating creators of these GPTs based on their

973
00:56:33,845 --> 00:56:37,525
Speaker 6:  usage. Which I think is a very interesting distinction from other

974
00:56:37,805 --> 00:56:38,045
Speaker 6:  platforms.

975
00:56:38,285 --> 00:56:42,125
Speaker 4:  I think two things that kind of, I, I read your piece and it was great. The

976
00:56:42,125 --> 00:56:45,325
Speaker 4:  two things that stuck out to me were one the compensation and how they're

977
00:56:45,405 --> 00:56:48,365
Speaker 4:  planning to do that. 'cause that seems like a whole mess. Yeah. And also

978
00:56:48,365 --> 00:56:51,965
Speaker 4:  just how quickly he seemed to gloss over

979
00:56:52,465 --> 00:56:53,925
Speaker 4:  the hallucination problem. Yeah,

980
00:56:53,925 --> 00:56:54,325
Speaker 3:  Just like any

981
00:56:54,525 --> 00:56:54,685
Speaker 6:  Enormous store,

982
00:56:54,685 --> 00:56:55,605
Speaker 4:  Like you're releasing an enormous app

983
00:56:55,605 --> 00:56:58,325
Speaker 3:  Store, you go into a Walmart or Target, you're like most products a lot like

984
00:56:58,325 --> 00:57:02,165
Speaker 4:  An enormous lying, enormous live store basically. Yeah. A normal store.

985
00:57:02,165 --> 00:57:02,445
Speaker 4:  Yeah.

986
00:57:02,465 --> 00:57:03,845
Speaker 3:  You go into a Walmart or a Target.

987
00:57:05,745 --> 00:57:09,715
Speaker 6:  Well yes, but the, the difference with the custom GPT is

988
00:57:09,715 --> 00:57:13,595
Speaker 6:  you can upload custom knowledge. So like on stage Sam made a bot and

989
00:57:13,595 --> 00:57:16,875
Speaker 6:  he used to be the head of Y Combinator, the startup ex incubator.

990
00:57:17,415 --> 00:57:21,235
Speaker 6:  And he was like, I have wanted to have a bot that just tells people

991
00:57:21,235 --> 00:57:24,995
Speaker 6:  what I would tell startup founders the advice I would give them for like

992
00:57:25,075 --> 00:57:28,915
Speaker 6:  a decade. And he just built it on stage in like three minutes and he

993
00:57:29,155 --> 00:57:33,075
Speaker 6:  uploaded transcripts of every talk he's given on these topics, right? So

994
00:57:33,575 --> 00:57:36,755
Speaker 6:  yes, the halluc it's still a problem. Like hallucination obviously is still

995
00:57:36,795 --> 00:57:39,475
Speaker 6:  a problem and he talked about this more after the keynote and a press thing

996
00:57:39,475 --> 00:57:43,275
Speaker 6:  I went to. But the fact that you can augment it with

997
00:57:43,375 --> 00:57:47,275
Speaker 6:  custom knowledge makes it more domain specific and

998
00:57:47,275 --> 00:57:49,595
Speaker 6:  so it's hopefully gonna alleviate a lot

999
00:57:49,595 --> 00:57:49,795
Speaker 3:  Of that.

1000
00:57:50,265 --> 00:57:53,515
Speaker 4:  Doesn't that kind of just make it like, I think the no code part is probably

1001
00:57:53,535 --> 00:57:56,395
Speaker 4:  the coolest part of this. Yeah, but doesn't this just make it like no code

1002
00:57:56,585 --> 00:58:00,555
Speaker 4:  chat chatbots like you had on AIM 20 years ago. Like if you

1003
00:58:00,555 --> 00:58:04,195
Speaker 4:  can upload like for him the the same Altman one in particular.

1004
00:58:04,705 --> 00:58:07,715
Speaker 4:  Okay, that's really, really cool. Somebody could have done that 20 years

1005
00:58:07,735 --> 00:58:10,635
Speaker 4:  ago, it just would've taken them quite a bit of time and now it Right.

1006
00:58:10,635 --> 00:58:13,795
Speaker 3:  But you have the sort of like Eliza problem, right? Right. Where it's like

1007
00:58:13,985 --> 00:58:17,955
Speaker 3:  there's some canned responses, this one is like gonna talk to you, it's

1008
00:58:17,955 --> 00:58:21,275
Speaker 3:  gonna be, I think the fidelity of the conversation is higher. The thing that's

1009
00:58:21,295 --> 00:58:24,515
Speaker 3:  really interesting to me is what you're saying Alex the compensation model.

1010
00:58:24,515 --> 00:58:27,035
Speaker 3:  Yeah. Yeah. So OpenAI is what, 20 bucks a month for plus

1011
00:58:27,035 --> 00:58:27,675
Speaker 6:  Yeah. Ish. Yeah.

1012
00:58:28,295 --> 00:58:31,035
Speaker 3:  And then they're, they're probably gonna what, just share out some of that.

1013
00:58:31,665 --> 00:58:35,555
Speaker 3:  Yeah. To the bot builders. That's the Spotify model,

1014
00:58:35,925 --> 00:58:39,435
Speaker 3:  which is historically right. Doesn't make anyone rich except Spotify.

1015
00:58:39,745 --> 00:58:43,635
Speaker 6:  Well Spotify important distinction. There's this whole thing called the

1016
00:58:43,635 --> 00:58:46,795
Speaker 6:  labels, right. That OpenAI doesn't have to deal with because they don't pay

1017
00:58:46,975 --> 00:58:50,475
Speaker 6:  for a lot of their content, which is something that Sam did not really wanna

1018
00:58:50,475 --> 00:58:54,315
Speaker 6:  talk about. Yeah. But yes, I actually think it's closer

1019
00:58:54,335 --> 00:58:58,075
Speaker 6:  to YouTube because the way they're gonna do it in initially is they're gonna

1020
00:58:58,085 --> 00:59:01,995
Speaker 6:  share some percentage, we don't know, we don't know if it's fixed, we

1021
00:59:01,995 --> 00:59:05,395
Speaker 6:  don't know if it fluctuates based on the category, but they're gonna share

1022
00:59:05,555 --> 00:59:08,715
Speaker 6:  a percentage of the companies. So OpenAI

1023
00:59:09,425 --> 00:59:13,355
Speaker 6:  Chad GPT subscription revenue with GPT creators and it's based on

1024
00:59:13,405 --> 00:59:17,195
Speaker 6:  usage. So this, and and Altman was very kind of high level about this on

1025
00:59:17,195 --> 00:59:20,515
Speaker 6:  stage. I think they're gonna share more in the next few weeks. But at this

1026
00:59:20,515 --> 00:59:24,405
Speaker 6:  press q and a I went to with after and I wrote about this in command line,

1027
00:59:24,505 --> 00:59:28,405
Speaker 6:  he elaborated and said there's going to be category specific bonuses

1028
00:59:29,905 --> 00:59:33,605
Speaker 6:  and that down the road they're open to actually letting you subscribe directly

1029
00:59:33,605 --> 00:59:37,325
Speaker 6:  to custom GPTs, letting GPT creators charge one-off fees

1030
00:59:37,465 --> 00:59:37,805
Speaker 6:  for them.

1031
00:59:39,465 --> 00:59:42,365
Speaker 6:  But it sounds like a lot of it is gonna get figured out in like the first

1032
00:59:42,365 --> 00:59:45,805
Speaker 6:  few months of the store they're rolling out with this initial kind of

1033
00:59:46,215 --> 00:59:50,045
Speaker 6:  mushy rev share thing. But it is YouTube esque. It's not Apple-esque because

1034
00:59:50,215 --> 00:59:54,085
Speaker 6:  there isn't going to be like a transactional piece to this. Right. Have

1035
00:59:54,085 --> 00:59:57,565
Speaker 4:  They asked chat GPT itself how it thinks it should handle

1036
00:59:57,945 --> 00:59:58,285
Speaker 4:  the Rev

1037
00:59:58,285 --> 00:59:58,525
Speaker 3:  Models?

1038
00:59:59,745 --> 01:00:01,045
Speaker 6:  I'm sure they have. I mean well

1039
01:00:01,105 --> 01:00:05,005
Speaker 3:  So this analogies are difficult 'cause it's weird and it's mushy.

1040
01:00:05,005 --> 01:00:05,245
Speaker 3:  Yeah.

1041
01:00:05,675 --> 01:00:06,165
Speaker 6:  YouTube

1042
01:00:06,995 --> 01:00:09,805
Speaker 3:  Like most YouTubers are getting a cut of the ads, right? Right. There's a

1043
01:00:10,355 --> 01:00:14,245
Speaker 3:  sort of like infinite amount of money coming into YouTube or at least an

1044
01:00:14,245 --> 01:00:18,205
Speaker 3:  uncapped amount of advertising money coming into YouTube and that goes

1045
01:00:18,225 --> 01:00:22,165
Speaker 3:  out, right? Spotify is like you pay Spotify premium and I

1046
01:00:22,165 --> 01:00:26,045
Speaker 3:  know my ever increasing amount of money I pay for Spotify premium, I listen

1047
01:00:26,045 --> 01:00:29,925
Speaker 3:  to a song, there's some formula that goes to the label in the artist and

1048
01:00:29,925 --> 01:00:33,725
Speaker 3:  it's pennies and this feels more like that, right? I pay ChatGPT 20 bucks

1049
01:00:33,765 --> 01:00:37,565
Speaker 3:  a month, I go use a bunch of GPTs, my usage gets

1050
01:00:37,565 --> 01:00:41,325
Speaker 3:  divvied up into fractions and goes to those creators and

1051
01:00:41,325 --> 01:00:45,125
Speaker 3:  that they got it. The reason I'm, I'm harping on this is

1052
01:00:45,125 --> 01:00:48,725
Speaker 3:  because of what David said earlier, which is the entire game is the app store

1053
01:00:48,725 --> 01:00:51,965
Speaker 3:  for ai and you cannot distribute this anywhere

1054
01:00:52,515 --> 01:00:56,125
Speaker 3:  because the second you start doing a better revenue model, which is

1055
01:00:56,555 --> 01:01:00,445
Speaker 3:  just pay for the apps or pay for a subscription or have

1056
01:01:00,445 --> 01:01:04,125
Speaker 3:  transactions, Apple's gonna sit there and take 30% away from you.

1057
01:01:04,195 --> 01:01:07,125
Speaker 3:  Well, yeah. And Google is gonna take 30% away from you Apple

1058
01:01:07,125 --> 01:01:11,005
Speaker 6:  Already. So the Apple already is taking 30% of chat GPT plus revenue

1059
01:01:11,265 --> 01:01:14,125
Speaker 6:  on iOS. Right. And I, because it's a subscription

1060
01:01:14,125 --> 01:01:14,925
Speaker 3:  Down. Well if you sign up on the

1061
01:01:14,925 --> 01:01:17,885
Speaker 6:  Phone, right. If you sign up on the phone. So yeah. And this is where I kind

1062
01:01:17,885 --> 01:01:20,765
Speaker 6:  of like ended with Command Line this week is like I would love to know what

1063
01:01:20,765 --> 01:01:24,005
Speaker 6:  Apple thinks about all this because Apple, they don't like stores within

1064
01:01:24,075 --> 01:01:27,405
Speaker 6:  apps. You know, like you can ask Mark Zuckerberg about that one. You can

1065
01:01:27,405 --> 01:01:30,125
Speaker 6:  ask a lot of people about that one. Actually a lot of people have tried versions

1066
01:01:30,125 --> 01:01:33,285
Speaker 6:  of this. The only one that's gotten it through for reasons that are too complex

1067
01:01:33,285 --> 01:01:37,085
Speaker 6:  to get into is WeChat in China. But I think they're gonna run into

1068
01:01:37,165 --> 01:01:41,005
Speaker 6:  a massive wall there eventually, which is Apple's take rate

1069
01:01:41,105 --> 01:01:44,965
Speaker 6:  and their historical hatred of an app

1070
01:01:44,965 --> 01:01:47,645
Speaker 6:  trying to recreate any kind of a digital store experience

1071
01:01:49,355 --> 01:01:53,295
Speaker 3:  And potentially having their own Yeah. ai. Right?

1072
01:01:53,745 --> 01:01:56,975
Speaker 3:  Right. And wanting their own AI to be their primary AI in the device.

1073
01:01:57,485 --> 01:02:00,855
Speaker 3:  Because once the AI is looking at books and just buying 'em for you,

1074
01:02:01,435 --> 01:02:04,535
Speaker 3:  that's a whole nother world of transactions that Apple can take money out

1075
01:02:04,535 --> 01:02:07,975
Speaker 3:  of and they're gonna wanna be the primary AI there. So it's

1076
01:02:08,565 --> 01:02:12,335
Speaker 3:  just to bring this together, you can see why people wanna build a not

1077
01:02:12,335 --> 01:02:16,175
Speaker 3:  phone. Hmm. Why Sam Altman is investing in 45 different

1078
01:02:16,295 --> 01:02:19,885
Speaker 3:  attempts at this and might be working with Johnny. Ive at some attempt at

1079
01:02:19,885 --> 01:02:23,245
Speaker 3:  this, there's a massive blocker to

1080
01:02:23,665 --> 01:02:26,725
Speaker 3:  mobile innovation here, which is that 30% cut.

1081
01:02:27,545 --> 01:02:31,045
Speaker 3:  And I just don't have any sense of how anyone will actually get around it

1082
01:02:31,475 --> 01:02:35,085
Speaker 3:  because building the Humane AI Pin or the rewind

1083
01:02:35,435 --> 01:02:39,365
Speaker 3:  pent or whatever Johnny Ive comes up with on the side,

1084
01:02:39,945 --> 01:02:42,285
Speaker 3:  the phone is still gonna be the primary device. Yeah,

1085
01:02:42,355 --> 01:02:46,285
Speaker 5:  Well the, the other part of it is like, this is not all that different

1086
01:02:46,355 --> 01:02:49,765
Speaker 5:  from what Amazon and Google were trying to do with skills for their voice

1087
01:02:49,765 --> 01:02:53,525
Speaker 5:  assistant. Like we've been down this road where Amazon

1088
01:02:53,625 --> 01:02:56,685
Speaker 5:  was like, okay, you're gonna interact with Alexa, but we're also gonna offer

1089
01:02:56,685 --> 01:03:00,525
Speaker 5:  you lots of ways to interact with other data and other services skills is

1090
01:03:00,525 --> 01:03:04,285
Speaker 5:  gonna be a huge business that got past Apple.

1091
01:03:04,465 --> 01:03:08,205
Speaker 5:  It was fine, it just turns out nobody wanted that because it has this he

1092
01:03:08,205 --> 01:03:12,125
Speaker 5:  discovery problem. The interface is actually not that good for

1093
01:03:12,125 --> 01:03:16,005
Speaker 5:  it in most ways because like the thing where you have to essentially say

1094
01:03:16,045 --> 01:03:18,845
Speaker 5:  a wake word to get to the thing and then another wake word to get to the

1095
01:03:18,845 --> 01:03:21,805
Speaker 5:  app that you want and then talk to it in a very specific way all to route

1096
01:03:21,805 --> 01:03:25,565
Speaker 5:  right back to you. Like that flow sucks and So I think what

1097
01:03:26,315 --> 01:03:30,205
Speaker 5:  chat GPT is gonna have to figure out is how to, like, again,

1098
01:03:30,205 --> 01:03:34,165
Speaker 5:  we're talking about abstracting all of this away, and by virtue of making

1099
01:03:34,265 --> 01:03:38,205
Speaker 5:  you choose things, you lose that abstraction in a really helpful way.

1100
01:03:38,205 --> 01:03:41,325
Speaker 5:  Like ChatGPT already has these plugins. Yeah. So the plugin system sucks.

1101
01:03:41,325 --> 01:03:44,925
Speaker 5:  Those are replacing, it's a, it's a, yeah. Perfectly useful thing, right?

1102
01:03:45,065 --> 01:03:48,725
Speaker 5:  But I think that is one way to go down that road

1103
01:03:49,785 --> 01:03:53,475
Speaker 5:  that doesn't work because it it, you can't find the stuff you have to like

1104
01:03:53,475 --> 01:03:56,355
Speaker 5:  turn it on every time you want to use it. It just doesn't work. And I think

1105
01:03:57,395 --> 01:04:01,075
Speaker 5:  figuring out how to do this thing they're trying to do in a way that doesn't

1106
01:04:01,075 --> 01:04:04,515
Speaker 5:  cause those exact same problems is gonna be really hard. Especially

1107
01:04:04,825 --> 01:04:08,595
Speaker 5:  because OpenAI is trying to have its cake and eat it too. Because

1108
01:04:08,695 --> 01:04:11,155
Speaker 5:  it also wants to be the interface to all of it. It does

1109
01:04:11,155 --> 01:04:11,435
Speaker 6:  Like,

1110
01:04:11,505 --> 01:04:14,875
Speaker 5:  Yeah, it wants to be the underlying infrastructure and the most

1111
01:04:15,395 --> 01:04:18,835
Speaker 5:  powerful, important product. And right at some point doing both of those

1112
01:04:18,835 --> 01:04:20,395
Speaker 5:  becomes really, really, really hard to do.

1113
01:04:20,505 --> 01:04:24,435
Speaker 6:  This is what Altman said in the q and a I was at after his keynote.

1114
01:04:25,015 --> 01:04:27,315
Speaker 6:  And I thought it was really interesting because it gets at that tension,

1115
01:04:27,315 --> 01:04:30,875
Speaker 6:  David, that you talk about where OpenAI is building a

1116
01:04:31,115 --> 01:04:34,955
Speaker 6:  platform very clearly. They're continuing to add things into chat

1117
01:04:34,975 --> 01:04:38,875
Speaker 6:  GPT, add different modalities, different data sets. At the same

1118
01:04:38,875 --> 01:04:42,075
Speaker 6:  time they're saying, come build on our platform, we'll share revenue with

1119
01:04:42,075 --> 01:04:45,915
Speaker 6:  you, come make your own GPT, et cetera. And

1120
01:04:46,775 --> 01:04:50,475
Speaker 6:  he got asked about this and he said very clearly, he was like, don't build

1121
01:04:50,595 --> 01:04:54,555
Speaker 6:  a thin wrapper on top of OpenAI. We are planning to build the obvious features

1122
01:04:54,555 --> 01:04:57,955
Speaker 6:  that you would expect for a robust platform over time. And there's enormous

1123
01:04:58,045 --> 01:05:01,955
Speaker 6:  value to build. So that's him just being straight up like we will

1124
01:05:02,155 --> 01:05:05,995
Speaker 6:  probably Sherlock you. Yeah. And, and I'm, I'm gonna see,

1125
01:05:06,035 --> 01:05:08,595
Speaker 6:  I see a lot of parallels here to the early app store.

1126
01:05:08,595 --> 01:05:09,355
Speaker 5:  Yeah. We're gonna build the app

1127
01:05:09,355 --> 01:05:12,315
Speaker 6:  And I think that's okay. Like You know the early app store had a bunch of

1128
01:05:12,315 --> 01:05:15,195
Speaker 6:  flashlight apps, right? Eventually that just became a feature on the phone

1129
01:05:15,615 --> 01:05:19,205
Speaker 6:  and that's, it was a, it's a better feature as a result. You

1130
01:05:19,205 --> 01:05:22,565
Speaker 3:  Can go in the app store right now and download a home interior

1131
01:05:22,985 --> 01:05:25,125
Speaker 3:  app that is a thin wrapper on Dolly.

1132
01:05:25,125 --> 01:05:25,565
Speaker 6:  Like Yeah.

1133
01:05:26,235 --> 01:05:29,645
Speaker 3:  Like we are, we're painting our house and I wanted an app that would just

1134
01:05:29,645 --> 01:05:33,445
Speaker 3:  like put paint colors on the wall and like half of them are now just

1135
01:05:33,575 --> 01:05:36,765
Speaker 3:  dolly wrappers. Right. And they're bananas. Like,

1136
01:05:37,635 --> 01:05:41,285
Speaker 3:  it's like, don't do this. Like I don't recommend them. Like your house starts

1137
01:05:41,285 --> 01:05:45,205
Speaker 3:  to look real wild after a minute, but it's, but there's obviously a

1138
01:05:45,205 --> 01:05:48,965
Speaker 3:  market for that thing to occur right now. And maybe one day

1139
01:05:48,965 --> 01:05:52,805
Speaker 3:  OpenAI will build that exact thing 'cause it's a useful case for Dolly.

1140
01:05:53,385 --> 01:05:56,245
Speaker 3:  But in the meantime, like they're just watching a bunch of these companies

1141
01:05:56,465 --> 01:06:00,365
Speaker 3:  try and fail and that's useful to them. And in the

1142
01:06:00,365 --> 01:06:02,485
Speaker 3:  short term useful, the companies are making some money. I

1143
01:06:02,485 --> 01:06:06,125
Speaker 4:  Just keep thinking about all the scummy apps

1144
01:06:06,315 --> 01:06:09,445
Speaker 4:  that are gonna come to this thing, right? Like there's the obvious ones where

1145
01:06:09,565 --> 01:06:12,605
Speaker 4:  I, as an adult call it scummy. A lot of our younger listeners would probably

1146
01:06:12,685 --> 01:06:16,245
Speaker 4:  love it, is like one that'll just write your papers for you. Right? That's,

1147
01:06:16,245 --> 01:06:19,685
Speaker 4:  that's the thing chat GPD can already do. Somebody will probably make a really,

1148
01:06:19,685 --> 01:06:23,645
Speaker 4:  really good version of it using this. And a lot of people will

1149
01:06:23,765 --> 01:06:26,565
Speaker 4:  probably be very upset about that and demand for it to be pulled down because

1150
01:06:26,565 --> 01:06:29,805
Speaker 4:  it's helping a bunch of kids cheat. There's ones where like, okay, you can

1151
01:06:29,805 --> 01:06:33,645
Speaker 4:  have an You know input, some actress that

1152
01:06:33,645 --> 01:06:36,525
Speaker 4:  you really wanna talk to. And then everybody's like, oh yeah, I can go talk

1153
01:06:36,525 --> 01:06:39,485
Speaker 4:  to that hot actress. And it's like, ooh, no, you can already

1154
01:06:39,505 --> 01:06:42,605
Speaker 6:  Do that Alex. I mean there's like, yeah, you can do that replica character

1155
01:06:42,605 --> 01:06:43,205
Speaker 6:  ai but that's

1156
01:06:43,205 --> 01:06:47,125
Speaker 4:  Gonna happen now on Yeah. OpenAI Yes form, right? Like

1157
01:06:47,125 --> 01:06:48,525
Speaker 4:  they're, they're gonna have to deal with that. Oh,

1158
01:06:48,525 --> 01:06:51,765
Speaker 3:  Someone's gonna build that GPT, right? Sure. If you build a GPT of me.

1159
01:06:52,465 --> 01:06:53,125
Speaker 3:  So it's over.

1160
01:06:53,385 --> 01:06:54,445
Speaker 6:  So that is like the law

1161
01:06:54,445 --> 01:06:55,085
Speaker 3:  Will, the law will

1162
01:06:55,085 --> 01:06:58,165
Speaker 6:  Come down. So Neli like we are a group, we, there's a lot of stuff on the

1163
01:06:58,325 --> 01:07:01,725
Speaker 6:  internet that could like be used to inform a chatbot about us because we

1164
01:07:01,725 --> 01:07:04,525
Speaker 6:  write on the internet. Like that is a perfect example. Someone could just

1165
01:07:04,525 --> 01:07:07,485
Speaker 6:  make a Neli bot and it's everything Eli's written on The Verge. And you ask

1166
01:07:07,795 --> 01:07:10,965
Speaker 6:  Neli questions about copyright You know forever, right? Yeah.

1167
01:07:11,185 --> 01:07:13,845
Speaker 3:  And then Neli bot is like, this is copyright infringement.

1168
01:07:14,065 --> 01:07:16,685
Speaker 6:  But you, just to be clear, well we about what's happening, I want to, we,

1169
01:07:16,685 --> 01:07:20,205
Speaker 6:  this is a little bit of a tangent, but like they, another thing OpenAI announced

1170
01:07:20,205 --> 01:07:23,685
Speaker 6:  this week is a copyright shield, which is a program where they will pay for

1171
01:07:23,745 --> 01:07:27,405
Speaker 6:  any legal fees incurred by their developers for copyright claims

1172
01:07:27,545 --> 01:07:31,405
Speaker 6:  of use of the technology. Oh. So theoretically someone builds a Eli bot,

1173
01:07:31,795 --> 01:07:35,245
Speaker 6:  they scrape The Verge for all Eli's stories, they put it in the bot,

1174
01:07:35,945 --> 01:07:39,405
Speaker 6:  Vox Media goes, Hey you stole our IP and you're making money off of this

1175
01:07:39,565 --> 01:07:43,525
Speaker 6:  'cause OpenAI is paying you, we're gonna sue you. And OpenAI is saying, now

1176
01:07:43,625 --> 01:07:47,605
Speaker 6:  we will fight all those legal Yeah. Legal things that

1177
01:07:47,605 --> 01:07:49,925
Speaker 6:  happen. So like what do you think about that Neli?

1178
01:07:49,985 --> 01:07:53,605
Speaker 5:  That's a bad answer to this problem. We should all just like, I just wanna

1179
01:07:53,605 --> 01:07:55,485
Speaker 5:  acknowledge that is a bad answer too. Oh yeah.

1180
01:07:55,485 --> 01:07:58,405
Speaker 4:  That's how OpenAI goes bankrupt real fast. Well

1181
01:07:58,405 --> 01:08:01,525
Speaker 5:  That's, well no there are also, there are so many interesting

1182
01:08:02,635 --> 01:08:06,605
Speaker 5:  ways of thinking about this content. Like Alex your paper example is a really

1183
01:08:06,805 --> 01:08:08,885
Speaker 5:  interesting one because like you put together all the announcements that

1184
01:08:08,885 --> 01:08:12,045
Speaker 5:  came out this week and what's gonna happen now is you're gonna be able to

1185
01:08:12,045 --> 01:08:15,885
Speaker 5:  upload your history textbook Yeah. To a GPT and you're gonna be able to write

1186
01:08:15,885 --> 01:08:19,685
Speaker 5:  papers from it. That is that it, it now has the a big enough context window

1187
01:08:20,185 --> 01:08:23,965
Speaker 5:  and it like the, they're using this thing. It's, it's like the

1188
01:08:24,185 --> 01:08:28,005
Speaker 5:  AI term for it is rag, which is basically like by doing this

1189
01:08:28,005 --> 01:08:31,165
Speaker 5:  custom knowledge, you're able to reduce it all the way down just to the stuff

1190
01:08:31,165 --> 01:08:34,645
Speaker 5:  that You know that it knows, which means it hallucinates less, which is actually

1191
01:08:34,645 --> 01:08:38,245
Speaker 5:  a really important part of this. It's gonna write you a good paper based

1192
01:08:38,245 --> 01:08:41,765
Speaker 5:  on your history textbook and that is just, it's all sitting there and that's

1193
01:08:42,125 --> 01:08:45,365
Speaker 5:  a mess in so many ways. And for OpenAI to just be like, don't worry about

1194
01:08:45,365 --> 01:08:47,885
Speaker 5:  it. We have lawyers is not a good answer to the problem.

1195
01:08:48,305 --> 01:08:51,805
Speaker 6:  So Sam was Sam's exact wording on this

1196
01:08:52,175 --> 01:08:55,285
Speaker 6:  issue was we don't like to do things that are illegal.

1197
01:08:56,045 --> 01:08:57,045
Speaker 6:  Congrats Sam. Which is like Sam

1198
01:08:57,115 --> 01:08:57,685
Speaker 3:  Cool, cool.

1199
01:08:57,865 --> 01:09:01,845
Speaker 6:  Sam, same. Sam, same. But like, I mean they clearly

1200
01:09:01,845 --> 01:09:04,765
Speaker 6:  think all this is under fair use and there's a lot You know you all have

1201
01:09:04,765 --> 01:09:07,085
Speaker 6:  talked about on the show, a lot of You know there's a lot of lawsuits right

1202
01:09:07,085 --> 01:09:09,685
Speaker 6:  now that it's trying, that are trying to You know they are gonna figure this

1203
01:09:09,685 --> 01:09:13,605
Speaker 6:  out. Whether this is fair use, this is happening while all this

1204
01:09:13,605 --> 01:09:14,765
Speaker 6:  is going on. It's a very, can

1205
01:09:14,765 --> 01:09:18,525
Speaker 3:  I color that in for you? Yeah. So the copyright office took comments about

1206
01:09:18,525 --> 01:09:21,805
Speaker 3:  whether it's fair use and all the big companies filed comments. Oh yeah,

1207
01:09:21,865 --> 01:09:24,685
Speaker 3:  that's good. And we have some of 'em on site and the one that just stuck

1208
01:09:24,685 --> 01:09:28,565
Speaker 3:  out was Andreesen Horowitz, big Silicon Valley firm. Their comment was, look,

1209
01:09:28,765 --> 01:09:31,885
Speaker 3:  companies have invested billions and billions of dollars into this technology

1210
01:09:32,125 --> 01:09:33,645
Speaker 3:  assuming it's fair use. So it's fair use.

1211
01:09:33,945 --> 01:09:37,445
Speaker 6:  No, I mean they were clear. They were like, if this is not fair use like

1212
01:09:37,455 --> 01:09:41,365
Speaker 6:  these investments go to zero. Basically it was like, it was like the whole,

1213
01:09:41,785 --> 01:09:45,725
Speaker 6:  the whole structure of all this is predicated on the idea that we can

1214
01:09:45,725 --> 01:09:47,165
Speaker 6:  have this for free. Yeah,

1215
01:09:48,285 --> 01:09:52,005
Speaker 3:  I know a lot of media executives, they don't

1216
01:09:52,055 --> 01:09:53,045
Speaker 3:  agree with all

1217
01:09:53,045 --> 01:09:53,245
Speaker 5:  You.

1218
01:09:56,635 --> 01:10:00,565
Speaker 3:  Yeah, not, not a little bit. We had Barack Obama on decoder this week. I

1219
01:10:00,565 --> 01:10:03,685
Speaker 3:  asked him this question like, you're an author, do you think this is fair

1220
01:10:03,685 --> 01:10:07,645
Speaker 3:  use? 'cause he really wants talk about ai and he, he was like, he's a

1221
01:10:07,645 --> 01:10:11,045
Speaker 3:  politician, he's very good at this. He's like, leave me out of it. I've sold

1222
01:10:11,045 --> 01:10:14,645
Speaker 3:  enough books. Like you watch him do it. He is like, no, no, no, no, no. I

1223
01:10:14,645 --> 01:10:18,565
Speaker 3:  refuse to engage this question. And then he said, this is a speed

1224
01:10:18,565 --> 01:10:22,485
Speaker 3:  bump. The revenue will get figured out. And So I think there's a

1225
01:10:22,485 --> 01:10:26,205
Speaker 3:  real tension in the industry. There's an expectation from the people who

1226
01:10:26,205 --> 01:10:29,965
Speaker 3:  make the IP that this is Yep. A speed bump that

1227
01:10:30,025 --> 01:10:33,405
Speaker 3:  the money will sort itself out. The creators will get paid. And there's the

1228
01:10:33,405 --> 01:10:37,395
Speaker 3:  expectation from the tech industry that they're gonna get to

1229
01:10:37,395 --> 01:10:41,115
Speaker 3:  do a Google and just take it. And like Google's

1230
01:10:41,115 --> 01:10:45,035
Speaker 3:  history is built on taking it. And they were the

1231
01:10:45,065 --> 01:10:48,915
Speaker 3:  scrappy upstart when they did that. When Google scraped all the books

1232
01:10:48,915 --> 01:10:52,235
Speaker 3:  on the internet and made Google books. They got sued but they showed up with

1233
01:10:52,235 --> 01:10:55,115
Speaker 3:  their like big colorful logo and their propeller hats.

1234
01:10:55,785 --> 01:10:59,715
Speaker 3:  Literally that image of Google protected it from these lawsuits when

1235
01:11:00,015 --> 01:11:03,715
Speaker 3:  Viacom sued Google over YouTube for just ingesting huge

1236
01:11:03,715 --> 01:11:06,875
Speaker 3:  amounts of Viacom television shows onto YouTube.

1237
01:11:07,225 --> 01:11:11,075
Speaker 3:  YouTube is like, but it's so cool. Like everyone loves it.

1238
01:11:11,215 --> 01:11:14,030
Speaker 3:  You could just watch South Park for free. And Viacom was like, yeah, that's

1239
01:11:14,030 --> 01:11:17,805
Speaker 3:  the problem that they won and they won running away because

1240
01:11:17,945 --> 01:11:21,925
Speaker 3:  Viacom was perceived as evil and Google again, stupid logo

1241
01:11:21,925 --> 01:11:24,485
Speaker 3:  slides in the office, beanie hat. And it was really useful.

1242
01:11:24,545 --> 01:11:27,915
Speaker 5:  That's part of the story. The rest of the story is that

1243
01:11:29,335 --> 01:11:33,195
Speaker 5:  Google had a reasonable defense, which was to say we're not the

1244
01:11:33,195 --> 01:11:36,515
Speaker 5:  ones uploading it. And it turns out there's like a large part of tech policy

1245
01:11:36,545 --> 01:11:38,675
Speaker 5:  that is based on that specific idea.

1246
01:11:39,175 --> 01:11:42,915
Speaker 3:  No, they, they wait, I mean their other defense was, and we built you content

1247
01:11:43,175 --> 01:11:46,965
Speaker 3:  id, we built you a private copyright enforcement

1248
01:11:46,965 --> 01:11:50,725
Speaker 3:  system that is unaccountable to everyone and pisses everybody off every day.

1249
01:11:51,215 --> 01:11:54,925
Speaker 3:  Right. That's the actual, that's the thing that they did to get outta that

1250
01:11:54,945 --> 01:11:55,565
Speaker 3:  jam. Google

1251
01:11:55,825 --> 01:11:59,365
Speaker 5:  Got so much out of this in a lot of ways by returning value to people like

1252
01:11:59,665 --> 01:12:03,365
Speaker 5:  Google made the case to Viacom that more people are watching Viacom stuff

1253
01:12:03,365 --> 01:12:05,525
Speaker 5:  because they're finding it on YouTube. And whether that's true or not, it's

1254
01:12:05,525 --> 01:12:09,285
Speaker 5:  a case to make and it's, it's done the same thing with

1255
01:12:09,285 --> 01:12:12,605
Speaker 5:  websites, right? Like there have been all these issues for years about like

1256
01:12:12,695 --> 01:12:15,485
Speaker 5:  media organizations being like, how are we supposed to feel about the fact

1257
01:12:15,485 --> 01:12:18,605
Speaker 5:  that there is a constantly updated version of our entire website sitting

1258
01:12:18,625 --> 01:12:22,485
Speaker 5:  on Google servers and that's what they're crawling and serving. Not our

1259
01:12:22,485 --> 01:12:26,405
Speaker 5:  website And. what Google says is we send you traffic. Yeah. And there's like,

1260
01:12:26,405 --> 01:12:30,045
Speaker 5:  there is some set of trades that you're willing to make there. And

1261
01:12:30,555 --> 01:12:34,325
Speaker 5:  chat GPT so far has not returned any value to the people

1262
01:12:35,015 --> 01:12:38,325
Speaker 5:  whose stuff that it is taking. It's not directing traffic, they're not getting

1263
01:12:38,325 --> 01:12:41,405
Speaker 5:  money. It's just like a nifty thing that other people get to do. And I think

1264
01:12:41,955 --> 01:12:45,725
Speaker 5:  without, if it can't figure out how to make that turn back to like,

1265
01:12:45,805 --> 01:12:49,645
Speaker 5:  here is what it's worth for you, I think it's much bigger than a speed bump.

1266
01:12:49,675 --> 01:12:53,285
Speaker 3:  Yeah. No, it will bring the whole thing down. Like it is the end of the industry

1267
01:12:53,285 --> 01:12:57,045
Speaker 3:  if this isn't fair use. And the the thing for all of these companies

1268
01:12:57,785 --> 01:13:01,685
Speaker 3:  is you cannot rely on the court system to

1269
01:13:02,195 --> 01:13:06,055
Speaker 3:  be reliable when it comes to fair use. Right? Like it

1270
01:13:06,055 --> 01:13:09,975
Speaker 3:  is not a stable decision making framework. It is a coin flip.

1271
01:13:10,645 --> 01:13:14,575
Speaker 3:  Like every time it is a coin flip even in just like recent memory. Like

1272
01:13:15,055 --> 01:13:17,415
Speaker 3:  I bring up this example all the time, but I'll do it again 'cause it's useful.

1273
01:13:18,115 --> 01:13:21,455
Speaker 3:  The estate of Marvin Gaye was not happy.

1274
01:13:22,315 --> 01:13:23,175
Speaker 3:  No, it just comes up

1275
01:13:23,505 --> 01:13:24,895
Speaker 5:  Drink somebody owes me $10.

1276
01:13:25,075 --> 01:13:27,855
Speaker 3:  I'm just telling you blur. The blurred lines case is gonna go down as like

1277
01:13:27,875 --> 01:13:31,215
Speaker 3:  the thing that upends maybe all of technology.

1278
01:13:32,275 --> 01:13:36,215
Speaker 3:  It did bring us Emily Ruski, many things happened because of the song.

1279
01:13:36,645 --> 01:13:40,615
Speaker 3:  They, Robin Thick Andre sued Marvin Gaye's estate preemptively

1280
01:13:40,735 --> 01:13:44,715
Speaker 3:  'cause they thought it was so clear cut, they hadn't stolen a note, it was

1281
01:13:44,715 --> 01:13:48,705
Speaker 3:  just a vibe and they lost. That is a totally unpredictable result.

1282
01:13:48,925 --> 01:13:52,465
Speaker 3:  And then the estate of Marvin Gaye embolden goes and Sues Ed Sheeran.

1283
01:13:53,225 --> 01:13:56,465
Speaker 3:  Ed Sheeran shows up in court all floppy hair and says,

1284
01:13:56,935 --> 01:14:00,905
Speaker 3:  well just the music. And he wins. And he actually took the course.

1285
01:14:00,925 --> 01:14:03,505
Speaker 5:  He played guitar in court. That was the different incredible edge shear impression

1286
01:14:03,505 --> 01:14:03,865
Speaker 5:  by the way.

1287
01:14:05,205 --> 01:14:08,745
Speaker 3:  You could use some accent work, but yeah, it was, it was All, right? He's

1288
01:14:08,745 --> 01:14:12,485
Speaker 3:  just a floppy guy. He's just a floppy guy. But that is a straight coin

1289
01:14:12,485 --> 01:14:16,285
Speaker 3:  flip. If I gave you the facts of that outside of the actual characters and

1290
01:14:16,285 --> 01:14:20,205
Speaker 3:  said these two didn't take any of the chords or the music. They

1291
01:14:20,205 --> 01:14:23,485
Speaker 3:  took a vibe and this guy took the actual chords

1292
01:14:24,185 --> 01:14:27,085
Speaker 3:  who, who should win? Who should lose, right? And it's the exact opposite

1293
01:14:27,085 --> 01:14:30,525
Speaker 3:  of what you expect. And if you are OpenAI or your Google or any, any one

1294
01:14:30,525 --> 01:14:34,445
Speaker 3:  of these other folks, you're staring at a problem where

1295
01:14:34,505 --> 01:14:38,005
Speaker 3:  it is so unpredictable that you should go and make some deals

1296
01:14:38,515 --> 01:14:42,445
Speaker 3:  just to bring some semblance of order to this marketplace.

1297
01:14:43,345 --> 01:14:47,325
Speaker 3:  But to what David's point is, no one knows what that deal's worth. The

1298
01:14:47,325 --> 01:14:50,685
Speaker 3:  second you make one deal, everyone else is gonna want a deal. The second

1299
01:14:50,825 --> 01:14:54,325
Speaker 3:  you make a deal, you have communicated to the world

1300
01:14:54,825 --> 01:14:58,445
Speaker 3:  you think deals might be necessary and everyone shows up with their hands

1301
01:14:58,505 --> 01:15:02,445
Speaker 3:  out. Like this is classic like rock in a hard place situation

1302
01:15:02,445 --> 01:15:06,165
Speaker 3:  for all these companies. And if they end up going to court and

1303
01:15:06,365 --> 01:15:09,285
Speaker 3:  fighting this out, no one knows if they're gonna get to the right answer

1304
01:15:09,465 --> 01:15:13,365
Speaker 3:  on either side. A couple weeks ago on decoder, we had a

1305
01:15:13,365 --> 01:15:16,805
Speaker 3:  record label executive and she said, look, the music industry is like this.

1306
01:15:17,305 --> 01:15:20,005
Speaker 3:  You sue each other in the morning and you go to dinner at night and that's

1307
01:15:20,005 --> 01:15:21,765
Speaker 3:  how It works. And like you're just gonna have to deal with it. And they're

1308
01:15:21,765 --> 01:15:24,285
Speaker 3:  gonna have to get good at that too, because that's how we, that's how we

1309
01:15:24,285 --> 01:15:27,645
Speaker 3:  do this. And I, I don't think Sam is ready for that. They

1310
01:15:27,645 --> 01:15:31,325
Speaker 6:  Are not thinking about it at all. It didn't come up on like the stage. It

1311
01:15:31,325 --> 01:15:34,525
Speaker 6:  didn't really come up afterwards in the conversations I was hearing. They're

1312
01:15:34,525 --> 01:15:38,365
Speaker 6:  very focused on what is the value we can give to developers and users

1313
01:15:38,945 --> 01:15:42,885
Speaker 6:  and the data that is powering all of this is like something they do not want

1314
01:15:42,885 --> 01:15:46,845
Speaker 6:  to talk about because of what we've all been saying. But I mean, I I

1315
01:15:46,845 --> 01:15:50,365
Speaker 6:  do wanna just like if to take devil's advocate here, like yes, like there's

1316
01:15:50,365 --> 01:15:54,205
Speaker 6:  a huge copyright question here. Generally I feel like with

1317
01:15:54,205 --> 01:15:57,645
Speaker 6:  these new things, the Viacom YouTube example is a great example where

1318
01:15:58,425 --> 01:16:02,325
Speaker 6:  better product experiences tend to win in the long run,

1319
01:16:02,415 --> 01:16:06,205
Speaker 6:  right? So and if I'm thinking about a GPT for The Verge,

1320
01:16:06,505 --> 01:16:10,165
Speaker 6:  unless I'm in kind of like discovery mode where I just wanna literally

1321
01:16:10,425 --> 01:16:13,565
Speaker 6:  You know, surf our beautiful site and look at things and I have no idea what

1322
01:16:13,565 --> 01:16:13,885
Speaker 6:  I'm wanting,

1323
01:16:15,485 --> 01:16:19,325
Speaker 6:  I actually would love the idea of just going, what should I know

1324
01:16:19,325 --> 01:16:22,965
Speaker 6:  about the new iPhone? Is it good or not? And a bot just, it

1325
01:16:22,965 --> 01:16:26,325
Speaker 6:  surfaces our article. It tells me, it says this is, this is the score,

1326
01:16:26,675 --> 01:16:29,405
Speaker 6:  this is the history of our coverage. Here's a link to the video. Or just

1327
01:16:29,405 --> 01:16:33,045
Speaker 6:  watch the video there. Like that's a pretty good experience. There's, there's

1328
01:16:33,045 --> 01:16:36,885
Speaker 6:  a lot of like finding things on the internet that like that

1329
01:16:36,995 --> 01:16:40,965
Speaker 6:  time collapses immediately when you're using an interface like this. And

1330
01:16:40,965 --> 01:16:44,565
Speaker 6:  that's gonna, I think provide value. Yeah. And so if you believe that that's

1331
01:16:44,805 --> 01:16:47,165
Speaker 6:  valuable and I think a lot of people will, and I think we're gonna start

1332
01:16:47,165 --> 01:16:50,645
Speaker 6:  seeing a lot of really interesting GPTs that are doing very dubious

1333
01:16:51,025 --> 01:16:54,205
Speaker 6:  You know fair use things, but are showing like, hey, this interface is actually

1334
01:16:54,205 --> 01:16:57,965
Speaker 6:  better for just finding information. I do think that

1335
01:16:57,965 --> 01:17:01,405
Speaker 6:  idea may win out in the long run regardless of what happens in the short

1336
01:17:01,405 --> 01:17:02,285
Speaker 6:  term with the legal system.

1337
01:17:02,785 --> 01:17:06,285
Speaker 4:  It just feels like they're moving way too fast. They are like, yeah, between

1338
01:17:06,285 --> 01:17:09,210
Speaker 4:  the lying thing and the fact that they haven't figured out whether or or

1339
01:17:09,210 --> 01:17:12,485
Speaker 4:  not they're wholesale ripping off a whole bunch of creators.

1340
01:17:13,075 --> 01:17:14,925
Speaker 4:  Like what are you doing?

1341
01:17:15,115 --> 01:17:17,125
Speaker 6:  What do you mean ripping off all these, what do you mean

1342
01:17:18,155 --> 01:17:20,485
Speaker 4:  What we just talked about? Like, like the payout structure. A lot of people

1343
01:17:20,515 --> 01:17:24,045
Speaker 4:  Yeah, a lot of people straight up accuse them of if they're, if they're ingesting

1344
01:17:24,045 --> 01:17:27,045
Speaker 4:  their stuff without paying them, they're, they're, that's ripping them off.

1345
01:17:27,265 --> 01:17:27,485
Speaker 4:  Oh,

1346
01:17:27,565 --> 01:17:28,125
Speaker 6:  Here, I'll just give

1347
01:17:28,125 --> 01:17:32,085
Speaker 3:  You, lemme give you just another example of this outside of ChatGPT. I think

1348
01:17:32,085 --> 01:17:34,885
Speaker 3:  about this with TikTok all the time. I keep a list of TikTok that I think

1349
01:17:34,885 --> 01:17:38,565
Speaker 3:  should be media PhDs. It's great. One day I'll publish the whole list

1350
01:17:39,425 --> 01:17:43,245
Speaker 3:  and there's one I keep coming back to Haggerty Media, which is a great

1351
01:17:43,465 --> 01:17:47,125
Speaker 3:  car YouTube channel. I love it. They did a stop motion video

1352
01:17:47,305 --> 01:17:51,165
Speaker 3:  of an engine being disassembled cool. This, this took forever.

1353
01:17:51,235 --> 01:17:54,645
Speaker 3:  Like just dis making a video of an engine being disassembled such that anyone

1354
01:17:54,645 --> 01:17:58,605
Speaker 3:  can understand what's happening takes a long time with people to do it

1355
01:17:58,605 --> 01:18:02,525
Speaker 3:  in stop motion is like a ma just obviously massive investment

1356
01:18:04,125 --> 01:18:08,105
Speaker 3:  and So I, it instantly got cut up into 10,000 different

1357
01:18:08,105 --> 01:18:11,385
Speaker 3:  TikTok channels that reposted into 10,000 different clips. 'cause it's, it

1358
01:18:11,385 --> 01:18:14,145
Speaker 3:  looks cool as hell. And if you're running one of these TikTok channels, they're

1359
01:18:14,145 --> 01:18:16,705
Speaker 3:  just trying to get views so you can like send people their TikTok shop. This

1360
01:18:16,705 --> 01:18:19,785
Speaker 3:  is great. Should that, should any of that exist media PhD?

1361
01:18:20,575 --> 01:18:23,505
Speaker 3:  Okay. Here's the real thing that I think about all the time. The comments

1362
01:18:24,125 --> 01:18:27,785
Speaker 3:  on all of the tos that have free booted this video are,

1363
01:18:28,015 --> 01:18:31,385
Speaker 3:  this took so long. This must have been so hard to do.

1364
01:18:31,935 --> 01:18:35,145
Speaker 3:  This is so cool. I'm so glad you spent all this time doing this. And it's

1365
01:18:35,145 --> 01:18:38,645
Speaker 3:  like, not like everyone can see the value

1366
01:18:39,185 --> 01:18:43,125
Speaker 3:  and the difficulty of the work and no one

1367
01:18:43,785 --> 01:18:47,485
Speaker 3:  not, not a cent of that value is going back to the person that made the video.

1368
01:18:47,575 --> 01:18:51,445
Speaker 3:  Right. And no, like, no one's confused that this was like a

1369
01:18:51,445 --> 01:18:52,205
Speaker 3:  hard thing to do.

1370
01:18:54,045 --> 01:18:57,705
Speaker 3:  TikTok is making the most money in that. The free booting channel is making

1371
01:18:57,705 --> 01:19:01,545
Speaker 3:  some money in that. The users are getting a ton of value, right? 'cause they're

1372
01:19:01,545 --> 01:19:04,265
Speaker 3:  seeing a video they wouldn't have ever seen. And the poor person who like

1373
01:19:04,685 --> 01:19:08,505
Speaker 3:  had to move the bolt like a 16th of an inch and then take another

1374
01:19:08,515 --> 01:19:10,705
Speaker 3:  frame gets nothing. Yeah.

1375
01:19:10,975 --> 01:19:11,265
Speaker 6:  Yeah.

1376
01:19:11,565 --> 01:19:15,545
Speaker 3:  And like I keep thinking like, I came up as a guy who when I was my short

1377
01:19:15,545 --> 01:19:18,865
Speaker 3:  stint as a lawyer, I defended the kids who used Kaza. And I'm sitting here

1378
01:19:18,865 --> 01:19:21,985
Speaker 3:  being like, am I the bad guy now? Like

1379
01:19:22,815 --> 01:19:26,665
Speaker 3:  what is going on here? Like that's like one step too far.

1380
01:19:26,725 --> 01:19:29,145
Speaker 3:  And that's just with video and everybody can understand. Everyone can see

1381
01:19:29,145 --> 01:19:33,065
Speaker 3:  it. If you make a GPT and you ingest all

1382
01:19:33,065 --> 01:19:36,465
Speaker 3:  of some YouTube creator's videos and you're like, now you can talk to Doug

1383
01:19:36,525 --> 01:19:40,465
Speaker 3:  DeMiro and he doesn't get a cent, like at some

1384
01:19:40,465 --> 01:19:42,225
Speaker 3:  point you have to say this is not like fair.

1385
01:19:42,445 --> 01:19:43,625
Speaker 6:  Oh, it's not even like not

1386
01:19:43,625 --> 01:19:45,145
Speaker 3:  Fair use or whatever. Just like

1387
01:19:45,655 --> 01:19:45,945
Speaker 6:  It's

1388
01:19:45,945 --> 01:19:46,945
Speaker 3:  Not, this is just like outrageous.

1389
01:19:47,135 --> 01:19:49,625
Speaker 6:  It's not even that the creator doesn't get a cent, it's that in that case,

1390
01:19:49,625 --> 01:19:52,945
Speaker 6:  YouTube doesn't get a cent. And guess who has a lot of lawyers who is comfortable

1391
01:19:52,955 --> 01:19:56,825
Speaker 6:  suing is Google, right? So like yeah, I think Open Eyes entering a world

1392
01:19:57,025 --> 01:20:00,945
Speaker 6:  of hurt here. Like I saw, again, I got a demo of the builder and you would

1393
01:20:00,945 --> 01:20:03,505
Speaker 6:  think that the ability to upload custom Knowledge, they would have some pretty

1394
01:20:03,505 --> 01:20:07,405
Speaker 6:  stringent guidelines on what you can upload and maybe

1395
01:20:07,405 --> 01:20:11,125
Speaker 6:  they scan it before they let you put it in the GPT to make sure it's You

1396
01:20:11,125 --> 01:20:14,965
Speaker 6:  know. Not something that is like very blatantly like paid copyrighted

1397
01:20:15,205 --> 01:20:18,645
Speaker 6:  material that is like behind a paywall or something like that. Yeah. It's

1398
01:20:18,645 --> 01:20:21,245
Speaker 6:  literally just a upload field. It's just an upload field. You can upload

1399
01:20:21,485 --> 01:20:25,285
Speaker 6:  anything. Like there, there's no nothing said about like how they're gonna

1400
01:20:25,485 --> 01:20:29,125
Speaker 6:  look for this stuff. So I, I don't think they're ready

1401
01:20:29,265 --> 01:20:32,645
Speaker 6:  for what's about to happen. Which is that like someone is going to be,

1402
01:20:33,165 --> 01:20:37,005
Speaker 6:  I I it's just obvious like you, like you take a paid product online,

1403
01:20:37,585 --> 01:20:41,525
Speaker 6:  you pay for it, you scrape it, you put it into, into a GPT and

1404
01:20:41,525 --> 01:20:44,765
Speaker 6:  you make money from it. And it's like in that

1405
01:20:45,395 --> 01:20:48,645
Speaker 6:  flow of events, like the value is just being stolen.

1406
01:20:49,255 --> 01:20:53,005
Speaker 6:  Right. And like, and OpenAI is saying at the same time, guess what? We're

1407
01:20:53,005 --> 01:20:56,605
Speaker 6:  gonna pay all your legal defenses. Yeah. It's just wild.

1408
01:20:57,465 --> 01:20:59,165
Speaker 3:  We should just get OpenAI sued. That's

1409
01:20:59,225 --> 01:21:01,525
Speaker 6:  The new challenge. They're already, they're being sued. They're being, I

1410
01:21:01,525 --> 01:21:02,525
Speaker 6:  mean they're, they

1411
01:21:02,875 --> 01:21:06,765
Speaker 3:  Look, I'm throwing out my wearable graph. The next episode is us just

1412
01:21:06,785 --> 01:21:10,765
Speaker 3:  us doing dubious things to get OpenAI suit. No, but wait,

1413
01:21:10,765 --> 01:21:12,845
Speaker 3:  they are getting sued by artists like Sarah Silver and all these people.

1414
01:21:12,845 --> 01:21:13,125
Speaker 6:  Yeah.

1415
01:21:13,265 --> 01:21:17,205
Speaker 3:  But this next turn where they're offering revenue to people to build

1416
01:21:17,205 --> 01:21:20,645
Speaker 3:  things. Yeah. And then protecting them from the claims. I'm gonna

1417
01:21:20,645 --> 01:21:23,965
Speaker 4:  Make a bot that adjusts all of the Game of Thrones books

1418
01:21:24,465 --> 01:21:25,685
Speaker 4:  and writes the final two.

1419
01:21:26,205 --> 01:21:28,605
Speaker 3:  I think George R. Martin at this point would just pay you for that.

1420
01:21:28,715 --> 01:21:31,165
Speaker 4:  Yeah. Yeah. Like that. That's that's, that's my loophole here.

1421
01:21:31,165 --> 01:21:33,445
Speaker 3:  He's gonna he should just do that. That's my loophole. George, if you're

1422
01:21:33,445 --> 01:21:34,365
Speaker 3:  listening and I know you are. I

1423
01:21:34,365 --> 01:21:34,845
Speaker 4:  Got you George.

1424
01:21:35,305 --> 01:21:37,925
Speaker 3:  You can, you can do it. It's, they've, they've expanded the field.

1425
01:21:38,685 --> 01:21:39,365
Speaker 6:  I I the

1426
01:21:39,605 --> 01:21:41,205
Speaker 3:  Context wing window's bigger. George,

1427
01:21:42,925 --> 01:21:46,485
Speaker 6:  I, I I do think this is like a huge problem, but I do wanna acknowledge like

1428
01:21:46,485 --> 01:21:49,325
Speaker 6:  these GPT, they're they're cool. They're gonna be cool. They're cool as hell.

1429
01:21:49,325 --> 01:21:52,325
Speaker 6:  Yeah. People are gonna like using them. The the dolly stuff is wild. Like

1430
01:21:52,325 --> 01:21:55,685
Speaker 6:  just having like something that can like generate Yeah. And like I have this

1431
01:21:55,685 --> 01:21:58,805
Speaker 6:  problem, I don't know about you guys with these, these chatbots, these like

1432
01:21:58,805 --> 01:22:02,565
Speaker 6:  all in one chatbots is like, it's just a, it's just a text bar. I don't know

1433
01:22:02,565 --> 01:22:06,285
Speaker 6:  what to do with it. I don't know all the things it can do. There's this context

1434
01:22:06,315 --> 01:22:10,205
Speaker 6:  problem with these bots that like barred and all these where they can do

1435
01:22:10,225 --> 01:22:14,125
Speaker 6:  so much that like, it's overwhelming frankly for me. So having like a cooking

1436
01:22:14,265 --> 01:22:18,005
Speaker 6:  bot that has like all the cooking information I would want to like make recipes.

1437
01:22:18,725 --> 01:22:22,165
Speaker 6:  I would pay separately for that because like, I actually just want that.

1438
01:22:22,325 --> 01:22:26,245
Speaker 6:  I don't want the all of it. Right. And So I, I do think it's a really powerful

1439
01:22:26,275 --> 01:22:30,005
Speaker 6:  idea and Meta's doing it, they're gonna have a bot creation platform next

1440
01:22:30,005 --> 01:22:33,885
Speaker 6:  year. There's already character AI replica the industry seems this

1441
01:22:33,885 --> 01:22:37,805
Speaker 6:  year to have realized that we are moving on from these like one bot

1442
01:22:37,805 --> 01:22:41,765
Speaker 6:  to rule them all concept to like millions of bots everywhere. Right?

1443
01:22:41,875 --> 01:22:45,725
Speaker 6:  Yeah. And that's a, that's a cool idea. It's just, yeah, there's all

1444
01:22:45,725 --> 01:22:48,245
Speaker 6:  this like copyright problem. But I think as an idea it's, it's,

1445
01:22:48,395 --> 01:22:51,885
Speaker 3:  It's, there's just existential risk to the center. The actually the model

1446
01:22:51,885 --> 01:22:52,685
Speaker 3:  thing is really interesting.

1447
01:22:54,595 --> 01:22:57,285
Speaker 3:  They had touch Nadella on Sage, Microsoft, they're obviously a huge open

1448
01:22:57,285 --> 01:23:01,165
Speaker 3:  air investor. Yeah. And part of the argument that they're making is

1449
01:23:01,435 --> 01:23:04,685
Speaker 3:  they're all just using one big model, but they're learning to segment it

1450
01:23:04,715 --> 01:23:08,605
Speaker 3:  into these use cases. Right? So the big model is getting better overall,

1451
01:23:08,695 --> 01:23:11,205
Speaker 3:  right. While people are doing these specialized things with it. And then

1452
01:23:11,225 --> 01:23:15,165
Speaker 3:  on the side, Microsoft research is trying to figure out

1453
01:23:15,165 --> 01:23:17,925
Speaker 3:  if you can take small models that could run locally or on these other places

1454
01:23:18,305 --> 01:23:21,965
Speaker 3:  and make them fully useful in like very narrow contexts.

1455
01:23:22,585 --> 01:23:26,525
Speaker 3:  So you see this like kind of parallel innovation pathway where like

1456
01:23:26,525 --> 01:23:29,965
Speaker 3:  the big model, maybe you can put guardrails around it and make these GPT

1457
01:23:30,505 --> 01:23:33,605
Speaker 3:  or you could have like the little model that's just good at what it does.

1458
01:23:34,185 --> 01:23:36,485
Speaker 3:  And there's something, and the funniest thing about the, I think the little

1459
01:23:36,485 --> 01:23:39,365
Speaker 3:  model training at Microsoft is doing is one of the prompts they give it is

1460
01:23:39,365 --> 01:23:40,605
Speaker 3:  pretend you're the big model,

1461
01:23:41,815 --> 01:23:42,165
Speaker 6:  Which

1462
01:23:42,325 --> 01:23:46,085
Speaker 3:  Is incredible. Yeah. Right. It's like, don't it, it's not

1463
01:23:46,085 --> 01:23:49,965
Speaker 3:  impossible. Like it's like that thing You know it's

1464
01:23:49,965 --> 01:23:53,445
Speaker 3:  pretty good. It's funny that we've had this whole conversation about voice

1465
01:23:53,445 --> 01:23:56,525
Speaker 3:  assistance and, and AI and all this stuff. And it's like we've brought up

1466
01:23:56,785 --> 01:24:00,505
Speaker 3:  Amazon and Google many times, which have voice

1467
01:24:00,505 --> 01:24:03,985
Speaker 3:  assistance. Millions of them deployed in homes today

1468
01:24:04,925 --> 01:24:07,185
Speaker 3:  and seem not ready for this next turn at all.

1469
01:24:07,645 --> 01:24:11,265
Speaker 6:  OpenAI is moving so fast. And that was just the takeaway from being there

1470
01:24:11,525 --> 01:24:14,825
Speaker 6:  and like talking to developers there and just, I mean, when you think about

1471
01:24:14,825 --> 01:24:18,345
Speaker 6:  the fact that ChatGPT PT is almost a year old and everything they've done

1472
01:24:18,365 --> 01:24:21,985
Speaker 6:  in a year and everything they just announced on stage and like the

1473
01:24:21,985 --> 01:24:25,945
Speaker 6:  demand for it. Like there was a massive chat GPT outage this week right after

1474
01:24:25,945 --> 01:24:29,665
Speaker 6:  they announced all this stuff. Yeah. Like their servers are melting. So they

1475
01:24:29,665 --> 01:24:32,865
Speaker 6:  have clearly like captured lightning in a bottle. It's their game to lose.

1476
01:24:33,025 --> 01:24:36,945
Speaker 6:  I really feel that it's just a matter of the legal ramifications

1477
01:24:36,945 --> 01:24:40,625
Speaker 6:  potentially catching up with them and the DIC of this store concept.

1478
01:24:41,005 --> 01:24:44,825
Speaker 6:  And like, if you're doing a usage based system, what's to keep

1479
01:24:44,825 --> 01:24:48,625
Speaker 6:  me from buying a bunch of bots to use my bot so that I

1480
01:24:48,695 --> 01:24:52,465
Speaker 6:  have the most used bot? Like how do they know that it's actually real?

1481
01:24:52,625 --> 01:24:55,625
Speaker 6:  I guess because it's through the subscription business. But So I guess that

1482
01:24:55,625 --> 01:24:59,305
Speaker 6:  is a good limiting factor on that. But there's just a lot of

1483
01:24:59,535 --> 01:25:03,265
Speaker 6:  like dicey like who decide, like I'm already imagining like

1484
01:25:03,455 --> 01:25:07,265
Speaker 6:  chat GBT is used to like influence the next election and they

1485
01:25:07,265 --> 01:25:10,625
Speaker 6:  decide to demonetize news, right? It's like everything old is new again.

1486
01:25:10,945 --> 01:25:11,865
Speaker 6:  Right? Like yeah,

1487
01:25:12,415 --> 01:25:16,025
Speaker 3:  Well that's like what is, what's gonna be the coco melon of chat GBT

1488
01:25:16,725 --> 01:25:19,705
Speaker 3:  that's on the good side, right? That's like the biggest YouTube success story

1489
01:25:19,705 --> 01:25:23,585
Speaker 3:  ever. Yeah. Enough so that Disney has listed it

1490
01:25:23,645 --> 01:25:25,745
Speaker 3:  not owning Coco Melon as like a risk. Yeah.

1491
01:25:26,015 --> 01:25:26,505
Speaker 6:  Because

1492
01:25:26,505 --> 01:25:29,585
Speaker 3:  It it, and that's just a YouTube success story. And it's like on the other

1493
01:25:29,585 --> 01:25:33,465
Speaker 3:  side is like, what is the horrible white noise playlist on Spotify

1494
01:25:33,855 --> 01:25:37,785
Speaker 3:  Yeah. App, right? That is just like zero value delivered and just

1495
01:25:37,785 --> 01:25:41,705
Speaker 3:  sucking money out of the ecosystem until Spotify has

1496
01:25:41,705 --> 01:25:43,665
Speaker 3:  to shut it down. Yeah. I dunno.

1497
01:25:44,885 --> 01:25:47,985
Speaker 6:  Yes. And and the next turn here, really, and this is what Sam was telling

1498
01:25:47,985 --> 01:25:51,785
Speaker 6:  us after the keynote, is he imagines that we will all have

1499
01:25:51,985 --> 01:25:55,785
Speaker 6:  our own GPT, which I'm really excited about. So a GPT that is trained

1500
01:25:55,805 --> 01:25:59,305
Speaker 6:  on all of your stuff, like a, the rewind idea, the pendant. But like

1501
01:25:59,885 --> 01:26:03,765
Speaker 6:  all of your software, all of your You know notes and your

1502
01:26:03,905 --> 01:26:07,885
Speaker 6:  GPT engages with other GPTs on your behalf. So it's like

1503
01:26:08,075 --> 01:26:11,925
Speaker 6:  bots on bots all the way down talking to each other. So it's like you tell

1504
01:26:12,075 --> 01:26:15,965
Speaker 6:  your GPT, I want to go to see a concert and have dinner with these friends

1505
01:26:15,985 --> 01:26:19,885
Speaker 6:  on this date, line up my ride, my reservation, the tickets. And it all

1506
01:26:19,885 --> 01:26:23,285
Speaker 6:  talks to different bots to do that. Sam thinks that's inevitable.

1507
01:26:24,065 --> 01:26:27,525
Speaker 6:  And what really? So open AI's stated mission is to build

1508
01:26:27,765 --> 01:26:31,565
Speaker 6:  AGI right? Artificial general intelligence, which is this like super, it's

1509
01:26:31,565 --> 01:26:34,645
Speaker 6:  like godlike AI that can think better than all of us combined.

1510
01:26:35,265 --> 01:26:38,605
Speaker 6:  People in the AI field that I've talked to over the last year think that

1511
01:26:38,755 --> 01:26:42,485
Speaker 6:  this next step where it's GPTs representing us,

1512
01:26:42,845 --> 01:26:46,605
Speaker 6:  engaging on our behalf unsupervised is

1513
01:26:46,825 --> 01:26:50,285
Speaker 6:  the environment that is needed to train the models to become

1514
01:26:50,845 --> 01:26:54,765
Speaker 6:  a step function even more intelligent. Because you need to have

1515
01:26:54,765 --> 01:26:58,605
Speaker 6:  this concept called self supervised learning, where these agents

1516
01:26:58,685 --> 01:27:02,285
Speaker 6:  are all interacting and training on their own. That's pretty freaky, right?

1517
01:27:02,285 --> 01:27:05,925
Speaker 6:  That's like matrix stuff. Yeah. That's coming in like the next couple of

1518
01:27:05,925 --> 01:27:09,725
Speaker 6:  years. I I think the world could look a lot different. It's just we're it

1519
01:27:09,725 --> 01:27:12,965
Speaker 6:  it, I think that, and that was the vibe there was like, we are kind of witnessing

1520
01:27:13,605 --> 01:27:16,565
Speaker 6:  a very, very exciting but also like very scary

1521
01:27:17,395 --> 01:27:18,085
Speaker 6:  next few years

1522
01:27:18,555 --> 01:27:22,365
Speaker 3:  Here. Yeah. And inside of it is a

1523
01:27:22,365 --> 01:27:22,805
Speaker 3:  copyright lawsuit.

1524
01:27:22,835 --> 01:27:24,645
Speaker 6:  It's a giant ticking time bomb of life.

1525
01:27:24,645 --> 01:27:27,605
Speaker 3:  It's just Ed Sheeran looking mad at you All. right? We gotta take a break.

1526
01:27:27,775 --> 01:27:30,245
Speaker 3:  We'll come back. We got a lightning round. We gotta get outta here. We'll

1527
01:27:30,245 --> 01:27:30,565
Speaker 3:  be right back

1528
01:28:22,975 --> 01:28:25,565
Speaker 3:  We're back. There's quite a lot on this lightning round, but we gotta go

1529
01:28:25,565 --> 01:28:28,085
Speaker 3:  fast. Cranz, why don't you start 'cause you got, I think you have the best

1530
01:28:28,085 --> 01:28:28,245
Speaker 3:  one.

1531
01:28:28,645 --> 01:28:31,205
Speaker 4:  I do have the best one. There's a new Steam Deck.

1532
01:28:32,635 --> 01:28:36,485
Speaker 4:  Sean Hollister has reviewed it. He naturally loves

1533
01:28:36,485 --> 01:28:40,445
Speaker 4:  it. I I, I think I called him like the day after he got it

1534
01:28:40,445 --> 01:28:42,605
Speaker 4:  and I was like, tell me everything. And he sure did.

1535
01:28:44,225 --> 01:28:47,365
Speaker 4:  He like, he really likes it. The the big things that have changed with this

1536
01:28:47,365 --> 01:28:50,725
Speaker 4:  one is there's a new processor. They went from the seven nanometer

1537
01:28:51,305 --> 01:28:55,085
Speaker 4:  ath processor named after the character from Final

1538
01:28:55,085 --> 01:28:58,805
Speaker 4:  Fantasy seven to the six nanometer Sphero

1539
01:28:59,315 --> 01:29:01,725
Speaker 4:  processor. Sames.

1540
01:29:03,155 --> 01:29:06,845
Speaker 4:  They, they've, they've, they've switched to an OLED display. They've switched,

1541
01:29:07,795 --> 01:29:11,725
Speaker 4:  they, they've, the battery license supposedly increased, which led to an

1542
01:29:11,725 --> 01:29:15,645
Speaker 4:  incredible quote from from Sean where he said,

1543
01:29:16,785 --> 01:29:20,325
Speaker 4:  for example, in 2022, I was able to play the

1544
01:29:20,525 --> 01:29:24,325
Speaker 4:  graphically intensive control for just under two hours on my original Steam

1545
01:29:24,325 --> 01:29:28,205
Speaker 4:  Deck review unit at 60 frames per second. This week I

1546
01:29:28,205 --> 01:29:31,205
Speaker 4:  played the same game at up to 80 frames per second

1547
01:29:32,025 --> 01:29:33,965
Speaker 4:  for two hours and 11 minutes.

1548
01:29:36,085 --> 01:29:39,525
Speaker 3:  I mean more frames You know. Yeah. More frames. He saw more frames. Frames.

1549
01:29:39,525 --> 01:29:43,445
Speaker 4:  He, he he, he does, he does add more stuff to it. It it is a pretty

1550
01:29:43,655 --> 01:29:47,045
Speaker 4:  measurably different big difference in battery life. Especially on games

1551
01:29:47,045 --> 01:29:50,845
Speaker 4:  that don't use as much of the, the CPU and the GPU. So like those,

1552
01:29:50,845 --> 01:29:54,645
Speaker 4:  those lower impact games are gonna last a lot, lot longer. The high impact

1553
01:29:54,645 --> 01:29:58,485
Speaker 4:  games. Those of us who are making our way through Boulder's Gate

1554
01:29:58,485 --> 01:30:01,405
Speaker 4:  three on the thing, we're not gonna see as much success.

1555
01:30:02,345 --> 01:30:06,285
Speaker 4:  The big shock for me was that there are no hu effect joysticks. I

1556
01:30:06,285 --> 01:30:09,245
Speaker 4:  just assumed that was gonna happen because there's been so much

1557
01:30:10,125 --> 01:30:12,685
Speaker 4:  frustration with the drift on the current joysticks. Yeah, they didn't do

1558
01:30:12,685 --> 01:30:16,205
Speaker 4:  that. That's a big bummer. But they got rid of the 32 gigabyte

1559
01:30:16,285 --> 01:30:20,045
Speaker 4:  EEMC version, which was the one that I bought. 'cause you could

1560
01:30:20,045 --> 01:30:23,565
Speaker 4:  replace the drive really cheap and easy. They got rid of that one now starts

1561
01:30:23,565 --> 01:30:27,285
Speaker 4:  at five 50 or 5 49. I'm

1562
01:30:27,285 --> 01:30:31,125
Speaker 4:  resisting buying one, but Sean loves it. Oh. And the power buttons now

1563
01:30:31,125 --> 01:30:31,445
Speaker 4:  orange.

1564
01:30:31,865 --> 01:30:32,645
Speaker 3:  Hey, that's pretty good.

1565
01:30:32,885 --> 01:30:34,605
Speaker 4:  I love it. Yeah, I'm excited. I

1566
01:30:34,605 --> 01:30:38,005
Speaker 3:  Would put this out as evidence as we've been saying there's more action in

1567
01:30:38,005 --> 01:30:41,565
Speaker 3:  gadgets than people ever give any credit to. And the love

1568
01:30:41,755 --> 01:30:45,685
Speaker 3:  that our audience and in particular Sean has for the Steam Deck is like

1569
01:30:45,745 --> 01:30:46,485
Speaker 3:  off the charts.

1570
01:30:46,875 --> 01:30:50,365
Speaker 4:  It's it's an awesome device and they don't have to do a bunch of really long

1571
01:30:50,485 --> 01:30:53,045
Speaker 4:  infomercials for, for us to be get hyped about it.

1572
01:30:53,075 --> 01:30:56,325
Speaker 3:  Yeah. You're like, look, you can play control at 80 frames per second for

1573
01:30:56,325 --> 01:30:58,885
Speaker 3:  11 minutes more. Yeah. That's all you gotta say.

1574
01:30:59,225 --> 01:31:01,725
Speaker 5:  All you got. I just love this thing. They just, they were just like, what

1575
01:31:01,725 --> 01:31:04,645
Speaker 5:  are all the things we can do to make this device better? And then they just

1576
01:31:04,865 --> 01:31:08,605
Speaker 5:  did all those things and now here we are, like, God

1577
01:31:08,605 --> 01:31:12,045
Speaker 5:  forbid more companies don't just like make their things better all the time.

1578
01:31:12,395 --> 01:31:15,445
Speaker 5:  Like it's, it's just, it's just makes me happy. And it was the same thing

1579
01:31:15,445 --> 01:31:17,885
Speaker 5:  with the Switch OLED, where they're like, we got this thing pretty right

1580
01:31:17,905 --> 01:31:21,365
Speaker 5:  the first time, but what if we just did a couple of things better? Wouldn't

1581
01:31:21,365 --> 01:31:24,645
Speaker 5:  that be cool? And everybody's like, yes. But it's like most companies are

1582
01:31:24,645 --> 01:31:28,245
Speaker 5:  like, okay, this is called the Steam Deck two and this time

1583
01:31:28,865 --> 01:31:32,805
Speaker 5:  it comes in six pieces and you have to like

1584
01:31:32,905 --> 01:31:36,605
Speaker 5:  do back flips while you use it. It's gonna be so dope. And it, and, and Valve

1585
01:31:36,605 --> 01:31:39,565
Speaker 5:  instead is just like, no, we like pretty much got this right. Yeah. We just

1586
01:31:39,565 --> 01:31:40,325
Speaker 5:  made it better now.

1587
01:31:40,555 --> 01:31:41,525
Speaker 4:  It's cool. It's like, yeah.

1588
01:31:41,525 --> 01:31:43,445
Speaker 5:  It's, that's the way. Just make your things better.

1589
01:31:43,645 --> 01:31:45,365
Speaker 4:  I yeah, I'm, that's all you need to do.

1590
01:31:45,565 --> 01:31:49,245
Speaker 5:  I just really appreciate that. One of the headline features of this one is

1591
01:31:49,245 --> 01:31:53,125
Speaker 5:  that the fan is both larger and quieter, which is like

1592
01:31:53,275 --> 01:31:53,925
Speaker 5:  crushed guys.

1593
01:31:54,265 --> 01:31:54,725
Speaker 4:  No more fan

1594
01:31:54,725 --> 01:31:57,245
Speaker 5:  One. This is some video game stuff here. Nailed it. We did it. It

1595
01:31:57,765 --> 01:32:01,245
Speaker 3:  I would know that. But I've been trying to scroll this article using Samsung

1596
01:32:01,455 --> 01:32:04,045
Speaker 3:  decks, which is not possible.

1597
01:32:04,405 --> 01:32:06,925
Speaker 4:  I didn't realize you were actively scrolling. I thought you were doing I

1598
01:32:06,925 --> 01:32:09,885
Speaker 4:  always do the thing where I like fidget and scroll up and down. No, you're

1599
01:32:09,885 --> 01:32:10,445
Speaker 4:  just trying to read

1600
01:32:10,445 --> 01:32:13,005
Speaker 3:  It. No, I'm just trying to land on a sentence that I can read because this

1601
01:32:13,005 --> 01:32:14,005
Speaker 3:  mouse is so fast.

1602
01:32:14,005 --> 01:32:16,925
Speaker 5:  You just flying top to bottom of the, the table. I was controlled of it.

1603
01:32:17,865 --> 01:32:21,605
Speaker 3:  And look Android as many things as an operating system and

1604
01:32:22,075 --> 01:32:24,605
Speaker 3:  Samsung's one UI as many things as a skin on Android,

1605
01:32:26,005 --> 01:32:29,205
Speaker 3:  a combination of things that is designed to let you change mouse tracking

1606
01:32:29,255 --> 01:32:33,125
Speaker 3:  speed is not on the list of priorities for either of those com companies.

1607
01:32:33,745 --> 01:32:35,485
Speaker 3:  All, right. David, what is your lightning round?

1608
01:32:35,765 --> 01:32:39,605
Speaker 5:  Mine's very simple. It's just that YouTube in its relentless

1609
01:32:39,885 --> 01:32:43,845
Speaker 5:  quest to be TikTok is now adding a for you page to,

1610
01:32:43,985 --> 01:32:47,645
Speaker 5:  or a for you carousel to people's channels,

1611
01:32:47,815 --> 01:32:51,445
Speaker 5:  which is basically like personalized recommended videos on people's

1612
01:32:51,445 --> 01:32:55,165
Speaker 5:  channels that you land on a very smart be

1613
01:32:55,595 --> 01:32:58,965
Speaker 5:  forever interesting to me that the phrase for you has just completely taken

1614
01:32:58,965 --> 01:33:02,565
Speaker 5:  over the internet. Like we've just decided that that is the term for every

1615
01:33:02,565 --> 01:33:06,165
Speaker 5:  personalized everything for you. Just one. And

1616
01:33:06,355 --> 01:33:10,325
Speaker 5:  also YouTube really ought to stop trying to be TikTok. Like just, oh

1617
01:33:10,325 --> 01:33:10,565
Speaker 5:  my God,

1618
01:33:10,715 --> 01:33:11,005
Speaker 6:  Just

1619
01:33:11,005 --> 01:33:13,765
Speaker 3:  Stop. I think everything is inevitably gonna be TikTok. But I,

1620
01:33:14,885 --> 01:33:18,765
Speaker 6:  I opened my YouTube mobile app to the shorts thing and it was just, it

1621
01:33:18,765 --> 01:33:22,725
Speaker 6:  was a, it was little tiles of shorts filling my entire serene

1622
01:33:22,755 --> 01:33:26,085
Speaker 6:  when I opened my YouTube app on my phone. Like, no, no.

1623
01:33:27,645 --> 01:33:30,405
Speaker 4:  I still sometimes accidentally scroll it forgetting I'm not in TikTok.

1624
01:33:31,265 --> 01:33:32,805
Speaker 3:  Oh yeah. That's that's incredible.

1625
01:33:32,995 --> 01:33:36,125
Speaker 4:  That and Instagram, I'm like, oh, that's why it's kind of crummy and weird.

1626
01:33:36,435 --> 01:33:39,885
Speaker 3:  Yeah. It's it's like watching the algorithms grow up.

1627
01:33:41,075 --> 01:33:44,805
Speaker 3:  Like they're at different stages of being toddlers and like

1628
01:33:44,805 --> 01:33:48,325
Speaker 3:  TikTok is like running away with the election You know. But

1629
01:33:48,755 --> 01:33:51,805
Speaker 3:  like YouTube is just like, is this anything? Will this make you mad? Will

1630
01:33:51,805 --> 01:33:55,325
Speaker 3:  it make you sad? Horny? Yeah. You know. Instagram is like, here's a

1631
01:33:55,325 --> 01:33:59,085
Speaker 3:  Kardashian You know. Like back to basics. Yeah. At least in my feed, they

1632
01:33:59,085 --> 01:34:02,565
Speaker 3:  figured out that I like truck jumps. A lot of truck jumps on my Instagram.

1633
01:34:02,565 --> 01:34:04,645
Speaker 3:  Like You know. It's like getting better You know. Yeah, yeah, yeah. Because

1634
01:34:04,725 --> 01:34:07,605
Speaker 3:  I engage with it more. And then TikTok is just like full crazy.

1635
01:34:08,125 --> 01:34:11,525
Speaker 4:  I I screwed up on TikTok the other day and I thought I had broken my algorithm

1636
01:34:11,765 --> 01:34:13,925
Speaker 4:  'cause it just kept showing me bagpipe videos.

1637
01:34:14,585 --> 01:34:17,685
Speaker 3:  No, that's what I mean. TikTok is now at the point where it's in the deepest

1638
01:34:17,755 --> 01:34:21,005
Speaker 3:  darkest recesses of your personality. Oh yeah. Yeah. If you've been scrolling

1639
01:34:21,005 --> 01:34:24,205
Speaker 3:  it for years and years, it's horrible. It's, it's like beyond truck jumps,

1640
01:34:24,205 --> 01:34:28,045
Speaker 3:  but it's like deep fried truck memes that like, I can't explain

1641
01:34:28,065 --> 01:34:31,845
Speaker 3:  to anyone else. There's a video I keep seeing of like a

1642
01:34:31,875 --> 01:34:35,285
Speaker 3:  motorized pizza peel that just picks up a pizza. Okay. But that's, and puts

1643
01:34:35,285 --> 01:34:37,205
Speaker 3:  it back down. Cool. It's,

1644
01:34:37,505 --> 01:34:38,485
Speaker 4:  It was bagpipes.

1645
01:34:39,825 --> 01:34:43,045
Speaker 3:  I'm saying Alex. It's telling you more about yourself than you need to know.

1646
01:34:43,525 --> 01:34:46,765
Speaker 3:  I never, it's in there. It's in there.

1647
01:34:46,915 --> 01:34:48,525
Speaker 4:  Five year old Alex thought they were cool.

1648
01:34:49,585 --> 01:34:49,805
Speaker 3:  So

1649
01:34:50,405 --> 01:34:52,565
Speaker 4:  I left it at five. Leave me alone. That

1650
01:34:52,565 --> 01:34:56,125
Speaker 3:  Guys TikTok has the reset algorithm button and I think about hitting it all

1651
01:34:56,125 --> 01:34:56,285
Speaker 3:  the time.

1652
01:34:56,985 --> 01:35:00,605
Speaker 4:  Oh my God, I should hit it. Mm. I think I have to at this point time.

1653
01:35:00,605 --> 01:35:04,085
Speaker 3:  We should all hit it here. Here's my pitch for this whole thing. I've now

1654
01:35:04,085 --> 01:35:07,405
Speaker 3:  watched more. Again, we've had a lot of copyright on the show, but I've watched

1655
01:35:07,515 --> 01:35:11,405
Speaker 3:  more movies on TikTok recently than anything. And all of these

1656
01:35:11,405 --> 01:35:14,125
Speaker 3:  video streaming platforms are trying to get you to engage again. And it's

1657
01:35:14,125 --> 01:35:17,725
Speaker 3:  like Netflix just make make the

1658
01:35:17,785 --> 01:35:18,565
Speaker 3:  TikTok. Yeah.

1659
01:35:18,875 --> 01:35:19,525
Speaker 4:  Just show me.

1660
01:35:19,595 --> 01:35:22,965
Speaker 3:  Just like, let me scroll through it and I'm like, oh man. Yeah. I'll start

1661
01:35:22,965 --> 01:35:26,725
Speaker 3:  couples retreat in the middle of it for some reason and then that'll play

1662
01:35:26,725 --> 01:35:29,605
Speaker 3:  on my TV and we'll be done. Yeah. And like a, I

1663
01:35:30,085 --> 01:35:33,365
Speaker 5:  Absolutely 100% guarantee you that's coming. Yeah. Like

1664
01:35:33,895 --> 01:35:37,885
Speaker 5:  there is not one shadow of a doubt in my mind that at some point in the very

1665
01:35:37,885 --> 01:35:41,365
Speaker 5:  near future, the Netflix mobile app will have that thing for you.

1666
01:35:41,665 --> 01:35:44,565
Speaker 5:  But I also think part of the reason I, I picked this one is my lightning

1667
01:35:44,565 --> 01:35:47,645
Speaker 5:  round thing is because it's just like, I've gotten totally obsessed with

1668
01:35:47,645 --> 01:35:51,525
Speaker 5:  the idea that scrolling is just like the, the behavior. And it doesn't matter

1669
01:35:51,525 --> 01:35:54,205
Speaker 5:  what you're scrolling, it doesn't matter what you're doing. It's just like

1670
01:35:54,305 --> 01:35:57,205
Speaker 5:  people wanna scroll. You just like have a moment where you're like, I'm gonna

1671
01:35:57,205 --> 01:35:59,925
Speaker 5:  get on my phone and just do some scrolling. It's just like a thing people

1672
01:35:59,925 --> 01:36:03,445
Speaker 5:  talk about and the activity is so much more important than what you're doing

1673
01:36:03,445 --> 01:36:07,245
Speaker 5:  it with. But it's just like, I, I now think about that all the time is it's

1674
01:36:07,245 --> 01:36:10,325
Speaker 5:  just like, that is a mode of engaging with technology is I'm just gonna scroll.

1675
01:36:10,625 --> 01:36:14,445
Speaker 5:  And I think the more that becomes the thing, especially as young people are

1676
01:36:14,445 --> 01:36:17,445
Speaker 5:  trained on that as like, I have five minutes, I'm gonna do some scrolling.

1677
01:36:17,725 --> 01:36:21,605
Speaker 5:  I just like, I hear more people say that and you're gonna see it

1678
01:36:21,925 --> 01:36:24,885
Speaker 5:  absolutely everywhere and it is gonna like completely change the internet

1679
01:36:24,885 --> 01:36:28,405
Speaker 5:  because it's like, instead of I'm gonna go like look at Reddit or whatever,

1680
01:36:28,405 --> 01:36:30,085
Speaker 5:  it's just like, I'm just gonna do some scrolling

1681
01:36:31,045 --> 01:36:33,505
Speaker 3:  And it's just the for you feed of scrolls everywhere.

1682
01:36:33,815 --> 01:36:34,865
Speaker 5:  Yeah. It's

1683
01:36:34,865 --> 01:36:38,345
Speaker 3:  Nuts. And it's just a bunch of algorithms starting at the very beginning

1684
01:36:38,485 --> 01:36:42,345
Speaker 3:  of like personalities, like angry, sad, hungry, horny

1685
01:36:42,725 --> 01:36:45,105
Speaker 3:  and then all the way to bagpipes. No.

1686
01:36:45,695 --> 01:36:45,985
Speaker 5:  Like,

1687
01:36:46,095 --> 01:36:49,865
Speaker 3:  Yeah. Whatever complex. I would love to see the database

1688
01:36:49,915 --> 01:36:53,825
Speaker 3:  field of like bagpipe emotional reactions that TikTok thinks

1689
01:36:53,825 --> 01:36:57,745
Speaker 3:  it's getting when it gets a bagpipe engagement. You know, like Netflix has

1690
01:36:57,745 --> 01:37:00,665
Speaker 3:  those like deeply over constructed movie categories. Yeah.

1691
01:37:01,265 --> 01:37:01,945
Speaker 3:  Bagpipes are

1692
01:37:01,945 --> 01:37:05,665
Speaker 4:  Like, it was all because I liked a friend's video. Yeah. And for some reason

1693
01:37:05,665 --> 01:37:08,505
Speaker 4:  it was like, I know which part of that we need to pull out for you

1694
01:37:09,225 --> 01:37:09,785
Speaker 5:  Bagpipes.

1695
01:37:09,965 --> 01:37:10,505
Speaker 3:  I'm telling you

1696
01:37:11,005 --> 01:37:13,785
Speaker 5:  One of the things I love about these conversations is it's always just people

1697
01:37:13,785 --> 01:37:16,105
Speaker 5:  telling on themselves. Yep. About the stuff that they watch on social media.

1698
01:37:16,215 --> 01:37:20,185
Speaker 5:  It's so fun. You're like, aren't you also on bagpipe TikTok? And it's like,

1699
01:37:20,185 --> 01:37:21,705
Speaker 5:  no, no one else is on just

1700
01:37:21,705 --> 01:37:22,505
Speaker 4:  You. There are a lot of people on Bag

1701
01:37:22,575 --> 01:37:23,265
Speaker 5:  Pipe TikTok,

1702
01:37:24,175 --> 01:37:24,785
Speaker 3:  Including

1703
01:37:24,785 --> 01:37:25,105
Speaker 4:  Me now.

1704
01:37:25,285 --> 01:37:28,545
Speaker 3:  All. right. Here's mine. Great piece by Jen Tuey this week.

1705
01:37:29,525 --> 01:37:32,825
Speaker 3:  If you're a smart home nerd, this really hits you. There's a company called

1706
01:37:32,825 --> 01:37:35,865
Speaker 3:  Chamberlain makes all the garage door openers, like if you buy a house,

1707
01:37:36,645 --> 01:37:39,785
Speaker 3:  the chance that you have a lift master garage door opener made by Chamberlain.

1708
01:37:39,855 --> 01:37:43,585
Speaker 3:  Very, very high. They, like many

1709
01:37:43,965 --> 01:37:47,825
Speaker 3:  bad companies, have smart devices, so you can make the, you can make the

1710
01:37:47,825 --> 01:37:51,665
Speaker 3:  garage opener smart. They have a proprietary platform called MyQ, which is

1711
01:37:52,055 --> 01:37:55,845
Speaker 3:  long since a disaster. MyQ is monetized through partnerships

1712
01:37:55,845 --> 01:37:59,425
Speaker 3:  with car companies. So you've got a car with like a

1713
01:37:59,975 --> 01:38:03,585
Speaker 3:  wifi connection in it. You can like push the button, cloud service happens,

1714
01:38:03,615 --> 01:38:07,465
Speaker 3:  your garage door opens, whatever. Man. Many,

1715
01:38:07,495 --> 01:38:11,425
Speaker 3:  many, many people have hacked into my queue with

1716
01:38:11,435 --> 01:38:15,425
Speaker 3:  HomeBridge and home assistant, which are really cool things. It's

1717
01:38:15,425 --> 01:38:18,505
Speaker 3:  very nerdy, but you can have a little tamagotchi of raspberry pie in your

1718
01:38:18,505 --> 01:38:21,265
Speaker 3:  house, so you have to reset every couple months. And it, it bridges all the

1719
01:38:21,265 --> 01:38:24,625
Speaker 3:  things together into HomeKit. That's my home bridge. Like my ring cameras

1720
01:38:24,695 --> 01:38:28,585
Speaker 3:  show up in HomeKit because of Home Bridge. My queue showed up in

1721
01:38:28,585 --> 01:38:31,665
Speaker 3:  HomeKit for a lot of people because of Home Bridge. My queue shut off this

1722
01:38:31,865 --> 01:38:32,305
Speaker 3:  integration.

1723
01:38:33,005 --> 01:38:33,465
Speaker 4:  Not cool.

1724
01:38:33,895 --> 01:38:36,945
Speaker 3:  They claim that they're getting DDoS through it, which is like, you could

1725
01:38:36,945 --> 01:38:39,825
Speaker 3:  have a real integration with HomeKit and everyone Shut up. You wouldn't get

1726
01:38:39,935 --> 01:38:43,705
Speaker 3:  DDoS. So they th and this is the part that just

1727
01:38:43,705 --> 01:38:47,345
Speaker 3:  rankles, they explain their decision by saying,

1728
01:38:47,475 --> 01:38:50,425
Speaker 3:  we've made the decision to prevent unauthorized usage of the MyQ ecosystem

1729
01:38:50,425 --> 01:38:53,345
Speaker 3:  through third party apps. It's so that we can continue to provide the best

1730
01:38:53,545 --> 01:38:57,425
Speaker 3:  possible experience for our 10 million plus users. We understand this

1731
01:38:57,425 --> 01:39:00,585
Speaker 3:  impacts a small percentage of users, but ultimately this will improve the

1732
01:39:00,585 --> 01:39:03,025
Speaker 3:  performance and reliability of MyQ benefiting all of our users.

1733
01:39:04,415 --> 01:39:07,345
Speaker 4:  That is the cleanest garage I've ever seen. Yeah.

1734
01:39:07,365 --> 01:39:11,265
Speaker 3:  The picture here, so they said this is 2%

1735
01:39:11,285 --> 01:39:15,185
Speaker 3:  of people. So Jen just did the math. You've got 10 million users. You've,

1736
01:39:15,525 --> 01:39:18,225
Speaker 3:  you've broken 200,000 garage doors,

1737
01:39:22,715 --> 01:39:26,625
Speaker 3:  Which is just incredible. So now they're freaking out. I will

1738
01:39:26,625 --> 01:39:29,945
Speaker 3:  tell you, MyQ has always been a disaster. They've always been pretty user

1739
01:39:29,945 --> 01:39:33,745
Speaker 3:  hostile. This is not a secret, it's just what people have and no one thinks

1740
01:39:33,745 --> 01:39:36,385
Speaker 3:  about replacing the garage door opener. So everyone's been hacking around

1741
01:39:36,385 --> 01:39:40,065
Speaker 3:  it. You can buy little adapters. I have one called

1742
01:39:40,675 --> 01:39:44,065
Speaker 3:  Miros, I think they're refurbished. Ones are called Ress. It's a very confusing,

1743
01:39:44,705 --> 01:39:48,425
Speaker 3:  they're like some, anywhere between 30 and 50 bucks depending on sale.

1744
01:39:48,885 --> 01:39:51,145
Speaker 3:  You can just wire 'em into your garage opener, put 'em at home. Kit they

1745
01:39:51,145 --> 01:39:55,065
Speaker 3:  work. There's other ones. Jen has a whole list of other alternatives.

1746
01:39:55,685 --> 01:39:57,785
Speaker 3:  If you're the sort of person who wants to open your garage door from your

1747
01:39:57,785 --> 01:40:00,385
Speaker 3:  phone, and I am that sort of person. You are, there are many ways to do this,

1748
01:40:00,385 --> 01:40:04,305
Speaker 3:  to do not involve the MyQ platform. And our article has irritated

1749
01:40:04,305 --> 01:40:08,265
Speaker 3:  them so much that they're issuing corrections and they're trying

1750
01:40:08,265 --> 01:40:11,385
Speaker 3:  to explain their decision even more when all they need to do is just either

1751
01:40:11,385 --> 01:40:14,545
Speaker 3:  build the home kit integration or let the people do the hacks.

1752
01:40:14,975 --> 01:40:17,385
Speaker 4:  Yeah. Just like say, okay, home assistance's fine.

1753
01:40:17,745 --> 01:40:20,625
Speaker 3:  I love it when companies are like, it's a tiny percentage of users. They're

1754
01:40:20,625 --> 01:40:23,665
Speaker 3:  like, you just broke 200,000 garage doors.

1755
01:40:25,415 --> 01:40:27,345
Speaker 4:  Only 200. Whatcha doing only 200 K

1756
01:40:27,805 --> 01:40:29,745
Speaker 3:  All, right? Alex, you got a lightning your hand? Yeah.

1757
01:40:29,745 --> 01:40:32,825
Speaker 6:  It's actually a thing. I've been trying, I've been test driving a rivian

1758
01:40:32,905 --> 01:40:36,705
Speaker 6:  R one s for the last week. Yeah. So RJ the CO is at code,

1759
01:40:36,925 --> 01:40:40,265
Speaker 6:  and this is not a, an ad by any means, but

1760
01:40:41,065 --> 01:40:44,225
Speaker 6:  I have had fun in it a lot. We took it offroading in Joshua Tree

1761
01:40:44,925 --> 01:40:48,865
Speaker 6:  and an electric, it was my first time in an electric

1762
01:40:48,865 --> 01:40:52,705
Speaker 6:  like SUV and the thinking goes zero to 60

1763
01:40:52,705 --> 01:40:55,885
Speaker 6:  in three, three seconds and has like 840 hor horsepower.

1764
01:40:56,585 --> 01:41:00,405
Speaker 6:  And it's just like driving a big ass go-kart through the desert that

1765
01:41:00,405 --> 01:41:04,245
Speaker 6:  Yeah, like it was just, and I like the, I mean just a lot of

1766
01:41:04,245 --> 01:41:08,085
Speaker 6:  the small decisions they've made in terms of the You know the OSS and

1767
01:41:08,085 --> 01:41:12,005
Speaker 6:  the, the interior of the car. The moment they do a mid-sized

1768
01:41:12,185 --> 01:41:15,725
Speaker 6:  one of these, I think it's gonna be a really compelling Tesla alternative.

1769
01:41:15,865 --> 01:41:19,645
Speaker 6:  But right now it's very like outdoorsy, You know, big, big hulking vehicles.

1770
01:41:20,145 --> 01:41:20,485
Speaker 6:  But yeah.

1771
01:41:20,585 --> 01:41:21,085
Speaker 3:  Had a lot of

1772
01:41:21,235 --> 01:41:22,845
Speaker 5:  Next up cyber truck for you.

1773
01:41:23,025 --> 01:41:24,445
Speaker 3:  That's the Yeah, I've

1774
01:41:24,445 --> 01:41:28,285
Speaker 6:  Heard, yeah, I've heard the rivian described as the, i I can't say it on

1775
01:41:28,385 --> 01:41:32,205
Speaker 6:  our air, but the, a version of the cyber truck that for people who

1776
01:41:32,275 --> 01:41:32,565
Speaker 6:  Yeah,

1777
01:41:33,125 --> 01:41:35,645
Speaker 3:  I, IIII

1778
01:41:35,665 --> 01:41:36,725
Speaker 6:  All, right? Yeah. Anyway,

1779
01:41:37,675 --> 01:41:41,005
Speaker 3:  Okay. Can I say my cyber truck thing? So I asked for pictures of the wiper

1780
01:41:41,365 --> 01:41:44,165
Speaker 3:  and there've been lots of cyber trucks everywhere and people keep sending

1781
01:41:44,165 --> 01:41:47,925
Speaker 3:  me not closeups. They're like, here's another video of the

1782
01:41:47,925 --> 01:41:51,805
Speaker 3:  wiper. And it's like you're six miles away. Hmm. So. I'm gonna be more specific

1783
01:41:52,225 --> 01:41:56,165
Speaker 3:  and need a closeup of the middle of the wiper. Hmm. The middle,

1784
01:41:56,345 --> 01:41:59,925
Speaker 3:  not the, not the arm, not the part at the bottom of the windshield that turns

1785
01:42:00,425 --> 01:42:04,365
Speaker 3:  the middle of the wiper blade. Or even better closeups all

1786
01:42:04,385 --> 01:42:08,125
Speaker 3:  the way up the wiper. Like connecting to the glass. Yes. And I'll be more

1787
01:42:08,605 --> 01:42:12,565
Speaker 3:  specific. I have heard rumors of how the actual blade is

1788
01:42:12,565 --> 01:42:16,525
Speaker 3:  constructed that I would like to verify. I will not tell you what they

1789
01:42:16,525 --> 01:42:20,165
Speaker 3:  are. Please. 'cause I don't want to put disinformation in the world and I

1790
01:42:20,165 --> 01:42:22,725
Speaker 3:  know the Tesla people are crazy and I don't wanna, I don't want clips of

1791
01:42:22,725 --> 01:42:26,605
Speaker 3:  me saying a thing that's wrong. I'm just saying I've heard some information

1792
01:42:27,155 --> 01:42:31,085
Speaker 3:  that I cannot verify about the wiper blade itself. So if

1793
01:42:31,085 --> 01:42:34,885
Speaker 3:  you encounter one, just go look at the blade, take some

1794
01:42:34,885 --> 01:42:37,725
Speaker 3:  pictures, start at the bottom, click, click, click, click, click all the

1795
01:42:37,725 --> 01:42:39,885
Speaker 3:  way at the top, see if you notice anything.

1796
01:42:40,245 --> 01:42:43,325
Speaker 6:  A thing about the rivian wipers, they work and there's two of them.

1797
01:42:44,085 --> 01:42:45,285
Speaker 3:  I was about to say I heard a

1798
01:42:45,285 --> 01:42:48,525
Speaker 6:  Plural there. Yeah, there's two of them. They cover the entire windshield

1799
01:42:48,585 --> 01:42:52,485
Speaker 6:  and they work. The only thing I don't like is there's not the cleaner fluid

1800
01:42:52,665 --> 01:42:56,485
Speaker 6:  boring. Right? Like you can't, like why are we making new cars that cost

1801
01:42:56,525 --> 01:42:57,925
Speaker 6:  a lot of money that don't have this? Wait, it

1802
01:42:57,925 --> 01:42:58,645
Speaker 3:  Doesn't have windshield washer,

1803
01:42:58,645 --> 01:43:02,445
Speaker 6:  It doesn't have the washer fluid. I'm like, I'm driving this thing through

1804
01:43:02,445 --> 01:43:03,045
Speaker 6:  the desert. I

1805
01:43:03,315 --> 01:43:04,245
Speaker 3:  It's empty. Or it just

1806
01:43:04,245 --> 01:43:05,525
Speaker 6:  Doesn't have it, it doesn't exist.

1807
01:43:05,985 --> 01:43:09,605
Speaker 5:  No, you guys, we cannot pick the windshield wiper fight again. We can't.

1808
01:43:09,935 --> 01:43:13,285
Speaker 5:  We've been, we've been down this road before. Neli iss about to say, just

1809
01:43:13,285 --> 01:43:15,045
Speaker 5:  put water in, it'll be fine. And we're gonna get

1810
01:43:15,045 --> 01:43:15,205
Speaker 6:  Emails

1811
01:43:15,205 --> 01:43:17,445
Speaker 3:  Again saying doesn't exist. He's saying there's no tank. There's no tank.

1812
01:43:17,445 --> 01:43:19,325
Speaker 3:  He's saying they've obviated the entire problem.

1813
01:43:20,435 --> 01:43:20,725
Speaker 6:  It's

1814
01:43:20,725 --> 01:43:24,125
Speaker 3:  Bizarre. Someone asked me if I was gonna ask Obama what win should washer

1815
01:43:24,125 --> 01:43:27,965
Speaker 3:  fluid. He says, we have picked a lot of dumb battles on the show and this

1816
01:43:27,965 --> 01:43:31,365
Speaker 3:  is easily the dumbest and do not just put water in your

1817
01:43:31,615 --> 01:43:32,645
Speaker 3:  fluid. Anyway, Neli

1818
01:43:32,755 --> 01:43:35,605
Speaker 6:  Neli, the one thing I did wanna ask you about Obama was how was the secret

1819
01:43:35,605 --> 01:43:35,885
Speaker 6:  service?

1820
01:43:37,145 --> 01:43:40,965
Speaker 3:  So this was our second round with him and we are in his offices. So they

1821
01:43:40,965 --> 01:43:43,805
Speaker 3:  were chill. Okay. The first round. What, did

1822
01:43:43,805 --> 01:43:44,405
Speaker 6:  You get a pat down?

1823
01:43:44,565 --> 01:43:48,245
Speaker 3:  I did not. We already, we were already all cleared from Okay. Before.

1824
01:43:49,105 --> 01:43:52,565
Speaker 3:  So when we were at Harvard, there was like intense security in the other

1825
01:43:52,565 --> 01:43:56,525
Speaker 3:  protests that day. 'cause he was speaking, we're on a college campus, there

1826
01:43:56,525 --> 01:44:00,045
Speaker 3:  were dogs. Like the whole thing happened. We hung out with the dogs and we

1827
01:44:00,045 --> 01:44:03,325
Speaker 3:  found out he wasn't gonna be there. So then we, it was cool. The dogs were

1828
01:44:03,325 --> 01:44:07,165
Speaker 3:  cool. They're very friendly. But at his offices we had already

1829
01:44:07,165 --> 01:44:10,845
Speaker 3:  been pre-screened and we were on his home turf.

1830
01:44:11,345 --> 01:44:14,805
Speaker 3:  So there was, there was a, there was a presence. He doesn't not have a presence.

1831
01:44:15,365 --> 01:44:17,805
Speaker 3:  And there was some discussion about whether the blinds could be open or closed

1832
01:44:17,825 --> 01:44:21,205
Speaker 3:  and the, and whatever. But it was, it was very chill. 'cause we were in his

1833
01:44:21,255 --> 01:44:25,085
Speaker 3:  space as opposed to when you're not in his, just moving him around

1834
01:44:25,145 --> 01:44:28,645
Speaker 3:  is a complicated, I would imagine is what we have discovered. Yeah.

1835
01:44:29,215 --> 01:44:33,165
Speaker 3:  Can't go to ban. Can I actually, can I tell you my one Obama

1836
01:44:33,165 --> 01:44:36,765
Speaker 3:  behind the scenes story and we can, we can end this episode to,

1837
01:44:37,465 --> 01:44:40,645
Speaker 3:  in that episode we talk a lot about free speech on the internet and regulating

1838
01:44:41,075 --> 01:44:44,365
Speaker 3:  free speech. And one of the things that's really hard to do is like you need

1839
01:44:44,435 --> 01:44:48,165
Speaker 3:  some constitutional authority to go ahead and regulate speech. So the one

1840
01:44:48,165 --> 01:44:51,965
Speaker 3:  that came up in the episode is the FCC has some authority over

1841
01:44:51,965 --> 01:44:55,645
Speaker 3:  broadcast stations. Right? So you swear

1842
01:44:55,825 --> 01:44:59,805
Speaker 3:  on A, B, C or CBS or NBC people can file complaints to

1843
01:44:59,805 --> 01:45:03,605
Speaker 3:  the FCC. The FCC can find them because you're using the public airwaves.

1844
01:45:03,675 --> 01:45:07,485
Speaker 3:  This is like a real thing. So like radio stations and broadcast

1845
01:45:07,585 --> 01:45:11,565
Speaker 3:  tv. There's some basis for the FCC to regulate the content in there.

1846
01:45:11,565 --> 01:45:14,765
Speaker 3:  Yeah. And that is wax and weight. This doesn't exist for cable tv, which

1847
01:45:14,765 --> 01:45:18,645
Speaker 3:  is why HBO can exist. Why cable news is like off the rails. Why?

1848
01:45:18,685 --> 01:45:22,565
Speaker 3:  I can just swear it will on the internet on this show. Right. The government

1849
01:45:22,565 --> 01:45:25,805
Speaker 3:  has basically no authority. 'cause there's no public airwaves, So, I ask

1850
01:45:25,805 --> 01:45:28,085
Speaker 3:  Obama this question. It's in the episode and he's like, Hey, you've seen

1851
01:45:28,085 --> 01:45:31,725
Speaker 3:  some hook. So then we get up and we take a group photo 'cause he's an

1852
01:45:31,925 --> 01:45:34,645
Speaker 3:  excellent politician and he knows they should take a group photo. So everyone's

1853
01:45:34,645 --> 01:45:38,445
Speaker 3:  like happy. And I did the least journalistic thing in my life, which

1854
01:45:38,685 --> 01:45:42,365
Speaker 3:  I took a video of him saying hi to Max. Which again,

1855
01:45:42,545 --> 01:45:45,085
Speaker 3:  I'm just admitting to you disclosure, the least journalistic thing I've ever

1856
01:45:45,085 --> 01:45:47,405
Speaker 3:  did in my life. But my daughter has a video of a bomb saying hi to her. Which

1857
01:45:47,505 --> 01:45:51,085
Speaker 3:  that's awesome. You can't get that on cameo. Yeah, yeah. It's like hard.

1858
01:45:51,085 --> 01:45:54,765
Speaker 3:  Yeah. And then he's leaving and I was like that

1859
01:45:54,915 --> 01:45:58,885
Speaker 3:  this like thing, like this feels like the problem to me. Like everyone

1860
01:45:58,885 --> 01:46:01,645
Speaker 3:  wants to do this, everyone wants to issue some speech regulation, but you

1861
01:46:01,645 --> 01:46:04,685
Speaker 3:  gotta get over this problem. And he looked at me and he goes, yeah, you just

1862
01:46:04,685 --> 01:46:06,685
Speaker 3:  need a hook. You just gotta figure it out. We'll figure it out. And he like

1863
01:46:06,685 --> 01:46:10,285
Speaker 3:  walked outta the room and I was like, oh, you used to be the most powerful

1864
01:46:10,285 --> 01:46:14,125
Speaker 3:  person in the world. Like your brain is just like, this problem should be

1865
01:46:14,125 --> 01:46:17,525
Speaker 3:  solved. Like figure it out. Yeah. And it was like Kate Cox,

1866
01:46:18,185 --> 01:46:20,885
Speaker 3:  the decoder producer and I, both our minds were just blown

1867
01:46:22,035 --> 01:46:25,285
Speaker 3:  because that's just like not, I don't have the ability to just like order

1868
01:46:25,285 --> 01:46:29,005
Speaker 3:  someone to solve a constitutional law problem. Yeah. And he was like, yeah,

1869
01:46:29,005 --> 01:46:32,445
Speaker 3:  just, I dunno, shut up. Just like what? Just delegate

1870
01:46:34,825 --> 01:46:37,525
Speaker 3:  It was and it was just like, you don't have that experience very often. Yeah.

1871
01:46:37,585 --> 01:46:41,565
Speaker 3:  The charisma too. Yeah. Yeah. It was a lot. Anyhow,

1872
01:46:41,565 --> 01:46:45,285
Speaker 3:  that's it. That's of our Chest. Please. If you see a cyber

1873
01:46:45,335 --> 01:46:48,885
Speaker 3:  truck just all the way up the wiper stock, I'm telling you there's something

1874
01:46:48,885 --> 01:46:52,005
Speaker 3:  there. I don't know what it is, but people tell me there's something there

1875
01:46:52,045 --> 01:46:52,485
Speaker 3:  I should look at

1876
01:46:52,545 --> 01:46:55,645
Speaker 6:  And make the Neli wearable index. GPT. Yeah,

1877
01:46:55,645 --> 01:46:59,485
Speaker 3:  The graph. The graph. We need the graph. Yeah. I'm telling you, we, we,

1878
01:46:59,705 --> 01:47:02,965
Speaker 3:  the Vision Pro, the whole Vision Pro review is just gonna be me doing math,

1879
01:47:03,125 --> 01:47:06,405
Speaker 6:  But, but I want it to be a input. I want it to be a GPT. Where you can say

1880
01:47:06,705 --> 01:47:10,525
Speaker 6:  is this thing, how does this fit into Eli's graph of wearable

1881
01:47:10,555 --> 01:47:11,245
Speaker 6:  awesomeness?

1882
01:47:11,815 --> 01:47:15,805
Speaker 3:  Right? fids Times face has to be less than or equal

1883
01:47:15,805 --> 01:47:18,925
Speaker 3:  to usefulness. That's the whole equation. You just figure that out for me.

1884
01:47:19,025 --> 01:47:21,445
Speaker 3:  All, right? That's it. Thanks everybody. That's rich cast. Back on

1885
01:47:27,185 --> 01:47:30,405
Speaker 1:  And that's a wrap for Vergecast this week. Hey, we'd love to hear from you.

1886
01:47:30,475 --> 01:47:34,445
Speaker 1:  Give us a call at eight six six Verge one. One The. Vergecast is

1887
01:47:34,445 --> 01:47:38,285
Speaker 1:  a production of The Verge and Box Media Podcast Network. The show is produced

1888
01:47:38,285 --> 01:47:42,045
Speaker 1:  by Andrew Marino and Liam James. This episode was mixed and edited by

1889
01:47:42,135 --> 01:47:44,725
Speaker 1:  Xandr Adams. And that's it. We'll see you next week.

