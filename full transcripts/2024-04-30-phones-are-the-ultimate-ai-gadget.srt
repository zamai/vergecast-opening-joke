1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: ea0a0cb9-cd25-46fc-9ed0-9f057a038aac
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-6502887084526400668/3424120342131868000/s93290-US-4374s-1714462819.mp3
Description: Today on the flagship podcast of dedicated AI hardware: 

The Verge’s David Pierce and Allison Johnson debate whether the emergence of standalone AI gadgets like the Humane Pin and the Rabbit R1 are better off as apps or should exist as its own hardware. 


Humane AI Pin review: not even close 


The Humane AI Pin worked better than I expected — until it didn’t 

A morning with the Rabbit R1: a fun, funky, unfinished AI gadget

Can Rabbit’s R1 outsmart the smartphone assistants? Let’s find out!

The future of AI gadgets is just phones


The Ray-Ban Meta smart glasses actually make the future look cool  


The Verge’s Alex Heath joins the show to discuss Meta’s big move into AI with its multimodal AI smart glasses and a new AI model called Llama 3. 

Q&amp;A: Mark Zuckerberg on winning the AI race 

Meta wants to be the Microsoft of headsets

Zuckerberg says it will take Meta years to make money from generative AI


Nilay Patel answers a question from The Vergecast Hotline about Microsoft and antitrust.

Microsoft splits Teams from Office as antitrust pressure ramps up

Microsoft and OpenAI deal may face anti-trust investigations in the EU. 


Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (11 ads detected)

2
00:00:02,715 --> 00:00:06,205
Speaker 2:  Welcome To The Vergecast, the flagship podcast of dedicated

3
00:00:06,505 --> 00:00:10,365
Speaker 2:  AI hardware. I'm your friend David Pierce and I have a label

4
00:00:10,565 --> 00:00:13,525
Speaker 2:  maker. One of the things I've learned about myself is that I

5
00:00:14,205 --> 00:00:17,965
Speaker 2:  procrastinate by organizing, which mostly means I just like take a thing

6
00:00:18,345 --> 00:00:22,005
Speaker 2:  that's over here and put it sort of arbitrarily over here.

7
00:00:22,325 --> 00:00:26,205
Speaker 2:  Although it improves nothing but it like scratches my brain in a very

8
00:00:26,205 --> 00:00:29,245
Speaker 2:  helpful way that makes me feel like I'm accomplishing things even when I'm

9
00:00:29,245 --> 00:00:33,125
Speaker 2:  mostly not. But now that I have a label maker, I'm actually gonna get

10
00:00:33,125 --> 00:00:36,605
Speaker 2:  stuff done. There's the spice rack over there. I'm gonna label all the

11
00:00:36,705 --> 00:00:40,565
Speaker 2:  spices. So I stop using paprika instead of cinnamon accidentally.

12
00:00:40,845 --> 00:00:44,245
Speaker 2:  I have a bunch of tea up here. I'm gonna label all of that. So I stop drinking

13
00:00:44,245 --> 00:00:48,045
Speaker 2:  caffeinated tea at night and staying up all night. I'm gonna label the

14
00:00:48,045 --> 00:00:51,165
Speaker 2:  baby bottles over here so I know what he's supposed to be drinking out of

15
00:00:51,165 --> 00:00:53,685
Speaker 2:  and what we're not supposed to be drinking out of anymore. I'm gonna label

16
00:00:53,705 --> 00:00:57,605
Speaker 2:  the milk with the word milk, which we'll accomplish nothing but will make

17
00:00:57,605 --> 00:01:00,725
Speaker 2:  me laugh every time I pull the milk outta the fridge. I'm just gonna label

18
00:01:00,725 --> 00:01:04,005
Speaker 2:  everything and then my wife is gonna come home from work and have a lot of

19
00:01:04,005 --> 00:01:07,885
Speaker 2:  feelings about what happens. So we'll see. Anyway, we are not here to

20
00:01:07,885 --> 00:01:11,645
Speaker 2:  talk about label makers. We are here to talk tech and we have a lot to do

21
00:01:11,745 --> 00:01:15,325
Speaker 2:  on the show. Today we're gonna talk to Allison Johnson about AI

22
00:01:15,325 --> 00:01:19,285
Speaker 2:  gadgets. She's been testing some, I've been testing some and she has a

23
00:01:19,285 --> 00:01:22,765
Speaker 2:  theory that maybe none of them need to exist and that maybe a phone is the

24
00:01:22,765 --> 00:01:25,845
Speaker 2:  answer. I have some thoughts. We're gonna talk about that. And then we're

25
00:01:25,845 --> 00:01:29,565
Speaker 2:  gonna talk about meta and AI because meta has been on a huge

26
00:01:29,705 --> 00:01:33,485
Speaker 2:  run of new AI products and there's a big story in there somewhere. So

27
00:01:33,645 --> 00:01:36,925
Speaker 2:  Alex Heath is gonna come on and help us figure it out. Then we're gonna do

28
00:01:36,925 --> 00:01:40,565
Speaker 2:  the hotline. Lots of talk about this week. All that is coming up in just

29
00:01:40,605 --> 00:01:44,405
Speaker 2:  a second. But first, I'm not kidding, I have a fresh roll of label tape.

30
00:01:44,665 --> 00:01:48,325
Speaker 2:  The battery is charged. I have work to do. This is The Vergecast.

31
00:01:49,055 --> 00:01:49,565
Speaker 2:  We'll be right

32
00:02:29,575 --> 00:02:33,465
Speaker 2:  Welcome back. All right. I believe I have labeled everything in my

33
00:02:33,465 --> 00:02:37,065
Speaker 2:  house that there is to be labeled and then some. We'll see how my wife

34
00:02:37,065 --> 00:02:40,585
Speaker 2:  responds when she gets home. Let's get into it. For the last few weeks I've

35
00:02:40,585 --> 00:02:44,465
Speaker 2:  been immersing myself in AI gadgets, the humane AI pin, the

36
00:02:44,465 --> 00:02:47,825
Speaker 2:  rabbit R one, the Meta Smart Glasses, this voice recorder I have

37
00:02:48,185 --> 00:02:52,145
Speaker 2:  called plot, a bunch of apps, everything I can get my hands

38
00:02:52,165 --> 00:02:56,105
Speaker 2:  on that is like a way to use ai. And the number one

39
00:02:56,305 --> 00:03:00,205
Speaker 2:  question in front of all of these devices in particular, I think is the same

40
00:03:00,205 --> 00:03:04,005
Speaker 2:  thing. Why isn't this just an app on your phone? Actually, that might be

41
00:03:04,005 --> 00:03:07,605
Speaker 2:  the number two question. Number one is just, is this thing any good? And

42
00:03:07,685 --> 00:03:11,325
Speaker 2:  a lot of them aren't, but let's focus on the phone thing for right now. The

43
00:03:11,445 --> 00:03:14,405
Speaker 2:  VIRs, Allison Johnson wrote a piece last week arguing essentially that a

44
00:03:14,405 --> 00:03:18,365
Speaker 2:  smartphone is the perfect AI gadget, that we don't need dedicated

45
00:03:18,365 --> 00:03:22,285
Speaker 2:  devices, we have the dedicated devices. And she ran an experiment

46
00:03:22,285 --> 00:03:25,525
Speaker 2:  to prove it. So of course I had to ask her to come on and tell us how it

47
00:03:25,525 --> 00:03:27,365
Speaker 2:  went. Allison, welcome back.

48
00:03:27,575 --> 00:03:28,005
Speaker 4:  Thank you.

49
00:03:28,625 --> 00:03:32,405
Speaker 2:  We usually talk about phones when you come on the show and this time we're

50
00:03:32,405 --> 00:03:35,525
Speaker 2:  gonna talk about phones, but maybe in the weirdest way that we've ever talked

51
00:03:35,525 --> 00:03:37,285
Speaker 2:  about phones, which I'm very excited about.

52
00:03:37,745 --> 00:03:38,485
Speaker 4:  I'm here for it.

53
00:03:39,185 --> 00:03:43,125
Speaker 2:  So let's just start at the beginning here. Tell me about this wearable

54
00:03:43,125 --> 00:03:45,205
Speaker 2:  phone experiment that you devised for yourself.

55
00:03:45,545 --> 00:03:49,245
Speaker 4:  Oh my gosh, I have never been so glad that I just work in my own house

56
00:03:49,295 --> 00:03:53,245
Speaker 4:  alone. I was just sort of ranting about how, you know,

57
00:03:53,245 --> 00:03:56,965
Speaker 4:  the humane pin reviews came out and I was like, this thing is just an

58
00:03:57,035 --> 00:04:00,885
Speaker 4:  Android phone, like a mid-range Android phone. And I was like, I have

59
00:04:00,905 --> 00:04:04,085
Speaker 4:  so many mid-range Android phones. So I just kind of set out to like

60
00:04:04,485 --> 00:04:08,285
Speaker 4:  recreate the capabilities with what I had. And

61
00:04:08,705 --> 00:04:12,605
Speaker 4:  it was surprisingly hard. I tried a flip phone, I had the Motorola Razor

62
00:04:12,635 --> 00:04:16,325
Speaker 4:  Plus, and I had it like kind of clamped over the collar of my shirt,

63
00:04:16,585 --> 00:04:20,285
Speaker 4:  you know, like you do. And I got like so far with that,

64
00:04:20,395 --> 00:04:23,165
Speaker 4:  like I could kind of talk to Google assistant.

65
00:04:23,425 --> 00:04:26,885
Speaker 2:  So wait, so you, you dangle it over with like what The front screen facing

66
00:04:27,065 --> 00:04:30,885
Speaker 2:  out? Yeah. And you just, and then you just sort of like, Hey g it

67
00:04:31,115 --> 00:04:32,405
Speaker 2:  from Yeah, there.

68
00:04:32,785 --> 00:04:36,605
Speaker 4:  And I figure, 'cause then you have cameras facing forward. You could do Google

69
00:04:36,755 --> 00:04:40,605
Speaker 4:  lens. That was the, the thought. It's not very easy to

70
00:04:40,605 --> 00:04:44,325
Speaker 4:  do any of that kind of out of the box. Like the flip phones

71
00:04:44,375 --> 00:04:48,325
Speaker 4:  won't let you download the Gemini assistant, which is strange. So you kind

72
00:04:48,325 --> 00:04:51,965
Speaker 4:  of have to, that's weird. Like side load it, if you try and and

73
00:04:51,965 --> 00:04:55,805
Speaker 4:  trigger the Google assistant with the phone closed and

74
00:04:55,805 --> 00:04:58,645
Speaker 4:  the cover, just using the cover screen, it tells you to open up the phone.

75
00:04:58,905 --> 00:05:02,365
Speaker 4:  So it kind of like, I was already not off to a great start.

76
00:05:03,305 --> 00:05:07,125
Speaker 2:  So did you ever find an actual solution

77
00:05:07,395 --> 00:05:11,365
Speaker 2:  with the phone itself? We're gonna get to headphones in a minute. Yeah. Foreshadowing.

78
00:05:11,785 --> 00:05:15,445
Speaker 2:  But did you ever find a version of like a wearable phone that actually kind

79
00:05:15,445 --> 00:05:15,725
Speaker 2:  of worked?

80
00:05:16,285 --> 00:05:20,205
Speaker 4:  I did not really. Okay. I had, I just ran chat GPT

81
00:05:20,665 --> 00:05:24,565
Speaker 4:  in the kind of like talking mode where the conversation mode

82
00:05:25,185 --> 00:05:28,885
Speaker 4:  and I had that on my shirt and just, but it's just always listening and like

83
00:05:28,885 --> 00:05:31,925
Speaker 4:  waiting for you to say something so you can't just like go about your day

84
00:05:31,945 --> 00:05:35,645
Speaker 4:  and Right. We had a very pleasant conversation about like the weather,

85
00:05:36,205 --> 00:05:40,045
Speaker 4:  whatever. But yeah, nothing, nothing really worked the way I thought it would.

86
00:05:40,435 --> 00:05:44,085
Speaker 2:  Well this is why I wanted to talk about this because I think you and I have

87
00:05:44,085 --> 00:05:47,125
Speaker 2:  been talking about this for months now and the question everybody brings

88
00:05:47,125 --> 00:05:50,965
Speaker 2:  up about every single AI gadget is like, why isn't this just an app

89
00:05:50,985 --> 00:05:54,885
Speaker 2:  on my phone? And my thoughts on the subject and

90
00:05:55,185 --> 00:05:58,365
Speaker 2:  why it shouldn't just be an app on your phone are actually becoming increasingly

91
00:05:58,665 --> 00:06:02,565
Speaker 2:  strong and intense. And I feel like I'm more right than I was before. But

92
00:06:02,565 --> 00:06:06,245
Speaker 2:  I am curious like at a, at a broad level, do you buy that

93
00:06:06,645 --> 00:06:09,245
Speaker 2:  argument? Like are you also one of the people who is just like, this is all

94
00:06:09,245 --> 00:06:11,885
Speaker 2:  ridiculous, this should just be an app on my phone. I think

95
00:06:11,985 --> 00:06:15,885
Speaker 4:  I'm, I'm like 20% rooting for like a

96
00:06:15,945 --> 00:06:19,885
Speaker 4:  gadget that does these things and 80% just, it just feels like

97
00:06:19,955 --> 00:06:23,725
Speaker 4:  realism. Like phones have already solved

98
00:06:23,905 --> 00:06:27,685
Speaker 4:  so many of these problems. Like these little gadgets get

99
00:06:28,105 --> 00:06:31,645
Speaker 4:  hot, they have to connect to the internet, they

100
00:06:32,155 --> 00:06:36,045
Speaker 4:  need cameras like check mark on all

101
00:06:36,045 --> 00:06:39,925
Speaker 4:  of those for phones, right? Like they did it battery life, you know,

102
00:06:40,235 --> 00:06:43,805
Speaker 4:  regular software updates. They got it. And I'm just like, I'm having

103
00:06:43,935 --> 00:06:47,845
Speaker 4:  flashbacks of covering compact digital cameras for

104
00:06:47,845 --> 00:06:51,685
Speaker 4:  years where it was like, you know, oh these are

105
00:06:51,705 --> 00:06:55,445
Speaker 4:  so much better than your phone. And which is like not even true

106
00:06:55,445 --> 00:06:59,245
Speaker 4:  anymore. But it was like, no, the phone is good enough for

107
00:06:59,245 --> 00:07:02,725
Speaker 4:  most people and people do not wanna carry around two things.

108
00:07:02,955 --> 00:07:06,765
Speaker 4:  That is, that is what we learned from that experiment is if you can carry

109
00:07:07,105 --> 00:07:09,885
Speaker 4:  one thing instead of two things, you will carry one thing.

110
00:07:10,525 --> 00:07:13,525
Speaker 2:  I do agree with that. And that that is a, I think maybe the most

111
00:07:14,055 --> 00:07:17,685
Speaker 2:  convincing argument against the AI gadget is just,

112
00:07:18,035 --> 00:07:21,605
Speaker 2:  it's, it's the good enough hypothesis, right? That it's like I don't need

113
00:07:21,605 --> 00:07:25,245
Speaker 2:  it to be as good. I just don't wanna carry and charge and worry about two

114
00:07:25,245 --> 00:07:29,085
Speaker 2:  things. But I think my most strident opinion on this subject

115
00:07:29,225 --> 00:07:33,125
Speaker 2:  is that I think we all have forgotten how annoying it is to

116
00:07:33,145 --> 00:07:37,005
Speaker 2:  use your phone. Hmm. Like we've just been doing it for so long that

117
00:07:37,045 --> 00:07:40,645
Speaker 2:  I think it's why I believe in flip phones for the same reason. It's like

118
00:07:40,645 --> 00:07:44,605
Speaker 2:  our phones are these big chunky things that you balance on top of

119
00:07:44,605 --> 00:07:47,805
Speaker 2:  your pinky to the point where most of us have like divots in our fingers

120
00:07:48,505 --> 00:07:52,245
Speaker 2:  now from trying to hold these things. You have to sort of like worm your

121
00:07:52,245 --> 00:07:56,005
Speaker 2:  thumb around this gigantic screen in order to touch anything. Like

122
00:07:56,395 --> 00:08:00,245
Speaker 2:  from a pure, in a vacuum user experience, it's not good.

123
00:08:00,605 --> 00:08:04,485
Speaker 2:  And if your hands are full, you can't do anything. And I spend like an

124
00:08:04,485 --> 00:08:08,045
Speaker 2:  alarming amount of my time with like dog and stroller

125
00:08:08,345 --> 00:08:11,805
Speaker 2:  in one arm and phone in the other. Like it's just bad. And I

126
00:08:12,075 --> 00:08:15,765
Speaker 2:  this idea that, okay, every time I wanna do anything, the right user experience

127
00:08:15,765 --> 00:08:19,605
Speaker 2:  is for me to dig into my purse, pull out my phone, unlock

128
00:08:19,605 --> 00:08:23,325
Speaker 2:  it, swipe, swipe pipe, open an app, tap on the app, log in,

129
00:08:23,385 --> 00:08:26,925
Speaker 2:  do a thing. Like that's bad. And the idea that you can

130
00:08:27,165 --> 00:08:31,045
Speaker 2:  shortcut all of this by just yelling a thing is good. I believe in that.

131
00:08:31,045 --> 00:08:33,765
Speaker 2:  Yeah. And this is why I like your experiment so much because it's like, okay,

132
00:08:33,785 --> 00:08:37,565
Speaker 2:  can I make my phone that thing? which I think is where we get to

133
00:08:37,595 --> 00:08:40,725
Speaker 2:  earbuds. Yeah. The longer we do this, the more I'm coming to the answer of

134
00:08:40,725 --> 00:08:44,245
Speaker 2:  like, maybe headphones are the problem solver in all of this.

135
00:08:44,715 --> 00:08:48,525
Speaker 4:  Yeah. And this is something that our colleague V song has

136
00:08:48,525 --> 00:08:52,485
Speaker 4:  written about is like with wearable tech, it's so

137
00:08:52,485 --> 00:08:56,245
Speaker 4:  important that it's just like something you want to put

138
00:08:56,265 --> 00:08:59,925
Speaker 4:  on your body and is comfortable and is kind of like socially accepted.

139
00:09:00,665 --> 00:09:04,485
Speaker 4:  And I just come back to earbuds like we're just, people just

140
00:09:04,485 --> 00:09:08,325
Speaker 4:  walk around with them. I feel like such an old person being like these kids,

141
00:09:08,435 --> 00:09:12,005
Speaker 4:  they just walk around with their earbuds all the time. But like you have

142
00:09:12,005 --> 00:09:15,765
Speaker 4:  a good transparency mode, you know? And those things could get

143
00:09:15,765 --> 00:09:19,525
Speaker 4:  smarter about the sounds they let in and the sounds they keep

144
00:09:19,525 --> 00:09:22,965
Speaker 4:  about. And then you have this connection to your phone that you can just

145
00:09:22,965 --> 00:09:26,765
Speaker 4:  like summon it when you want it and keep it in your pocket and

146
00:09:26,765 --> 00:09:30,485
Speaker 4:  not have to do the whole dance of like tapping on things. 'cause I think

147
00:09:30,485 --> 00:09:33,965
Speaker 4:  we've come, we're sort of blind to that. Like we are just so

148
00:09:33,965 --> 00:09:37,245
Speaker 4:  conditioned to like, this is how you get things done on a phone. You take

149
00:09:37,245 --> 00:09:41,045
Speaker 4:  it out, you do all this, you get distracted. And yeah. Once you start kind

150
00:09:41,045 --> 00:09:44,085
Speaker 4:  of like conceptualizing not having to do that,

151
00:09:44,755 --> 00:09:48,485
Speaker 4:  it's like, oh yeah, I, I buy into that vision and I, but I

152
00:09:48,485 --> 00:09:52,325
Speaker 4:  think we're just gonna be able to do it without touching our

153
00:09:52,325 --> 00:09:53,725
Speaker 4:  phone. Right. But it's still gonna be phone.

154
00:09:54,465 --> 00:09:57,045
Speaker 2:  Yes. And well, and that turned out to be sort of the second half of your

155
00:09:57,045 --> 00:10:01,005
Speaker 2:  experience with, with this phone AI gadget thing,

156
00:10:01,005 --> 00:10:03,445
Speaker 2:  right? You you basically went like full pixel.

157
00:10:03,795 --> 00:10:07,765
Speaker 4:  Yeah, I did. Like I downloaded the Gemini assistant and

158
00:10:07,965 --> 00:10:11,245
Speaker 4:  switched into it and there's still like weird workarounds you have to do

159
00:10:11,245 --> 00:10:14,965
Speaker 4:  because you can have Gemini assistant running on the phone. The

160
00:10:15,155 --> 00:10:18,845
Speaker 4:  earbuds are not going to trigger Gemini 'cause

161
00:10:18,845 --> 00:10:20,285
Speaker 4:  they're, they don't talk yet.

162
00:10:20,395 --> 00:10:23,045
Speaker 2:  This is the most Google thing you've ever said. Yeah,

163
00:10:23,195 --> 00:10:27,165
Speaker 4:  Seriously. Well like the pixel watch you, you can't have Gemini on

164
00:10:27,165 --> 00:10:30,365
Speaker 4:  it yet. So you're like talking to one assistant here, you're talking to a

165
00:10:30,365 --> 00:10:33,485
Speaker 4:  different assistant on the phone. The Google home in my kitchen is constantly

166
00:10:33,485 --> 00:10:37,285
Speaker 4:  being like, what? So I had to like unplug that

167
00:10:37,625 --> 00:10:41,525
Speaker 4:  and I kept Gemini like open and running on the phone, which is not an

168
00:10:41,525 --> 00:10:45,285
Speaker 4:  ideal user experience, but had the earbuds in and my like

169
00:10:45,495 --> 00:10:49,045
Speaker 4:  light bulb moment was, I took a picture of this recipe

170
00:10:49,565 --> 00:10:53,525
Speaker 4:  I was making and I was like, look at this and remember it. And

171
00:10:53,565 --> 00:10:56,805
Speaker 4:  I just walked around my kitchen doing things and I would ask it questions

172
00:10:56,805 --> 00:11:00,645
Speaker 4:  like how long do I put the fish in? How do I chop the vegetables? Just kind

173
00:11:00,645 --> 00:11:04,325
Speaker 4:  of out of order. And like as I was doing things and it,

174
00:11:04,625 --> 00:11:08,445
Speaker 4:  it got it right all the time and it was like, oh this is super helpful.

175
00:11:08,845 --> 00:11:12,685
Speaker 4:  I would be running back to this recipe with like stuff

176
00:11:12,745 --> 00:11:16,325
Speaker 4:  on my fingers trying to figure it out. And it's like man, those

177
00:11:16,395 --> 00:11:16,885
Speaker 4:  earbuds,

178
00:11:17,355 --> 00:11:21,245
Speaker 2:  That is maybe the single coolest use of AI I've ever heard by

179
00:11:21,245 --> 00:11:24,965
Speaker 2:  the way, to take a picture of a recipe and then just pepper

180
00:11:25,305 --> 00:11:28,845
Speaker 2:  the assistant with questions Yeah. About that recipe as you go. That's very

181
00:11:28,845 --> 00:11:29,085
Speaker 2:  cool.

182
00:11:29,685 --> 00:11:33,285
Speaker 4:  I was super into it and it's that kind of thing of like I run outta things

183
00:11:33,305 --> 00:11:37,125
Speaker 4:  to talk about with chat JBT I'm like, there's only so many like business

184
00:11:37,405 --> 00:11:41,245
Speaker 4:  proposals you can brainstorm. But I'm like, if you give it a little data

185
00:11:41,305 --> 00:11:45,005
Speaker 4:  set of like I'm doing this thing help me out.

186
00:11:45,355 --> 00:11:48,285
Speaker 4:  Like it makes so much more sense to me that way. Yeah.

187
00:11:48,505 --> 00:11:51,485
Speaker 2:  Are you a voice assistant person in your day to day?

188
00:11:51,985 --> 00:11:55,885
Speaker 4:  Not at all. I set timers and I asked the weather and like trying

189
00:11:55,905 --> 00:11:59,805
Speaker 4:  to get myself, I'm starting to like realize I can use

190
00:11:59,805 --> 00:12:03,565
Speaker 4:  it a little more. Especially if I am using a phone with

191
00:12:03,665 --> 00:12:07,645
Speaker 4:  Gemini on it. Hmm. Like I asked but you just, man you gotta keep an

192
00:12:07,645 --> 00:12:11,245
Speaker 4:  eye on it 'cause it'll make up a word. Or like I was asking

193
00:12:11,465 --> 00:12:14,885
Speaker 4:  it if the bridge they were working on in my neighborhood had reopened

194
00:12:15,465 --> 00:12:19,205
Speaker 4:  and, and it like got the days backwards. It was like, yes

195
00:12:19,275 --> 00:12:21,885
Speaker 4:  it's open but it's opening tomorrow so

196
00:12:23,185 --> 00:12:23,405
Speaker 2:  You

197
00:12:23,405 --> 00:12:27,005
Speaker 4:  Just have to keep an eye on it. But it's, it's like converting me a little

198
00:12:27,025 --> 00:12:29,845
Speaker 4:  bit to more of a voice assistant person I think. Okay.

199
00:12:30,355 --> 00:12:34,085
Speaker 2:  Yeah. I think one of the big questions I have about this space right now

200
00:12:34,225 --> 00:12:37,525
Speaker 2:  is did voice assistance work because they were a bad idea?

201
00:12:38,105 --> 00:12:41,845
Speaker 2:  Or rather did voice assistance not work because they were a bad idea? Or

202
00:12:41,845 --> 00:12:45,245
Speaker 2:  did they not work because they weren't very good? And I think they definitely

203
00:12:45,245 --> 00:12:48,245
Speaker 2:  aren't very good. Right. And I think that the Siri experience

204
00:12:48,955 --> 00:12:51,645
Speaker 2:  sort of proves that yes, for a lot of people and I think a lot of people's

205
00:12:51,645 --> 00:12:54,525
Speaker 2:  first experience with these things was Siri. And you ask it to do something

206
00:12:54,525 --> 00:12:56,685
Speaker 2:  and it just fails or it's like, here's what I found on the web. And you're

207
00:12:56,685 --> 00:13:00,165
Speaker 2:  like, well this is useless and then you never really try again. And so the

208
00:13:00,165 --> 00:13:04,125
Speaker 2:  first attempt, we all had to build that habit, it failed

209
00:13:04,315 --> 00:13:07,885
Speaker 2:  spectacularly and there are a lot of like kids

210
00:13:08,505 --> 00:13:12,125
Speaker 2:  who grew up asking Alexa silly questions and getting silly responses and

211
00:13:12,125 --> 00:13:14,325
Speaker 2:  that was the thing that worked. So I think the attach rate there has actually

212
00:13:14,325 --> 00:13:18,125
Speaker 2:  been better as a result. But then I come around to, okay,

213
00:13:18,425 --> 00:13:22,245
Speaker 2:  we all bet on this a decade ago. Like the tech industry was like,

214
00:13:22,245 --> 00:13:25,885
Speaker 2:  this is the thing, conversational interfaces, they're gonna change everything.

215
00:13:26,285 --> 00:13:29,965
Speaker 2:  And they super didn't. And now to some extent we're back in that exact

216
00:13:29,995 --> 00:13:33,685
Speaker 2:  same thing and the promise is no, it was the right

217
00:13:33,755 --> 00:13:37,565
Speaker 2:  idea with the wrong underlying technology. And at least

218
00:13:37,825 --> 00:13:41,165
Speaker 2:  my experience so far has been, I still see the theory of it,

219
00:13:42,145 --> 00:13:45,765
Speaker 2:  but even as I use all these things, I'm not, I'm using it more.

220
00:13:46,445 --> 00:13:50,005
Speaker 2:  I, I agree with you. I've definitely found more reasons to do this stuff.

221
00:13:50,005 --> 00:13:53,765
Speaker 2:  Particularly with like I really like to use an AI voice notes

222
00:13:53,785 --> 00:13:56,765
Speaker 2:  app to like say my to-do list out loud. And they're actually getting pretty

223
00:13:56,765 --> 00:14:00,285
Speaker 2:  good at turning that into just a structured list of stuff I have to do today.

224
00:14:00,445 --> 00:14:04,365
Speaker 2:  I feel like a doctor who's like saying all of my charts into my voice

225
00:14:04,605 --> 00:14:07,765
Speaker 2:  recorder and then like something magical happens, it's great, but I still

226
00:14:07,765 --> 00:14:11,645
Speaker 2:  don't feel like I have hit the thing where it just sort of, that becomes

227
00:14:11,645 --> 00:14:15,525
Speaker 2:  the main way I use my phone and what people say when I say that to

228
00:14:15,525 --> 00:14:18,405
Speaker 2:  them is, oh it's just an interface question. It's because once you have your

229
00:14:18,405 --> 00:14:20,565
Speaker 2:  phone out, why not just use your phone the way that you're used to? Which

230
00:14:20,565 --> 00:14:23,445
Speaker 2:  is why they're all excited about these new gadgets. And so I just, I don't

231
00:14:23,445 --> 00:14:27,245
Speaker 2:  know, I spin in circles about like what is the actual sort of roadblock

232
00:14:27,245 --> 00:14:30,725
Speaker 2:  here to this actually becoming real? 'cause it seems like there are a lot

233
00:14:30,725 --> 00:14:31,005
Speaker 2:  of them.

234
00:14:31,435 --> 00:14:35,205
Speaker 4:  Yeah. There's just sort of like a pain versus reward,

235
00:14:35,585 --> 00:14:39,485
Speaker 4:  you know, ratio of like, it, I feel like it's starting to even

236
00:14:39,585 --> 00:14:43,165
Speaker 4:  out a little bit with the voice assistants, but you still get burned

237
00:14:43,375 --> 00:14:46,525
Speaker 4:  where you're like, yeah there's nothing worse than like being at the grocery

238
00:14:46,575 --> 00:14:50,365
Speaker 4:  store and being like, I'm gonna use the voice assistant. And then it's, it

239
00:14:50,365 --> 00:14:54,285
Speaker 4:  just fails and you have to like either keep talking to it or like

240
00:14:54,285 --> 00:14:58,045
Speaker 4:  take your phone out. I think once you had that experience you're like, oh

241
00:14:58,115 --> 00:15:01,885
Speaker 4:  just use my phone for this. Yeah. But yeah, I think, I think it's something

242
00:15:01,905 --> 00:15:05,805
Speaker 4:  we can adjust to and it, it really has to like that

243
00:15:06,285 --> 00:15:09,565
Speaker 4:  threshold of public embarrassment or just like

244
00:15:09,925 --> 00:15:13,885
Speaker 4:  frustration has to change and there's like signals that

245
00:15:13,885 --> 00:15:17,525
Speaker 4:  that will happen I think. But yeah, it's, it's not proven.

246
00:15:17,765 --> 00:15:18,365
Speaker 4:  I wouldn't say.

247
00:15:18,675 --> 00:15:22,325
Speaker 2:  Yeah. One of my developing theories is that

248
00:15:22,325 --> 00:15:26,285
Speaker 2:  headphones are gonna be where a lot of this goes for that reason. It's

249
00:15:26,285 --> 00:15:30,205
Speaker 2:  like, it's a thing you can wear one of in the grocery store and

250
00:15:30,465 --> 00:15:34,245
Speaker 2:  no one will look twice at you. Like we have solved the public weirdness

251
00:15:34,245 --> 00:15:38,005
Speaker 2:  problem of headphones. Sony did that for us 45 years ago. Yeah,

252
00:15:38,325 --> 00:15:41,925
Speaker 2:  thank you Sony. But what I wonder now is like

253
00:15:42,225 --> 00:15:46,165
Speaker 2:  should you have the AI system that is just purely baked into

254
00:15:46,165 --> 00:15:48,525
Speaker 2:  your headphones? Like one of, one of the conspiracy theories that I have

255
00:15:48,525 --> 00:15:52,445
Speaker 2:  no evidence for but believe to my soul is that the next AirPods

256
00:15:52,535 --> 00:15:56,205
Speaker 2:  maxes are going to be local AI devices because they're big

257
00:15:56,625 --> 00:15:59,685
Speaker 2:  and they're heavy and they have a big battery and they can do stuff. But

258
00:15:59,685 --> 00:16:03,285
Speaker 2:  like if you try to run some of that stuff on AirPods or the Pixel

259
00:16:03,515 --> 00:16:07,085
Speaker 2:  buds or it's just the battery's gonna die in one minute and there's just,

260
00:16:07,085 --> 00:16:10,765
Speaker 2:  there's literally no place to put d AI stuff. Yeah.

261
00:16:11,185 --> 00:16:14,885
Speaker 2:  But maybe it's, maybe that's okay. Maybe Bluetooth headphones and your phone

262
00:16:14,885 --> 00:16:18,245
Speaker 2:  in your pocket or purse is enough interface. And it does seem like in your

263
00:16:18,245 --> 00:16:19,365
Speaker 2:  experiment that's kind of where you landed.

264
00:16:19,715 --> 00:16:23,045
Speaker 4:  Yeah. That, that kind of proved out in, that didn't solve

265
00:16:23,705 --> 00:16:27,685
Speaker 4:  the camera thing, which is also like an intriguing related,

266
00:16:28,305 --> 00:16:32,245
Speaker 4:  you know, like feature which the humane pin has the camera

267
00:16:32,665 --> 00:16:36,485
Speaker 4:  and you we're both like, wanna look at something and ask a

268
00:16:36,605 --> 00:16:39,445
Speaker 4:  question about it. Or if you want a hands-free way to take pictures, which

269
00:16:39,475 --> 00:16:43,285
Speaker 4:  both are kind of intriguing to me. So I mean everybody keeps

270
00:16:43,485 --> 00:16:47,405
Speaker 4:  bringing up the meta ray bands and man that kind of solves

271
00:16:47,565 --> 00:16:50,005
Speaker 4:  a couple of those problems and doesn't look stupid.

272
00:16:50,275 --> 00:16:51,725
Speaker 2:  True. Do you wear glasses normally?

273
00:16:52,125 --> 00:16:56,045
Speaker 4:  I wear contacts. Okay. So I'm, I'm so close to buying

274
00:16:56,045 --> 00:16:59,845
Speaker 4:  these things. I don't know really. Yeah. I, it'd be so good for

275
00:16:59,845 --> 00:17:03,805
Speaker 4:  just like taking picture of a kid running around and I

276
00:17:03,805 --> 00:17:06,805
Speaker 4:  don't know, but another gadget, do I want another gadget?

277
00:17:07,525 --> 00:17:10,965
Speaker 2:  I dunno. I I hear you. I've been, I've been debating this with some of the

278
00:17:10,965 --> 00:17:14,685
Speaker 2:  folks on our team for a while 'cause I think they're definitely a huge

279
00:17:14,755 --> 00:17:18,605
Speaker 2:  leap better than like having a headset on your face to

280
00:17:18,605 --> 00:17:22,205
Speaker 2:  have the, the metas smartt glasses and meta is leaning into the fact that

281
00:17:22,205 --> 00:17:25,925
Speaker 2:  we've also solved, having glasses on your face is not weird. And I don't

282
00:17:25,925 --> 00:17:28,245
Speaker 2:  know what your experience has been, but when I wear the meta glasses, no

283
00:17:28,245 --> 00:17:31,965
Speaker 2:  one ever looks at me sideways. Yeah. Like I got more public

284
00:17:32,385 --> 00:17:36,205
Speaker 2:  notice, more sort of weird double takes wearing the humane pin

285
00:17:36,395 --> 00:17:40,165
Speaker 2:  than I ever have wearing the meta glasses by a mile. Yeah. So that's,

286
00:17:40,165 --> 00:17:44,045
Speaker 2:  that's a huge victory. But I also don't wear glasses normally and I don't

287
00:17:44,045 --> 00:17:47,605
Speaker 2:  wanna start wearing glasses. Like I wear sunglasses when I'm

288
00:17:47,675 --> 00:17:51,005
Speaker 2:  outside and I have mostly replaced my sunglasses with the meta glasses, which

289
00:17:51,005 --> 00:17:54,685
Speaker 2:  I like a lot. But in terms of a sort of always on assistant like I'm not

290
00:17:54,685 --> 00:17:57,885
Speaker 2:  gonna get to work and keep wearing glasses I don't need

291
00:17:58,395 --> 00:17:59,965
Speaker 2:  just to use my assistant.

292
00:18:00,155 --> 00:18:02,645
Speaker 4:  It's a different vibe. Yeah. You have to be ready for that shift.

293
00:18:03,045 --> 00:18:07,005
Speaker 2:  Yeah. Like I are, if we can normalize sunglasses indoors just in

294
00:18:07,005 --> 00:18:10,165
Speaker 2:  order to make AI happen Yeah, I'm here for it. This, this works.

295
00:18:10,295 --> 00:18:11,005
Speaker 4:  We'll work on that.

296
00:18:11,185 --> 00:18:14,485
Speaker 2:  But yeah, I do think I have definitely come around to the idea in testing

297
00:18:14,985 --> 00:18:18,285
Speaker 2:  the humane pin and now the rabbit and some of the other stuff that's out

298
00:18:18,285 --> 00:18:21,285
Speaker 2:  there now that actually what we need to figure out is how to put AI into

299
00:18:21,285 --> 00:18:25,205
Speaker 2:  other socially acceptable things that already exist before we try and

300
00:18:25,205 --> 00:18:27,965
Speaker 2:  invent some who new thing.

301
00:18:28,235 --> 00:18:32,125
Speaker 4:  Yeah. Like the humane pin kind of tries to hide it, but

302
00:18:32,125 --> 00:18:35,925
Speaker 4:  it's sort of like, look at this, this is kind of a cool gadget. I appreciate

303
00:18:35,925 --> 00:18:39,565
Speaker 4:  that. The rabbit is just like, this is a thing. It is bright orange.

304
00:18:40,065 --> 00:18:43,565
Speaker 4:  It like practically glows in the dark. I don't know if you had this

305
00:18:43,565 --> 00:18:46,965
Speaker 4:  experience where I walk into a dark room and I'm like,

306
00:18:47,115 --> 00:18:48,205
Speaker 4:  there's the rabbit art.

307
00:18:49,745 --> 00:18:53,485
Speaker 2:  It really does it, it sort of screams at you from wherever it is. What do

308
00:18:53,485 --> 00:18:55,645
Speaker 2:  you think about the rabbit, speaking of experiments you've been doing, you've

309
00:18:55,645 --> 00:18:58,805
Speaker 2:  been, you've been testing rabbit's, AI stuff with some of the other stuff

310
00:18:58,805 --> 00:19:00,445
Speaker 2:  that's out there. How's that gone? It

311
00:19:00,545 --> 00:19:01,845
Speaker 4:  It, boy

312
00:19:04,305 --> 00:19:08,205
Speaker 4:  that's a tough, that sounds right. It's a tough one. The, the battery

313
00:19:08,315 --> 00:19:12,205
Speaker 4:  life is just god awful. Which you know, as it turns

314
00:19:12,205 --> 00:19:15,965
Speaker 4:  out is important. 'cause I don't, if I just have it in my bag and I go

315
00:19:15,965 --> 00:19:19,845
Speaker 4:  somewhere, I'm like, I'm gonna leave it off because the minute I turn it

316
00:19:19,845 --> 00:19:23,645
Speaker 4:  on it's gonna be like draining the battery. So like

317
00:19:23,885 --> 00:19:27,085
Speaker 4:  a, if there's no battery it doesn't help you do anything. It's just a cool

318
00:19:27,105 --> 00:19:30,965
Speaker 4:  orange thing. I've seen a couple of moments when

319
00:19:30,985 --> 00:19:34,965
Speaker 4:  I'm like, okay, this is maybe doing a little more than a Gemini. I

320
00:19:34,965 --> 00:19:38,525
Speaker 4:  took a picture of my plant on my desk and it, it did the typical thing of

321
00:19:38,525 --> 00:19:42,285
Speaker 4:  like, this is a POTOs plant, blah blah blah. And it, it kind of

322
00:19:42,285 --> 00:19:46,125
Speaker 4:  like went into the care instructions and it was like, your plant looks

323
00:19:46,125 --> 00:19:49,765
Speaker 4:  happy in its current pot. And I'm like, you know, great job. I don't, I don't

324
00:19:49,765 --> 00:19:53,605
Speaker 4:  get that assessment from Gemini. It just sort of spits out a bunch of stuff

325
00:19:53,605 --> 00:19:57,525
Speaker 4:  that you like, don't overwater it, don't underwater it. So I don't know there's,

326
00:19:57,525 --> 00:20:01,285
Speaker 4:  there's like something that is a little more like

327
00:20:01,575 --> 00:20:05,445
Speaker 4:  could tune itself to like what you're actually doing or looking at

328
00:20:05,515 --> 00:20:09,485
Speaker 4:  that feels like not just googling something. Hmm. But I don't

329
00:20:09,485 --> 00:20:13,245
Speaker 4:  wanna give this thing too much credit. It is not done that

330
00:20:13,245 --> 00:20:16,845
Speaker 4:  very often and it, it kind of feels that basic stuff.

331
00:20:17,195 --> 00:20:20,765
Speaker 2:  Yeah. It's bad. Like I think yeah, we're gonna have, we're gonna have more

332
00:20:20,765 --> 00:20:24,285
Speaker 2:  to say about it both on the site and on this podcast, but the, the, the rabbit

333
00:20:24,405 --> 00:20:27,805
Speaker 2:  R one is not good is is a pretty easy takeaway. But I'm curious, you've been

334
00:20:27,805 --> 00:20:31,005
Speaker 2:  testing it against Chad GPT in particular pretty

335
00:20:31,395 --> 00:20:35,245
Speaker 2:  strenuously, right? Are they, my assumption would be that most of these things

336
00:20:35,525 --> 00:20:39,205
Speaker 2:  should come out relatively the same because it's all kind of pulling from

337
00:20:39,205 --> 00:20:43,045
Speaker 2:  the same backend infrastructure. Have you noticed anything interesting poking

338
00:20:43,045 --> 00:20:43,565
Speaker 2:  at all this stuff?

339
00:20:43,955 --> 00:20:47,565
Speaker 4:  Well I think that something you mentioned I think is that it's running

340
00:20:47,855 --> 00:20:51,605
Speaker 4:  perplexity so you get a little more like real

341
00:20:51,635 --> 00:20:55,445
Speaker 4:  time info than a chat GPT that or the

342
00:20:55,445 --> 00:20:59,325
Speaker 4:  free chat GPTI used that was trained whenever ago. Right.

343
00:20:59,585 --> 00:21:03,565
Speaker 4:  So I can ask it like, is that bridge open that was closed over

344
00:21:03,565 --> 00:21:06,925
Speaker 4:  the weekend and I get a real response. So

345
00:21:07,425 --> 00:21:11,365
Speaker 4:  that's interesting. But it's, it's such a strange little

346
00:21:12,515 --> 00:21:13,005
Speaker 2:  Strange

347
00:21:13,005 --> 00:21:16,885
Speaker 4:  Little creature. It's like very adorable. I think there's an

348
00:21:17,085 --> 00:21:21,045
Speaker 4:  adorable factor. It perks up its little ears when it's listening

349
00:21:21,045 --> 00:21:23,245
Speaker 4:  to you. But TBD on that,

350
00:21:23,635 --> 00:21:26,805
Speaker 2:  Does that do anything for you actually? 'cause I think I, I have, I've definitely

351
00:21:26,805 --> 00:21:30,085
Speaker 2:  had the same experience that I like that it, the little rabbit on the screen

352
00:21:30,085 --> 00:21:33,805
Speaker 2:  sort of bounces waiting for you to talk and then you ask it a question and

353
00:21:33,805 --> 00:21:36,445
Speaker 2:  it perks up its ears and stops bouncing. And when you're playing music it,

354
00:21:36,465 --> 00:21:40,125
Speaker 2:  it wears headphones and there's a lot of sort of charming little things about

355
00:21:40,125 --> 00:21:44,005
Speaker 2:  it I think. And I can't decide if that's what I

356
00:21:44,005 --> 00:21:47,845
Speaker 2:  want or is totally the opposite of what I want from a device like this because

357
00:21:47,845 --> 00:21:51,685
Speaker 2:  you, you use Gemini or chat GPT or Siri or

358
00:21:51,885 --> 00:21:55,565
Speaker 2:  whatever and it's all very matter of fact, right? Like they are, they are

359
00:21:55,615 --> 00:21:59,325
Speaker 2:  tools to do a job and actually every time we've seen these things exhibit

360
00:21:59,325 --> 00:22:03,285
Speaker 2:  personality, it's been in really bad ugly problematic ways mostly.

361
00:22:03,825 --> 00:22:07,205
Speaker 2:  And so they've all learned to just sort of shut all that down and just be

362
00:22:07,205 --> 00:22:10,965
Speaker 2:  there to execute whatever they're required to execute and then move on. And

363
00:22:10,965 --> 00:22:14,165
Speaker 2:  part of me is sort of endeared to rabbit for trying something else and part

364
00:22:14,165 --> 00:22:17,925
Speaker 2:  of me is also like, well maybe if it worked but was less cute, I would like

365
00:22:17,925 --> 00:22:20,565
Speaker 2:  it better. Yeah. And maybe that's what actually matters here.

366
00:22:20,835 --> 00:22:24,725
Speaker 4:  Yeah. Like there's a cute little animation of the rabbit

367
00:22:24,945 --> 00:22:28,605
Speaker 4:  in a hamster wheel or something when you, when you need to charge it.

368
00:22:28,945 --> 00:22:32,765
Speaker 4:  I'm like, this is adorable, but I am annoyed that I need to charge

369
00:22:32,795 --> 00:22:36,245
Speaker 4:  this thing again. Like cute only gets you so far I think.

370
00:22:36,795 --> 00:22:40,685
Speaker 2:  Yeah. Do you think there's gonna be ways to put more of this

371
00:22:40,685 --> 00:22:44,165
Speaker 2:  stuff onto phones in interesting ways? Like is is the next round of flip

372
00:22:44,165 --> 00:22:46,925
Speaker 2:  phones and foldable phones and smartphone gonna have some of the stuff we're

373
00:22:46,925 --> 00:22:50,725
Speaker 2:  seeing in these gadgets? Or are we just gonna keep getting phones because

374
00:22:50,725 --> 00:22:53,325
Speaker 2:  like I I just keep thinking about the razor as you're talking about it and

375
00:22:53,325 --> 00:22:56,565
Speaker 2:  it's like what if the external screen of the razor could run an assistant

376
00:22:57,025 --> 00:23:00,645
Speaker 2:  and could have that outward facing camera. Like it kind of has all the hardware

377
00:23:00,645 --> 00:23:04,005
Speaker 2:  it needs to do that. Are we gonna get that do

378
00:23:04,005 --> 00:23:07,965
Speaker 4:  You think? I I would like it. I mean it would be an ideal

379
00:23:08,055 --> 00:23:12,005
Speaker 4:  world where you could kind of choose from your AI assistant and you

380
00:23:12,005 --> 00:23:15,765
Speaker 4:  don't just have to use the one that you're operating system baked in fair.

381
00:23:15,985 --> 00:23:19,885
Speaker 4:  But yeah, then you get into like the tech company's gonna

382
00:23:19,945 --> 00:23:23,925
Speaker 4:  let the app makers in, you know like get into

383
00:23:23,925 --> 00:23:27,805
Speaker 4:  the system the way it would have to where you don't run into a wall like

384
00:23:27,995 --> 00:23:31,765
Speaker 4:  chat GPT can't change settings on my phone or like

385
00:23:31,905 --> 00:23:35,525
Speaker 4:  put something on my calendar. You know? So I feel like there is a pretty

386
00:23:35,595 --> 00:23:39,205
Speaker 4:  like firm wall on that right now. And and that would be

387
00:23:39,445 --> 00:23:43,165
Speaker 4:  interesting. I think that's kind of like rabbit's, you know,

388
00:23:43,495 --> 00:23:47,485
Speaker 4:  ethos is like how do we get around that? And it, the answer is a weird

389
00:23:47,545 --> 00:23:51,405
Speaker 4:  system where you log into stuff and Right on,

390
00:23:51,685 --> 00:23:53,085
Speaker 4:  I don't know, a virtual computer.

391
00:23:53,555 --> 00:23:56,525
Speaker 2:  Yeah. The answer is largely you can't right now. Yeah. It turns out

392
00:23:56,675 --> 00:24:00,365
Speaker 4:  That that seems to be not working also. So, but

393
00:24:00,365 --> 00:24:03,925
Speaker 2:  Yeah, it's, it's an interesting question and and my sense would be that's

394
00:24:03,925 --> 00:24:07,805
Speaker 2:  only happening if regulators make it happen, right? It seems, it seems to

395
00:24:07,805 --> 00:24:11,485
Speaker 2:  some extent like more powerful than setting your default browser,

396
00:24:12,485 --> 00:24:15,845
Speaker 2:  which you can do even more powerful than setting your default search engine,

397
00:24:15,975 --> 00:24:19,445
Speaker 2:  which has only become a thing because of regulatory pressure

398
00:24:19,865 --> 00:24:23,645
Speaker 2:  and essentially you could kind of argue it's like allowing you to install

399
00:24:23,645 --> 00:24:26,885
Speaker 2:  another operating system on top of your operating system. Like if you were

400
00:24:26,885 --> 00:24:30,765
Speaker 2:  allowed to let chat GPT run your phone, that's a pretty big thing for any

401
00:24:30,765 --> 00:24:33,485
Speaker 2:  of these companies to be able to do. I kind of agree that that's how it should

402
00:24:33,495 --> 00:24:36,285
Speaker 2:  shake out. If this tech is gonna get better in the way that everybody says

403
00:24:36,285 --> 00:24:39,685
Speaker 2:  it's gonna get better, it'll be cool to have options and not just be forced

404
00:24:40,065 --> 00:24:43,965
Speaker 2:  to use better Siri or better Gemini or

405
00:24:43,965 --> 00:24:47,525
Speaker 2:  better CHATT or whatever. I don't know that I would bet on that ever being

406
00:24:47,845 --> 00:24:48,405
Speaker 2:  a possibility.

407
00:24:48,915 --> 00:24:52,805
Speaker 4:  Yeah, that does seem like a big question mark. And then you get

408
00:24:52,805 --> 00:24:56,605
Speaker 4:  back to just like, well what if you could just talk to

409
00:24:56,605 --> 00:25:00,325
Speaker 4:  your operating system while you're walking around the kitchen and

410
00:25:01,275 --> 00:25:05,165
Speaker 4:  it's like not the ideal future that I want. I think I

411
00:25:05,165 --> 00:25:09,005
Speaker 4:  want a future where there's weird gadgets or you can download

412
00:25:09,165 --> 00:25:13,085
Speaker 4:  a, a, a virtual assistant that can actually do things for

413
00:25:13,085 --> 00:25:16,725
Speaker 4:  you. Yeah. I think that's why I am, I am on like team phones

414
00:25:17,165 --> 00:25:21,005
Speaker 4:  'cause it's the realist in me is like, oh, they're not gonna let

415
00:25:21,005 --> 00:25:21,645
Speaker 4:  this stuff happen.

416
00:25:22,645 --> 00:25:25,285
Speaker 2:  I think that's probably right. But I do also remember somebody, I forget

417
00:25:25,285 --> 00:25:28,445
Speaker 2:  who it was, somebody at one of the phone makers years ago was like, we're

418
00:25:28,445 --> 00:25:31,805
Speaker 2:  gonna get to a point where the phone in your pocket is basically just a cellular

419
00:25:31,975 --> 00:25:35,925
Speaker 2:  modem and all the accessories around are

420
00:25:35,925 --> 00:25:39,325
Speaker 2:  going to be connected to that in some way. Your wearable is gonna use that

421
00:25:39,325 --> 00:25:41,205
Speaker 2:  to connect to the internet. Your headphones, you gonna use that to connect

422
00:25:41,205 --> 00:25:44,005
Speaker 2:  to the internet. Your laptop is gonna use that to connect to the internet,

423
00:25:44,265 --> 00:25:48,045
Speaker 2:  but it is, it's going to be less the device you use for everything and

424
00:25:48,045 --> 00:25:51,805
Speaker 2:  more the device that just lets you use everything everywhere. Yeah. And I

425
00:25:51,805 --> 00:25:54,925
Speaker 2:  think that's a really interesting version of the future and is kind of what

426
00:25:54,925 --> 00:25:58,605
Speaker 2:  you discovered with this is like I can just use my phone as a

427
00:25:58,605 --> 00:26:02,445
Speaker 2:  go-between to all these services that exist and with headphones that I already

428
00:26:02,445 --> 00:26:06,165
Speaker 2:  own. I actually don't need to invent a completely new pipeline

429
00:26:06,305 --> 00:26:09,045
Speaker 2:  for that stuff. I just need a new way to kind of get at that pipeline.

430
00:26:09,275 --> 00:26:12,925
Speaker 4:  Yeah. And I've become just an addict for this smartwatch.

431
00:26:13,315 --> 00:26:17,285
Speaker 4:  Like I over the past couple years, like I never used one before

432
00:26:17,705 --> 00:26:21,645
Speaker 4:  and now I cannot live without one. It's just like, it's the

433
00:26:21,645 --> 00:26:25,485
Speaker 4:  right kind of light interface for when I need to do stuff. Like I'm out on

434
00:26:25,485 --> 00:26:29,405
Speaker 4:  a walk and I need to pause music or just check a

435
00:26:29,405 --> 00:26:33,245
Speaker 4:  quick notification or a text and it's like you run into the

436
00:26:33,245 --> 00:26:37,205
Speaker 4:  wall very quickly of like, you can't really respond to a text very well

437
00:26:37,305 --> 00:26:40,805
Speaker 4:  or you can thumbs up emoji something. Right. But

438
00:26:41,185 --> 00:26:44,925
Speaker 4:  boy has that been like, that is the point where I'm like, I am

439
00:26:45,505 --> 00:26:49,165
Speaker 4:  so willing to charge this extra thing and it has its own special

440
00:26:49,165 --> 00:26:52,965
Speaker 4:  charger that I have to bring along when I travel. And like

441
00:26:53,425 --> 00:26:57,405
Speaker 4:  it crosses that threshold of like, yes, I will do all of that because

442
00:26:57,485 --> 00:26:58,525
Speaker 4:  I find it so useful.

443
00:26:59,195 --> 00:27:02,845
Speaker 2:  Yeah. I I think smartwatch is probably land in the, the

444
00:27:03,035 --> 00:27:06,725
Speaker 2:  earbuds category of that kind of light input system

445
00:27:06,995 --> 00:27:10,205
Speaker 2:  that really works. Like when I wrote, when I wrote the humane pin review,

446
00:27:10,205 --> 00:27:13,525
Speaker 2:  the overwhelming response was what you're describing is just a better Apple

447
00:27:13,525 --> 00:27:17,445
Speaker 2:  watch. Yeah. And I think other than the camera stuff, which is left

448
00:27:17,445 --> 00:27:20,845
Speaker 2:  to be solved, but right now is only kind of somewhat

449
00:27:21,025 --> 00:27:24,965
Speaker 2:  useful anyway. That is true. Like my big

450
00:27:24,965 --> 00:27:28,645
Speaker 2:  regret right now is that I didn't buy a cell connected Apple watch. Right.

451
00:27:28,925 --> 00:27:32,245
Speaker 2:  I know. And I wish, I wish that I had because then I could do all that sort

452
00:27:32,245 --> 00:27:35,045
Speaker 2:  of lightweight stuff you're talking about without needing my phone in my

453
00:27:35,045 --> 00:27:35,885
Speaker 2:  pocket. Yeah, that'd

454
00:27:35,885 --> 00:27:38,605
Speaker 4:  Be awesome. You just leave the house. Can you imagine?

455
00:27:39,405 --> 00:27:42,725
Speaker 2:  I mean, first of all no, no, but also it sounds wonderful.

456
00:27:43,155 --> 00:27:43,445
Speaker 4:  Yeah.

457
00:27:44,185 --> 00:27:48,005
Speaker 2:  The idea of your phone as a thing that lives in your pocket that it lets

458
00:27:48,005 --> 00:27:51,085
Speaker 2:  everything else connect to the internet strikes me as much more the sort

459
00:27:51,085 --> 00:27:54,605
Speaker 2:  of AI gadget future than like these all encompassing

460
00:27:55,105 --> 00:27:56,405
Speaker 2:  new ideas about gadgets.

461
00:27:56,555 --> 00:27:59,685
Speaker 4:  Yeah, I know it's kind of depressing, but

462
00:28:00,355 --> 00:28:01,125
Speaker 4:  it's also I think

463
00:28:01,125 --> 00:28:01,365
Speaker 2:  True.

464
00:28:01,365 --> 00:28:01,725
Speaker 4:  It's a little bit,

465
00:28:02,515 --> 00:28:05,805
Speaker 2:  Yeah. And it means like all these other companies, humane Rabbit, everybody

466
00:28:05,805 --> 00:28:08,365
Speaker 2:  else is like desperately trying to get around your phone, just like you said

467
00:28:08,505 --> 00:28:12,245
Speaker 2:  and it's like, oh, this maybe keeps the world the way that it is if

468
00:28:12,245 --> 00:28:16,085
Speaker 2:  that's how we do it more than I would like. But it also just

469
00:28:16,085 --> 00:28:16,645
Speaker 2:  might be inevitable.

470
00:28:17,035 --> 00:28:20,965
Speaker 4:  Yeah. And it's an interesting moment at least. Yeah. Boy is

471
00:28:20,965 --> 00:28:23,125
Speaker 4:  it not dull right now.

472
00:28:24,245 --> 00:28:27,685
Speaker 2:  I know and I'm I'm still thinking about the, the camera comparison and cameras

473
00:28:27,735 --> 00:28:31,565
Speaker 2:  never came back. No. They just like mid-range. We found a lot of

474
00:28:31,805 --> 00:28:35,405
Speaker 2:  interesting stuff to do at like the high end and I think about like the Fuji

475
00:28:35,405 --> 00:28:39,085
Speaker 2:  series that people really love. Like I feel like if you want like a good

476
00:28:39,085 --> 00:28:43,005
Speaker 2:  camera, your options have never been better. But if you want a pretty

477
00:28:43,005 --> 00:28:45,445
Speaker 2:  good camera, they don't exist because you have a smartphone.

478
00:28:45,445 --> 00:28:46,605
Speaker 4:  Yeah. You already have it.

479
00:28:46,755 --> 00:28:49,765
Speaker 2:  Yeah. And I wonder maybe that's where we're gonna go with AI stuff and we're

480
00:28:49,765 --> 00:28:53,645
Speaker 2:  ways away from the ai, ai gadgets being any good. So

481
00:28:53,645 --> 00:28:56,605
Speaker 2:  maybe we're, we're gonna be in a real sort of valley with that stuff for

482
00:28:56,605 --> 00:28:57,525
Speaker 2:  a little while it feels like.

483
00:28:57,715 --> 00:28:59,285
Speaker 4:  Yeah. We're gonna find out,

484
00:28:59,505 --> 00:29:01,725
Speaker 2:  But luckily phones are getting better, so I'll take

485
00:29:01,725 --> 00:29:04,125
Speaker 4:  To that. Yeah, yeah. Phones are sticking around I think.

486
00:29:04,825 --> 00:29:07,725
Speaker 2:  And you've given me yet another reason to buy a flip phone, which oh man

487
00:29:07,745 --> 00:29:10,125
Speaker 2:  is all I really needed from you was more recent.

488
00:29:10,565 --> 00:29:14,525
Speaker 4:  I took that Motorola out of the closet and I was like, this is so nice.

489
00:29:15,265 --> 00:29:18,365
Speaker 4:  Ugh. Just shutting it. Just shutting. It feels good. Yeah.

490
00:29:18,505 --> 00:29:21,725
Speaker 2:  I'm gonna start buying t-shirts with the pocket on the, on the breast. Yeah,

491
00:29:21,725 --> 00:29:24,525
Speaker 2:  exactly. I'm gonna, I'm gonna fold it out and just stick it right there and

492
00:29:24,525 --> 00:29:26,245
Speaker 2:  chat with it all day. It's gonna be amazing. Yeah, it

493
00:29:26,245 --> 00:29:27,205
Speaker 4:  Looks super cool.

494
00:29:27,715 --> 00:29:31,685
Speaker 2:  Yeah, I I I believe that, I bet you made a lot of friends and everybody thought

495
00:29:31,685 --> 00:29:33,485
Speaker 2:  you were super normal and cool all the time.

496
00:29:33,555 --> 00:29:37,205
Speaker 4:  Yeah, I was taking pictures of it for the article and my husband walked into

497
00:29:37,205 --> 00:29:41,005
Speaker 4:  the kitchen and just slowly backed away. Like, I don't

498
00:29:41,045 --> 00:29:43,485
Speaker 4:  understand what's happening here and I'm not gonna question it.

499
00:29:44,105 --> 00:29:47,245
Speaker 2:  That's, that's the correct way to react to AI of all sorts. Yeah,

500
00:29:47,605 --> 00:29:48,725
Speaker 4:  Probably just back away.

501
00:29:50,025 --> 00:29:53,725
Speaker 2:  All right Allison, thank you as always. Thank you. We gotta take a break

502
00:29:53,745 --> 00:29:54,645
Speaker 2:  and then we're gonna talk that

503
00:31:02,545 --> 00:31:05,935
Speaker 2:  We're back while we're on the subject of ai, there's one company that we

504
00:31:05,935 --> 00:31:09,695
Speaker 2:  don't talk about much as an AI powerhouse and we probably

505
00:31:09,695 --> 00:31:13,535
Speaker 2:  should. That company is meta and it's been on a tear with new AI

506
00:31:13,535 --> 00:31:17,455
Speaker 2:  products recently it added multimodal AI to the smart glasses that it

507
00:31:17,455 --> 00:31:21,015
Speaker 2:  makes with RayBan. It launched a new model called LAMA three that it says

508
00:31:21,035 --> 00:31:25,015
Speaker 2:  can keep up with Gemini and Jet GPT and the rest. It added meta AI

509
00:31:25,115 --> 00:31:28,655
Speaker 2:  to like every one of its apps if you use any meta product

510
00:31:28,955 --> 00:31:32,775
Speaker 2:  and statistically if you are a person on earth, you do, you basically

511
00:31:32,935 --> 00:31:35,895
Speaker 2:  can't avoid Meta's AI anymore. There's a big

512
00:31:36,295 --> 00:31:40,255
Speaker 2:  company-wide move happening here and it feels like meta, which

513
00:31:40,255 --> 00:31:44,215
Speaker 2:  was all in on the Metaverse not that long ago, might be all in on

514
00:31:44,215 --> 00:31:48,055
Speaker 2:  something else. Now Alex Heath on our team knows this situation better than

515
00:31:48,055 --> 00:31:51,135
Speaker 2:  anybody and so he's here to help make sense of all of it for us.

516
00:31:51,645 --> 00:31:52,895
Speaker 2:  Alex Heath, welcome back.

517
00:31:53,245 --> 00:31:55,535
Speaker 5:  It's good to be back on the People's podcast, David.

518
00:31:55,995 --> 00:31:59,695
Speaker 2:  Yes, we who cares what the judges think Alex, I don't, I don't care about

519
00:31:59,695 --> 00:32:00,415
Speaker 2:  them. That's

520
00:32:00,415 --> 00:32:01,055
Speaker 5:  A webby reference.

521
00:32:01,475 --> 00:32:05,295
Speaker 2:  So there's a lot of stuff going on at Meta and basically you've been reporting

522
00:32:05,295 --> 00:32:08,455
Speaker 2:  out a bunch of like puzzle pieces over the last few weeks of what's going

523
00:32:08,455 --> 00:32:12,335
Speaker 2:  on there and I wanna see if we can like make sense of them all together.

524
00:32:12,755 --> 00:32:16,415
Speaker 2:  But let's start with last week with the Big Horizon os

525
00:32:16,895 --> 00:32:20,655
Speaker 2:  announcement. Explain to me what Meta is up to with Horizon Os and

526
00:32:20,655 --> 00:32:24,135
Speaker 2:  trying to like open up the Quest universe to other people.

527
00:32:24,485 --> 00:32:28,175
Speaker 5:  Yeah, so Meta's been doing headsets since before it was meta for a very

528
00:32:28,335 --> 00:32:32,255
Speaker 5:  long time, 10 years actually when they bought the Oculus startup,

529
00:32:32,565 --> 00:32:36,535
Speaker 5:  that then became Quest later. And what they

530
00:32:36,535 --> 00:32:39,765
Speaker 5:  did recently, which I think it actually makes a lot of sense in the bigger

531
00:32:39,765 --> 00:32:43,085
Speaker 5:  picture of Meadow, which I know we'll get into, is that they decided that

532
00:32:43,085 --> 00:32:47,045
Speaker 5:  the OS that they built for the Quest, which they've now called Horizon,

533
00:32:47,545 --> 00:32:51,445
Speaker 5:  and we are approaching Google level naming confusion here. It truly

534
00:32:51,645 --> 00:32:55,525
Speaker 5:  David insane. So ask me to to explain if, if it's confusing at

535
00:32:55,525 --> 00:32:59,445
Speaker 5:  any point. But they are taking the OS that they've

536
00:32:59,445 --> 00:33:03,325
Speaker 5:  built over time for the Quest and starting to license it out to

537
00:33:04,275 --> 00:33:08,165
Speaker 5:  OEMs. So basically replicating the Android playbook that

538
00:33:08,165 --> 00:33:12,085
Speaker 5:  Google used to become the number one player by user base

539
00:33:12,085 --> 00:33:15,685
Speaker 5:  and mobile and they think a similar thing could happen with headsets. It's

540
00:33:15,845 --> 00:33:19,685
Speaker 5:  very early. So all they've announced is that Lenovo is gonna work on

541
00:33:19,725 --> 00:33:23,445
Speaker 5:  a custom headset using their Horizon OS and

542
00:33:23,755 --> 00:33:27,325
Speaker 5:  Asus is also doing something and then there's gonna be less custom,

543
00:33:27,545 --> 00:33:30,805
Speaker 5:  but I think more just special edition with some, you know,

544
00:33:31,315 --> 00:33:35,165
Speaker 5:  bundled controllers, Xbox themed Quest. And I think

545
00:33:35,165 --> 00:33:38,805
Speaker 5:  what they've decided is that they want to be the general purpose headset

546
00:33:39,285 --> 00:33:43,005
Speaker 5:  maker. So making kind of the state-of-the-art thing that could be used for

547
00:33:43,125 --> 00:33:47,045
Speaker 5:  a lot of different tasks on your face. And they want a wide network of

548
00:33:47,315 --> 00:33:51,285
Speaker 5:  OEMs to basically invent all of these more niche narrow headset

549
00:33:51,665 --> 00:33:55,445
Speaker 5:  use cases. Whether that's something for work, something for gaming, something

550
00:33:55,465 --> 00:33:59,365
Speaker 5:  for who knows, construction, whatever the idea is, just let

551
00:33:59,485 --> 00:34:01,085
Speaker 5:  a million flowers bloom here.

552
00:34:01,345 --> 00:34:04,005
Speaker 2:  That's really interesting. So yeah, my, my next question is gonna be what

553
00:34:04,005 --> 00:34:07,405
Speaker 2:  do you think this means for Meta's hardware plans? But that's a really interesting

554
00:34:07,405 --> 00:34:11,245
Speaker 2:  way to think about it, that actually this is meta saying we want to

555
00:34:11,325 --> 00:34:15,085
Speaker 2:  build this sort of all purpose headset but then let

556
00:34:15,155 --> 00:34:18,165
Speaker 2:  lots of people build lots of other kinds of headsets which I think actually

557
00:34:18,165 --> 00:34:21,965
Speaker 2:  makes sense. Like we, we certainly have not arrived at the perfect hardware

558
00:34:22,145 --> 00:34:25,285
Speaker 2:  for a headset, so you might as well let lots of people try lots of things.

559
00:34:25,785 --> 00:34:28,965
Speaker 5:  No, and the quest is still pretty small. I mean it's approaching

560
00:34:29,355 --> 00:34:33,045
Speaker 5:  console level yearly volume in terms of sales. I think they've sold

561
00:34:33,285 --> 00:34:37,205
Speaker 5:  a little over 20 million quests to date now how many people actually use

562
00:34:37,205 --> 00:34:40,325
Speaker 5:  those quests on a regular basis is a whole nother story. Right, fair. They

563
00:34:40,325 --> 00:34:43,965
Speaker 5:  were giving them away basically for a little while. But yeah, I think even

564
00:34:43,965 --> 00:34:47,845
Speaker 5:  with this market being so young, they see the potential for

565
00:34:47,845 --> 00:34:51,605
Speaker 5:  others to come in and help them grow it. And I think crucially for meta here

566
00:34:51,605 --> 00:34:55,445
Speaker 5:  with the OS is that it comes with Horizon, the social

567
00:34:55,445 --> 00:34:59,325
Speaker 5:  network. So yes, horizon Os also has what has been called

568
00:34:59,355 --> 00:35:03,245
Speaker 5:  Horizon, the 3D kind of Sims meets Roblox

569
00:35:03,245 --> 00:35:07,205
Speaker 5:  social network that we have relentlessly made fun of here on The

570
00:35:07,525 --> 00:35:07,685
Speaker 5:  Verge.

571
00:35:08,005 --> 00:35:09,045
Speaker 2:  I was gonna say, which sucks

572
00:35:09,625 --> 00:35:13,565
Speaker 5:  To be fair, they have squashed a lot of the bugs and

573
00:35:13,565 --> 00:35:17,445
Speaker 5:  the avatars look better. But yeah it's still pretty rough. But

574
00:35:17,475 --> 00:35:21,165
Speaker 5:  that comes bundled with this os it kind of reminds me of when

575
00:35:21,425 --> 00:35:25,125
Speaker 5:  Google was licensing Android. I mean it's, that's still how they do it. That

576
00:35:25,385 --> 00:35:29,365
Speaker 5:  really to get the best experience you've got to have Google services

577
00:35:29,455 --> 00:35:33,005
Speaker 5:  baked into Android and this has actually gotten 'em into some anti-competitive,

578
00:35:33,385 --> 00:35:36,205
Speaker 5:  you know, regulation trouble over the years with how they bundled things.

579
00:35:36,225 --> 00:35:39,125
Speaker 5:  But Meta is kind of kind of doing the same thing here. They're bundling what

580
00:35:39,125 --> 00:35:43,005
Speaker 5:  is their actual way that they see monetizing all this metaverse stuff over

581
00:35:43,005 --> 00:35:46,845
Speaker 5:  time, which is their social network which will be monetized with ads of

582
00:35:46,845 --> 00:35:50,645
Speaker 5:  course and commerce assuming that enough people eventually

583
00:35:50,695 --> 00:35:51,405
Speaker 5:  start using it.

584
00:35:52,045 --> 00:35:55,805
Speaker 2:  Interesting. So I've been trying to think what is in this

585
00:35:56,025 --> 00:35:59,605
Speaker 2:  for meta because on the one hand I think Meta's assumption would be that

586
00:35:59,785 --> 00:36:03,525
Speaker 2:  the more people use any kind of immersive headset

587
00:36:03,695 --> 00:36:06,725
Speaker 2:  stuff, the better, right? Like we're still very much in a sort of rising

588
00:36:06,755 --> 00:36:10,605
Speaker 2:  tide lift all boats world then if you're meta you're like, okay, do

589
00:36:10,605 --> 00:36:14,525
Speaker 2:  we want to be the ones who run the store and we just take a commission on

590
00:36:14,525 --> 00:36:17,525
Speaker 2:  everything? Everybody buys on every headset And that's something, do we want

591
00:36:17,525 --> 00:36:20,805
Speaker 2:  to be the ones who like charge? Do we wanna charge a licensing fee and go

592
00:36:20,805 --> 00:36:24,485
Speaker 2:  like the Windows way and make all of our money that way? But you, you think

593
00:36:24,635 --> 00:36:28,325
Speaker 2:  it's actually the big Galaxy brain play here is actually a social networking

594
00:36:28,325 --> 00:36:28,605
Speaker 2:  play.

595
00:36:29,005 --> 00:36:31,685
Speaker 5:  I just think that's how they're gonna make money off of it. Okay. They've

596
00:36:31,685 --> 00:36:35,645
Speaker 5:  decided that they won't be making money on a hardware margin

597
00:36:35,715 --> 00:36:39,645
Speaker 5:  like Apple and you know, they're leaning into what they're inherently good

598
00:36:39,645 --> 00:36:43,565
Speaker 5:  at, which is like scaled ad-based attention services

599
00:36:44,145 --> 00:36:47,325
Speaker 5:  and I think they think that they can translate that to headsets over time.

600
00:36:47,585 --> 00:36:51,205
Speaker 5:  And so that's why Horizon, the social network is in Horizon the os

601
00:36:51,905 --> 00:36:55,645
Speaker 5:  I'm sure they're gonna get paid also just for someone to license the os,

602
00:36:55,645 --> 00:36:59,605
Speaker 5:  but I don't think they see that as a key moneymaker. And I

603
00:36:59,605 --> 00:37:02,685
Speaker 5:  know I've been mentioning Android and Google already a lot, but I would say

604
00:37:02,685 --> 00:37:05,565
Speaker 5:  that if you were to talk to, to Mark Zuckerberg like I have, he would really

605
00:37:05,565 --> 00:37:09,125
Speaker 5:  rather be me making a comparison to Microsoft because, oh interesting.

606
00:37:09,565 --> 00:37:13,525
Speaker 5:  Microsoft when they, you know, began licensing Windows OEMs really

607
00:37:13,525 --> 00:37:17,445
Speaker 5:  wanted it, it was actually the leading desktop PC software platform

608
00:37:17,445 --> 00:37:21,205
Speaker 5:  because Microsoft had invented the most useful productivity software at the

609
00:37:21,205 --> 00:37:24,925
Speaker 5:  time, right? Whereas Android was very clearly a reaction to the iPhone.

610
00:37:25,265 --> 00:37:29,245
Speaker 5:  It wasn't so much something that OEMs were seeking out because of

611
00:37:29,245 --> 00:37:32,885
Speaker 5:  how good it was. It was more of a defensive play. And to

612
00:37:32,905 --> 00:37:36,525
Speaker 5:  meta's credit, they invested in this category before anyone else at the level

613
00:37:36,525 --> 00:37:40,445
Speaker 5:  that they have. It hasn't gotten them a lot yet, but I think with this opening

614
00:37:40,545 --> 00:37:44,365
Speaker 5:  up play that they're doing, it positions them better long term,

615
00:37:44,365 --> 00:37:47,285
Speaker 5:  especially with Apple getting into the picture with the Vision Pro.

616
00:37:47,925 --> 00:37:51,565
Speaker 2:  I mean it, it is a funny time because there was just this report that Apple

617
00:37:51,705 --> 00:37:55,365
Speaker 2:  is scaling back its Vision Pro plans because it hasn't been

618
00:37:55,365 --> 00:37:59,045
Speaker 2:  selling the way that it hoped. It kind of feels like everything you just

619
00:37:59,045 --> 00:38:02,205
Speaker 2:  described makes quite a bit of sense to me, right, actually for like both

620
00:38:02,205 --> 00:38:06,165
Speaker 2:  what Meta is good at and also where this space is headed. Building

621
00:38:06,185 --> 00:38:09,285
Speaker 2:  an operating system makes a lot of sense and Meta has done this really well,

622
00:38:09,545 --> 00:38:12,965
Speaker 2:  but I feel like the, there's it, it all still assumes that eventually there

623
00:38:12,965 --> 00:38:16,405
Speaker 2:  will be a real market for headsets and I don't know that I'm less

624
00:38:16,405 --> 00:38:19,165
Speaker 2:  confident in that thesis than I would've been a couple years ago, but I certainly

625
00:38:19,165 --> 00:38:22,125
Speaker 2:  don't feel like I'm more confident in that thesis. But you've been talking

626
00:38:22,185 --> 00:38:25,925
Speaker 2:  to Mark, where do you feel like his head is with how

627
00:38:25,925 --> 00:38:28,045
Speaker 2:  this market is going in general right now?

628
00:38:28,445 --> 00:38:32,245
Speaker 5:  I think they still see these headsets that fully immerse yourself in

629
00:38:32,245 --> 00:38:35,565
Speaker 5:  virtual environments as akin to like the future

630
00:38:36,065 --> 00:38:40,005
Speaker 5:  PC market in terms of volume and scale. So you're not

631
00:38:40,005 --> 00:38:43,325
Speaker 5:  talking hundreds of millions of devices a year. You're probably just looking

632
00:38:43,385 --> 00:38:47,205
Speaker 5:  at, you know, low to med, tens of millions. And this is

633
00:38:47,235 --> 00:38:50,245
Speaker 5:  over a period of the next several years. I think the industry is in single

634
00:38:50,245 --> 00:38:54,125
Speaker 5:  digit millions right now, the AR side, so where

635
00:38:54,125 --> 00:38:57,605
Speaker 5:  the Meta Ray Band glasses are now, but eventually where they want to get

636
00:38:57,605 --> 00:39:01,565
Speaker 5:  to with displays in them. I think they see that by the end of this

637
00:39:01,565 --> 00:39:05,205
Speaker 5:  decade, early 2030 as something that could be approaching mobile

638
00:39:05,505 --> 00:39:09,245
Speaker 5:  in terms of its just widespread usage and adoption. So there's

639
00:39:09,245 --> 00:39:13,085
Speaker 5:  kind of two parts of the same play and a lot of the headset

640
00:39:13,295 --> 00:39:17,245
Speaker 5:  technology eventually makes its way into the AR technology. It goes back

641
00:39:17,245 --> 00:39:20,325
Speaker 5:  and forth and you know, we saw that with the Quest three, right? It has the

642
00:39:20,325 --> 00:39:24,005
Speaker 5:  mixed reality component that you're also seeing in a lot of AR

643
00:39:24,515 --> 00:39:25,005
Speaker 5:  devices.

644
00:39:25,435 --> 00:39:28,845
Speaker 2:  Yeah. And I guess that also makes the social emphasis make

645
00:39:29,095 --> 00:39:33,085
Speaker 2:  sense because if you're trying to build sort of from both directions,

646
00:39:33,745 --> 00:39:36,525
Speaker 2:  one thing that will will flow kind of across that whole spectrum, all the

647
00:39:36,525 --> 00:39:39,805
Speaker 2:  way up from smart glasses to like really high end headsets is just people

648
00:39:39,835 --> 00:39:43,405
Speaker 2:  want to talk to each other and hang out. And so if, if Meta can build that

649
00:39:43,405 --> 00:39:46,765
Speaker 2:  stuff for that whole spectrum, it can sort of win no matter how long it takes

650
00:39:46,785 --> 00:39:49,045
Speaker 2:  us to get to the final thing.

651
00:39:49,355 --> 00:39:53,245
Speaker 5:  Yeah, I mean they see this hardware naturally as like an inherently social

652
00:39:53,245 --> 00:39:57,085
Speaker 5:  medium. I think that's a bit of a dichotomy when you think about strapping

653
00:39:57,085 --> 00:40:00,645
Speaker 5:  something to your face. There's something inherently antisocial and putting

654
00:40:00,645 --> 00:40:04,365
Speaker 5:  something on your face between you and other people. But it's a pretty

655
00:40:04,365 --> 00:40:07,805
Speaker 5:  different approach to what we've seen at least Apple do to date with the

656
00:40:07,805 --> 00:40:11,365
Speaker 5:  Vision Pro, right? They're really treating it as more of a general purpose

657
00:40:11,715 --> 00:40:15,485
Speaker 5:  computing platform. They're not leaning into social the only social thing

658
00:40:15,485 --> 00:40:19,245
Speaker 5:  they have. The FaceTime personas are pretty creepy based on my experience.

659
00:40:19,245 --> 00:40:23,125
Speaker 5:  They're so creepy. Yeah. So I think Meta recognizes where it

660
00:40:23,185 --> 00:40:26,925
Speaker 5:  can play to its strengths and that's in building social experiences at scale

661
00:40:27,705 --> 00:40:31,605
Speaker 5:  and being this kind of quasi open player that is

662
00:40:31,605 --> 00:40:33,885
Speaker 5:  the Microsoft to Apple's apple.

663
00:40:34,235 --> 00:40:38,165
Speaker 2:  Yeah. Let's talk about the open player thing because there was also, you

664
00:40:38,165 --> 00:40:42,005
Speaker 2:  know, this, this push to Llama three, the new large language model and there

665
00:40:42,225 --> 00:40:45,925
Speaker 2:  meta's trying to be open about how it approaches a lot of the big language

666
00:40:45,925 --> 00:40:49,605
Speaker 2:  model stuff. It's very strange to me that the company that built

667
00:40:50,005 --> 00:40:53,565
Speaker 2:  Facebook, which you had to be logged into in order to see anything

668
00:40:53,865 --> 00:40:57,405
Speaker 2:  and was so famously like its own walled garden and made a ton of money by

669
00:40:57,405 --> 00:40:59,605
Speaker 2:  being that walled garden that people came into and spent all of their time

670
00:40:59,605 --> 00:41:03,405
Speaker 2:  into, has now made this grand pivot into being like

671
00:41:03,485 --> 00:41:07,085
Speaker 2:  a, a good citizen of the internet and an open player working with everybody.

672
00:41:07,225 --> 00:41:10,525
Speaker 2:  And they're talking about this with Federating threads, they're talking about

673
00:41:10,525 --> 00:41:13,365
Speaker 2:  this with the AI stuff, they're talking about it with the horizon stuff like

674
00:41:13,715 --> 00:41:16,685
Speaker 2:  what happened here? Is it like Mark Zuckerberg got like really into fighting

675
00:41:16,705 --> 00:41:19,125
Speaker 2:  and was just like, I love the world now. Like what, what happened here?

676
00:41:19,845 --> 00:41:23,445
Speaker 5:  I don't know if fighting would get you into being like embracing open source.

677
00:41:23,505 --> 00:41:24,685
Speaker 5:  That's an interesting theory.

678
00:41:26,025 --> 00:41:29,005
Speaker 2:  He, he just has somewhere else to put all of his rage, so now he's happy

679
00:41:29,005 --> 00:41:29,685
Speaker 2:  at work. I don't

680
00:41:29,685 --> 00:41:33,525
Speaker 5:  Know. I would challenge a little bit of your, your history retelling there.

681
00:41:33,685 --> 00:41:37,365
Speaker 5:  I mean you remember the Farmville era? Sure. They actually made a huge

682
00:41:37,505 --> 00:41:41,125
Speaker 5:  bet very early to become a developer platform. So yeah, they've been a walled

683
00:41:41,125 --> 00:41:44,365
Speaker 5:  garden in the sense that they've pretty tightly controlled the, the walls

684
00:41:44,365 --> 00:41:48,045
Speaker 5:  around their apps. So it's not like, you know, the code to Instagram is open

685
00:41:48,045 --> 00:41:51,605
Speaker 5:  source and I it never will be. But I would say they have

686
00:41:51,885 --> 00:41:55,845
Speaker 5:  historically, actually even on the hardware side, thinking about very

687
00:41:55,855 --> 00:41:59,445
Speaker 5:  early, you know, Oculus, rifs were done with companies like

688
00:41:59,945 --> 00:42:03,525
Speaker 5:  xmi, Samsung, Lenovo, so

689
00:42:03,875 --> 00:42:07,365
Speaker 5:  they have a history of this. I really think that meta is

690
00:42:07,765 --> 00:42:11,365
Speaker 5:  becoming a kind of new Microsoft. Hmm. And

691
00:42:11,745 --> 00:42:15,645
Speaker 5:  it makes sense because Zuckerberg has seen Bill Gates as a, as

692
00:42:15,685 --> 00:42:19,325
Speaker 5:  a kind of a business idol and mentor personally for a very long time.

693
00:42:19,465 --> 00:42:23,005
Speaker 5:  And I, I've heard him talk about Microsoft's influence on his thinking

694
00:42:23,325 --> 00:42:26,045
Speaker 5:  over the years and he is talked about it with me recently. In

695
00:42:26,045 --> 00:42:29,445
Speaker 2:  What sense? Like what, what about Microsoft do you feel like sticks out to

696
00:42:29,445 --> 00:42:29,845
Speaker 2:  him is

697
00:42:29,845 --> 00:42:33,645
Speaker 5:  Appealing? I think the way they leveraged their expertise

698
00:42:34,065 --> 00:42:37,765
Speaker 5:  in software and then licensing that software

699
00:42:38,065 --> 00:42:41,885
Speaker 5:  across different players to really just build a

700
00:42:41,885 --> 00:42:45,845
Speaker 5:  true platform. There's this old Bill Gates quote and I'm gonna butcher

701
00:42:45,845 --> 00:42:49,765
Speaker 5:  it, but essentially he said, you know, you know, you've built a true platform

702
00:42:49,795 --> 00:42:53,605
Speaker 5:  when the total value of what has been built on top of your platform

703
00:42:53,785 --> 00:42:57,165
Speaker 5:  is greater than the platform itself. Right? Right. And I think that's what

704
00:42:57,165 --> 00:43:00,765
Speaker 5:  Met is hoping for on the headset side and especially on the AI side.

705
00:43:01,065 --> 00:43:04,805
Speaker 5:  You know, with Lama for example, I just interviewed Mark for

706
00:43:05,025 --> 00:43:08,805
Speaker 5:  the LAMA three release. It's interesting what they're doing because they're

707
00:43:09,165 --> 00:43:12,605
Speaker 5:  spending billions of dollars on compute to build these cutting edge models

708
00:43:12,635 --> 00:43:16,085
Speaker 5:  that compete with open ai, Andro and others. And they're just giving them

709
00:43:16,115 --> 00:43:19,805
Speaker 5:  away. And that seems counterintuitive for a company like Meta to do,

710
00:43:20,645 --> 00:43:23,685
Speaker 5:  but when you think about where they're kind of strategically positioned relative

711
00:43:23,685 --> 00:43:27,325
Speaker 5:  to the rest of the players here, they don't have their own cloud

712
00:43:27,725 --> 00:43:31,365
Speaker 5:  business, right? So they're not selling access to their servers and

713
00:43:31,735 --> 00:43:35,605
Speaker 5:  their business is building consumer social products that

714
00:43:36,205 --> 00:43:40,165
Speaker 5:  monetize attention and rather than boil the ocean, I

715
00:43:40,165 --> 00:43:42,965
Speaker 5:  think they're realizing that's where they're, they can play to their strengths.

716
00:43:43,625 --> 00:43:47,445
Speaker 5:  And the history with Open source is if you get a bunch of people

717
00:43:47,665 --> 00:43:51,205
Speaker 5:  to build on your frameworks that you've open sourced,

718
00:43:51,415 --> 00:43:55,325
Speaker 5:  guess what? It makes the technology you build more pervasive

719
00:43:55,785 --> 00:43:59,565
Speaker 5:  and value flows back to you in the sense that the community helps

720
00:43:59,675 --> 00:44:03,445
Speaker 5:  improve it. Right? And Zuckerberg said something on a podcast

721
00:44:03,925 --> 00:44:07,885
Speaker 5:  recently where if the developers using LAMA three

722
00:44:07,945 --> 00:44:11,925
Speaker 5:  can help them lower their costs on inference, which is the cost of like

723
00:44:11,925 --> 00:44:15,885
Speaker 5:  running these models over time, even just slightly, it

724
00:44:15,885 --> 00:44:19,445
Speaker 5:  will make up for the entirety that they've spent on training all future

725
00:44:19,445 --> 00:44:23,085
Speaker 5:  versions of Llama just in terms of the cost of what they need to, to run

726
00:44:23,085 --> 00:44:26,965
Speaker 5:  this stuff in the cloud, in their cloud. So it's a pure

727
00:44:26,985 --> 00:44:30,805
Speaker 5:  infrastructure play that I think Mark has correctly

728
00:44:31,525 --> 00:44:35,325
Speaker 5:  identified. They're in a unique position to make and it creates more

729
00:44:35,325 --> 00:44:39,045
Speaker 5:  competition because LAMA three can be used by other developers. It forces

730
00:44:39,275 --> 00:44:42,725
Speaker 5:  open AI and others I think to do more in open source, which I think we'll

731
00:44:42,725 --> 00:44:45,885
Speaker 5:  see over the course of this year. And you know, meta has a long history of

732
00:44:45,885 --> 00:44:49,805
Speaker 5:  doing open source actually on the kind of just regular coding

733
00:44:49,915 --> 00:44:50,845
Speaker 5:  side. That's

734
00:44:50,845 --> 00:44:51,285
Speaker 2:  True. Yeah.

735
00:44:51,345 --> 00:44:54,845
Speaker 5:  It, it makes sense for them and it's, again, I just think we're seeing kind

736
00:44:54,845 --> 00:44:58,805
Speaker 5:  of like the rise of perhaps the next Microsoft in terms of how they're

737
00:44:58,805 --> 00:45:00,525
Speaker 5:  attacking each of these new technologies.

738
00:45:01,115 --> 00:45:05,005
Speaker 2:  Yeah, it seems smart too because right now most of those other companies

739
00:45:05,005 --> 00:45:08,725
Speaker 2:  you just named are all desperately trying to be the next Apple. Hmm. Like

740
00:45:08,985 --> 00:45:12,405
Speaker 2:  OpenAI is very much trying to sort of build a universe inside of

741
00:45:13,105 --> 00:45:15,925
Speaker 2:  OpenAI. They like license some of this stuff out, but like it's clear that

742
00:45:15,925 --> 00:45:18,285
Speaker 2:  the thing that they want to do is like, they want to build, they want to

743
00:45:18,355 --> 00:45:22,245
Speaker 2:  make custom gpt and have you use their stuff inside of their stuff and

744
00:45:22,705 --> 00:45:26,405
Speaker 2:  so many, and like Google is desperately trying to like pull the whole ecosystem

745
00:45:26,585 --> 00:45:30,525
Speaker 2:  in around itself with ai and you get the sense everybody sees this

746
00:45:30,545 --> 00:45:34,005
Speaker 2:  as a chance to sort of own the whole experience and Meta is going the other

747
00:45:34,005 --> 00:45:37,725
Speaker 2:  way, being like, this is a chance for us to like own the whole experience

748
00:45:37,725 --> 00:45:40,725
Speaker 2:  everywhere, which actually strikes me as very smart in this moment.

749
00:45:41,155 --> 00:45:44,805
Speaker 5:  Yeah, and you know, it's, yeah, I mean Meta should maybe be called open AI

750
00:45:44,805 --> 00:45:48,125
Speaker 5:  and OpenAI should probably find a new name at this point. I mean, OpenAI

751
00:45:48,125 --> 00:45:52,005
Speaker 5:  isn't open at all really. Yeah, it's, it's interesting. I, I agree with you.

752
00:45:52,405 --> 00:45:55,445
Speaker 5:  I think for a while Meadow was trying to have its cake and eat it too on

753
00:45:55,445 --> 00:45:59,085
Speaker 5:  the reality labs Quest side. And I kept pushing them on this that like they

754
00:45:59,085 --> 00:46:02,885
Speaker 5:  were trying to be too much like Apple in the sense that they were doing all

755
00:46:02,885 --> 00:46:06,485
Speaker 5:  of the hardware fully vertically integrated and the

756
00:46:06,725 --> 00:46:10,285
Speaker 5:  software. And I was like, when are you gonna, if you wanna be the open player,

757
00:46:10,285 --> 00:46:13,565
Speaker 5:  you've gotta start licensing some of this stuff because Apple, you can't

758
00:46:13,565 --> 00:46:17,125
Speaker 5:  compete with Apple at its own game, right? Like Apple is the best at this

759
00:46:17,665 --> 00:46:21,365
Speaker 5:  and I think they finally realized that and are embracing it and

760
00:46:21,475 --> 00:46:24,165
Speaker 5:  it's what Microsoft did and it's smart.

761
00:46:24,355 --> 00:46:27,765
Speaker 2:  Yeah, no, I, I I think, I think that's right and it also seems like it's

762
00:46:27,765 --> 00:46:31,405
Speaker 2:  given meta real momentum. Like my, my sense is this,

763
00:46:31,705 --> 00:46:35,245
Speaker 2:  the, the LAMA relaunch and just the sort of meta AI

764
00:46:35,435 --> 00:46:39,245
Speaker 2:  percolation around all of Meta's different apps really worked

765
00:46:39,475 --> 00:46:43,125
Speaker 2:  very quickly. Like, it, it it kind of, it became a big player pretty

766
00:46:43,195 --> 00:46:47,125
Speaker 2:  fast in, in this space and I think in part because it has been

767
00:46:47,515 --> 00:46:50,645
Speaker 2:  sort of out in the ecosystem much more rather than just saying, you know,

768
00:46:50,995 --> 00:46:54,165
Speaker 2:  here is an AI you can talk to inside of Instagram, which I think everybody

769
00:46:54,165 --> 00:46:57,565
Speaker 2:  would've kind of just sniffed at and moved on. It feels like Meta is able

770
00:46:57,565 --> 00:47:01,125
Speaker 2:  to make these big plays by playing this game differently.

771
00:47:01,315 --> 00:47:04,685
Speaker 5:  They are, I have a framework for evaluating companies, which is like, are

772
00:47:04,685 --> 00:47:07,965
Speaker 5:  you in the show me or tell me phase? And I think Meta's been doing a lot

773
00:47:07,965 --> 00:47:11,645
Speaker 5:  of tell me stuff with ai, like the assistant is gonna be everywhere. Right?

774
00:47:11,645 --> 00:47:14,565
Speaker 5:  That was the recent news. They're putting it in the search box of Instagram,

775
00:47:14,735 --> 00:47:15,485
Speaker 5:  which I hate

776
00:47:15,485 --> 00:47:16,325
Speaker 2:  By the way. Yeah.

777
00:47:16,625 --> 00:47:19,925
Speaker 5:  So that's where I'm going like yeah, they're putting it everywhere. And now

778
00:47:19,925 --> 00:47:23,245
Speaker 5:  the real test is do people actually want this? Does it make sense to have

779
00:47:23,285 --> 00:47:27,205
Speaker 5:  a chat GPT like experience in your Instagram or your WhatsApp where

780
00:47:27,205 --> 00:47:31,005
Speaker 5:  you're used to just talking with people and Zuckerberg really thinks we're

781
00:47:31,005 --> 00:47:34,645
Speaker 5:  all gonna be interacting with synthetic AI type personas. Like people

782
00:47:35,185 --> 00:47:38,525
Speaker 5:  in conversations with people, you know, like you can add the meta AI to a

783
00:47:38,725 --> 00:47:42,525
Speaker 5:  WhatsApp thread with other people to give recommendations or something. So

784
00:47:42,675 --> 00:47:45,725
Speaker 5:  they've gotta prove that people actually want that, that running joke inside

785
00:47:45,725 --> 00:47:49,645
Speaker 5:  Meta before they put the AI everywhere was that like no one was using

786
00:47:49,645 --> 00:47:53,365
Speaker 5:  this, it was something you had to kind of find. It was pretty buried and

787
00:47:53,585 --> 00:47:56,805
Speaker 5:  not even like people at Meta were using it. So now that it's got Lama three

788
00:47:56,805 --> 00:48:00,645
Speaker 5:  under it, which is pretty competitive as a model and they've got Google results

789
00:48:00,645 --> 00:48:03,405
Speaker 5:  in there and Bing results in there, which are the only chat bot that does

790
00:48:03,405 --> 00:48:07,365
Speaker 5:  realtime search from both those, it's got a good shot at at

791
00:48:07,365 --> 00:48:11,085
Speaker 5:  capturing a lot of the attention from people who have maybe never tried

792
00:48:11,695 --> 00:48:15,565
Speaker 5:  Chatt PT or Anthropics Claude. Because you know, for people who listen

793
00:48:15,565 --> 00:48:19,165
Speaker 5:  to the show, they know all these names, but you gotta think the, the boomers

794
00:48:19,195 --> 00:48:23,165
Speaker 5:  sharing, you know, spaghetti Jesus viral generative

795
00:48:23,265 --> 00:48:27,005
Speaker 5:  AI memes on Facebook, they've probably never used a chat bot

796
00:48:27,005 --> 00:48:29,965
Speaker 5:  before. There's, there's millions, hundreds of millions of people who have

797
00:48:29,965 --> 00:48:32,765
Speaker 5:  never used one of these things. And I think Zuckerberg's bet is just like

798
00:48:32,765 --> 00:48:36,445
Speaker 5:  he did with stories and Snapchat and reels with TikTok, he's gonna

799
00:48:36,445 --> 00:48:40,405
Speaker 5:  introduce this new format, this new way of interacting with the internet

800
00:48:40,625 --> 00:48:44,485
Speaker 5:  to more people than anyone else possibly could because his

801
00:48:44,515 --> 00:48:48,125
Speaker 5:  true inherent leverage and competitive edge

802
00:48:48,385 --> 00:48:50,605
Speaker 5:  is over 3 billion daily users.

803
00:48:51,115 --> 00:48:54,525
Speaker 2:  Yeah. I think that all strategically makes a lot of sense to me. And I think

804
00:48:54,525 --> 00:48:58,045
Speaker 2:  as a product, it works in some places. Like I actually think putting the

805
00:48:58,045 --> 00:49:01,965
Speaker 2:  generative AI stuff inside of Facebook has terrifying

806
00:49:01,965 --> 00:49:05,805
Speaker 2:  implications, but as a product makes some sense. I hate that they have

807
00:49:06,005 --> 00:49:09,845
Speaker 2:  replaced the Instagram search bar with meta ai. I just wanna find a

808
00:49:09,845 --> 00:49:13,205
Speaker 2:  video. I don't wanna talk to the ai, I wanna search for a video. Like it's

809
00:49:13,395 --> 00:49:16,605
Speaker 2:  this idea that we have decided AI and search are the same thing, just drives

810
00:49:16,605 --> 00:49:19,525
Speaker 2:  me insane. And I wish Meta would stop doing it.

811
00:49:19,715 --> 00:49:22,885
Speaker 5:  They might, you know, if there's enough uproar, they might, they did this

812
00:49:22,885 --> 00:49:26,725
Speaker 5:  like overnight Meta is notorious, they invented the kind of ruthless

813
00:49:26,785 --> 00:49:30,565
Speaker 5:  AB testing in Silicon Valley. So in a normal situation they would've tested

814
00:49:30,595 --> 00:49:34,485
Speaker 5:  this with some, you know, in different markets for weeks and on

815
00:49:34,485 --> 00:49:37,125
Speaker 5:  end, months on end figured out if people actually wanted, they just flipped

816
00:49:37,125 --> 00:49:41,045
Speaker 5:  it on for everyone overnight, which means it's a Zuck level top down.

817
00:49:41,265 --> 00:49:44,245
Speaker 5:  We are betting the farm on this as a modality

818
00:49:45,105 --> 00:49:47,525
Speaker 5:  and now he's gotta be proven right or wrong.

819
00:49:47,865 --> 00:49:50,165
Speaker 2:  So that was actually one of the things I wanted to ask you about is like

820
00:49:50,165 --> 00:49:54,005
Speaker 2:  connect the AI push to that sort of big picture meta

821
00:49:54,195 --> 00:49:58,085
Speaker 2:  idea for me because I think I saw AI for

822
00:49:58,165 --> 00:50:02,125
Speaker 2:  a while at Meta as kind of an end around Metaverse play. Like

823
00:50:02,185 --> 00:50:05,765
Speaker 2:  AI got really cool right as the Metaverse stopped being cool and they could

824
00:50:05,765 --> 00:50:09,165
Speaker 2:  build AI stuff that was actually Metaverse stuff, but they just had to call

825
00:50:09,165 --> 00:50:12,925
Speaker 2:  it AI stuff. But now it seems like there is this idea that

826
00:50:13,355 --> 00:50:17,045
Speaker 2:  some part of that like engagement based

827
00:50:17,315 --> 00:50:21,125
Speaker 2:  community stuff going on inside of Meta's products, they also seem to think

828
00:50:21,265 --> 00:50:25,165
Speaker 2:  AI is part of that in a way that I can't quite wrap my head

829
00:50:25,165 --> 00:50:29,045
Speaker 2:  around. Like as you talk to people like Zuckerberg, like

830
00:50:29,195 --> 00:50:33,005
Speaker 2:  what is the sort of Galaxy brand vision for AI right now?

831
00:50:33,635 --> 00:50:37,605
Speaker 5:  It's this kind of scary vision that I, frankly I can't

832
00:50:37,605 --> 00:50:41,245
Speaker 5:  decide how dystopian it is, but it's this idea that there are just going

833
00:50:41,245 --> 00:50:45,045
Speaker 5:  to be thousands if not millions of ais that we chat

834
00:50:45,305 --> 00:50:49,005
Speaker 5:  and interact with in our feeds and our messaging threads

835
00:50:49,275 --> 00:50:52,925
Speaker 5:  that are indistinguishable from people and that

836
00:50:53,325 --> 00:50:57,245
Speaker 5:  creators that Kylie Jenner on Instagram will have an AI version of herself

837
00:50:57,715 --> 00:51:01,605
Speaker 5:  that she's trained with certain parameters that means that all her

838
00:51:01,605 --> 00:51:05,365
Speaker 5:  fans can interact with quote unquote Kylie Jenner, right? And

839
00:51:05,475 --> 00:51:09,365
Speaker 5:  they're doing some, I think, kind of clever early engagement

840
00:51:09,415 --> 00:51:13,045
Speaker 5:  hacks with meta ai. There was an example of I think 4 0 4 media

841
00:51:13,495 --> 00:51:17,205
Speaker 5:  where it was really cringe, but it was an interesting way to kind of juice

842
00:51:17,255 --> 00:51:20,925
Speaker 5:  human engagement on Facebook. There was this group

843
00:51:21,645 --> 00:51:24,885
Speaker 5:  full of parents with special needs kids and

844
00:51:25,155 --> 00:51:28,765
Speaker 5:  there's this new feature that Meta is trying in a, in a group where if someone

845
00:51:28,835 --> 00:51:32,485
Speaker 5:  doesn't comment under a group post for I think like more than an hour

846
00:51:32,595 --> 00:51:36,245
Speaker 5:  meta AI will comment with its own just prompt to try to Oh wow.

847
00:51:36,295 --> 00:51:39,885
Speaker 5:  Spark conversation with the actual human beings. And in this example,

848
00:51:40,115 --> 00:51:43,845
Speaker 5:  Medi made up that it had a kid or a couple of kids with special

849
00:51:43,845 --> 00:51:47,525
Speaker 5:  needs and it was pretty creepy and hallucinating and all that.

850
00:51:48,105 --> 00:51:51,405
Speaker 5:  But you look at the thread, it got a lot of humans commenting underneath

851
00:51:51,405 --> 00:51:55,205
Speaker 5:  it, right? So it's like you're using AI as the beginning

852
00:51:55,335 --> 00:51:58,685
Speaker 5:  flame to get a bunch of people to actually talk to each other. I

853
00:51:58,685 --> 00:52:02,405
Speaker 2:  Like the idea that the AI's job is to just say some wild insane

854
00:52:02,415 --> 00:52:06,245
Speaker 2:  stuff on every post on the internet just to just to get the fire.

855
00:52:06,445 --> 00:52:06,565
Speaker 2:  I

856
00:52:06,565 --> 00:52:10,445
Speaker 5:  Mean we can all just rage at it. Yeah. And yeah, I think that's

857
00:52:10,445 --> 00:52:13,605
Speaker 5:  part of it. But yeah, it's, it's all of these things and it's also the, the

858
00:52:13,745 --> 00:52:17,485
Speaker 5:  the visual generation stuff, there's gonna be multimodality. So

859
00:52:17,485 --> 00:52:21,125
Speaker 5:  generating video you can already generate images. They upgraded it where

860
00:52:21,345 --> 00:52:24,325
Speaker 5:  it like generates the image as you type. Have you played with this?

861
00:52:25,075 --> 00:52:26,125
Speaker 2:  It's really cool. Yeah,

862
00:52:26,125 --> 00:52:29,485
Speaker 5:  It's cool. And you know, obviously they're, they're still figuring out the

863
00:52:29,485 --> 00:52:32,365
Speaker 5:  like things that will and won't do that are cringe, you know, the Gemini

864
00:52:32,835 --> 00:52:36,685
Speaker 5:  type diversity scandal stuff. Yeah. But I think we're still in the very early

865
00:52:36,925 --> 00:52:40,245
Speaker 5:  innings of this. It's the, and now it's the show me like I was saying, they've

866
00:52:40,245 --> 00:52:42,485
Speaker 5:  gotta show that people actually want all this stuff.

867
00:52:42,865 --> 00:52:46,045
Speaker 2:  That's a very broad way of thinking about it. 'cause like the, the way you

868
00:52:46,045 --> 00:52:49,445
Speaker 2:  were describing it earlier right, is that there is that sort of unifying

869
00:52:49,535 --> 00:52:52,645
Speaker 2:  sense of what Meta is very good at, which is, which is basically building

870
00:52:52,645 --> 00:52:56,085
Speaker 2:  products that people engage with at scale, right? Like you, you can boil

871
00:52:56,325 --> 00:52:59,925
Speaker 2:  a lot of the things that Meta does down to that. And the

872
00:53:00,145 --> 00:53:03,805
Speaker 2:  AI seems like it's partly that, but it could also be lots of stuff.

873
00:53:04,065 --> 00:53:07,925
Speaker 2:  And I get the sense that Meta is maybe just gonna explore that

874
00:53:07,925 --> 00:53:11,405
Speaker 2:  stuff to see what happens. But also I think like a lot of companies is going

875
00:53:11,405 --> 00:53:15,005
Speaker 2:  to be prone to being distracted by whatever shiny thing you can do

876
00:53:15,115 --> 00:53:18,845
Speaker 2:  with a language model and just shove it inside of apps and think people

877
00:53:18,845 --> 00:53:22,645
Speaker 2:  will use it. And I feel like meta more than most might have a chance

878
00:53:22,645 --> 00:53:26,245
Speaker 2:  to like really ruin some of its products by overextending AI

879
00:53:26,355 --> 00:53:26,805
Speaker 2:  into it.

880
00:53:27,105 --> 00:53:31,045
Speaker 5:  The thing is, they're a large mature publicly traded company. They won't

881
00:53:31,055 --> 00:53:34,965
Speaker 5:  torch everything for this. Like if they see that it's tanking key metrics

882
00:53:35,195 --> 00:53:38,805
Speaker 5:  that will impact, you know, the next earnings, they'll roll this stuff back.

883
00:53:39,125 --> 00:53:41,725
Speaker 2:  I mean, I would point out they torched a lot of things in the name of the

884
00:53:41,725 --> 00:53:44,605
Speaker 2:  metaverse for a while there. Like this wouldn't be totally unheard of.

885
00:53:44,955 --> 00:53:48,925
Speaker 5:  Well, kind of. I mean it was all, it's all still separate, right? Like yeah.

886
00:53:48,925 --> 00:53:52,885
Speaker 5:  So it's a lot of sunken money into it, but nothing they've done

887
00:53:52,885 --> 00:53:56,645
Speaker 5:  on the Quest has like impacted your experience as an average Instagram

888
00:53:56,675 --> 00:53:56,965
Speaker 5:  user.

889
00:53:57,105 --> 00:53:59,405
Speaker 2:  That's fair. They're not fighting Facebook. Yeah.

890
00:53:59,405 --> 00:54:02,525
Speaker 5:  That's what's different about me AI is that they're literally putting it

891
00:54:02,525 --> 00:54:06,125
Speaker 5:  everywhere. So they're in the, like, like you said, shove it everywhere and

892
00:54:06,125 --> 00:54:09,645
Speaker 5:  find out phase and we're all about to find out. Yeah.

893
00:54:09,995 --> 00:54:13,805
Speaker 2:  It's, yeah. So what, what's your sense from talking to Mark

894
00:54:13,935 --> 00:54:17,205
Speaker 2:  about this Google stuff, by the way, this is, this is the like small detail

895
00:54:17,205 --> 00:54:20,805
Speaker 2:  of, of your most recent story with him and this whole meta

896
00:54:21,065 --> 00:54:23,645
Speaker 2:  AI rollout that I found the most fascinating. They're very excited about

897
00:54:23,645 --> 00:54:27,165
Speaker 2:  having Google integrated into meta AI search results. Yeah.

898
00:54:27,335 --> 00:54:30,765
Speaker 2:  Which just seems bizarre to me that that is a thing that exists. What, what,

899
00:54:30,765 --> 00:54:32,445
Speaker 2:  what do you know about what's going on there? I

900
00:54:32,445 --> 00:54:36,085
Speaker 5:  Thought it was interesting too, reading between the lines. You know, I asked

901
00:54:36,085 --> 00:54:39,765
Speaker 5:  him what is the deal here because Google has not licensed

902
00:54:40,225 --> 00:54:43,805
Speaker 5:  its realtime search results to any of these chat bots yet. And

903
00:54:44,125 --> 00:54:47,925
Speaker 5:  I was like, are you paying them? Are they paying you? He basically said

904
00:54:47,925 --> 00:54:51,845
Speaker 5:  that Meta is paying Google, but it's not quote a ton. I don't

905
00:54:51,845 --> 00:54:55,685
Speaker 5:  know if a ton is billionaire who owns half of Kauai a

906
00:54:55,765 --> 00:54:59,485
Speaker 5:  ton or normal a ton. I'm sure that will come out. But

907
00:54:59,835 --> 00:55:03,685
Speaker 5:  yeah, it's smart because that's one of the main problems with

908
00:55:03,685 --> 00:55:07,605
Speaker 5:  these chatbots, right? Is their recency isn't very good. They make up

909
00:55:07,665 --> 00:55:11,285
Speaker 5:  events. I was using meta AI without the Google

910
00:55:11,315 --> 00:55:15,245
Speaker 5:  integration and it like used last year's Coachella lineup

911
00:55:15,245 --> 00:55:18,805
Speaker 5:  when I asked it, who was just performing at Coachella last weekend. So

912
00:55:19,065 --> 00:55:22,805
Speaker 5:  you've gotta fix that realtime search can do that. Zuckerberg hinted to me

913
00:55:22,805 --> 00:55:26,525
Speaker 5:  that Google's building a model, and I don't mean AI model, but I think a

914
00:55:26,725 --> 00:55:30,525
Speaker 5:  business model around this, around licensing search to these chatbots.

915
00:55:30,545 --> 00:55:34,445
Speaker 5:  So it makes sense because that is Google's bread and butter

916
00:55:34,655 --> 00:55:38,325
Speaker 5:  still, I thought that Google would keep that for Gemini,

917
00:55:38,395 --> 00:55:42,045
Speaker 5:  that that would make their chat bot experience, you know, more

918
00:55:42,045 --> 00:55:45,925
Speaker 5:  unique and their leverage there. But I think this means that we

919
00:55:45,945 --> 00:55:48,485
Speaker 5:  may start seeing them license it elsewhere as well.

920
00:55:48,825 --> 00:55:52,805
Speaker 2:  That's fascinating. A sucks for perplexity. You had a good

921
00:55:52,885 --> 00:55:56,565
Speaker 2:  run, perplexity great job. But b, the, the tension there

922
00:55:56,595 --> 00:55:59,325
Speaker 2:  just for Google, this is a total aside, but the tension there for Google

923
00:55:59,325 --> 00:56:03,005
Speaker 2:  between like search has made it all of the money in the known universe and

924
00:56:03,075 --> 00:56:06,885
Speaker 2:  also they're betting a lot of that search money on AI working. But then you

925
00:56:06,885 --> 00:56:10,845
Speaker 2:  look at this and there is just a giant money faucet licensing this data to

926
00:56:11,435 --> 00:56:15,405
Speaker 2:  chat bots all of whom would want it and pay for it. Fascinating. The

927
00:56:15,405 --> 00:56:19,285
Speaker 2:  like the, the business machinations between those two things are

928
00:56:19,285 --> 00:56:21,685
Speaker 2:  going to be really interesting to watch in the next couple of years.

929
00:56:22,045 --> 00:56:25,965
Speaker 5:  I agree. I I'm eager to hear Google maybe talk about that more at IO

930
00:56:25,965 --> 00:56:26,245
Speaker 5:  soon.

931
00:56:26,625 --> 00:56:30,565
Speaker 2:  So big picture as, as the person who broke the story of

932
00:56:31,045 --> 00:56:34,765
Speaker 2:  Facebook renaming to Meta, I'm curious how you feel about Meta as a metaverse

933
00:56:34,765 --> 00:56:38,085
Speaker 2:  company at this moment in time. Like there is part of me that is like rewind

934
00:56:38,365 --> 00:56:42,325
Speaker 2:  whatever, five years and knowing what he knows now, mark would've renamed

935
00:56:42,325 --> 00:56:46,165
Speaker 2:  the company like artificial or something instead of meta. Like is

936
00:56:46,165 --> 00:56:49,165
Speaker 2:  the Metaverse still the thing when you talk to Meta and Mark?

937
00:56:49,665 --> 00:56:53,485
Speaker 5:  No, I mean it's not the thing they want to talk about the most, but I

938
00:56:53,485 --> 00:56:56,925
Speaker 5:  don't know man, I, I think you can walk on chew gum at the same time and

939
00:56:57,805 --> 00:57:01,725
Speaker 5:  I still think 10 years out they see more of their

940
00:57:01,725 --> 00:57:05,165
Speaker 5:  optionality on the future being on the Metaverse stuff because okay,

941
00:57:05,235 --> 00:57:09,005
Speaker 5:  they've hit a plateau with Facebook, it's not getting young people back

942
00:57:09,495 --> 00:57:13,125
Speaker 5:  eventually and they talk about this pretty frankly internally, literally

943
00:57:13,365 --> 00:57:15,685
Speaker 5:  everyone's just gonna die, right? Because like if you're not getting new

944
00:57:15,685 --> 00:57:18,365
Speaker 5:  users on your platform, your platform's in terminal decline,

945
00:57:19,285 --> 00:57:22,645
Speaker 5:  Instagram is their next Facebook, right? It will be,

946
00:57:23,015 --> 00:57:26,005
Speaker 5:  we're all gonna be on there in probably 10 years and it's gonna feel just

947
00:57:26,005 --> 00:57:29,845
Speaker 5:  like Facebook does now to the younger generation, whether they can buy the

948
00:57:29,845 --> 00:57:33,685
Speaker 5:  next thing to keep that whole cycle going 40 years from now, TBD

949
00:57:34,485 --> 00:57:38,125
Speaker 5:  unlikely, but this whole push to control the next computing

950
00:57:38,405 --> 00:57:42,365
Speaker 5:  platform and or at least have more say in it so that Apple just doesn't

951
00:57:42,365 --> 00:57:45,605
Speaker 5:  invent the whole next, you know, headset modality. I think it still makes

952
00:57:45,605 --> 00:57:49,485
Speaker 5:  sense. I mean, I don't know, I've been playing with the The Ray band smart

953
00:57:49,485 --> 00:57:52,845
Speaker 5:  classes with multimodal AI on them. You have it too, right?

954
00:57:53,745 --> 00:57:57,405
Speaker 5:  And you know, it's not great. It's wrong

955
00:57:57,515 --> 00:58:00,925
Speaker 5:  half of the time. I will say I'm impressed with the speed of it, but you

956
00:58:00,925 --> 00:58:04,285
Speaker 5:  know, it's just makes stuff up still a lot. Yeah. But you can see when they

957
00:58:04,285 --> 00:58:07,845
Speaker 5:  iron out these kinks like, oh the, the basis of this makes sense. Like

958
00:58:08,245 --> 00:58:11,685
Speaker 5:  I actually do see over time as the tech gets figured out, a lot of people

959
00:58:11,685 --> 00:58:15,445
Speaker 5:  wanting to use technology more through their eyes

960
00:58:15,505 --> 00:58:18,525
Speaker 5:  and through their kind of egocentric view of the world. And you're covering

961
00:58:18,525 --> 00:58:21,645
Speaker 5:  this a lot in the wearable AI side with you know, humane and rabbit and all

962
00:58:21,645 --> 00:58:25,405
Speaker 5:  that. But this is all kind of converging over time. This generative AI

963
00:58:25,535 --> 00:58:29,285
Speaker 5:  meets wearables into what I think is a pretty compelling next

964
00:58:29,285 --> 00:58:33,085
Speaker 5:  computing platform. And if anything I think meta feels like

965
00:58:33,425 --> 00:58:37,205
Speaker 5:  AI has only kind of made that more apparent and

966
00:58:37,845 --> 00:58:41,765
Speaker 5:  actually pushed off the need to have the whole display stuff that we

967
00:58:41,765 --> 00:58:44,765
Speaker 5:  always hear about that we're gonna have all these holograms around us and

968
00:58:44,765 --> 00:58:47,605
Speaker 5:  it just keeps getting pushed out and out and out. And now maybe they think

969
00:58:47,605 --> 00:58:50,965
Speaker 5:  like, oh maybe all we really need for the next five-ish years or so is

970
00:58:51,045 --> 00:58:54,965
Speaker 5:  compelling AI in a wearable form factor. So yeah, I mean all these big tech

971
00:58:55,205 --> 00:58:59,005
Speaker 5:  companies, they over rotate on the topic de jour and that's generative AI

972
00:58:59,005 --> 00:59:02,725
Speaker 5:  right now. I don't think they're changing the company name again, I think

973
00:59:02,755 --> 00:59:06,685
Speaker 5:  they're still investing a lot of money in reality labs and will have, yeah,

974
00:59:07,245 --> 00:59:11,125
Speaker 5:  a pretty wild product next year that I've scooped, which is The Ray bands

975
00:59:11,125 --> 00:59:15,005
Speaker 5:  with a display and a bracelet that does right EMG kind of neural

976
00:59:15,365 --> 00:59:19,125
Speaker 5:  interface control. So imagine like typing and clicking and

977
00:59:19,125 --> 00:59:23,015
Speaker 5:  pointing with I guess just basically thinking. So we're about to hit

978
00:59:23,055 --> 00:59:26,895
Speaker 5:  a lot of really interesting new stuff there. It's,

979
00:59:26,895 --> 00:59:30,815
Speaker 5:  it's definitely like they, they over rotated on it like way too early, but

980
00:59:30,815 --> 00:59:34,455
Speaker 5:  that's like the story of meta they, they spent, you know, whatever they spent

981
00:59:34,455 --> 00:59:38,135
Speaker 5:  on Oculus 10 years ago and whereas it really gotten them in terms of ROI

982
00:59:38,615 --> 00:59:42,295
Speaker 5:  even now, so them like going too hard too early

983
00:59:42,475 --> 00:59:43,175
Speaker 5:  is nothing new.

984
00:59:44,165 --> 00:59:46,935
Speaker 2:  That is fair. That is what they do. And I will say to their credit, they,

985
00:59:46,965 --> 00:59:50,535
Speaker 2:  they both like built the way too early thing

986
00:59:50,955 --> 00:59:54,295
Speaker 2:  and the kind of right on time thing. Like the, I think the smart glasses

987
00:59:54,725 --> 00:59:58,615
Speaker 2:  were the correct move for now. The big push after the

988
00:59:58,615 --> 01:00:02,055
Speaker 2:  headset and the metaverse is like way too early, which is a classic Microsoft

989
01:00:02,055 --> 01:00:05,975
Speaker 2:  move, so good job again Z. But they're doing a surprisingly good job

990
01:00:05,975 --> 01:00:09,335
Speaker 2:  of both sort of running those in parallel and pushing them towards each other.

991
01:00:09,395 --> 01:00:12,615
Speaker 2:  And I think that is like, if you can keep that up and you can afford to keep

992
01:00:12,615 --> 01:00:15,975
Speaker 2:  that up until they actually hit each other, that's it. That ends up being

993
01:00:16,215 --> 01:00:19,295
Speaker 2:  a pretty powerful place to be. As opposed to somebody like Apple who now

994
01:00:19,315 --> 01:00:23,255
Speaker 2:  has this massively overpowered thing that it's gonna have to

995
01:00:23,255 --> 01:00:26,695
Speaker 2:  figure out how to like pull back into actual mainstream

996
01:00:27,055 --> 01:00:27,495
Speaker 2:  interests over

997
01:01:52,785 --> 01:01:56,585
Speaker 7:  browser is kind of like popping up constantly, even if you've selected it

998
01:01:56,645 --> 01:02:00,185
Speaker 7:  not as your default browser. Same thing with the cloud

999
01:02:00,335 --> 01:02:03,785
Speaker 7:  storage. And I wanted to know, number one, why do they think they can get

1000
01:02:03,785 --> 01:02:06,985
Speaker 7:  away with doing the exact same thing that got them in trouble in the nineties?

1001
01:02:07,145 --> 01:02:10,385
Speaker 7:  I mean, I understand the market share for their browser has dropped, but

1002
01:02:10,775 --> 01:02:14,465
Speaker 7:  it's still the same anti-competitive practice. And two, why

1003
01:02:14,465 --> 01:02:18,065
Speaker 7:  aren't we putting a stop to it legally? Anyway, thanks.

1004
01:02:18,255 --> 01:02:19,365
Speaker 7:  Hope you can answer the question.

1005
01:02:19,985 --> 01:02:23,445
Speaker 2:  All right. You ask for a Eli rant, you get a Eli rant, NELI Patel. Hello.

1006
01:02:23,905 --> 01:02:24,685
Speaker 8:  Hi, how's it going?

1007
01:02:24,985 --> 01:02:28,965
Speaker 2:  So we, we've talked sort of obliquely about Microsoft as part of all

1008
01:02:28,965 --> 01:02:32,925
Speaker 2:  of these big antitrust things, so I figured this is useful and to just hit

1009
01:02:33,155 --> 01:02:35,565
Speaker 2:  head on, what are your thoughts? What do you make of this question?

1010
01:02:36,165 --> 01:02:39,645
Speaker 8:  I dunno if you're gonna get so much. A legal rant from me is just a rant

1011
01:02:39,645 --> 01:02:42,245
Speaker 8:  about the fact that nothing can displace Chrome.

1012
01:02:43,595 --> 01:02:46,405
Speaker 8:  Like governments around the world have been trying to displace Chrome for

1013
01:02:46,985 --> 01:02:50,885
Speaker 8:  10 years more and, and nothing. They try works and I think they're

1014
01:02:50,885 --> 01:02:53,245
Speaker 8:  kind of at a point where they're like, I don't know, maybe Microsoft can

1015
01:02:53,245 --> 01:02:56,605
Speaker 8:  do some stuff and it doesn't work. And I, I think that

1016
01:02:57,445 --> 01:03:01,365
Speaker 8:  specifically with the Edge browser and the prompts to make edge the default

1017
01:03:01,365 --> 01:03:05,165
Speaker 8:  and all that stuff, I think the market is very clear that it hates it.

1018
01:03:05,385 --> 01:03:08,405
Speaker 8:  People don't like it whenever we write about it, people are like, I hate

1019
01:03:08,405 --> 01:03:12,285
Speaker 8:  this. This is stupid. And then it doesn't work anyway. And I suspect

1020
01:03:12,285 --> 01:03:14,365
Speaker 8:  that regulators in Europe and the United States are kinda looking at that

1021
01:03:14,365 --> 01:03:18,205
Speaker 8:  and being like, fine, it's just not working. Right? If you want to destroy

1022
01:03:18,205 --> 01:03:22,165
Speaker 8:  your own reputation, try to make this work. Like have at it. When I interviewed

1023
01:03:22,325 --> 01:03:25,725
Speaker 8:  Jonathan Cantor on decoder, the head of antitrust at Department of Justice,

1024
01:03:26,305 --> 01:03:30,125
Speaker 8:  he said his framework for decisions was hips. He's called it

1025
01:03:30,125 --> 01:03:34,005
Speaker 8:  hands on Hips. Okay, high impact programmatically significant, huh?

1026
01:03:34,025 --> 01:03:37,525
Speaker 8:  And I think you just look at that equation and you're like, this is low impact,

1027
01:03:37,615 --> 01:03:41,165
Speaker 8:  right? Because it's not working. Anyway, that said, Microsoft is not

1028
01:03:41,235 --> 01:03:45,005
Speaker 8:  getting away with it. Totally. They rolled out teams, they bundled teams

1029
01:03:45,115 --> 01:03:48,885
Speaker 8:  into office that basically put a ton of pressure on Slack. It forced

1030
01:03:48,895 --> 01:03:52,725
Speaker 8:  Slack into selling itself into Salesforce. The European regulators

1031
01:03:52,725 --> 01:03:55,885
Speaker 8:  looked at that very sternly and Microsoft

1032
01:03:56,275 --> 01:03:59,965
Speaker 8:  Unbundled teams from office and they have now done that

1033
01:04:00,205 --> 01:04:04,125
Speaker 8:  globally because I think they saw the pressure was rising globally. So there,

1034
01:04:04,245 --> 01:04:07,525
Speaker 8:  I think there is, I think there are some places where Microsoft is responding

1035
01:04:07,525 --> 01:04:11,445
Speaker 8:  to the pressure, but specifically when it comes to irritating

1036
01:04:11,685 --> 01:04:15,125
Speaker 8:  everyone around Edge, I think regulators are like, yeah, that shit doesn't

1037
01:04:15,125 --> 01:04:15,285
Speaker 8:  work.

1038
01:04:15,735 --> 01:04:19,445
Speaker 2:  Right. Yeah. My, my read on this was that there are interesting Microsoft

1039
01:04:19,615 --> 01:04:23,205
Speaker 2:  Antit trucks questions probably around Windows and

1040
01:04:23,525 --> 01:04:26,885
Speaker 2:  probably around office and especially as it shoves

1041
01:04:27,315 --> 01:04:30,485
Speaker 2:  copilot AI stuff into all of that, there might be interesting questions to

1042
01:04:30,485 --> 01:04:34,405
Speaker 2:  come up there, but you just cannot make the case that Edge is

1043
01:04:34,725 --> 01:04:38,645
Speaker 2:  powerful. Like you just, you just can't. And I, I think you're

1044
01:04:38,645 --> 01:04:42,445
Speaker 2:  right that everyone has been desperate for there to be a meaningful

1045
01:04:42,445 --> 01:04:46,045
Speaker 2:  competitor to Chrome for a really long time. Mozilla has been running around

1046
01:04:46,045 --> 01:04:49,205
Speaker 2:  saying, you know, Firefox is the thing, we're gonna do everything differently.

1047
01:04:49,465 --> 01:04:53,125
Speaker 2:  Do all these browsers now even run chromium. Like Mozilla is one of the few

1048
01:04:53,445 --> 01:04:56,925
Speaker 2:  companies that isn't using the blink engine with the Chromium project

1049
01:04:56,925 --> 01:05:00,845
Speaker 2:  underneath all of the browser stuff. Like Edge is just another

1050
01:05:00,955 --> 01:05:04,605
Speaker 2:  skin on the same stuff that Chrome runs. And so it's all

1051
01:05:04,825 --> 01:05:08,645
Speaker 2:  really mely and weird, but for everything that Microsoft is doing, edge

1052
01:05:08,645 --> 01:05:11,845
Speaker 2:  still has like teeny tiny market share. And I feel like if I'm a regulator,

1053
01:05:11,945 --> 01:05:15,765
Speaker 2:  I'm like, Aw, you're trying so hard this it, this would be sketchy if only

1054
01:05:15,765 --> 01:05:16,365
Speaker 2:  it were working.

1055
01:05:16,675 --> 01:05:19,925
Speaker 8:  Yeah, I mean the regulators in Europe basically imposed the same thing that

1056
01:05:20,125 --> 01:05:22,125
Speaker 8:  Microsoft is doing, which is like, would you like to try another browser?

1057
01:05:23,065 --> 01:05:27,005
Speaker 8:  And everyone was like, no. Yeah. And so Microsoft is Welcome To try

1058
01:05:27,005 --> 01:05:30,445
Speaker 8:  to decrease the market share of Chrome, which is the problem that those regulators

1059
01:05:30,445 --> 01:05:34,005
Speaker 8:  have been focused on. They're just not doing a good job. The other thing

1060
01:05:34,005 --> 01:05:37,765
Speaker 8:  that I think is interesting and kind of Vir Chasity is the reason Microsoft

1061
01:05:37,785 --> 01:05:41,445
Speaker 8:  was terrified of Netscape back then was they were very

1062
01:05:41,515 --> 01:05:45,485
Speaker 8:  worried that the application model for desktop computing would

1063
01:05:45,615 --> 01:05:49,205
Speaker 8:  leave the Wintel monopoly. So they were doing

1064
01:05:49,205 --> 01:05:52,445
Speaker 8:  anti-competitive stuff to protect that monopoly, protect the dominance of

1065
01:05:52,445 --> 01:05:56,165
Speaker 8:  Windows, make sure Windows was the place where all applications were

1066
01:05:56,445 --> 01:06:00,405
Speaker 8:  deployed and they saw correctly that deploying applications to

1067
01:06:00,405 --> 01:06:04,245
Speaker 8:  the web would break that model and that 100% happened.

1068
01:06:04,505 --> 01:06:08,325
Speaker 8:  Yes. So the fact that Edge runs chromium is because

1069
01:06:08,425 --> 01:06:12,165
Speaker 8:  if you want web apps to run well, it is better to standardize the

1070
01:06:12,165 --> 01:06:15,765
Speaker 8:  browser engines. They all work in the same place in the same way. And that's

1071
01:06:15,765 --> 01:06:19,565
Speaker 8:  how all apps are deployed now. And so I, it's just weird that like

1072
01:06:19,565 --> 01:06:23,205
Speaker 8:  we've come full circle, but actually the thing Microsoft was the most

1073
01:06:23,205 --> 01:06:27,125
Speaker 8:  worried about absolutely happened. And what the browser Chrome is

1074
01:06:27,195 --> 01:06:30,805
Speaker 8:  like the, the way the browser looks and works and what default

1075
01:06:30,805 --> 01:06:34,445
Speaker 8:  search service it uses versus what is the browser engine like that war is

1076
01:06:34,445 --> 01:06:34,645
Speaker 8:  lost.

1077
01:06:34,915 --> 01:06:38,445
Speaker 2:  Yeah. And I think, I mean it's, it's a weird place to be in because my next

1078
01:06:38,645 --> 01:06:42,365
Speaker 2:  question for you is gonna be, should we be talking more about Microsoft in

1079
01:06:42,365 --> 01:06:45,285
Speaker 2:  an antitrust way? Because again, it's either the first or second biggest

1080
01:06:45,285 --> 01:06:49,125
Speaker 2:  company on Earth. It is, it is clearly as powerful as anybody. It's just

1081
01:06:49,325 --> 01:06:53,165
Speaker 2:  so much more diversified in that way that it doesn't, it doesn't

1082
01:06:53,165 --> 01:06:57,045
Speaker 2:  have kind of the one thing that feels like it's thing the same way, but I

1083
01:06:57,045 --> 01:07:00,685
Speaker 2:  guess it would be Windows. But Windows has been completely commoditized.

1084
01:07:00,685 --> 01:07:04,245
Speaker 2:  Like I think desktop operating systems in general have been made totally

1085
01:07:04,245 --> 01:07:07,925
Speaker 2:  irrelevant by web browsers essentially. And the internet, like

1086
01:07:08,085 --> 01:07:11,605
Speaker 2:  most people, you buy a laptop, you download a browser, and that is, that

1087
01:07:11,605 --> 01:07:15,125
Speaker 2:  is your portal to everything else. And so maybe if I'm an antitrust regulator,

1088
01:07:15,125 --> 01:07:18,445
Speaker 2:  there's just no upside in caring about desktop operating systems anymore,

1089
01:07:18,515 --> 01:07:21,885
Speaker 2:  even though Microsoft is like by a mile the dominant player.

1090
01:07:22,265 --> 01:07:26,085
Speaker 8:  But there's a handful of things that are worth considering. But

1091
01:07:26,085 --> 01:07:29,045
Speaker 8:  even there you can see there's enough competition. So the one I would just

1092
01:07:29,045 --> 01:07:32,445
Speaker 8:  throw right back at your example is games. Sure. If you want to play video

1093
01:07:32,445 --> 01:07:36,245
Speaker 8:  games on a pc, you're gonna buy a Windows pc. Yeah. And you are really

1094
01:07:36,355 --> 01:07:40,285
Speaker 8:  reliant on Windows and all of its graphic support and the ecosystem of

1095
01:07:40,445 --> 01:07:42,125
Speaker 8:  graphics cards available for Windows. There

1096
01:07:42,125 --> 01:07:45,125
Speaker 2:  Are like three people with Steam running on their Macs who are like really

1097
01:07:45,125 --> 01:07:46,165
Speaker 2:  upset that you said that, but

1098
01:07:46,195 --> 01:07:48,765
Speaker 8:  Yeah, and they're, you're correct. They're running it at very low frameworks

1099
01:07:49,765 --> 01:07:53,165
Speaker 8:  compared to any anyone pc, right? Like, and so there's a little

1100
01:07:53,705 --> 01:07:57,045
Speaker 8:  market there that feels very tightly controlled, but then you look around

1101
01:07:57,065 --> 01:08:00,845
Speaker 8:  and you're like, oh, there's all these like cool handhelds that run

1102
01:08:01,165 --> 01:08:04,565
Speaker 8:  Linux and Steam, and you can just see, okay, there's like

1103
01:08:04,975 --> 01:08:08,885
Speaker 8:  meaningful, like the game developers themselves know they don't wanna

1104
01:08:08,885 --> 01:08:12,765
Speaker 8:  be totally stuck here forever. So they're creating new markets and

1105
01:08:12,765 --> 01:08:16,645
Speaker 8:  there's new distribution and Windows itself is not even Microsoft. Some

1106
01:08:16,645 --> 01:08:20,565
Speaker 8:  particular advantage in gaming Xbox, it's other big consumer gaming brand.

1107
01:08:20,995 --> 01:08:24,325
Speaker 8:  It's is not the winner. Like Phil Spencer will happily tell you they're not

1108
01:08:24,325 --> 01:08:28,085
Speaker 8:  the winner all day and all night. So I I, I think it's

1109
01:08:28,085 --> 01:08:31,445
Speaker 8:  hard for a regulator to point at some big consumer harm in gaming.

1110
01:08:31,875 --> 01:08:35,205
Speaker 8:  Like they couldn't even make the case to keep Microsoft from my Activision.

1111
01:08:35,345 --> 01:08:38,805
Speaker 8:  So, so I think that's challenging and plus you, you know, that space does

1112
01:08:38,805 --> 01:08:42,045
Speaker 8:  feel competitive and then everything else kind of doesn't touch consumers

1113
01:08:42,045 --> 01:08:45,805
Speaker 8:  that Microsoft does Slack and teams and it

1114
01:08:45,865 --> 01:08:49,485
Speaker 8:  got the scrutiny and they had to stop bundling teams. What else is there?

1115
01:08:49,765 --> 01:08:53,445
Speaker 8:  Everything else, all of Microsoft's other business is a bunch of enterprise

1116
01:08:53,845 --> 01:08:57,485
Speaker 8:  business. It's Azure, it's cloud computing services, it's

1117
01:08:57,685 --> 01:09:01,605
Speaker 8:  security to some shaky extent, like that's competitive now. Yeah. I think

1118
01:09:01,605 --> 01:09:05,565
Speaker 8:  it's hard to put together the political case. Like here's a real

1119
01:09:05,565 --> 01:09:08,325
Speaker 8:  problem that people are complaining about unless it's something a can consumer

1120
01:09:08,345 --> 01:09:10,005
Speaker 8:  can see, or like lots of people can see

1121
01:09:10,505 --> 01:09:14,325
Speaker 2:  In the way that like blue and green bubbles is like, it has that like visceral

1122
01:09:14,335 --> 01:09:17,245
Speaker 2:  thing that you can point to. Microsoft doesn't have any of those. Yeah.

1123
01:09:17,245 --> 01:09:20,845
Speaker 8:  And then when it does it, it's so blatantly annoying. Like, would you like

1124
01:09:20,845 --> 01:09:23,485
Speaker 8:  to try Edge that? Like the market just sort of corrects for it? Yeah. By

1125
01:09:23,485 --> 01:09:26,365
Speaker 8:  being like, no, I will now I will never do this.

1126
01:09:27,465 --> 01:09:28,005
Speaker 8:  And that's fine.

1127
01:09:28,555 --> 01:09:31,845
Speaker 2:  Yeah. So you don't, you don't necessarily think any of this is even necessarily

1128
01:09:31,845 --> 01:09:35,005
Speaker 2:  coming for Microsoft. Like I think the, the two I've been waiting for are

1129
01:09:35,235 --> 01:09:38,325
Speaker 2:  YouTube scrutiny and Microsoft scrutiny and I think YouTube scrutiny is probably

1130
01:09:38,325 --> 01:09:38,645
Speaker 2:  still coming.

1131
01:09:38,875 --> 01:09:41,125
Speaker 8:  I'll, I'll take the opposite. I'll take the other end of that bet. Oh,

1132
01:09:41,125 --> 01:09:41,805
Speaker 2:  Okay. I,

1133
01:09:41,805 --> 01:09:44,925
Speaker 8:  YouTube is just a blind spot for everyone. We, we treat it like the ocean.

1134
01:09:45,555 --> 01:09:46,285
Speaker 8:  Like there it is

1135
01:09:47,845 --> 01:09:50,725
Speaker 8:  forever changing and always the same. Powerful.

1136
01:09:51,065 --> 01:09:53,485
Speaker 2:  It will be there long after we're all dead. Yeah. Yeah.

1137
01:09:53,485 --> 01:09:57,125
Speaker 8:  It's like there, there lies the ocean. It's like who knows

1138
01:09:57,195 --> 01:10:00,965
Speaker 8:  what, what vast mysteries exist in the deep, it's an

1139
01:10:01,245 --> 01:10:04,405
Speaker 8:  absolute blind spot for everyone. It, no one treats like a business. Microsoft,

1140
01:10:04,765 --> 01:10:08,205
Speaker 8:  I think their deal with OpenAI, how they will integrate those

1141
01:10:08,325 --> 01:10:12,205
Speaker 8:  products, the exclusivity around that deal, whether it's a, whether

1142
01:10:12,205 --> 01:10:15,965
Speaker 8:  they sort of like acquired OpenAI without acquiring it. I think

1143
01:10:15,965 --> 01:10:19,045
Speaker 8:  there's a lot of scrutiny left to come there, particularly as that market

1144
01:10:19,135 --> 01:10:22,645
Speaker 8:  heats up. But I, I dunno what shape it will take, but I think people will

1145
01:10:22,655 --> 01:10:26,525
Speaker 8:  understand that that's weird and there's money in power and characters over

1146
01:10:26,525 --> 01:10:30,125
Speaker 8:  there before anyone even tries

1147
01:10:30,465 --> 01:10:31,485
Speaker 8:  to understand YouTube.

1148
01:10:31,965 --> 01:10:35,765
Speaker 2:  Hmm. That's fair. And that is to your point, that like directly

1149
01:10:36,115 --> 01:10:40,005
Speaker 2:  regular human relevant thing. Like yeah. If, if you want, if you

1150
01:10:40,005 --> 01:10:42,885
Speaker 2:  wanna start talking about what happens to chat GPT, like now you're back

1151
01:10:43,085 --> 01:10:46,405
Speaker 2:  squarely in regular consumer territory, which is where that goes.

1152
01:10:46,665 --> 01:10:50,445
Speaker 8:  Yes. Did Microsoft illegally invest in whatever, blah, blah, blah with

1153
01:10:50,445 --> 01:10:54,325
Speaker 8:  open AI to make these guys rich? Sure

1154
01:10:54,325 --> 01:10:57,445
Speaker 8:  you can tell that story. It's, I think it's very hard to be like, we're gonna

1155
01:10:57,445 --> 01:10:57,965
Speaker 8:  mess with YouTube.

1156
01:10:58,275 --> 01:11:01,405
Speaker 2:  It's just the ocean. That, that might be the best metaphor for YouTube I've

1157
01:11:01,405 --> 01:11:04,205
Speaker 2:  ever heard. I, I very much enjoy that. That's not what we came here to do.

1158
01:11:04,205 --> 01:11:05,325
Speaker 2:  But I'm glad that's really went

1159
01:11:06,245 --> 01:11:09,925
Speaker 8:  I one day someone's gonna be like, huh, YouTube's a company and

1160
01:11:10,175 --> 01:11:11,685
Speaker 8:  we'll see what happens after that.

1161
01:11:12,315 --> 01:11:15,405
Speaker 2:  Yeah. Fair enough. Alright, Josh, I hope that helps, Eli. Thank you.

1162
01:11:15,665 --> 01:11:16,085
Speaker 3:  Thanks.

1163
01:11:17,275 --> 01:11:19,925
Speaker 2:  Alright, that's it for The Vergecast today. Thanks to everybody who came

1164
01:11:19,925 --> 01:11:22,685
Speaker 2:  on the show, and thank you as always for listening. There's lots more on

1165
01:11:22,685 --> 01:11:26,405
Speaker 2:  everything we talked about from Rabbit to Allison's experiment, to

1166
01:11:26,785 --> 01:11:30,245
Speaker 2:  Alex's coverage of all things going on at Meta at The Verge dot com. We'll

1167
01:11:30,245 --> 01:11:33,765
Speaker 2:  put lots of links in the show notes. But as always, read The Verge dot com.

1168
01:11:33,795 --> 01:11:37,205
Speaker 2:  It's a good website. If you have thoughts, questions, feelings, or other

1169
01:11:37,385 --> 01:11:40,725
Speaker 2:  AI gadgets you want me to try, you can always email us at Vergecast at The

1170
01:11:40,845 --> 01:11:44,565
Speaker 2:  Verge dot com. Call the hotline, eight six six VERGE one, one. Thanks again

1171
01:11:44,565 --> 01:11:47,485
Speaker 2:  to everybody who voted for us for the webs. We're still riding high on the

1172
01:11:47,485 --> 01:11:51,365
Speaker 2:  victory there, thrilled that you all listen to and like this show. It means

1173
01:11:51,385 --> 01:11:54,605
Speaker 2:  the world to us. Send us all your thoughts and questions, ideas, everything.

1174
01:11:54,605 --> 01:11:56,885
Speaker 2:  We wanna hear them all. Call the hotline, send an email. we love hearing

1175
01:11:56,885 --> 01:12:00,165
Speaker 2:  from you. This show is produced by Angel Marino, Liam James, and Will Poor

1176
01:12:00,185 --> 01:12:02,925
Speaker 2:  The. Vergecast is VERGE production part of the Vox Media podcast network.

1177
01:12:03,275 --> 01:12:06,885
Speaker 2:  Neli, Alex and I will be back on Friday to talk about presumably more AI

1178
01:12:06,885 --> 01:12:10,325
Speaker 2:  gadgets, some iPads coming up next week and everything else going on in tech.

1179
01:12:10,455 --> 01:12:11,805
Speaker 2:  We'll see you then. Rock and roll.

