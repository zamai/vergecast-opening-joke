1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 52008a50-597b-11ed-82bf-5321e0c0f2e5
Status: Done
Stage: Done
Title: Cybersecurity Hotline Special
Audio URL: https://jfe93e.s3.amazonaws.com/-35526882113895854/3248333836622212411/s93290-US-3124s-1667262104.mp3
Description: We asked listeners to send in all their questions related to cybersecurity for this special Vergecast Hotline episode. David Pierce talks to Nilay Patel and Russell Brandom to get you the best advice for staying safe online.

Email us at vergecast@theverge.com or call us at 866-VERGE11, we'd love to hear from you.
We are conducting a short audience survey to help plan for our future and hear from you. To participate, head toÂ vox.com/podsurvey, and thank you!
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Disabled

2
00:00:00,060 --> 00:00:04,020
Speaker 1:  Support for this episode comes from Zelle. You'd never

3
00:00:04,020 --> 00:00:07,820
Speaker 1:  fall for an online scam, right? You use

4
00:00:07,820 --> 00:00:11,460
Speaker 1:  two factor authentication, ignore calls from everyone named

5
00:00:11,690 --> 00:00:14,660
Speaker 1:  spam risk and never use the password.

6
00:00:15,740 --> 00:00:15,900
Speaker 1:  Password.

7
00:00:17,480 --> 00:00:21,260
Speaker 1:  But scammers are getting more sophisticated and more active,

8
00:00:21,260 --> 00:00:24,900
Speaker 1:  which means they're finding millions of new victims every

9
00:00:24,900 --> 00:00:28,740
Speaker 1:  single year. The good news is that there's a lot you can do to

10
00:00:28,740 --> 00:00:32,700
Speaker 1:  protect yourself on the wild, wild web. For starters,

11
00:00:32,740 --> 00:00:36,420
Speaker 1:  Zelle wants to remind you only send money to people you know

12
00:00:36,440 --> 00:00:40,420
Speaker 1:  and trust. Zelle is available to United States bank account

13
00:00:40,580 --> 00:00:42,420
Speaker 1:  holders only terms and conditions apply.

14
00:00:55,470 --> 00:00:56,140
Speaker 2:  Welcome

15
00:00:56,140 --> 00:01:00,060
Speaker 3:  To the Verge Cast, the flagship podcast of turning off wifi

16
00:01:00,110 --> 00:01:04,020
Speaker 3:  on all of your devices and putting on tinfoil hats. I'm your friend David Pierce.

17
00:01:04,020 --> 00:01:05,780
Speaker 3:  Russell Brando is here. Hi Russell.

18
00:01:06,350 --> 00:01:06,700
Speaker 4:  Hi.

19
00:01:07,040 --> 00:01:10,820
Speaker 3:  And this is a, this is very special Vergecast episode. This is,

20
00:01:10,830 --> 00:01:14,380
Speaker 3:  is this the second week of cybersecurity week Russell? Is this like infrastructure

21
00:01:14,380 --> 00:01:15,460
Speaker 3:  week? It's kind of every week. I,

22
00:01:15,460 --> 00:01:18,740
Speaker 4:  Well, I, it's hard because for me every week is cybersecurity week. That's

23
00:01:18,740 --> 00:01:18,860
Speaker 3:  Fair.

24
00:01:19,040 --> 00:01:20,380
Speaker 4:  And it should be for you too,

25
00:01:21,280 --> 00:01:25,140
Speaker 3:  But here, here on the Verge cast today is cybersecurity week, I

26
00:01:25,140 --> 00:01:25,300
Speaker 3:  guess.

27
00:01:25,300 --> 00:01:26,300
Speaker 4:  Yeah, yeah, exactly.

28
00:01:26,880 --> 00:01:30,180
Speaker 3:  And we were thinking about a bunch of things to do and we decided that the

29
00:01:30,180 --> 00:01:34,020
Speaker 3:  best thing we could do is just see what everyone wanted

30
00:01:34,020 --> 00:01:36,540
Speaker 3:  to know about. So we opened up the Verge Cast hotline. We got a bunch of

31
00:01:36,540 --> 00:01:38,740
Speaker 3:  really great questions. You and I have been doing a bunch of research in

32
00:01:38,740 --> 00:01:42,020
Speaker 3:  prep and talking to folks, and we are gonna just roll through some of the

33
00:01:42,380 --> 00:01:45,740
Speaker 3:  most interesting and popular questions we got. It's gonna be fun. I'm, I'm

34
00:01:45,740 --> 00:01:48,460
Speaker 3:  very excited. I have been out some weird rabbit holes in getting ready for

35
00:01:48,460 --> 00:01:50,820
Speaker 3:  this that I did not expect. I don't know how it's been for you, but it's,

36
00:01:50,820 --> 00:01:52,220
Speaker 3:  I'm in a weird corner of the internet

37
00:01:52,220 --> 00:01:55,340
Speaker 4:  Now. Oh yeah. I've, I've learned about myself and cybersecurity.

38
00:01:55,340 --> 00:01:58,260
Speaker 3:  It's beautiful. Isn't that, It's really the same thing if you think about

39
00:01:58,260 --> 00:01:58,420
Speaker 3:  it.

40
00:01:58,840 --> 00:02:01,330
Speaker 4:  It's really wonder. No, no, they're, they're different.

41
00:02:02,090 --> 00:02:04,970
Speaker 3:  Alright, let's get to our first question, which I think let's just start

42
00:02:04,970 --> 00:02:08,410
Speaker 3:  with, I think the one that is actually in a strange way, the easiest to answer,

43
00:02:08,410 --> 00:02:09,930
Speaker 3:  which is from Alex.

44
00:02:10,590 --> 00:02:13,650
Speaker 5:  Hey there, this is Alex from Madison, Wisconsin.

45
00:02:14,430 --> 00:02:17,530
Speaker 5:  I'm walking, walking my dog in the Bri Wisconsin

46
00:02:17,860 --> 00:02:21,410
Speaker 5:  evening. And I had a question about a

47
00:02:21,580 --> 00:02:25,330
Speaker 5:  browser malware extension. So I

48
00:02:25,530 --> 00:02:28,970
Speaker 5:  recently cleaned my computer of something called Bundle

49
00:02:29,790 --> 00:02:33,450
Speaker 5:  and it was really weird. It, it sort of, it,

50
00:02:33,460 --> 00:02:37,290
Speaker 5:  it hid itself by making it so that whenever I tried to

51
00:02:37,290 --> 00:02:41,010
Speaker 5:  access the page to manage my extensions, it just wouldn't go there. It was

52
00:02:41,010 --> 00:02:43,010
Speaker 5:  like redirect the traffic. What I have is

53
00:02:44,610 --> 00:02:48,370
Speaker 5:  whenever I tried to search for something in the like

54
00:02:48,440 --> 00:02:52,010
Speaker 5:  Chrome search bar, it would redirect my search to Bing.

55
00:02:52,070 --> 00:02:55,970
Speaker 5:  Why would I do that? What, what benefit does some malware program have

56
00:02:56,100 --> 00:02:59,920
Speaker 5:  of redirecting my search through Bing instead of Google also who

57
00:02:59,920 --> 00:03:01,750
Speaker 5:  uses Bing? Thank you.

58
00:03:02,520 --> 00:03:03,910
Speaker 4:  Malware victims. There's

59
00:03:04,070 --> 00:03:07,870
Speaker 3:  Apparently good questions in here. Yeah. Who uses Bing is no one on purpose,

60
00:03:08,570 --> 00:03:12,350
Speaker 3:  but sometimes it happens. We, we've all been there. You just end up on Bing.

61
00:03:12,410 --> 00:03:14,950
Speaker 3:  You don't know how you got there. You don't, you don't know where you are,

62
00:03:14,950 --> 00:03:18,750
Speaker 3:  but it's dark outside and you're, you're bleeding from

63
00:03:18,750 --> 00:03:20,790
Speaker 3:  somewhere and you don't know.

64
00:03:22,500 --> 00:03:26,190
Speaker 4:  Yeah. So I, there's a lot to dig into here. I will say the thing that's,

65
00:03:26,340 --> 00:03:30,270
Speaker 4:  that that I was kind of coming around to listening to this is

66
00:03:30,620 --> 00:03:34,190
Speaker 4:  a lot of people being savvy about security

67
00:03:34,800 --> 00:03:38,790
Speaker 4:  is sort of knowing implicitly what to trust. So it's like,

68
00:03:38,790 --> 00:03:42,470
Speaker 4:  is it sketchy if I, you know, I'm reading

69
00:03:42,480 --> 00:03:46,470
Speaker 4:  my credit card number out over the phone. Yep. And it's like, well if

70
00:03:46,470 --> 00:03:49,870
Speaker 4:  you called the restaurant and their thing is down, like that's not actually

71
00:03:49,870 --> 00:03:52,550
Speaker 4:  that weird. Like you're probably okay, it's okay to put your credit card

72
00:03:52,550 --> 00:03:56,350
Speaker 4:  number into amazon.com. Like it's fine. And, and then like

73
00:03:56,350 --> 00:03:59,230
Speaker 4:  there's other things where I got a weird email and lots of stuff is misspelled.

74
00:03:59,230 --> 00:04:02,630
Speaker 4:  It's like that's obviously sketchy. You should, you should be careful I think

75
00:04:03,040 --> 00:04:06,830
Speaker 4:  browser extensions and particularly bundled browser

76
00:04:06,830 --> 00:04:10,740
Speaker 4:  extensions cuz he mentioned that he got this bundle. People who work

77
00:04:10,740 --> 00:04:13,620
Speaker 4:  in this stuff have known for a while that this is a real

78
00:04:14,250 --> 00:04:18,060
Speaker 4:  sort of place where scams happen and just, Yep. It's a

79
00:04:18,060 --> 00:04:21,780
Speaker 4:  mess in there and you shouldn't trust anything. But I don't know if

80
00:04:21,780 --> 00:04:25,420
Speaker 4:  the average user has really gotten around to like

81
00:04:26,320 --> 00:04:30,180
Speaker 4:  the vast majority of browser extensions are

82
00:04:30,580 --> 00:04:32,660
Speaker 4:  actively exploited and harmful and bad.

83
00:04:32,770 --> 00:04:36,700
Speaker 3:  Well I'm not, I vast majority might be slightly overstated, but it

84
00:04:36,700 --> 00:04:37,340
Speaker 3:  is. There's

85
00:04:37,340 --> 00:04:39,940
Speaker 4:  A lot of, there's a lot of crap. There's a lot of crap I guess.

86
00:04:40,010 --> 00:04:43,460
Speaker 3:  Yeah. Well and it's, and it's tricky because like over the years

87
00:04:43,610 --> 00:04:47,340
Speaker 3:  Google Chrome in particular has sort of devoted more and more

88
00:04:47,540 --> 00:04:51,380
Speaker 3:  resources to fighting that. But even still, it's basically like an

89
00:04:51,380 --> 00:04:54,980
Speaker 3:  automated system. And the thing that's crazy about Chrome extensions and

90
00:04:54,980 --> 00:04:58,940
Speaker 3:  browser extensions in general is when you install them, most of

91
00:04:58,940 --> 00:05:02,820
Speaker 3:  them require you to give them access to everything you do inside of your

92
00:05:02,820 --> 00:05:06,140
Speaker 3:  browser. Like in order to do the very basic thing that they want to do,

93
00:05:06,480 --> 00:05:10,340
Speaker 3:  you have to, because there's no like middle setting in many cases

94
00:05:10,340 --> 00:05:14,140
Speaker 3:  you have to turn on like Sure. Read later app. You can see

95
00:05:14,140 --> 00:05:18,100
Speaker 3:  every single thing I do in my browser and that's just bonkers and it's

96
00:05:18,100 --> 00:05:19,580
Speaker 3:  like systemically, this

97
00:05:19,580 --> 00:05:23,260
Speaker 4:  Is not like you can replace what I'm seeing

98
00:05:23,270 --> 00:05:27,140
Speaker 4:  on the screen with other stuff. Yes. So you know that there, like the

99
00:05:27,140 --> 00:05:30,780
Speaker 4:  classic joke extension is like, we made it so it replaces the word

100
00:05:31,020 --> 00:05:34,620
Speaker 4:  millennial with snake people. Like, haha isn't that funny? It's like, no,

101
00:05:34,620 --> 00:05:38,500
Speaker 4:  this is, you're looking at the website and what you're seeing is

102
00:05:38,500 --> 00:05:42,460
Speaker 4:  something different from what the website is trying to show you. Like do

103
00:05:42,460 --> 00:05:44,700
Speaker 4:  you not see how this is potentially a problem?

104
00:05:45,110 --> 00:05:48,860
Speaker 3:  We had one of those when I was at Wired, this was during the 2016 election

105
00:05:48,880 --> 00:05:52,820
Speaker 3:  and somebody had downloaded a bunch of like jokey

106
00:05:52,820 --> 00:05:56,700
Speaker 3:  political extensions onto their computer and one of them replaced every

107
00:05:56,700 --> 00:05:59,920
Speaker 3:  mention of Donald Trump with the phrase Tiny Hands. And,

108
00:06:01,060 --> 00:06:03,560
Speaker 3:  and it's like a very funny joke cuz you're reading the internet. But then

109
00:06:03,560 --> 00:06:06,680
Speaker 3:  they filed a story about Donald Trump and

110
00:06:07,150 --> 00:06:10,800
Speaker 3:  missed one of the times where the browser had changed like in

111
00:06:10,800 --> 00:06:14,640
Speaker 3:  our CMS to Tiny Hands. So we published a story in which

112
00:06:14,870 --> 00:06:18,680
Speaker 3:  this person had typed Donald Trump and it spit out tiny

113
00:06:18,680 --> 00:06:20,680
Speaker 3:  hands. It was really, it was really something special

114
00:06:21,180 --> 00:06:24,450
Speaker 4:  Man. Yeah. Okay, so so

115
00:06:24,880 --> 00:06:28,530
Speaker 4:  what he said bundle this is I, I did wanna, I want to see on this too

116
00:06:28,530 --> 00:06:32,410
Speaker 4:  because a lot of what happens is you were talking about Chrome.

117
00:06:32,460 --> 00:06:35,790
Speaker 4:  So like Chrome's a very popular browser as you know,

118
00:06:36,910 --> 00:06:40,790
Speaker 4:  it's free and a calm and scammy thing is people will bundle

119
00:06:40,820 --> 00:06:44,750
Speaker 4:  free software with this predatory unwanted

120
00:06:44,950 --> 00:06:48,750
Speaker 4:  software if we're allowed to call it malware, spyware, adware.

121
00:06:48,920 --> 00:06:52,110
Speaker 4:  It like gets into weird legal territory. Sure. But basically there's the

122
00:06:52,270 --> 00:06:55,790
Speaker 4:  software you want and you should just get that. But instead you're getting

123
00:06:55,790 --> 00:06:59,710
Speaker 4:  this bundle that has a bunch of other bad things in it. And

124
00:06:59,710 --> 00:07:03,670
Speaker 4:  that happens very often with browsers. I suspect that what

125
00:07:03,670 --> 00:07:07,470
Speaker 4:  is happening here, the reason it's sending him to Bing is there is

126
00:07:07,470 --> 00:07:11,370
Speaker 4:  some, I don't think Microsoft is purposefully

127
00:07:11,370 --> 00:07:15,090
Speaker 4:  paying this like adware company. Like I don't think that

128
00:07:15,090 --> 00:07:18,610
Speaker 4:  that's what's happening. But it is broadly true

129
00:07:18,880 --> 00:07:22,200
Speaker 4:  that like Google pays Mozilla

130
00:07:23,010 --> 00:07:26,960
Speaker 4:  to have the default Firefox search go to google.com

131
00:07:27,090 --> 00:07:30,800
Speaker 4:  because Google makes money through advertising when people search things

132
00:07:30,800 --> 00:07:34,280
Speaker 4:  on google.com. And so for a while they were paying

133
00:07:34,560 --> 00:07:38,040
Speaker 4:  Apple, I mean they're still paying Apple, right? For, for the iPhone searches

134
00:07:38,040 --> 00:07:41,880
Speaker 4:  that direct to google.com billions of dollars. So

135
00:07:41,880 --> 00:07:45,600
Speaker 4:  I suspect that one way or another that

136
00:07:45,600 --> 00:07:49,400
Speaker 4:  money is going from being to

137
00:07:49,400 --> 00:07:53,320
Speaker 4:  these scammy people with lots of intermediaries that make it

138
00:07:53,320 --> 00:07:56,880
Speaker 4:  hard for Bing to know who exactly they're paying. Yeah,

139
00:07:56,880 --> 00:07:59,880
Speaker 3:  I think that's right. And that like, to be clear, all of that can be done

140
00:07:59,880 --> 00:08:03,640
Speaker 3:  without Microsoft doing anything nefarious or even anything at all.

141
00:08:03,640 --> 00:08:06,440
Speaker 3:  Like I think it's very unlikely that Microsoft is like loading adware into

142
00:08:06,440 --> 00:08:09,960
Speaker 3:  Chrome extensions. It'd be amazing if that were true, but I mean it'd be

143
00:08:09,960 --> 00:08:10,200
Speaker 3:  amazing.

144
00:08:10,550 --> 00:08:14,240
Speaker 4:  I think it's, I think it's probably a Microsoft executive who had had too

145
00:08:14,240 --> 00:08:18,080
Speaker 4:  much and sort of went joker mode and then they're like, we don't know

146
00:08:18,080 --> 00:08:18,600
Speaker 4:  what happened to

147
00:08:18,600 --> 00:08:20,280
Speaker 3:  Him. Just call it bundle, leave it alone. He

148
00:08:20,280 --> 00:08:22,200
Speaker 4:  Just makes, he just makes it ad where now.

149
00:08:22,450 --> 00:08:26,200
Speaker 3:  No, but it is one of the main things that these extensions

150
00:08:26,450 --> 00:08:30,000
Speaker 3:  do is they will insert ads onto

151
00:08:30,070 --> 00:08:33,800
Speaker 3:  websites in such a way that if you then click those ads,

152
00:08:34,030 --> 00:08:37,280
Speaker 3:  they make money. Right. So like the main two things, a browser extension

153
00:08:37,280 --> 00:08:41,120
Speaker 3:  that you don't want will do is collect all of your data and sell it to somebody

154
00:08:41,530 --> 00:08:45,440
Speaker 3:  or put ads into your experience or replace the ads you're

155
00:08:45,440 --> 00:08:49,360
Speaker 3:  seeing with other ads and make money that way. So it's

156
00:08:49,360 --> 00:08:52,880
Speaker 3:  like 98% of the time it's like if some hack is happening on the internet,

157
00:08:52,880 --> 00:08:55,040
Speaker 3:  it's somebody trying to find like a cheap way to make money, right? Like,

158
00:08:55,120 --> 00:08:58,880
Speaker 3:  like that's usually what this is. Yeah. And in in this case, that is kind

159
00:08:58,880 --> 00:09:01,680
Speaker 3:  of the overwhelming thing And one of the things that I've seen a lot of,

160
00:09:01,680 --> 00:09:05,320
Speaker 3:  and there was this big story, I forget when relatively recently where

161
00:09:05,550 --> 00:09:09,120
Speaker 3:  McAfee found a bunch of really well known Chrome extensions

162
00:09:09,120 --> 00:09:12,560
Speaker 3:  including the one Netflix party where you can like CoWatch Netflix with your

163
00:09:12,560 --> 00:09:16,360
Speaker 3:  friends. What Netflix party was doing was all the things Netflix

164
00:09:16,360 --> 00:09:20,040
Speaker 3:  party was supposed to do. But then also doing things

165
00:09:20,040 --> 00:09:23,880
Speaker 3:  like adding affiliate links when you go to e-commerce websites

166
00:09:23,890 --> 00:09:26,840
Speaker 3:  so that it makes money and all this is just like sneakily happening in the

167
00:09:26,840 --> 00:09:30,440
Speaker 3:  background and you never see it because it's able to get in between you and

168
00:09:30,440 --> 00:09:33,800
Speaker 3:  the browser because that's what Chrome extensions do. And so it's just that

169
00:09:33,800 --> 00:09:37,440
Speaker 3:  kind of thing that it's like often you might not even notice but they are,

170
00:09:37,440 --> 00:09:40,680
Speaker 3:  they are aware of what you're doing and they're either changing it in some

171
00:09:40,680 --> 00:09:44,560
Speaker 3:  way or taking something from you. So my read of it is not that

172
00:09:44,560 --> 00:09:48,320
Speaker 3:  like all chrome extensions are dangerous, I do think the fact that they automatically

173
00:09:48,320 --> 00:09:52,240
Speaker 3:  update in the background without telling you is dangerous, but also like

174
00:09:52,240 --> 00:09:55,760
Speaker 3:  it is the kind of thing that you should be super, super, super aware of what

175
00:09:55,760 --> 00:09:58,680
Speaker 3:  you're downloading and make sure, the advice I always give people is never

176
00:09:58,680 --> 00:10:02,440
Speaker 3:  search in the store. Always go from like if you're looking for an app's Chrome

177
00:10:02,440 --> 00:10:06,160
Speaker 3:  extension, go to the app and find the link to the Chrome extension from there.

178
00:10:06,160 --> 00:10:08,920
Speaker 3:  Cuz if you go into the Chrome web store and search for an app, you're gonna

179
00:10:08,920 --> 00:10:12,440
Speaker 3:  get a hundred things that all look the same. 99 of them are scammy and one

180
00:10:12,440 --> 00:10:15,840
Speaker 3:  of them is real and it's often very hard to know. So if you need to have

181
00:10:15,840 --> 00:10:19,040
Speaker 3:  a Chrome extension and you should use only the bare minimum for all the reasons

182
00:10:19,040 --> 00:10:22,760
Speaker 3:  we've been describing, like find it from somewhere else. Don't just go hunting

183
00:10:22,760 --> 00:10:25,920
Speaker 3:  through the Chrome store for things that seem cool because like there be

184
00:10:25,920 --> 00:10:26,360
Speaker 3:  dragons.

185
00:10:26,360 --> 00:10:29,480
Speaker 4:  Yeah. And, and if you're listening to this and and being like, what can I

186
00:10:29,480 --> 00:10:33,120
Speaker 4:  do? Like go to your Chrome, you know,

187
00:10:33,120 --> 00:10:37,040
Speaker 4:  check out what the extensions are. It's chrome colon

188
00:10:37,200 --> 00:10:40,920
Speaker 4:  slash slash extensions. Right. But you can see

189
00:10:40,920 --> 00:10:44,240
Speaker 4:  it'll just show you everything and you can

190
00:10:44,670 --> 00:10:48,640
Speaker 4:  like a absolutely delete anything you do not recognize or that you are like,

191
00:10:48,690 --> 00:10:52,040
Speaker 4:  oh I sort of remember installing that, but I don't really use it. Like you

192
00:10:52,040 --> 00:10:54,200
Speaker 4:  should really have as few as possible. Yeah,

193
00:10:54,310 --> 00:10:58,200
Speaker 3:  A hundred percent. If you, if you don't use it like every day, you probably

194
00:10:58,200 --> 00:11:02,000
Speaker 3:  shouldn't have it. It's, it's worth the like extra two clicks

195
00:11:02,050 --> 00:11:05,800
Speaker 3:  in the browser to not have this stuff sitting between you and everything

196
00:11:05,800 --> 00:11:07,560
Speaker 3:  you do on the internet is my general read.

197
00:11:07,560 --> 00:11:09,200
Speaker 4:  Totally. All right, should we do another one?

198
00:11:09,200 --> 00:11:12,960
Speaker 3:  Yeah, let's, let's move on. So let's see, we have, we have a couple

199
00:11:12,960 --> 00:11:16,800
Speaker 3:  in a row about passwords. So let's, let's knock those out first.

200
00:11:16,800 --> 00:11:19,880
Speaker 3:  First we have one from Eduardo to that

201
00:11:20,240 --> 00:11:23,760
Speaker 6:  Hello the Verge team. Second question about

202
00:11:24,280 --> 00:11:28,120
Speaker 6:  passwords. I use different passwords for every single app

203
00:11:28,120 --> 00:11:31,640
Speaker 6:  or website that I log into, but

204
00:11:31,700 --> 00:11:35,640
Speaker 6:  I'm deeply into the Apple ecosystem. So my question

205
00:11:36,170 --> 00:11:39,560
Speaker 6:  is should I go into the suggested

206
00:11:40,080 --> 00:11:43,880
Speaker 6:  passwords or should I just keep using my own

207
00:11:44,320 --> 00:11:47,960
Speaker 6:  different passwords for everything? Is there any

208
00:11:48,230 --> 00:11:51,880
Speaker 6:  drawback on going into the Apple passwords

209
00:11:52,250 --> 00:11:56,160
Speaker 6:  or we'll all be able to do everything

210
00:11:56,160 --> 00:11:58,400
Speaker 6:  I've been doing so far. Thanks.

211
00:11:59,560 --> 00:12:03,290
Speaker 3:  Okay, so first of all Eduardo, kudos for using different passwords for

212
00:12:03,290 --> 00:12:06,250
Speaker 3:  everything that immediately puts you ahead of almost everybody. So congratulations.

213
00:12:06,920 --> 00:12:10,890
Speaker 3:  This is the question that sent me down the deepest, weirdest rabbit hole

214
00:12:11,020 --> 00:12:14,890
Speaker 3:  of any of the questions we got. But before I get real weird about random

215
00:12:14,890 --> 00:12:17,850
Speaker 3:  number generators, which is gonna happen, what are your thoughts Russell?

216
00:12:18,680 --> 00:12:22,490
Speaker 4:  Yeah, I mean I think, so partially this is like a question about password

217
00:12:22,930 --> 00:12:26,290
Speaker 4:  strength, right? Because if he's coming up with these different passwords,

218
00:12:26,420 --> 00:12:30,330
Speaker 4:  they're presumably sort of human comprehensible passwords. Yes.

219
00:12:30,330 --> 00:12:34,170
Speaker 4:  Not just random strengths of, And so it's like is it better to

220
00:12:34,170 --> 00:12:37,730
Speaker 4:  use the random strings that Apple is using? And I don't

221
00:12:37,730 --> 00:12:41,050
Speaker 4:  think it's a huge deal. I think the main,

222
00:12:41,860 --> 00:12:45,850
Speaker 4:  so the the, the place where your password strength is

223
00:12:45,850 --> 00:12:49,730
Speaker 4:  most likely to be tested is you can't just guess

224
00:12:49,800 --> 00:12:53,770
Speaker 4:  millions of passwords at once on like the Gmail login screen.

225
00:12:53,770 --> 00:12:57,610
Speaker 4:  Like they will, they will catch on, they're like, this seems like a robot

226
00:12:57,610 --> 00:13:00,850
Speaker 4:  is just doing 1, 2, 3 and then the next one.

227
00:13:01,750 --> 00:13:05,250
Speaker 4:  But occasionally hash is hashed versions. So

228
00:13:05,520 --> 00:13:09,130
Speaker 4:  hash is what the website is checking your password against. It sort of has

229
00:13:09,130 --> 00:13:13,010
Speaker 4:  a hashing function that does a special dance to the password

230
00:13:13,010 --> 00:13:16,850
Speaker 4:  that you inputted and then it checks it against, well this is

231
00:13:16,850 --> 00:13:19,810
Speaker 4:  what all this is what the correct password should look like after the special

232
00:13:19,810 --> 00:13:23,450
Speaker 4:  dance so that it doesn't just immediately know what your password is and

233
00:13:23,450 --> 00:13:26,890
Speaker 4:  have that information for everything. And so you're not supposed to be able

234
00:13:26,890 --> 00:13:30,850
Speaker 4:  to work back from the hash to the actual password, but if you

235
00:13:30,850 --> 00:13:34,370
Speaker 4:  already know what the password is or you have, if

236
00:13:34,550 --> 00:13:38,370
Speaker 4:  you like check a bunch of really common passwords and then that can help

237
00:13:38,370 --> 00:13:41,690
Speaker 4:  you crack the hashing algorithm and undo the other ones,

238
00:13:42,520 --> 00:13:46,050
Speaker 4:  then it's a problem. But like the difference between

239
00:13:47,280 --> 00:13:51,090
Speaker 4:  a fairly uncommon word that's in a dictionary and a couple like

240
00:13:51,090 --> 00:13:54,690
Speaker 4:  random characters and the complete gobbly book that

241
00:13:54,930 --> 00:13:58,850
Speaker 4:  Apple spits out is just really not that significant. And I

242
00:13:58,850 --> 00:14:02,730
Speaker 4:  wouldn't worry about it. I think the main question is like are you more

243
00:14:02,730 --> 00:14:05,690
Speaker 4:  likely to lose it because it's not automatically being

244
00:14:06,480 --> 00:14:10,210
Speaker 4:  logged by Apple's password manager, but that

245
00:14:10,210 --> 00:14:13,770
Speaker 4:  also doesn't seem like a huge difference since it sounds like this system

246
00:14:13,770 --> 00:14:14,490
Speaker 4:  is working for him.

247
00:14:14,840 --> 00:14:18,650
Speaker 3:  Yeah, I think I agree with all of that actually. And I think the, the

248
00:14:18,650 --> 00:14:22,570
Speaker 3:  two parts of this just jumped out to me are one the like

249
00:14:22,570 --> 00:14:26,210
Speaker 3:  convenience thing you just talked about and I think to me the biggest

250
00:14:26,770 --> 00:14:30,490
Speaker 3:  downside of using apple's the, the whole like key chain

251
00:14:30,490 --> 00:14:34,410
Speaker 3:  system within Apple is it's very convenient but it's also tied to all of

252
00:14:34,410 --> 00:14:37,810
Speaker 3:  your other Apple stuff, right? And like one very common sense cybersecurity

253
00:14:37,810 --> 00:14:41,290
Speaker 3:  piece of advice is don't have all your eggs in one basket. And so

254
00:14:41,470 --> 00:14:44,970
Speaker 3:  the idea of if somebody gets into my iCloud they can also get into my password

255
00:14:45,120 --> 00:14:48,010
Speaker 3:  should make you nervous and there's a, there are good ways to get around

256
00:14:48,010 --> 00:14:50,610
Speaker 3:  that by like protecting your iCloud and multifactor authentication and all

257
00:14:50,610 --> 00:14:54,000
Speaker 3:  that stuff stuff. But like I would, I would say the only reason not to do

258
00:14:54,000 --> 00:14:57,360
Speaker 3:  it is that a, it's not very cross platform. So if you use other devices pass

259
00:14:57,840 --> 00:15:01,360
Speaker 3:  key chain doesn't work all that well and it is attached to all of your other

260
00:15:01,360 --> 00:15:03,960
Speaker 3:  Apple stuff. So do it that way. You will, To your point about the randomness,

261
00:15:04,070 --> 00:15:07,560
Speaker 3:  I think you're exactly right. I think my worry would be that for somebody

262
00:15:07,560 --> 00:15:10,840
Speaker 3:  like our friend Eduardo here, that when they say I have different passwords

263
00:15:10,840 --> 00:15:14,440
Speaker 3:  for everything, what they actually mean is like variations on a theme where

264
00:15:14,440 --> 00:15:18,400
Speaker 3:  it's like I have one password that is like cool kids and then I have

265
00:15:18,400 --> 00:15:20,760
Speaker 3:  another one that's cool kids with two S's and then I have one that's cool

266
00:15:20,760 --> 00:15:24,440
Speaker 3:  kids six and that is not great password hygiene,

267
00:15:24,440 --> 00:15:27,600
Speaker 3:  it's better than having the same password for everything, but it's not great

268
00:15:27,840 --> 00:15:30,040
Speaker 3:  password hygiene because Well

269
00:15:30,610 --> 00:15:33,920
Speaker 4:  Go ahead. Yeah. And, and it's worth going into what the attack is there.

270
00:15:33,920 --> 00:15:37,840
Speaker 4:  So there are these like LinkedIn

271
00:15:37,840 --> 00:15:41,760
Speaker 4:  was breached in like 2009 or something 2011, it's, it was

272
00:15:41,760 --> 00:15:45,440
Speaker 4:  a while ago. This is all public, this is not, I'm not blowing up on your

273
00:15:45,440 --> 00:15:49,120
Speaker 4:  spot, but then this means that like anyone on the internet who wants it

274
00:15:49,120 --> 00:15:52,960
Speaker 4:  basically by now has the LinkedIn password I was using

275
00:15:53,210 --> 00:15:57,040
Speaker 4:  in 2011. Yes. Right. And so if I was using

276
00:15:57,040 --> 00:16:00,680
Speaker 4:  that for anything else, when they were able to get to it, they

277
00:16:01,120 --> 00:16:05,080
Speaker 4:  probably went through, you know, well what's Russell's Gmail account? What's

278
00:16:05,080 --> 00:16:08,560
Speaker 4:  Russell's other account? Let's see if the same password

279
00:16:08,560 --> 00:16:11,840
Speaker 4:  works. Right? Right. And a lot of times they have this password

280
00:16:12,220 --> 00:16:15,680
Speaker 4:  and the breach isn't public, no one knows about it. And so

281
00:16:16,050 --> 00:16:19,840
Speaker 4:  they're likely to have sort of find out about it before you do. Now

282
00:16:20,100 --> 00:16:23,960
Speaker 4:  are they going to try, well, okay, what about Russell's password?

283
00:16:24,060 --> 00:16:27,360
Speaker 4:  But the number's slightly different. Exactly. What about Russell's password?

284
00:16:27,360 --> 00:16:31,280
Speaker 4:  And there's some extra S's on one hand, like they're doing this

285
00:16:31,560 --> 00:16:35,440
Speaker 4:  with 10 million passwords and just running through the list because

286
00:16:35,440 --> 00:16:39,360
Speaker 4:  it's fairly, it's rare that you get a hit, it's just like it's worth

287
00:16:39,360 --> 00:16:43,080
Speaker 4:  it if we get a hit. On the other hand, they have a lot of

288
00:16:43,080 --> 00:16:47,040
Speaker 4:  time and not a lot of these breach things, so they're probably gonna try

289
00:16:47,040 --> 00:16:50,880
Speaker 4:  as much as they can get away with and that's, that's why

290
00:16:50,880 --> 00:16:54,120
Speaker 4:  the password hygiene of like here's this slight alteration

291
00:16:54,530 --> 00:16:58,400
Speaker 4:  is not great if a password very, very similar to

292
00:16:58,400 --> 00:16:59,720
Speaker 4:  that gets breached.

293
00:17:00,070 --> 00:17:03,360
Speaker 3:  Yeah, I think that's right. And I think the, the deep rabbit hole I went

294
00:17:03,360 --> 00:17:07,080
Speaker 3:  down was about pseudo random number generators and this idea

295
00:17:07,080 --> 00:17:11,000
Speaker 3:  that even things that purport to give you random sets

296
00:17:11,000 --> 00:17:14,800
Speaker 3:  of characters are not random, right? Like computers on their own can't do

297
00:17:14,800 --> 00:17:18,520
Speaker 3:  random. There is the logic. So what they do is they start basically from

298
00:17:18,520 --> 00:17:22,440
Speaker 3:  what's called a seed and then run an algorithm to do what is this, what looks

299
00:17:22,510 --> 00:17:25,920
Speaker 3:  a lot like randomness? But if you can figure out what that seed is,

300
00:17:26,180 --> 00:17:28,960
Speaker 3:  you then know where the randomness started from and you can start to figure

301
00:17:28,960 --> 00:17:32,680
Speaker 3:  it out. So Bruce Schneider, who is a, a security researcher who has done

302
00:17:32,680 --> 00:17:35,200
Speaker 3:  a lot of work on this over the years, a thing he talks about that I like

303
00:17:35,200 --> 00:17:38,040
Speaker 3:  and I think is a useful framework to think about all of this is basically

304
00:17:38,040 --> 00:17:41,840
Speaker 3:  like degrees of entropy, right? Like if I, if all of my passwords

305
00:17:41,840 --> 00:17:45,720
Speaker 3:  are one last character off, that's one degree of entropy, right? If

306
00:17:45,720 --> 00:17:48,040
Speaker 3:  you know all of my password except that one character, all you have to do

307
00:17:48,040 --> 00:17:50,200
Speaker 3:  is figure out one thing. But if you, if you don't know any of my password

308
00:17:50,200 --> 00:17:52,880
Speaker 3:  because it's all different, you're starting from nothing and suddenly the

309
00:17:52,880 --> 00:17:56,440
Speaker 3:  like work you have to do to get there is much, much stronger. So I think

310
00:17:56,440 --> 00:17:59,720
Speaker 3:  in a case like this, what I went the rabbit hole I really went down is like,

311
00:17:59,720 --> 00:18:03,560
Speaker 3:  is Apple's pseudo random number generator better than the competition?

312
00:18:03,560 --> 00:18:07,240
Speaker 3:  Pseudo random number generators? And the answer is like, not really.

313
00:18:07,240 --> 00:18:11,000
Speaker 3:  There are a lot of very good ones out there. And the real question is

314
00:18:11,000 --> 00:18:14,840
Speaker 3:  just about a lot of other things that actually, like the random number

315
00:18:14,840 --> 00:18:18,160
Speaker 3:  generators are not likely what's gonna screw up your passwords over time.

316
00:18:19,550 --> 00:18:23,440
Speaker 4:  Yeah. That's like, that's like your, you're like, is this plane

317
00:18:23,440 --> 00:18:27,040
Speaker 4:  I'm getting on safe? And they're like, well let's look at the fuel.

318
00:18:27,050 --> 00:18:30,520
Speaker 4:  Exactly. Who's using the best tool? It's like, it's, it's, it's probably,

319
00:18:30,750 --> 00:18:32,640
Speaker 4:  it's not gonna come down to that. Is it information?

320
00:18:33,140 --> 00:18:36,800
Speaker 3:  Yes. But is is it, is it gonna be the difference? Almost certainly not.

321
00:18:36,820 --> 00:18:40,560
Speaker 3:  But generally, like what I would say is, is think about entropy, right? And

322
00:18:40,560 --> 00:18:44,040
Speaker 3:  if you want to keep your password as secure as possible, have as much entropy

323
00:18:44,040 --> 00:18:47,280
Speaker 3:  between your different passwords as you possibly can. And if that means letting

324
00:18:47,280 --> 00:18:51,200
Speaker 3:  Apple fill it out for you and save it in ways that everyone agrees are

325
00:18:51,360 --> 00:18:55,040
Speaker 3:  solid and secure, that works. I use one password, which I love

326
00:18:55,040 --> 00:18:59,000
Speaker 3:  very much. I I will stand for one password to anyone who listens. It's

327
00:18:59,000 --> 00:19:01,320
Speaker 3:  a really good password manager. There are a lot of good ones out there and

328
00:19:01,320 --> 00:19:05,080
Speaker 3:  I think having them be randomly selected for me, I don't know

329
00:19:05,080 --> 00:19:08,240
Speaker 3:  most of my passwords. And I like it that way and I feel like that's, that's

330
00:19:08,240 --> 00:19:11,640
Speaker 3:  where everyone should be. Let let a system know my passwords and then I don't

331
00:19:11,640 --> 00:19:11,960
Speaker 3:  have to.

332
00:19:12,560 --> 00:19:13,410
Speaker 4:  Absolutely.

333
00:19:13,890 --> 00:19:14,650
Speaker 3:  Alright, let's move on.

334
00:19:14,650 --> 00:19:17,410
Speaker 4:  So what's the, what's the other, yeah, what's the other password? I mean

335
00:19:17,650 --> 00:19:21,210
Speaker 3:  Our next one is even weirder and I'm very excited. It's from Leon.

336
00:19:21,210 --> 00:19:21,970
Speaker 3:  Let's hear it.

337
00:19:22,110 --> 00:19:25,370
Speaker 7:  Hey Vergecast team, it's Leon from Virginia.

338
00:19:25,520 --> 00:19:29,490
Speaker 7:  What's up with websites having a maximum character limit

339
00:19:29,770 --> 00:19:32,970
Speaker 7:  for their passwords? Like why do I have to create a password

340
00:19:33,600 --> 00:19:37,530
Speaker 7:  with a maximum of eight or 12 characters?

341
00:19:37,870 --> 00:19:39,650
Speaker 7:  Why is that safe? Thanks.

342
00:19:40,200 --> 00:19:43,170
Speaker 3:  This is the other question that sent me down a crazy rabbit hole. Yeah. And

343
00:19:43,170 --> 00:19:44,570
Speaker 3:  it was deeply unsatisfied. It's,

344
00:19:44,570 --> 00:19:48,450
Speaker 4:  I mean, it's not safe, right? Like broadly these limits are

345
00:19:48,450 --> 00:19:52,290
Speaker 4:  bad security people don't like them. I think that the reason

346
00:19:52,290 --> 00:19:56,090
Speaker 4:  that you see them is kind of has to do with

347
00:19:56,090 --> 00:19:59,890
Speaker 4:  making life easier for the architects of these systems more than

348
00:19:59,890 --> 00:20:03,770
Speaker 4:  like security fundamentals. But it sounds like David, you have

349
00:20:03,770 --> 00:20:05,490
Speaker 4:  a lot of research to, to spill out.

350
00:20:05,940 --> 00:20:09,610
Speaker 3:  No, so you're, you're right, but you're missing one piece of the

351
00:20:09,890 --> 00:20:13,690
Speaker 3:  equation, which is users. The simple truth of the matter, based on all the

352
00:20:13,770 --> 00:20:15,970
Speaker 3:  research I've done and all the people I've talked to, is that fundamentally

353
00:20:16,140 --> 00:20:20,010
Speaker 3:  if you allow people to have 500 character passwords, they

354
00:20:20,010 --> 00:20:23,930
Speaker 3:  will forget them. They will type them incorrectly. They will call customer

355
00:20:23,930 --> 00:20:27,370
Speaker 3:  service to reset their passwords more often. And that will suck for everyone

356
00:20:27,610 --> 00:20:31,410
Speaker 3:  involved. And so I went down looking for like

357
00:20:31,410 --> 00:20:35,370
Speaker 3:  really useful technical answers and what it seems like is, on the one

358
00:20:35,370 --> 00:20:39,210
Speaker 3:  hand, you're right that like, and especially in the early days of the

359
00:20:39,370 --> 00:20:42,810
Speaker 3:  internet, there was a lot of like technological systems that just couldn't

360
00:20:42,810 --> 00:20:45,530
Speaker 3:  handle that much data coming in at the same time. Like if you're sending

361
00:20:45,730 --> 00:20:49,680
Speaker 3:  somebody a one megabyte password, that's a lot. And it was especially a lot

362
00:20:49,680 --> 00:20:51,360
Speaker 3:  many years ago. So,

363
00:20:51,510 --> 00:20:55,160
Speaker 4:  Well, and and also that like remember there's the hashing algorithms, like

364
00:20:55,160 --> 00:20:58,480
Speaker 4:  if you can know the amount of

365
00:20:58,480 --> 00:21:02,200
Speaker 4:  information very precisely, it lets you, it just sort of

366
00:21:02,200 --> 00:21:03,400
Speaker 4:  smooths things out. Totally.

367
00:21:03,400 --> 00:21:06,840
Speaker 3:  Yeah. And then there that just sort of became the norm. So people kept doing

368
00:21:06,840 --> 00:21:10,320
Speaker 3:  it. But then I was looking and like the, the National Institute of Standards

369
00:21:10,320 --> 00:21:13,920
Speaker 3:  in Technology has a bunch of rules about passwords and what they actually

370
00:21:13,920 --> 00:21:17,120
Speaker 3:  recommend is that you allow at least 64 characters

371
00:21:17,330 --> 00:21:21,160
Speaker 3:  because yeah, the, the kind of new password theory

372
00:21:21,160 --> 00:21:23,960
Speaker 3:  is like, what you don't need is just a bunch of like random he decimal stuff,

373
00:21:23,960 --> 00:21:27,880
Speaker 3:  which you need is like a long, relatively random string of words

374
00:21:28,350 --> 00:21:31,920
Speaker 3:  that you can remember that that's actually the most useful thing. Like if

375
00:21:31,920 --> 00:21:32,520
Speaker 3:  it's five words

376
00:21:32,520 --> 00:21:32,800
Speaker 4:  In a

377
00:21:32,800 --> 00:21:34,000
Speaker 3:  Row, I, so I will, you can do it.

378
00:21:34,750 --> 00:21:38,600
Speaker 4:  I will say at the risk of compromising my own security, so this is the

379
00:21:38,600 --> 00:21:42,520
Speaker 4:  past phrase, right? Think of some phrase that you remember and I'm a

380
00:21:42,520 --> 00:21:46,160
Speaker 4:  word guy, so I remember phrases like, there are a lot of phrases where

381
00:21:46,400 --> 00:21:50,360
Speaker 4:  I'm like, oh yeah, that one line really meant something to me. And so

382
00:21:50,830 --> 00:21:54,440
Speaker 4:  when we, I was setting up my like work account for

383
00:21:54,440 --> 00:21:58,200
Speaker 4:  Vox Media, I did the phrase and I was like, I'm not gonna, you know,

384
00:21:58,200 --> 00:22:00,680
Speaker 4:  this is gonna be in my password manager, I'm not gonna type this in that

385
00:22:00,680 --> 00:22:04,270
Speaker 4:  much. And then as it went on, it sort of

386
00:22:04,270 --> 00:22:08,070
Speaker 4:  propagated until now it's the thing I need to

387
00:22:08,070 --> 00:22:10,590
Speaker 4:  type in every time I wake up my laptop

388
00:22:11,790 --> 00:22:15,530
Speaker 4:  and it's like, it's like a bunch of words long, like it's over

389
00:22:15,530 --> 00:22:19,450
Speaker 4:  50 characters. It's it, and and also if I can't see

390
00:22:19,450 --> 00:22:23,370
Speaker 4:  it, I'll like have a typo sometimes. So it'll take me like two or three tries

391
00:22:23,750 --> 00:22:27,610
Speaker 4:  and I'm just trying to like mute whatever song is playing. It's

392
00:22:27,890 --> 00:22:31,250
Speaker 4:  really, so that's a, that's a cautionary tale. I guess

393
00:22:31,250 --> 00:22:34,410
Speaker 3:  It's just like the first chapter of Moby Dick every time you wanna wake up

394
00:22:34,410 --> 00:22:34,810
Speaker 3:  your computer.

395
00:22:34,980 --> 00:22:38,330
Speaker 4:  Oh my god, how did you guys, No, but I mean it's really like,

396
00:22:38,640 --> 00:22:41,850
Speaker 4:  yeah, I'm trying not to do it. I'm, I'm trying not to put any information

397
00:22:41,850 --> 00:22:45,680
Speaker 4:  out there that would, that would make it easy for that actors. But yeah,

398
00:22:45,680 --> 00:22:49,280
Speaker 4:  it's, so think about the fact that you'll have to

399
00:22:49,480 --> 00:22:50,920
Speaker 4:  maybe type this in a lot,

400
00:22:50,980 --> 00:22:54,720
Speaker 3:  But I think yeah, in general it is. I I think I would agree bad

401
00:22:54,720 --> 00:22:58,040
Speaker 3:  form if they only allow you to have something like eight characters, what

402
00:22:58,040 --> 00:23:01,840
Speaker 3:  does seem to be true is there are pretty severe diminishing returns after

403
00:23:01,840 --> 00:23:05,600
Speaker 3:  about 20, Hmm I dunno if that's a magic number, but it that,

404
00:23:05,600 --> 00:23:09,120
Speaker 3:  that one kind of came up a bunch that like the difference between a 20 character

405
00:23:09,520 --> 00:23:13,320
Speaker 3:  password and a 200 character password is not massive in

406
00:23:13,320 --> 00:23:17,040
Speaker 3:  terms of like, can a regular person brute force their way into getting your

407
00:23:17,160 --> 00:23:20,840
Speaker 3:  password? Whereas the difference between eight and 20 is when it goes from

408
00:23:20,840 --> 00:23:24,240
Speaker 3:  like hard but feasible to hard and sort of infeasible.

409
00:23:24,490 --> 00:23:28,240
Speaker 3:  So I think, yeah, if, if you're only allowed to have short passwords, you

410
00:23:28,240 --> 00:23:31,120
Speaker 3:  should, you should yell at them to let you have longer ones is what I would

411
00:23:31,120 --> 00:23:31,280
Speaker 3:  say.

412
00:23:31,470 --> 00:23:31,960
Speaker 4:  Yeah.

413
00:23:31,960 --> 00:23:35,440
Speaker 3:  Cool. Do you have short passwords? I feel like I just shamed you by accident.

414
00:23:35,720 --> 00:23:36,920
Speaker 3:  You're changing all your passwords

415
00:23:36,920 --> 00:23:40,480
Speaker 4:  For, I will say I, I changed from on the,

416
00:23:40,570 --> 00:23:44,320
Speaker 4:  in the iPhone unlock they give you the option of four

417
00:23:44,320 --> 00:23:48,240
Speaker 4:  numbers, which the security people really don't like. Or six

418
00:23:48,240 --> 00:23:52,160
Speaker 4:  numbers, which is, you know, it's powers of 10 or it's a

419
00:23:52,160 --> 00:23:56,000
Speaker 4:  hundred times more secure or this alpha numeric thing, which

420
00:23:56,000 --> 00:23:59,440
Speaker 4:  you're gonna type in I keyboard, which is probably the best thing to do.

421
00:23:59,440 --> 00:24:01,880
Speaker 4:  But I mean, who has the time? I'm not gonna

422
00:24:02,880 --> 00:24:06,600
Speaker 3:  Honestly like it's, I'm a four, I'm a four digit

423
00:24:06,630 --> 00:24:10,520
Speaker 3:  iPhone password person. Wow. And I'm ashamed of it like I am. It's

424
00:24:10,520 --> 00:24:14,080
Speaker 3:  not great and and the best part is it defaults to six and it makes you

425
00:24:14,080 --> 00:24:17,440
Speaker 3:  change to four when you set up your phone it's like, do you wanna make a

426
00:24:17,440 --> 00:24:20,680
Speaker 3:  stupid decision right now? And every time I'm like, yes I do. Thank you for

427
00:24:20,680 --> 00:24:20,920
Speaker 3:  asking.

428
00:24:22,460 --> 00:24:26,440
Speaker 3:  I'm very happy to do so. Yeah. Right. We need to take a break and then we're

429
00:24:26,440 --> 00:24:29,480
Speaker 3:  gonna come back and answer a couple more questions, including with our very

430
00:24:29,480 --> 00:24:31,400
Speaker 3:  good friends Eli Patel. We'll be right back.

431
00:24:38,360 --> 00:24:39,260
Speaker 9:  Fox Creative,

432
00:24:40,050 --> 00:24:42,140
Speaker 11:  This is advertiser content from Zelle.

433
00:24:44,510 --> 00:24:48,300
Speaker 11:  So you find the perfect puppy online, you email, they

434
00:24:48,300 --> 00:24:51,900
Speaker 11:  send cute photos and say act fast. Lots of people want this puppy.

435
00:24:52,210 --> 00:24:55,860
Speaker 11:  Send me the $500 adoption fee tonight and she's all yours.

436
00:24:56,360 --> 00:24:59,460
Speaker 11:  You send it, you wait and they just

437
00:24:59,620 --> 00:25:02,340
Speaker 11:  disappear. No puppy, no money.

438
00:25:02,840 --> 00:25:06,220
Speaker 10:  You know, everyone thinks these are new since the internet. They're not

439
00:25:06,770 --> 00:25:10,700
Speaker 11:  Robert Burda, CEO of the Cyber Crime Support Network says

440
00:25:10,700 --> 00:25:14,300
Speaker 11:  puppy scams are just one example of countless online marketplace

441
00:25:14,300 --> 00:25:18,260
Speaker 11:  scams. Scams aren't new but the Internet's speed and anonymity are,

442
00:25:18,470 --> 00:25:22,460
Speaker 10:  They've always exploited human nature. They're always trying to build that

443
00:25:22,460 --> 00:25:26,220
Speaker 10:  sense of urgency. There's nothing that can't wait. Slow

444
00:25:26,420 --> 00:25:30,340
Speaker 10:  down spot check and don't send the money because they know

445
00:25:30,340 --> 00:25:33,820
Speaker 10:  that once they convince you to let it go, very little chance is coming back.

446
00:25:34,620 --> 00:25:38,300
Speaker 11:  Elle wants you to pay it safe, treat every transaction like cash

447
00:25:38,480 --> 00:25:42,460
Speaker 11:  and only send money to people you know and trust. Learn more safety

448
00:25:42,460 --> 00:25:44,020
Speaker 11:  tips@ellepay.com.

449
00:25:50,270 --> 00:25:54,240
Speaker 3:  Okay, next up we have a question I think a

450
00:25:54,240 --> 00:25:57,800
Speaker 3:  bunch of us got very excited about this is from our friend Chuck who has

451
00:25:57,800 --> 00:25:58,400
Speaker 3:  a question about babies.

452
00:25:59,220 --> 00:26:03,080
Speaker 12:  Hey guys, this might be very niche so maybe it doesn't go on the show,

453
00:26:03,080 --> 00:26:06,880
Speaker 12:  but maybe it could go on an article sometime I have a

454
00:26:06,880 --> 00:26:10,680
Speaker 12:  newborn and I don't know how to keep her safe from

455
00:26:10,950 --> 00:26:14,560
Speaker 12:  online stuff. Like do I need to do anything? Do I do nothing?

456
00:26:16,530 --> 00:26:20,380
Speaker 12:  So yeah, I just, I don't want to leave doors

457
00:26:20,380 --> 00:26:24,250
Speaker 12:  open, but yeah, I don't, there's nothing good out there

458
00:26:24,250 --> 00:26:28,130
Speaker 12:  that says like, do this and you'll be safe. Thanks. Love the show.

459
00:26:28,130 --> 00:26:28,370
Speaker 12:  Bye.

460
00:26:28,440 --> 00:26:32,290
Speaker 3:  Okay. I, this question gives me so many feelings and you can just,

461
00:26:32,290 --> 00:26:36,210
Speaker 3:  you can just feel the like vibes and energy from

462
00:26:36,210 --> 00:26:38,970
Speaker 3:  this person. Neil Patella is here to help us answer this question. Hello

463
00:26:38,970 --> 00:26:41,250
Speaker 3:  Nelia, you are our resident baby expert. Congratulations.

464
00:26:41,600 --> 00:26:42,410
Speaker 4:  That's bad

465
00:26:42,410 --> 00:26:42,970
Speaker 13:  For America.

466
00:26:45,380 --> 00:26:49,160
Speaker 4:  You are all that stands between the horrors of the

467
00:26:49,400 --> 00:26:52,920
Speaker 4:  internet and the Children of America and

468
00:26:53,270 --> 00:26:54,200
Speaker 4:  some of Canada.

469
00:26:55,580 --> 00:26:58,160
Speaker 13:  The whole show to parent blogging, that's what I do here. Yeah.

470
00:26:58,590 --> 00:27:01,440
Speaker 3:  I will say has been threatening to do this ever since he had children. So

471
00:27:01,760 --> 00:27:04,840
Speaker 3:  it's, it's gonna happen eventually. What I feel like we, we could start almost

472
00:27:05,120 --> 00:27:09,000
Speaker 3:  anywhere here, but there's like, is there stock parenting

473
00:27:09,000 --> 00:27:12,760
Speaker 3:  advice for what you should do? Like, cuz there, there, there comes a time

474
00:27:12,760 --> 00:27:15,760
Speaker 3:  when like your kid is gonna start wanting to be online and you have to do

475
00:27:15,760 --> 00:27:18,720
Speaker 3:  things about that. But we're talking even way before then. This is the like

476
00:27:18,720 --> 00:27:22,640
Speaker 3:  they don't even know what the internet is yet. Do you have responsibilities

477
00:27:22,650 --> 00:27:24,760
Speaker 3:  to them? Which I think is a super interesting question.

478
00:27:25,030 --> 00:27:28,360
Speaker 13:  Yeah, well David, you're about to have a kid so you're gonna become the

479
00:27:28,360 --> 00:27:30,840
Speaker 13:  new, Am I allowed to say that on the show? Do the people know,

480
00:27:31,540 --> 00:27:34,840
Speaker 3:  The people don't know, They know Now I this is, this is as good a time as

481
00:27:34,840 --> 00:27:38,120
Speaker 3:  any, my wife is due December and I'm gonna be the version is the next baby

482
00:27:38,120 --> 00:27:38,680
Speaker 3:  correspondent.

483
00:27:38,740 --> 00:27:39,600
Speaker 4:  Congratulations.

484
00:27:39,670 --> 00:27:41,040
Speaker 3:  It's great. Thank you. You

485
00:27:41,040 --> 00:27:44,720
Speaker 13:  Gotta, you gotta rotate in a new one because the kids keep getting relentlessly

486
00:27:44,720 --> 00:27:48,600
Speaker 13:  older. It's a real problem. So here's what I would say

487
00:27:48,600 --> 00:27:52,440
Speaker 13:  to everybody, almost everything about being a first

488
00:27:52,440 --> 00:27:56,320
Speaker 13:  time parent is just how afraid you choose to be and how

489
00:27:56,320 --> 00:28:00,080
Speaker 13:  much you allow corporate America to pray on that fear.

490
00:28:00,180 --> 00:28:03,480
Speaker 13:  The best advice I ever got was from my friend Spencer Hall who some of you

491
00:28:03,480 --> 00:28:07,200
Speaker 13:  may know from other endeavors in the internet and he just looked at me very

492
00:28:07,200 --> 00:28:09,840
Speaker 13:  seriously. I think he has like four kids and he was like, babies are more

493
00:28:10,040 --> 00:28:13,520
Speaker 13:  resilient than you think and if you just hold that in your head, you're

494
00:28:13,520 --> 00:28:17,000
Speaker 13:  gonna be like, just remember people did this before any technology

495
00:28:17,000 --> 00:28:20,680
Speaker 13:  existed. That's how you got here. So he's like, hold onto that. The second

496
00:28:20,680 --> 00:28:24,600
Speaker 13:  thing I'll say much more specifically about computers is

497
00:28:24,750 --> 00:28:28,360
Speaker 13:  once you realize that all that's designed to pray in your fear, you realize

498
00:28:28,360 --> 00:28:32,280
Speaker 13:  that a lot of like computer products exist to just reassure you

499
00:28:32,430 --> 00:28:35,400
Speaker 13:  that actually provide you like no additional safety.

500
00:28:36,370 --> 00:28:40,280
Speaker 13:  So there's like infinity baby monitors and socks

501
00:28:40,280 --> 00:28:44,200
Speaker 13:  and snooze that you can buy and you just have to decide how much you

502
00:28:44,200 --> 00:28:48,160
Speaker 13:  wanna like participate in that universe of things. Our decision

503
00:28:48,820 --> 00:28:52,560
Speaker 13:  was that we didn't want anything that required a user

504
00:28:52,560 --> 00:28:56,080
Speaker 13:  account. Ooh. Like we just didn't wanna start sharing data about our kid

505
00:28:56,330 --> 00:29:00,160
Speaker 13:  at any moment. So we did not buy a connected

506
00:29:00,160 --> 00:29:03,760
Speaker 13:  baby camera. We bought the UFE one that's on Amazon I'm sure by now.

507
00:29:03,800 --> 00:29:07,480
Speaker 13:  There are many iterations of it, but there was one that everyone bought

508
00:29:07,480 --> 00:29:10,640
Speaker 13:  and then ufe, which is the division of anchor, came out with like this slightly

509
00:29:10,640 --> 00:29:14,480
Speaker 13:  upgraded riff on that one And we, we just have that one and it works

510
00:29:14,480 --> 00:29:18,320
Speaker 13:  great and it is, you know, like a proprietary RF protocol. It's a

511
00:29:18,320 --> 00:29:22,160
Speaker 13:  rock solid in our house, no wifi involved and it has a little

512
00:29:22,160 --> 00:29:26,080
Speaker 13:  piece of hardware. That's great. So I, we carry around a little

513
00:29:26,080 --> 00:29:28,440
Speaker 13:  screen in our house to watch our kid that is

514
00:29:28,440 --> 00:29:32,000
Speaker 3:  So your your thinking was not only like no user

515
00:29:32,000 --> 00:29:35,920
Speaker 3:  accounts for your kid but no user accounts related to your

516
00:29:35,920 --> 00:29:38,360
Speaker 3:  kid like, cuz it wouldn't have been, it wouldn't have been your kid's name

517
00:29:38,360 --> 00:29:42,120
Speaker 3:  on the UVA or on the baby monitor thing, it would've been yours but even

518
00:29:42,120 --> 00:29:43,240
Speaker 3:  still like that crossed the line.

519
00:29:43,430 --> 00:29:47,360
Speaker 13:  Yeah, I wanted, I'll use a security term here. I wanted to air gap the baby

520
00:29:47,360 --> 00:29:49,000
Speaker 13:  from the internet for as long as possible.

521
00:29:49,050 --> 00:29:51,160
Speaker 3:  Oh I love that. Okay, that's good. Yeah,

522
00:29:51,240 --> 00:29:54,960
Speaker 13:  Right. So I didn't want anything connected to the internet near

523
00:29:54,960 --> 00:29:57,720
Speaker 13:  the baby for as long as I could hold off air

524
00:29:57,720 --> 00:30:01,560
Speaker 3:  Gap. Your baby is a verge t-shirt that needs to, this is a thing we're gonna

525
00:30:01,560 --> 00:30:03,880
Speaker 3:  make onesies, it's gonna say air gap, it's gonna be incredible.

526
00:30:04,180 --> 00:30:08,080
Speaker 13:  One thing you're gonna want to do is track a bunch of baby

527
00:30:08,080 --> 00:30:11,960
Speaker 13:  stuff related to a newborn when they eat, when they poop, when they sleep

528
00:30:12,220 --> 00:30:15,920
Speaker 13:  and there's infinity apps that are, you know, you can log into and you can

529
00:30:15,920 --> 00:30:19,480
Speaker 13:  use them and we don't wanna use them. So we use one called Sprout which

530
00:30:19,480 --> 00:30:22,920
Speaker 13:  syncs the database over iCloud to another phone using a key.

531
00:30:23,010 --> 00:30:26,480
Speaker 13:  There's no user accounts and they actually advertise it as having no user

532
00:30:26,480 --> 00:30:29,520
Speaker 13:  accounts. Now this is like four years ago, we have not tracked our four

533
00:30:29,520 --> 00:30:33,400
Speaker 13:  year olds sleeping and pooping. We're good. She lets us know when it's

534
00:30:33,400 --> 00:30:33,680
Speaker 13:  gonna happen.

535
00:30:34,070 --> 00:30:37,920
Speaker 4:  I mean are you weighing it? What, what's the fine grain of the

536
00:30:37,920 --> 00:30:40,840
Speaker 4:  data? How much data on the poop are we collecting here?

537
00:30:40,870 --> 00:30:44,520
Speaker 13:  Well so when you have a newborn you don't sleep very often.

538
00:30:45,090 --> 00:30:47,640
Speaker 13:  So your memory of what happened when

539
00:30:48,530 --> 00:30:52,160
Speaker 13:  becomes very limited and you're like, did I feed the baby?

540
00:30:52,180 --> 00:30:55,240
Speaker 13:  And you just don't remember and you certainly don't remember how much or

541
00:30:55,240 --> 00:30:58,840
Speaker 13:  what you fed them cause it's like two in the morning. So like how many

542
00:30:58,840 --> 00:31:02,520
Speaker 13:  ounces of milk did I feed? The baby is like, you are just not gonna remember

543
00:31:02,520 --> 00:31:05,600
Speaker 13:  this five hours from now. So you just track that stuff. Just have a log

544
00:31:05,600 --> 00:31:08,000
Speaker 13:  between you and your partner. Cuz ideally one of you is asleep while the

545
00:31:08,000 --> 00:31:11,360
Speaker 13:  other thing is happening. Ideally, I said sometimes it's just a pure panic

546
00:31:11,360 --> 00:31:14,640
Speaker 13:  situation. So we just constantly made choices

547
00:31:15,410 --> 00:31:19,400
Speaker 13:  to keep the kid away from the internet and at some

548
00:31:19,400 --> 00:31:23,320
Speaker 13:  point your parenting choice is going to be in order

549
00:31:23,320 --> 00:31:26,680
Speaker 13:  to make this child eat or survive a restaurant or be in an airplane,

550
00:31:26,760 --> 00:31:30,320
Speaker 13:  we're gonna give them a tablet and then all of it kind of goes up the window.

551
00:31:30,860 --> 00:31:34,840
Speaker 13:  But for us anyhow, our choice was very much no user

552
00:31:34,840 --> 00:31:38,480
Speaker 13:  accounts, no wifi connected toys, none of this stuff where I have to become

553
00:31:38,480 --> 00:31:42,440
Speaker 13:  the IT manager of like a very helpless human

554
00:31:42,440 --> 00:31:46,280
Speaker 13:  beings internet presence. Because once you open the door

555
00:31:46,470 --> 00:31:48,600
Speaker 13:  you're just fully down the road.

556
00:31:49,090 --> 00:31:49,580
Speaker 4:  Yeah.

557
00:31:49,580 --> 00:31:53,220
Speaker 3:  How about that? The, the only other big picture question I can think of that

558
00:31:53,220 --> 00:31:56,740
Speaker 3:  it would be useful to ask or to like have answers to pretty early on would

559
00:31:56,740 --> 00:32:00,620
Speaker 3:  be like how do we take and share pictures of our

560
00:32:00,620 --> 00:32:02,780
Speaker 3:  child in a way that makes us comfortable?

561
00:32:02,810 --> 00:32:06,620
Speaker 4:  Yeah, I I will say, so my, I have two

562
00:32:06,620 --> 00:32:10,580
Speaker 4:  nieces and my brother, I think I was impressed by this although I

563
00:32:10,580 --> 00:32:14,420
Speaker 4:  think maybe some people would be worried about it, but basically they didn't

564
00:32:14,420 --> 00:32:18,260
Speaker 4:  want to be on Facebook or putting like have

565
00:32:18,260 --> 00:32:21,820
Speaker 4:  for Facebook to have accessible record of various pictures of

566
00:32:21,950 --> 00:32:25,900
Speaker 4:  of the girls. And so, but you wanna

567
00:32:25,900 --> 00:32:29,180
Speaker 4:  share it with family. And so they set up this Tumblr

568
00:32:29,650 --> 00:32:33,380
Speaker 4:  that is public to the internet but that like only

569
00:32:33,380 --> 00:32:36,900
Speaker 4:  cer like only the family members really have the address to, and it doesn't

570
00:32:36,900 --> 00:32:40,760
Speaker 4:  have their names on it or any like words whatsoever. It's

571
00:32:40,760 --> 00:32:44,640
Speaker 4:  just a public facing webpage on the internet, which I

572
00:32:44,640 --> 00:32:48,120
Speaker 4:  think worked out for them. I mean the main thing is like grandma doesn't

573
00:32:48,120 --> 00:32:52,000
Speaker 4:  have to get a Facebook account or like maintain access to a Facebook account,

574
00:32:52,000 --> 00:32:55,800
Speaker 4:  which I think is probably good And like, you know, grandma

575
00:32:55,800 --> 00:32:59,680
Speaker 4:  doesn't really wanna see, Grandma doesn't wanna go on Facebook,

576
00:32:59,680 --> 00:33:03,640
Speaker 4:  she just wants to see pictures of her grandchildren. So yes, it's fine. Yeah.

577
00:33:04,070 --> 00:33:07,760
Speaker 4:  Yeah. I so I, I don't know if that's like, if that solves every problem

578
00:33:07,760 --> 00:33:10,720
Speaker 4:  but that is a baby picture hack I have heard.

579
00:33:11,210 --> 00:33:15,040
Speaker 13:  So that one I buy it right cuz you, you're not inside a big tech like

580
00:33:15,120 --> 00:33:18,680
Speaker 13:  Tumblr is not big tech. It's like small struggling tech.

581
00:33:18,990 --> 00:33:21,200
Speaker 4:  Yeah, small to medium tech I think. Yeah.

582
00:33:21,200 --> 00:33:24,720
Speaker 13:  But public to the internet terrifies me. Right. Like that's a, that's just

583
00:33:24,720 --> 00:33:28,040
Speaker 13:  a URL on the internet that could get picked up. That clear view. AI is looking

584
00:33:28,040 --> 00:33:30,120
Speaker 13:  at pictures like who knows, you can't see what's happening.

585
00:33:30,120 --> 00:33:33,840
Speaker 4:  Well it's not tied to their names. Right. So it's just these

586
00:33:33,840 --> 00:33:37,280
Speaker 4:  pictures of people on the internet. It's true. It could get scraped them.

587
00:33:37,430 --> 00:33:38,560
Speaker 4:  I mean they didn't,

588
00:33:38,770 --> 00:33:42,040
Speaker 13:  So one day this family is gonna be in a Walmart looking at like empty photo

589
00:33:42,040 --> 00:33:43,760
Speaker 13:  frames and being like, well that is us.

590
00:33:44,350 --> 00:33:44,840
Speaker 4:  Yeah.

591
00:33:46,230 --> 00:33:50,160
Speaker 13:  Very unfortunate. So we, we kind of went the other way

592
00:33:50,160 --> 00:33:54,080
Speaker 13:  which requires more trust but feels more closed. So

593
00:33:54,080 --> 00:33:58,040
Speaker 13:  we, we started with Apple Photos which seemed very closed share photo

594
00:33:58,040 --> 00:34:01,800
Speaker 13:  streams and then more people wanna be involved. And then in particular

595
00:34:01,800 --> 00:34:05,560
Speaker 13:  this thing about grandma wanting to see the photos, we wanted to set up

596
00:34:05,560 --> 00:34:08,600
Speaker 13:  digital photo frames that just had the newest photos on them for various

597
00:34:08,600 --> 00:34:12,080
Speaker 13:  people in our family. So then we switched to Google photos,

598
00:34:12,680 --> 00:34:16,200
Speaker 13:  which Google will insist as a closed private ecosystem.

599
00:34:17,020 --> 00:34:20,680
Speaker 13:  But we were able to just set up Google homes and then

600
00:34:20,680 --> 00:34:24,560
Speaker 13:  lots of digital photo frames, like just the ones you can buy have Google

601
00:34:24,560 --> 00:34:28,080
Speaker 13:  photos, connections. So they just, we just have one album that we add photos

602
00:34:28,080 --> 00:34:31,680
Speaker 13:  to and it shows up on picture frames in our various family member's house.

603
00:34:31,680 --> 00:34:35,000
Speaker 13:  So they don't have to do anything. And the more that I, and that feels like

604
00:34:35,000 --> 00:34:38,640
Speaker 13:  I am just in control of that database of pictures and I am

605
00:34:38,640 --> 00:34:42,120
Speaker 13:  control of who has access to it. And that feels

606
00:34:42,280 --> 00:34:46,000
Speaker 13:  important to me as she's gotten older, as Max has gotten older, I've stopped

607
00:34:46,000 --> 00:34:49,760
Speaker 13:  posting her Instagram as much. I feel like babies are fine and now

608
00:34:49,760 --> 00:34:53,480
Speaker 13:  she's like a person with a personality and I feel very like exploitive

609
00:34:53,480 --> 00:34:57,360
Speaker 13:  of her personality. Like that's just a choice that she's gonna have to make

610
00:34:57,360 --> 00:34:59,240
Speaker 13:  over time and I'm gonna tell her not to do it. But,

611
00:35:00,030 --> 00:35:03,040
Speaker 4:  Well I mean if you're, if you're going through the GR sponsorship

612
00:35:03,150 --> 00:35:04,360
Speaker 13:  When she was first

613
00:35:04,540 --> 00:35:05,680
Speaker 4:  Get brand deals and

614
00:35:05,680 --> 00:35:06,080
Speaker 13:  She was first

615
00:35:06,080 --> 00:35:07,080
Speaker 4:  Born, she's gotta be on the gram,

616
00:35:07,170 --> 00:35:10,680
Speaker 13:  We gotta get this kid in the catalog and then it turns out Benton doesn't

617
00:35:10,680 --> 00:35:12,640
Speaker 13:  exist anymore. So my one target was gone.

618
00:35:14,790 --> 00:35:17,680
Speaker 3:  I dunno, she's got good hair. You could work with that. Yeah. Lot of places

619
00:35:17,680 --> 00:35:20,560
Speaker 3:  to go. Any, are there any other like, I'm just even trying to think about

620
00:35:20,560 --> 00:35:24,520
Speaker 3:  like what are the questions you should be asking early on and like conversations

621
00:35:24,520 --> 00:35:26,960
Speaker 3:  you should be having? Are there other big ones we haven't hit on yet?

622
00:35:27,760 --> 00:35:31,360
Speaker 13:  The main ones are really just how public do you want your

623
00:35:31,360 --> 00:35:35,320
Speaker 13:  child's life to be? Yeah. Right. Like you start to have all these

624
00:35:35,320 --> 00:35:38,080
Speaker 13:  experiences and it turns out being a parent, parent is like among the most

625
00:35:38,080 --> 00:35:40,840
Speaker 13:  universal experience you can have. You can relate to all kinds of people

626
00:35:41,270 --> 00:35:44,600
Speaker 13:  just by talking about your child. That's why people talk about their kids

627
00:35:44,600 --> 00:35:47,760
Speaker 13:  so much. It's very annoying before you have a kid and then you have a kid

628
00:35:47,760 --> 00:35:50,240
Speaker 13:  and you realize why it keeps happening. There's a reason people wanna show

629
00:35:50,240 --> 00:35:50,440
Speaker 13:  you photos.

630
00:35:50,440 --> 00:35:52,920
Speaker 3:  There's kids in the weather. Like those are the only two things. Yeah.

631
00:35:52,920 --> 00:35:56,560
Speaker 13:  Sort of lots and lots of people have all Yeah. Which is great

632
00:35:56,940 --> 00:36:00,120
Speaker 13:  but there's an instinct to do it very publicly on the internet. There's

633
00:36:00,120 --> 00:36:03,800
Speaker 13:  an instinct to profit running for some people. Like you could be a, a

634
00:36:03,800 --> 00:36:07,240
Speaker 13:  parent YouTuber like all this stuff. I think you gotta make that decision

635
00:36:07,240 --> 00:36:11,040
Speaker 13:  early and like hold onto it because otherwise you're, I

636
00:36:11,270 --> 00:36:14,600
Speaker 13:  I think a lot of people are gonna grow up and unless your parents been talking

637
00:36:14,600 --> 00:36:18,320
Speaker 13:  about them in public for a very long time and they are not like they lose,

638
00:36:18,430 --> 00:36:22,040
Speaker 13:  they might lose a sense of agency over who they get to be cuz there's this

639
00:36:22,040 --> 00:36:24,800
Speaker 13:  rich backstory about their life. And I think you just have to make that

640
00:36:24,960 --> 00:36:28,040
Speaker 13:  decision that I have thought about that a lot with our kid. I've talked

641
00:36:28,040 --> 00:36:30,680
Speaker 13:  to other parents who think about that a lot. I think there's a rich and

642
00:36:30,680 --> 00:36:34,480
Speaker 13:  very debate about that in the world of the internet,

643
00:36:34,480 --> 00:36:38,040
Speaker 13:  like in the world of internet parenting. And so that's the one that I would

644
00:36:38,040 --> 00:36:41,480
Speaker 13:  just encourage people to think about. It's like really hard to see when

645
00:36:41,480 --> 00:36:45,280
Speaker 13:  you have a newborn. It's just a picture of a newborn like that is kind of

646
00:36:45,280 --> 00:36:49,000
Speaker 13:  a story about you and how much you're not sleeping but soon it becomes a

647
00:36:49,000 --> 00:36:51,040
Speaker 13:  story about them and you gotta flip that switch.

648
00:36:51,430 --> 00:36:54,920
Speaker 3:  Yeah. I will say the, the point that the internet is just there to make you

649
00:36:54,920 --> 00:36:58,320
Speaker 3:  afraid and sell you stuff is a useful one to remember and has been very useful

650
00:36:58,320 --> 00:37:01,960
Speaker 3:  for me to remember even in this like we're in the phase now of like trying

651
00:37:01,960 --> 00:37:05,120
Speaker 3:  to figure out like what happens when you give birth and all the things you

652
00:37:05,120 --> 00:37:07,840
Speaker 3:  have to do. And it's like I've started reading about sleep training and the

653
00:37:07,840 --> 00:37:10,320
Speaker 3:  only thing I've learned about sleep training is no matter what you do, you're

654
00:37:10,320 --> 00:37:14,040
Speaker 3:  wrong and you're a bad person and you hate your kids and it costs millions

655
00:37:14,040 --> 00:37:16,240
Speaker 3:  of dollars to buy all the right products to solve all of their problems.

656
00:37:16,260 --> 00:37:19,080
Speaker 3:  And so I just, like, every time I go on the internet now I have to just be

657
00:37:19,080 --> 00:37:21,920
Speaker 3:  like the internet is here to lie to me and sell me things and that is what

658
00:37:21,920 --> 00:37:25,840
Speaker 3:  this is for. And I, I am now like I am everyone's target demo,

659
00:37:25,840 --> 00:37:28,280
Speaker 3:  like new parents are everyone's target demo.

660
00:37:28,600 --> 00:37:31,440
Speaker 13:  Cause you're terrified. Yeah. Yeah. And you and you

661
00:37:32,520 --> 00:37:35,800
Speaker 13:  whatever, whatever you might say about modern society, you don't have all

662
00:37:35,800 --> 00:37:38,920
Speaker 13:  the people around you to keep you from being terrified, which is like a

663
00:37:38,920 --> 00:37:39,240
Speaker 13:  real thing.

664
00:37:39,800 --> 00:37:42,760
Speaker 3:  Right. We need to let you go. We have some more questions to get to you,

665
00:37:42,760 --> 00:37:46,400
Speaker 3:  but thank you and, and you know, good luck to all of us. Good luck Dave.

666
00:37:47,150 --> 00:37:47,960
Speaker 13:  I'll buy you some

667
00:37:47,960 --> 00:37:51,160
Speaker 3:  Booze. Alright Russell, we have a couple more. We're just gonna, we're gonna

668
00:37:51,160 --> 00:37:54,560
Speaker 3:  blow through these really fast one that we didn't

669
00:37:54,560 --> 00:37:57,600
Speaker 3:  technically choose to do here. But I just wanna talk about quickly cuz this,

670
00:37:57,600 --> 00:38:01,200
Speaker 3:  this is a thing that comes up a lot in conversations that we have with like

671
00:38:01,200 --> 00:38:05,120
Speaker 3:  regular people thinking about cybersecurity is VPNs. Liam, do we have Ben's

672
00:38:05,200 --> 00:38:06,720
Speaker 3:  question we can play about VPNs?

673
00:38:07,060 --> 00:38:10,600
Speaker 14:  Hey Verge, this is Ben. I've been thinking about

674
00:38:11,000 --> 00:38:14,840
Speaker 14:  subscribing to a VPN service, but it's very

675
00:38:14,970 --> 00:38:18,920
Speaker 14:  unclear which one is the best. Some of the most

676
00:38:19,270 --> 00:38:23,120
Speaker 14:  promoted ones like Express and Nord VPN

677
00:38:23,310 --> 00:38:27,080
Speaker 14:  have questionable audits. And yeah,

678
00:38:27,150 --> 00:38:31,120
Speaker 14:  just very curious about what VPN service people should

679
00:38:31,400 --> 00:38:32,360
Speaker 14:  subscribe to. Thanks.

680
00:38:33,110 --> 00:38:37,040
Speaker 3:  I love this question because I find find it completely impossible and I

681
00:38:37,040 --> 00:38:41,000
Speaker 3:  feel like we've talked a lot about just deciding who to trust and who not

682
00:38:41,000 --> 00:38:44,400
Speaker 3:  to trust in the course of this episode. Yeah, and I feel like this is sort

683
00:38:44,400 --> 00:38:48,240
Speaker 3:  of the same thing. Like fundamentally if you get

684
00:38:48,240 --> 00:38:52,120
Speaker 3:  a vpn you are, it it it's like, it's like letting someone into

685
00:38:52,120 --> 00:38:55,960
Speaker 3:  your house, right? Like you're, you are giving them a massive amount of

686
00:38:56,020 --> 00:38:59,080
Speaker 3:  theoretical access to your stuff and a lot of them say they don't want it.

687
00:38:59,080 --> 00:39:02,880
Speaker 3:  A lot of 'em say they don't store it, but like you just don't

688
00:39:02,880 --> 00:39:06,040
Speaker 3:  know for absolute certain and it is fundamentally a question of trust. But

689
00:39:06,040 --> 00:39:09,720
Speaker 3:  then if you don't do it, you're trusting, I don't know all of your browser

690
00:39:09,720 --> 00:39:13,080
Speaker 3:  extensions and you're trusting Google and you're trusting Facebook, which

691
00:39:13,080 --> 00:39:16,280
Speaker 3:  has all this kind of information. Like you just can't browse the internet

692
00:39:16,280 --> 00:39:19,480
Speaker 3:  without somebody knowing. And at some point you just have to decide who you

693
00:39:19,480 --> 00:39:22,320
Speaker 3:  trust the most. And I feel like it gets very nihilistic for me from there,

694
00:39:22,420 --> 00:39:24,040
Speaker 3:  but that's kind of where I land.

695
00:39:24,940 --> 00:39:28,470
Speaker 4:  Yeah, I, it's funny, I was, I was writing about,

696
00:39:28,910 --> 00:39:32,470
Speaker 4:  actually in the wifi coconut piece, there's this question of like, how much

697
00:39:32,680 --> 00:39:36,590
Speaker 4:  control does like a router, like if the bad guy is controlling the

698
00:39:36,590 --> 00:39:40,230
Speaker 4:  router that you're logging in through how like that's not good. How much

699
00:39:40,230 --> 00:39:44,150
Speaker 4:  of a threat is that? And one of the, I I was thinking about it and I

700
00:39:44,270 --> 00:39:48,110
Speaker 4:  realized one of the fundamental things you see in internet technology

701
00:39:48,360 --> 00:39:51,990
Speaker 4:  at all sorts of different levels is you're basically using

702
00:39:52,180 --> 00:39:56,150
Speaker 4:  encryption to secure info infrastructure. So it

703
00:39:56,150 --> 00:39:59,350
Speaker 4:  doesn't matter who's controlling the router, it doesn't matter who's at the

704
00:39:59,350 --> 00:40:03,310
Speaker 4:  is p you know, the router you, you'll have like WPA keys that are keeping

705
00:40:03,310 --> 00:40:07,110
Speaker 4:  it from, from sort of being a point of attack. The ISP

706
00:40:07,110 --> 00:40:10,870
Speaker 4:  ideal U have ssl and that's that level of encryption. The

707
00:40:10,870 --> 00:40:14,040
Speaker 4:  VPN is kind of like adding another one. And so

708
00:40:14,770 --> 00:40:18,200
Speaker 4:  there's a sense in which the attacks

709
00:40:18,310 --> 00:40:22,240
Speaker 4:  that it helps against are if you are

710
00:40:22,240 --> 00:40:25,400
Speaker 4:  working for a company like it's corporate security

711
00:40:25,820 --> 00:40:29,080
Speaker 4:  and they're like, these are the plans to our next hot new airplane

712
00:40:29,860 --> 00:40:33,720
Speaker 4:  and the bad guys wanna steal the plant. And so

713
00:40:34,410 --> 00:40:38,200
Speaker 4:  we know exactly where the perimeter is of like who we trust and who we

714
00:40:38,200 --> 00:40:42,040
Speaker 4:  don't. And if the VPN is running through the company, then ideally you're

715
00:40:42,040 --> 00:40:45,920
Speaker 4:  not expanding the trust at, you know, at all because it's, it's running

716
00:40:45,920 --> 00:40:49,320
Speaker 4:  through that same infrastructure. It's the same group of people who are gonna

717
00:40:49,320 --> 00:40:53,240
Speaker 4:  have access to it, but you can have sort of an extra

718
00:40:53,240 --> 00:40:56,520
Speaker 4:  layer of protection to log in through the conference wifi

719
00:40:57,020 --> 00:41:01,000
Speaker 4:  and not get injected because the only thing you connect to is

720
00:41:01,270 --> 00:41:05,200
Speaker 4:  this wi you know, this VPN service which then hardens the tunnel

721
00:41:05,200 --> 00:41:09,000
Speaker 4:  through the rest of it. Right. Having said that, that is not

722
00:41:09,160 --> 00:41:13,080
Speaker 4:  generally how I see people talking about why they're using

723
00:41:13,310 --> 00:41:17,240
Speaker 4:  VPNs. Oh honestly, the most, maybe this is cuz I'm hanging

724
00:41:17,240 --> 00:41:20,480
Speaker 4:  with a sketchy crowd, but like the most common thing is people like, well

725
00:41:20,480 --> 00:41:24,320
Speaker 4:  this will keep me from getting sued for pirating movies or

726
00:41:24,320 --> 00:41:24,760
Speaker 4:  something. Oh,

727
00:41:24,760 --> 00:41:28,360
Speaker 3:  Absolutely. Or this is how I can watch stuff on Netflix that's only available

728
00:41:28,450 --> 00:41:30,600
Speaker 3:  in another country. Those are the two things I hear the most.

729
00:41:30,720 --> 00:41:33,680
Speaker 4:  Right. Well I, I mean, so the Netflix

730
00:41:34,020 --> 00:41:37,880
Speaker 4:  geolocation thing that might be robust, I don't think that

731
00:41:37,880 --> 00:41:41,760
Speaker 4:  Netflix is really cracking down on it yet because it would just be so,

732
00:41:42,330 --> 00:41:46,160
Speaker 4:  it would be so much of a pain for them without, they don't really care if

733
00:41:46,160 --> 00:41:49,400
Speaker 4:  this is happening anyway. So, but anything illegal you're doing,

734
00:41:49,810 --> 00:41:53,720
Speaker 4:  if you're paying for a commercial VPN service, it's just

735
00:41:53,720 --> 00:41:57,600
Speaker 4:  another subpoena that the police have to send. So they subpoena the VPN

736
00:41:57,600 --> 00:42:01,080
Speaker 4:  service to then subpoena the ISP to then get to you

737
00:42:01,300 --> 00:42:04,960
Speaker 4:  and like maybe it's in Switzerland so it's harder,

738
00:42:04,980 --> 00:42:08,800
Speaker 4:  but like if you are the dread pirate Roberts or something, this is not going

739
00:42:08,800 --> 00:42:12,200
Speaker 4:  to help you. And I see people talking as if it is,

740
00:42:12,700 --> 00:42:16,600
Speaker 4:  and that, that to me is like my main experience

741
00:42:16,600 --> 00:42:20,440
Speaker 4:  of engaging with VPNs. Very rarely do I see it lining up with

742
00:42:20,540 --> 00:42:23,560
Speaker 4:  the actual threat model it could protect against

743
00:42:23,750 --> 00:42:27,520
Speaker 3:  Yeah. That, that totally tracks. And I think in general, we talked a little

744
00:42:27,520 --> 00:42:31,480
Speaker 3:  bit about VPNs with McKenna a while ago on this show, and one of the

745
00:42:31,480 --> 00:42:35,360
Speaker 3:  things I heard over and over even from VPN people is

746
00:42:35,360 --> 00:42:39,320
Speaker 3:  that if you're just a regular person doing regular internet things, you

747
00:42:39,520 --> 00:42:42,040
Speaker 3:  probably don't need one. And even to your point about like what if the bad

748
00:42:42,040 --> 00:42:45,880
Speaker 3:  guys are on the router, Like thanks to things like https, like they

749
00:42:45,880 --> 00:42:49,160
Speaker 3:  can start to know what website I went to,

750
00:42:49,460 --> 00:42:52,880
Speaker 3:  but not what I did there. And that stuff is

751
00:42:53,040 --> 00:42:56,320
Speaker 3:  increasingly encrypted and the internet is, is much harder as a result. And

752
00:42:56,320 --> 00:43:00,080
Speaker 3:  so the, the advice I've gotten from folks is basically like if you know

753
00:43:00,080 --> 00:43:03,840
Speaker 3:  you have a specific thing that either you need to avoid being

754
00:43:03,840 --> 00:43:07,680
Speaker 3:  seen for or you have a specific thing you

755
00:43:07,680 --> 00:43:10,320
Speaker 3:  need access to that you can't get access to, those are good reasons to get

756
00:43:10,320 --> 00:43:13,840
Speaker 3:  a vpn. But if you're just like a person in a coffee shop worried about security,

757
00:43:14,310 --> 00:43:18,160
Speaker 3:  a VPN to your point is not actually going to solve that

758
00:43:18,160 --> 00:43:20,040
Speaker 3:  sort of same mainstream problem.

759
00:43:20,270 --> 00:43:24,080
Speaker 4:  Yeah. Also, I trust my coffee shop more than I trust my

760
00:43:24,080 --> 00:43:27,600
Speaker 4:  family. That's, to me, that's the pinnacle of, they're so nice.

761
00:43:29,400 --> 00:43:33,280
Speaker 3:  Shout out to Russell's coffee shop somewhere in deep upstate New York,

762
00:43:33,310 --> 00:43:35,040
Speaker 3:  I think, I don't know who's

763
00:43:35,040 --> 00:43:35,280
Speaker 4:  To say.

764
00:43:35,970 --> 00:43:38,840
Speaker 3:  Russell doesn't tell us where he is. That's his real cybersecurity trick.

765
00:43:39,240 --> 00:43:42,320
Speaker 4:  Yeah. I I keep operational security. No one knows where I am at any

766
00:43:42,320 --> 00:43:45,560
Speaker 3:  Time. Yeah. He's in a, he's in a dark room with a black background and no

767
00:43:45,560 --> 00:43:48,000
Speaker 3:  one's allowed to know what he's doing. All right, let's move on. We have

768
00:43:48,000 --> 00:43:51,800
Speaker 3:  one more question to do before we get outta here. It is from Emmanuel.

769
00:43:51,800 --> 00:43:52,760
Speaker 3:  Let's hear it. Hey

770
00:43:52,760 --> 00:43:56,640
Speaker 15:  Verge, this is Emmanuel. I had a question related to your

771
00:43:56,920 --> 00:44:00,600
Speaker 15:  security podcast that's coming up. I've always wondered what

772
00:44:00,600 --> 00:44:04,480
Speaker 15:  could a bad website do to me? So for example, I,

773
00:44:04,550 --> 00:44:08,440
Speaker 15:  I clicked on a, on a, a bad ad on, on Google search results

774
00:44:08,460 --> 00:44:11,800
Speaker 15:  and I stayed on the website, didn't do anything. Then close my browser,

775
00:44:11,800 --> 00:44:15,480
Speaker 15:  closed the window. What is the worst thing that could have happened to my

776
00:44:15,480 --> 00:44:19,200
Speaker 15:  machine? Could, could and what could be installed? Like what do I have to

777
00:44:19,200 --> 00:44:20,160
Speaker 15:  worry about? Thanks.

778
00:44:20,160 --> 00:44:23,200
Speaker 3:  This is such a good question and made me realize I also have this question.

779
00:44:23,370 --> 00:44:27,360
Speaker 4:  It made me very worried for Emmanuel Safety. Like, we're gonna find

780
00:44:27,360 --> 00:44:29,840
Speaker 4:  out, this is the last communication anyone got

781
00:44:30,160 --> 00:44:33,400
Speaker 3:  Emmanuel's just clicking every link everywhere just to see

782
00:44:33,790 --> 00:44:37,200
Speaker 3:  like what could happen, what's the worst? So what's the answer? What is the

783
00:44:37,200 --> 00:44:37,800
Speaker 3:  worst that could happen?

784
00:44:39,310 --> 00:44:43,200
Speaker 4:  I, to me it's like, it's like what if the brakes stopped

785
00:44:43,200 --> 00:44:46,600
Speaker 4:  working on your car? It's like there are so many bad things that could happen.

786
00:44:47,110 --> 00:44:50,400
Speaker 4:  I almost don't even want to think about it. It's like, no, you should be

787
00:44:50,400 --> 00:44:51,400
Speaker 4:  able to use the Right, Well

788
00:44:51,400 --> 00:44:53,280
Speaker 3:  Let's let's frame this slightly differently then like

789
00:44:53,280 --> 00:44:54,240
Speaker 4:  Yeah, yeah. It's,

790
00:44:54,710 --> 00:44:58,600
Speaker 3:  I think one, one obvious bad thing that can happen to you on a computer

791
00:44:58,610 --> 00:45:02,600
Speaker 3:  is that like you go to a website and it installs a keylogger and takes

792
00:45:02,600 --> 00:45:05,920
Speaker 3:  your credit card information, right. Like that we grant, But I think the,

793
00:45:05,920 --> 00:45:09,600
Speaker 3:  what seems to me to be implicit in this question is if I go to a bad

794
00:45:09,800 --> 00:45:13,560
Speaker 3:  website and I leave that bad website before I do anything, can it do

795
00:45:13,800 --> 00:45:17,600
Speaker 3:  anything to my computer after I leave the website?

796
00:45:17,600 --> 00:45:21,080
Speaker 3:  And the answer to that I think is like unequivocally Yes, absolutely. Right?

797
00:45:21,080 --> 00:45:24,920
Speaker 4:  Yeah, because you loaded loading the website. When, when we say

798
00:45:24,920 --> 00:45:28,880
Speaker 4:  I'm on the website, what that means is the server

799
00:45:29,060 --> 00:45:32,600
Speaker 4:  has sent this information to

800
00:45:32,600 --> 00:45:36,480
Speaker 4:  your browser and your browser has used that information and sort of

801
00:45:36,510 --> 00:45:40,480
Speaker 4:  executed the files to build the thing that you're looking

802
00:45:40,480 --> 00:45:44,120
Speaker 4:  at. Right? But that could have also included, they're like,

803
00:45:44,420 --> 00:45:48,400
Speaker 4:  run this code, it'll make this fancy cool ad that with the

804
00:45:48,400 --> 00:45:52,360
Speaker 4:  little guy who dances around, he's gonna love it. And like it made

805
00:45:52,360 --> 00:45:55,760
Speaker 4:  a fancy cool ad with a guy who danced around and you did love it, but it

806
00:45:55,760 --> 00:45:58,960
Speaker 4:  also installed this malware, which is now like

807
00:45:59,420 --> 00:46:03,390
Speaker 4:  encrypting your entire computer and gonna ransomware you or just

808
00:46:03,390 --> 00:46:07,350
Speaker 4:  like hang out there for six months and gradually sort of expand its

809
00:46:07,350 --> 00:46:10,550
Speaker 4:  access in the network. I mean I think the, the tricky thing is once

810
00:46:11,050 --> 00:46:14,790
Speaker 4:  the perimeter has been breached, it's kind of like, well,

811
00:46:14,790 --> 00:46:18,070
Speaker 4:  what mischief do they want to do? One, they're on the inside. It's like,

812
00:46:18,070 --> 00:46:21,990
Speaker 4:  I, I don't know. I hope nothing too bad, but like it's out of my

813
00:46:21,990 --> 00:46:22,630
Speaker 4:  control now.

814
00:46:22,630 --> 00:46:25,670
Speaker 3:  Yeah. I I spent a lot of time in prepping for this, talking to people and

815
00:46:25,670 --> 00:46:29,150
Speaker 3:  reading about exploit kits. Oh yeah. It's just my god. Terrifying.

816
00:46:29,550 --> 00:46:32,030
Speaker 3:  And basically it's, it's exactly what you're talking about. The idea is like

817
00:46:32,030 --> 00:46:35,590
Speaker 3:  you, you load this website and what it does is it installs software on your

818
00:46:35,750 --> 00:46:38,990
Speaker 3:  computer that essentially just looks for vulnerabilities, right? And it says

819
00:46:38,990 --> 00:46:42,750
Speaker 3:  like, you know, do you have this technology to play

820
00:46:43,020 --> 00:46:46,590
Speaker 3:  videos in your browser? No. Do you have this one to read PDFs in your browser?

821
00:46:46,640 --> 00:46:50,110
Speaker 3:  Ah, you do. I'm gonna use that and just sort of open up a little hole in

822
00:46:50,110 --> 00:46:53,590
Speaker 3:  it and then download whatever I feel like it's like,

823
00:46:53,730 --> 00:46:57,510
Speaker 3:  and it's, it's, it's just once it's in it, it does sort of all this different

824
00:46:57,510 --> 00:47:00,750
Speaker 3:  hunting, it finds its place and then it can just sit there and it can wait

825
00:47:00,750 --> 00:47:04,710
Speaker 3:  as long as it needs to and then it just has access to your computer. And

826
00:47:04,710 --> 00:47:08,350
Speaker 3:  this stuff is like, it, it's dormant forever. It's tiny, it's hard to find

827
00:47:08,370 --> 00:47:12,310
Speaker 3:  and it just sits there sort of waiting to be activated. And it's gotten really

828
00:47:12,310 --> 00:47:15,590
Speaker 3:  sophisticated to the point where like, like flash, if I'm remembering this

829
00:47:15,590 --> 00:47:18,550
Speaker 3:  right, used to be like the main problem here, right? And it was like flash

830
00:47:18,550 --> 00:47:20,070
Speaker 3:  was a garbage piece of software.

831
00:47:20,070 --> 00:47:20,630
Speaker 4:  It was a very big

832
00:47:20,630 --> 00:47:24,550
Speaker 3:  Problem that had full of loopholes and exploits and every

833
00:47:24,550 --> 00:47:27,870
Speaker 3:  time you would load a flash thing it was loading a, an app from your computer

834
00:47:28,010 --> 00:47:31,800
Speaker 3:  and running it. And by doing so they could put code in it and then

835
00:47:31,800 --> 00:47:34,960
Speaker 3:  load other stuff. But there are still a bunch of ways like that into your

836
00:47:35,080 --> 00:47:35,520
Speaker 3:  computer now.

837
00:47:35,710 --> 00:47:39,640
Speaker 4:  Yeah, I would say, I mean the other thing that classically people were worried

838
00:47:39,640 --> 00:47:43,160
Speaker 4:  about, and this is something that, you know, in the time I've been working

839
00:47:43,160 --> 00:47:47,040
Speaker 4:  at the Verge https adoption, including by the verge.com,

840
00:47:47,300 --> 00:47:51,160
Speaker 4:  has just gone from sort of zero to 60. Yeah. Like it was, it was maybe

841
00:47:51,160 --> 00:47:54,960
Speaker 4:  a third of websites in 20 I it was, it was definitely below

842
00:47:54,960 --> 00:47:58,120
Speaker 4:  half and now we're well over 95%. It's sort of rare that you go somewhere

843
00:47:58,120 --> 00:48:02,040
Speaker 4:  on the internet that doesn't have it. The result of that is these

844
00:48:02,040 --> 00:48:05,920
Speaker 4:  injection attacks where someone is at the I S P or at the router or

845
00:48:06,330 --> 00:48:09,960
Speaker 4:  at sort of tapping into some cable because they're the NSA

846
00:48:10,180 --> 00:48:14,160
Speaker 4:  and they want to, you're sort of opening a legitimate

847
00:48:14,160 --> 00:48:18,040
Speaker 4:  website, but they are injecting malware into it as sort of turning

848
00:48:18,040 --> 00:48:21,920
Speaker 4:  it into a malicious website. You know, those are still

849
00:48:22,240 --> 00:48:25,480
Speaker 4:  possible, but they're much harder to do. And so

850
00:48:26,000 --> 00:48:29,920
Speaker 4:  seems to me in many ways things have gotten better. But it still is the

851
00:48:29,920 --> 00:48:33,720
Speaker 4:  case that you should not like clicking the problem

852
00:48:33,720 --> 00:48:37,680
Speaker 4:  is clicking the link to the website. Right. Not like once I'm on the

853
00:48:37,680 --> 00:48:39,920
Speaker 4:  website, what do I do? Yeah.

854
00:48:39,980 --> 00:48:43,960
Speaker 3:  But we should say, and I think I, I think not to like downplay any of this

855
00:48:43,960 --> 00:48:46,480
Speaker 3:  stuff, but I feel like it, it continues to be a useful thing to remind people

856
00:48:46,480 --> 00:48:49,840
Speaker 3:  that like most people get into trouble because they enter

857
00:48:49,840 --> 00:48:53,200
Speaker 3:  information in places that they shouldn't. And this is like what you were

858
00:48:53,200 --> 00:48:55,400
Speaker 3:  talking about earlier about like, don't give your credit card over the phone

859
00:48:55,400 --> 00:48:58,920
Speaker 3:  and this like basic stuff. But like anytime you're gonna type in your username

860
00:48:58,920 --> 00:49:01,520
Speaker 3:  and password, anytime you're gonna click on a link in your email, anytime

861
00:49:01,520 --> 00:49:04,280
Speaker 3:  you're gonna type in your social security number or your home address or

862
00:49:04,400 --> 00:49:07,520
Speaker 3:  whatever, like most people get into trouble because they give up information

863
00:49:07,520 --> 00:49:11,200
Speaker 3:  willingly to people. They shouldn't. Like the malware stuff is real, the

864
00:49:11,200 --> 00:49:13,680
Speaker 3:  exploit kit stuff is real. The stuff they can download is real. But like

865
00:49:13,680 --> 00:49:17,400
Speaker 3:  you are much more likely to get in trouble by reading your credit card number

866
00:49:17,400 --> 00:49:21,320
Speaker 3:  to someone you shouldn't than by like having your routers tapped by the nsa.

867
00:49:22,190 --> 00:49:26,000
Speaker 4:  Well and also I think I, I mean even more immediately, the thing

868
00:49:26,000 --> 00:49:29,640
Speaker 4:  right now that people are most likely to get taken by is

869
00:49:29,970 --> 00:49:33,280
Speaker 4:  someone calling you on the phone and telling you they're from Texas.

870
00:49:33,750 --> 00:49:34,240
Speaker 3:  Yeah.

871
00:49:34,300 --> 00:49:37,640
Speaker 4:  And it's like Microsoft will not call you on the phone

872
00:49:38,530 --> 00:49:42,320
Speaker 4:  to, because there's a problem with your office installation that will not

873
00:49:42,320 --> 00:49:46,240
Speaker 4:  happen. They, they, they have email, they're good

874
00:49:46,240 --> 00:49:49,240
Speaker 4:  at it. They know a lot about email. They'll send you an email and

875
00:49:49,240 --> 00:49:52,080
Speaker 3:  Mostly they're avoiding you. They don't want to call you, they don't want

876
00:49:52,080 --> 00:49:55,360
Speaker 3:  you to call them. Like if, if you ever get a customer support person who's

877
00:49:55,360 --> 00:49:58,400
Speaker 3:  like desperate to talk to you, that should set off all your alarm bells because

878
00:49:58,400 --> 00:50:00,920
Speaker 3:  that is not a thing that happens in real life. I'm just

879
00:50:00,920 --> 00:50:01,680
Speaker 4:  Saying. Absolutely.

880
00:50:02,990 --> 00:50:06,800
Speaker 3:  I like it. Alright. Any, any other like common sense security stuff we should

881
00:50:06,800 --> 00:50:09,480
Speaker 3:  tell people? I feel like the advice we always give is like two factor authentication

882
00:50:09,480 --> 00:50:10,400
Speaker 3:  is good. Do that.

883
00:50:10,760 --> 00:50:14,640
Speaker 4:  Yeah, absolutely. I would say, yeah. I mean, work on

884
00:50:14,640 --> 00:50:18,520
Speaker 4:  knife skills. Knife skills are always useful. That's more of a physical

885
00:50:18,800 --> 00:50:22,040
Speaker 4:  security thing, but I tell everyone, you know, practice with a knife,

886
00:50:22,040 --> 00:50:23,240
Speaker 3:  Online knife skills, couple a week.

887
00:50:23,240 --> 00:50:23,480
Speaker 4:  Yep.

888
00:50:23,800 --> 00:50:27,520
Speaker 3:  Yeah, I like it. All right, we should go. Russell. Thank you.

889
00:50:27,520 --> 00:50:31,040
Speaker 3:  This has been very fun and enlightening and if anybody ever wants to talk

890
00:50:31,040 --> 00:50:34,640
Speaker 3:  for many, many hours about pseudo random number generators, I now have all

891
00:50:34,640 --> 00:50:38,360
Speaker 3:  of this information in my head and nothing to do with it. I know which ones

892
00:50:38,360 --> 00:50:41,640
Speaker 3:  are good. I know which ones are bad. I have a lot of thoughts, but anyway,

893
00:50:41,640 --> 00:50:44,920
Speaker 3:  follow all of us on Twitter. There's a ton more good cybersecurity stuff

894
00:50:44,920 --> 00:50:48,560
Speaker 3:  we've done at the Verge the last couple of weeks and always because it is

895
00:50:48,560 --> 00:50:52,120
Speaker 3:  always cybersecurity week on the verge.com. Russell is Russell

896
00:50:52,120 --> 00:50:55,800
Speaker 3:  Brando on Twitter. Ne Eli is reckless on Twitter. I'm pierce on Twitter.

897
00:50:55,800 --> 00:50:58,800
Speaker 3:  Thank you for listening. This is the Verge Cast. We will be back on Wednesday

898
00:50:58,800 --> 00:51:01,480
Speaker 3:  and Friday with our regularly scheduled programming. We will see you then.

899
00:51:01,480 --> 00:51:02,040
Speaker 3:  Rock and roll

900
00:51:19,200 --> 00:51:23,110
Speaker 1:  Support for this episode comes from Zelle. You'd never

901
00:51:23,110 --> 00:51:27,070
Speaker 1:  fall for an online scam, right? You use

902
00:51:27,070 --> 00:51:30,670
Speaker 1:  two factor authentication. Ignore calls from everyone named

903
00:51:30,900 --> 00:51:33,950
Speaker 1:  spam risk and never use the password.

904
00:51:35,030 --> 00:51:35,190
Speaker 1:  Password.

905
00:51:36,810 --> 00:51:40,430
Speaker 1:  But scammers are getting more sophisticated and more active,

906
00:51:40,430 --> 00:51:44,350
Speaker 1:  which means they're finding millions of new victims every single

907
00:51:44,350 --> 00:51:48,270
Speaker 1:  year. The good news is that there's a lot you can do to protect

908
00:51:48,470 --> 00:51:51,910
Speaker 1:  yourself on the Wild, wild Web. For starters,

909
00:51:52,030 --> 00:51:55,790
Speaker 1:  Zelle wants to remind you only send money to people you know

910
00:51:55,850 --> 00:51:59,830
Speaker 1:  and trust. Zelle is available to United States Bank account holders.

911
00:51:59,830 --> 00:52:01,510
Speaker 1:  Only terms and conditions apply.

