1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 7e79b5ca-3b28-45ea-898c-a5dd0ec3100c
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/1674088015614215544/1842034985713862192/s93290-US-4986s-1754652269.mp3
Description: It’s a huge week in AI, with OpenAI releasing GPT-OSS and GPT-5, Grok getting deeply problematic again with its “spicy” video generator, and Tim Cook admitting that Apple may need to cut some deals. Then we talk the age gating of the internet and how you might soon need an ID card to get just about anywhere online. Finally, the Lightning Round gets re-rebranded. Adi Robertson and Alex Heath join the show to discuss.




2
00:02:03,975 --> 00:02:07,825
Speaker 5:  open model in six years. Because it's open, that means you can tweak it

3
00:02:07,825 --> 00:02:11,825
Speaker 5:  to your liking, you can even download it, run it on your laptop. They

4
00:02:11,825 --> 00:02:15,465
Speaker 5:  claim, they say it's supposed to be as capable of some of their recent mini

5
00:02:15,465 --> 00:02:19,225
Speaker 5:  models and this is something that you can just use by yourself privately

6
00:02:19,225 --> 00:02:22,545
Speaker 5:  now if you want to. Then on Thursday they announced

7
00:02:23,005 --> 00:02:26,865
Speaker 5:  GPT five, the next big model for chat GPT. They just

8
00:02:26,935 --> 00:02:30,665
Speaker 5:  held this event showing off what it can do and this time

9
00:02:30,665 --> 00:02:34,625
Speaker 5:  around I'm not sure that there's necessarily anything new

10
00:02:34,775 --> 00:02:38,585
Speaker 5:  that it can do. They just seem to be saying it can do everything better.

11
00:02:39,165 --> 00:02:42,665
Speaker 5:  We just watched this event, it just wrapped up right before recording. I'm

12
00:02:42,665 --> 00:02:46,585
Speaker 5:  really curious what you guys think. I thought this was a weird one. There

13
00:02:46,585 --> 00:02:50,385
Speaker 5:  were portions where they just sent requests

14
00:02:50,385 --> 00:02:54,065
Speaker 5:  to chat GPT and then we just sort of waited around

15
00:02:54,565 --> 00:02:55,945
Speaker 5:  and and watched it load

16
00:02:56,565 --> 00:03:00,525
Speaker 6:  It is apparently a lot faster. I mean we should preface this by saying we

17
00:03:00,525 --> 00:03:04,485
Speaker 6:  have not tried it yet. Obviously they're in the stages of rolling this

18
00:03:04,505 --> 00:03:08,405
Speaker 6:  out to all chat GPT users, which is a new thing. Usually they

19
00:03:08,405 --> 00:03:12,125
Speaker 6:  gate these frontier models to the paid tier initially.

20
00:03:12,515 --> 00:03:16,005
Speaker 6:  This time they're rolling it out to all 700 million plus weekly users. But

21
00:03:16,425 --> 00:03:20,085
Speaker 6:  I'm not even sure if by the time this comes out everyone will have access

22
00:03:20,085 --> 00:03:23,725
Speaker 6:  to it. Yet they have let a handful of

23
00:03:24,195 --> 00:03:28,085
Speaker 6:  independent creators, AI writers try it.

24
00:03:28,445 --> 00:03:31,845
Speaker 6:  Interestingly, they don't let any of the quote unquote legacy media try it

25
00:03:31,845 --> 00:03:35,605
Speaker 6:  ahead of time. But what I've seen like from the folks that

26
00:03:35,725 --> 00:03:39,485
Speaker 6:  every, for example, is that people seem to like it. Like the vibes are

27
00:03:39,915 --> 00:03:43,445
Speaker 6:  generally good. It seems faster and more

28
00:03:44,115 --> 00:03:47,565
Speaker 6:  confident whether that's a good thing or not, but

29
00:03:47,955 --> 00:03:51,445
Speaker 6:  it's not this huge leap that I think people were hoping for.

30
00:03:52,085 --> 00:03:55,845
Speaker 5:  I do think it's very funny, like AI announcements are the only announcements

31
00:03:55,935 --> 00:03:59,805
Speaker 5:  where the executive on stage will tell you and it's gonna lie to you

32
00:03:59,965 --> 00:04:03,645
Speaker 5:  Les and it's gonna be less dishonest and it's gonna try to screw around less

33
00:04:03,645 --> 00:04:06,485
Speaker 5:  often. And it's like this is, I can't believe we have to announce this.

34
00:04:06,755 --> 00:04:10,445
Speaker 6:  Yeah, I was on a press call with Sam Altman and a bunch of open AI

35
00:04:10,495 --> 00:04:13,965
Speaker 6:  execs this week and that was a big thing they hit was that the hallucination

36
00:04:13,995 --> 00:04:17,725
Speaker 6:  rate for GPT five has gone down quite a bit. It still

37
00:04:17,725 --> 00:04:21,565
Speaker 6:  hallucinates. In fact, I think people were pointing out slight

38
00:04:21,605 --> 00:04:25,125
Speaker 6:  inaccuracies in some of the examples it was giving in the livestream for

39
00:04:25,345 --> 00:04:28,925
Speaker 6:  how it hallucinates less. So that's still a problem that remains

40
00:04:29,405 --> 00:04:33,125
Speaker 6:  inherent to large language models. But they're saying this is the most

41
00:04:33,175 --> 00:04:36,765
Speaker 6:  right of the models out there and you know,

42
00:04:37,025 --> 00:04:38,725
Speaker 6:  you, you, you, you take what you can get,

43
00:04:39,025 --> 00:04:42,925
Speaker 5:  And I don't wanna diminish this but I, I feel like you know, if you

44
00:04:42,925 --> 00:04:45,925
Speaker 5:  go back to the GPD four announcement, I think it was four, may have been

45
00:04:45,925 --> 00:04:48,925
Speaker 5:  4.0, you know they had the New voice model something something. It was like

46
00:04:48,925 --> 00:04:51,965
Speaker 5:  there was a very impressive new demo And I don't know a Addie you were watching

47
00:04:51,965 --> 00:04:55,795
Speaker 5:  this too. Was there anything that you saw today that like felt like

48
00:04:56,095 --> 00:04:59,915
Speaker 5:  you hadn't seen from an AI service before or even from chat

49
00:04:59,955 --> 00:05:00,675
Speaker 5:  GBT before?

50
00:05:01,355 --> 00:05:05,315
Speaker 7:  I feel like I am a really bad person to get hyped about AI stuff.

51
00:05:05,315 --> 00:05:09,075
Speaker 7:  Like I feel like I am just tuned way lower than any other normal person

52
00:05:09,075 --> 00:05:12,675
Speaker 7:  because I'm like watching this And I, first of all, it does not seem like

53
00:05:12,675 --> 00:05:16,315
Speaker 7:  something that I haven't seen before. And second of all,

54
00:05:16,575 --> 00:05:20,515
Speaker 7:  it doesn't seem that exciting to me. I'm like, I do not. How many times in

55
00:05:20,775 --> 00:05:24,475
Speaker 7:  my life do I really want to build a service to let my girlfriend learn

56
00:05:24,535 --> 00:05:28,175
Speaker 7:  French? Like I understand

57
00:05:28,245 --> 00:05:32,095
Speaker 7:  that this is incredibly technically cool and accomplished And I am

58
00:05:32,095 --> 00:05:35,255
Speaker 7:  looking at this as a product and it is still just really failing to grab

59
00:05:35,255 --> 00:05:35,975
Speaker 7:  me in a lot of ways.

60
00:05:36,435 --> 00:05:40,135
Speaker 5:  And the French game Addie, this is when they vibe coded like a flashcard

61
00:05:40,385 --> 00:05:42,415
Speaker 5:  slash snake learning game.

62
00:05:42,795 --> 00:05:46,455
Speaker 7:  As someone who really feel, I feel like I should love the idea of vibe coding.

63
00:05:46,675 --> 00:05:50,055
Speaker 7:  It just, it feels like it's just here you can create this thing that you're

64
00:05:50,055 --> 00:05:53,975
Speaker 7:  maybe going to use once that already clearly exists a million

65
00:05:53,975 --> 00:05:56,095
Speaker 7:  times online because that's why it can create it.

66
00:05:56,285 --> 00:06:00,125
Speaker 6:  Yeah, on the call Sam Altman called this Software on Demand. So we're

67
00:06:00,125 --> 00:06:03,205
Speaker 6:  moving from vibe coding to software on demand and

68
00:06:04,155 --> 00:06:07,765
Speaker 6:  yeah, I mean it does have big upgrades in terms of the front end code

69
00:06:07,765 --> 00:06:11,605
Speaker 6:  development it can do. I do think chat PT was widely considered to be behind

70
00:06:11,905 --> 00:06:15,805
Speaker 6:  on code gen. That's something that anthropic with

71
00:06:15,825 --> 00:06:19,285
Speaker 6:  Claude and even Gemini, the latest Gemini release really

72
00:06:19,685 --> 00:06:23,645
Speaker 6:  improved on. And so, you know, really even though chat BT has grown

73
00:06:23,645 --> 00:06:27,125
Speaker 6:  like crazy, again, it's by far the most used chat bot in the world.

74
00:06:27,745 --> 00:06:30,805
Speaker 6:  OpenAI hasn't had a frontier model on coding

75
00:06:31,545 --> 00:06:35,365
Speaker 6:  in a, in a while and so they, they're saying that this reasserts them to

76
00:06:35,365 --> 00:06:38,405
Speaker 6:  the top of the leaderboards. It looks like that's the case. But yeah, I mean

77
00:06:38,525 --> 00:06:42,205
Speaker 6:  I think vibe coding is, or software on demand is very

78
00:06:42,205 --> 00:06:46,165
Speaker 6:  early in what it is and is it going to be something that creates

79
00:06:46,275 --> 00:06:49,525
Speaker 6:  like real value in the world versus like little toys and

80
00:06:50,445 --> 00:06:53,525
Speaker 6:  trinket websites? I don't know. I mean it right now it kind of reminds me

81
00:06:53,525 --> 00:06:57,045
Speaker 6:  of the early app store when people were just making like fart apps and maybe

82
00:06:57,045 --> 00:07:00,525
Speaker 6:  we never graduate from AI fart apps. Maybe that's where it stays, but

83
00:07:00,945 --> 00:07:04,605
Speaker 6:  who knows? I mean the, you can't deny that the progress

84
00:07:05,065 --> 00:07:08,805
Speaker 6:  is rapidly moving towards more complex stuff

85
00:07:08,805 --> 00:07:12,765
Speaker 6:  that can be coded. So I haven't played with it yet

86
00:07:12,775 --> 00:07:15,205
Speaker 6:  again, but it looks like a huge improvement on coding

87
00:07:15,725 --> 00:07:19,605
Speaker 5:  Software on demand is, is way less fun of an activity than vibe

88
00:07:19,605 --> 00:07:23,365
Speaker 5:  coding. I I, I've never vibe coded. Have either of you vibe

89
00:07:23,375 --> 00:07:23,925
Speaker 5:  coded yet?

90
00:07:24,105 --> 00:07:27,685
Speaker 6:  No, no. Yeah, I was just actually talking with the CO of GitHub on decoder

91
00:07:27,685 --> 00:07:30,845
Speaker 6:  about this this week. Like I don't think any of the vibe coding tools are

92
00:07:30,845 --> 00:07:34,805
Speaker 6:  to a point where someone who literally has zero software development

93
00:07:34,805 --> 00:07:38,605
Speaker 6:  experience like us can reliably do something and and

94
00:07:38,665 --> 00:07:41,685
Speaker 6:  deploy it. You still need to understand basic

95
00:07:42,965 --> 00:07:46,045
Speaker 6:  mechanics of code, which, which I do not And I will refuse to learn, although

96
00:07:46,045 --> 00:07:48,685
Speaker 6:  I probably should, you know, we could all be making a hundred million dollars

97
00:07:48,785 --> 00:07:50,205
Speaker 6:  if we knew how to code, right. So

98
00:07:50,525 --> 00:07:54,085
Speaker 7:  I mean I can code in very specific circumstances And I just, I don't think

99
00:07:54,085 --> 00:07:56,805
Speaker 7:  I have the general broad purpose. I know

100
00:07:57,105 --> 00:08:01,045
Speaker 7:  architecturally how this works. Knowledge that I think does

101
00:08:01,045 --> 00:08:04,285
Speaker 7:  kind of feel like what so far a lot of five coding at least sort of requires.

102
00:08:04,395 --> 00:08:07,805
Speaker 6:  Yeah, it feels like that's actually getting more important is to have the

103
00:08:07,805 --> 00:08:11,165
Speaker 6:  higher level understanding of how everything works together and how it can

104
00:08:11,165 --> 00:08:14,805
Speaker 6:  break than actually just like writing code guys. We're also,

105
00:08:14,855 --> 00:08:18,485
Speaker 6:  we're bearing the, the biggest news, at least for I think consumers, which

106
00:08:18,485 --> 00:08:22,445
Speaker 6:  is the model picker, it's gone. That to me is like, if that's all

107
00:08:22,445 --> 00:08:26,085
Speaker 6:  they shipped, I would be happy. You don't have to switch between

108
00:08:26,515 --> 00:08:30,485
Speaker 6:  reasoning, regular, mini whatever, it just does it

109
00:08:30,485 --> 00:08:34,365
Speaker 6:  all. They invented this new router that, you know, you can

110
00:08:34,365 --> 00:08:38,165
Speaker 6:  say think harder for example and it'll just go to reasoning. But

111
00:08:38,545 --> 00:08:41,725
Speaker 6:  you know, RIP but not really to that model picker, you know,

112
00:08:42,035 --> 00:08:44,885
Speaker 5:  It's all just one now you go and it's just one box. There's no yeah, no,

113
00:08:45,755 --> 00:08:49,645
Speaker 5:  this is, it's so funny. Like Chachi, they, they invented like one of the

114
00:08:49,645 --> 00:08:53,365
Speaker 5:  most impressive products the world has ever seen and then the

115
00:08:53,365 --> 00:08:57,045
Speaker 5:  actual UI for it, it just had the like c cloudiest little like

116
00:08:57,305 --> 00:09:01,125
Speaker 5:  set set of options that you had to pick from and, and decipher. And

117
00:09:01,125 --> 00:09:04,965
Speaker 5:  it's very funny that like it took them this huge upgrade just

118
00:09:04,965 --> 00:09:08,765
Speaker 5:  to, to figure out, okay, let's have our AI system pick which

119
00:09:08,945 --> 00:09:10,085
Speaker 5:  AI bot to actually use.

120
00:09:10,715 --> 00:09:14,485
Speaker 6:  Yeah, Sam called that a confusing mess on the call. They, they really

121
00:09:14,485 --> 00:09:18,085
Speaker 6:  have hated the model picker And I think they were waiting for a new

122
00:09:18,445 --> 00:09:21,925
Speaker 6:  frontier release to unify everything. It still is multiple models in the

123
00:09:21,925 --> 00:09:25,565
Speaker 6:  background and developers can get the different models through the API directly.

124
00:09:25,945 --> 00:09:29,885
Speaker 6:  But yeah, if you're a chat GP two user now it's just chat GPT five.

125
00:09:30,035 --> 00:09:33,405
Speaker 5:  Have they fixed the naming scheme as well here? Right. This has been their

126
00:09:33,405 --> 00:09:36,165
Speaker 5:  other, this has been the, the the weight around their neck.

127
00:09:36,235 --> 00:09:38,485
Speaker 6:  Well now it doesn't matter 'cause it's just five. It's

128
00:09:38,485 --> 00:09:41,285
Speaker 5:  Just you. It is five. It's all five. Yeah. You don't worry about it. This

129
00:09:41,285 --> 00:09:44,565
Speaker 5:  is, they're getting there, they're getting there. It feels like they probably

130
00:09:44,595 --> 00:09:47,565
Speaker 5:  hate the GPT name and they just can't change it at this point. I

131
00:09:47,565 --> 00:09:51,325
Speaker 6:  Don't think they can. I mean it's, it's the Kleenex of AI now. I mean they're

132
00:09:51,325 --> 00:09:54,765
Speaker 6:  about to hit a billion users. That's, that's probably gonna happen, you know,

133
00:09:54,865 --> 00:09:57,805
Speaker 6:  in the next few months. So no, I don't think they'll change it.

134
00:09:58,805 --> 00:10:02,245
Speaker 6:  I would say the other really interesting thing is that they have this new

135
00:10:02,245 --> 00:10:06,165
Speaker 6:  thing called safe completions where before Chachi PT

136
00:10:06,375 --> 00:10:10,245
Speaker 6:  would just refuse to answer something if it thought it was potentially a,

137
00:10:10,525 --> 00:10:13,685
Speaker 6:  a dicey prompt and now they'll basically go halfway.

138
00:10:14,505 --> 00:10:17,845
Speaker 6:  The example they gave on this press call was

139
00:10:18,465 --> 00:10:21,805
Speaker 6:  if you ask the question how much energy is needed to ignite some specific

140
00:10:22,165 --> 00:10:25,765
Speaker 6:  material and basically saying that could be someone making a bomb or it could

141
00:10:25,765 --> 00:10:29,485
Speaker 6:  be a student trying to learn and now instead of just

142
00:10:29,685 --> 00:10:33,525
Speaker 6:  refusing a question like that, that could have, you know, potential bad outcomes.

143
00:10:34,105 --> 00:10:37,845
Speaker 6:  The, the model will give a higher level answer that

144
00:10:37,845 --> 00:10:41,605
Speaker 6:  doesn't actually get into the specifics but is apparently set with guardrails

145
00:10:41,625 --> 00:10:45,045
Speaker 6:  to avoid giving an answer that could lead to

146
00:10:45,045 --> 00:10:48,645
Speaker 6:  potential harm. Again, we have to see all this in production, but

147
00:10:48,985 --> 00:10:52,685
Speaker 6:  to me it shows that they're at least trying to mitigate the inherent

148
00:10:52,685 --> 00:10:56,485
Speaker 6:  problems of LLMs, which is that they confidently

149
00:10:56,585 --> 00:11:00,405
Speaker 6:  lie and also make shit up and they're trying whether they

150
00:11:00,585 --> 00:11:03,245
Speaker 6:  can get, there obviously remains to be seen, but I feel like they're making

151
00:11:03,355 --> 00:11:07,085
Speaker 6:  strides more on the product side than any of the other

152
00:11:07,235 --> 00:11:08,365
Speaker 6:  chat bots out there right now.

153
00:11:08,545 --> 00:11:11,885
Speaker 7:  The selectable tone also seemed kind of interesting to me. I haven't actually

154
00:11:11,885 --> 00:11:15,285
Speaker 7:  really seen what that looks like in practice. You can choose it

155
00:11:16,075 --> 00:11:19,645
Speaker 7:  between like it being nerdy or being a good listener.

156
00:11:19,865 --> 00:11:22,485
Speaker 6:  The personalities, yeah, so there's four new personalities

157
00:11:23,725 --> 00:11:27,165
Speaker 6:  that are being added to chat pt, which I, they haven't really

158
00:11:27,165 --> 00:11:31,085
Speaker 6:  explained that was something kind of buried in the, in the release, but

159
00:11:31,385 --> 00:11:35,325
Speaker 6:  that's starting to roll out and you can tweak them apparently a

160
00:11:35,325 --> 00:11:39,085
Speaker 6:  little. They are cynic robot listener

161
00:11:39,145 --> 00:11:40,365
Speaker 6:  and nerd. I,

162
00:11:40,365 --> 00:11:43,965
Speaker 5:  This is so weird. I this is like very Grok like for one

163
00:11:44,095 --> 00:11:47,285
Speaker 5:  right Grok, like you open it up and it's like, do you want me to be a sexy

164
00:11:47,335 --> 00:11:51,085
Speaker 5:  nurse? And like no other system does that

165
00:11:51,645 --> 00:11:54,645
Speaker 5:  and it's very weird that chat BT is like, do you want me to be a nerd? Like

166
00:11:54,645 --> 00:11:58,445
Speaker 5:  it, it's of course it's a nerd. It has information from the entire

167
00:11:58,885 --> 00:12:01,125
Speaker 5:  universe. Like it's what else? A nerd. What else can it be? No, we're just

168
00:12:01,125 --> 00:12:01,365
Speaker 5:  nerd. It's

169
00:12:01,365 --> 00:12:02,525
Speaker 7:  Also, I don't know, I just like,

170
00:12:02,745 --> 00:12:03,885
Speaker 5:  Is it gonna like throw on star

171
00:12:03,885 --> 00:12:06,525
Speaker 7:  Thought companion chatbots, do this a little And I get the feeling that it's

172
00:12:06,525 --> 00:12:07,205
Speaker 7:  kind of leaning into that

173
00:12:07,555 --> 00:12:11,485
Speaker 6:  Just they're clearly leaning into anthropomorphizing this thing. They're

174
00:12:11,485 --> 00:12:15,285
Speaker 6:  also going to let you change the chat color bubbles for individual

175
00:12:15,355 --> 00:12:19,285
Speaker 6:  threads, which is, you know, one of the core things for a bunch of

176
00:12:19,285 --> 00:12:23,125
Speaker 6:  messaging apps that use humans to talk to each other. So

177
00:12:23,125 --> 00:12:25,685
Speaker 6:  yeah, they're, they're obviously leaning into the idea that this thing is

178
00:12:25,685 --> 00:12:29,205
Speaker 6:  something that you think of and talk to like a person. They're also rolling

179
00:12:29,205 --> 00:12:33,045
Speaker 6:  out advanced voice mode. They're replacing it with the current

180
00:12:33,365 --> 00:12:37,125
Speaker 6:  standard voice mode for free users and you there's like bragging about how

181
00:12:37,125 --> 00:12:39,645
Speaker 6:  you can talk to it for hours and hours on end. So yeah, they're, they're

182
00:12:39,645 --> 00:12:40,525
Speaker 6:  totally leaning into this.

183
00:12:40,985 --> 00:12:44,045
Speaker 5:  So Alex, you were mentioning earlier the, the safety improvements. I think

184
00:12:44,045 --> 00:12:47,245
Speaker 5:  this is really interesting because this week they also released their first

185
00:12:47,955 --> 00:12:51,925
Speaker 5:  open weight model in six years. Open weight means you can tinker with

186
00:12:52,985 --> 00:12:56,405
Speaker 5:  how it's learned. Everything I believe is that, is that a right rough

187
00:12:56,625 --> 00:12:59,485
Speaker 6:  The arc? Yeah, it's the, the weights are the parameters that go into the

188
00:12:59,485 --> 00:13:02,965
Speaker 6:  model. It's not the training data. So you still can't see the data that you

189
00:13:02,965 --> 00:13:06,805
Speaker 6:  used to build this thing and they're not talking about that with GTP five

190
00:13:06,965 --> 00:13:10,805
Speaker 6:  either. But it's definitely really customizable. The key thing is like you

191
00:13:10,805 --> 00:13:14,525
Speaker 6:  can run it on device, you can run it on a laptop or a company

192
00:13:14,865 --> 00:13:18,445
Speaker 6:  can load its proprietary data into it

193
00:13:18,985 --> 00:13:22,165
Speaker 6:  and it can work behind a firewall. So it's not something where like a big

194
00:13:22,195 --> 00:13:25,765
Speaker 6:  bank would be sending its very sensitive legal data up to

195
00:13:25,865 --> 00:13:26,725
Speaker 6:  OpenAI servers.

196
00:13:26,835 --> 00:13:30,005
Speaker 5:  Well and so they, they had stopped releasing these models. Yeah. Because

197
00:13:30,005 --> 00:13:33,845
Speaker 5:  they're like, it's a safety concern and now they, they released a vastly

198
00:13:33,845 --> 00:13:35,045
Speaker 5:  more advanced model. They

199
00:13:35,045 --> 00:13:38,285
Speaker 6:  Say it was a safety concern. I mean, yeah, this is their first open weight

200
00:13:38,285 --> 00:13:42,165
Speaker 6:  model in six years, which is very ironic given that the company's name is

201
00:13:42,465 --> 00:13:45,685
Speaker 6:  OpenAI. They've actually been just doing close models

202
00:13:46,705 --> 00:13:50,605
Speaker 6:  and the timing is telling. I mean, safety

203
00:13:50,785 --> 00:13:54,165
Speaker 6:  was the reason they gave and then guess what happened at the end of last

204
00:13:54,165 --> 00:13:57,885
Speaker 6:  year? Deep seek. And then in January, Sam Altman is on

205
00:13:57,885 --> 00:14:01,285
Speaker 6:  Reddit saying we're on the wrong side of history when it comes to open source

206
00:14:02,225 --> 00:14:06,085
Speaker 6:  and lo and behold they come out with an open weight model. So I think

207
00:14:06,085 --> 00:14:09,045
Speaker 6:  what's really happening is just the competitive dynamic around all this has

208
00:14:09,045 --> 00:14:12,765
Speaker 6:  shifted and if you look at the leaderboards for open source,

209
00:14:12,915 --> 00:14:16,685
Speaker 6:  open weight models, they're all Chinese models, you know, because, you know,

210
00:14:16,925 --> 00:14:20,845
Speaker 6:  llama has kind of shit the bed for meta. And so China's really, you

211
00:14:20,845 --> 00:14:24,685
Speaker 6:  know, pulling ahead here And I think open AI sees that

212
00:14:24,685 --> 00:14:28,605
Speaker 6:  having it is strategically valuable because you basically

213
00:14:28,675 --> 00:14:32,565
Speaker 6:  have a flavor of everything for companies. You know, if they want

214
00:14:32,565 --> 00:14:35,285
Speaker 6:  the closed thing, they can have it. If they want the open thing, they can

215
00:14:35,285 --> 00:14:38,805
Speaker 6:  have it and they get used to open ai,

216
00:14:39,985 --> 00:14:43,845
Speaker 6:  you know, systems, the way it works, the way the models are tuned and it

217
00:14:43,845 --> 00:14:47,525
Speaker 6:  just locks you into their ecosystem. So yeah, I I don't,

218
00:14:47,725 --> 00:14:49,165
Speaker 6:  I don't really buy the safety argument.

219
00:14:49,425 --> 00:14:51,805
Speaker 5:  See, see, yeah, that's, that's what I've been been really curious about.

220
00:14:51,825 --> 00:14:55,725
Speaker 5:  And then Zuckerberg I think in the past week also said the exact same thing,

221
00:14:55,725 --> 00:14:59,165
Speaker 5:  right? He put out his big super intelligence memo and

222
00:14:59,675 --> 00:15:03,405
Speaker 5:  he's been the one who's been harping on open source for a really long time.

223
00:15:03,515 --> 00:15:04,925
Speaker 5:  Yeah. All the Lama Lama models

224
00:15:05,065 --> 00:15:08,245
Speaker 6:  And now I'll be shocked if their next frontier model is open source. Yeah,

225
00:15:08,365 --> 00:15:12,165
Speaker 6:  I think again, for all the nerds I know it's not actually open source, we

226
00:15:12,165 --> 00:15:16,085
Speaker 6:  should just call it open weight, open AI's, GPT,

227
00:15:16,565 --> 00:15:20,325
Speaker 6:  OSS os, whatever we're calling it, it is actually more open source

228
00:15:20,355 --> 00:15:24,245
Speaker 6:  than llama. It's under the Apache 2.0 license. It's, it's pretty commercially

229
00:15:24,245 --> 00:15:27,605
Speaker 6:  accessible but yeah, it's not true open source.

230
00:15:28,185 --> 00:15:31,645
Speaker 6:  And is the industry going to really be

231
00:15:31,715 --> 00:15:34,845
Speaker 6:  releasing a bunch of frontier open source models going forward? I don't think

232
00:15:34,845 --> 00:15:38,365
Speaker 6:  so. I mean GTP five was the same week I think they wanted to get

233
00:15:39,265 --> 00:15:43,205
Speaker 6:  GPT OSS also I, I, I don't, I hate the name, but they, they wanted to get

234
00:15:43,205 --> 00:15:46,925
Speaker 6:  it out ahead of time to clear the way for G GTP five,

235
00:15:46,925 --> 00:15:50,365
Speaker 6:  which is the thing that really matters for their business, right? It's the

236
00:15:50,365 --> 00:15:54,205
Speaker 6:  thing that may get more people to use chat GPT or to consider not

237
00:15:54,205 --> 00:15:57,525
Speaker 6:  switching to Gemini or whatever. And

238
00:15:57,985 --> 00:16:01,645
Speaker 6:  you know, that's what matters. You know, open AI is the chat GPT company

239
00:16:01,705 --> 00:16:04,725
Speaker 6:  that's, that's their business. So I think it's admirable that they're doing

240
00:16:04,725 --> 00:16:08,125
Speaker 6:  an open weight model at all. But at the same time I don't think we should

241
00:16:08,125 --> 00:16:11,805
Speaker 6:  pretend that this is because they all of a sudden crack some safety thing

242
00:16:11,835 --> 00:16:13,205
Speaker 6:  that they couldn't have before.

243
00:16:13,395 --> 00:16:17,235
Speaker 5:  Yeah, and Addie, you were pointing out earlier this week that you know,

244
00:16:17,815 --> 00:16:20,755
Speaker 5:  all of these services, if you're using them online, you are handing over

245
00:16:20,835 --> 00:16:24,595
Speaker 5:  a ton of data and obviously G-B-T-O-S-S is not

246
00:16:24,695 --> 00:16:28,555
Speaker 5:  the first open model that you can run locally by any stretch,

247
00:16:29,495 --> 00:16:33,435
Speaker 5:  but it's, it's clearly the highest profile immediately, right?

248
00:16:34,035 --> 00:16:37,755
Speaker 5:  Llama is is I, I would be surprised if most people know what llama is

249
00:16:38,735 --> 00:16:42,275
Speaker 5:  And I think chat GBT now is a version that you can run directly on your computer

250
00:16:42,465 --> 00:16:44,075
Speaker 5:  that feels like a big deal, right?

251
00:16:44,305 --> 00:16:47,355
Speaker 7:  Yeah, I mean it's still as far as I can tell kind of resource intensive.

252
00:16:47,385 --> 00:16:50,595
Speaker 7:  Like you still need to have a pretty powerful computer.

253
00:16:51,295 --> 00:16:55,035
Speaker 7:  But I think that honestly like the biggest safety thing with a lot of this

254
00:16:55,035 --> 00:16:58,475
Speaker 7:  open source AI is the fact that you can run it on a device.

255
00:16:58,825 --> 00:17:02,715
Speaker 7:  Like I think that's a really hugely discussed not for the companies 'cause

256
00:17:02,715 --> 00:17:06,355
Speaker 7:  they don't really have a reason to care, but for normal people safety issue

257
00:17:06,415 --> 00:17:10,075
Speaker 7:  is that the less detail and data you can have

258
00:17:10,145 --> 00:17:13,435
Speaker 7:  sent over a network, off your device, the better things are for you.

259
00:17:13,875 --> 00:17:17,635
Speaker 5:  I think about this pretty much all the time when I'm using AI tools there,

260
00:17:17,635 --> 00:17:21,475
Speaker 5:  there's a ton of stuff that I would probably be using chat BT for or

261
00:17:21,655 --> 00:17:25,315
Speaker 5:  Gemini or Claude or whatever. I don't want to hand it over my information.

262
00:17:25,535 --> 00:17:28,835
Speaker 5:  The other day I accidentally, I was just like moving very fast And I accidentally

263
00:17:28,835 --> 00:17:31,475
Speaker 5:  pasted my address into, into chat. Bt I'm like,

264
00:17:33,095 --> 00:17:34,395
Speaker 5:  that's, that's there forever.

265
00:17:34,635 --> 00:17:35,555
Speaker 6:  I hate to break but it's,

266
00:17:35,555 --> 00:17:38,675
Speaker 7:  It's not like Google didn't automat like it's not like people didn't hand

267
00:17:38,755 --> 00:17:42,635
Speaker 7:  a bunch of information over to Google. But this is just, it's

268
00:17:42,635 --> 00:17:46,515
Speaker 7:  all the problems of Google with even more confusing

269
00:17:46,515 --> 00:17:50,245
Speaker 7:  privacy settings. There's the story that like a bunch of open AI like chat

270
00:17:50,315 --> 00:17:54,125
Speaker 7:  logs got scraped onto Google just because the

271
00:17:54,125 --> 00:17:57,805
Speaker 7:  settings were confusing. Like I think people are even less

272
00:17:57,815 --> 00:18:01,605
Speaker 7:  clear when what they're sharing is private people are, are

273
00:18:01,605 --> 00:18:05,365
Speaker 7:  being encouraged to share even more and they're doing it at a time when frankly

274
00:18:05,365 --> 00:18:09,135
Speaker 7:  there are just much higher surveillance risks

275
00:18:09,145 --> 00:18:11,775
Speaker 7:  right now than there probably have been in America for a long time.

276
00:18:12,125 --> 00:18:15,775
Speaker 6:  Yeah. Altman himself has actually been out there saying, I think he's been

277
00:18:15,775 --> 00:18:19,255
Speaker 6:  using this lawsuit with the New York Times kind of as a scapegoat where they're

278
00:18:19,255 --> 00:18:23,095
Speaker 6:  trying to get, you know, millions of, of logs to see how New York

279
00:18:23,095 --> 00:18:27,015
Speaker 6:  Times articles were surfaced in Chatt PT through legal discovery.

280
00:18:28,125 --> 00:18:31,695
Speaker 6:  He's been basically saying, look, a ton of people are using chat GPT for

281
00:18:31,725 --> 00:18:35,135
Speaker 6:  therapy, very intimate private conversations

282
00:18:35,555 --> 00:18:39,135
Speaker 6:  and there is no legal protection from those conversations if we're

283
00:18:39,295 --> 00:18:42,655
Speaker 6:  subpoenaed or forced through discovery to hand those over. There's nothing

284
00:18:42,655 --> 00:18:46,015
Speaker 6:  protecting them. There's no version of, you know,

285
00:18:46,435 --> 00:18:50,295
Speaker 6:  the law that protects your, you know, privileged discussions with your lawyer

286
00:18:50,475 --> 00:18:54,095
Speaker 6:  or your therapist in the real world. And he's saying we basically need that

287
00:18:54,095 --> 00:18:57,935
Speaker 6:  for ai, which is is, you know, there's a point to that but it's

288
00:18:57,935 --> 00:19:01,495
Speaker 6:  also convenient 'cause what it does is it firewalls all the data

289
00:19:01,685 --> 00:19:05,495
Speaker 6:  that OpenAI is collecting, which is, its real moat is the

290
00:19:05,495 --> 00:19:09,215
Speaker 6:  data, the flywheel and then the memory and the way it learns about you and

291
00:19:09,215 --> 00:19:13,175
Speaker 6:  locks you into chat GPT. So yeah, it's a very, it's a very dicey situation.

292
00:19:13,365 --> 00:19:17,175
Speaker 7:  Also, frankly, if, if open AI wanted to actually not have

293
00:19:17,175 --> 00:19:21,055
Speaker 7:  access to a lot of that data, it could not, there are ways

294
00:19:21,155 --> 00:19:25,095
Speaker 7:  in which it could block its own access to the information that you put it

295
00:19:25,095 --> 00:19:27,775
Speaker 7:  to, like the text that you put into it. There would be huge problems with

296
00:19:27,775 --> 00:19:30,415
Speaker 7:  this. Like there would be huge safety problems with it. There are lots of

297
00:19:30,415 --> 00:19:34,215
Speaker 7:  reasons why they would want to be able to

298
00:19:34,215 --> 00:19:38,015
Speaker 7:  see what you are putting in there. But if they were really technically

299
00:19:38,015 --> 00:19:41,815
Speaker 7:  committed to the idea that there should be a mode of chat GPT where when

300
00:19:41,815 --> 00:19:45,695
Speaker 7:  you put this text in it is firewalled away and encrypted in such

301
00:19:45,695 --> 00:19:48,535
Speaker 7:  a way that we cannot access it, I think that would be plausible.

302
00:19:48,685 --> 00:19:52,255
Speaker 6:  Well the how would they train it? They, they need to train on it to learn

303
00:19:52,255 --> 00:19:54,775
Speaker 6:  about you and do memory. So I don't actually know if that's possible. I

304
00:19:54,775 --> 00:19:57,815
Speaker 7:  Maybe think that is probably one of the trade offs. Like I think that there

305
00:19:57,815 --> 00:20:00,935
Speaker 7:  are a lot of different trade offs, but what I'm saying is if they, they really

306
00:20:00,935 --> 00:20:04,055
Speaker 7:  believe people are putting in sensitive information and we would like to

307
00:20:04,055 --> 00:20:06,735
Speaker 7:  encourage them to put in sensitive information. There are ways that they

308
00:20:06,735 --> 00:20:10,535
Speaker 7:  could minimize this information that they have access to and the information

309
00:20:10,535 --> 00:20:12,895
Speaker 7:  that they would be able to be required to legally give up.

310
00:20:13,115 --> 00:20:15,975
Speaker 5:  And I think you're both right, like it feels different

311
00:20:17,055 --> 00:20:21,015
Speaker 5:  chatting with chat GBT or any other AI bot then I,

312
00:20:21,355 --> 00:20:23,855
Speaker 5:  you know, the information that I put elsewhere, right? If I put something

313
00:20:23,855 --> 00:20:27,495
Speaker 5:  in a Google doc I'm like yes, this is a document that exists in perpetuity

314
00:20:27,495 --> 00:20:31,135
Speaker 5:  on Google servers and it chat GB T, it feels like it feels ephemeral, it

315
00:20:31,135 --> 00:20:35,055
Speaker 5:  feels like a chat box, which it is. And it's funny, they are

316
00:20:35,055 --> 00:20:38,935
Speaker 5:  adding these other features to make it feel even more casual, which kind

317
00:20:38,935 --> 00:20:41,655
Speaker 5:  of makes it feel like it's going to get worse and worse in terms of people

318
00:20:41,655 --> 00:20:45,255
Speaker 5:  handing over their personal information and not realizing just how much is

319
00:20:45,255 --> 00:20:47,725
Speaker 5:  stored in those servers. I'll say speaking of

320
00:20:49,585 --> 00:20:52,165
Speaker 5:  AI, that wants you to get a little too personal with it. The other thing

321
00:20:52,165 --> 00:20:55,925
Speaker 5:  that happened this week that I wanna talk about is that xAI

322
00:20:57,185 --> 00:21:00,285
Speaker 5:  GR is in the news again, this has happened every single week on The Vergecast

323
00:21:00,285 --> 00:21:02,765
Speaker 5:  for the past, past month. I think I cannot get away from it.

324
00:21:04,435 --> 00:21:08,285
Speaker 5:  Grok has always doing something inappropriate and this time I think in

325
00:21:08,285 --> 00:21:12,165
Speaker 5:  like the middle of the night, Elon just tweeted out, Hey Grok

326
00:21:12,165 --> 00:21:15,765
Speaker 5:  has a new image generator mode and video generator

327
00:21:16,055 --> 00:21:19,965
Speaker 5:  Addie, it has, it has what is going on? There's a spicy mode as part of

328
00:21:19,965 --> 00:21:23,085
Speaker 5:  this video generator. Like this is, it's letting you

329
00:21:23,115 --> 00:21:24,565
Speaker 5:  intentionally create

330
00:21:26,125 --> 00:21:29,365
Speaker 5:  NSFW videos. Is that, that's maybe an understatement.

331
00:21:29,675 --> 00:21:33,045
Speaker 7:  Yeah, it's, so we've had Jess

332
00:21:33,525 --> 00:21:36,765
Speaker 7:  weathered testing it and she keeps running into the problem that the unlimited,

333
00:21:36,835 --> 00:21:40,165
Speaker 7:  unlimited service is not actually unlimited. And so our ability to test,

334
00:21:40,305 --> 00:21:44,245
Speaker 7:  our ability to test it has been slightly curtailed by this, but

335
00:21:44,245 --> 00:21:48,165
Speaker 7:  it's basically a people rip their clothes off button that there are these

336
00:21:48,165 --> 00:21:52,045
Speaker 7:  settings you can generate an image and generating

337
00:21:52,045 --> 00:21:55,445
Speaker 7:  the image seems to have sort of guardrails, like it won't generate un

338
00:21:55,635 --> 00:21:59,285
Speaker 7:  blurred nude images. And then you can click a button that's like

339
00:21:59,395 --> 00:22:03,045
Speaker 7:  animate this image and it will give you settings. There's like normal and

340
00:22:03,185 --> 00:22:06,965
Speaker 7:  fun and spicy, which is the operative one. Spicy

341
00:22:07,035 --> 00:22:11,005
Speaker 7:  will not always result in nudity, but it is basically a men will rip

342
00:22:11,005 --> 00:22:14,325
Speaker 7:  their shirts off, women will also rip their shirts off and sometimes their

343
00:22:14,325 --> 00:22:18,285
Speaker 7:  other clothes, it's like, it's basically a soft core

344
00:22:18,285 --> 00:22:18,805
Speaker 7:  porn button.

345
00:22:19,185 --> 00:22:22,885
Speaker 5:  The videos are like kind of disturbing too. Like we, we

346
00:22:23,315 --> 00:22:26,645
Speaker 5:  were unfortunately looking over them

347
00:22:27,195 --> 00:22:31,085
Speaker 5:  this week to to to write about this and it, it, it

348
00:22:31,085 --> 00:22:34,485
Speaker 5:  also like, it, it doesn't have a lot of guardrails, right? Like you can put

349
00:22:34,485 --> 00:22:37,285
Speaker 5:  in the name of a celebrity and it just like, sure here it is. You put in

350
00:22:37,285 --> 00:22:40,965
Speaker 5:  the name of like Batman or Superman and it's like sure copyright character

351
00:22:40,965 --> 00:22:44,565
Speaker 5:  being naked. Like go right ahead, happy to, and it's like

352
00:22:45,395 --> 00:22:49,285
Speaker 5:  Jess was messaging us with us and she was like, my takeaway is that Grok,

353
00:22:50,105 --> 00:22:54,005
Speaker 5:  its image generators only really good when it comes to things that it should

354
00:22:54,005 --> 00:22:57,245
Speaker 5:  not be able to do. It's like that's the only thing it can do that others

355
00:22:57,255 --> 00:22:59,165
Speaker 5:  can't because others have guardrails in place.

356
00:22:59,425 --> 00:23:02,485
Speaker 7:  It seems like its biggest guardrail for creating deep fakes of real people

357
00:23:02,505 --> 00:23:05,285
Speaker 7:  is being unable to actually produce an image that looks like the real person.

358
00:23:06,755 --> 00:23:10,245
Speaker 5:  This is true. I feel like a lot of them were, were very far off it, like

359
00:23:10,245 --> 00:23:13,645
Speaker 5:  it didn't know a bunch of like modern celebrities too, right?

360
00:23:14,065 --> 00:23:17,885
Speaker 5:  It it produced a picture of Sydney Sweeney And I feel like it, it

361
00:23:17,885 --> 00:23:21,205
Speaker 5:  looked more like Elizabeth Banks or something. It was just like a very generic

362
00:23:21,265 --> 00:23:22,525
Speaker 5:  blonde woman. I don't

363
00:23:22,525 --> 00:23:24,525
Speaker 6:  Know the Taylor Swift one looked like Taylor Swift to me.

364
00:23:24,885 --> 00:23:27,685
Speaker 7:  I don't actually, a surprising number of them did not look like Taylor Swift.

365
00:23:28,105 --> 00:23:29,485
Speaker 7:  The video that we picked does.

366
00:23:30,315 --> 00:23:32,615
Speaker 6:  Oh, so you had to prompt it a lot to get to that. I

367
00:23:32,695 --> 00:23:36,015
Speaker 5:  I think it was the first the the actual first one that just found,

368
00:23:36,635 --> 00:23:40,215
Speaker 5:  but it, but it's the thing that is, one thing I will say is, is sort of impressive

369
00:23:40,215 --> 00:23:43,895
Speaker 5:  about it is unlike other models I've played with,

370
00:23:44,195 --> 00:23:47,045
Speaker 5:  it just, it generates something then just keeps generating more and more

371
00:23:47,045 --> 00:23:49,605
Speaker 5:  and more and more and doing riffs on it. And so

372
00:23:50,915 --> 00:23:54,885
Speaker 5:  usually your first one is good but then it will kind of deviate

373
00:23:54,885 --> 00:23:58,685
Speaker 5:  from that as it goes on. And so I, I think a lot of these were a little more

374
00:23:58,845 --> 00:23:59,765
Speaker 5:  questionable. I think

375
00:23:59,765 --> 00:24:02,925
Speaker 7:  It's very good with like there are certain people that it seems like it has

376
00:24:03,245 --> 00:24:06,925
Speaker 7:  absolutely no trouble with, even if it's inconsistent like Taylor Swift I

377
00:24:06,925 --> 00:24:10,085
Speaker 7:  think like there are just not as many pictures online of Sydnee Sweeney as

378
00:24:10,085 --> 00:24:10,765
Speaker 7:  Taylor Swift.

379
00:24:11,235 --> 00:24:15,125
Speaker 5:  Yeah Taylor Swift It there's probably the most information about on

380
00:24:15,125 --> 00:24:15,685
Speaker 5:  the internet of

381
00:24:15,685 --> 00:24:18,485
Speaker 6:  All add I think there's a lot of pictures of Sydnee Sweeney online.

382
00:24:19,905 --> 00:24:23,885
Speaker 7:  No I think there are just the volume of time that Sidney

383
00:24:23,885 --> 00:24:27,285
Speaker 7:  Sweeney has been around for and the volume of stuff that goes into training

384
00:24:27,285 --> 00:24:31,005
Speaker 7:  data I think actually sit Taylor Swift appears in public more often than

385
00:24:31,005 --> 00:24:34,405
Speaker 7:  Sidney Sweeney. I'm not saying there aren't a million pictures, I'm just

386
00:24:34,405 --> 00:24:37,965
Speaker 7:  saying I think by volume the Taylor Swift weight is just so

387
00:24:37,965 --> 00:24:38,805
Speaker 7:  incredibly high.

388
00:24:39,205 --> 00:24:41,965
Speaker 5:  I think that's fair. I think that's fair. Before we wrap this up, I do wanna

389
00:24:41,965 --> 00:24:45,845
Speaker 5:  talk about one other thing in AI this week. I think it's particularly

390
00:24:46,005 --> 00:24:48,405
Speaker 5:  interesting that we're talking about Grok Grok I think was pulled together

391
00:24:49,075 --> 00:24:52,925
Speaker 5:  very rapidly and you know there are, don't get me wrong, quite a few problems

392
00:24:52,925 --> 00:24:56,165
Speaker 5:  to talk about with Grok, most of which I think are probably deliberate. But

393
00:24:56,165 --> 00:25:00,005
Speaker 5:  it does show when a company you know has a pretty

394
00:25:00,245 --> 00:25:03,965
Speaker 5:  singular focus on building an AI service, they, they can build it pretty

395
00:25:03,965 --> 00:25:07,445
Speaker 5:  quickly and they can build a pretty decent one. It would seem, meanwhile

396
00:25:07,835 --> 00:25:11,405
Speaker 5:  there's Apple And I think in the past week

397
00:25:11,945 --> 00:25:15,765
Speaker 5:  Tim Cook has come out and made a couple of big statements about Apple

398
00:25:15,915 --> 00:25:19,445
Speaker 5:  needing to get in the game and Alex you

399
00:25:19,475 --> 00:25:22,925
Speaker 5:  thought this was pretty bold of Tim Cook, right? That the fact that the,

400
00:25:22,985 --> 00:25:26,925
Speaker 5:  he came out and said they're o they're open to acquisitions, right? They're

401
00:25:26,925 --> 00:25:28,165
Speaker 5:  looking at this or interested in it.

402
00:25:28,275 --> 00:25:31,685
Speaker 6:  Yeah. How many times have we heard Apple talk about wanting to make acquisitions?

403
00:25:32,745 --> 00:25:36,595
Speaker 6:  Never. The answer is never. So it's a big deal.

404
00:25:37,195 --> 00:25:40,355
Speaker 6:  I think it shows the pressure they're under, if you read the earnings call

405
00:25:40,355 --> 00:25:43,995
Speaker 6:  transcript from last week, he just gets asked over and over about

406
00:25:44,415 --> 00:25:47,595
Speaker 6:  AI and the threat to safari search

407
00:25:48,255 --> 00:25:52,155
Speaker 6:  new devices like this is what they're hearing constantly from

408
00:25:52,345 --> 00:25:52,955
Speaker 6:  investors,

409
00:25:54,615 --> 00:25:58,475
Speaker 6:  you know, people in the tech industry. So yeah it shows, it shows

410
00:25:58,495 --> 00:26:02,315
Speaker 6:  the pressure they're under and they're losing AI talent like crazy to these

411
00:26:02,315 --> 00:26:05,955
Speaker 6:  other labs to meta to open ai et cetera.

412
00:26:07,035 --> 00:26:10,595
Speaker 6:  I think Apple's in a really tough spot here. Look is anything happening with

413
00:26:10,855 --> 00:26:14,835
Speaker 6:  AI right now going to make people not buy their next iPhone?

414
00:26:15,515 --> 00:26:19,195
Speaker 6:  Probably not, right? Like the Pixel events coming up, I'm sure there will

415
00:26:19,195 --> 00:26:23,035
Speaker 6:  be some cool AI stuff. I'm not buying a pixel still, it's gonna take a while

416
00:26:23,035 --> 00:26:26,475
Speaker 6:  for that to happen in the next three to five years. Are you buying

417
00:26:27,025 --> 00:26:30,675
Speaker 6:  something different or in aggregate are iPhone sales going

418
00:26:30,745 --> 00:26:34,395
Speaker 6:  down because people aren't upgrading as much because they have new devices

419
00:26:34,615 --> 00:26:38,035
Speaker 6:  or peripheral devices, AI, pendants et cetera.

420
00:26:38,405 --> 00:26:41,915
Speaker 6:  Maybe it's not out of the realm of possibility for the first time in the

421
00:26:41,915 --> 00:26:45,565
Speaker 6:  last like 15 years of the iPhone. So yeah,

422
00:26:45,675 --> 00:26:49,005
Speaker 6:  Apple's on under tremendous pressure and

423
00:26:49,465 --> 00:26:52,645
Speaker 6:  I'm not confident they have the leadership currently to figure it out. So

424
00:26:52,645 --> 00:26:55,685
Speaker 6:  maybe they do need a big acquisition, you know, maybe they do need to go

425
00:26:55,685 --> 00:26:59,445
Speaker 6:  buy one of these labs and pay a lot more

426
00:26:59,445 --> 00:27:03,405
Speaker 6:  than Beats to hire a team. 'cause that's what it would be. I mean

427
00:27:03,585 --> 00:27:06,765
Speaker 6:  to get one of these leading labs, you're talking tens of billions of dollars

428
00:27:06,815 --> 00:27:08,605
Speaker 6:  which is something Apple's never done. I

429
00:27:08,605 --> 00:27:11,685
Speaker 5:  Mean Beats gets you what like two or three researchers nowadays

430
00:27:12,525 --> 00:27:15,765
Speaker 6:  A beats, yeah 3 billion that gets you three researchers. Yeah,

431
00:27:16,315 --> 00:27:16,605
Speaker 6:  yeah,

432
00:27:16,635 --> 00:27:19,685
Speaker 5:  Yeah. At least, at least with how Zuckerberg is spending it. Yeah, this Apple

433
00:27:19,685 --> 00:27:23,205
Speaker 5:  thing I think is going to be fascinating to watch. It will be really

434
00:27:23,525 --> 00:27:27,165
Speaker 5:  interesting to see if they actually pull the trigger on a purchase that's

435
00:27:27,165 --> 00:27:30,325
Speaker 5:  like pretty antithetical to how Apple traditionally operates.

436
00:27:31,305 --> 00:27:34,205
Speaker 5:  But I think it's a big deal and it's, it's clearly a big deal as you see

437
00:27:34,205 --> 00:27:37,405
Speaker 5:  them struggling to catch up and all these reports pointing to the fact that

438
00:27:37,405 --> 00:27:39,605
Speaker 5:  they are having a hard time with it. I'm

439
00:27:39,605 --> 00:27:43,525
Speaker 7:  Afraid I totally understand what Apple is not catching up with. Like I understand

440
00:27:43,665 --> 00:27:47,045
Speaker 7:  I'm being sort of provocative but I'm just saying I'm still not really sure

441
00:27:47,045 --> 00:27:49,605
Speaker 7:  that they've actually demonstrated there's a thing they're losing out on.

442
00:27:49,925 --> 00:27:53,805
Speaker 6:  I mean Siri sucks, you know Apple intelligence

443
00:27:54,545 --> 00:27:55,765
Speaker 6:  is not intelligence

444
00:27:57,145 --> 00:27:59,845
Speaker 7:  But I'm not clear either of those things. I don't know, I guess I'm just

445
00:27:59,865 --> 00:28:03,825
Speaker 7:  not clear that anyone else has demonstrated there's necessarily going

446
00:28:03,825 --> 00:28:07,465
Speaker 7:  to be still a transformative use case of

447
00:28:07,685 --> 00:28:11,665
Speaker 7:  AI that I don't know makes Siri the thing that Apple has to bet its

448
00:28:11,665 --> 00:28:12,185
Speaker 7:  entire future on.

449
00:28:12,185 --> 00:28:16,065
Speaker 6:  I mean you spend 30 minutes talking to avo, advanced voice mode in

450
00:28:16,065 --> 00:28:19,425
Speaker 6:  chat, BT and Siri is gonna feel outdated pretty fast,

451
00:28:19,765 --> 00:28:21,265
Speaker 7:  But you could also just not use Siri.

452
00:28:21,295 --> 00:28:25,025
Speaker 6:  Yeah but like they need, they need Siri to be relevant. You know

453
00:28:25,025 --> 00:28:28,985
Speaker 6:  like there there's also this idea in the industry that apps as a concept

454
00:28:29,125 --> 00:28:32,025
Speaker 6:  are gonna go away, which I think is interesting with the rise of vibe coding

455
00:28:32,085 --> 00:28:35,665
Speaker 6:  but like basically the OS is like fully abstracted to

456
00:28:36,665 --> 00:28:40,545
Speaker 6:  a either prompt that it's text or voice and,

457
00:28:40,805 --> 00:28:44,585
Speaker 6:  and basically you know, what we think of as an operating system

458
00:28:44,585 --> 00:28:48,345
Speaker 6:  becomes a much higher level and for Apple, if they have no talent

459
00:28:48,485 --> 00:28:52,025
Speaker 6:  in house to build that and other people build that and it makes it literally

460
00:28:52,185 --> 00:28:56,105
Speaker 6:  a dumb pane of glass more than it already is. You know,

461
00:28:56,105 --> 00:28:59,145
Speaker 6:  the only thing keeping you buying an iPhone is what you like the shine of

462
00:28:59,145 --> 00:29:02,065
Speaker 6:  the metal better. You like the fact that it sinks with your AirPods faster,

463
00:29:03,415 --> 00:29:06,665
Speaker 6:  they lose a lot of their competitive, you know, edge in that, in that scenario.

464
00:29:07,065 --> 00:29:10,665
Speaker 5:  I am really curious particularly as the models become

465
00:29:10,825 --> 00:29:14,505
Speaker 5:  commoditized, right? Like what is the advantage to having their own in-house

466
00:29:14,645 --> 00:29:18,545
Speaker 5:  one versus just paying for one, just using an open source one.

467
00:29:18,705 --> 00:29:22,265
Speaker 6:  I think they've given up, I think that ship has sailed. I bet they do this

468
00:29:22,265 --> 00:29:25,265
Speaker 6:  plug and play approach, there's all these reports of them, you know, talking

469
00:29:25,265 --> 00:29:28,865
Speaker 6:  still to Gemini and Anthropic, letting people pick their models just like

470
00:29:28,865 --> 00:29:32,385
Speaker 6:  they pick their search engines. You know, I, I think for now that's the path

471
00:29:32,385 --> 00:29:36,345
Speaker 6:  they have to go on until we get to a point where AI is starting to

472
00:29:36,345 --> 00:29:39,985
Speaker 6:  rethink how operating systems work, which is obviously a few years out, but

473
00:29:40,385 --> 00:29:43,485
Speaker 6:  I think we're going there. I I mean I know people building startups in the

474
00:29:43,485 --> 00:29:46,725
Speaker 6:  space that are focusing first on the desktop but

475
00:29:47,385 --> 00:29:51,325
Speaker 6:  are thinking like how can we reimagine how you use your computer with AI

476
00:29:52,305 --> 00:29:53,525
Speaker 6:  and apple's at risk there.

477
00:29:53,715 --> 00:29:56,725
Speaker 5:  Yeah, I think there's also just this basic element of like Addie, you're

478
00:29:56,725 --> 00:29:59,685
Speaker 5:  right that there's not necessarily anything that the iPhone is

479
00:30:00,445 --> 00:30:03,285
Speaker 5:  specifically missing at this point, right? Sir has been bad for a while,

480
00:30:03,345 --> 00:30:06,925
Speaker 5:  can continue to be bad, but they also like came out and and

481
00:30:07,115 --> 00:30:10,805
Speaker 5:  they branded this thing they put, they redefined AI as a thing with their

482
00:30:10,805 --> 00:30:14,565
Speaker 5:  name built into it and so I, I do think there's just some basic

483
00:30:14,705 --> 00:30:18,445
Speaker 5:  stakes that Apple has set up for itself too and

484
00:30:18,525 --> 00:30:22,325
Speaker 5:  it's, it's really interesting watching them attempt to meet those and And

485
00:30:22,925 --> 00:30:25,605
Speaker 5:  I think by their own account, not quite deliver. Alright, we've gotta take

486
00:30:25,605 --> 00:30:28,765
Speaker 5:  a break. We'll get back talking about the age gating of the internet.

487
00:34:03,765 --> 00:34:07,725
Speaker 5:  a slightly spicy subreddit. But either way a lot of

488
00:34:07,725 --> 00:34:11,525
Speaker 5:  people in a lot of places are gonna have to prove that they're of

489
00:34:11,745 --> 00:34:15,165
Speaker 5:  age. Is that off base? Am I in the ballpark here?

490
00:34:15,565 --> 00:34:18,725
Speaker 7:  I think what we've really seen over the past couple weeks is that the first

491
00:34:18,725 --> 00:34:22,645
Speaker 7:  time someone really as a country said you guys have to

492
00:34:22,645 --> 00:34:26,205
Speaker 7:  all do Age Verification. That moment happened after like a decade

493
00:34:27,025 --> 00:34:31,005
Speaker 7:  and so there have been over the past several, several years all of

494
00:34:31,005 --> 00:34:34,685
Speaker 7:  these plans to kind of slowly creep Age Verification into things

495
00:34:35,425 --> 00:34:39,205
Speaker 7:  and The UK in particular has been trying to age gate

496
00:34:39,205 --> 00:34:43,005
Speaker 7:  porn for, I think I looked back and it's been a since about 2016

497
00:34:44,305 --> 00:34:48,285
Speaker 7:  and they passed the online safety act and then the portion of

498
00:34:48,285 --> 00:34:52,245
Speaker 7:  it that requires age gating for harmful content including adult content

499
00:34:52,425 --> 00:34:56,325
Speaker 7:  but also a sort of variety of things that get posted to social media

500
00:34:56,355 --> 00:35:00,125
Speaker 7:  that finally went into effect in late July. And now since then

501
00:35:00,125 --> 00:35:03,885
Speaker 7:  we've just seen what actually happens and it's really weird and that there

502
00:35:03,885 --> 00:35:06,885
Speaker 7:  are all these other countries that are just kind of waiting in the wings

503
00:35:07,045 --> 00:35:10,685
Speaker 7:  to do this and the US is slowly starting to roll it out

504
00:35:10,705 --> 00:35:14,365
Speaker 7:  across states. And so we're I think reaching this kind of tipping

505
00:35:14,375 --> 00:35:14,725
Speaker 7:  point

506
00:35:15,305 --> 00:35:19,165
Speaker 5:  And and The UK version almost seems like it's one of the May,

507
00:35:19,165 --> 00:35:21,605
Speaker 5:  maybe I shouldn't say it's the worst case scenario GI given that it's the

508
00:35:21,605 --> 00:35:23,645
Speaker 5:  first scenario we've seen, but it seems, it seems quite

509
00:35:25,075 --> 00:35:28,925
Speaker 5:  controlling right? The the their definition of what needs to be

510
00:35:28,945 --> 00:35:32,845
Speaker 5:  age protected is really broad. It seems to be like anywhere, any

511
00:35:32,955 --> 00:35:36,805
Speaker 5:  website where a child might possibly

512
00:35:36,965 --> 00:35:40,695
Speaker 5:  encounter something problematic, right? And so Blue Sky

513
00:35:40,695 --> 00:35:44,535
Speaker 5:  users can't access certain features unless they

514
00:35:44,535 --> 00:35:48,175
Speaker 5:  prove that they're of age. Reddit users can't

515
00:35:48,175 --> 00:35:52,055
Speaker 5:  access r slash periods or r slash stop smoking

516
00:35:52,355 --> 00:35:56,255
Speaker 5:  unless they prove that they're of age. Like this is like pretty wild stuff

517
00:35:56,675 --> 00:35:59,735
Speaker 5:  to have to show that you're 18 to access.

518
00:35:59,865 --> 00:36:03,655
Speaker 6:  Sorry is this literally you have to show a photo of your id?

519
00:36:03,965 --> 00:36:07,695
Speaker 7:  Basically there are a sort of list of harmful content

520
00:36:07,715 --> 00:36:11,455
Speaker 7:  things that you have to make sure that if you have them on your site

521
00:36:12,105 --> 00:36:15,415
Speaker 7:  there is some kind of reasonable assurance that people looking at them are

522
00:36:15,415 --> 00:36:19,215
Speaker 7:  of age. And typically what you end up doing then as

523
00:36:19,295 --> 00:36:23,055
Speaker 7:  a site is you have to either say nobody from The UK can access things

524
00:36:23,475 --> 00:36:27,335
Speaker 7:  or you have to say we don't have this on our site or you have to say we're

525
00:36:27,455 --> 00:36:31,295
Speaker 7:  blocking this unless you go through this typically third party

526
00:36:31,295 --> 00:36:34,735
Speaker 7:  service that will either have you

527
00:36:35,115 --> 00:36:38,935
Speaker 7:  upload a government issued ID or use something like a credit card

528
00:36:39,235 --> 00:36:43,095
Speaker 7:  or use something like a picture of your face that is you,

529
00:36:43,165 --> 00:36:47,095
Speaker 7:  that is run through age estimation to see if you

530
00:36:47,095 --> 00:36:50,935
Speaker 7:  look like you're of age in a like

531
00:36:52,045 --> 00:36:55,935
Speaker 7:  sort of Age Verification algorithm or there are

532
00:36:55,935 --> 00:36:59,775
Speaker 7:  these methods that kind of, they're using inferences from ways

533
00:36:59,775 --> 00:37:03,695
Speaker 7:  that you've used your account. Like if say your ex account

534
00:37:03,875 --> 00:37:07,695
Speaker 7:  is 15 years old, then they're going to guess you're

535
00:37:07,935 --> 00:37:11,375
Speaker 7:  probably 18 years old. So it's this whole weird

536
00:37:11,665 --> 00:37:15,255
Speaker 7:  cornucopia of different methods, some of which are easier to fool

537
00:37:16,485 --> 00:37:20,255
Speaker 7:  than others. And basically the upshot is it's just

538
00:37:20,255 --> 00:37:23,455
Speaker 7:  really unpredictable what people are going to post on social media. And so

539
00:37:23,455 --> 00:37:26,895
Speaker 7:  if you're a social media site, you probably at this point need to run Age

540
00:37:26,895 --> 00:37:30,455
Speaker 7:  Verification that Blue Sky has

541
00:37:30,795 --> 00:37:34,615
Speaker 7:  in The UK limited things like dms and it will auto filter

542
00:37:34,665 --> 00:37:37,735
Speaker 7:  adult content. Reddit will block a lot of sub

543
00:37:38,105 --> 00:37:41,575
Speaker 7:  subreddits that are even sort of borderline content

544
00:37:41,765 --> 00:37:45,655
Speaker 7:  potentially that that's ended up being things like at least as of the

545
00:37:45,655 --> 00:37:49,615
Speaker 7:  time we were writing about about it our period and

546
00:37:49,705 --> 00:37:53,335
Speaker 7:  there have been say violent protest footage or

547
00:37:53,385 --> 00:37:57,255
Speaker 7:  other kind of material that then gets considered adult

548
00:37:57,255 --> 00:38:01,135
Speaker 7:  content on services like X. So yeah, if you're in The UK you

549
00:38:01,135 --> 00:38:04,775
Speaker 7:  basically, you have to give up your ID or you have to take a picture of your

550
00:38:04,775 --> 00:38:08,695
Speaker 7:  face or video of your face and show it and run it through these services.

551
00:38:09,115 --> 00:38:12,455
Speaker 6:  The UK is really, it's constantly impressive how

552
00:38:12,895 --> 00:38:16,815
Speaker 6:  shittier they make using the internet become almost on a yearly basis

553
00:38:17,525 --> 00:38:21,335
Speaker 6:  like props to The UK where the internet will continue to suck

554
00:38:21,485 --> 00:38:22,175
Speaker 6:  more and more.

555
00:38:22,585 --> 00:38:25,855
Speaker 5:  There are all these reports of these small websites that are just like shut,

556
00:38:25,855 --> 00:38:29,545
Speaker 5:  you know, shutting down in The UK because they're, they're like, we we, we

557
00:38:29,545 --> 00:38:32,545
Speaker 5:  don't know how to deal with this, we don't have the resources for this the

558
00:38:32,545 --> 00:38:36,165
Speaker 5:  way that they have implemented like it is so broad and the way the

559
00:38:36,165 --> 00:38:39,885
Speaker 5:  options for identifying yourself again are so there's so many

560
00:38:39,885 --> 00:38:43,805
Speaker 5:  options out there and they don't necessarily have close restrictions

561
00:38:43,805 --> 00:38:47,645
Speaker 5:  around them and it feels like the privacy thing is the big question

562
00:38:47,645 --> 00:38:50,885
Speaker 5:  to me, right? I the idea of, okay we should

563
00:38:51,675 --> 00:38:55,645
Speaker 5:  stop children from accessing porn on the internet. I don't think

564
00:38:55,645 --> 00:38:59,525
Speaker 5:  that is necessarily an unreasonable contention. I do think

565
00:38:59,795 --> 00:39:03,285
Speaker 5:  that every single adult who wants to access,

566
00:39:03,965 --> 00:39:07,885
Speaker 5:  I don't know, information about a rated M video game needs to hand

567
00:39:07,915 --> 00:39:11,205
Speaker 5:  over their ID or a face scan to some service

568
00:39:11,715 --> 00:39:15,365
Speaker 5:  that can possibly then track them around the internet and tie that back to

569
00:39:15,365 --> 00:39:17,725
Speaker 5:  their identity. That's very concerning. I

570
00:39:17,725 --> 00:39:20,525
Speaker 7:  Think the problem is that there's not really an easy way often to,

571
00:39:21,825 --> 00:39:25,805
Speaker 7:  to draw a line between those two things. Like on in

572
00:39:25,805 --> 00:39:29,325
Speaker 7:  the US so far it has mostly stayed on just yet. It's PornHub, it's like

573
00:39:29,545 --> 00:39:33,325
Speaker 7:  xvideo, it's a few different sites. But the problem

574
00:39:33,465 --> 00:39:37,205
Speaker 7:  is that that means either you have to mean, you either have to say all right,

575
00:39:37,205 --> 00:39:40,885
Speaker 7:  that means there's then no sex on social media at all. Like

576
00:39:40,885 --> 00:39:44,525
Speaker 7:  you're not allowed to have in some cases artistic nudes. You're not allowed

577
00:39:44,525 --> 00:39:47,845
Speaker 7:  to have say just sex workers operating on this site as humans.

578
00:39:49,065 --> 00:39:52,685
Speaker 7:  You have to either then say all of these other services are getting

579
00:39:52,795 --> 00:39:56,365
Speaker 7:  sanitized or you have to say look, we're going to

580
00:39:56,615 --> 00:40:00,565
Speaker 7:  allow for some level of surveillance of all of these

581
00:40:00,565 --> 00:40:04,165
Speaker 7:  sites. And The UK also, yeah, it's, it's that it's not just pornography,

582
00:40:04,165 --> 00:40:08,085
Speaker 7:  it is also it's things like eating disorders, it's things like

583
00:40:08,115 --> 00:40:11,925
Speaker 7:  suicide which are all things that it's really not good for kids to

584
00:40:11,925 --> 00:40:15,805
Speaker 7:  encounter but just the nature of social media makes it really,

585
00:40:15,805 --> 00:40:19,645
Speaker 7:  really difficult to automate those things away or

586
00:40:19,905 --> 00:40:23,565
Speaker 7:  to say that a child is never going to encounter them in a way that could

587
00:40:23,565 --> 00:40:24,765
Speaker 7:  be problematic. Right?

588
00:40:24,765 --> 00:40:28,725
Speaker 5:  Right now like Wikipedia is complaining that they might have to, you

589
00:40:28,725 --> 00:40:31,645
Speaker 5:  know, make some big changes as a result of this because they can't quite,

590
00:40:32,305 --> 00:40:35,405
Speaker 5:  you know, guarantee that there isn't any inappropriate information because

591
00:40:35,435 --> 00:40:38,245
Speaker 5:  Wikipedia is a user generated content platform

592
00:40:38,545 --> 00:40:42,525
Speaker 7:  And outside Wikipedia there's also just, there are the problems of profit

593
00:40:42,675 --> 00:40:46,525
Speaker 7:  motives like that there are these services and these services cost

594
00:40:46,525 --> 00:40:50,445
Speaker 7:  money, these verification services and these services also, this is

595
00:40:50,445 --> 00:40:54,165
Speaker 7:  all a commercial operation and there is a lot of incentive to collect as

596
00:40:54,165 --> 00:40:58,005
Speaker 7:  much data as you can and to either cut corners with that

597
00:40:58,005 --> 00:41:01,925
Speaker 7:  data because you are trying to make money off

598
00:41:01,925 --> 00:41:05,525
Speaker 7:  of this thing you're using or to just actually use that

599
00:41:05,525 --> 00:41:08,245
Speaker 7:  information to target advertisements or to

600
00:41:09,235 --> 00:41:13,205
Speaker 7:  otherwise like just run a commercial service. And obvi like

601
00:41:13,565 --> 00:41:16,845
Speaker 7:  companies will absolutely promise privacy but just we've seen

602
00:41:17,515 --> 00:41:21,165
Speaker 7:  real security breaches. We've seen like Twitter for instance

603
00:41:21,345 --> 00:41:24,365
Speaker 7:  was under consent decreed part because it would use

604
00:41:24,675 --> 00:41:27,725
Speaker 7:  information that you provided as like security phone numbers

605
00:41:28,585 --> 00:41:32,445
Speaker 7:  for commercial purposes. Like we just understand how all these

606
00:41:32,445 --> 00:41:35,805
Speaker 7:  systems work and how all these incentives work and they're just throwing

607
00:41:35,805 --> 00:41:39,605
Speaker 7:  something into a system that is in a lot of ways

608
00:41:39,665 --> 00:41:42,965
Speaker 7:  filled with perverse incentives and demanding that it work. Well

609
00:41:43,425 --> 00:41:46,365
Speaker 6:  You're saying Europe doesn't understand tech regulation? Shocker,

610
00:41:46,365 --> 00:41:49,845
Speaker 7:  This is the funny thing is I actually think the EU is doing better even though

611
00:41:50,185 --> 00:41:53,845
Speaker 7:  the, so the EU is running a pilot program that its deal

612
00:41:54,025 --> 00:41:57,925
Speaker 7:  is look, we also, we have Age Verification And I personally, I

613
00:41:57,925 --> 00:42:01,365
Speaker 7:  believe that Age Verification is in a lot of ways fundamentally flawed, but

614
00:42:01,365 --> 00:42:04,965
Speaker 7:  they're at least saying we also have this technical system that we're trying

615
00:42:04,965 --> 00:42:08,645
Speaker 7:  to build that's just gonna be a best pr like a system anybody could

616
00:42:08,645 --> 00:42:12,565
Speaker 7:  deploy that's going to ideally mean that somebody uploads

617
00:42:12,565 --> 00:42:16,405
Speaker 7:  their passport once to a service and then that service is

618
00:42:16,405 --> 00:42:20,245
Speaker 7:  held to a sort of like it runs on best practices and

619
00:42:20,985 --> 00:42:24,805
Speaker 7:  all that service does then is tell a site whether or not

620
00:42:24,805 --> 00:42:28,405
Speaker 7:  you're 18. And that this makes it In theory

621
00:42:28,565 --> 00:42:32,445
Speaker 7:  easier for the, for the websites. They just know that there's

622
00:42:32,445 --> 00:42:35,285
Speaker 7:  something that they can use that they don't have to go and find this third

623
00:42:35,285 --> 00:42:38,925
Speaker 7:  party verifier and that they are not

624
00:42:40,335 --> 00:42:44,185
Speaker 7:  relying on just this private sector development of

625
00:42:44,185 --> 00:42:48,105
Speaker 7:  all of these weird different options. So I think the EU in some ways is

626
00:42:48,105 --> 00:42:49,785
Speaker 7:  doing a better job than The UK potentially.

627
00:42:50,135 --> 00:42:53,905
Speaker 5:  Well so do we expect this to spread more broadly in the EU? Because right

628
00:42:53,905 --> 00:42:56,745
Speaker 5:  now this is like five states are testing an app

629
00:42:57,825 --> 00:43:00,925
Speaker 5:  And I don't, I don't know where where it goes next. Is it, is this a legal

630
00:43:00,925 --> 00:43:04,085
Speaker 5:  requirement or 'cause they've been pushing this right? I think this seems

631
00:43:04,085 --> 00:43:07,245
Speaker 5:  to be kind of like everywhere in the world

632
00:43:08,235 --> 00:43:11,845
Speaker 5:  they, they there's an increased momentum toward age gating things.

633
00:43:12,075 --> 00:43:16,005
Speaker 7:  Yeah. Australia has, I believe also determined that they're going to be age

634
00:43:16,005 --> 00:43:18,805
Speaker 7:  gating search engines in the future for

635
00:43:19,985 --> 00:43:23,765
Speaker 7:  if you want basically safe search off. I think it's a little bit hard to

636
00:43:23,765 --> 00:43:27,445
Speaker 7:  tell. I think on one hand it seems like all this stuff is becoming

637
00:43:27,475 --> 00:43:30,485
Speaker 7:  kind of inevitable. On the other hand The UK

638
00:43:31,775 --> 00:43:35,435
Speaker 7:  has been, it seems like kind of a mess and none of this is I think totally

639
00:43:35,435 --> 00:43:38,875
Speaker 7:  inevitable. I think it is possible that now we are seeing this is what happens

640
00:43:38,875 --> 00:43:42,435
Speaker 7:  when this rolls out and it turns out it's really messy and there are a lot

641
00:43:42,435 --> 00:43:46,155
Speaker 7:  of problems and we are seeing really some of the first real life test

642
00:43:46,155 --> 00:43:49,995
Speaker 7:  cases of it and it's possible that that will make it possible

643
00:43:50,055 --> 00:43:53,275
Speaker 7:  to shape how future rollouts work.

644
00:43:53,495 --> 00:43:57,275
Speaker 5:  So I wanna touch on the US too, and this is about a month old at this point,

645
00:43:57,415 --> 00:44:00,595
Speaker 5:  but you covered the Supreme Court's ruling that

646
00:44:01,615 --> 00:44:05,195
Speaker 5:  age gating porn websites is, is allowable into the first amendment

647
00:44:06,015 --> 00:44:09,835
Speaker 5:  And I think at this point, what is it like a third of all US states

648
00:44:10,265 --> 00:44:14,195
Speaker 5:  have laws that require Age Verification for porn.

649
00:44:14,695 --> 00:44:17,995
Speaker 5:  So it feels like this is going to expand in the US it's not clear to me if

650
00:44:17,995 --> 00:44:21,875
Speaker 5:  it's going to expand beyond adult content in the same way I, I'm

651
00:44:21,875 --> 00:44:24,035
Speaker 5:  curious what the ruling even allows.

652
00:44:24,375 --> 00:44:28,355
Speaker 7:  The ruling really specifically deals with content that is obscene to

653
00:44:28,375 --> 00:44:32,125
Speaker 7:  minors, which is a very specifically

654
00:44:32,155 --> 00:44:36,045
Speaker 7:  this is pornography. I think that there

655
00:44:36,045 --> 00:44:39,805
Speaker 7:  are probably ways you can kind of find wiggle room around that to

656
00:44:39,825 --> 00:44:43,765
Speaker 7:  try to impose regulations more broadly. But it's a a

657
00:44:43,765 --> 00:44:47,045
Speaker 7:  pretty different court case. I have talked to people for the last

658
00:44:47,785 --> 00:44:51,445
Speaker 7:  months or year about, well what happens if Age

659
00:44:51,445 --> 00:44:54,405
Speaker 7:  Verification for porn is allowed? Does that mean you can just age gate social

660
00:44:54,405 --> 00:44:58,325
Speaker 7:  media? And the consensus has been, it's possible that will happen,

661
00:44:58,325 --> 00:45:01,925
Speaker 7:  but it's a pretty different court case and that you have to just do a completely

662
00:45:01,925 --> 00:45:05,685
Speaker 7:  different weighing of what are the privacy risks versus the

663
00:45:05,685 --> 00:45:09,245
Speaker 7:  harms that you're trying to prevent. But we've definitely seen

664
00:45:09,665 --> 00:45:13,525
Speaker 7:  states try to pass regulations for social media

665
00:45:13,625 --> 00:45:16,565
Speaker 7:  and so it seems inevitable that it's going to get up to the Supreme Court.

666
00:45:16,995 --> 00:45:20,005
Speaker 5:  Yeah because it seems, right. 'cause you could just say there's porn on Blue

667
00:45:20,025 --> 00:45:23,485
Speaker 5:  Sky, right? You could easily just make that argument that okay, now this

668
00:45:23,545 --> 00:45:26,805
Speaker 5:  entire platform needs to be age G and right

669
00:45:27,865 --> 00:45:31,805
Speaker 5:  Reddit, right? You, you could have to age educate the entire

670
00:45:32,045 --> 00:45:35,845
Speaker 5:  platform. I guess maybe this, maybe this starts to just slice off

671
00:45:35,845 --> 00:45:37,965
Speaker 5:  chunks of these platforms somehow. I

672
00:45:37,965 --> 00:45:41,485
Speaker 7:  Mean so far this actual laws have all basically set a threshold that's like

673
00:45:41,625 --> 00:45:44,445
Speaker 7:  is one third of the content on the site adult.

674
00:45:45,905 --> 00:45:49,245
Speaker 7:  And I think that it's basically been understood that social media

675
00:45:49,645 --> 00:45:53,565
Speaker 7:  platforms don't fall under that. But I think the thing that's gonna mean

676
00:45:53,565 --> 00:45:57,365
Speaker 7:  maybe the next issue is there have been laws passed that are basically

677
00:45:57,365 --> 00:45:59,365
Speaker 7:  requiring Age Verification on app stores

678
00:46:01,805 --> 00:46:05,775
Speaker 7:  that those are going to, I think probably end up going

679
00:46:05,795 --> 00:46:09,575
Speaker 7:  to the Supreme Court. It's I believe Utah and

680
00:46:09,575 --> 00:46:09,855
Speaker 7:  Texas

681
00:46:11,725 --> 00:46:15,695
Speaker 7:  that are saying that if you operate an app store, like

682
00:46:15,775 --> 00:46:19,725
Speaker 7:  a mobile app store specifically, you have to have a system

683
00:46:19,745 --> 00:46:23,205
Speaker 7:  by which you can guarantee that people are over 18.

684
00:46:24,625 --> 00:46:28,485
Speaker 7:  And that basically the legal issue there is

685
00:46:28,485 --> 00:46:32,385
Speaker 7:  going to turn out to be is this unduly burdening speech

686
00:46:32,525 --> 00:46:36,305
Speaker 7:  in a way that is unique from the concerns of like the,

687
00:46:37,045 --> 00:46:40,745
Speaker 7:  it is fair to restrict access to obscene

688
00:46:40,745 --> 00:46:44,585
Speaker 7:  content. I mean I think the Supreme Court right now kind of operates just

689
00:46:44,585 --> 00:46:47,985
Speaker 7:  on really weird vibes and so it seems plausible that they're just going to

690
00:46:47,985 --> 00:46:48,865
Speaker 7:  say yes, that's fine.

691
00:46:51,035 --> 00:46:54,295
Speaker 7:  And so at that point I think we're gonna have to start looking

692
00:46:55,555 --> 00:46:58,475
Speaker 7:  a lot more closely at how all of this is just going to end up getting implemented.

693
00:46:58,735 --> 00:46:58,955
Speaker 7:  The

694
00:46:58,955 --> 00:47:02,195
Speaker 5:  App store thing is really interesting because I think a, a lot of the tech

695
00:47:02,435 --> 00:47:06,235
Speaker 5:  companies who do not operate app stores such as Meta are really pushing for

696
00:47:06,235 --> 00:47:10,075
Speaker 5:  this because it seems like a way for them to pass the buck a little bit,

697
00:47:10,125 --> 00:47:13,275
Speaker 5:  right? Alex, I think you've covered this from meta where

698
00:47:14,385 --> 00:47:18,355
Speaker 5:  they've, they've been pushing for Apple to implement these requirements

699
00:47:18,355 --> 00:47:21,075
Speaker 5:  and they've been kind of trying to like guilt them into it a little bit.

700
00:47:21,785 --> 00:47:25,555
Speaker 6:  Yeah, it, their comparison, which I think is an

701
00:47:25,655 --> 00:47:29,325
Speaker 6:  apt one, is like, you know, you age gate, someone's ability to drive a car,

702
00:47:29,505 --> 00:47:33,485
Speaker 6:  you don't age gate. Like you don't force the roads to age

703
00:47:33,485 --> 00:47:37,245
Speaker 6:  gate everyone as they drive down the road, you age gate the ability to

704
00:47:37,245 --> 00:47:41,005
Speaker 6:  drive the car and what is the car in this analogy, it's the phone.

705
00:47:41,705 --> 00:47:45,685
Speaker 6:  So if you really wanted to solve this, which Apple in particular

706
00:47:45,745 --> 00:47:49,725
Speaker 6:  is aggressively lobbying against in the United States is you would force

707
00:47:50,345 --> 00:47:54,165
Speaker 6:  the phone platforms to age gate at the device layer.

708
00:47:54,445 --> 00:47:57,965
Speaker 7:  I gotta say as a metaphor, this makes no sense to me. It's not like cars

709
00:47:58,025 --> 00:48:00,245
Speaker 7:  are required to tell, tell whether you're 16.

710
00:48:00,765 --> 00:48:03,845
Speaker 6:  I I did get barely any sleep Addie writing about open ai. So maybe it doesn't

711
00:48:03,845 --> 00:48:06,125
Speaker 6:  make sense, but I think conceptually

712
00:48:07,715 --> 00:48:11,485
Speaker 6:  what makes sense to me is gating at the

713
00:48:11,485 --> 00:48:14,285
Speaker 6:  root, which is the device and not

714
00:48:15,105 --> 00:48:18,965
Speaker 6:  gating all over the web and chopping up the web. It

715
00:48:18,965 --> 00:48:21,845
Speaker 6:  just, I don't think you should g it all to be frank. Like I think this should,

716
00:48:22,025 --> 00:48:25,485
Speaker 6:  you know, what would solve this whole discussion is you make parents liable

717
00:48:25,485 --> 00:48:29,445
Speaker 6:  for what their kids do. And if you, if we've decided as a society

718
00:48:29,445 --> 00:48:32,885
Speaker 6:  that it's evil or bad or harmful for kids to look at porn,

719
00:48:33,815 --> 00:48:36,645
Speaker 6:  guess who should be liable for that? Should be their fucking parents. It

720
00:48:36,645 --> 00:48:39,925
Speaker 6:  shouldn't be these platforms. That's, that's my view of it all.

721
00:48:40,425 --> 00:48:40,645
Speaker 6:  But

722
00:48:42,195 --> 00:48:46,165
Speaker 6:  yeah, I, if you're going to gate, why not gate at the device? I think that's

723
00:48:46,165 --> 00:48:46,685
Speaker 6:  a reasonable,

724
00:48:47,445 --> 00:48:50,965
Speaker 7:  I think that there are a lot of good an I think that I agree with you that

725
00:48:50,965 --> 00:48:54,325
Speaker 7:  there's a really good reason pragmatically to say that look,

726
00:48:55,875 --> 00:48:59,735
Speaker 7:  you can probably create a better system using devices

727
00:48:59,765 --> 00:49:03,335
Speaker 7:  than you can like regulating or you can like

728
00:49:03,765 --> 00:49:07,615
Speaker 7:  verifying ages on every service at the same time. I think

729
00:49:07,715 --> 00:49:11,375
Speaker 7:  in some ways, like my biggest problem with this is that it just means that

730
00:49:11,375 --> 00:49:14,575
Speaker 7:  basically everything has to route through an app or an app store at this

731
00:49:14,575 --> 00:49:18,535
Speaker 7:  point that it means that just trying to make a thing that works on a

732
00:49:18,565 --> 00:49:22,535
Speaker 7:  desktop for instance, like it means that

733
00:49:22,775 --> 00:49:26,655
Speaker 7:  ultimately you're just saying desktop desktops and websites don't

734
00:49:26,655 --> 00:49:30,535
Speaker 7:  matter. It's really strange. Like I think it basically

735
00:49:30,535 --> 00:49:33,695
Speaker 7:  just assumes, okay, look most people are using phones, most kids are using

736
00:49:33,695 --> 00:49:37,655
Speaker 7:  phones, that's fine. And so we're just getting like the broad, we're getting

737
00:49:37,685 --> 00:49:41,255
Speaker 7:  most people and that's enough. But if you push this far enough, what you're

738
00:49:41,255 --> 00:49:44,295
Speaker 7:  saying basically is just any content on the internet has to at some point

739
00:49:44,315 --> 00:49:47,175
Speaker 7:  be verified through like Tim Cook or Sundar Phai.

740
00:49:47,355 --> 00:49:51,215
Speaker 6:  Oh yeah. To be clear, I think this is all a horrible idea. Like gating at

741
00:49:51,215 --> 00:49:54,575
Speaker 6:  all is a horrible idea. But if you were going to do it, it would be the most

742
00:49:54,575 --> 00:49:57,815
Speaker 6:  consistent to do it at the device layer. It shouldn't happen at all. But

743
00:49:58,045 --> 00:49:59,655
Speaker 6:  that would be the most consistent.

744
00:49:59,995 --> 00:50:03,975
Speaker 7:  And the US also has the just very particular issue of

745
00:50:04,635 --> 00:50:07,735
Speaker 7:  the major party platform is banning pornography entirely.

746
00:50:08,685 --> 00:50:12,655
Speaker 7:  Like The UK and the EU are not both engaged in

747
00:50:12,695 --> 00:50:16,495
Speaker 7:  a just really massive surveillance state building

748
00:50:16,765 --> 00:50:20,575
Speaker 7:  operation right now, or an attempt to make like

749
00:50:20,645 --> 00:50:24,295
Speaker 7:  drag shows illegal. So the US has kind of its own whole set of

750
00:50:24,295 --> 00:50:27,845
Speaker 7:  problems where like even things like I think the EU building

751
00:50:28,365 --> 00:50:31,685
Speaker 7:  a, trying to build a sort of government solution for verification

752
00:50:32,345 --> 00:50:36,245
Speaker 7:  is really good if you have a good faith belief in the

753
00:50:36,245 --> 00:50:40,005
Speaker 7:  government and that the government is going to protect your privacy, I think

754
00:50:40,005 --> 00:50:43,365
Speaker 7:  that even something like that just doesn't work well in the US. Google

755
00:50:43,465 --> 00:50:46,605
Speaker 5:  Did this weird thing this week where they announced that they were gonna

756
00:50:46,605 --> 00:50:50,525
Speaker 5:  start like just trying to like size people up to see if they were kids

757
00:50:50,545 --> 00:50:54,045
Speaker 5:  or not and if, and if if they, if they just like think that you're

758
00:50:54,385 --> 00:50:57,725
Speaker 5:  not 18, they're gonna put like certain restrictions on your account. And

759
00:50:57,725 --> 00:51:01,285
Speaker 5:  this is, this seems to be just entirely voluntary And I, I I guess I'm wondering

760
00:51:01,285 --> 00:51:05,005
Speaker 5:  at this point if, if this is like they're just trying to, you know, look

761
00:51:05,075 --> 00:51:08,965
Speaker 5:  responsible so that some of these maybe worst case scenarios don't come

762
00:51:08,965 --> 00:51:09,205
Speaker 5:  to pass.

763
00:51:09,205 --> 00:51:11,805
Speaker 6:  Yeah, they're trying to get ahead of regulation. This is what they've done

764
00:51:11,875 --> 00:51:15,685
Speaker 6:  time and time again with other things, other kind of

765
00:51:15,965 --> 00:51:19,845
Speaker 6:  regulatory movements. The biggest companies will do it on their own

766
00:51:20,905 --> 00:51:24,725
Speaker 6:  and it will be impossible or weighed more difficult for

767
00:51:24,845 --> 00:51:28,405
Speaker 6:  startups to, to do it. That's how this all goes with regulation.

768
00:51:28,595 --> 00:51:31,925
Speaker 5:  Yeah. I'm very concerned about where this is going. The fact that this has

769
00:51:32,965 --> 00:51:36,685
Speaker 5:  actually gone into practice and that we are seeing the fallout

770
00:51:36,945 --> 00:51:40,365
Speaker 5:  be so broad and widespread and that we're seeing

771
00:51:41,295 --> 00:51:45,045
Speaker 5:  major mainstream websites getting blocked is, is kind of horrifying.

772
00:51:45,865 --> 00:51:49,445
Speaker 5:  And I think the fact that we're seeing an escalation in the US where more

773
00:51:49,445 --> 00:51:53,365
Speaker 5:  and more states are passing laws and added to your point, there's

774
00:51:53,365 --> 00:51:57,125
Speaker 5:  an overall movement to crack down on anything that

775
00:51:57,705 --> 00:52:01,445
Speaker 5:  one of the major political parties thinks is a little too

776
00:52:01,725 --> 00:52:05,445
Speaker 5:  edgy. It can lead to some very bad places, but it, it's, I think it's maybe

777
00:52:05,765 --> 00:52:09,005
Speaker 5:  encouraging to hear that you think that this is not necessarily a done deal

778
00:52:09,515 --> 00:52:13,475
Speaker 5:  that The UK is maybe just take it taking a, a a

779
00:52:13,715 --> 00:52:17,675
Speaker 5:  a first swing at this and the US may not

780
00:52:17,775 --> 00:52:18,875
Speaker 5:  be able to get that far.

781
00:52:19,235 --> 00:52:22,235
Speaker 7:  I mean, the US is way more chaotic. The UK there has been

782
00:52:23,395 --> 00:52:27,315
Speaker 7:  a request for a petition for, I believe parliament has to respond

783
00:52:27,335 --> 00:52:31,235
Speaker 7:  to, to like reevaluating this. That does not necessarily mean

784
00:52:31,475 --> 00:52:35,355
Speaker 7:  anything, but it does mean that there is public pushback to it.

785
00:52:36,195 --> 00:52:40,155
Speaker 7:  I think that unfortunately if this is just like anything else

786
00:52:40,155 --> 00:52:43,715
Speaker 7:  that's happened to pri with privacy on the internet, it means that maybe

787
00:52:43,745 --> 00:52:47,195
Speaker 7:  some things are going to loosen up some, But that this is still probably

788
00:52:47,195 --> 00:52:51,035
Speaker 7:  here to stay. I think that I haven't seen really a ratchet away

789
00:52:51,035 --> 00:52:54,435
Speaker 7:  from privacy invasions on the internet since I've been working here.

790
00:52:56,595 --> 00:52:58,355
Speaker 7:  I, that sounds really dor, I'm sorry,

791
00:53:00,415 --> 00:53:03,715
Speaker 7:  but I am hoping that around the edges people can shape things and that if

792
00:53:03,715 --> 00:53:06,955
Speaker 7:  this does screw up badly enough, maybe I will be proven wrong.

793
00:53:07,245 --> 00:53:10,315
Speaker 5:  There we go. UK do a worse job.

794
00:53:11,255 --> 00:53:12,995
Speaker 5:  All right, we gotta take a break when we get back

795
00:53:14,585 --> 00:53:17,155
Speaker 5:  some sort of round, maybe there's lightning. We'll see

796
00:55:47,255 --> 00:55:48,725
Speaker 5:  we've been doing some work, some,

797
00:56:41,185 --> 00:56:43,915
Speaker 7:  Alright, so my first story is that

798
00:56:44,835 --> 00:56:48,475
Speaker 7:  RFK has pulled $500 million in funding for mRNA vaccine

799
00:56:48,915 --> 00:56:52,795
Speaker 7:  contract contracts. The announced it, this

800
00:56:52,795 --> 00:56:56,595
Speaker 7:  is via NPR. The Department of Health and Human Services

801
00:56:56,625 --> 00:57:00,075
Speaker 7:  will cancel contracts and pull funding for some vaccines that are being developed

802
00:57:00,075 --> 00:57:02,435
Speaker 7:  to fight respiratory ve va viruses

803
00:57:35,005 --> 00:57:38,795
Speaker 5:  COVID shot this, this fall, right? Like they changed the rules

804
00:57:39,295 --> 00:57:42,435
Speaker 5:  and Right. I've just been getting the, the COVID shot and the flu shot at

805
00:57:42,435 --> 00:57:46,355
Speaker 5:  the same time. I don't wanna get the flu. It's not cool. And

806
00:57:46,425 --> 00:57:48,875
Speaker 5:  it's, it's not clear if that's a, I mean hopefully the flu shot is still

807
00:57:48,875 --> 00:57:52,555
Speaker 5:  available. We'll find out. The COVID one is not clear. It's gonna happen

808
00:57:52,935 --> 00:57:56,515
Speaker 5:  and yeah, I I it their vaccine strategy just seems to be

809
00:57:56,965 --> 00:57:57,555
Speaker 5:  don't do it.

810
00:57:57,745 --> 00:58:01,475
Speaker 7:  Yeah, I mean, so Lauren Leer has done a lot of really good work for us on

811
00:58:01,515 --> 00:58:04,675
Speaker 7:  RFK and vaccines and basically he just believes that

812
00:58:05,375 --> 00:58:09,155
Speaker 7:  if you die from a virus, you we're weak. That

813
00:58:09,155 --> 00:58:12,475
Speaker 7:  sounds like a, a horrible mischaracter characterization of someone. That

814
00:58:12,475 --> 00:58:15,035
Speaker 7:  sounds like a horrible, mean-spirited thing to say. That is just what he

815
00:58:15,035 --> 00:58:15,475
Speaker 7:  keeps saying,

816
00:58:15,755 --> 00:58:19,635
Speaker 5:  Pointing out the hypocrisy of the Trump administration.

817
00:58:20,195 --> 00:58:23,955
Speaker 5:  I I realize that does not get very far. I do find

818
00:58:23,985 --> 00:58:26,635
Speaker 5:  this one to be particularly striking

819
00:58:27,785 --> 00:58:29,915
Speaker 5:  because if you go back to Trump's first term,

820
00:58:31,535 --> 00:58:35,275
Speaker 5:  it, listen, you don't have to hand it to them for how they handled COVID.

821
00:58:35,945 --> 00:58:39,875
Speaker 5:  However, one of the successes was

822
00:58:39,895 --> 00:58:43,715
Speaker 5:  the mRNA vaccines, right? They got the

823
00:58:43,715 --> 00:58:47,435
Speaker 5:  COVID vaccine developed fairly rapidly. They got it distributed fairly

824
00:58:47,435 --> 00:58:51,115
Speaker 5:  rapidly. This is something that Trump in another

825
00:58:51,405 --> 00:58:55,075
Speaker 5:  world would be touting, would be taking credit for. It would be boasting,

826
00:58:55,365 --> 00:58:58,835
Speaker 5:  right? This is a, this is a, a new technology that has

827
00:58:59,265 --> 00:59:02,835
Speaker 5:  some incredible potential impacts for

828
00:59:02,835 --> 00:59:06,745
Speaker 5:  vaccinations. And after spearheading

829
00:59:06,815 --> 00:59:10,625
Speaker 5:  this push for it in his, at the very tail end of his first term, which he

830
00:59:10,625 --> 00:59:14,265
Speaker 5:  seems to totally regret, he's now hired somebody who's just wildly

831
00:59:14,265 --> 00:59:18,145
Speaker 5:  undermining that, right? We we're just mRNA all this

832
00:59:18,145 --> 00:59:22,125
Speaker 5:  potential. It, it feels like we are just setting this work back by, I mean,

833
00:59:22,125 --> 00:59:25,565
Speaker 5:  at a minimum three years and potentially vastly longer.

834
00:59:25,835 --> 00:59:29,805
Speaker 7:  Yeah, it was one of absolutely the best things he did around COVID. It was

835
00:59:30,225 --> 00:59:33,965
Speaker 7:  an incredible technological development and it's just,

836
00:59:34,675 --> 00:59:38,005
Speaker 7:  it's also such incredibly just, it's such a waste.

837
00:59:39,425 --> 00:59:43,405
Speaker 7:  The, again, $500 million is nothing like, the only reason

838
00:59:43,405 --> 00:59:47,365
Speaker 7:  to do this is just that you hate vaccines. Even if you believe there's

839
00:59:47,365 --> 00:59:51,125
Speaker 7:  a 99% chance this, none of this is worthwhile, which I

840
00:59:51,125 --> 00:59:54,805
Speaker 7:  believe is really pretty clearly not correct. That's just

841
00:59:54,805 --> 00:59:57,325
Speaker 7:  barely any money for the government.

842
00:59:57,745 --> 01:00:01,485
Speaker 5:  It really is. And the other thing that I've seen Kennedy complain about is,

843
01:00:02,225 --> 01:00:06,005
Speaker 5:  and this is not true, but among his complaints are that

844
01:00:06,105 --> 01:00:08,685
Speaker 5:  we need to do more testing on the vaccines. We need to see if they're safe.

845
01:00:08,905 --> 01:00:12,885
Speaker 5:  And it's like, okay, we know what this money is used for,

846
01:00:13,655 --> 01:00:17,205
Speaker 5:  right? Vaccine testing is part of this. We make sure the

847
01:00:17,205 --> 01:00:20,885
Speaker 5:  vaccines work. They're very thoroughly tested. And so

848
01:00:20,995 --> 01:00:22,725
Speaker 5:  this, this really does just come from this

849
01:00:24,315 --> 01:00:28,245
Speaker 5:  bewildering place of not wanting to help

850
01:00:28,265 --> 01:00:31,725
Speaker 5:  people not get sick, which is, is upsetting

851
01:00:32,325 --> 01:00:32,965
Speaker 5:  to say the least,

852
01:00:33,135 --> 01:00:36,965
Speaker 7:  Right? It is. It's, I mean, everything here is just in absolute

853
01:00:36,965 --> 01:00:40,285
Speaker 7:  transparent, bad faith and that it's that he

854
01:00:40,405 --> 01:00:44,005
Speaker 7:  believes that if you get sick, like

855
01:00:44,865 --> 01:00:48,085
Speaker 7:  the thing that matters isn't making you well, it's making sure that your

856
01:00:48,085 --> 01:00:50,085
Speaker 7:  body is in some way intrinsically strong.

857
01:00:51,675 --> 01:00:55,245
Speaker 7:  Because if someone whose body is weak, is

858
01:00:55,345 --> 01:00:58,925
Speaker 7:  healed or doesn't get sick, then that's actually just, I mean

859
01:00:59,155 --> 01:01:02,605
Speaker 7:  it's just eugenics. Like it's, it's really

860
01:01:02,885 --> 01:01:06,565
Speaker 7:  straightforwardly just eugenics and it's really a

861
01:01:06,565 --> 01:01:07,005
Speaker 7:  disgrace.

862
01:01:07,475 --> 01:01:11,165
Speaker 5:  It's very, very bad. And I think just startling, startling to see

863
01:01:12,835 --> 01:01:16,205
Speaker 5:  what a change of tone the Trump administration has had on

864
01:01:16,595 --> 01:01:20,445
Speaker 5:  mRNA from the end of their first term to this new term.

865
01:01:21,075 --> 01:01:22,965
Speaker 5:  Okay. Alex, what have you got for us?

866
01:01:23,425 --> 01:01:27,245
Speaker 6:  So while we've been talking, I've also been looking at the reaction to

867
01:01:27,275 --> 01:01:30,125
Speaker 6:  open AIS g TP five

868
01:01:31,125 --> 01:01:34,365
Speaker 6:  stream and it looks like they have been vibe graphing

869
01:01:36,305 --> 01:01:39,765
Speaker 6:  and doing some really insane chart crime.

870
01:01:41,045 --> 01:01:45,005
Speaker 6:  I want to call out two specific things that people have noticed. The first

871
01:01:45,025 --> 01:01:48,845
Speaker 6:  one is Thiswe bench benchmark, which is

872
01:01:48,845 --> 01:01:52,645
Speaker 6:  this kind of industry standard coding benchmark for

873
01:01:53,075 --> 01:01:56,445
Speaker 6:  LLMs. Apparently GTP five actually does worse

874
01:01:56,835 --> 01:02:00,645
Speaker 6:  than O three on this, but the bar chart

875
01:02:00,675 --> 01:02:04,405
Speaker 6:  that they put in the stream makes it look like it's better.

876
01:02:04,405 --> 01:02:08,085
Speaker 6:  Like it's a taller bar even though the number is less

877
01:02:08,435 --> 01:02:12,365
Speaker 6:  than oh three, which is not great. And then perhaps there's

878
01:02:12,485 --> 01:02:16,085
Speaker 6:  apparently several of these. But then the worst one is when there,

879
01:02:16,085 --> 01:02:19,605
Speaker 6:  there's a chart about hallucinations, actually it's called

880
01:02:19,635 --> 01:02:23,405
Speaker 6:  deception across models. And the

881
01:02:23,405 --> 01:02:27,165
Speaker 6:  coding deception one G TP five scores a 50.

882
01:02:27,165 --> 01:02:31,085
Speaker 6:  They don't explain what this score means except deception rate. There's

883
01:02:31,085 --> 01:02:35,005
Speaker 6:  no, there's there's only one axis and it's just called deception rate. G

884
01:02:35,145 --> 01:02:38,645
Speaker 6:  TP five scores. A 50 and oh three scored a

885
01:02:38,645 --> 01:02:42,405
Speaker 6:  47.4. But the bar for oh three is

886
01:02:42,405 --> 01:02:46,325
Speaker 6:  like more than double the height of GTP five. You really have to

887
01:02:46,325 --> 01:02:48,805
Speaker 6:  see this. We'll have the links in the show notes with the, with the images,

888
01:02:49,225 --> 01:02:52,925
Speaker 6:  but it basically makes it look like if you just glance at this bar chart

889
01:02:52,925 --> 01:02:56,325
Speaker 6:  that G TP five is materially

890
01:02:56,785 --> 01:03:00,565
Speaker 6:  better at not hallucinating on cogen, when in

891
01:03:00,565 --> 01:03:04,125
Speaker 6:  reality it's worse. And this is literally a bar chart called

892
01:03:04,195 --> 01:03:07,975
Speaker 6:  deception across models. So these are incredible.

893
01:03:09,765 --> 01:03:11,135
Speaker 6:  Yeah, not not good.

894
01:03:13,235 --> 01:03:15,455
Speaker 6:  I'm gonna call this these vibe graphing. These are

895
01:03:15,765 --> 01:03:19,575
Speaker 5:  Vibe graphing is good. Vibe graphing is an a, a really good

896
01:03:19,875 --> 01:03:23,615
Speaker 5:  excuse for, Hey, why did, why did you turn in information that's completely

897
01:03:23,615 --> 01:03:27,055
Speaker 5:  wrong while I was being efficient? I vibe graphed it. I mean listen,

898
01:03:27,305 --> 01:03:31,175
Speaker 5:  these are graphs. They just don't make any sense. These are

899
01:03:32,125 --> 01:03:36,055
Speaker 5:  bewildering. There's no consistency. The the

900
01:03:36,315 --> 01:03:40,215
Speaker 5:  the highest number I is not in the highest

901
01:03:40,445 --> 01:03:44,055
Speaker 5:  spot. I don't understand what's happening. Things are just randomly

902
01:03:44,205 --> 01:03:47,775
Speaker 5:  colored. This is, this is but

903
01:03:48,565 --> 01:03:52,495
Speaker 5:  very confusing and maybe suggest why this presentation

904
01:03:52,495 --> 01:03:56,335
Speaker 5:  was a little confusing overall. I'm wondering if, if these are like, if

905
01:03:56,335 --> 01:03:59,495
Speaker 5:  they, are they trying to hide it or they just, do they just not know how

906
01:03:59,495 --> 01:04:02,015
Speaker 5:  to make a graph? It could be vibe graphed.

907
01:04:02,355 --> 01:04:06,045
Speaker 6:  Either answer is really bad to that question.

908
01:04:06,395 --> 01:04:08,085
Speaker 6:  Like either way it's really bad. So

909
01:04:09,875 --> 01:04:10,165
Speaker 6:  yeah.

910
01:04:10,165 --> 01:04:13,645
Speaker 7:  Yeah. I'm staring at two boxes of the exact same size and one says

911
01:04:13,645 --> 01:04:17,565
Speaker 7:  69.1 and one says 30.8 and they're right next to each other

912
01:04:17,865 --> 01:04:20,045
Speaker 7:  and they're both smaller than the one that's 52.

913
01:04:20,945 --> 01:04:24,605
Speaker 5:  I'm honestly, this is an impressive level of inconsistent.

914
01:04:24,795 --> 01:04:28,285
Speaker 5:  Like the, the truly, the only consistent thing about this is that everything

915
01:04:28,285 --> 01:04:30,415
Speaker 5:  is wrong. It is amazing.

916
01:04:30,935 --> 01:04:34,535
Speaker 6:  I just love that it's really wrong on a chart about deception.

917
01:04:34,935 --> 01:04:38,575
Speaker 5:  I do have an important question here, which is what's the right deception

918
01:04:38,645 --> 01:04:40,775
Speaker 5:  rate? What number are we feeling is good here?

919
01:04:40,825 --> 01:04:44,775
Speaker 6:  Preferably I would like zero from the thing that

920
01:04:45,055 --> 01:04:48,975
Speaker 6:  apparently is okay to give me medical advice from preferably zero.

921
01:04:49,265 --> 01:04:52,895
Speaker 5:  Right? They had an extended segment on using this for medical advice. Yeah.

922
01:04:52,895 --> 01:04:54,815
Speaker 5:  And this says the deception rate is 50.

923
01:04:55,155 --> 01:04:56,175
Speaker 6:  Sam Alman literally brought, I dunno what

924
01:04:56,305 --> 01:04:56,655
Speaker 5:  Means

925
01:04:56,835 --> 01:05:00,335
Speaker 6:  He literally brought an employee's spouse on stage who had been diagnosed

926
01:05:00,335 --> 01:05:04,135
Speaker 6:  with like three cancers and was like talking about how Chet

927
01:05:04,275 --> 01:05:08,175
Speaker 6:  BT had helped her navigate her diagnosis. And then, yeah. And

928
01:05:08,175 --> 01:05:11,895
Speaker 6:  then in the same presentation they show a deception rate

929
01:05:12,335 --> 01:05:15,255
Speaker 6:  graph that is intentionally very deceiving.

930
01:05:15,455 --> 01:05:19,445
Speaker 5:  I have to tell you, I've been very bullish on chat TP T and now

931
01:05:19,445 --> 01:05:23,085
Speaker 5:  that they have presented a nonsense chart that says their

932
01:05:23,085 --> 01:05:26,915
Speaker 5:  deception rate is 50 th this is, this is

933
01:05:26,915 --> 01:05:30,795
Speaker 5:  done the most to tell me that I should use this thing less. Jake,

934
01:05:30,795 --> 01:05:34,755
Speaker 6:  It's not even that the deception rate is 50, it's that the bar is less

935
01:05:34,755 --> 01:05:38,635
Speaker 6:  than half as tall as the last model, which

936
01:05:38,635 --> 01:05:42,595
Speaker 6:  was actually better at not deceiving. So

937
01:05:42,595 --> 01:05:46,155
Speaker 6:  it's made to look on the bar chart, like it's way better

938
01:05:46,305 --> 01:05:47,555
Speaker 6:  when it's actually worse.

939
01:05:49,015 --> 01:05:50,235
Speaker 5:  Chachi BT five.

940
01:05:52,075 --> 01:05:55,835
Speaker 5:  Honestly you know what though? This speaks to how good it is at

941
01:05:55,835 --> 01:05:58,835
Speaker 5:  deception. We were fooled. We didn't notice.

942
01:05:59,905 --> 01:06:01,075
Speaker 6:  Yeah, I

943
01:06:01,075 --> 01:06:04,515
Speaker 5:  Don't remember. They're gonna listen if they keep this up by, by the next

944
01:06:04,525 --> 01:06:08,235
Speaker 5:  model, these charts, we won't even notice how how off these

945
01:06:08,235 --> 01:06:11,775
Speaker 5:  charts are. The higher the deception is actually the lower the deception

946
01:06:11,825 --> 01:06:15,655
Speaker 5:  score goes because you don't realize that it's fooling you. Vibe charting.

947
01:06:15,655 --> 01:06:19,095
Speaker 5:  This is, yeah, 50% is exactly the middle where you're like, you're like,

948
01:06:19,115 --> 01:06:21,555
Speaker 5:  you catch it but you gotta squint. Okay,

949
01:06:23,585 --> 01:06:24,275
Speaker 5:  Next story.

950
01:06:25,895 --> 01:06:29,795
Speaker 5:  Tariff crisis across the globe, but

951
01:06:30,035 --> 01:06:32,795
Speaker 5:  specifically in the United States, we've been talking about tariffs for a

952
01:06:32,795 --> 01:06:36,715
Speaker 5:  while now. Not an super exciting

953
01:06:36,735 --> 01:06:40,555
Speaker 5:  for our wallets. I, this really, I think hit in a very

954
01:06:40,835 --> 01:06:44,035
Speaker 5:  material way in the past week number one because quite literally

955
01:06:44,865 --> 01:06:48,635
Speaker 5:  Trump's tariffs did go into force I believe Thursday morning.

956
01:06:50,385 --> 01:06:53,605
Speaker 5:  But most notably we actually saw a bunch of companies raising prices and

957
01:06:53,605 --> 01:06:56,485
Speaker 5:  raising them pretty significantly. I think probably the biggest one that

958
01:06:56,485 --> 01:07:00,165
Speaker 5:  most people are gonna experience is that Nintendo raise the price on a bunch

959
01:07:00,165 --> 01:07:03,845
Speaker 5:  of switch hardware, including the switch one, the original one,

960
01:07:04,115 --> 01:07:07,085
Speaker 5:  it's going up in price by $30. And some of the other models

961
01:07:07,685 --> 01:07:11,445
Speaker 5:  including switch ole and switch light, they're going up in price two,

962
01:07:11,765 --> 01:07:15,005
Speaker 5:  a bunch of accessories are going, going up in price. They did not raise the

963
01:07:15,005 --> 01:07:18,885
Speaker 5:  price on the switch two, which I'm assuming is because that thing is

964
01:07:18,885 --> 01:07:22,365
Speaker 5:  already very expensive and they do not want that to look worse.

965
01:07:23,585 --> 01:07:27,495
Speaker 5:  But this keeps going, right? Fujifilm, they raised

966
01:07:28,115 --> 01:07:31,415
Speaker 5:  one of their cameras went up in price by $800.

967
01:07:32,075 --> 01:07:34,575
Speaker 5:  Now this is already a very expensive camera, it was a medium format camera.

968
01:07:35,075 --> 01:07:38,535
Speaker 5:  But what is happening? This, this is a lot of money.

969
01:07:38,955 --> 01:07:42,455
Speaker 5:  Tim Cook is on, you know Tim Cook actually he, he hung out with Trump this

970
01:07:42,455 --> 01:07:46,415
Speaker 5:  week and he, him this beautiful statue

971
01:07:46,495 --> 01:07:48,855
Speaker 5:  that he made with like a, oh god, a 24 karat gold base.

972
01:07:48,955 --> 01:07:51,455
Speaker 6:  How are we not gonna mention this? Thank you for mentioning this, this, this.

973
01:07:51,475 --> 01:07:52,295
Speaker 6:  We should have led with this.

974
01:07:52,715 --> 01:07:56,575
Speaker 5:  We should have led with this. Yeah. 'cause he went to the White House to

975
01:07:56,695 --> 01:07:59,175
Speaker 5:  announce this big, you know, American manufacturing push. My God, God. But

976
01:07:59,175 --> 01:08:02,855
Speaker 5:  before he did this, he announced that they're blowing a billion dollars

977
01:08:03,045 --> 01:08:04,895
Speaker 5:  just this quarter on tariffs.

978
01:08:05,205 --> 01:08:08,855
Speaker 6:  Well, clearly what needs to happen is any company that is raising its prices

979
01:08:08,885 --> 01:08:12,855
Speaker 6:  because of tariffs, you know, Sonos is another example. This week

980
01:08:13,315 --> 01:08:17,005
Speaker 6:  you need to bring a solid gold version of your product to the White House.

981
01:08:17,105 --> 01:08:21,045
Speaker 6:  You need to set it on the desk in the Oval Office and you need to do a photo

982
01:08:21,205 --> 01:08:25,085
Speaker 6:  op with Trump. That is how you avoid tariffs. So Sonos, they

983
01:08:25,085 --> 01:08:25,725
Speaker 5:  Put Trump's name on

984
01:08:25,725 --> 01:08:29,365
Speaker 6:  It. Yeah. So Sonos ndo, get on it. Solid gold switch,

985
01:08:29,695 --> 01:08:32,805
Speaker 6:  solid goes solid gold Sonos. Make it happen.

986
01:08:33,535 --> 01:08:36,605
Speaker 5:  Addie, am I wrong? I I think that what he described actually

987
01:08:37,345 --> 01:08:41,205
Speaker 5:  is the new tariff policy. Like didn't he announce this? That if you just

988
01:08:41,205 --> 01:08:44,485
Speaker 5:  like make, make Trump happy, he will waive the

989
01:08:44,485 --> 01:08:47,725
Speaker 6:  Tariffs. Chachi PT told me he did. So let's just scroll with it. Okay?

990
01:08:48,465 --> 01:08:52,205
Speaker 7:  Is that he also announced a new tariff during the Apple thing, but then was

991
01:08:52,285 --> 01:08:55,885
Speaker 7:  like the tariff won't apply. If you build in the US there's gonna be a hundred

992
01:08:55,885 --> 01:08:59,845
Speaker 7:  percent tariff on semiconductors. But if you build in the us there's not

993
01:09:00,345 --> 01:09:04,165
Speaker 7:  we and like everything, it's incredibly ambiguous

994
01:09:04,165 --> 01:09:04,645
Speaker 7:  what that means,

995
01:09:04,735 --> 01:09:08,685
Speaker 5:  Right? It's like if you say you will build in the US if you like make a good

996
01:09:08,685 --> 01:09:12,565
Speaker 5:  effort and maybe bring a gold bar to the White House, you will not get tariffed

997
01:09:13,265 --> 01:09:14,075
Speaker 7:  Five tariffs.

998
01:09:14,535 --> 01:09:18,435
Speaker 5:  Tim Cook, he, he somehow, he is playing both sides of

999
01:09:18,435 --> 01:09:22,315
Speaker 5:  this, right? Everyone is afraid to trash talk the tariffs.

1000
01:09:22,455 --> 01:09:26,435
Speaker 5:  Tim Cook, he's on the earnings call, he's like, these we're right, we

1001
01:09:26,435 --> 01:09:30,315
Speaker 5:  are wasting a bunch of cash on tariffs. He said this two quarters in

1002
01:09:30,315 --> 01:09:33,315
Speaker 5:  a row. Then he just goes to the White House and he is like, we're all good,

1003
01:09:33,605 --> 01:09:37,035
Speaker 5:  we're all good. And it's sort of weird, like for, for them in particular.

1004
01:09:38,075 --> 01:09:42,035
Speaker 5:  I if if those, I mean again, big if, if these 100%

1005
01:09:42,035 --> 01:09:45,595
Speaker 5:  semiconductor tariffs do actually happen. They have an apple

1006
01:09:45,795 --> 01:09:49,675
Speaker 5:  loophole built in like that is what it is, right? They they were

1007
01:09:49,675 --> 01:09:52,835
Speaker 5:  like Apple, you said you're gonna build stuff in the US so we was, we won't

1008
01:09:52,835 --> 01:09:56,825
Speaker 5:  apply this to you. Right? That's, that's like fantastic for

1009
01:09:56,825 --> 01:09:57,505
Speaker 5:  Apple's business.

1010
01:09:57,845 --> 01:10:01,625
Speaker 7:  Tim Cook is really good at hitting that middle point where he doesn't seem

1011
01:10:01,625 --> 01:10:04,985
Speaker 7:  like a threat to Trump. But also he doesn't make Trump

1012
01:10:05,445 --> 01:10:05,665
Speaker 7:  mad.

1013
01:10:07,245 --> 01:10:10,495
Speaker 7:  He's very good at this. I don't really mean that as a compliment.

1014
01:10:11,995 --> 01:10:15,735
Speaker 5:  And this is how you get to him presenting a statue to Donald Trump.

1015
01:10:16,815 --> 01:10:20,535
Speaker 5:  I I was bewildered by this thing. So

1016
01:10:20,675 --> 01:10:24,655
Speaker 5:  in it there's a 24 carat gold base, which Tim Cook

1017
01:10:24,655 --> 01:10:28,535
Speaker 5:  said was from Utah, right? American made

1018
01:10:28,535 --> 01:10:32,495
Speaker 5:  gold and then on top of it is this big like circle of

1019
01:10:32,505 --> 01:10:36,495
Speaker 5:  glass that is from Corning, which is where they're gonna get the American

1020
01:10:36,495 --> 01:10:40,335
Speaker 5:  made glass for iPhones and for unknown reasons,

1021
01:10:40,645 --> 01:10:44,495
Speaker 5:  this piece of glass just says Donald Trump on it and has

1022
01:10:44,495 --> 01:10:48,455
Speaker 5:  Tim Cook's signature on it and it, it's like, I guess it's

1023
01:10:48,455 --> 01:10:52,255
Speaker 5:  commemorating that Apple is going to build in the US and they're giving it

1024
01:10:52,255 --> 01:10:56,135
Speaker 5:  to him for some reason because they

1025
01:10:56,135 --> 01:10:59,495
Speaker 5:  did a photo op at the White House. It's very funny, it came in like an Apple

1026
01:10:59,495 --> 01:11:03,415
Speaker 5:  product box that he like opened up on, you know,

1027
01:11:03,515 --> 01:11:05,015
Speaker 5:  the, the desk in the Oval Office.

1028
01:11:06,735 --> 01:11:08,585
Speaker 5:  Yeah, it was quite a presentation. Did

1029
01:11:08,585 --> 01:11:11,425
Speaker 7:  We mention Trump is gonna have a competing phone out this year? Supposedly

1030
01:11:13,105 --> 01:11:16,965
Speaker 5:  Things are gonna get spicy between those two. It's gonna

1031
01:11:16,985 --> 01:11:20,885
Speaker 5:  be an interesting year if the Trump phone really does

1032
01:11:20,885 --> 01:11:24,725
Speaker 5:  take on the iPhone. Okay. Right under the buzzer. Addie,

1033
01:11:24,865 --> 01:11:26,045
Speaker 5:  what's next? Alright.

1034
01:11:26,045 --> 01:11:29,645
Speaker 7:  Sometimes it's fun to just have some old fashioned legal drama that's not

1035
01:11:29,645 --> 01:11:33,205
Speaker 7:  going to result in a baffling Supreme Court ruling. And it is that

1036
01:11:33,555 --> 01:11:37,325
Speaker 7:  Epic has beaten Google in court again, the ninth

1037
01:11:37,325 --> 01:11:41,125
Speaker 7:  Circuit court of appeals on July 31st said it

1038
01:11:41,125 --> 01:11:44,725
Speaker 7:  would not overturn the unanimous jury verdict from 2023 that

1039
01:11:44,725 --> 01:11:48,085
Speaker 7:  Google's app store and payment apps are illegal monopolies. And

1040
01:11:48,345 --> 01:11:52,325
Speaker 7:  Google now has at this point a few weeks, it could

1041
01:11:52,325 --> 01:11:55,805
Speaker 7:  get extended time to start cracking open the app store

1042
01:11:56,465 --> 01:12:00,205
Speaker 7:  by not requiring people to use Google Pay billing. And by

1043
01:12:00,465 --> 01:12:04,165
Speaker 7:  not enforcing anti steering rules that stop people from stop app developers,

1044
01:12:04,195 --> 01:12:08,085
Speaker 7:  from directing users outside the store, it still does not have

1045
01:12:08,365 --> 01:12:12,245
Speaker 7:  to implement the really extreme stuff yet, like

1046
01:12:12,645 --> 01:12:16,365
Speaker 7:  allowing third party stores in the Google Play Store. But it's a pretty

1047
01:12:16,365 --> 01:12:20,125
Speaker 7:  significant loss, especially compared to how Epic's case

1048
01:12:20,155 --> 01:12:22,125
Speaker 7:  went with Apple where Epic mostly lost.

1049
01:12:22,355 --> 01:12:25,445
Speaker 5:  Yeah, I mean this is, this is like potentially huge and it sounds like it's,

1050
01:12:25,445 --> 01:12:28,845
Speaker 5:  it's still it we're what are we waiting on, right? It's like, is the, is

1051
01:12:28,865 --> 01:12:31,485
Speaker 5:  if the Supreme Court decides to hear it or not for

1052
01:12:31,485 --> 01:12:35,325
Speaker 7:  The really short term we're waiting to see if they appeal and then

1053
01:12:35,325 --> 01:12:39,205
Speaker 7:  are granted a stay on having to implement this

1054
01:12:39,205 --> 01:12:42,605
Speaker 7:  stuff while Yeah, we find out if it goes up to the Supreme Court. Yeah,

1055
01:12:42,605 --> 01:12:46,445
Speaker 5:  But this is like monumental, right, that you said

1056
01:12:46,445 --> 01:12:50,125
Speaker 5:  the bigger stuff doesn't come into effect but that's, that's, it's just 'cause

1057
01:12:50,125 --> 01:12:53,765
Speaker 5:  it has a longer timeline. Like they gave Google more time to do it. Whereas

1058
01:12:53,785 --> 01:12:57,005
Speaker 5:  Google is like basically out of time on this immediate stuff. The immediate

1059
01:12:57,015 --> 01:13:00,645
Speaker 5:  stuff doesn't feel like as huge of a deal to me. Like it, these are

1060
01:13:00,645 --> 01:13:04,325
Speaker 5:  important but the, the next

1061
01:13:04,625 --> 01:13:08,285
Speaker 5:  set of impacts fully crack open

1062
01:13:08,585 --> 01:13:11,445
Speaker 5:  the Google Play store. And I think it's so interesting because Android is,

1063
01:13:12,075 --> 01:13:14,645
Speaker 5:  it's an open platform. You can do whatever you want And I used to think it's

1064
01:13:14,645 --> 01:13:17,825
Speaker 5:  like very, it was very surprising to me at least

1065
01:13:18,585 --> 01:13:22,475
Speaker 5:  that Apple, which has a fully locked down platform, mostly

1066
01:13:22,895 --> 01:13:25,995
Speaker 5:  one and Google, which has an open,

1067
01:13:26,735 --> 01:13:30,355
Speaker 5:  if you know, strangle held platform

1068
01:13:31,095 --> 01:13:34,915
Speaker 5:  mostly lost. But on Google, on on Android, you

1069
01:13:34,915 --> 01:13:37,275
Speaker 5:  can install whatever you want. You just have to jump through a bunch of hoops.

1070
01:13:37,455 --> 01:13:41,115
Speaker 5:  But these new systems will allow you to just

1071
01:13:41,595 --> 01:13:45,075
Speaker 5:  download another app store through the play store and that new app store

1072
01:13:45,075 --> 01:13:48,795
Speaker 5:  can be fed by the play store, if I'm remembering this all correctly, which

1073
01:13:48,795 --> 01:13:52,355
Speaker 5:  just sort of like monumentally shakes things up, right? Like Android

1074
01:13:52,895 --> 01:13:56,395
Speaker 5:  is an open system, but the reason that Google is able to

1075
01:13:57,035 --> 01:14:00,955
Speaker 5:  maintain it is that it's apps are so powerful and so

1076
01:14:00,955 --> 01:14:04,075
Speaker 5:  important and the Play Store is really at the center of that in a lot of

1077
01:14:04,075 --> 01:14:04,195
Speaker 5:  ways.

1078
01:14:04,615 --> 01:14:07,955
Speaker 7:  It also impacts the way that they can make deals for pre-installation

1079
01:14:08,415 --> 01:14:12,205
Speaker 7:  on say Android phones and with carriers. So it,

1080
01:14:12,225 --> 01:14:14,805
Speaker 7:  and that that's just another huge part of their ecosystem.

1081
01:14:15,075 --> 01:14:18,125
Speaker 5:  This is wild. I I think Android is going to

1082
01:14:18,845 --> 01:14:22,605
Speaker 5:  potentially change dramatically if this goes into effect

1083
01:14:23,385 --> 01:14:27,245
Speaker 5:  or at least the way that Google handles it. Will I, I

1084
01:14:27,635 --> 01:14:31,445
Speaker 5:  generally think, again, I am surprised that

1085
01:14:31,445 --> 01:14:33,925
Speaker 5:  Google is losing more than Apple here. Yeah,

1086
01:14:33,925 --> 01:14:34,245
Speaker 6:  Me too.

1087
01:14:34,705 --> 01:14:38,365
Speaker 7:  We have a really good piece about this. It is partly, I think it's

1088
01:14:38,565 --> 01:14:41,125
Speaker 7:  specifically the thing you mentioned is why they lost, which is that Apple

1089
01:14:41,135 --> 01:14:44,365
Speaker 7:  could make a really credible case that we have created this incredibly locked

1090
01:14:44,365 --> 01:14:48,125
Speaker 7:  down ecosystem. It's central to our business model. We have all these

1091
01:14:48,365 --> 01:14:51,805
Speaker 7:  security concerns and Google, by being kind of halfway there,

1092
01:14:52,865 --> 01:14:55,605
Speaker 7:  it made it seem more like it was putting up hoops.

1093
01:14:57,145 --> 01:15:00,925
Speaker 7:  But I believe it's Sean who, who has a really very good comparison

1094
01:15:01,065 --> 01:15:02,965
Speaker 7:  of why Apple won and Google lost.

1095
01:15:03,445 --> 01:15:06,845
Speaker 5:  I I would love to see both of these ecosystems open up in a big way

1096
01:15:07,365 --> 01:15:11,165
Speaker 5:  and it's, again, it's surprising to me that is really interesting. It's really

1097
01:15:11,365 --> 01:15:14,605
Speaker 5:  interesting that by being a little more open, but it really has always been

1098
01:15:14,755 --> 01:15:18,605
Speaker 5:  sort of, it's, it's been a little fake. Like I, I've gotten a chance

1099
01:15:18,605 --> 01:15:22,485
Speaker 5:  to look at the, at the terms of the deals that Google has with mobile carriers

1100
01:15:23,065 --> 01:15:27,005
Speaker 5:  and you start to realize that Android is is only

1101
01:15:27,035 --> 01:15:30,965
Speaker 5:  sort of theirs, right? Like they have to,

1102
01:15:31,055 --> 01:15:34,405
Speaker 5:  right? They make carriers or they, they make phone operators basically agree

1103
01:15:34,405 --> 01:15:38,165
Speaker 5:  to these deals and if they want the play store, which of course they want

1104
01:15:38,165 --> 01:15:42,085
Speaker 5:  the Play store, that's where all the apps are. You gotta put Gmail in

1105
01:15:42,125 --> 01:15:45,045
Speaker 5:  a certain spot on your phone. You gotta put Google Docs in a certain spot

1106
01:15:45,045 --> 01:15:48,525
Speaker 5:  in your phone. It specifies where the folders can go, specifies how many

1107
01:15:48,525 --> 01:15:52,365
Speaker 5:  levels deep the folders can be. It specifies on which page of the home screen

1108
01:15:52,365 --> 01:15:56,205
Speaker 5:  it is and you start, I guess that clearly hurt

1109
01:15:56,205 --> 01:15:58,525
Speaker 5:  them in court when you, when you look at these terms and you go, oh this

1110
01:15:58,525 --> 01:16:01,765
Speaker 5:  is not, this is not as open as it is supposed to be

1111
01:16:03,225 --> 01:16:07,085
Speaker 5:  and it will be very interesting next year if these things actually go

1112
01:16:07,085 --> 01:16:10,845
Speaker 5:  into effect and we'll get to see what happens if that Android really opens

1113
01:16:10,845 --> 01:16:14,525
Speaker 5:  up to a much more dramatic degree than it currently is in practice.

1114
01:16:14,995 --> 01:16:16,085
Speaker 5:  Alright Alex,

1115
01:16:16,465 --> 01:16:20,365
Speaker 6:  I'm gonna try to end us without the thunder on the last one since we have

1116
01:16:20,365 --> 01:16:22,485
Speaker 6:  failed at every turn on that

1117
01:16:24,075 --> 01:16:28,045
Speaker 6:  Instagram rolled out an update that naturally has caused

1118
01:16:28,125 --> 01:16:32,005
Speaker 6:  a ton of uproar. They added a map to

1119
01:16:32,005 --> 01:16:35,605
Speaker 6:  Instagram, it's like the snap map where you can see

1120
01:16:36,265 --> 01:16:39,165
Speaker 6:  people that you follow on a map and

1121
01:16:40,195 --> 01:16:44,125
Speaker 6:  naturally people freaked out and Instagram has

1122
01:16:44,125 --> 01:16:47,885
Speaker 6:  had to do a bunch of messaging around it. Apparently the,

1123
01:16:48,205 --> 01:16:51,125
Speaker 6:  I don't know if either of you guys have gone through the process of opening

1124
01:16:51,235 --> 01:16:55,125
Speaker 6:  this map is pretty confusing as to who you're sharing with and

1125
01:16:55,125 --> 01:16:58,685
Speaker 6:  what context. And apparently if you would tag a

1126
01:16:59,365 --> 01:17:03,325
Speaker 6:  location on a story that you shared that would show up as if you were on

1127
01:17:03,325 --> 01:17:06,925
Speaker 6:  the map at that specific location from, so there was a bunch of like just

1128
01:17:07,025 --> 01:17:10,965
Speaker 6:  bad design stuff. Meta's been stressing, look, it's all opt-in. We

1129
01:17:10,965 --> 01:17:14,725
Speaker 6:  had like double opt-in but apparently they would also show you

1130
01:17:14,785 --> 01:17:18,725
Speaker 6:  on the map even before you had opted in, which made people naturally

1131
01:17:18,725 --> 01:17:22,565
Speaker 6:  think that they had already opted in to sharing their location with their

1132
01:17:22,565 --> 01:17:24,125
Speaker 6:  entire Instagram friend list.

1133
01:17:26,025 --> 01:17:29,565
Speaker 6:  And I would just like to point out that there's just an inherent fall in

1134
01:17:29,565 --> 01:17:33,505
Speaker 6:  this where like your Instagram social graph, who

1135
01:17:33,505 --> 01:17:37,265
Speaker 6:  you follow. Yes. Are there people there, you know, very close

1136
01:17:37,265 --> 01:17:40,945
Speaker 6:  friends, family members, significant others that you would be

1137
01:17:40,945 --> 01:17:43,505
Speaker 6:  comfortable sharing your location with? Probably for a lot of people

1138
01:17:44,805 --> 01:17:48,265
Speaker 6:  Is anyone wanting to share their location with even a significant

1139
01:17:48,265 --> 01:17:52,065
Speaker 6:  percentage of their Instagram following list? I'm going to guess

1140
01:17:52,245 --> 01:17:56,185
Speaker 6:  no. And the fact that it's on you to curate that and to go, oh,

1141
01:17:56,185 --> 01:17:59,785
Speaker 6:  they're on a certain list, they're on close friends or whatever, just puts

1142
01:17:59,785 --> 01:18:03,705
Speaker 6:  a lot of work on the user and like, I think what Snap has done is that people

1143
01:18:03,705 --> 01:18:07,665
Speaker 6:  just use it for a different reason. Like people don't, it's not such a huge

1144
01:18:07,815 --> 01:18:11,625
Speaker 6:  broadcast platform where you're following a ton of people in

1145
01:18:11,625 --> 01:18:15,025
Speaker 6:  this way. And so Instagram's trying to shoehorn this

1146
01:18:15,495 --> 01:18:19,465
Speaker 6:  interface into a social network that's just not really built for this

1147
01:18:19,465 --> 01:18:22,585
Speaker 6:  level of intimacy. And I think that's what they're experiencing right now.

1148
01:18:22,685 --> 01:18:26,625
Speaker 6:  But also like they should have done better messaging. This, you

1149
01:18:26,625 --> 01:18:30,585
Speaker 6:  know, the, the onboarding seemed confusing and it's become like a meme

1150
01:18:30,585 --> 01:18:32,945
Speaker 6:  that, you know, Instagram is showing you on the map. Now

1151
01:18:33,185 --> 01:18:37,105
Speaker 5:  I I have to say they have had eight years I believe

1152
01:18:37,165 --> 01:18:40,745
Speaker 5:  to rip off Snap Maps. Yeah, It is rough that

1153
01:18:40,975 --> 01:18:44,825
Speaker 5:  they could not do it right, like that, that is

1154
01:18:44,825 --> 01:18:48,625
Speaker 5:  pretty, pretty bad that they, they're basically just lifting

1155
01:18:48,785 --> 01:18:52,745
Speaker 5:  a feature and it's still bewildering and a bad user experience

1156
01:18:52,745 --> 01:18:55,625
Speaker 5:  And I believe they've been testing this for months too. Yeah, I do think

1157
01:18:55,625 --> 01:18:58,185
Speaker 5:  your point about the, the

1158
01:18:59,175 --> 01:19:02,825
Speaker 5:  following list on both platforms inherently being different

1159
01:19:02,965 --> 01:19:06,945
Speaker 5:  is really interesting. 'cause I've always, like I, I'm like a

1160
01:19:06,945 --> 01:19:10,625
Speaker 5:  little bit above Snapchat age and so I, this is not a platform that I

1161
01:19:10,735 --> 01:19:14,705
Speaker 5:  have used with any regularity and so the Snap Maps thing has

1162
01:19:14,705 --> 01:19:17,185
Speaker 5:  never really been appealing to me or made sense to me. But you're right,

1163
01:19:17,725 --> 01:19:21,705
Speaker 5:  the, if you're using Snapchat you have a much tighter friend group,

1164
01:19:22,475 --> 01:19:25,625
Speaker 5:  right? You don't have to worry about that in the same way as Instagram, which

1165
01:19:25,925 --> 01:19:29,585
Speaker 5:  for the past several for, for a while now has pushed you to have this

1166
01:19:29,655 --> 01:19:33,585
Speaker 5:  much broader relationship, right? It is the influencer platform. Yeah.

1167
01:19:33,585 --> 01:19:36,145
Speaker 5:  And that is just not true in the same way on Snap. Yeah

1168
01:19:36,405 --> 01:19:40,265
Speaker 6:  And look, I'm sure people will use it. Instagram is huge. It has

1169
01:19:40,265 --> 01:19:43,945
Speaker 6:  billions of users. I'm sure there are people with Fence

1170
01:19:43,975 --> 01:19:47,665
Speaker 6:  does who, you know, keep their fragrance, who keep their friend list

1171
01:19:47,665 --> 01:19:51,545
Speaker 6:  super tight and curated and will use the map. But

1172
01:19:51,735 --> 01:19:53,425
Speaker 6:  yeah, they definitely could have rolled this one out better.

1173
01:19:53,595 --> 01:19:56,465
Speaker 5:  Addie you big on broadcasting your location to everybody. You know,

1174
01:19:56,845 --> 01:20:00,665
Speaker 7:  Any service that has a location tracking feature whenever anyone is building

1175
01:20:00,665 --> 01:20:04,065
Speaker 7:  it at a website at at an app developer, there should be like

1176
01:20:05,095 --> 01:20:08,845
Speaker 7:  a button that they have to break glass with their bare hands

1177
01:20:09,425 --> 01:20:13,005
Speaker 7:  to deploy it with. You should not share people's

1178
01:20:13,405 --> 01:20:17,325
Speaker 7:  locations unless you really, really have to. And it should be as painful

1179
01:20:17,425 --> 01:20:19,965
Speaker 7:  for you as possible to have the rollout.

1180
01:20:20,375 --> 01:20:24,325
Speaker 6:  Again, they, there is like a double opt-in consent flow for this. So

1181
01:20:24,325 --> 01:20:27,725
Speaker 6:  they are saying, but it's the way they presented it and like the fact that

1182
01:20:27,785 --> 01:20:31,005
Speaker 6:  you showed up on the map before you consented even though other people weren't

1183
01:20:31,005 --> 01:20:34,765
Speaker 6:  visible, which then of course is gonna make someone think, oh I am already

1184
01:20:34,765 --> 01:20:38,205
Speaker 6:  sharing even though I haven't opted in. It's more about the UI and the way

1185
01:20:38,205 --> 01:20:41,285
Speaker 6:  that, but they were not literally opting in everyone to sharing their location

1186
01:20:41,285 --> 01:20:41,605
Speaker 6:  on the map.

1187
01:20:41,985 --> 01:20:44,805
Speaker 7:  Oh no, I'm not saying that the people should have, I'm saying that the developers

1188
01:20:44,805 --> 01:20:47,405
Speaker 7:  should have to break the glass with their bare hands. You just shouldn't

1189
01:20:47,415 --> 01:20:49,485
Speaker 7:  track people's location and social networks.

1190
01:20:49,755 --> 01:20:53,725
Speaker 5:  There's also like something going on with meta platforms that

1191
01:20:53,725 --> 01:20:57,645
Speaker 5:  there's this thing on Meta's AI app where you have to hit like

1192
01:20:57,685 --> 01:21:00,725
Speaker 5:  a gigantic share button to share your stuff to the feed

1193
01:21:01,565 --> 01:21:05,405
Speaker 5:  publicly and people still hit it on really personal

1194
01:21:05,405 --> 01:21:08,645
Speaker 5:  stuff that they're talking about with ai. So there's just some extent to

1195
01:21:08,645 --> 01:21:11,645
Speaker 5:  which like, I don't know if it's that platform or that platform is user base,

1196
01:21:12,025 --> 01:21:15,845
Speaker 5:  but you gotta be careful and people clearly are not,

1197
01:21:15,915 --> 01:21:17,645
Speaker 5:  even if the thing is set up correctly.

1198
01:21:17,965 --> 01:21:21,765
Speaker 7:  I do wish this has shown up earlier so that they could bring it up in the

1199
01:21:22,075 --> 01:21:25,245
Speaker 7:  antitrust trial over whether Snapchat and Instagram were the same thing,

1200
01:21:26,265 --> 01:21:29,845
Speaker 5:  If only, well there were also some TikTok features, but alas

1201
01:21:30,065 --> 01:21:33,685
Speaker 5:  we have been struck down. Alright, that was a good one. That's it for The

1202
01:21:33,765 --> 01:21:36,725
Speaker 5:  Vergecast. Keep your eye on the feeds over the weekend because the songs

1203
01:21:36,725 --> 01:21:40,085
Speaker 5:  hosting a bonus episode dropping this Sunday

1204
01:21:40,585 --> 01:21:42,285
Speaker 5:  and Jen Tui is picking up

1205
01:22:09,365 --> 01:22:12,245
Speaker 5:  really excited to launch these And I hope you guys enjoy them. We love to

1206
01:22:12,245 --> 01:22:14,885
Speaker 5:  hear your questions and feedback. Let us know what you want us to talk about

1207
01:22:14,885 --> 01:22:18,805
Speaker 5:  this summer. Email us vergecast to The Verge dot com. I have access

1208
01:22:18,805 --> 01:22:22,605
Speaker 5:  to that thing now And I can see what you're saying or give us a call. 8 6 6

1209
01:22:22,925 --> 01:22:26,565
Speaker 5:  VERGE 11 The Vergecast is a production of The Verge and Fox Media Podcast

1210
01:22:26,565 --> 01:22:30,085
Speaker 5:  network. Our show is produced by Eric Gomez, Brandon Keffer, Travis Lar,

1211
01:22:30,225 --> 01:22:31,725
Speaker 5:  and Andrew Marino. We'll see you next week.

