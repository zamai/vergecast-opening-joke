1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 2f0f53ed-dc34-4387-bc86-8e6181ed3cc1
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/8267171490980933325/4690319473789287517/s93290-US-6786s-1748222745.mp3
Description: Bad news if you don't care about AI: this week was absolutely chock-full of AI news. First, Nilay, David, and The Verge's Alex Heath talk about the news that OpenAI and Jony Ive are teaming up to build... something. A gadget, for sure, maybe lots of gadgets. We don't know much, but we have a lot of thoughts, and a lot of questions. After that, the hosts talk through all the news at Google I/O, including what's new with Gemini, Google Search, Project Astra, Project Mariner, and the countless other ways Google is putting AI absolutely everywhere. Finally, in the lightning round, we buckle up for another round of Brendan Carr is a Dummy, talk through some late-breaking Apple gadget news, and marvel over the future of conference calls.


2
00:00:32,365 --> 00:00:36,355
Speaker 4:  Hello. Welcome to our chest, the flagship podcast of Alex Heath and I

3
00:00:36,355 --> 00:00:39,475
Speaker 4:  trying to talk to Sergei Brynn and being deflected at every opportunity.

4
00:00:41,175 --> 00:00:42,835
Speaker 4:  It was rough. It was bad.

5
00:00:45,255 --> 00:00:48,635
Speaker 4:  Hi your friend Neil. Alex Heath is here. Hey, what's going on? David Pierce

6
00:00:48,655 --> 00:00:52,315
Speaker 4:  is here. Hello. So Alex and I were at IO at google io. It's big news of the

7
00:00:52,315 --> 00:00:54,835
Speaker 4:  week. Google announced a bunch of stuff. We're gonna talk about it where

8
00:00:54,835 --> 00:00:58,505
Speaker 4:  there was this moment where Sergey was in the AI

9
00:00:58,505 --> 00:01:01,305
Speaker 4:  sandbox, like the demo area. He was there last year too and he like talked

10
00:01:01,305 --> 00:01:04,945
Speaker 4:  to reporters for a long time just riffing about ai. So he was there at the

11
00:01:04,945 --> 00:01:08,865
Speaker 4:  end this year and I kept describing it as like the sword

12
00:01:08,865 --> 00:01:12,625
Speaker 4:  in the stone. Like all these reporters kept on trying to talk to him and

13
00:01:12,625 --> 00:01:14,905
Speaker 4:  just like failing, including me,

14
00:01:16,455 --> 00:01:18,305
Speaker 4:  including both of us. This

15
00:01:18,305 --> 00:01:21,945
Speaker 5:  Makes me think somebody must have given him a speech not to talk to reporters.

16
00:01:22,105 --> 00:01:24,425
Speaker 5:  'cause the other thing that Strike a Bri likes to do is just say some truly

17
00:01:24,655 --> 00:01:27,465
Speaker 5:  wild stuff that is not helpful if you're Google.

18
00:01:28,935 --> 00:01:32,315
Speaker 5:  But he likes to talk to people. So I assume somebody gave him a stern speech

19
00:01:32,855 --> 00:01:33,955
Speaker 5:  not to talk to reporters.

20
00:01:34,175 --> 00:01:38,155
Speaker 4:  No, I don't think it was that Actually he was working on a demo. Like

21
00:01:38,175 --> 00:01:41,235
Speaker 4:  he was in the demo. Right. He was like playing with the stuff. Right. And

22
00:01:41,495 --> 00:01:45,195
Speaker 4:  he, it was flow, the new video tool that Google released that runs on VO

23
00:01:45,195 --> 00:01:47,875
Speaker 4:  three where it's like, it's basically just, it's like iMovie for AI video.

24
00:01:47,875 --> 00:01:51,475
Speaker 4:  Right. And he was trying to get it, I think to make

25
00:01:51,665 --> 00:01:55,555
Speaker 4:  himself like he was prompting it to make himself. I miss

26
00:01:55,555 --> 00:01:57,635
Speaker 4:  missed that. That's that's really good. That's what I heard. Like secondhand.

27
00:01:57,695 --> 00:02:01,555
Speaker 4:  But he was, I, I mean I watched him using it and say out

28
00:02:01,555 --> 00:02:03,995
Speaker 4:  loud like, this isn't working the way it should. And then someone else, you

29
00:02:03,995 --> 00:02:05,915
Speaker 4:  know, then all the Googlers were like, this isn't working. And he looked,

30
00:02:06,455 --> 00:02:08,955
Speaker 4:  he was very locked in on whatever was doing. And then someone else told me

31
00:02:08,955 --> 00:02:12,595
Speaker 4:  that it's not supposed to make Sergei brand. So he was trying to make it

32
00:02:12,595 --> 00:02:13,275
Speaker 4:  make Sergey

33
00:02:13,275 --> 00:02:14,275
Speaker 5:  Brand. That's amazing.

34
00:02:15,675 --> 00:02:18,435
Speaker 4:  So he was like totally focused on this thing not working, which is why he

35
00:02:18,435 --> 00:02:21,675
Speaker 4:  wasn't talking to anyone. And then he went over to do the glasses demo. Yeah.

36
00:02:22,055 --> 00:02:25,075
Speaker 5:  Is that the AI executive of like Googling yourself every once in a while?

37
00:02:25,075 --> 00:02:28,955
Speaker 5:  You like train the new model and then see how successfully it can

38
00:02:29,195 --> 00:02:33,115
Speaker 5:  replicate you and that's, that's your model of how big a deal

39
00:02:33,115 --> 00:02:33,475
Speaker 5:  you are.

40
00:02:34,055 --> 00:02:38,035
Speaker 4:  It is it? Yeah, probably. I mean how often do you think Ssar Googles himself?

41
00:02:39,145 --> 00:02:42,945
Speaker 4:  I like that must be crazy making 'cause you are the one person who can

42
00:02:43,185 --> 00:02:44,905
Speaker 4:  actually just change the search rankings and be

43
00:02:46,515 --> 00:02:50,385
Speaker 4:  likes like Sundar Pecha is, is super cool. Dot com is gonna be the first

44
00:02:50,405 --> 00:02:50,825
Speaker 4:  result.

45
00:02:50,825 --> 00:02:54,705
Speaker 5:  Everything is an episode of Silicon Valley, Eli, every single thing.

46
00:02:54,905 --> 00:02:57,545
Speaker 4:  I mean, it was just very funny 'cause everyone saw him. He was just like

47
00:02:57,545 --> 00:03:01,505
Speaker 4:  in the middle of the room and I had just failed. I walked over and

48
00:03:01,505 --> 00:03:04,865
Speaker 4:  I was like, Hey man. And he was like, whatever. And I just like left and

49
00:03:04,865 --> 00:03:08,265
Speaker 4:  then like three other reporters had that experience. And then Alex, you know,

50
00:03:08,935 --> 00:03:12,585
Speaker 4:  star reporter Alex, he like super confidently saunters over there.

51
00:03:12,805 --> 00:03:15,265
Speaker 4:  And I was like, he's gonna, he's gonna try to pull that sword out the stone

52
00:03:15,285 --> 00:03:18,825
Speaker 4:  man. He just like did, didn't happen. The

53
00:03:18,845 --> 00:03:22,745
Speaker 6:  Eye roll that I got from a Google Communications executive as I approached

54
00:03:23,025 --> 00:03:25,345
Speaker 6:  Sergei was was was really good. Yeah,

55
00:03:25,485 --> 00:03:29,145
Speaker 4:  It was good. But later he crashed. He crashed a another keynote.

56
00:03:29,455 --> 00:03:33,225
Speaker 4:  Yeah. Like Demi, the ceo EO of Google DeepMind was doing a like a keynote

57
00:03:33,655 --> 00:03:37,465
Speaker 4:  interview type deal, fireside chat and just like came

58
00:03:37,465 --> 00:03:40,545
Speaker 4:  on stage with him and everyone's like, here I'm, and then he did in fact

59
00:03:40,605 --> 00:03:42,505
Speaker 4:  say just bonkers things. Yeah.

60
00:03:42,775 --> 00:03:46,025
Speaker 6:  Including like, I just get to tell this guy what to do and

61
00:03:46,605 --> 00:03:50,185
Speaker 6:  we live. We may be in a stack of simulations. Yeah. There was a lot. Yeah.

62
00:03:50,185 --> 00:03:53,545
Speaker 4:  Yeah. His answer to do we live in a simulation was if you think we live in

63
00:03:53,545 --> 00:03:57,185
Speaker 4:  one simulation and the people who made that simulation also live in a simulation

64
00:03:57,185 --> 00:03:58,425
Speaker 4:  and you have to deal with that. Yeah.

65
00:03:58,735 --> 00:04:01,785
Speaker 6:  Neli, you've been going, you've been going to IO for longer than me. Did

66
00:04:01,785 --> 00:04:05,705
Speaker 6:  this feel like a return to early IO for you? Like I

67
00:04:05,705 --> 00:04:08,425
Speaker 6:  heard a few people be like, especially the Sergei talk, but

68
00:04:09,845 --> 00:04:13,745
Speaker 6:  it just felt like 10 years ago. Like, I don't know, I don do

69
00:04:13,865 --> 00:04:15,185
Speaker 6:  you, do you see what I'm saying there?

70
00:04:15,825 --> 00:04:19,705
Speaker 4:  I do. It was different. Google's just more corporate now. Yeah. In

71
00:04:19,705 --> 00:04:22,585
Speaker 4:  in that way. We'll come to it. I think it was very confident and I think

72
00:04:22,585 --> 00:04:23,665
Speaker 4:  that's what people were picking up on.

73
00:04:23,665 --> 00:04:27,025
Speaker 5:  Google's feeling itself again in a, in a real way that we should talk about.

74
00:04:27,175 --> 00:04:27,465
Speaker 5:  Yeah.

75
00:04:27,465 --> 00:04:30,545
Speaker 4:  They were like, we can do stuff. And they haven't felt that way like

76
00:04:31,245 --> 00:04:33,625
Speaker 4:  for like the past several years. Like here's what we've done. We've added

77
00:04:33,645 --> 00:04:36,425
Speaker 4:  one feature to Android and Samsung is gonna use it. And that has been IO

78
00:04:37,125 --> 00:04:39,705
Speaker 4:  and this was very different. There was a lot of confidence in IO this year.

79
00:04:39,725 --> 00:04:42,505
Speaker 4:  We should After that. We're gonna come to that in a second. I, we, I think

80
00:04:42,505 --> 00:04:46,185
Speaker 4:  we have to start with Johnny, ive and Sam Altman

81
00:04:46,565 --> 00:04:50,505
Speaker 4:  trying to upstage IO by announcing a company called io. Like

82
00:04:50,505 --> 00:04:53,705
Speaker 4:  literally the next day before we get into all the news, we're gonna talk

83
00:04:53,705 --> 00:04:56,185
Speaker 4:  about that. We're gonna talk about, like I said, Google io. There's lightning

84
00:04:56,185 --> 00:04:59,705
Speaker 4:  round. Brendan has Brendan, two pieces of housekeeping.

85
00:05:00,085 --> 00:05:03,585
Speaker 4:  One, we have a survey about The Vergecast, what you want from us, what you

86
00:05:03,585 --> 00:05:07,545
Speaker 4:  want from our podcast. That's a vme.com/survey. Go go take the survey.

87
00:05:08,215 --> 00:05:12,145
Speaker 4:  Someone will make a chart and then we'll, we'll, we'll listen to them.

88
00:05:12,385 --> 00:05:15,505
Speaker 4:  I think we're obligated to say that. We'll listen to 'em. I think you, you

89
00:05:15,505 --> 00:05:15,945
Speaker 4:  know the truth.

90
00:05:16,205 --> 00:05:20,065
Speaker 5:  Here's what I promise is that if you take the survey, I will give a

91
00:05:20,065 --> 00:05:23,865
Speaker 5:  readout on this podcast of Neli yelling at whoever presents us the chart

92
00:05:24,085 --> 00:05:26,185
Speaker 5:  at the end of this process. So do the survey.

93
00:05:26,685 --> 00:05:30,265
Speaker 4:  No one can tell me what to do. That's what we sell here. And then second,

94
00:05:30,625 --> 00:05:33,465
Speaker 4:  speaking of no one can tell us what to do. One of the ways that we protect

95
00:05:33,465 --> 00:05:36,865
Speaker 4:  ourselves from being told what to do is by having a direct financial relationship

96
00:05:36,865 --> 00:05:40,705
Speaker 4:  with you, the reader slash listener and VERGE subscriptions are on sale

97
00:05:41,145 --> 00:05:43,945
Speaker 4:  for Memorial Day. 'cause we're like, why not have a sale? They're 40% off,

98
00:05:43,945 --> 00:05:47,865
Speaker 4:  which means they're $35 a year. That's The Verge dot com slash subscribe.

99
00:05:48,205 --> 00:05:51,825
Speaker 4:  And what you were buying, just to be very clear, yes. A bunch of great reporting.

100
00:05:52,035 --> 00:05:55,025
Speaker 4:  David getting increasingly redder on the YouTube channel every week. That's

101
00:05:55,025 --> 00:05:59,005
Speaker 4:  something that we provide for that, that money. And then, you

102
00:05:59,005 --> 00:06:01,765
Speaker 4:  know, our ethics, what we fundamentally sell here is our ethics policy. You

103
00:06:01,765 --> 00:06:05,325
Speaker 4:  just can't tell us what to do. We don't do brand integrations, sponsorships.

104
00:06:06,125 --> 00:06:09,845
Speaker 4:  I don't think a phone casemaker in, in the middle of the review videos. 'cause

105
00:06:09,845 --> 00:06:13,125
Speaker 4:  no one can tell us what to do. And we, we get to do that 'cause you pay us

106
00:06:13,125 --> 00:06:15,605
Speaker 4:  the money directly and then take a survey and then we don't listen to the

107
00:06:15,605 --> 00:06:19,405
Speaker 4:  survey. It's a weird feedback loop, but it works. Works. I think it's been

108
00:06:19,405 --> 00:06:20,725
Speaker 4:  working for us. It's

109
00:06:20,725 --> 00:06:23,765
Speaker 6:  Good. 30 $35 is good too. That's like a salad in Mountain View.

110
00:06:25,465 --> 00:06:26,005
Speaker 4:  It really

111
00:06:26,105 --> 00:06:26,325
Speaker 6:  Is.

112
00:06:26,865 --> 00:06:30,565
Speaker 5:  That's one era, one smoothie. Yeah. In LA terms it's

113
00:06:30,565 --> 00:06:33,725
Speaker 4:  Good. We're, we don't, we're, you know, we're new to this particular game.

114
00:06:33,725 --> 00:06:37,685
Speaker 4:  This is our first ever sale. I've, we, I feel like, I literally feel

115
00:06:37,685 --> 00:06:41,445
Speaker 4:  like I'm trying to sell you a Cadillac right now. But we have,

116
00:06:41,465 --> 00:06:44,685
Speaker 4:  we are learning all these moves and it's like kind of fun. It's like new,

117
00:06:44,865 --> 00:06:48,485
Speaker 4:  new problems to solve. So let us know how we're doing and by let us know

118
00:06:48,485 --> 00:06:52,445
Speaker 4:  how we're doing. I mean spend $35 a year on our product. Thank you.

119
00:06:53,655 --> 00:06:55,035
Speaker 4:  Is that how you sell anything?

120
00:06:55,285 --> 00:06:55,875
Speaker 5:  Doing great.

121
00:06:56,365 --> 00:06:59,315
Speaker 6:  Eli's on three hours of sleep. Guys, we just, I just wanna say that going

122
00:06:59,315 --> 00:06:59,915
Speaker 6:  into this show,

123
00:07:00,615 --> 00:07:04,595
Speaker 4:  My flight was delayed coming home from IO and so it effectively turned

124
00:07:04,595 --> 00:07:08,475
Speaker 4:  into a red eye and I'm just super loopy. I'm, I'm

125
00:07:08,595 --> 00:07:12,395
Speaker 4:  drinking coffee at during the afternoon Vergecast for the first time in years.

126
00:07:12,935 --> 00:07:16,645
Speaker 4:  So we're gonna get through it. It's gonna be great. Okay,

127
00:07:16,735 --> 00:07:20,695
Speaker 4:  let's start with Johnny. Ive in

128
00:07:20,695 --> 00:07:21,135
Speaker 4:  his arms,

129
00:07:23,095 --> 00:07:26,325
Speaker 4:  I mean that's what they announced. They announced Johnny Ives walk, right.

130
00:07:26,505 --> 00:07:29,925
Speaker 4:  Sam Altman, Johnny Ive released a video. I think they got scooped a little

131
00:07:29,925 --> 00:07:33,085
Speaker 4:  bit. Alex like, I think the Journal and Bloomberg kind of had it

132
00:07:34,345 --> 00:07:37,405
Speaker 4:  and then they, they like put out this video to announce it. That's, that's

133
00:07:37,405 --> 00:07:37,965
Speaker 4:  my sense of it.

134
00:07:38,395 --> 00:07:42,285
Speaker 6:  They had some embargoed stuff. Johnny and Sam Altman did

135
00:07:42,485 --> 00:07:46,085
Speaker 6:  a couple interviews. They did one with Bloomberg, one with the Times and

136
00:07:46,085 --> 00:07:50,045
Speaker 6:  then in a very good spot people saw Johnny

137
00:07:50,445 --> 00:07:53,285
Speaker 6:  actually filming this video that he did with Sam on

138
00:07:53,505 --> 00:07:57,485
Speaker 6:  May 2nd, like early May. That's, so this

139
00:07:57,485 --> 00:07:59,965
Speaker 6:  video was done almost a month ago.

140
00:08:00,225 --> 00:08:03,285
Speaker 4:  So there's this announcement that the basics of the announcement is near,

141
00:08:03,285 --> 00:08:07,125
Speaker 4:  as I can understand It, is that Johnny Ives started a new company that

142
00:08:07,125 --> 00:08:11,045
Speaker 4:  is staffed by a bunch of his star former Apple designers. That company

143
00:08:11,105 --> 00:08:14,885
Speaker 4:  is called io. Very funny because this company lasted for

144
00:08:14,905 --> 00:08:18,285
Speaker 4:  two minutes and it has been sold to OpenAI

145
00:08:18,785 --> 00:08:22,685
Speaker 4:  for 6.5 billion in OpenAI stock. Which is also a very complicated

146
00:08:22,715 --> 00:08:26,005
Speaker 4:  idea. And then Johnny, ive himself is not

147
00:08:26,535 --> 00:08:30,445
Speaker 4:  going to go work at OpenAI. He's gonna stay at his design agency love

148
00:08:30,445 --> 00:08:33,085
Speaker 4:  firm, which will now have no other clients except for OpenAI

149
00:08:33,465 --> 00:08:34,885
Speaker 6:  At some point. Yeah. The

150
00:08:34,885 --> 00:08:38,285
Speaker 5:  Only tick that's more complicated than that is that open AI's

151
00:08:38,285 --> 00:08:42,245
Speaker 5:  investment firm I believe was associated with io.

152
00:08:42,985 --> 00:08:46,965
Speaker 5:  But OpenAI will tell you that open AI's investment firm is not part of OpenAI.

153
00:08:47,875 --> 00:08:51,725
Speaker 6:  Yeah. Fun fact, a thing I found out this week is that the OpenAI startup

154
00:08:51,795 --> 00:08:53,365
Speaker 6:  fund is not part of OpenAI.

155
00:08:53,875 --> 00:08:57,645
Speaker 5:  None of this makes any sense. It's all pretend like the thing to remember

156
00:08:57,645 --> 00:09:01,565
Speaker 5:  at the beginning of this is all the money is pretend like all the money that

157
00:09:01,565 --> 00:09:05,245
Speaker 5:  Sam Altman can raise is always pretend all of the different machinations

158
00:09:05,245 --> 00:09:09,005
Speaker 5:  of how the money moves around is pretend whether OpenAI is or is not a for-profit

159
00:09:09,005 --> 00:09:12,965
Speaker 5:  company is pretend this new company they created is pretend like it's,

160
00:09:13,195 --> 00:09:14,605
Speaker 5:  it's all made up.

161
00:09:14,865 --> 00:09:18,845
Speaker 4:  And the, the products that they've announced the most pretend of all. Yes.

162
00:09:19,905 --> 00:09:22,645
Speaker 4:  So they, they released a video so they announced this deal, they're gonna

163
00:09:22,645 --> 00:09:24,965
Speaker 4:  work together, they're gonna make a new generation of hardware products.

164
00:09:25,005 --> 00:09:28,125
Speaker 4:  I think they're coming the first start coming next year. You gotta,

165
00:09:28,545 --> 00:09:32,405
Speaker 6:  So I would say like announced at the end of next year in shipping in

166
00:09:32,405 --> 00:09:33,845
Speaker 6:  2027 if I

167
00:09:33,845 --> 00:09:36,885
Speaker 4:  Had to guess. But what they really shipped was this video.

168
00:09:37,385 --> 00:09:37,605
Speaker 6:  Yes.

169
00:09:38,315 --> 00:09:41,935
Speaker 4:  And I, I think the reason my adult, my sleep deprived brain

170
00:09:42,515 --> 00:09:46,015
Speaker 4:  is like there the video was the people, lots of people saw them making this

171
00:09:46,015 --> 00:09:49,955
Speaker 4:  video in early May and I think everybody understood something was up.

172
00:09:50,935 --> 00:09:54,595
Speaker 4:  And I, the fundamental thing they announced in this video is that Johnny

173
00:09:54,775 --> 00:09:56,515
Speaker 4:  Ive walks around like he owns the place.

174
00:09:57,025 --> 00:09:57,795
Speaker 6:  Yeah. Like

175
00:09:57,795 --> 00:10:00,395
Speaker 4:  You just have, you just have to watch it. It isn't, It is literally one of

176
00:10:00,395 --> 00:10:02,155
Speaker 4:  the most confident walks in history. It's

177
00:10:02,155 --> 00:10:02,435
Speaker 5:  Very good

178
00:10:02,655 --> 00:10:06,555
Speaker 6:  And it's this very, you know, grandiose thing and it looks like they're just

179
00:10:06,555 --> 00:10:10,235
Speaker 6:  walking through San Francisco, you know, with a camera following them and

180
00:10:10,235 --> 00:10:13,875
Speaker 6:  it, you know, it's just the normal city street on a given day. And then you

181
00:10:13,875 --> 00:10:17,035
Speaker 6:  see people shooting when they were actually filming and people were like

182
00:10:17,035 --> 00:10:20,635
Speaker 6:  filming the, the set and it's all staged. Like everyone walking

183
00:10:20,655 --> 00:10:23,915
Speaker 6:  across the like intersection with Johnny with his arms swinging. Like

184
00:10:24,445 --> 00:10:27,835
Speaker 6:  every it, it wasn't made by AI but it also wasn't,

185
00:10:27,905 --> 00:10:31,395
Speaker 4:  Yeah it does have a lot of advice. Lots of people on, on various social platforms

186
00:10:31,395 --> 00:10:34,995
Speaker 4:  are like counting the wine bottles behind them. I saw one person fully crash

187
00:10:35,055 --> 00:10:38,435
Speaker 4:  out being like the cups are moving from shot to shot And it's like, yeah

188
00:10:38,625 --> 00:10:42,435
Speaker 4:  it's 'cause it's a lot of different shots they edited together. It's fine

189
00:10:42,535 --> 00:10:46,515
Speaker 4:  you guys. Anyway, the video is those two sitting in a a

190
00:10:46,715 --> 00:10:50,635
Speaker 4:  restaurant having little cups of espresso together basically

191
00:10:50,635 --> 00:10:54,435
Speaker 4:  saying that they love each other and that there's a new class of

192
00:10:54,435 --> 00:10:58,275
Speaker 4:  computing devices to be made with AI and then Johnny's going to

193
00:10:58,575 --> 00:11:02,465
Speaker 4:  do that in some way with open ai. That is the sum

194
00:11:02,465 --> 00:11:04,495
Speaker 4:  of the announcement in the video.

195
00:11:05,045 --> 00:11:08,655
Speaker 6:  Also San Francisco is awesome. There's a lot of love for San Francisco

196
00:11:09,235 --> 00:11:13,095
Speaker 6:  and just generally like what if we change the world? You know, like

197
00:11:13,165 --> 00:11:17,135
Speaker 6:  it's, it's just this big like, you know, idea of two guys changing

198
00:11:17,155 --> 00:11:19,135
Speaker 6:  the world. I mean Yeah. Who doesn't love that?

199
00:11:19,415 --> 00:11:23,375
Speaker 5:  I think the only like specific thing I can think of that comes out of

200
00:11:23,375 --> 00:11:26,975
Speaker 5:  It is there is one device that already exists

201
00:11:27,155 --> 00:11:30,935
Speaker 5:  in some form. They have made a thing together. Yes. And there's been

202
00:11:30,935 --> 00:11:34,215
Speaker 5:  reporting that these two have been working together on hardware for a long

203
00:11:34,215 --> 00:11:37,815
Speaker 5:  time now. So something exists, I don't know how many of them there are, I

204
00:11:37,815 --> 00:11:41,135
Speaker 5:  don't know how finished It is, but like It is clear that there is one

205
00:11:41,405 --> 00:11:45,175
Speaker 5:  product in actual honest to God development. But their plan

206
00:11:45,195 --> 00:11:47,175
Speaker 5:  is to build a whole family of devices.

207
00:11:47,525 --> 00:11:50,935
Speaker 6:  Yeah. So Altman actually says in the video that he just took the prototype

208
00:11:50,935 --> 00:11:54,405
Speaker 6:  home, right? So this was in early May. You just got to take the first device

209
00:11:54,405 --> 00:11:58,205
Speaker 6:  home. I think we should talk about the reporting on the device and how it

210
00:11:58,205 --> 00:12:01,725
Speaker 6:  may or may not work. I just wanna really quick through, really quickly run

211
00:12:01,725 --> 00:12:05,445
Speaker 6:  through how this whole thing came about. 'cause I think this probably

212
00:12:05,445 --> 00:12:09,245
Speaker 6:  surprised a lot of people when they saw this. So I've talked to

213
00:12:09,425 --> 00:12:13,205
Speaker 6:  OpenAI and Love from now and the rundown is basically that

214
00:12:13,385 --> 00:12:17,125
Speaker 6:  two years ago Sam and Johnny were

215
00:12:17,125 --> 00:12:21,005
Speaker 6:  introduced about a year ago, actually almost exactly a year ago.

216
00:12:21,005 --> 00:12:24,965
Speaker 6:  They decided to start a new division inside Love from to work on

217
00:12:25,225 --> 00:12:28,765
Speaker 6:  AI devices together. And that division includes a bunch of

218
00:12:29,075 --> 00:12:32,765
Speaker 6:  legendary early Apple design leaders that worked under,

219
00:12:32,905 --> 00:12:36,645
Speaker 6:  ive including Evans Hanky who ran the industrial design group at Apple

220
00:12:36,695 --> 00:12:40,605
Speaker 6:  after he left. And then she left in 2023 and went to

221
00:12:40,605 --> 00:12:44,365
Speaker 6:  Love from, and they were doing this for several months and then employees

222
00:12:44,365 --> 00:12:47,685
Speaker 6:  at Open Eye, employees at OpenAI were starting to come in and work

223
00:12:48,505 --> 00:12:52,325
Speaker 6:  at Love from together with them. And then towards the end of last year,

224
00:12:53,595 --> 00:12:56,885
Speaker 6:  love from was thinking about raising a lot more money for

225
00:12:57,985 --> 00:13:01,895
Speaker 6:  io, this division of the company doing hardware. And that

226
00:13:01,895 --> 00:13:05,655
Speaker 6:  was when Altman was like, what if we just buy you guys fully? So they had

227
00:13:05,655 --> 00:13:09,495
Speaker 6:  already invested in this love from subsidiary through

228
00:13:09,515 --> 00:13:12,255
Speaker 6:  the OpenAI startup fund that is not owned by OpenAI

229
00:13:13,675 --> 00:13:17,375
Speaker 6:  and they just bought the rest of it. And so now a team of about

230
00:13:17,375 --> 00:13:21,215
Speaker 6:  55 people who are a lot of early Apple design

231
00:13:21,215 --> 00:13:25,055
Speaker 6:  people are going over to OpenAI and a new hardware

232
00:13:25,255 --> 00:13:29,015
Speaker 6:  division that's rolling up to directly to Sam. And Johnny is

233
00:13:29,015 --> 00:13:32,415
Speaker 6:  staying at love from with his design team but will now also

234
00:13:32,565 --> 00:13:36,255
Speaker 6:  oversee all of design at OpenAI, not just hardware.

235
00:13:36,835 --> 00:13:40,695
Speaker 6:  And Johnny also works with Airbnb. He

236
00:13:40,695 --> 00:13:44,495
Speaker 6:  helps Brian Chesky on high level design stuff. He works with Ferrari. He's

237
00:13:44,655 --> 00:13:48,575
Speaker 6:  actually designing the first ever electric Ferrari. And I

238
00:13:48,575 --> 00:13:51,535
Speaker 6:  think he wants to keep doing other things. But I think once there are projects

239
00:13:51,685 --> 00:13:55,615
Speaker 6:  that they're doing, you know, end, they're gonna be almost fully devoted

240
00:13:55,615 --> 00:13:59,175
Speaker 6:  to open ai. But he's also staying independent. So

241
00:13:59,595 --> 00:14:01,135
Speaker 6:  pretty unusual because if

242
00:14:01,135 --> 00:14:04,815
Speaker 4:  You're Johnny I the idea that you'll ever have another boss has to just not

243
00:14:04,815 --> 00:14:07,815
Speaker 4:  even, yeah it's just like not a square on the bingo card that exists.

244
00:14:08,085 --> 00:14:11,975
Speaker 5:  Yeah. But also like Nila, your point about the thing that they made is this

245
00:14:11,975 --> 00:14:15,335
Speaker 5:  video is the thing, right? Like this is, this is the

246
00:14:15,625 --> 00:14:19,615
Speaker 5:  cache that OpenAI literally couldn't buy for any amount of

247
00:14:19,615 --> 00:14:23,295
Speaker 5:  money except to give it all to Johnny. Ive there is like this company is

248
00:14:23,295 --> 00:14:26,895
Speaker 5:  desperate to convince people that it can make good, interesting, cool products

249
00:14:26,915 --> 00:14:30,895
Speaker 5:  and that It is not going to get lapped by big tech companies as all of the

250
00:14:30,895 --> 00:14:34,695
Speaker 5:  like science behind this stuff gets closer and closer and like

251
00:14:35,835 --> 00:14:39,735
Speaker 5:  the way to win here is to convince everybody that your Apple and that

252
00:14:40,055 --> 00:14:44,015
Speaker 5:  you're gonna do the hardware software services thing. And boy is there

253
00:14:44,015 --> 00:14:47,015
Speaker 5:  not a better way to do that than to put Johnny Ive and his confident walk

254
00:14:47,035 --> 00:14:50,975
Speaker 5:  in your video. So like, I frankly would be shocked if it, at the end of all

255
00:14:51,085 --> 00:14:54,925
Speaker 5:  of this Johnny Ive is like day-to-day involved in making products with OpenAI.

256
00:14:55,665 --> 00:14:59,485
Speaker 5:  But it doesn't matter. This, this today it was like the

257
00:14:59,645 --> 00:15:02,325
Speaker 5:  announcement was the thing and that's, that's all you need from Johnny.

258
00:15:02,555 --> 00:15:06,165
Speaker 4:  Yeah. Literally just need his voice in their keynote Right.

259
00:15:06,665 --> 00:15:10,045
Speaker 4:  For you know, being like plastic's inevitable and like it's fine. He'll be,

260
00:15:10,045 --> 00:15:13,875
Speaker 4:  he'll be fine. But I, before we get into the device,

261
00:15:14,075 --> 00:15:16,235
Speaker 4:  'cause there's some reporting about what it might be there, we have a lot

262
00:15:16,235 --> 00:15:19,995
Speaker 4:  of guesses over what it might be. The idea here is that Sam Halman is Steve

263
00:15:19,995 --> 00:15:23,315
Speaker 4:  Jobs, right? Yep. That it's very clear. Like even in that video

264
00:15:23,865 --> 00:15:26,435
Speaker 4:  he's like, I'm, everything in my life has led to this moment.

265
00:15:28,035 --> 00:15:32,025
Speaker 4:  Altman makes reference to the iPhone and the MacBook Air and he is like,

266
00:15:32,025 --> 00:15:34,905
Speaker 4:  we're gonna build the next version of this. And then, you know, they just

267
00:15:34,905 --> 00:15:37,425
Speaker 4:  spend a lot of time looking deeply into each other's eyes and talking about

268
00:15:37,425 --> 00:15:40,585
Speaker 4:  working together in California. And you're like, oh, I know exactly what

269
00:15:40,585 --> 00:15:44,145
Speaker 4:  they want people to take from this. Right? It's, it's just very clear

270
00:15:44,975 --> 00:15:48,545
Speaker 4:  they, they want to be Apple and they hired Apple's legendary design team

271
00:15:49,005 --> 00:15:52,665
Speaker 4:  and then you're gonna position Altman as the next Steve Jobs and Altman

272
00:15:52,865 --> 00:15:56,785
Speaker 4:  revere Steve Jobs, he's best friends with Brian Chesky, who also revere

273
00:15:56,785 --> 00:16:00,775
Speaker 4:  Steve Jobs. Chesky was just on decoder. He talks about Apple as though he

274
00:16:01,275 --> 00:16:04,935
Speaker 4:  is like the world's foremost PhD student of Apple. It's like very funny.

275
00:16:05,285 --> 00:16:08,095
Speaker 4:  Also, that episode doesn't come out, it'll come out next week by the time

276
00:16:08,095 --> 00:16:12,055
Speaker 4:  you're listening to this. I asked Ju very directly about Ive

277
00:16:12,055 --> 00:16:15,175
Speaker 4:  and Alman and all he said was, I'm very proud to have introduced them. Good

278
00:16:15,175 --> 00:16:18,695
Speaker 4:  Dodge. I told, I congratulated him on that dodge. But

279
00:16:20,685 --> 00:16:24,535
Speaker 4:  there's some big like foundational things to point

280
00:16:24,775 --> 00:16:28,535
Speaker 4:  out about wanting to be the next Steve Jobs with AI is

281
00:16:28,915 --> 00:16:32,455
Speaker 4:  the foundational technology for the next device. And I think the most important

282
00:16:32,555 --> 00:16:35,695
Speaker 4:  one is that the AI systems don't work well yet.

283
00:16:36,335 --> 00:16:39,655
Speaker 4:  Like you just have to acknowledge it, right? Like

284
00:16:41,125 --> 00:16:44,975
Speaker 4:  I've and Jobs together, the first thing they

285
00:16:44,975 --> 00:16:48,855
Speaker 4:  made was the iMac, right? That's the first big hit product that those

286
00:16:48,875 --> 00:16:52,765
Speaker 4:  two combined to make. And the iMac

287
00:16:53,385 --> 00:16:56,805
Speaker 4:  was an right, it was just an all in one CRT. It is a piece of hardware.

288
00:16:57,235 --> 00:17:01,165
Speaker 4:  Like the big innovation was that it was blue and the thing that made it

289
00:17:01,245 --> 00:17:05,205
Speaker 4:  a hit was that the web existed, right? That

290
00:17:05,205 --> 00:17:08,325
Speaker 4:  was the whole thing of the iMac was there's a new

291
00:17:08,835 --> 00:17:12,605
Speaker 4:  application, there's a new killer app for desktop computers and

292
00:17:12,605 --> 00:17:15,885
Speaker 4:  we're gonna sell you this blue one with a handle and it's easier to set up

293
00:17:15,885 --> 00:17:18,885
Speaker 4:  than the Windows pc. And that rejuvenated Apple and

294
00:17:19,665 --> 00:17:22,645
Speaker 4:  the, the technology bet was that people wanted to get on the internet and

295
00:17:22,645 --> 00:17:26,605
Speaker 4:  that worked. That was just there, it was, it was happening. The

296
00:17:26,605 --> 00:17:30,565
Speaker 4:  next one was the iPod. And I, it's easy to forget now, but the

297
00:17:30,565 --> 00:17:34,245
Speaker 4:  technology bet there was John Rubenstein went and found a tiny

298
00:17:34,275 --> 00:17:37,995
Speaker 4:  Toshiba hard drive and Phil Schiller invented the click wheel

299
00:17:38,775 --> 00:17:42,515
Speaker 4:  and and they were able to package that together into the iPod. The, but the

300
00:17:42,515 --> 00:17:46,275
Speaker 4:  core technology was Toshiba made this little hard drive and they don't know

301
00:17:46,275 --> 00:17:50,205
Speaker 4:  what to do with it. And you can like go read Stephen Levy's book,

302
00:17:50,235 --> 00:17:53,685
Speaker 4:  it's called the, the, the Perfect Thing, is that what it's called? But it's

303
00:17:53,685 --> 00:17:55,565
Speaker 4:  literally like, what are we gonna do with this hard drive? And the music

304
00:17:55,565 --> 00:17:59,535
Speaker 4:  player, the iPhone was like, we invented Multitouch and that is a

305
00:17:59,535 --> 00:18:02,695
Speaker 4:  thing. We're gonna make a whole phone outta this and you just keep going

306
00:18:02,695 --> 00:18:06,335
Speaker 4:  on up. There's these like big core technologies that I was able to

307
00:18:07,045 --> 00:18:11,015
Speaker 4:  package into new kinds of products with a visionary who was like, I

308
00:18:11,015 --> 00:18:14,565
Speaker 4:  know what kinds of technologies will lead to products

309
00:18:15,035 --> 00:18:18,845
Speaker 4:  like the iPhone for example. They were doing the iPad first and Steve Jobs

310
00:18:18,845 --> 00:18:22,205
Speaker 4:  was like, no make phone then we'll do come back into the iPad. Right? Like

311
00:18:22,265 --> 00:18:25,005
Speaker 4:  that's the dynamic between the two characters here.

312
00:18:26,585 --> 00:18:30,325
Speaker 4:  AI isn't that product yet. Like this is like, I made an

313
00:18:30,515 --> 00:18:33,045
Speaker 4:  iPod and sometimes the hard drive just lies to you,

314
00:18:34,635 --> 00:18:38,385
Speaker 4:  right? Like that would be weird. I may, I invented the iPhone, but the

315
00:18:38,385 --> 00:18:42,175
Speaker 4:  screen sometimes tries to bang Kevin Rus is

316
00:18:42,175 --> 00:18:45,975
Speaker 4:  like that. That is the, the level of capability that the core

317
00:18:45,975 --> 00:18:49,175
Speaker 4:  technology they're trying to build these products has. And I,

318
00:18:49,895 --> 00:18:53,375
Speaker 4:  I, I'm just, I find myself just like utterly full of skepticism

319
00:18:54,315 --> 00:18:58,285
Speaker 4:  that open ai, open AI as it's presently configured, having lost

320
00:18:58,325 --> 00:19:00,845
Speaker 4:  a bunch of its research people and its product people

321
00:19:02,175 --> 00:19:05,595
Speaker 4:  can, can turn the corner. I don't I'm I

322
00:19:06,215 --> 00:19:10,195
Speaker 4:  the it like maybe I'm just like very sleepy, you know, and

323
00:19:10,195 --> 00:19:13,395
Speaker 4:  more cranky than usual. But I'm just like all of the hit products

324
00:19:13,975 --> 00:19:17,115
Speaker 4:  had a core technology innovation that definitely worked

325
00:19:18,055 --> 00:19:20,925
Speaker 4:  right? And people were like really skeptical of some of them. People were

326
00:19:20,925 --> 00:19:24,325
Speaker 4:  really, really skeptical of typing on a Multitouch keyboard

327
00:19:25,215 --> 00:19:28,075
Speaker 4:  and the thing they were able to do was design their way through that and

328
00:19:28,075 --> 00:19:28,675
Speaker 4:  make it good.

329
00:19:29,275 --> 00:19:32,675
Speaker 5:  I mean I think that's a, a lot of credit to give to the first generation

330
00:19:32,675 --> 00:19:35,835
Speaker 5:  of all of those technologies, which in many, like the first iPhone was not

331
00:19:36,145 --> 00:19:40,035
Speaker 5:  good. It was like fascinating and cool, but it was like it was bad

332
00:19:40,035 --> 00:19:40,835
Speaker 5:  at a lot. But

333
00:19:40,835 --> 00:19:41,515
Speaker 4:  Multitouch worked.

334
00:19:41,845 --> 00:19:45,595
Speaker 5:  Multitouch worked. I think so. Okay. Here's, here's what's wild to me and

335
00:19:45,595 --> 00:19:49,395
Speaker 5:  I think this, this is gonna be a theme of today's episode, is I think

336
00:19:49,465 --> 00:19:52,955
Speaker 5:  your assessment of how good AI is is correct

337
00:19:53,455 --> 00:19:57,275
Speaker 5:  and I think most people don't care. I think most people think AI is very

338
00:19:57,275 --> 00:20:00,795
Speaker 5:  good. Yeah. And I, I increasingly don't know what to

339
00:20:01,235 --> 00:20:05,155
Speaker 5:  do with that. That like people, the the like approval rating

340
00:20:05,155 --> 00:20:08,955
Speaker 5:  of chat GPT is actually very, very, very, very high. Yeah.

341
00:20:08,955 --> 00:20:12,885
Speaker 5:  Whether or not it's any good or lies to you or not

342
00:20:13,065 --> 00:20:17,005
Speaker 5:  is is relevant but also in a weird way sort of beside the point at

343
00:20:17,005 --> 00:20:20,765
Speaker 5:  this point, like people are so enamored with this stuff

344
00:20:20,875 --> 00:20:23,925
Speaker 5:  that they're willing to look past all that stuff. And so maybe it doesn't

345
00:20:23,945 --> 00:20:27,205
Speaker 5:  matter and I'm like this, this is where I'm so torn on all of this that like

346
00:20:27,745 --> 00:20:31,645
Speaker 5:  the big question about all of these AI devices, right? Is like a do

347
00:20:31,645 --> 00:20:35,565
Speaker 5:  people want another device that they have to buy? I think the answer

348
00:20:35,565 --> 00:20:39,405
Speaker 5:  to that is yes. Like everybody always wants know their device to buy.

349
00:20:39,515 --> 00:20:40,725
Speaker 5:  Yeah. Like that's fine.

350
00:20:40,725 --> 00:20:42,805
Speaker 4:  Yeah. Point and shoot cameras have made a comeback. David, I think we can

351
00:20:43,065 --> 00:20:45,965
Speaker 5:  Can, but people got onboard with, with smart watches when there was a reason

352
00:20:45,965 --> 00:20:48,725
Speaker 5:  for smart watches, people got on board with wireless headphones that are

353
00:20:48,725 --> 00:20:52,205
Speaker 5:  really expensive. Like people will keep buying new gadgets if there's a reason

354
00:20:52,205 --> 00:20:55,805
Speaker 5:  for them. The question is, is there an AI specific thing to do?

355
00:20:56,225 --> 00:21:00,125
Speaker 5:  And I think to me that is both a, like is the tech any good question

356
00:21:00,345 --> 00:21:04,205
Speaker 5:  and is there a UI for this? That makes sense. Question. The UI for this that

357
00:21:04,205 --> 00:21:08,045
Speaker 5:  makes sense I think is still a wildly open question. But like is

358
00:21:08,045 --> 00:21:11,765
Speaker 5:  the tech good enough for people to want to use it all day every day? I think

359
00:21:11,765 --> 00:21:14,805
Speaker 5:  the answer's yes. Whether it should be or not is a different question, but

360
00:21:14,805 --> 00:21:15,285
Speaker 5:  I think It is

361
00:21:15,625 --> 00:21:19,325
Speaker 6:  The UI is voice and I, I think back to Eli's point, like there was a key

362
00:21:19,635 --> 00:21:23,405
Speaker 6:  tech bet of how people wanted to use it with each of the things

363
00:21:23,645 --> 00:21:27,565
Speaker 6:  that Johnny did with with jobs. I think here people do want to

364
00:21:27,565 --> 00:21:31,405
Speaker 6:  talk to chat GPT, they want to do advanced voice mode and they

365
00:21:31,405 --> 00:21:34,365
Speaker 6:  don't want to have to configure the action button on the iPhone and wait

366
00:21:34,365 --> 00:21:38,005
Speaker 6:  three seconds for it to load up and connect to the internet to actually work.

367
00:21:38,085 --> 00:21:41,565
Speaker 6:  I think if you had a always on listening

368
00:21:41,715 --> 00:21:45,205
Speaker 6:  ambient aware thing that you could have a conversation with,

369
00:21:45,925 --> 00:21:48,325
Speaker 6:  I think people would do that. I mean, my wife told me yesterday, she was

370
00:21:48,325 --> 00:21:52,125
Speaker 6:  like, I just said thank you back to CHATT and it made me feel very

371
00:21:52,125 --> 00:21:56,045
Speaker 6:  strange like unprompted, you know, like yep. People are building real,

372
00:21:56,285 --> 00:22:00,165
Speaker 6:  I mean we were hearing this at io like afterwards like people are having

373
00:22:01,195 --> 00:22:04,085
Speaker 6:  real almost relationships with these AI

374
00:22:05,505 --> 00:22:08,765
Speaker 6:  and yeah, if you introduce something that you can just talk to all day long,

375
00:22:08,875 --> 00:22:12,725
Speaker 6:  it's, it's the her concept, which Sam Altman is obsessed with the Spike

376
00:22:12,725 --> 00:22:16,565
Speaker 6:  Jones film. And you know what I've heard about this product and the form

377
00:22:16,565 --> 00:22:20,285
Speaker 6:  factor is, is a little mushy is basically the

378
00:22:20,285 --> 00:22:23,645
Speaker 6:  ideas that bike demo we saw at IO this week where

379
00:22:24,285 --> 00:22:27,725
Speaker 6:  a guy's trying to fix his bike and he's talking to the AI throughout it and

380
00:22:27,725 --> 00:22:31,485
Speaker 6:  it knows what he's looking at and it's guiding him through it. And

381
00:22:31,865 --> 00:22:35,365
Speaker 6:  if it does work, if it's not hallucinating, that is just fundamentally a

382
00:22:35,365 --> 00:22:39,245
Speaker 6:  more human way to use technology. It's just like talking to it

383
00:22:39,465 --> 00:22:40,645
Speaker 6:  and it's helping you.

384
00:22:42,195 --> 00:22:45,925
Speaker 6:  Yeah. Does it lie? Does it get things wrong? Yes.

385
00:22:47,355 --> 00:22:50,405
Speaker 6:  Does it matter and will it be good enough maybe?

386
00:22:51,095 --> 00:22:54,985
Speaker 4:  Yeah. It's very funny how many tech demos at every stage of

387
00:22:54,985 --> 00:22:58,905
Speaker 4:  technology involve trying to fix something. Like my

388
00:22:58,905 --> 00:23:02,305
Speaker 4:  first ever Microsoft HoloLens demo, they had me change a spark plug in an

389
00:23:02,305 --> 00:23:05,305
Speaker 4:  A TV. Yeah. And they're like weird. This is the future of changing these

390
00:23:05,305 --> 00:23:08,585
Speaker 4:  spark plugs and like we're just doing it again. And lots of people have change,

391
00:23:08,585 --> 00:23:11,665
Speaker 4:  lots of spark plugs without like data centers in overdrive. Yeah.

392
00:23:11,665 --> 00:23:15,465
Speaker 5:  There's only like three ideas, right? It's like every new technology is

393
00:23:15,725 --> 00:23:19,185
Speaker 5:  is for playing Assassin's Creed, fixing your car more

394
00:23:19,185 --> 00:23:22,385
Speaker 5:  successfully and buying elaborate vacation

395
00:23:22,835 --> 00:23:24,905
Speaker 5:  plans. Like that's it in Spanish. That's all hundred percent

396
00:23:24,905 --> 00:23:28,305
Speaker 4:  Percent in Spanish. Yeah. Yeah. Those are the three. Which one is it? You

397
00:23:28,305 --> 00:23:32,105
Speaker 4:  just tell me which one. It's right. I get all that. I, I you know, my

398
00:23:32,105 --> 00:23:35,825
Speaker 4:  skepticism about AI is not do people like it? I've, I've really come around

399
00:23:35,925 --> 00:23:39,825
Speaker 4:  to there is a widening gulf between how people talk about

400
00:23:39,825 --> 00:23:43,505
Speaker 4:  how much they hate AI online and then people using it in

401
00:23:43,735 --> 00:23:47,645
Speaker 4:  real life and how much they use it. How much they like it, it,

402
00:23:47,825 --> 00:23:51,565
Speaker 4:  you can sense it, it's there is a turn Sundar io on

403
00:23:51,565 --> 00:23:55,045
Speaker 4:  stage said we're entering a new phase of the platform shift.

404
00:23:55,425 --> 00:23:59,085
Speaker 4:  And what he meant was the products are coming out now, right? That we're

405
00:23:59,085 --> 00:24:01,365
Speaker 4:  not just talking about model scores. Like here's a bunch of new products.

406
00:24:02,165 --> 00:24:05,565
Speaker 4:  I think chatt PT is that product for OpenAI, but it does it chat GB as It

407
00:24:05,565 --> 00:24:08,965
Speaker 4:  is currently configured will never return on the massive amount of investment

408
00:24:08,965 --> 00:24:12,885
Speaker 4:  that OpenAI has taken. Correct. They need to be Apple, they need the iPhone,

409
00:24:12,885 --> 00:24:16,165
Speaker 4:  they need the app economy of the iPhone. They need to be the richest company

410
00:24:16,165 --> 00:24:19,365
Speaker 4:  in the world. But this literally that is the only target that makes sense

411
00:24:19,385 --> 00:24:23,205
Speaker 4:  for their level of investment. That's a big problem. And I'm just,

412
00:24:23,615 --> 00:24:27,505
Speaker 4:  again, I'm just coming back to does the core technology let them

413
00:24:27,565 --> 00:24:31,265
Speaker 4:  do the thing they wanna do or is it people just like it so much that they

414
00:24:31,265 --> 00:24:33,905
Speaker 4:  will skip over the fact that it's brittle And

415
00:24:35,245 --> 00:24:39,215
Speaker 4:  what I've has traditionally been successful at is designing

416
00:24:39,215 --> 00:24:43,065
Speaker 4:  to the limitation, right? He makes, he makes the

417
00:24:43,065 --> 00:24:46,945
Speaker 4:  limitation the the most successful part of the product. It's a little hard

418
00:24:46,945 --> 00:24:50,185
Speaker 4:  drive. The whole thing looks like a little hard drive. Yeah. And we're gonna

419
00:24:50,185 --> 00:24:53,945
Speaker 4:  talk about how many songs it has. Remember unapologetically plastic,

420
00:24:54,015 --> 00:24:57,485
Speaker 4:  like these are his moves, right? Like he says, I

421
00:24:58,085 --> 00:25:01,525
Speaker 4:  I made it feel inevitable. And the thing that's inevitable is the limitation

422
00:25:01,545 --> 00:25:05,245
Speaker 4:  of the technology is the product and you don't feel the

423
00:25:05,245 --> 00:25:09,045
Speaker 4:  limitation 'cause it's designed to make that the, the thing that makes the

424
00:25:09,045 --> 00:25:12,445
Speaker 4:  product good. And you might think that I'm just like a, a sleepy man who's

425
00:25:12,605 --> 00:25:16,045
Speaker 4:  rambling, but think about the first iMac and how it was translucent. So you

426
00:25:16,045 --> 00:25:19,825
Speaker 4:  could see that CRT display that was the limitation of the product.

427
00:25:20,055 --> 00:25:23,665
Speaker 4:  They, they had to build it around this giant vacuum

428
00:25:24,015 --> 00:25:27,945
Speaker 4:  display and he made it the centerpiece of the product so

429
00:25:27,945 --> 00:25:30,665
Speaker 4:  that over the lifetime of the product they made that plastic clearer and

430
00:25:30,665 --> 00:25:34,465
Speaker 4:  clearer. That was the investment in the materials was to do the appropriate

431
00:25:34,585 --> 00:25:38,105
Speaker 4:  EMF shielding in the plastic itself to be transparent instead of just translucent.

432
00:25:38,485 --> 00:25:42,465
Speaker 4:  And so that the CRT became the center of this, that that's, ive like at the

433
00:25:42,465 --> 00:25:45,785
Speaker 4:  highest level that is Johnny Ive, we're gonna make the antenna the band of

434
00:25:45,785 --> 00:25:49,625
Speaker 4:  the iPhone four. So you can't, if you hold it wrong, the signal drops. That's

435
00:25:49,625 --> 00:25:53,555
Speaker 4:  Johnny Ives stuff. Right. How do you design to the limitation of AI

436
00:25:53,815 --> 00:25:55,475
Speaker 4:  is it currently exists. I, I think the

437
00:25:55,635 --> 00:25:59,515
Speaker 5:  Question of how much credit for that kind of stuff Ive gets is going to be

438
00:25:59,515 --> 00:26:02,035
Speaker 5:  one of the pieces of this that is really interesting because if you look

439
00:26:02,035 --> 00:26:05,155
Speaker 5:  at the Johnny Ive era at Apple after Steve Jobs,

440
00:26:06,175 --> 00:26:09,585
Speaker 5:  it's a lot less of what you just described. Yep. And It is a lot more of

441
00:26:09,585 --> 00:26:13,225
Speaker 5:  Johnny Ive pursuing things that are aesthetically lovely for the sake of

442
00:26:13,225 --> 00:26:17,145
Speaker 5:  making things that are aesthetically lovely and that's, that's fine. That's

443
00:26:17,145 --> 00:26:20,345
Speaker 5:  a perfectly good approach to doing a lot of things. That's also how you get

444
00:26:20,345 --> 00:26:24,025
Speaker 5:  the butterfly keyboard and it's how you get like laptops that literally

445
00:26:24,215 --> 00:26:27,785
Speaker 5:  just stopped working for people for a gen. Like they just made a series of

446
00:26:27,785 --> 00:26:30,425
Speaker 5:  bad decisions about laptops in order to make pretty laptops.

447
00:26:31,055 --> 00:26:33,465
Speaker 4:  It's how you get a mouse that you have to charge upside

448
00:26:33,465 --> 00:26:35,985
Speaker 5:  Down. Right. That's just the thing that happened. And I think about even

449
00:26:36,175 --> 00:26:39,665
Speaker 5:  like the, the Apple watch, which was a true, like Johnny

450
00:26:39,895 --> 00:26:43,805
Speaker 5:  I've led project Johnny I've decided to build a

451
00:26:43,805 --> 00:26:47,125
Speaker 5:  watch and then worked backwards towards the Apple Watch and they landed in

452
00:26:47,125 --> 00:26:50,245
Speaker 5:  the wrong place. They, they landed in a place that I think is really fascinating

453
00:26:50,245 --> 00:26:54,125
Speaker 5:  but it, it took like two pivots from the first idea before Apple

454
00:26:54,185 --> 00:26:57,005
Speaker 5:  had a hit product. And So I think this question of like

455
00:26:57,965 --> 00:27:01,605
Speaker 5:  Johnny's capability to make beautiful things

456
00:27:01,825 --> 00:27:02,765
Speaker 5:  is like beyond question.

457
00:27:03,035 --> 00:27:06,005
Speaker 6:  Well we'll see he's doing the electric Ferrari. We'll see how good that looks.

458
00:27:06,925 --> 00:27:10,645
Speaker 5:  I mean nobody's perfect every time. Right. But I think like in terms of

459
00:27:11,485 --> 00:27:14,925
Speaker 5:  I would bet, I would bet a lot of money on whatever they make

460
00:27:15,595 --> 00:27:17,405
Speaker 5:  looking and feeling great.

461
00:27:17,875 --> 00:27:21,525
Speaker 6:  Yeah. I think that's the bet that has to be the, I mean look, so Sam Altman

462
00:27:21,705 --> 00:27:24,765
Speaker 6:  had this meeting inside OpenAI that was reported on by the Wall Street Journal

463
00:27:25,385 --> 00:27:28,845
Speaker 6:  and he told them that this 6.5 billion acquisition

464
00:27:29,425 --> 00:27:33,405
Speaker 6:  has the potential to add 1 trillion in value to OpenAI. Sure.

465
00:27:33,405 --> 00:27:35,725
Speaker 6:  Which is just like, you know, making up numbers, you can

466
00:27:35,725 --> 00:27:36,205
Speaker 5:  Just say things,

467
00:27:36,465 --> 00:27:40,045
Speaker 6:  But like this is where he started to talk a little bit about those

468
00:27:40,045 --> 00:27:43,965
Speaker 6:  limitations neli that you're talking about. He said the device isn't going

469
00:27:43,965 --> 00:27:47,245
Speaker 6:  to be a pair of glasses and that it's something

470
00:27:48,065 --> 00:27:51,765
Speaker 6:  to wear. Although it sounds like they're not gonna want to call it a

471
00:27:51,765 --> 00:27:55,735
Speaker 6:  wearable. And then, you know, he's saying we're

472
00:27:55,735 --> 00:27:59,455
Speaker 6:  not gonna ship a hundred million devices literally on day one. Duh.

473
00:27:59,675 --> 00:28:03,415
Speaker 6:  But he does think they'll get there faster than any company ever.

474
00:28:04,515 --> 00:28:07,735
Speaker 6:  And you know, Alman is just, this is, he's great at hype, right? Yeah. Like

475
00:28:07,735 --> 00:28:10,695
Speaker 6:  this is obviously he's hyping this up to employees who are like, why did

476
00:28:10,695 --> 00:28:14,135
Speaker 6:  we just give Johnny Ive $6 billion and he's not joining us.

477
00:28:15,755 --> 00:28:19,655
Speaker 6:  But he did. So this is from the meeting he said that the product will

478
00:28:19,655 --> 00:28:23,215
Speaker 6:  be capable of being fully aware of a user's surroundings and life will be

479
00:28:23,285 --> 00:28:26,895
Speaker 6:  unobtrusive able to rest in one's pocket or on one's desk

480
00:28:27,395 --> 00:28:31,295
Speaker 6:  and will be a third core device a person would have after

481
00:28:31,415 --> 00:28:35,375
Speaker 6:  a MacBook Pro and an iPhone. So here we go. This is like a better

482
00:28:35,595 --> 00:28:36,295
Speaker 6:  humane pin.

483
00:28:37,175 --> 00:28:40,975
Speaker 4:  I mean not for nothing. The Humane pin was designed by two

484
00:28:40,975 --> 00:28:43,655
Speaker 4:  former Apple designers who regretted their work on the iPhone,

485
00:28:43,655 --> 00:28:46,815
Speaker 5:  Many of whom were called legendary when they started this company. Imran

486
00:28:46,815 --> 00:28:49,495
Speaker 5:  Chowdry as legendary as it comes coming out of Apple. Did

487
00:28:49,495 --> 00:28:52,415
Speaker 6:  You get Altman gave them millions of dollars in funding. He

488
00:28:52,415 --> 00:28:56,295
Speaker 5:  Was the single biggest investor in the company. Like it's just, we're doing

489
00:28:56,315 --> 00:28:58,095
Speaker 5:  Humane again. But now it's Johnny I've

490
00:28:58,975 --> 00:29:02,815
Speaker 4:  I did say that Humane Pin was a poor product which tracks with

491
00:29:02,815 --> 00:29:04,055
Speaker 4:  your review, David. Yes.

492
00:29:04,195 --> 00:29:07,775
Speaker 5:  And the Truth. That's right. Good call by Johnny.

493
00:29:08,575 --> 00:29:11,535
Speaker 4:  I would say that I called that watching the first demo video the Humane pin

494
00:29:12,195 --> 00:29:14,935
Speaker 4:  and maybe I've, maybe I've lost some friends forever because of that.

495
00:29:16,695 --> 00:29:20,415
Speaker 4:  Yeah. I've also called the Rabbit a poor product, which I think Jesse, Lou

496
00:29:20,415 --> 00:29:23,415
Speaker 4:  and Rabbit loved because Johnny I knows who they are

497
00:29:24,495 --> 00:29:27,335
Speaker 4:  and they released a a long statement being like, we love that the pie is

498
00:29:27,335 --> 00:29:28,735
Speaker 4:  growing and I Good

499
00:29:28,735 --> 00:29:32,095
Speaker 6:  Luck guys. Let me read this. Jesse sent us a statement, it's actually amazing.

500
00:29:32,155 --> 00:29:35,695
Speaker 6:  He says it's an honor to get mentioned by Johnny Ivan, Sam Altman, however

501
00:29:35,795 --> 00:29:39,535
Speaker 6:  we don't like to be put side by side with Humane. A company that stopped

502
00:29:39,535 --> 00:29:42,085
Speaker 6:  trying got acquired and shut down. That's

503
00:29:42,085 --> 00:29:43,645
Speaker 4:  Very good. That's brutal.

504
00:29:43,905 --> 00:29:46,685
Speaker 5:  You can call us shit, but don't compare us to Humane.

505
00:29:47,745 --> 00:29:48,445
Speaker 4:  That's very good.

506
00:29:48,945 --> 00:29:49,645
Speaker 6:  That's very good Jesse.

507
00:29:49,645 --> 00:29:50,285
Speaker 4:  That's really good.

508
00:29:51,515 --> 00:29:51,805
Speaker 6:  Yeah,

509
00:29:52,605 --> 00:29:55,945
Speaker 4:  You can tell that he wrote that not not the robot. You know, like you can

510
00:29:55,945 --> 00:29:59,625
Speaker 4:  tell when it's like that's human. Like don't compare me to Humane

511
00:30:01,925 --> 00:30:05,225
Speaker 4:  but that those are the products and there's a million of them. David, you

512
00:30:05,345 --> 00:30:08,785
Speaker 4:  reviewed a bunch of like note taking products, right? Like I'm gonna have

513
00:30:08,785 --> 00:30:11,825
Speaker 4:  a microphone that's talking like recording all the time. Yeah. Last time

514
00:30:11,825 --> 00:30:15,105
Speaker 4:  I saw Joanna Stern, she was wearing a wristband that records everything and

515
00:30:15,135 --> 00:30:17,535
Speaker 4:  the lights were backwards. Yeah. It was red when it was off.

516
00:30:17,805 --> 00:30:21,535
Speaker 5:  It's called the B vs. Reviewed it for us. It's, it's, it's a delightful and

517
00:30:21,695 --> 00:30:22,175
Speaker 5:  terrible product,

518
00:30:22,505 --> 00:30:25,575
Speaker 4:  Right? I mean there's just like a million of these devices that's like what

519
00:30:25,575 --> 00:30:29,095
Speaker 4:  if we just feed input into an AI and then it does

520
00:30:29,095 --> 00:30:32,855
Speaker 4:  something. Yeah. Right? It's summarized or talk to you or whatever. And that's

521
00:30:32,855 --> 00:30:36,695
Speaker 4:  what I mean by the limitation. Like the limitation of those products is very,

522
00:30:36,725 --> 00:30:40,495
Speaker 4:  very obvious. Right? The ais can't, they can't take actions for you. Right?

523
00:30:40,495 --> 00:30:43,535
Speaker 4:  The agents don't work yet. Maybe they will. We'll talk about that with io.

524
00:30:43,535 --> 00:30:47,055
Speaker 4:  Like there's a lot of activity around how to make the agents work and

525
00:30:47,435 --> 00:30:51,295
Speaker 4:  go do stuff for you when you ask them to do it. The ai right? They don't

526
00:30:51,455 --> 00:30:54,335
Speaker 4:  actually know a bunch of stuff that the search features don't work as well

527
00:30:54,335 --> 00:30:57,455
Speaker 4:  as they should. Like there's just a bunch of stuff where the limitation of

528
00:30:57,455 --> 00:31:01,335
Speaker 4:  the product is so obvious because the interface is

529
00:31:01,355 --> 00:31:04,655
Speaker 4:  so good. Yeah, I'm just gonna talk to this thing and it's gonna talk to me.

530
00:31:05,275 --> 00:31:09,175
Speaker 4:  And then everything beyond that is like house of cards and I, I just,

531
00:31:09,945 --> 00:31:12,515
Speaker 4:  yeah, okay. Maybe we can make a more beautiful humane pin and we won't call

532
00:31:12,515 --> 00:31:16,275
Speaker 4:  it a pin. Maybe we can make earbuds that talk to you like her, which

533
00:31:16,635 --> 00:31:19,995
Speaker 4:  Altman really has been obsessed with for a long time. What's it gonna do?

534
00:31:20,385 --> 00:31:22,595
Speaker 4:  Yeah. That makes it valuable. Yeah,

535
00:31:22,835 --> 00:31:26,315
Speaker 6:  I do, I do come back to that Google bike demo. I think the idea is just always

536
00:31:26,415 --> 00:31:30,235
Speaker 6:  on ai. That can be your girlfriend, boyfriend or your

537
00:31:30,235 --> 00:31:33,915
Speaker 6:  assistant on the device itself. Mxi quo who's a pretty

538
00:31:33,915 --> 00:31:36,395
Speaker 6:  reputable analyst, he, he covers Apple.

539
00:31:38,025 --> 00:31:41,635
Speaker 6:  He's saying that mass production for this is expected to start in 2027.

540
00:31:42,335 --> 00:31:46,195
Speaker 6:  The current prototype is slightly larger than the AI pin with a form factor

541
00:31:46,255 --> 00:31:49,755
Speaker 6:  as compact and elegant as an iPod shuffle. Okay.

542
00:31:50,935 --> 00:31:54,195
Speaker 6:  And then one of the intended use cases is wearing the device around the neck.

543
00:31:54,645 --> 00:31:58,155
Speaker 6:  It'll have cameras and microphones for environment detection, no display.

544
00:31:58,625 --> 00:32:02,505
Speaker 6:  It's expected to connect to smartphones and PCs. So yeah,

545
00:32:02,505 --> 00:32:05,905
Speaker 6:  it's one of these just always on listening AI devices but made from

546
00:32:06,335 --> 00:32:08,105
Speaker 6:  brushed aluminum. Yeah.

547
00:32:08,565 --> 00:32:12,265
Speaker 5:  Eli, to your point, this is everybody's idea, right? Like this is, this is

548
00:32:12,325 --> 00:32:16,305
Speaker 5:  the thing everyone is, is pushing towards. I have, I have four of them on

549
00:32:16,305 --> 00:32:20,185
Speaker 5:  my desk right now that are like a, a little thing with a microphone in

550
00:32:20,185 --> 00:32:23,625
Speaker 5:  it that you wear on a lanyard. Yeah. That goes around with you. And I think

551
00:32:23,725 --> 00:32:27,715
Speaker 5:  in a lot of ways that is actually probably the correct format for this

552
00:32:27,715 --> 00:32:30,875
Speaker 5:  kind of stuff Right now if the goal is just like lots and lots of AI input,

553
00:32:31,045 --> 00:32:33,835
Speaker 5:  especially if this thing is gonna have a camera, which I think it has to,

554
00:32:33,895 --> 00:32:37,635
Speaker 5:  to come even remotely close to what Sam and

555
00:32:37,655 --> 00:32:40,765
Speaker 5:  Johnny are talking about here. Sure. Like a little thing on a lanyard makes

556
00:32:40,765 --> 00:32:44,245
Speaker 5:  a lot of sense. I will say I went back this morning and watched the, the

557
00:32:44,255 --> 00:32:48,245
Speaker 5:  intro to the original iPod Shuffle, which was from 2005. And

558
00:32:48,295 --> 00:32:51,685
Speaker 5:  Steve Jobs like shows off the thing, does a, a great reveal the whole thing.

559
00:32:51,825 --> 00:32:55,685
Speaker 5:  And then they have this giant set of promotional photos of people

560
00:32:55,685 --> 00:32:59,565
Speaker 5:  wearing it around a lanyard like Apple really wanted to make this the thing

561
00:32:59,565 --> 00:33:02,525
Speaker 5:  that you would wear it. They had the white lanyard. It was like the, the

562
00:33:02,745 --> 00:33:06,485
Speaker 5:  cap for the USB port was also the cap for the lanyard. It looked so

563
00:33:06,485 --> 00:33:10,405
Speaker 5:  stupid and just looking back, I'm like, I don't think I ever once saw a

564
00:33:10,405 --> 00:33:14,005
Speaker 5:  person in the wild wearing an iPod shuffle around their neck. The shuffle

565
00:33:14,005 --> 00:33:17,925
Speaker 5:  became like a pocket device. And the problem

566
00:33:18,195 --> 00:33:22,045
Speaker 5:  with like the humane pin, maybe not above all else, but one real

567
00:33:22,045 --> 00:33:24,925
Speaker 5:  problem with the humane pin was that it, it sat right here. It was a thing

568
00:33:24,925 --> 00:33:27,845
Speaker 5:  you had to look at all the time and it was looking at you all the time. And

569
00:33:28,915 --> 00:33:32,605
Speaker 5:  this is very similar to that. I think you have to do it if you want to have

570
00:33:32,645 --> 00:33:36,605
Speaker 5:  a camera. But that is a, that's just a high bar to clear for any

571
00:33:36,605 --> 00:33:37,725
Speaker 5:  of these kinds of products.

572
00:33:37,915 --> 00:33:41,765
Speaker 4:  Yeah. Also anything that size does not have a battery or a chip in it

573
00:33:41,765 --> 00:33:44,125
Speaker 4:  that can do any of the things that they want it to do.

574
00:33:44,445 --> 00:33:48,325
Speaker 5:  I will say I think if you, there, there are a lot of ways to do this

575
00:33:48,385 --> 00:33:52,285
Speaker 5:  and one of them is just to like offload a ton of that onto your phone

576
00:33:52,665 --> 00:33:56,565
Speaker 5:  and just do all the work over like Bluetooth and local

577
00:33:56,595 --> 00:33:59,885
Speaker 5:  wifi and not worry so much about it. Which is like that's what

578
00:34:00,305 --> 00:34:03,765
Speaker 5:  meta's glasses do. Humane didn't do it and it was a huge mistake

579
00:34:04,225 --> 00:34:07,125
Speaker 5:  and I suspect anyone who is doing this going forward will learn that lesson

580
00:34:07,145 --> 00:34:09,845
Speaker 5:  of like, this thing actually isn't ready to be a fully self-contained device.

581
00:34:10,585 --> 00:34:14,365
Speaker 5:  But I think if you just take it as like a pure input system

582
00:34:14,385 --> 00:34:15,925
Speaker 5:  and do all of the work on your phone,

583
00:34:17,805 --> 00:34:20,805
Speaker 5:  I can sort of see how you'd get a battery that could last.

584
00:34:21,185 --> 00:34:24,165
Speaker 4:  So you're selling 100 million Johnny, ive designed

585
00:34:24,595 --> 00:34:28,525
Speaker 4:  microphones and cameras that can, that require an iPhone to use. You know

586
00:34:28,525 --> 00:34:28,645
Speaker 4:  what

587
00:34:28,645 --> 00:34:31,925
Speaker 5:  It makes me think of? Do you remember the Google Clips camera? Yeah. It's

588
00:34:31,925 --> 00:34:33,045
Speaker 5:  gonna be that with a microphone.

589
00:34:33,485 --> 00:34:36,685
Speaker 4:  I mean there's, you know, there's a couple of players in there who have a

590
00:34:36,685 --> 00:34:40,445
Speaker 4:  vested interest in not letting Johnny I supplant their products.

591
00:34:40,645 --> 00:34:43,765
Speaker 4:  Yeah, right. One of 'em is Apple. They're, they're stumbling around lately

592
00:34:43,905 --> 00:34:47,765
Speaker 4:  but I think after this video in which Johnny I have referred to his own products

593
00:34:47,785 --> 00:34:51,645
Speaker 4:  as legacy devices that are 10 years old and should be replaced are not

594
00:34:51,645 --> 00:34:55,485
Speaker 4:  going to show up and be like, yes you can have preferred Bluetooth access

595
00:34:55,585 --> 00:34:59,565
Speaker 4:  to it. You're always on chat GBT, like good luck. No, no watchmaker in

596
00:34:59,565 --> 00:35:03,045
Speaker 4:  history has gotten that access to iOS and maybe,

597
00:35:03,625 --> 00:35:06,445
Speaker 4:  you know, maybe governments around the world will shatter Apple into a thousand

598
00:35:06,445 --> 00:35:10,165
Speaker 4:  pieces and let this happen. But you, you've now made a $1 trillion bet on

599
00:35:10,165 --> 00:35:10,845
Speaker 4:  that. So good luck.

600
00:35:11,155 --> 00:35:14,725
Speaker 5:  Well the deep irony of all of this is that is the exact thing that

601
00:35:15,025 --> 00:35:18,085
Speaker 5:  OpenAI and all these other companies are desperate to get around. They're

602
00:35:18,085 --> 00:35:21,125
Speaker 5:  like, we have to build the next platform so that not everything is, you know,

603
00:35:21,125 --> 00:35:24,205
Speaker 5:  intermediated by your phone and these two operating system makers

604
00:35:24,985 --> 00:35:28,925
Speaker 5:  and yet they all are going to have to be completely intermediated by

605
00:35:28,925 --> 00:35:31,085
Speaker 5:  your phone, otherwise the hardware's gonna be bad.

606
00:35:31,085 --> 00:35:34,485
Speaker 4:  Right. And then the other one is Android, which is more open and would allow

607
00:35:34,485 --> 00:35:38,445
Speaker 4:  many, many more of these things to happen. But Google is as we'll

608
00:35:38,445 --> 00:35:42,005
Speaker 4:  talk about in the next segment, a pretty ferocious competitor to

609
00:35:42,065 --> 00:35:43,845
Speaker 4:  OpenAI when it comes to AI products.

610
00:35:45,665 --> 00:35:48,845
Speaker 4:  And then your market in Lucy the United States is people with Android phones.

611
00:35:50,195 --> 00:35:53,325
Speaker 4:  Good luck, Johnny. Like I, there's just a problem there like to solve like

612
00:35:53,385 --> 00:35:57,165
Speaker 4:  either all the compute is in the device, which is what Meta is pushing towards.

613
00:35:57,905 --> 00:36:01,005
Speaker 4:  And I mean we've heard Zuck, I think Zuck has said it to you, Alex, like

614
00:36:01,275 --> 00:36:04,805
Speaker 4:  he's furious that Apple won't give him access to the Bluetooth stack on the

615
00:36:04,805 --> 00:36:08,565
Speaker 4:  iPhone. Yeah. And the glasses are worse on the iPhone than on Android. I

616
00:36:08,725 --> 00:36:11,925
Speaker 4:  I don't think Altman's gonna get that deal. Like the, I

617
00:36:12,375 --> 00:36:15,605
Speaker 4:  again, I'm just like, what are the limitations? Johnny has great success

618
00:36:15,625 --> 00:36:18,765
Speaker 4:  and maybe he did a better job at it when Steve Jobs was there than after.

619
00:36:19,465 --> 00:36:23,045
Speaker 4:  But his great success is designing to the limitations. Making the limitations

620
00:36:23,045 --> 00:36:26,965
Speaker 4:  feel like they're not there because they're so apparent in, in having

621
00:36:26,965 --> 00:36:30,235
Speaker 4:  covered Apple for all of that time, right? In like

622
00:36:30,775 --> 00:36:34,595
Speaker 4:  gadget blog days where we would obsess, we would write thousands of words

623
00:36:34,595 --> 00:36:38,435
Speaker 4:  about screen radius on on LCD screens like we did it

624
00:36:38,895 --> 00:36:42,595
Speaker 4:  and we, I I we are immersed in the culture that that

625
00:36:42,595 --> 00:36:45,315
Speaker 4:  developed but it's now the design culture everywhere. Part of the problem

626
00:36:45,335 --> 00:36:49,315
Speaker 4:  for Ive like in a big way is every product company has

627
00:36:49,435 --> 00:36:52,825
Speaker 4:  a design culture that is his design culture. Right? Like

628
00:36:53,005 --> 00:36:53,705
Speaker 6:  Why is that a problem?

629
00:36:54,545 --> 00:36:57,465
Speaker 4:  'cause he has to do something new. He has to stand out. Like his moves are,

630
00:36:57,805 --> 00:36:58,145
Speaker 4:  you know,

631
00:36:59,735 --> 00:37:02,305
Speaker 4:  when I was like learning to play the guitar, my guitar teacher was trying

632
00:37:02,305 --> 00:37:05,385
Speaker 4:  to teach me how to play Guns and Roses and Van Halen and I was like, this

633
00:37:05,385 --> 00:37:07,985
Speaker 4:  stuff is whatever, it's a garbage. It is like, it's so cliche. And he was

634
00:37:07,985 --> 00:37:11,025
Speaker 4:  like, that's 'cause they invented it and now everything is this. Right? Yeah.

635
00:37:11,185 --> 00:37:14,625
Speaker 4:  That's Johnny I, Johnny I is Eddie Van Halen. But you listen to like Van

636
00:37:14,625 --> 00:37:17,545
Speaker 4:  Halen now you're like, this is going, this is some campy stuff And at the

637
00:37:17,545 --> 00:37:21,355
Speaker 4:  time it was like a revolution and it's because it's everything. Yeah.

638
00:37:21,425 --> 00:37:25,115
Speaker 4:  Like it's the foundation for everything And like he's got, he's gotta reinvent

639
00:37:25,115 --> 00:37:27,195
Speaker 4:  some moves here and I just, I'm just, I'm focused on

640
00:37:28,925 --> 00:37:32,475
Speaker 4:  there are some real limitations to these products in making the limitations,

641
00:37:32,495 --> 00:37:35,355
Speaker 4:  the strengths. Maybe he'll do it. I'm not a great designer. Right. He's the

642
00:37:35,355 --> 00:37:38,395
Speaker 4:  legendary designer. But what's, we've seen a lot of shots.

643
00:37:39,215 --> 00:37:42,665
Speaker 6:  Yeah. I am going to remain cautiously

644
00:37:42,995 --> 00:37:46,865
Speaker 6:  optimistic about this just because of I the fact that

645
00:37:46,945 --> 00:37:50,785
Speaker 6:  I do think a lot of people would talk to Chad GPT in

646
00:37:50,825 --> 00:37:54,385
Speaker 6:  a device like this if it worked, if it was a good looking version of the

647
00:37:54,385 --> 00:37:58,065
Speaker 6:  humane pin that actually worked, that didn't take five minutes to

648
00:37:58,095 --> 00:38:01,665
Speaker 6:  load, that didn't also like blow up after five minutes 'cause the battery

649
00:38:01,665 --> 00:38:04,785
Speaker 6:  doesn't work. Like if all of that is fixed, which has is a huge if

650
00:38:06,245 --> 00:38:10,185
Speaker 6:  and you get access to you know, the leading chat PT models and it syncs

651
00:38:10,185 --> 00:38:13,345
Speaker 6:  with your chat chat PT account that by the way already knows everything about

652
00:38:13,345 --> 00:38:16,185
Speaker 6:  you 'cause you are saying thank you to it even when you don't need to.

653
00:38:17,725 --> 00:38:20,785
Speaker 6:  That's interesting. And you know, one of the things that Altman said in this

654
00:38:20,785 --> 00:38:24,625
Speaker 6:  meeting that really stuck out to me was he said, we both got excited about

655
00:38:24,625 --> 00:38:28,385
Speaker 6:  the idea that if you subscribe to chat PT we should just mail you

656
00:38:28,405 --> 00:38:32,345
Speaker 6:  new computers. Sure. And he actually said something else at

657
00:38:32,425 --> 00:38:36,045
Speaker 6:  a VC conference I caught a couple weeks ago where he said the goal

658
00:38:36,625 --> 00:38:40,005
Speaker 6:  for open AI is to now be the core AI subscription for your life.

659
00:38:40,785 --> 00:38:44,325
Speaker 6:  So my prediction, and this is an informed prediction, is that

660
00:38:45,035 --> 00:38:48,965
Speaker 6:  chat p subscribers will just get these devices or they'll get them

661
00:38:48,965 --> 00:38:52,565
Speaker 6:  heavily subsidized and he's gonna build a subscription model

662
00:38:52,665 --> 00:38:56,205
Speaker 6:  around not just software but hardware, which everyone has tried.

663
00:38:56,575 --> 00:39:00,285
Speaker 6:  Apple has tried, it didn't really work. I mean they have the installment

664
00:39:00,315 --> 00:39:04,205
Speaker 6:  program but they wanted to bundle everything more and that program never

665
00:39:04,205 --> 00:39:06,125
Speaker 6:  got off the ground. We'll see if they can make it work.

666
00:39:06,345 --> 00:39:08,685
Speaker 4:  Do you know why that program never got off the ground? 'cause the cell carriers

667
00:39:08,685 --> 00:39:10,085
Speaker 4:  wouldn't let them get it off the ground. Ah,

668
00:39:10,085 --> 00:39:10,405
Speaker 6:  There you go.

669
00:39:10,505 --> 00:39:12,645
Speaker 4:  The cell carriers want that recurring revenue.

670
00:39:12,985 --> 00:39:15,885
Speaker 6:  And that's another thing is like, do they need cell carriers for this? Right.

671
00:39:15,905 --> 00:39:18,405
Speaker 6:  How does that change things? TBD probably

672
00:39:18,405 --> 00:39:22,125
Speaker 4:  Yeah. A a different set of gatekeepers. Yeah. Who are

673
00:39:22,125 --> 00:39:24,605
Speaker 4:  even worse to deal with than Apple. Yeah.

674
00:39:24,605 --> 00:39:27,885
Speaker 5:  The question of how ambitious this thing sets out to be, I think is gonna

675
00:39:28,145 --> 00:39:31,445
Speaker 5:  be just as interesting as anything else because like

676
00:39:32,235 --> 00:39:36,165
Speaker 5:  Alex to your point, there are, there are a lot of pieces of the puzzle here

677
00:39:36,305 --> 00:39:40,285
Speaker 5:  and OpenAI and Johnny I have together control more

678
00:39:40,285 --> 00:39:44,125
Speaker 5:  of those pieces than anybody else has. Right? Yeah. Like in terms of

679
00:39:44,535 --> 00:39:48,365
Speaker 5:  being able to actually make the whole stack work together, that's a lot of

680
00:39:48,365 --> 00:39:52,205
Speaker 5:  the stack, but it's not all of the stack and a lot of the stack

681
00:39:52,305 --> 00:39:56,245
Speaker 5:  is either out of their control or will require like honest to

682
00:39:56,245 --> 00:39:59,085
Speaker 5:  god like physics breakthroughs that no one has made yet.

683
00:39:59,225 --> 00:40:03,125
Speaker 6:  If there's no display, the physics breakthroughs are probably not as bad

684
00:40:03,125 --> 00:40:06,925
Speaker 6:  as we think. I mean they probably have disassembled humane and figured out

685
00:40:06,995 --> 00:40:10,885
Speaker 6:  exactly everything they did wrong and are gonna learn from it. And Sam saw

686
00:40:10,885 --> 00:40:14,445
Speaker 6:  that as like a I'm gonna invest in this thing to see how it doesn't work

687
00:40:14,905 --> 00:40:17,365
Speaker 6:  and I'm gonna do it the right way with Johnny. We'll see,

688
00:40:17,685 --> 00:40:21,645
Speaker 5:  I mean Humane was not full of dumb people. No, It is like the cautionary

689
00:40:21,645 --> 00:40:24,205
Speaker 5:  tale of Humane is, it's like, like one of the things Conan O'Brien always

690
00:40:24,205 --> 00:40:27,285
Speaker 5:  says is it takes a lot of really smart people working really hard to make

691
00:40:27,285 --> 00:40:31,125
Speaker 5:  a bad movie. Yeah. And like the humane pin is, is the

692
00:40:31,125 --> 00:40:34,845
Speaker 5:  perfect version of that. To me it's like a lot of people doing their best

693
00:40:34,865 --> 00:40:37,685
Speaker 5:  who are very smart and very capable and have really long track records who

694
00:40:37,685 --> 00:40:40,925
Speaker 5:  made a horrible product. Yeah. And there's like, that's such an easy way

695
00:40:40,925 --> 00:40:41,925
Speaker 5:  to go. Yeah.

696
00:40:42,105 --> 00:40:44,885
Speaker 6:  We should, we should not beat up on Humane too much more. I just am saying

697
00:40:44,885 --> 00:40:45,085
Speaker 6:  like

698
00:40:45,285 --> 00:40:47,445
Speaker 4:  Disagree. No, I think you should read the Jesse statement again actually.

699
00:40:48,745 --> 00:40:52,605
Speaker 6:  But like if by the end of next year there's an option in chat GPT to

700
00:40:52,605 --> 00:40:56,485
Speaker 6:  get this, you know, Johnny I Polka Deck e

701
00:40:56,985 --> 00:41:00,885
Speaker 6:  mailed to your house for an extra $20 a month and your pro subscription

702
00:41:00,945 --> 00:41:04,605
Speaker 6:  and it'll automatically hook into everything chat. GPT already knows about

703
00:41:04,605 --> 00:41:08,045
Speaker 6:  you and it's an always on listening device for talking to chat

704
00:41:08,205 --> 00:41:11,845
Speaker 6:  GPT. Do I think people will want that? Yeah, I think a lot of people would

705
00:41:11,845 --> 00:41:12,085
Speaker 6:  want that.

706
00:41:12,365 --> 00:41:15,725
Speaker 5:  I I totally agree. We'll see if you're Sam Altman, you've looked at how many

707
00:41:15,725 --> 00:41:18,685
Speaker 5:  people happily ponied up $200 a month just to use your cool stuff.

708
00:41:19,665 --> 00:41:23,165
Speaker 5:  And I think the, the like subscription is

709
00:41:23,805 --> 00:41:26,885
Speaker 5:  whatever they want it to be for this kind of stuff. I think It is where they're

710
00:41:26,885 --> 00:41:27,085
Speaker 5:  headed.

711
00:41:27,425 --> 00:41:30,325
Speaker 4:  All right. We should wrap this up. I will say that I can already hear the

712
00:41:30,325 --> 00:41:33,605
Speaker 4:  people furiously writing us tweets and emails about the iPhone upgrade program,

713
00:41:33,615 --> 00:41:37,445
Speaker 4:  which it currently exists. What we are talking about, just to be a hundred

714
00:41:37,445 --> 00:41:41,165
Speaker 4:  percent clear, is that Apple wanted to bundle getting a new iPhone every

715
00:41:41,165 --> 00:41:44,005
Speaker 4:  year into Apple One. And they were not able to do that.

716
00:41:44,235 --> 00:41:47,605
Speaker 5:  Also, subscription hardware is a bad idea and everyone should feel bad about

717
00:41:47,605 --> 00:41:51,125
Speaker 5:  doing it. Do you remember when Logitech said A Forever Mouse? And we laughed

718
00:41:51,365 --> 00:41:53,165
Speaker 5:  at them. This is a forever Mouse.

719
00:41:53,585 --> 00:41:57,285
Speaker 4:  Sam Altman's Forever Mouse sounds like an incredible movie. I just swear

720
00:41:57,285 --> 00:42:01,085
Speaker 4:  like I would go watch whatever that is, that's a stage spectacular that I

721
00:42:01,085 --> 00:42:04,365
Speaker 4:  would absolutely go watch. I don't know what It is. It feels like one of

722
00:42:04,365 --> 00:42:07,965
Speaker 4:  those like places in London that's like next to the wax museum. Sam Altman's

723
00:42:07,965 --> 00:42:11,885
Speaker 4:  Forever Mouse. Like what I was You ask chatt PT to render that, email that

724
00:42:11,885 --> 00:42:13,685
Speaker 4:  to us instead of your angry emails about the iPhone.

725
00:42:13,685 --> 00:42:14,325
Speaker 5:  Great. There we go.

726
00:42:14,765 --> 00:42:17,205
Speaker 4:  I bad give you. All right, we gotta take a break and we're gonna talk about

727
00:42:17,205 --> 00:42:21,085
Speaker 4:  the other side of io, which is actually Google io. See

728
00:42:21,085 --> 00:42:22,525
Speaker 4:  what I did there. We'll be right back.

729
00:42:57,025 --> 00:43:00,975
Speaker 4:  All right, we're back. We gotta talk about Google io, which if not

730
00:43:01,075 --> 00:43:05,035
Speaker 4:  for Johnny Ivan, Sam Altman literally trying to upstage Google io

731
00:43:05,035 --> 00:43:08,315
Speaker 4:  with their own company called IO would've easily been the o like only thing

732
00:43:08,315 --> 00:43:10,475
Speaker 4:  we talked about in the today. 'cause so much happened there.

733
00:43:10,715 --> 00:43:14,475
Speaker 5:  I do feel a little bad. We, we allowed open AI and Sam Altman to get away

734
00:43:14,475 --> 00:43:17,435
Speaker 5:  with this by putting it first in the show. They

735
00:43:17,435 --> 00:43:20,115
Speaker 4:  Did it again. That was you by the way. I wanna be a hundred percent clear.

736
00:43:20,115 --> 00:43:21,435
Speaker 4:  That was David's decision. Yeah, I

737
00:43:21,675 --> 00:43:25,475
Speaker 5:  I I did that and I stand by it and I am deeply annoyed that

738
00:43:25,735 --> 00:43:27,315
Speaker 5:  I'm giving Johnny and Sam what they want.

739
00:43:30,175 --> 00:43:33,595
Speaker 4:  It was, it was interesting timing because Google had a great io

740
00:43:34,025 --> 00:43:37,075
Speaker 4:  like by all accounts. Yeah, I think they announced a bunch of stuff that

741
00:43:37,075 --> 00:43:40,075
Speaker 4:  people were excited about. The demos are all really good. They announced

742
00:43:40,075 --> 00:43:44,035
Speaker 4:  a bunch of stuff that is shipping soon. Like we got to try a bunch

743
00:43:44,035 --> 00:43:47,875
Speaker 4:  of this stuff in including the glasses, which are, you know,

744
00:43:47,875 --> 00:43:51,275
Speaker 4:  they're just more prototype AR glasses. But they were there and we people

745
00:43:51,295 --> 00:43:54,955
Speaker 4:  got to try 'em. There's a video with V wearing them and they

746
00:43:55,065 --> 00:43:58,675
Speaker 4:  Googled just everyone from Sundar on down just full of

747
00:43:58,675 --> 00:44:02,335
Speaker 4:  confidence. David, run us through the, the big news

748
00:44:02,535 --> 00:44:03,775
Speaker 4:  and let's get into it. Oh

749
00:44:03,775 --> 00:44:07,735
Speaker 5:  Boy. Okay. So I think the, the overarching story

750
00:44:07,755 --> 00:44:11,575
Speaker 5:  of Google IO is Google is absolutely convinced it's winning at

751
00:44:11,675 --> 00:44:15,575
Speaker 5:  ai. Yeah. And especially I talked to a bunch of the folks over there

752
00:44:15,745 --> 00:44:19,415
Speaker 5:  ahead of IO including De who runs DeepMind and all their AI stuff

753
00:44:19,675 --> 00:44:23,255
Speaker 5:  and they are like really feeling themselves with

754
00:44:23,315 --> 00:44:27,295
Speaker 5:  Gemini 2.5 and are are convinced that not only are they ahead, but

755
00:44:27,295 --> 00:44:31,095
Speaker 5:  they might be ahead in a way that is hard for others to quickly catch

756
00:44:31,125 --> 00:44:34,605
Speaker 5:  up with. Which I think is fascinating. And so now Google is like, okay,

757
00:44:35,025 --> 00:44:38,405
Speaker 5:  we have, we have what we perceive to be the best model. We have

758
00:44:39,395 --> 00:44:43,165
Speaker 5:  vast distribution everywhere on the internet and we are going to put AI in

759
00:44:43,165 --> 00:44:46,805
Speaker 5:  front of you in every single imaginable way all the time.

760
00:44:47,095 --> 00:44:50,925
Speaker 5:  Everywhere. So there were a bunch of things, there were new models,

761
00:44:51,145 --> 00:44:53,925
Speaker 5:  new Imagine and video models to make images and video,

762
00:44:55,655 --> 00:44:59,285
Speaker 5:  which were like not super impressive. I don't know how they felt in in the

763
00:44:59,285 --> 00:45:02,445
Speaker 5:  crowd at Shoreline, but like watching some of those on YouTube, it was like,

764
00:45:02,445 --> 00:45:05,685
Speaker 5:  this is like not as cool as you think. It's

765
00:45:05,925 --> 00:45:09,485
Speaker 6:  You're talking about the video stuff? Yeah. Oh no, I disagree.

766
00:45:09,635 --> 00:45:09,925
Speaker 6:  Some

767
00:45:09,925 --> 00:45:13,805
Speaker 4:  Of the video stuff was cool, but the part where they demoed the,

768
00:45:13,805 --> 00:45:17,485
Speaker 4:  like the the woman, the filmmaker who made a video about herself being born

769
00:45:18,205 --> 00:45:20,905
Speaker 4:  and it was called Ancestral and I was like, this is a horror movie. Do you

770
00:45:20,905 --> 00:45:21,785
Speaker 4:  know that you made a horror movie?

771
00:45:22,415 --> 00:45:26,065
Speaker 6:  It's Darren Aronofsky. Yeah, yeah. I agree that what they showed on stage

772
00:45:26,065 --> 00:45:30,025
Speaker 6:  was not as compelling, but the stuff I've seen on like x that people have

773
00:45:30,025 --> 00:45:33,145
Speaker 6:  been making with this for sure, for sure is mind blowing. It's like full

774
00:45:33,165 --> 00:45:34,785
Speaker 6:  on reality is over

775
00:45:35,115 --> 00:45:35,465
Speaker 4:  Stuff.

776
00:45:35,775 --> 00:45:39,425
Speaker 5:  Yeah, no, some of It is very good. So anyway, so there were a lot of like

777
00:45:40,325 --> 00:45:44,065
Speaker 5:  new things coming to existing products, sort of improvements inside of Gmail.

778
00:45:44,825 --> 00:45:48,425
Speaker 5:  But I would say the biggest things we heard a lot about were

779
00:45:48,965 --> 00:45:52,705
Speaker 5:  AI mode, which is the, the new tab inside of search that is

780
00:45:52,705 --> 00:45:55,785
Speaker 5:  basically just a full AI experience next to

781
00:45:56,295 --> 00:45:59,665
Speaker 5:  traditional search in a way that is messy and complicated and we should talk

782
00:45:59,665 --> 00:46:01,745
Speaker 5:  about. There's also,

783
00:46:03,285 --> 00:46:06,425
Speaker 5:  Gemini is now being baked into Chrome, which is

784
00:46:07,065 --> 00:46:09,945
Speaker 5:  fascinating in the context of everything else going on with Google, which

785
00:46:10,245 --> 00:46:12,865
Speaker 5:  may or may not have to sell Chrome at some point in the very near future.

786
00:46:14,325 --> 00:46:17,865
Speaker 5:  The Gemini app continues to get better. So the Google has set it up so that

787
00:46:17,865 --> 00:46:21,585
Speaker 5:  there's Project Astra, which is, it's like, they called it a concept car

788
00:46:21,645 --> 00:46:25,345
Speaker 5:  of an AI assistant and that's where they try all their like truly wild stuff.

789
00:46:26,125 --> 00:46:29,305
Speaker 5:  And it was Astra where they're showing off all of the like

790
00:46:30,335 --> 00:46:33,465
Speaker 5:  wild demos where it's like watching you do stuff as you live your life and

791
00:46:33,465 --> 00:46:36,865
Speaker 5:  then can speak up. But as stuff gets better from there, it all graduates

792
00:46:36,865 --> 00:46:40,265
Speaker 5:  into the Gemini app, which got a bunch of new stuff. There's new features

793
00:46:40,265 --> 00:46:44,225
Speaker 5:  for Gemini Live, there's now like some of the live stuff is also in

794
00:46:44,225 --> 00:46:46,545
Speaker 5:  the search app. It's very confusing.

795
00:46:46,645 --> 00:46:48,705
Speaker 6:  You were saying this like it makes sense and it does not.

796
00:46:49,005 --> 00:46:49,425
Speaker 4:  No it

797
00:46:49,425 --> 00:46:50,745
Speaker 5:  Doesn't, none of it makes any sense.

798
00:46:50,935 --> 00:46:54,665
Speaker 4:  There's one part, there's one little part that makes sense, which is

799
00:46:54,665 --> 00:46:58,585
Speaker 4:  Google has figured out a vocabulary for

800
00:46:59,825 --> 00:47:03,295
Speaker 4:  what things are shipping mainstream products

801
00:47:03,715 --> 00:47:07,695
Speaker 4:  and what stuff is weird Google ideas. Yes. Right. So there are projects

802
00:47:07,715 --> 00:47:10,535
Speaker 4:  of which there are too many and all of their names are confusing because

803
00:47:10,565 --> 00:47:14,555
Speaker 4:  it's Google. The projects are like, here are the demos, right? Here's this

804
00:47:14,555 --> 00:47:18,435
Speaker 4:  thing Project Mariner, the is the tech demo of we're gonna

805
00:47:18,575 --> 00:47:22,255
Speaker 4:  let our agent go click around the website. And then the

806
00:47:22,415 --> 00:47:26,295
Speaker 4:  verb which David just used is that stuff graduates to the

807
00:47:26,295 --> 00:47:29,955
Speaker 4:  actual products like search. And then there's like weird hybrids where search

808
00:47:29,955 --> 00:47:33,555
Speaker 4:  has the new AI mode tab, which is not a project but an actual thing that

809
00:47:33,555 --> 00:47:36,675
Speaker 4:  rolled out to everybody in the United States. But eventually things in AI

810
00:47:36,675 --> 00:47:39,795
Speaker 4:  mode will graduate to the main search experience. So they're, they've just

811
00:47:39,795 --> 00:47:43,555
Speaker 4:  introduced, I'm not saying it, it, it's great. I'm saying they've

812
00:47:43,555 --> 00:47:47,235
Speaker 4:  introduced a verb to make sense of like how

813
00:47:47,435 --> 00:47:51,315
Speaker 4:  Google does stuff, which is instead of shipping products,

814
00:47:51,625 --> 00:47:55,515
Speaker 4:  they now just graduate. Right. Oh my God. Like, like you graduate from

815
00:47:55,515 --> 00:47:59,275
Speaker 4:  preschool. Cool. It's very simple guys. Project Astra is the Google lens

816
00:47:59,275 --> 00:48:00,555
Speaker 4:  to Gemini's AI mode.

817
00:48:02,365 --> 00:48:05,515
Speaker 5:  Jesus. That's true. I hate that. Yeah,

818
00:48:05,785 --> 00:48:09,475
Speaker 4:  It's a lot. But but that little once you like understand that

819
00:48:09,475 --> 00:48:13,355
Speaker 4:  Google needed a way to make people stop asking when the

820
00:48:13,355 --> 00:48:14,755
Speaker 4:  thing would become the real thing

821
00:48:16,425 --> 00:48:20,045
Speaker 4:  and they came up with the word graduate, like some of it clicks into place

822
00:48:20,305 --> 00:48:22,365
Speaker 4:  and then everyone at Google would be like, yeah, it's still pretty bad though.

823
00:48:22,365 --> 00:48:22,885
Speaker 4:  Right? The

824
00:48:22,885 --> 00:48:26,605
Speaker 5:  Important thing though is that there are essentially two places

825
00:48:26,675 --> 00:48:30,605
Speaker 5:  that really matter to Google. One is the Gemini app and one is search. Yep.

826
00:48:30,785 --> 00:48:34,605
Speaker 5:  And so every other thing it's working on is at some point

827
00:48:35,305 --> 00:48:39,245
Speaker 5:  moving towards one of those two places. And the Gemini app is

828
00:48:39,405 --> 00:48:43,165
Speaker 5:  designed to be your assistant, right? Like that's the thing. It's trying

829
00:48:43,165 --> 00:48:46,885
Speaker 5:  to be what? What chat GPT is for lots of people it's competing with Claude.

830
00:48:46,885 --> 00:48:50,645
Speaker 5:  Like It is the all in one operating system for your life thing that

831
00:48:50,645 --> 00:48:54,525
Speaker 5:  Google is trying to do. Search is search and and

832
00:48:54,985 --> 00:48:58,565
Speaker 5:  AI mode and all this stuff is, is a big bet in a slightly different

833
00:48:58,565 --> 00:49:02,485
Speaker 5:  direction that it can be what search has been for

834
00:49:02,525 --> 00:49:06,405
Speaker 5:  25 years but like much better. And I think Gemini and search are

835
00:49:06,565 --> 00:49:09,565
Speaker 5:  actually, they overlap in sort of how you can use them but they are different

836
00:49:09,565 --> 00:49:12,845
Speaker 5:  things in Google's mind. Yes. But those are the two places that all of this

837
00:49:12,865 --> 00:49:14,285
Speaker 5:  is designed to end up.

838
00:49:14,865 --> 00:49:18,845
Speaker 4:  Gemini is the big age agentic assistant that goes and

839
00:49:18,845 --> 00:49:21,805
Speaker 4:  does things for you. That's the demo. Right? Alex has been talking about

840
00:49:21,805 --> 00:49:25,165
Speaker 4:  the bike demo. If you go watch the video of that demo in particular,

841
00:49:25,755 --> 00:49:29,565
Speaker 4:  it's the guy's got his phone open, he's looking at his bike, he says

842
00:49:29,565 --> 00:49:33,365
Speaker 4:  pull up the instruction manual Gemini goes and does a search, gets the

843
00:49:33,445 --> 00:49:36,845
Speaker 4:  PDF of the bike's instruction manual. And he says scroll to the page about

844
00:49:37,165 --> 00:49:41,045
Speaker 4:  whatever breaks. And it finds that we scan like you have to

845
00:49:41,045 --> 00:49:45,005
Speaker 4:  believe that Huffy has produced an OCR capable PDF but like whatever,

846
00:49:45,005 --> 00:49:48,325
Speaker 4:  there's a lot of steps in there. But it scans the manual finds a thing, you

847
00:49:48,325 --> 00:49:52,165
Speaker 4:  watch it literally scroll the phone in the background, it calls the bike

848
00:49:52,165 --> 00:49:55,205
Speaker 4:  shop and has a whole interaction on the phone with the bike shop. That's

849
00:49:55,335 --> 00:49:59,325
Speaker 4:  agent stuff like big hairy agent stuff. And

850
00:49:59,325 --> 00:50:02,685
Speaker 4:  then search is like the vision for AI mode

851
00:50:03,105 --> 00:50:06,925
Speaker 4:  is that every search result page you get is a custom built

852
00:50:06,995 --> 00:50:10,895
Speaker 4:  application all the way down to like we can build charts for you.

853
00:50:11,275 --> 00:50:15,215
Speaker 4:  We might actually build custom code apps. Like you ask a

854
00:50:15,335 --> 00:50:19,295
Speaker 4:  question about stats in the NFL and we, we will build a data visualization

855
00:50:19,315 --> 00:50:23,015
Speaker 4:  app for you on the fly. Yep. And show that to you as a search result. Which

856
00:50:23,015 --> 00:50:26,855
Speaker 4:  is a big, a huge idea but not an agent. Right. And at

857
00:50:26,855 --> 00:50:27,965
Speaker 4:  some point they're gonna overlap.

858
00:50:28,225 --> 00:50:30,965
Speaker 6:  Gemini can do that. This is what I don't understand. I mean the reason these

859
00:50:30,965 --> 00:50:34,445
Speaker 6:  products are separate is because Google makes all of its money from search.

860
00:50:34,715 --> 00:50:38,005
Speaker 6:  Sure. That that's, and they are shipping their org chart quite literally,

861
00:50:38,005 --> 00:50:41,805
Speaker 6:  which is like Demis has his fiefdom of DeepMind and now the

862
00:50:41,805 --> 00:50:45,525
Speaker 6:  app and he's brought product into his org. So now he controls the app fully.

863
00:50:46,065 --> 00:50:49,925
Speaker 6:  He doesn't control search. That's a whole nother org that is 10 times as

864
00:50:49,925 --> 00:50:53,205
Speaker 6:  large that is the most profitable software business in the history of the

865
00:50:53,205 --> 00:50:57,045
Speaker 6:  world. And they cannot disrupt that to the degree that they're willing

866
00:50:57,105 --> 00:51:00,925
Speaker 6:  to do wild stuff in Gemini. And should these

867
00:51:00,985 --> 00:51:04,965
Speaker 6:  be one products? Like does OpenAI say no, we have a separate

868
00:51:05,185 --> 00:51:08,765
Speaker 6:  app for you to search with chat GPT? No they don't. It's just one big product

869
00:51:09,305 --> 00:51:12,805
Speaker 6:  and it's actually, it's way simpler to understand. And Google's struggle

870
00:51:13,165 --> 00:51:17,085
Speaker 6:  I think was that from this IO is that yes they have the best models,

871
00:51:17,515 --> 00:51:21,435
Speaker 6:  they suck at product, they suck at making it work

872
00:51:21,715 --> 00:51:25,595
Speaker 6:  holistically and making it simple for people to understand. And that's still

873
00:51:25,595 --> 00:51:27,355
Speaker 6:  something they're, they're working through. That's

874
00:51:27,355 --> 00:51:30,275
Speaker 4:  Google. Google loves to ship its org chart and it was even on like a parent

875
00:51:30,335 --> 00:51:34,275
Speaker 4:  on stage at IO where like different executives got to announce the same product

876
00:51:34,335 --> 00:51:37,995
Speaker 4:  in different ways. Yeah. 'cause everybody needed a bite, you know, like this

877
00:51:37,995 --> 00:51:41,315
Speaker 4:  stuff happens. But from the product perspective,

878
00:51:42,315 --> 00:51:45,445
Speaker 4:  what I actually saw was all of their big bets

879
00:51:46,105 --> 00:51:50,075
Speaker 4:  that they've taken over time and all of their data about people

880
00:51:50,855 --> 00:51:54,755
Speaker 4:  is now resulting in products that look like they might work. Which

881
00:51:54,755 --> 00:51:57,115
Speaker 4:  again has been my criticism of AI for a long time. I don't know if they're

882
00:51:57,155 --> 00:52:00,875
Speaker 4:  actually gonna work but they have Gmail. Yeah. They just have

883
00:52:00,875 --> 00:52:04,675
Speaker 4:  Gmail. And so a lot of their like personal context boils down

884
00:52:04,675 --> 00:52:08,555
Speaker 4:  to well we ha we we we're just gonna read your email. Right. Like we know

885
00:52:08,555 --> 00:52:11,715
Speaker 4:  when your flights are 'cause there it's in your email so we don't have to

886
00:52:11,715 --> 00:52:15,475
Speaker 4:  do anything else. Like what if we were just better at reading your email

887
00:52:15,495 --> 00:52:18,955
Speaker 4:  to you is like an incredible product for AI to solve. And now Google has

888
00:52:18,955 --> 00:52:22,795
Speaker 4:  all this like research and the models to do it and then you can just

889
00:52:22,795 --> 00:52:26,235
Speaker 4:  productize that. And I think the search piece of it,

890
00:52:27,235 --> 00:52:30,035
Speaker 4:  I, yeah I do think there's two different org charts and that's all their

891
00:52:30,035 --> 00:52:33,635
Speaker 4:  money so they can't screw with it too much. But there's a world in which

892
00:52:34,135 --> 00:52:37,325
Speaker 4:  you would make that decision anyway because

893
00:52:38,265 --> 00:52:40,765
Speaker 4:  you don't want every search to be like agentic.

894
00:52:40,995 --> 00:52:41,285
Speaker 6:  Sure.

895
00:52:41,535 --> 00:52:45,525
Speaker 4:  Right. You do just want some information presented back to you without like

896
00:52:45,525 --> 00:52:49,165
Speaker 4:  the emotional package of your agent like reading your email to you. And I,

897
00:52:49,445 --> 00:52:52,045
Speaker 4:  I don't know where that line is and I think it's actually useful for Google

898
00:52:52,825 --> 00:52:56,285
Speaker 4:  to know like on some timeline they converge

899
00:52:56,625 --> 00:52:58,925
Speaker 4:  but not know when or how and just see how it plays out.

900
00:52:59,405 --> 00:53:03,365
Speaker 5:  I also think if you're at Google there is very little evidence

901
00:53:03,425 --> 00:53:06,725
Speaker 5:  to suggest that you need to overhaul search soon.

902
00:53:08,065 --> 00:53:11,325
Speaker 5:  Google like the, the everybody is like, oh you have to cannibalize your own

903
00:53:11,325 --> 00:53:14,885
Speaker 5:  product or somebody else to do it for Google just continues to destroy

904
00:53:14,885 --> 00:53:18,325
Speaker 5:  everybody at search and and everybody's like, oh you know

905
00:53:18,395 --> 00:53:22,285
Speaker 5:  Zoomers use chat GPT and and and Google is just like show us in the

906
00:53:22,285 --> 00:53:23,565
Speaker 5:  market share friend os like

907
00:53:25,265 --> 00:53:29,245
Speaker 6:  So there's they are playing games with their

908
00:53:29,245 --> 00:53:33,205
Speaker 6:  metrics. Yeah. Pretty flatly. 'cause you know Eddie Q

909
00:53:33,745 --> 00:53:37,685
Speaker 6:  was at, at the search remedies trial and he

910
00:53:37,685 --> 00:53:40,805
Speaker 6:  said look, you don't need to do this Search is already under threat searches

911
00:53:40,985 --> 00:53:44,685
Speaker 6:  in iOS Safari have gone down for the first time in 22 years. And Google's,

912
00:53:44,705 --> 00:53:47,765
Speaker 6:  we talked about this Google stock price dropped like that day 'cause everyone

913
00:53:47,765 --> 00:53:51,365
Speaker 6:  is waiting Yeah. For the pin to drop. Right. So

914
00:53:51,645 --> 00:53:55,165
Speaker 6:  Sundar is on decoder, you'll hear it on Monday. And So I asked about that

915
00:53:55,445 --> 00:53:59,245
Speaker 6:  directly and we'll just, we'll just play the clip at EQ

916
00:53:59,825 --> 00:54:03,805
Speaker 6:  on the stand and the trial the other day said search in Safari for the

917
00:54:03,805 --> 00:54:07,605
Speaker 6:  last month dropped for the first time in 22 years. That's a big

918
00:54:07,635 --> 00:54:11,405
Speaker 6:  stat. Obviously your stock price was affected by it. And there was a

919
00:54:11,405 --> 00:54:14,965
Speaker 6:  statement is that trend bearing out that the standard Google Search is, is

920
00:54:15,125 --> 00:54:17,445
Speaker 6:  dropping from devices and different kinds of searches are increasing?

921
00:54:18,825 --> 00:54:22,485
Speaker 9:  No. Look we've been very clear. We are seeing overall query growth in search.

922
00:54:24,185 --> 00:54:25,205
Speaker 9:  You know, it looks,

923
00:54:26,185 --> 00:54:27,805
Speaker 6:  But have you actually seen the drop in safari?

924
00:54:29,295 --> 00:54:33,065
Speaker 9:  Look we have a comprehensive view of how we look at data across the board.

925
00:54:33,465 --> 00:54:37,145
Speaker 9:  There's a lot of, there can be a lot of noise in search data, but

926
00:54:37,195 --> 00:54:40,625
Speaker 9:  everything we see tells us we are seeing query growth

927
00:54:40,625 --> 00:54:44,145
Speaker 9:  including across apple's devices and platforms. And

928
00:54:44,585 --> 00:54:47,945
Speaker 9:  specifically, you know, I think we quantified the query growth from AI or

929
00:54:47,945 --> 00:54:51,425
Speaker 9:  views and what's what's healthy is that the query growth

930
00:54:52,325 --> 00:54:53,745
Speaker 9:  is continuing to grow over time.

931
00:54:54,165 --> 00:54:57,505
Speaker 6:  So first of all children, if your parents ask you how you did on your report

932
00:54:57,505 --> 00:55:00,745
Speaker 6:  card, you should just say I have a comprehensive view of the data. That's

933
00:55:00,745 --> 00:55:04,425
Speaker 6:  good. That's really good. Incredible answer. That whole, that whole interview

934
00:55:04,425 --> 00:55:07,745
Speaker 6:  coming on Monday was a good one. But Alex you weren't laughing 'cause he

935
00:55:07,745 --> 00:55:11,705
Speaker 6:  keeps saying query growth. Yeah and I don't, from what I

936
00:55:11,705 --> 00:55:14,625
Speaker 6:  have gathered from our conversations over the past couple days, you're, you're

937
00:55:14,625 --> 00:55:18,385
Speaker 6:  very skeptical of this. Well so two things can be true. So overall queries

938
00:55:18,385 --> 00:55:22,265
Speaker 6:  for Google can be growing but the growth rate can be declining. So

939
00:55:22,325 --> 00:55:26,025
Speaker 6:  yes. Do I think that queries in Google products or across

940
00:55:26,025 --> 00:55:29,945
Speaker 6:  everywhere that search touches have just like literally gone negative

941
00:55:30,095 --> 00:55:33,625
Speaker 6:  like year over year? No. Like in aggregate is it probably growing? Yes. Is

942
00:55:33,625 --> 00:55:37,185
Speaker 6:  it growing as fast? Google will not say. And also like

943
00:55:37,765 --> 00:55:41,625
Speaker 6:  are the amount of searches done per person growing or are fewer people

944
00:55:41,625 --> 00:55:45,025
Speaker 6:  doing more searches? Right. There's a lot that they are not saying

945
00:55:45,445 --> 00:55:48,425
Speaker 6:  and they're just trying to flat out say like broadly query growth is growing

946
00:55:48,495 --> 00:55:51,585
Speaker 6:  because the moment you drill down into what's actually happening, maybe you

947
00:55:51,585 --> 00:55:54,985
Speaker 6:  learn, oh wait, like maybe in North America in

948
00:55:55,585 --> 00:55:59,345
Speaker 6:  a key cohort for advertisers, which is like, you know 20 to 35,

949
00:56:00,105 --> 00:56:03,945
Speaker 6:  a significant chunk of people are doing increasingly commercial intent searches

950
00:56:04,125 --> 00:56:08,095
Speaker 6:  on CHATT PT And that's like if you actually try to drill down into how

951
00:56:08,095 --> 00:56:10,855
Speaker 6:  search works, which they don't wanna do, they wanna just talk about it at

952
00:56:10,855 --> 00:56:14,775
Speaker 6:  a high level, you maybe learn stuff like that And you know, we saw

953
00:56:14,775 --> 00:56:17,895
Speaker 6:  that reaction in the stock price because I think investors are so on edge

954
00:56:18,315 --> 00:56:21,415
Speaker 6:  and you can see it in the way Google just how the stock is traded. Like it's,

955
00:56:21,415 --> 00:56:25,325
Speaker 6:  it's heavily discounted relative to its peers because there

956
00:56:25,325 --> 00:56:28,685
Speaker 6:  is this fear not only of will the company be broken up but

957
00:56:29,425 --> 00:56:33,165
Speaker 6:  is chat GPT and these other AR products actually eating

958
00:56:33,235 --> 00:56:36,845
Speaker 6:  into search in a meaningful way. And so far Google's answer is,

959
00:56:37,705 --> 00:56:41,685
Speaker 6:  are types of queries are expanding, people are using search in

960
00:56:41,685 --> 00:56:45,165
Speaker 6:  deeper new ways. Which yes that's true. That's how these AI products work.

961
00:56:45,225 --> 00:56:49,165
Speaker 6:  You wanna thank it and have a conversation with it. That doesn't mean

962
00:56:49,165 --> 00:56:52,725
Speaker 6:  that overall query volume is growing like it did.

963
00:56:53,105 --> 00:56:55,605
Speaker 5:  I'm just not sure there's any evidence to suggest either way. Right. Like

964
00:56:55,785 --> 00:56:59,435
Speaker 5:  the all, all we know is that

965
00:57:00,535 --> 00:57:02,475
Speaker 5:  people like chat GPT, right? Like

966
00:57:02,675 --> 00:57:02,795
Speaker 4:  I

967
00:57:02,795 --> 00:57:06,715
Speaker 6:  Mean no there's there is, I mean there's the fact that like 500 million

968
00:57:06,715 --> 00:57:10,595
Speaker 6:  people use chat GPT every week and people also

969
00:57:10,595 --> 00:57:13,355
Speaker 6:  use all these other AI chat bots. There was like a bunch of things that did

970
00:57:13,355 --> 00:57:16,115
Speaker 6:  not exist three years ago that now exist that directly

971
00:57:17,335 --> 00:57:21,275
Speaker 6:  do what Google used to do. And like we all see it and I bet a lot

972
00:57:21,275 --> 00:57:23,315
Speaker 6:  of our listeners feel it like in their daily lives.

973
00:57:23,495 --> 00:57:26,715
Speaker 5:  You think all 500 million of those people every week are firing up chat GBT

974
00:57:26,715 --> 00:57:27,355
Speaker 5:  to do web search.

975
00:57:27,795 --> 00:57:30,675
Speaker 6:  I think I'm getting answers from chat GBT that I've normally used Google

976
00:57:30,735 --> 00:57:33,955
Speaker 6:  for. Whether it's like searching the web or not. It could be like

977
00:57:34,535 --> 00:57:38,395
Speaker 6:  do a math equation. For me it could be like what is a time zone conversion?

978
00:57:38,395 --> 00:57:42,315
Speaker 6:  Like there's a ton of things you use Google for and yes chat GT doesn't do

979
00:57:42,315 --> 00:57:45,395
Speaker 6:  the commercial shopping stuff especially yet though I think they will And

980
00:57:45,395 --> 00:57:48,995
Speaker 6:  that's when it really gets potentially scary for Google. But

981
00:57:49,105 --> 00:57:53,075
Speaker 6:  yeah. Do I and my family members and my friends, are we using

982
00:57:53,265 --> 00:57:56,635
Speaker 6:  chat GPT? Like we used to use search. Absolutely. And I think a lot of people

983
00:57:56,635 --> 00:57:57,515
Speaker 6:  are, there's

984
00:57:57,515 --> 00:57:59,315
Speaker 4:  Also other search replacements. Yeah.

985
00:57:59,365 --> 00:58:00,395
Speaker 6:  Perplexity last

986
00:58:00,395 --> 00:58:03,275
Speaker 4:  Year we talked about TikTok a bunch, right? Like people just search in TikTok

987
00:58:03,275 --> 00:58:06,915
Speaker 4:  and TikTok has search surfaces whether or not they're useful.

988
00:58:07,465 --> 00:58:10,595
Speaker 4:  Like I think TikTok search has like dramatically declined in quality since

989
00:58:10,595 --> 00:58:14,475
Speaker 4:  all these other products at the scene. I, I think the point is like to

990
00:58:14,475 --> 00:58:18,395
Speaker 4:  connect this to, there's Gemini in their search, they need search

991
00:58:18,415 --> 00:58:22,275
Speaker 4:  to remain competitive and vital. Yeah. And that's why you don't

992
00:58:22,545 --> 00:58:26,515
Speaker 4:  make your big bet on Gemini. 'cause then you are head up against chat

993
00:58:26,635 --> 00:58:29,995
Speaker 4:  GPT and perplexity and whatever else, right? But you're saying, oh actually

994
00:58:29,995 --> 00:58:33,175
Speaker 4:  this thing is just really good. Like you don't need to open this other app.

995
00:58:33,175 --> 00:58:36,975
Speaker 4:  Like Google Search is now just incredible and when assistance happen

996
00:58:37,025 --> 00:58:38,135
Speaker 4:  we're gonna have one ready for you.

997
00:58:38,555 --> 00:58:42,535
Speaker 5:  The real fear for Google here I think is less

998
00:58:42,705 --> 00:58:46,415
Speaker 5:  about how sort of meaningful a competitor chat GPT is right now

999
00:58:46,715 --> 00:58:50,695
Speaker 5:  and how good a brand chat GPT is like the, the

1000
00:58:50,695 --> 00:58:53,855
Speaker 5:  thing that no one has ever been able to touch with Google is like a sheer

1001
00:58:53,855 --> 00:58:57,815
Speaker 5:  awareness. And Chad GPT is like running at

1002
00:58:57,815 --> 00:59:01,335
Speaker 5:  it really fast in terms of like when I need to do something, where do I go?

1003
00:59:01,715 --> 00:59:05,375
Speaker 5:  The answer for 20 years has been Google and

1004
00:59:05,525 --> 00:59:09,295
Speaker 5:  like the, the Omnibox in Chrome was the most important surface on the internet

1005
00:59:09,295 --> 00:59:13,015
Speaker 5:  for two decades. And chat GPT is is pushing

1006
00:59:13,075 --> 00:59:16,735
Speaker 5:  at that about as fast and hard as you could possibly imagine And like

1007
00:59:16,925 --> 00:59:20,535
Speaker 5:  chat GPT is a is a terrible business and has many

1008
00:59:20,535 --> 00:59:23,885
Speaker 5:  limitations as a product. But like again It is, It is hitting

1009
00:59:24,635 --> 00:59:28,245
Speaker 5:  that like mainstream Kleenex isation

1010
00:59:28,875 --> 00:59:32,165
Speaker 5:  like kind of at record speed. And I think if I'm Google that's the thing

1011
00:59:32,165 --> 00:59:34,845
Speaker 5:  I'm afraid of And that's the reason Neil, to your point, that's why you don't

1012
00:59:34,845 --> 00:59:37,925
Speaker 5:  bet on Gemini because your brand is Google and so what you, what you want

1013
00:59:37,925 --> 00:59:41,605
Speaker 5:  everybody to think of is you want to think of Google as the thing that is

1014
00:59:41,605 --> 00:59:45,525
Speaker 5:  like chat GPT, not Gemini. Gemini is like off here doing other stuff.

1015
00:59:45,825 --> 00:59:49,645
Speaker 4:  But I really think like the, the confidence we saw from Google this

1016
00:59:49,645 --> 00:59:53,435
Speaker 4:  last week is 'cause it's there like

1017
00:59:53,435 --> 00:59:56,715
Speaker 4:  they have a lot of products Yeah. That are good.

1018
00:59:57,295 --> 01:00:01,155
Speaker 4:  Not just chat CPT, right? Like yeah SOA is

1019
01:00:01,155 --> 01:00:03,355
Speaker 4:  not a great video generator yet.

1020
01:00:03,895 --> 01:00:05,635
Speaker 6:  SOA doesn't exist yet. You know what I mean?

1021
01:00:05,905 --> 01:00:09,715
Speaker 4:  Like it's like it's around like some people have used it sometimes

1022
01:00:10,845 --> 01:00:13,525
Speaker 4:  VO three is just like you can just screw with it. Yeah. I watched Serge Brain

1023
01:00:13,525 --> 01:00:14,845
Speaker 4:  try to generate his own face for a while.

1024
01:00:16,465 --> 01:00:19,765
Speaker 4:  At least that's what I was told. Like it's just people are using it today

1025
01:00:20,345 --> 01:00:23,325
Speaker 4:  and it will lead to some outcomes. I think some of those outcomes might be

1026
01:00:23,325 --> 01:00:27,285
Speaker 4:  like negative for the film industry. Like who knows? But the products exist

1027
01:00:27,345 --> 01:00:30,285
Speaker 4:  and the changes are gonna happen around them. Some of the age agentic stuff,

1028
01:00:30,675 --> 01:00:34,365
Speaker 4:  like I just literally watched a live demo of Project Mariner

1029
01:00:34,995 --> 01:00:38,605
Speaker 4:  like look for jobs and it was just clicking around a remote

1030
01:00:38,605 --> 01:00:42,405
Speaker 4:  desktop chrome session and you know, I have a lot of

1031
01:00:42,565 --> 01:00:45,125
Speaker 4:  questions about that. Like why would any of those services agree to that?

1032
01:00:45,155 --> 01:00:49,005
Speaker 4:  Like I don't know, is that pretty brittle and like should you just use

1033
01:00:49,005 --> 01:00:52,865
Speaker 4:  some of these new technologies like MCP, like I don't know maybe, but

1034
01:00:52,865 --> 01:00:56,465
Speaker 4:  it works like it's there and the rate of

1035
01:00:56,615 --> 01:01:00,585
Speaker 4:  improvement is so fast that Google is like oh we're gonna,

1036
01:01:00,585 --> 01:01:03,185
Speaker 4:  we're gonna do this. Yeah. Like this is definitely just happening for us.

1037
01:01:03,295 --> 01:01:03,585
Speaker 4:  They

1038
01:01:03,585 --> 01:01:07,305
Speaker 6:  Have the best models and that is a tremendous advantage. I was talking to

1039
01:01:07,305 --> 01:01:11,265
Speaker 6:  someone who works in another big AI lab and they were saying model quality

1040
01:01:11,495 --> 01:01:15,345
Speaker 6:  impacts usage of the products more than anything we, we don't

1041
01:01:15,345 --> 01:01:19,185
Speaker 6:  think it does. But if you release a shittier model in a chat bot, people

1042
01:01:19,245 --> 01:01:22,985
Speaker 6:  use the chat bot less people can feel how these models react

1043
01:01:23,525 --> 01:01:27,185
Speaker 6:  and what they want to get out of them in a way that maybe we just broadly

1044
01:01:27,585 --> 01:01:31,025
Speaker 6:  discount and how we cover them. And Google is objectively leading

1045
01:01:31,445 --> 01:01:34,585
Speaker 6:  in model quality right now. And that was like the first thing Sunar came

1046
01:01:34,585 --> 01:01:37,305
Speaker 6:  out on stage and talked about was like, we're number one on all the leaderboards.

1047
01:01:37,525 --> 01:01:41,465
Speaker 6:  I'm like, that is true but is everyone talking about Gemini? Is

1048
01:01:41,525 --> 01:01:45,185
Speaker 6:  Gemini like the thing I hear when I go home for like Memorial Day weekend?

1049
01:01:45,685 --> 01:01:49,385
Speaker 6:  No, no one even knows Gemini has an app except like early adopters and

1050
01:01:49,625 --> 01:01:53,385
Speaker 6:  certain, you know, pockets like chat GBT is the Kleenex of all this

1051
01:01:53,405 --> 01:01:57,305
Speaker 6:  and that's a problem for Google unless we hit a GI like

1052
01:01:57,355 --> 01:01:59,025
Speaker 6:  Demis once and none of this matters anyway.

1053
01:01:59,385 --> 01:02:03,305
Speaker 4:  Yeah. And Demis the only one left standing just openly being like I'm

1054
01:02:03,305 --> 01:02:06,745
Speaker 4:  doing a GI. Yeah. That's what this is all for. I'm on stage with Sergei brand

1055
01:02:06,745 --> 01:02:10,625
Speaker 4:  being like the a GI moment is coming, we're we're taking bets

1056
01:02:10,625 --> 01:02:14,105
Speaker 4:  on whether it happens before or after 2030 and they were six months on like

1057
01:02:14,105 --> 01:02:15,025
Speaker 4:  either side of the line,

1058
01:02:16,685 --> 01:02:18,985
Speaker 4:  You know, like every other company stop talking about it 'cause they have

1059
01:02:19,005 --> 01:02:22,205
Speaker 4:  have to make some products to make some money to justify all this huge investment.

1060
01:02:22,265 --> 01:02:25,745
Speaker 4:  And Google's like, yeah we did it already. You know, I think Google

1061
01:02:26,075 --> 01:02:29,545
Speaker 4:  maybe last year, particularly two years ago was incredibly

1062
01:02:29,905 --> 01:02:33,705
Speaker 4:  insecure. Yes. About having done all of the research investment like

1063
01:02:33,705 --> 01:02:37,625
Speaker 4:  we've been joking for years now that if you say chat GPTA Google

1064
01:02:37,635 --> 01:02:40,825
Speaker 4:  comms person like leaps out of a bush and It is like the T and chat GPT was

1065
01:02:40,985 --> 01:02:44,625
Speaker 4:  invented at Google. Like yeah, they're over that, right? They're like, okay

1066
01:02:44,675 --> 01:02:48,545
Speaker 4:  we're we caught up Like we we're gonna ship products that are

1067
01:02:48,545 --> 01:02:52,345
Speaker 4:  good. But I think the interesting part of that, Alex is maybe

1068
01:02:52,415 --> 01:02:55,705
Speaker 4:  chat PT has the brand name Google has the distribution. Yeah.

1069
01:02:55,995 --> 01:02:56,345
Speaker 6:  Right?

1070
01:02:56,525 --> 01:02:59,905
Speaker 4:  So they're loud about AI overviews being the most single most popular

1071
01:03:00,235 --> 01:03:01,865
Speaker 4:  generative AI product at scale.

1072
01:03:02,075 --> 01:03:05,545
Speaker 6:  Right. Hundreds of millions of people. I think you could disti you could

1073
01:03:05,765 --> 01:03:09,585
Speaker 6:  remove popular and used like Yes. Yeah. Do people know what they're using?

1074
01:03:09,645 --> 01:03:13,585
Speaker 6:  Do they know that ASRA is the lens to deep minds, you

1075
01:03:13,585 --> 01:03:17,505
Speaker 6:  know, AI mode? No, but like that's the problem is like yes.

1076
01:03:17,905 --> 01:03:21,545
Speaker 6:  I I think it's more of a ego thing honestly. And I think it's a recruiting

1077
01:03:21,545 --> 01:03:25,425
Speaker 6:  thing and I think for Google, yeah they're seeing metrics that are not

1078
01:03:25,525 --> 01:03:29,185
Speaker 6:  as bad as everyone thought, but this, this race, this

1079
01:03:29,185 --> 01:03:32,665
Speaker 6:  perception of the race, this perception of how far ahead chat GPT is and

1080
01:03:32,985 --> 01:03:36,865
Speaker 6:  just consumer mind share is it, it's an ego problem

1081
01:03:36,865 --> 01:03:40,305
Speaker 6:  for them. It's maybe a long-term strategy problem and I think it just hits

1082
01:03:40,305 --> 01:03:43,345
Speaker 6:  them more in that realm. I don't think it actually, it's clearly not hitting

1083
01:03:43,345 --> 01:03:45,905
Speaker 6:  them on the business and that's what you and I were feeling being on site,

1084
01:03:46,005 --> 01:03:49,665
Speaker 6:  but like is it still a problem for them in that they are not winning

1085
01:03:49,885 --> 01:03:53,825
Speaker 6:  at at all? Yes. There I met, I was with a Google exec who's been there forever

1086
01:03:54,845 --> 01:03:58,745
Speaker 6:  at one point yesterday and they were like, yeah, you know, just like a

1087
01:03:58,745 --> 01:04:02,145
Speaker 6:  thing we really have a problem with is like not being first, you know, like

1088
01:04:02,145 --> 01:04:06,025
Speaker 6:  that's, and that's like every company, but I think Google for so long just

1089
01:04:06,645 --> 01:04:10,305
Speaker 6:  had this perch that was untouchable. Yeah, right. They just ran the

1090
01:04:10,465 --> 01:04:14,305
Speaker 6:  internet and now they're getting challenged on all sides. And I think

1091
01:04:14,305 --> 01:04:17,865
Speaker 6:  that just that that messes with the, with the ego a little bit

1092
01:04:18,145 --> 01:04:21,825
Speaker 6:  in, in a way that makes it seeps into the products. It

1093
01:04:21,825 --> 01:04:24,905
Speaker 6:  seeps into the decisions they make. But yes, as the business may be doing

1094
01:04:24,905 --> 01:04:27,545
Speaker 6:  better than we thought. Yeah. I mean I thought it was crazy like at the very

1095
01:04:27,545 --> 01:04:31,385
Speaker 6:  end of the keynote Sunar mentions Waymo for like 30 seconds. It's like,

1096
01:04:31,845 --> 01:04:35,185
Speaker 6:  you know, they may have a trillion dollar company in Waymo that has solved

1097
01:04:35,185 --> 01:04:39,065
Speaker 6:  self-driving and is now doing more rides in San Francisco than Lyft. Like

1098
01:04:39,215 --> 01:04:42,505
Speaker 6:  that is happening in one part of Google. Yeah. YouTube is

1099
01:04:43,055 --> 01:04:46,225
Speaker 6:  unshakeable, YouTube is probably gonna eat the entire entertainment industry.

1100
01:04:46,725 --> 01:04:50,225
Speaker 6:  So they have all these products that are just huge and dominant

1101
01:04:51,045 --> 01:04:52,625
Speaker 6:  and I think they were feeling that, yeah.

1102
01:04:52,625 --> 01:04:56,185
Speaker 4:  At the end of the keynote. So I just casually mentioned their satellite constellation

1103
01:04:56,185 --> 01:04:59,985
Speaker 4:  to detect fires and he literally was like, it will be fully operational

1104
01:05:00,005 --> 01:05:02,505
Speaker 4:  by your end. And I was like, that's how they announced the the second death

1105
01:05:02,505 --> 01:05:03,185
Speaker 4:  star. I wanna be

1106
01:05:04,695 --> 01:05:04,985
Speaker 6:  Like

1107
01:05:05,285 --> 01:05:08,505
Speaker 4:  That's, those are the words they used for the death star dude. And then he

1108
01:05:08,505 --> 01:05:10,785
Speaker 4:  was like, come good I everybody and I laughed

1109
01:05:12,255 --> 01:05:15,465
Speaker 6:  Also. I think they're feeling really great because you know, Dieter is more

1110
01:05:15,465 --> 01:05:17,685
Speaker 6:  famous than Giannis, so that's

1111
01:05:17,685 --> 01:05:20,685
Speaker 4:  True. Dieter got the biggest single biggest cheer at the io keynote. Shout

1112
01:05:20,685 --> 01:05:24,575
Speaker 4:  out to Dieter. It was very fun seeing him as a popular

1113
01:05:24,575 --> 01:05:28,415
Speaker 4:  Google executive on that stage. Also when he came out

1114
01:05:28,695 --> 01:05:31,855
Speaker 4:  we were, we were all sitting next to each other live blogging and V just

1115
01:05:31,855 --> 01:05:35,095
Speaker 4:  goes Dieter. Like it's like totally quiet,

1116
01:05:35,295 --> 01:05:36,375
Speaker 6:  Right? Like

1117
01:05:37,315 --> 01:05:40,895
Speaker 4:  She just screams teeter like incredible moment.

1118
01:05:41,115 --> 01:05:42,095
Speaker 4:  That's awesome. It was very good.

1119
01:05:44,445 --> 01:05:48,055
Speaker 4:  There's the distribution, right? That we have AI over reviews.

1120
01:05:48,215 --> 01:05:52,135
Speaker 4:  Everyone's using 'em as it popular, does a brand work on some timeline, it

1121
01:05:52,135 --> 01:05:55,735
Speaker 4:  doesn't matter because they're just there and they get really, really good

1122
01:05:56,435 --> 01:05:59,935
Speaker 4:  and then you just, you know, the next new person never even thinks to get

1123
01:05:59,935 --> 01:06:03,255
Speaker 4:  chat GPT and that feels like a big part of this bet, right? That's where

1124
01:06:03,355 --> 01:06:06,975
Speaker 4:  AI mode really comes to play, where you've got this big new search

1125
01:06:06,975 --> 01:06:10,415
Speaker 4:  experience and over time things will graduate to the main search experience

1126
01:06:10,995 --> 01:06:14,815
Speaker 4:  and then maybe no one's ever using chat CPT 'cause yeah, the

1127
01:06:14,815 --> 01:06:17,895
Speaker 4:  people who have yet to experience AI are just like, whatever, Google just

1128
01:06:17,895 --> 01:06:20,415
Speaker 4:  does this. Like I'm looking at this other, you want me to use this app that's

1129
01:06:20,415 --> 01:06:22,015
Speaker 4:  doing the thing that's already happening? It could

1130
01:06:22,015 --> 01:06:25,975
Speaker 6:  Be like stories, it could be like how meta copied Snapchat and Yeah, Snapchat

1131
01:06:26,035 --> 01:06:29,655
Speaker 6:  got big and it's still big, but it never became a multi-trillion dollar company

1132
01:06:30,005 --> 01:06:33,415
Speaker 6:  because a company with tremendous scale just copied it quickly and

1133
01:06:33,415 --> 01:06:35,615
Speaker 6:  leveraged its distribution. Yeah, that definitely

1134
01:06:35,615 --> 01:06:38,655
Speaker 5:  Could happen. Well this is, this is where Google shipping its org chart I

1135
01:06:38,655 --> 01:06:42,135
Speaker 5:  think is is hurting it pretty badly because the thing I can't figure out

1136
01:06:42,355 --> 01:06:46,175
Speaker 5:  and that no one at Google will give me an honest answer to is do they care

1137
01:06:46,175 --> 01:06:49,295
Speaker 5:  about Gemini as a like public facing

1138
01:06:49,825 --> 01:06:53,615
Speaker 5:  brand? Like is It, is it important to Google that we know

1139
01:06:53,635 --> 01:06:55,575
Speaker 5:  and care about Gemini? Yes.

1140
01:06:56,245 --> 01:06:58,735
Speaker 6:  It's funny they wouldn't tell you that. I think it's because they know they're

1141
01:06:58,735 --> 01:07:01,255
Speaker 6:  nowhere near where they need to be and they don't wanna say that I'm, but

1142
01:07:01,255 --> 01:07:01,375
Speaker 6:  I mean

1143
01:07:01,375 --> 01:07:03,415
Speaker 5:  They, they all say yes, but I don't believe them.

1144
01:07:03,695 --> 01:07:07,095
Speaker 6:  No, it's, it's, it's actually Sundar told employees at the beginning of there

1145
01:07:07,095 --> 01:07:10,855
Speaker 6:  that It is the top goal for the company is that Gemini like wins the

1146
01:07:10,965 --> 01:07:12,655
Speaker 6:  chatbot race. Like, like we

1147
01:07:12,655 --> 01:07:12,855
Speaker 4:  Need. So

1148
01:07:12,855 --> 01:07:15,615
Speaker 5:  I people to think of Geminis. So I believe that. But I think like if, if

1149
01:07:15,615 --> 01:07:19,375
Speaker 5:  it were, if Google were a different company where it was

1150
01:07:19,875 --> 01:07:23,815
Speaker 5:  easier to succeed by making things good and not by launching new things

1151
01:07:24,435 --> 01:07:28,415
Speaker 5:  Mm. Would it have made a lot more sense to just do all of this

1152
01:07:28,415 --> 01:07:32,215
Speaker 5:  work inside of AI mode from the very beginning? Yes.

1153
01:07:32,275 --> 01:07:36,055
Speaker 5:  And and you go back to how can we make google.com the most important thing

1154
01:07:36,055 --> 01:07:39,495
Speaker 5:  on the internet again forever. And you, you, yes. You just, you pull this

1155
01:07:39,495 --> 01:07:42,375
Speaker 5:  thing from a tab slowly into search. Like the thing I think they're doing

1156
01:07:42,375 --> 01:07:45,735
Speaker 5:  with AI mode makes a lot of sense. And the fact that it's not called

1157
01:07:45,955 --> 01:07:49,615
Speaker 5:  Gemini doesn't make any sense. And so I'm just lost on this thing about like

1158
01:07:49,615 --> 01:07:53,575
Speaker 5:  Google is actually trying to point you in so many places that I think

1159
01:07:53,575 --> 01:07:56,935
Speaker 5:  It is risking like diluting the actually interesting work that it's doing.

1160
01:07:57,205 --> 01:07:59,975
Speaker 6:  It's 'cause it's the money search is the money they can't, yeah. They can't

1161
01:07:59,975 --> 01:08:03,695
Speaker 6:  do this. Like they can't disrupt. This is classic innovator's dilemma. This

1162
01:08:03,695 --> 01:08:07,415
Speaker 6:  is what open AI is attacking head on. Yeah. Is that Google cannot change

1163
01:08:07,515 --> 01:08:11,375
Speaker 6:  its business fast enough for the consumer awareness

1164
01:08:11,795 --> 01:08:14,935
Speaker 6:  and just attention sucking that chat GPT is doing right now.

1165
01:08:15,585 --> 01:08:18,445
Speaker 6:  And yeah, just put it in the chat. That is exactly what it should be. Gemini

1166
01:08:18,445 --> 01:08:21,525
Speaker 6:  should be searched. There should be no distinction. The problem is, is that

1167
01:08:21,525 --> 01:08:25,485
Speaker 6:  Google isn't a company. It's a combination of like 14 companies with

1168
01:08:25,985 --> 01:08:29,925
Speaker 6:  its own, their own CEOs. Right. You know, and Yeah. Should West Us

1169
01:08:29,925 --> 01:08:33,485
Speaker 6:  like run it all? Probably like would things run smoother? Yes. But there's

1170
01:08:33,485 --> 01:08:35,045
Speaker 6:  a lot of lords and they all have their own,

1171
01:08:35,165 --> 01:08:37,845
Speaker 5:  I mean I even think about like there's, there's a, there's a Google app

1172
01:08:37,845 --> 01:08:39,325
Speaker 4:  That was a seamless Game of Thrones

1173
01:08:39,405 --> 01:08:42,845
Speaker 5:  Reference that was really good. But like there's a Google app in which you

1174
01:08:42,845 --> 01:08:46,685
Speaker 5:  can either do Google Search or with one button go to Gemini and then

1175
01:08:46,685 --> 01:08:50,525
Speaker 5:  there's also a Gemini app. And in no sane world, do both of those

1176
01:08:50,525 --> 01:08:52,245
Speaker 5:  things need to exist? No,

1177
01:08:52,565 --> 01:08:55,085
Speaker 4:  I can't, I cannot believe I'm the person who's saying this. I don't think

1178
01:08:55,085 --> 01:08:57,125
Speaker 4:  any of this shit matters. I really don't. I

1179
01:08:57,225 --> 01:08:57,805
Speaker 5:  It might not.

1180
01:08:58,235 --> 01:09:02,125
Speaker 4:  Like, I think the main search experience is gonna get better.

1181
01:09:02,945 --> 01:09:06,525
Speaker 4:  People are gonna see the words AI mode and like click it 'cause it just says

1182
01:09:06,705 --> 01:09:10,645
Speaker 4:  AI mode and that is a clicker thing than the word Gemini,

1183
01:09:10,645 --> 01:09:14,485
Speaker 4:  which means nothing. Then they're gonna be like, wow, people are using AI

1184
01:09:14,485 --> 01:09:17,645
Speaker 4:  mode and that's gonna become the main thing. And at some point

1185
01:09:18,515 --> 01:09:22,125
Speaker 4:  they will do the thing that Google is uniquely capable of doing

1186
01:09:22,545 --> 01:09:26,325
Speaker 4:  and say, click this link to download the Gemini app and they will just

1187
01:09:26,345 --> 01:09:29,805
Speaker 4:  get Gemini distribution. Yeah. And that will all just happen.

1188
01:09:30,075 --> 01:09:34,045
Speaker 4:  Like they're, they are uniquely capable of doing that. One of the

1189
01:09:34,195 --> 01:09:36,645
Speaker 4:  they have, and yes, they got the innovators still have problem. Like they

1190
01:09:36,645 --> 01:09:40,565
Speaker 4:  can't burn down the most lucrative business in favor of the the cheap

1191
01:09:40,565 --> 01:09:43,245
Speaker 4:  disruptive thing. That's almost good enough. But works better for some people,

1192
01:09:43,245 --> 01:09:46,925
Speaker 4:  right? Like yeah, they have, they got that problem. I think there other problem

1193
01:09:47,925 --> 01:09:51,825
Speaker 4:  is they're basically gonna torch

1194
01:09:51,925 --> 01:09:55,865
Speaker 4:  the web. Like along the way the web as we know It is

1195
01:09:55,865 --> 01:09:59,785
Speaker 4:  coming to an end. And they are, you know what Shar will

1196
01:09:59,785 --> 01:10:02,665
Speaker 4:  tell you, it's not a zero sum game that the web is growing. They're crawling

1197
01:10:02,665 --> 01:10:06,625
Speaker 4:  more web pages than ever. You'll hear that in decoder next week. Every

1198
01:10:06,625 --> 01:10:09,705
Speaker 4:  Google executive there is like more stuff is happening on the web.

1199
01:10:10,555 --> 01:10:14,425
Speaker 4:  There, there are more and better applications on the web than ever.

1200
01:10:15,485 --> 01:10:18,295
Speaker 4:  Like the web is the place where you deploy new apps.

1201
01:10:19,525 --> 01:10:23,135
Speaker 4:  High point of like maybe of all time. Yeah. Because of all this app store

1202
01:10:23,135 --> 01:10:26,135
Speaker 4:  ruling and the Epic case, the web is the place where you go to buy stuff

1203
01:10:26,425 --> 01:10:30,375
Speaker 4:  about to become incredibly important again. Right? All these apps are

1204
01:10:30,375 --> 01:10:33,175
Speaker 4:  gonna kick you out of in-app purchases to the web and we're gonna all be

1205
01:10:33,175 --> 01:10:35,735
Speaker 4:  buying stuff on the web. 'cause you don't have to pay the Apple Tech. Like

1206
01:10:35,735 --> 01:10:39,375
Speaker 4:  it's all that stuff is great, but like the web is a media platform where

1207
01:10:39,375 --> 01:10:42,615
Speaker 4:  the new information is. It is like, it's already under pressure from all

1208
01:10:42,615 --> 01:10:45,535
Speaker 4:  these tools, right? It's already dying because of these tools. You can see

1209
01:10:45,535 --> 01:10:49,495
Speaker 4:  it every day. All that, all that new information is on. I

1210
01:10:49,495 --> 01:10:52,735
Speaker 4:  mean you're, you're listening to this in your podcast player or on YouTube,

1211
01:10:52,805 --> 01:10:56,535
Speaker 4:  like it's in these other weird platforms. It's on

1212
01:10:56,595 --> 01:11:00,525
Speaker 4:  TikTok and Google has to figure out how to get that information

1213
01:11:00,985 --> 01:11:04,965
Speaker 4:  or it has to figure out how it's agents can go and look at all those databases

1214
01:11:04,965 --> 01:11:08,895
Speaker 4:  and make that worthwhile for people. And I don't,

1215
01:11:08,975 --> 01:11:11,575
Speaker 4:  I don't know if that stuff makes sense yet. I don't even, it makes sense

1216
01:11:11,575 --> 01:11:15,185
Speaker 4:  for OpenAI, but at least they're the upstart, right? Like yeah.

1217
01:11:15,445 --> 01:11:15,905
Speaker 6:  If their

1218
01:11:15,905 --> 01:11:19,185
Speaker 4:  Core business fails, like Sam Altman will just lit, lit a bunch of money

1219
01:11:19,185 --> 01:11:21,665
Speaker 4:  on fire and that's kinda what we expected him to do. If Google can't pull

1220
01:11:21,665 --> 01:11:25,425
Speaker 4:  it off, like a lot of things go wrong, but they have to, they have to work

1221
01:11:25,425 --> 01:11:29,355
Speaker 4:  that curve of like the web is the place where the

1222
01:11:29,355 --> 01:11:33,315
Speaker 4:  information is. Yeah. Is quickly getting abstracted away to the

1223
01:11:33,315 --> 01:11:36,995
Speaker 4:  web. Is the database that the new AI Google Search

1224
01:11:37,145 --> 01:11:38,155
Speaker 4:  synthesizes for you?

1225
01:11:38,665 --> 01:11:42,515
Speaker 6:  Yeah, I heard that too at io I heard the execs saying like, no, like

1226
01:11:42,515 --> 01:11:45,235
Speaker 6:  the content on the web is bigger than it ever has. We have more to crawl

1227
01:11:45,235 --> 01:11:48,515
Speaker 6:  than we ever have. And I just was thinking like, wait, isn't that all AI

1228
01:11:48,675 --> 01:11:52,395
Speaker 6:  slop? Like, isn't the reason you have more to crawl is because your models

1229
01:11:52,575 --> 01:11:56,555
Speaker 6:  are producing a bunch of garbage? And I I really think they view data

1230
01:11:56,665 --> 01:12:00,595
Speaker 6:  like, like sand. Like it's just like the beach has gotten bigger,

1231
01:12:01,095 --> 01:12:03,995
Speaker 6:  but it's like Yeah, but there's like a bunch of dead animals in it. Yeah.

1232
01:12:03,995 --> 01:12:06,235
Speaker 6:  Like that's kind of how they view data.

1233
01:12:06,345 --> 01:12:08,875
Speaker 4:  Look at the amount of rubble we're standing on. It's like, well this, you

1234
01:12:08,915 --> 01:12:10,195
Speaker 4:  blew up the city. It's

1235
01:12:10,195 --> 01:12:13,515
Speaker 6:  A real problem. So I guess in aggregate, is there more data for them to crawl?

1236
01:12:13,995 --> 01:12:17,835
Speaker 6:  Probably. How much of that is human? How much of that is high quality? We

1237
01:12:17,835 --> 01:12:21,755
Speaker 6:  know we feel this as people in the media business. Like all of the best content

1238
01:12:21,815 --> 01:12:24,955
Speaker 6:  is going behind paywalls. It's, it's gonna be hard for agents to get to this

1239
01:12:24,955 --> 01:12:28,515
Speaker 6:  stuff. And how does that work? How do you solve the fundamental

1240
01:12:28,955 --> 01:12:32,795
Speaker 6:  business tension of people who make the good stuff cannot support

1241
01:12:32,945 --> 01:12:34,875
Speaker 6:  what they do on the web as it exists today?

1242
01:12:35,105 --> 01:12:38,875
Speaker 4:  Well actually let's take a, like a tiny detour into MCP 'cause David

1243
01:12:38,885 --> 01:12:42,395
Speaker 4:  wrote about it in the context of Microsoft Build This week we had

1244
01:12:42,395 --> 01:12:46,155
Speaker 4:  Kevin Scott, Microsoft, CTO, under Dakota this week. MCP

1245
01:12:46,235 --> 01:12:50,115
Speaker 4:  is like a phenomenon, right? It's a standard from Anthropic that

1246
01:12:50,215 --> 01:12:54,075
Speaker 4:  is basically here's how an agent shows up to a website or a database and

1247
01:12:54,315 --> 01:12:57,275
Speaker 4:  interacts with, it's basically just an API replacement. Like it's, it's,

1248
01:12:57,305 --> 01:13:00,515
Speaker 4:  it's not more complicated than like we made some APIs for agents to go do

1249
01:13:00,515 --> 01:13:04,435
Speaker 4:  stuff and if you just run it all the way to the end, it's kind of

1250
01:13:04,635 --> 01:13:08,395
Speaker 4:  like, oh, you don't need websites anymore. You just have, you need off and

1251
01:13:08,395 --> 01:13:12,235
Speaker 4:  maybe payments to databases and then the agents can just

1252
01:13:12,235 --> 01:13:15,395
Speaker 4:  like go and API into the databases and come back and give you stuff. Yeah.

1253
01:13:15,415 --> 01:13:18,035
Speaker 4:  And I, I, I just, I don't know how that business works, but David, you wrote

1254
01:13:18,035 --> 01:13:18,395
Speaker 4:  about it this week.

1255
01:13:18,675 --> 01:13:22,515
Speaker 5:  I mean, I think built into the way that you're talking about It is the assumption

1256
01:13:22,785 --> 01:13:26,095
Speaker 5:  that everyone will access every part of the web

1257
01:13:27,135 --> 01:13:28,475
Speaker 5:  via some chatbot,

1258
01:13:30,045 --> 01:13:32,755
Speaker 5:  which is what a bunch of these companies would like you to believe. But I

1259
01:13:32,755 --> 01:13:34,635
Speaker 5:  don't think is true and I don't really think has ever been true.

1260
01:13:36,495 --> 01:13:40,355
Speaker 5:  The, the theory is that like, okay, what we, what we

1261
01:13:40,355 --> 01:13:43,595
Speaker 5:  can do in one of two ways is help

1262
01:13:44,505 --> 01:13:47,595
Speaker 5:  plug AI into the web, right? And I think that's like

1263
01:13:48,235 --> 01:13:51,755
Speaker 5:  MCP stands for model context protocol. And basically the idea is it's like

1264
01:13:51,795 --> 01:13:55,475
Speaker 5:  a structured way for me to say I have a bunch of information on my website

1265
01:13:56,025 --> 01:13:59,995
Speaker 5:  here is how to give the model access to that information in order to

1266
01:13:59,995 --> 01:14:03,955
Speaker 5:  be able to do things fine. Microsoft is also working

1267
01:14:03,955 --> 01:14:06,715
Speaker 5:  on this thing called NL Web that I think is very cool. Which is basically

1268
01:14:06,735 --> 01:14:10,595
Speaker 5:  an open version of a protocol like that that says,

1269
01:14:10,625 --> 01:14:14,565
Speaker 5:  okay, what if I could actually run all of that AI stuff on my website?

1270
01:14:15,345 --> 01:14:19,325
Speaker 5:  And so now instead of going to chat GPT to talk to my website or go to Claude

1271
01:14:19,325 --> 01:14:22,605
Speaker 5:  to talk to my website, you come to my website to talk to my website. And

1272
01:14:22,605 --> 01:14:24,805
Speaker 5:  that is like, as somebody who believes in the open web, that is a future

1273
01:14:24,965 --> 01:14:28,685
Speaker 5:  I am much more interested in over time. But I think

1274
01:14:29,705 --> 01:14:33,685
Speaker 5:  to Alex's point, I don't think these companies care

1275
01:14:33,775 --> 01:14:36,765
Speaker 5:  about the information on the internet. Like I think a lot about, I think

1276
01:14:36,765 --> 01:14:40,125
Speaker 5:  Mark Zuckerberg has said a few times, which is that like everybody talks

1277
01:14:40,125 --> 01:14:43,645
Speaker 5:  a lot about news content on Facebook and that is like a vanishingly small

1278
01:14:43,835 --> 01:14:47,205
Speaker 5:  portion of what people do on Facebook. And so eventually they just stop caring

1279
01:14:47,475 --> 01:14:51,365
Speaker 5:  like, this is no longer worth our time and investment and energy

1280
01:14:51,425 --> 01:14:54,525
Speaker 5:  to care that much about what happens with news content because it doesn't

1281
01:14:54,525 --> 01:14:58,405
Speaker 5:  matter that much to our platform. And I think most of these companies feel

1282
01:14:58,425 --> 01:15:02,285
Speaker 5:  the same way that actually most of this is just headache and what

1283
01:15:02,285 --> 01:15:06,245
Speaker 5:  people come to Google for is to like find Adele music videos

1284
01:15:06,465 --> 01:15:10,325
Speaker 5:  and chocolate chip cookie recipes and not read politics news. Yeah. And

1285
01:15:10,455 --> 01:15:13,845
Speaker 5:  there are all kinds of terrifying downstream effects from the fact that they

1286
01:15:13,845 --> 01:15:17,365
Speaker 5:  don't really care, but I think they don't really care. And, and the idea

1287
01:15:17,365 --> 01:15:20,605
Speaker 5:  that what they're gonna do is set up a world in which everybody who owns

1288
01:15:20,605 --> 01:15:24,405
Speaker 5:  a website has to stop thinking about Google distribution and go build

1289
01:15:24,415 --> 01:15:28,325
Speaker 5:  themselves into a destination people go to on purpose. And that that's

1290
01:15:28,325 --> 01:15:31,645
Speaker 5:  gonna require reprogramming 20 years of behavior of how we do the internet.

1291
01:15:31,645 --> 01:15:35,605
Speaker 5:  Like it's a mess and it's gonna kill a lot of the web. And

1292
01:15:35,685 --> 01:15:39,245
Speaker 5:  I just don't think these companies care because It is, there's new stuff

1293
01:15:39,245 --> 01:15:39,925
Speaker 5:  to do. I think

1294
01:15:39,925 --> 01:15:43,875
Speaker 4:  Google wants you to believe that it cares, but it on on

1295
01:15:44,015 --> 01:15:46,475
Speaker 4:  the longest timelines that Google thinks it does not.

1296
01:15:46,735 --> 01:15:49,355
Speaker 5:  So I would say it the other way. I think Google cares a lot about the web

1297
01:15:49,655 --> 01:15:53,595
Speaker 5:  and very little about websites like the making

1298
01:15:53,595 --> 01:15:56,955
Speaker 5:  sure that websites that exist today get to continue as they exist today.

1299
01:15:57,075 --> 01:15:58,955
Speaker 5:  I don't think Google cares that much about that. Oh sure.

1300
01:15:58,955 --> 01:16:02,835
Speaker 4:  No, we're agreeing. But I I I think Google cares about the web. Is this like

1301
01:16:02,835 --> 01:16:06,035
Speaker 4:  really the web is a miracle. Like I just, like

1302
01:16:06,395 --> 01:16:10,355
Speaker 4:  straightforwardly the web is a miracle we have in the world a giant

1303
01:16:10,575 --> 01:16:14,275
Speaker 4:  interconnected, interdependent, mostly open application

1304
01:16:14,555 --> 01:16:16,295
Speaker 4:  platform. That's weird.

1305
01:16:16,765 --> 01:16:18,335
Speaker 5:  Yeah, right. Thank you. Government spending,

1306
01:16:18,715 --> 01:16:22,615
Speaker 4:  You can just, yeah, like, right. Yeah. We like, we just horse-powered

1307
01:16:22,615 --> 01:16:26,605
Speaker 4:  this thing into existence because honestly, mark Andreessen got a

1308
01:16:26,605 --> 01:16:29,445
Speaker 4:  bunch of government funding in the middle of the country. Like that's a thing

1309
01:16:29,445 --> 01:16:32,685
Speaker 4:  that happened. Like that's mosaic it and it turned into Netscape and like

1310
01:16:32,685 --> 01:16:36,515
Speaker 4:  here we are and it's all here and now Figma exists and like

1311
01:16:37,335 --> 01:16:41,235
Speaker 4:  Johnny, ive legendary designer probably opens a web

1312
01:16:41,235 --> 01:16:44,315
Speaker 4:  app to do a bunch of design work That's crazy.

1313
01:16:44,895 --> 01:16:46,475
Speaker 5:  Now he uses pages for everything.

1314
01:16:47,505 --> 01:16:51,395
Speaker 6:  Yeah. It's funny you brought up Andreessen. I actually talked to open AI

1315
01:16:51,395 --> 01:16:55,275
Speaker 6:  to their MCP lead this week because they joined the SN committee for MCP.

1316
01:16:55,655 --> 01:16:59,155
Speaker 6:  So they're now working with basically every AI lab has agreed that this is

1317
01:16:59,155 --> 01:17:03,115
Speaker 6:  the new standard. And I ac actually came away thinking we are actually

1318
01:17:03,115 --> 01:17:07,035
Speaker 6:  still pre Netscape on agents as it relates to the actual plumbing

1319
01:17:07,545 --> 01:17:11,445
Speaker 6:  that it takes to hook them together. So this OpenAI lead was like,

1320
01:17:11,555 --> 01:17:15,365
Speaker 6:  there's still no registry. Like there's no, there's no discoverability

1321
01:17:15,475 --> 01:17:19,445
Speaker 6:  mechanism for a developer to even know who has an MCP server.

1322
01:17:19,515 --> 01:17:23,085
Speaker 6:  Like we know there's a few of them, there's like Stripe and Zendesk or whatever,

1323
01:17:23,425 --> 01:17:27,245
Speaker 6:  but no one's even built the interface for then the developer to go,

1324
01:17:27,435 --> 01:17:30,725
Speaker 6:  okay, X model go make this happen with this server

1325
01:17:31,305 --> 01:17:34,685
Speaker 6:  and that that will happen. It'll probably happen this year, but we're not

1326
01:17:34,685 --> 01:17:37,725
Speaker 6:  even at the browser point. It is funny to be talking in these words because

1327
01:17:37,725 --> 01:17:41,125
Speaker 6:  like yes, agents are supposed to like replace browsers, but like we are not

1328
01:17:41,145 --> 01:17:44,925
Speaker 6:  in the browser phase of agent building yet. Like this is not

1329
01:17:44,925 --> 01:17:48,765
Speaker 6:  even been connected in that way. It's still very, very early and

1330
01:17:48,765 --> 01:17:52,725
Speaker 6:  it will be enterprise and very developer focused first. It's not

1331
01:17:52,725 --> 01:17:56,485
Speaker 6:  like we're gonna be getting some all powerful agent that can just like go

1332
01:17:56,505 --> 01:17:59,845
Speaker 6:  and order a bunch of stuff for you and talk to a bunch of apps like anytime

1333
01:17:59,875 --> 01:18:03,125
Speaker 6:  soon. Even though Google showed that off. Like this is gonna be a developer

1334
01:18:03,125 --> 01:18:06,925
Speaker 6:  replacing function calling with telling a model to go talk to an API.

1335
01:18:07,315 --> 01:18:09,725
Speaker 4:  Yeah. And I and that's really interesting. That's interesting.

1336
01:18:09,725 --> 01:18:10,605
Speaker 6:  Yeah, exactly.

1337
01:18:10,825 --> 01:18:14,565
Speaker 4:  You can see there's a new web you would build on that. Like Yeah, and that's

1338
01:18:14,565 --> 01:18:16,845
Speaker 4:  what I mean, like I agree with David, I think they care a lot about the web.

1339
01:18:17,365 --> 01:18:20,925
Speaker 4:  'cause this big interconnected application platform where you can like

1340
01:18:20,995 --> 01:18:24,725
Speaker 4:  instantly deliver complex applications

1341
01:18:25,545 --> 01:18:29,325
Speaker 4:  to a runtime like Chrome is a miracle. And

1342
01:18:29,645 --> 01:18:33,565
Speaker 4:  I think Google is very invested in that particular miracle, I think

1343
01:18:33,705 --> 01:18:37,405
Speaker 4:  as a media platform where you read a bunch of text and it's a doc, they don't

1344
01:18:37,405 --> 01:18:40,525
Speaker 4:  care about that at all. And like I know this 'cause Alex and I were sitting

1345
01:18:40,525 --> 01:18:44,365
Speaker 4:  in the audience for the de asaba and Sergey brand

1346
01:18:44,365 --> 01:18:48,325
Speaker 4:  fireside chat with Alex Kantrowitz and Kreitz asked,

1347
01:18:48,555 --> 01:18:51,485
Speaker 4:  what do you think the web is in 10 years? And Sergey was like, who knows?

1348
01:18:51,675 --> 01:18:51,965
Speaker 4:  Like

1349
01:18:52,855 --> 01:18:53,605
Speaker 6:  Right. He was

1350
01:18:53,605 --> 01:18:57,005
Speaker 4:  Like, the world will, I'll live it, like hit the singularity. Like I will

1351
01:18:57,005 --> 01:19:00,885
Speaker 4:  be one with the sun like off in space. And Demis said

1352
01:19:00,915 --> 01:19:04,845
Speaker 4:  it's, it's a good question. And then quote, I think the web in the near term,

1353
01:19:04,865 --> 01:19:08,045
Speaker 4:  the web is going to change a lot. If you think about an agent first web.

1354
01:19:08,665 --> 01:19:11,885
Speaker 4:  Do the agents have to see renders in things that we need to see as humans

1355
01:19:11,915 --> 01:19:14,525
Speaker 4:  website? Do we need to actually render webpage? Do we need

1356
01:19:14,525 --> 01:19:17,685
Speaker 5:  Websites? That's the question. And he's, I mean his answer clearly is no.

1357
01:19:18,505 --> 01:19:21,765
Speaker 4:  His answer is no. And then I asked Sundar a riff on this question, I read

1358
01:19:21,765 --> 01:19:24,485
Speaker 4:  him that quote and he was like, what is the web but a series of databases,

1359
01:19:24,575 --> 01:19:28,325
Speaker 4:  which first of all is the Verst VERGE answer of all time. Yes.

1360
01:19:28,325 --> 01:19:30,725
Speaker 4:  Like I literally started, you're gonna hear him. And I just started laughing.

1361
01:19:31,125 --> 01:19:33,925
Speaker 4:  I was like, thank you. Like I feel very validated. Like what are we, but

1362
01:19:33,925 --> 01:19:34,925
Speaker 4:  a series of dine

1363
01:19:34,925 --> 01:19:35,685
Speaker 5:  Out on that one for a while,

1364
01:19:38,145 --> 01:19:41,965
Speaker 4:  But like if you reconceptualize the entire interconnected application

1365
01:19:41,965 --> 01:19:44,325
Speaker 4:  environment of the web, It is just a bunch of databases that you can go ask

1366
01:19:44,325 --> 01:19:48,005
Speaker 4:  for stuff. And some of those databases have Toyota's Camrys in them and some

1367
01:19:48,005 --> 01:19:51,285
Speaker 4:  of them have vacation houses in them and some of 'em have sandwiches. It's

1368
01:19:51,285 --> 01:19:54,005
Speaker 4:  like, oh, those are just businesses that are going away. Right?

1369
01:19:54,615 --> 01:19:58,155
Speaker 6:  Or they're businesses that get funded through API calls instead of

1370
01:19:58,575 --> 01:20:02,235
Speaker 6:  ads on their web pages. Like will the incentives figure themselves out? Because

1371
01:20:02,455 --> 01:20:06,195
Speaker 6:  do agents need things to do? Yes. So like

1372
01:20:06,575 --> 01:20:10,405
Speaker 6:  if everything that an agent would talk to suddenly goes away because

1373
01:20:10,505 --> 01:20:13,565
Speaker 6:  its business model is destroyed. Like guess what? Agents don't work. So

1374
01:20:14,235 --> 01:20:17,605
Speaker 6:  will this get figured out? Will it be super messy? Will a lot of companies

1375
01:20:17,625 --> 01:20:20,845
Speaker 6:  go away? Will whole new categories of companies get invented? Yes.

1376
01:20:21,625 --> 01:20:25,245
Speaker 6:  Yes. That's all true. Right? But do I think like the fundamental

1377
01:20:25,645 --> 01:20:29,545
Speaker 6:  building blocks of the plumbing of the web, like content and

1378
01:20:29,545 --> 01:20:32,345
Speaker 6:  things that, that an agent could do will go away? No. 'cause then there's

1379
01:20:32,345 --> 01:20:36,225
Speaker 6:  no agent. And what Google and Chat GBT are both racing to be is the interface

1380
01:20:36,965 --> 01:20:40,785
Speaker 6:  to interact with all those databases on your behalf, which is like the greatest

1381
01:20:41,985 --> 01:20:45,945
Speaker 6:  abstraction of technology I think maybe we've ever seen like in our lifetimes.

1382
01:20:45,945 --> 01:20:49,705
Speaker 6:  Yeah. If that happens at scale and the value that will accrue to the interface

1383
01:20:50,365 --> 01:20:53,705
Speaker 6:  is arguably greater than the value that Google gets now from running ads

1384
01:20:53,705 --> 01:20:54,145
Speaker 6:  on search.

1385
01:20:54,625 --> 01:20:57,105
Speaker 5:  I mean this is, this is the funny thing is like Alex, you and I have now

1386
01:20:57,105 --> 01:20:59,665
Speaker 5:  spent a lot of time on this podcast in the last week talking about display

1387
01:20:59,665 --> 01:21:03,345
Speaker 5:  ads. But like the fundamental question here is what is

1388
01:21:03,695 --> 01:21:06,745
Speaker 5:  what happens when display ads don't work anymore? What is the business of

1389
01:21:06,745 --> 01:21:10,225
Speaker 5:  the internet without display ads for the people who have traditionally

1390
01:21:10,365 --> 01:21:11,425
Speaker 5:  relied on display ads

1391
01:21:11,685 --> 01:21:14,265
Speaker 6:  Paying 250 a month for Google Ultra?

1392
01:21:14,925 --> 01:21:17,985
Speaker 5:  And maybe that's the answer. Maybe Google, maybe the end of this is Google

1393
01:21:18,045 --> 01:21:21,665
Speaker 5:  and open AI pay websites directly that

1394
01:21:22,145 --> 01:21:25,985
Speaker 5:  I don't love that. But that is one possible outcome of this. But if,

1395
01:21:26,005 --> 01:21:29,745
Speaker 5:  if the web is a series of un rendered databases, the whole

1396
01:21:29,985 --> 01:21:32,265
Speaker 5:  business of the web is suddenly immediately gone.

1397
01:21:32,565 --> 01:21:36,465
Speaker 4:  Ben Thompson, by the way, wrote about this this week based on MCP and

1398
01:21:36,465 --> 01:21:40,345
Speaker 4:  he was like, this is what stable coins are for. And like a like full

1399
01:21:40,345 --> 01:21:42,065
Speaker 4:  body grown basically is

1400
01:21:42,065 --> 01:21:45,105
Speaker 5:  Yeah, I ha I boy I hate that. Right? Oh man, I hate so much. But

1401
01:21:45,105 --> 01:21:48,865
Speaker 4:  Like, this is the idea that we'll, we'll have these like crypto microtransactions

1402
01:21:49,405 --> 01:21:52,865
Speaker 4:  pegged to the dollar in some way that make all that happen.

1403
01:21:53,145 --> 01:21:56,785
Speaker 5:  I would just say we, we have, if one more person says micro transactions

1404
01:21:56,805 --> 01:22:00,745
Speaker 5:  are the answer, I'm gonna send everyone a thousand links to

1405
01:22:00,745 --> 01:22:03,305
Speaker 5:  all the times we've tried Microtransactions and it didn't work. And then

1406
01:22:03,325 --> 01:22:04,665
Speaker 5:  I'm gonna heave myself out of a window.

1407
01:22:04,865 --> 01:22:08,505
Speaker 4:  I I I will Again, full body cringe in reading that Stablecoin said the answer,

1408
01:22:08,565 --> 01:22:12,065
Speaker 4:  but he did make one good point. Benson made it at one particularly excellent

1409
01:22:12,065 --> 01:22:15,545
Speaker 4:  point here in that the web already runs on

1410
01:22:16,225 --> 01:22:19,585
Speaker 4:  thousands if not millions of microtransactions every second because that's

1411
01:22:19,585 --> 01:22:20,505
Speaker 4:  how the ads work.

1412
01:22:20,805 --> 01:22:21,025
Speaker 6:  Yep.

1413
01:22:21,615 --> 01:22:22,585
Speaker 5:  Yeah. But it's not me.

1414
01:22:22,885 --> 01:22:24,825
Speaker 4:  All of those banner ads are, are microtransactions.

1415
01:22:25,615 --> 01:22:25,905
Speaker 5:  Sure.

1416
01:22:26,805 --> 01:22:28,305
Speaker 4:  But but it's there. It's

1417
01:22:28,365 --> 01:22:29,945
Speaker 6:  You just hate stable coins. I get it. I

1418
01:22:30,485 --> 01:22:32,105
Speaker 4:  No, but that's whatever those are that's,

1419
01:22:32,105 --> 01:22:35,985
Speaker 5:  That's sure all of that exists. But if you made me type

1420
01:22:36,005 --> 01:22:39,945
Speaker 5:  in the IP address of every web website I wanted to go to that wouldn't, that

1421
01:22:39,945 --> 01:22:41,905
Speaker 5:  doesn't work either. Like making users, you're not gonna,

1422
01:22:42,305 --> 01:22:45,635
Speaker 6:  A model's gonna do it falls apart. You'll be like, go order me a sandwich

1423
01:22:45,935 --> 01:22:47,075
Speaker 6:  and it's gonna be like, or

1424
01:22:47,175 --> 01:22:50,835
Speaker 4:  Or you have some subscription to one of these centralized indexes that needs

1425
01:22:50,835 --> 01:22:54,675
Speaker 4:  to get built and that subscription like has the weird

1426
01:22:54,675 --> 01:22:58,635
Speaker 4:  Spotify model where it's like you get a penny and you get a penny and

1427
01:22:58,635 --> 01:23:02,075
Speaker 4:  Taylor Swift gets most of the pennies and like, maybe that's how it plays

1428
01:23:02,075 --> 01:23:06,045
Speaker 4:  out. But you can see just at io in the conversations they're

1429
01:23:06,045 --> 01:23:09,725
Speaker 4:  having, the the web we have today is, is

1430
01:23:10,085 --> 01:23:13,805
Speaker 4:  reaching, its like termination point. And this Yes. New other kind of web

1431
01:23:14,345 --> 01:23:17,565
Speaker 4:  is definitely like the thing they all want to build. It's the thing Microsoft

1432
01:23:17,565 --> 01:23:20,685
Speaker 4:  wants to build. It's right. That's why they're doing NL Web. It's the thing

1433
01:23:20,685 --> 01:23:24,205
Speaker 4:  Google is just sort of openly talking about. It's,

1434
01:23:24,595 --> 01:23:28,485
Speaker 4:  it's what OpenAI needs to have happen to make its dreams realize like if

1435
01:23:28,485 --> 01:23:32,365
Speaker 4:  you wear your Johnny Ive iPod shuffle and it can't do anything,

1436
01:23:32,945 --> 01:23:36,405
Speaker 4:  you're kind of stuck, right? Like it, it needs that whole

1437
01:23:36,405 --> 01:23:39,725
Speaker 4:  ecosystem to be developed and built and for the money to all work for that

1438
01:23:39,725 --> 01:23:43,685
Speaker 4:  product to be as useful as they want it to be. And I, I think this is a marker.

1439
01:23:43,705 --> 01:23:46,245
Speaker 4:  And I think part of Google's confidence is they think they can horsepower

1440
01:23:46,355 --> 01:23:50,325
Speaker 4:  into existence while sort of preserving the best of

1441
01:23:50,325 --> 01:23:54,205
Speaker 4:  the old web and bridging the revenue into the future. I will say publishers

1442
01:23:54,385 --> 01:23:58,205
Speaker 4:  are furious about this. Yeah. Oh, they're super mad. We have a

1443
01:23:58,205 --> 01:24:01,565
Speaker 4:  statement on our site from the news media alliance

1444
01:24:01,935 --> 01:24:05,605
Speaker 4:  disclosure, Vox Media News and the News Media Alliance along with Conde Nast.

1445
01:24:05,605 --> 01:24:07,525
Speaker 4:  It's, it's the business side of the company, but

1446
01:24:07,525 --> 01:24:09,045
Speaker 6:  We're also taking money from open ai.

1447
01:24:10,065 --> 01:24:12,005
Speaker 4:  Oh, it's true. We have an open deal. It doesn't, it's a good disclosure.

1448
01:24:12,025 --> 01:24:14,765
Speaker 4:  It doesn't even occur to me. I also had a Netflix show. You should watch

1449
01:24:14,765 --> 01:24:15,125
Speaker 4:  it. It's great

1450
01:24:16,715 --> 01:24:19,645
Speaker 4:  that that's relevant, right? It's not just me bragging

1451
01:24:19,965 --> 01:24:22,405
Speaker 6:  Google wanted to buy Netflix. That that's true. Google,

1452
01:24:22,455 --> 01:24:25,645
Speaker 4:  There you go. Sooner announced that Google and wanted to buy Netflix at one

1453
01:24:25,645 --> 01:24:25,765
Speaker 4:  point,

1454
01:24:27,705 --> 01:24:31,205
Speaker 4:  but the publishers are like, the whole point of this deal was us getting

1455
01:24:31,205 --> 01:24:34,965
Speaker 4:  traffic and now that's gone. Yeah. And then they called it theft. Yeah. Like

1456
01:24:34,965 --> 01:24:38,525
Speaker 4:  that is, that is what the publisher industry, the news industry is saying

1457
01:24:38,525 --> 01:24:42,405
Speaker 4:  about AI mode. And I think Google is like, well, it'll be

1458
01:24:42,405 --> 01:24:46,285
Speaker 4:  sad when you're dead. Like that's kind of how it feels. Yeah. And I, I,

1459
01:24:46,485 --> 01:24:49,045
Speaker 4:  I think that will get litigated even harder. Not to mention the fact that

1460
01:24:49,045 --> 01:24:51,725
Speaker 4:  they might have to sell Chrome, which I don't know what will happen if they

1461
01:24:51,725 --> 01:24:52,005
Speaker 4:  have to sell Chrome.

1462
01:24:52,045 --> 01:24:54,765
Speaker 6:  I don't think they're gonna have to sell Chrome. No one, I got no vibe that

1463
01:24:54,825 --> 01:24:57,565
Speaker 6:  anyone was worried about Chrome at io. Well,

1464
01:24:57,565 --> 01:24:59,925
Speaker 4:  What are they gonna, they're not gonna like wander around their own party

1465
01:24:59,925 --> 01:25:01,445
Speaker 4:  being like, well this is gonna suck.

1466
01:25:01,545 --> 01:25:04,005
Speaker 6:  No, no, no. But like, you know, like when you're, when you're actually like

1467
01:25:04,005 --> 01:25:07,165
Speaker 6:  having a drink and you're staring at someone's face a few feet away and you

1468
01:25:07,165 --> 01:25:10,485
Speaker 6:  ask them like, are you worried about Chrome? Like yeah, you're trained, but

1469
01:25:10,485 --> 01:25:14,125
Speaker 6:  like you do it enough and you kind of get a vibe. And I did this on a few

1470
01:25:14,125 --> 01:25:17,845
Speaker 6:  topics and like, I was very curious like, are they worried about this?

1471
01:25:18,595 --> 01:25:21,805
Speaker 6:  Like up and down the, the chain. And

1472
01:25:22,765 --> 01:25:26,325
Speaker 6:  I think they know they're gonna have to stop just like

1473
01:25:26,895 --> 01:25:30,285
Speaker 6:  mafia style buying out search distribution, like that's gonna go away.

1474
01:25:30,945 --> 01:25:34,765
Speaker 6:  But do they think they're still the best search engine? Yeah. So

1475
01:25:35,245 --> 01:25:37,445
Speaker 6:  I don't think they think they're gonna have a divestiture. And that's another

1476
01:25:37,445 --> 01:25:40,845
Speaker 6:  topic. I mean, on the website thing, I'm curious what you guys think about

1477
01:25:40,845 --> 01:25:44,605
Speaker 6:  this. Our website's just going to become like driving a vintage car.

1478
01:25:44,715 --> 01:25:48,525
Speaker 6:  It's gonna be this thing you do because it's a luxury. It feels

1479
01:25:48,525 --> 01:25:52,245
Speaker 6:  good. It's like a really bespoke, unique like visceral experience.

1480
01:25:52,795 --> 01:25:56,725
Speaker 6:  It's not as efficient, but you do it and you spend more money. Like,

1481
01:25:57,045 --> 01:25:59,245
Speaker 6:  I kind of feel like that's the direction websites are gonna go.

1482
01:25:59,525 --> 01:26:02,645
Speaker 4:  I do like thinking of The Verge dot com as like a cranky vintage Jaguar.

1483
01:26:02,795 --> 01:26:06,685
Speaker 4:  Yeah. Right. It's kind of always broken, but you love it. It's gonna be great.

1484
01:26:06,685 --> 01:26:09,605
Speaker 4:  Yeah. I don't know. I, there's other kinds of web,

1485
01:26:10,425 --> 01:26:13,915
Speaker 4:  like, you know, Substack and Ghost are, are web properties, blue Sky,

1486
01:26:14,145 --> 01:26:17,595
Speaker 4:  like at Protocol built on a lot of web ideas. And

1487
01:26:18,215 --> 01:26:22,195
Speaker 4:  the thing about Google is that all of these websites are effectively addicted

1488
01:26:22,215 --> 01:26:25,675
Speaker 4:  to Google as distribution. Right? That's why SEO has polluted the web so

1489
01:26:25,675 --> 01:26:29,155
Speaker 4:  badly. There wasn't another choice. And so like, yeah, maybe agents are gonna

1490
01:26:29,155 --> 01:26:32,955
Speaker 4:  turn everyone in a day basis. Like I think if you're a retailer, if you are

1491
01:26:32,955 --> 01:26:36,875
Speaker 4:  Walmart or Target or Macy's or whatever, and you're looking at Google's

1492
01:26:36,875 --> 01:26:40,725
Speaker 4:  new try on mode, which looks very cool, like the promise there

1493
01:26:40,725 --> 01:26:44,485
Speaker 4:  is, once you've tried on the clothes, virtually the

1494
01:26:44,485 --> 01:26:48,395
Speaker 4:  demo was you set a price and then at some point Google just

1495
01:26:48,395 --> 01:26:52,155
Speaker 4:  buys the clothes when it hits a sale. If you're a Target, you just totally

1496
01:26:52,215 --> 01:26:55,835
Speaker 4:  got disintermediated. Now you're, now you're just doing high frequency trading

1497
01:26:55,835 --> 01:26:59,435
Speaker 4:  with a Google bond. Like this is a really weird business to be in.

1498
01:26:59,655 --> 01:27:02,115
Speaker 4:  And Google's kind of answer there It is. Like, oh, well, we'll talk to them

1499
01:27:02,115 --> 01:27:06,075
Speaker 4:  and make sure that we show some of their webpages sometimes. And I said,

1500
01:27:06,075 --> 01:27:08,635
Speaker 4:  well, what's that negotiation gonna look like? And Google's like, it won't

1501
01:27:08,635 --> 01:27:12,355
Speaker 4:  be a negotiation, it'll be a conversation. This is ice cold,

1502
01:27:12,485 --> 01:27:16,155
Speaker 4:  right? Like there's a whole universe of businesses that

1503
01:27:16,695 --> 01:27:20,155
Speaker 4:  are gonna have to get reconfigured and a lot of 'em are gonna wanna say,

1504
01:27:20,155 --> 01:27:22,195
Speaker 4:  well actually we don't want anything to do with the salt. We want you to

1505
01:27:22,195 --> 01:27:25,555
Speaker 4:  come to our website Substack for, you know, it's many, many faults

1506
01:27:26,055 --> 01:27:29,555
Speaker 4:  is a web property. It's expressed on the web. There's not like a Substack

1507
01:27:30,145 --> 01:27:33,515
Speaker 4:  desktop app. And the innovation is they've solved distribution inside of

1508
01:27:33,515 --> 01:27:36,715
Speaker 4:  their own middle network. And that is really, really lucrative for a lot

1509
01:27:36,715 --> 01:27:37,515
Speaker 4:  of people on Substack.

1510
01:27:37,655 --> 01:27:40,915
Speaker 5:  All of that is the optimistic case, right? The the pessimistic case is, is

1511
01:27:40,915 --> 01:27:44,715
Speaker 5:  yes, websites die and everything becomes the high frequency

1512
01:27:44,715 --> 01:27:48,035
Speaker 5:  trading. You're, you're essentially fighting for Google scraps. The upside

1513
01:27:48,035 --> 01:27:50,675
Speaker 5:  is, for the first time in two decades, everybody's gonna have to care about

1514
01:27:50,675 --> 01:27:53,875
Speaker 5:  their website because it has to be, I mean this is, this is The Verge bet,

1515
01:27:53,875 --> 01:27:56,435
Speaker 5:  right? Like this is, this is the thing we've been talking about for a while

1516
01:27:56,435 --> 01:28:00,355
Speaker 5:  that like, the bet is we have to make something that

1517
01:28:00,355 --> 01:28:03,835
Speaker 5:  you come to on purpose or else we can't be that upset when nobody comes.

1518
01:28:04,675 --> 01:28:07,835
Speaker 5:  I think I think maybe that's a place we're gonna land. And a and a lot of

1519
01:28:07,835 --> 01:28:09,635
Speaker 5:  that's gonna make things harder for a lot of the way. There's gonna be a

1520
01:28:09,635 --> 01:28:13,555
Speaker 5:  lot of pain along the way. But like, if, if we do

1521
01:28:13,555 --> 01:28:14,995
Speaker 5:  this correctly,

1522
01:28:16,635 --> 01:28:20,035
Speaker 5:  I think, and the, the, this guy Guha, who I was talking to about all of the

1523
01:28:20,135 --> 01:28:23,515
Speaker 5:  NL web stuff, he's like, the fact that the web trends towards

1524
01:28:23,515 --> 01:28:27,355
Speaker 5:  centralization is bad and it has been bad several generations

1525
01:28:27,355 --> 01:28:30,995
Speaker 5:  in a row. And we have to stop it. And the only way to stop It is to distribute

1526
01:28:30,995 --> 01:28:34,875
Speaker 5:  the good technology everywhere. And I think there, like the way

1527
01:28:34,875 --> 01:28:38,635
Speaker 5:  we think about like the, the federated social networks, the way we think

1528
01:28:38,635 --> 01:28:42,555
Speaker 5:  about MCP, the way we think about NL Web, like bringing some of this AI stuff

1529
01:28:42,555 --> 01:28:45,955
Speaker 5:  to websites is like, maybe there's a chance that actually this ecosystem

1530
01:28:45,955 --> 01:28:49,355
Speaker 5:  gets bigger. Instead of you just going to google.com landing on a webpage

1531
01:28:49,355 --> 01:28:53,025
Speaker 5:  you've never heard of and then never going back because like that's what

1532
01:28:53,025 --> 01:28:56,305
Speaker 5:  we've been doing. But you could argue that's not the correct outcome,

1533
01:28:56,925 --> 01:28:59,105
Speaker 5:  but it's like it's an outcome.

1534
01:28:59,705 --> 01:29:03,485
Speaker 4:  I mean, what is this index of MCP servers accept

1535
01:29:03,485 --> 01:29:06,405
Speaker 4:  incredible centralization on the web that will reemerge. Right, right.

1536
01:29:06,525 --> 01:29:09,565
Speaker 5:  I mean, there's a reason the AI labs are the ones doing this, right? Like

1537
01:29:11,115 --> 01:29:13,965
Speaker 5:  this is not Tim Burner's lead being like, this is great for the internet.

1538
01:29:13,965 --> 01:29:17,085
Speaker 5:  This is anthropic being like, this is how we get internet access. Yeah.

1539
01:29:17,155 --> 01:29:20,045
Speaker 4:  Because they can't, I mean, you know, their first shot at it was what if

1540
01:29:20,045 --> 01:29:22,685
Speaker 4:  we click around your website? Right? Which is just incredibly brittle. Like,

1541
01:29:22,685 --> 01:29:26,645
Speaker 4:  it, it just, it's never, that is, that is never gonna work. That will

1542
01:29:26,645 --> 01:29:29,925
Speaker 4:  be the backup plan for all of this. And MCP will be plan A.

1543
01:29:30,695 --> 01:29:34,485
Speaker 4:  We'll see, I again, we like, we walked around Google io and they were

1544
01:29:34,505 --> 01:29:38,125
Speaker 4:  all feeling it like they're riding hide. Like their theme they kept saying

1545
01:29:38,125 --> 01:29:41,845
Speaker 4:  was research and reality. And it's like, what they mean is you didn't believe

1546
01:29:41,845 --> 01:29:45,475
Speaker 4:  us for a long time that we had all this stuff in the labs that we were working

1547
01:29:45,475 --> 01:29:48,635
Speaker 4:  on it, that it was coming to fruition. And now here all like all at once,

1548
01:29:48,635 --> 01:29:52,475
Speaker 4:  here's all of it. And you know, so it has to ship

1549
01:29:52,535 --> 01:29:56,115
Speaker 4:  and, and you know, people are, I'm confident we're gonna get feedback on

1550
01:29:56,115 --> 01:29:59,275
Speaker 4:  this part of the episode that's like they're stealing our jobs and boiling

1551
01:29:59,275 --> 01:30:02,995
Speaker 4:  the ocean. There are some really weird downsides to all this stuff

1552
01:30:02,995 --> 01:30:03,315
Speaker 4:  happening.

1553
01:30:03,345 --> 01:30:06,275
Speaker 5:  Yeah. Both of those things are true by the way. They're they're true. They

1554
01:30:06,275 --> 01:30:06,555
Speaker 5:  are true.

1555
01:30:07,815 --> 01:30:11,435
Speaker 4:  But the weirdest one is that we are staring at the beginning of a

1556
01:30:11,435 --> 01:30:14,595
Speaker 4:  wholesale re-architecture of the internet. Yep. And that

1557
01:30:15,255 --> 01:30:17,835
Speaker 4:  we should probably pay a little more attention to that to make sure it's

1558
01:30:17,835 --> 01:30:21,795
Speaker 4:  good this time because last time like four people just got to be in charge

1559
01:30:21,795 --> 01:30:22,235
Speaker 4:  of everything.

1560
01:30:22,745 --> 01:30:23,555
Speaker 5:  Yeah. If

1561
01:30:23,555 --> 01:30:27,035
Speaker 4:  We, and like I might take the trade off if you can actually

1562
01:30:27,035 --> 01:30:30,035
Speaker 4:  decentralized a little bit more. We'll see. All right. We gotta take a break.

1563
01:30:30,045 --> 01:30:32,555
Speaker 4:  We're gonna come back. We the lighting round. We'll right back.

1564
01:31:25,245 --> 01:31:25,965
Speaker 4:  off this week. Next

1565
01:31:25,965 --> 01:31:27,405
Speaker 5:  Week it'll say Sponsored for flavor

1566
01:31:29,145 --> 01:31:30,525
Speaker 5:  and then we'll make new t-shirts.

1567
01:31:30,835 --> 01:31:34,685
Speaker 4:  It's gonna be Doritos, it's gonna be great. You guys actually, does anyone

1568
01:31:34,685 --> 01:31:36,605
Speaker 4:  know anyone at Doritos? Listen, we should reach out.

1569
01:31:36,745 --> 01:31:40,645
Speaker 5:  Yes, I will eat Doritos constantly through an entire first cast

1570
01:31:40,745 --> 01:31:41,965
Speaker 5:  if Doritos would like to pay for

1571
01:31:41,965 --> 01:31:45,405
Speaker 4:  That. It's funny, you know, our great rivals for podcasts within a podcast,

1572
01:31:46,065 --> 01:31:49,005
Speaker 4:  my brother, my brother and me, they were once sponsored, I believe, by to

1573
01:31:49,205 --> 01:31:50,245
Speaker 4:  Tostitos Pizza rolls.

1574
01:31:50,355 --> 01:31:54,165
Speaker 5:  They were, they made a whole Totinos episode and it was excellent

1575
01:31:54,165 --> 01:31:57,165
Speaker 5:  content, if that makes sense. So myself, we've got a lot to live up to on

1576
01:31:57,165 --> 01:31:57,445
Speaker 5:  that front.

1577
01:31:57,585 --> 01:32:00,885
Speaker 4:  We do, we will never do that. I I just wanna be clear, you can't pay us money

1578
01:32:00,885 --> 01:32:02,525
Speaker 4:  to tell us what to do. That's where the flavor comes

1579
01:32:02,525 --> 01:32:05,645
Speaker 5:  From. I guess in this case this would be Brendan Carr, the Dummy brought

1580
01:32:05,645 --> 01:32:06,565
Speaker 5:  to you by the FCC.

1581
01:32:08,645 --> 01:32:10,085
Speaker 4:  Speaking of which it's time David,

1582
01:32:10,215 --> 01:32:13,875
Speaker 5:  Let's do this one more time just for this week and then

1583
01:32:13,875 --> 01:32:17,755
Speaker 5:  hopefully never again. Every week I root for it to be the last

1584
01:32:17,755 --> 01:32:19,595
Speaker 5:  version of Bread and Carr as a Dummy for a

1585
01:32:19,595 --> 01:32:20,315
Speaker 4:  Lot of reasons he

1586
01:32:20,315 --> 01:32:23,475
Speaker 5:  Won't. And every week It is not, It is time, once again for

1587
01:32:23,535 --> 01:32:26,635
Speaker 5:  2026 Webby award-winning podcast within a podcast

1588
01:32:27,485 --> 01:32:30,355
Speaker 5:  bread and a Carr is a Dummy neela, you've slept for three hours. I expect

1589
01:32:30,355 --> 01:32:31,555
Speaker 5:  this one to be very, very good.

1590
01:32:31,745 --> 01:32:34,275
Speaker 4:  There's, there's only, there's only two items in this one. The first one

1591
01:32:34,335 --> 01:32:35,675
Speaker 4:  is so stupid,

1592
01:32:37,305 --> 01:32:41,235
Speaker 4:  like, so, so stupid. And the other one is so disappointing and that's

1593
01:32:41,235 --> 01:32:44,515
Speaker 4:  really the heart of Brennan Carr is a Dummy, right? How can you disappoint

1594
01:32:44,515 --> 01:32:47,795
Speaker 4:  me in ever more stupid ways? So this week, Brennan Carr, America's number

1595
01:32:47,795 --> 01:32:51,555
Speaker 4:  one sensor who continues to make such a splash

1596
01:32:52,095 --> 01:32:55,915
Speaker 4:  as a wood, be speech cop that like more and more

1597
01:32:55,955 --> 01:32:59,915
Speaker 4:  profiles in coverage of him emerge. Politico wrote a profile this week.

1598
01:33:00,305 --> 01:33:04,115
Speaker 4:  John Oliver did an entire segment that is basically Brendan Carr is a

1599
01:33:04,115 --> 01:33:07,315
Speaker 4:  Dummy. Thank you to John Oliver for validating us and thank, thank you everyone

1600
01:33:07,315 --> 01:33:08,155
Speaker 4:  who sent that in. He

1601
01:33:08,155 --> 01:33:11,155
Speaker 5:  Is a real test of all presses. Good press. Our, our buddy Brendan,

1602
01:33:11,385 --> 01:33:15,315
Speaker 4:  It's not, you're a traitorous. More on Brendan. And I know this because this

1603
01:33:15,315 --> 01:33:18,675
Speaker 4:  week he has started to open an investigation

1604
01:33:19,025 --> 01:33:23,005
Speaker 4:  into NBC over Kamala Harris appearing on Saturday Night Live

1605
01:33:25,155 --> 01:33:26,465
Speaker 4:  disclosure NBC

1606
01:33:28,185 --> 01:33:31,615
Speaker 4:  investor and our parent company Vox Media. Just putting that out there you

1607
01:33:31,615 --> 01:33:35,375
Speaker 4:  can do it. That what you will again air mind you, no one can pay me to say

1608
01:33:35,535 --> 01:33:37,855
Speaker 4:  anything. Stop where subscriptions are on sale.

1609
01:33:40,035 --> 01:33:43,715
Speaker 4:  Hey, do you guys know who won the election? Do you remember who it was?

1610
01:33:44,595 --> 01:33:45,405
Speaker 4:  That doesn't matter.

1611
01:33:45,725 --> 01:33:47,085
Speaker 5:  I forget. Yeah, it's, it's so

1612
01:33:47,085 --> 01:33:50,045
Speaker 4:  Long ago. It is. Do you, do you remember who lost definitively lost and is

1613
01:33:50,045 --> 01:33:54,005
Speaker 4:  not the president? Do, do you know who, who that is? It was Kamel here. I

1614
01:33:54,005 --> 01:33:57,755
Speaker 4:  believe that she lost right? She right. Yeah. She did not win. We can

1615
01:33:57,755 --> 01:33:59,635
Speaker 4:  agree that she's not currently the president of the United States, correct.

1616
01:33:59,635 --> 01:34:03,435
Speaker 4:  Yes. The other guys. Okay, well Brendan here in

1617
01:34:03,585 --> 01:34:07,515
Speaker 4:  mid-May of 2025 has decided the most pressing

1618
01:34:07,515 --> 01:34:10,595
Speaker 4:  speech issue in the country is whether Saturday Night Live

1619
01:34:11,715 --> 01:34:15,555
Speaker 4:  violated the equal time rule for broadcast television stations

1620
01:34:15,975 --> 01:34:18,275
Speaker 4:  by having Kamala Harris appear on on the show.

1621
01:34:20,535 --> 01:34:24,355
Speaker 4:  Do you know what I also know definitively is that Brendan knows they didn't.

1622
01:34:25,455 --> 01:34:29,275
Speaker 4:  But yet he has decided here in May of 2025 to reopen an

1623
01:34:29,555 --> 01:34:32,685
Speaker 4:  investigation which was closed into whether

1624
01:34:33,085 --> 01:34:37,005
Speaker 4:  NBC and Saturn Night Live violated the equal time rule by having Kamala

1625
01:34:37,005 --> 01:34:40,565
Speaker 4:  Harris and Saturn live. The equal time rule is like an archaic broadcast

1626
01:34:40,565 --> 01:34:44,445
Speaker 4:  rule. It says if a broadcaster that uses the public airwaves

1627
01:34:44,445 --> 01:34:48,435
Speaker 4:  to broadcast content gives time to one candidate, they gotta give equal time

1628
01:34:48,435 --> 01:34:50,755
Speaker 4:  to other candidate. It's a little more complicated that, but that's right.

1629
01:34:50,755 --> 01:34:51,995
Speaker 4:  It's called the Equal time rule,

1630
01:34:52,045 --> 01:34:53,715
Speaker 5:  Which NBC did the next day.

1631
01:34:53,915 --> 01:34:57,845
Speaker 4:  NBC had Donald Trump on the next day. He got a bunch of time during NASCAR

1632
01:34:57,845 --> 01:34:58,045
Speaker 4:  race.

1633
01:35:00,065 --> 01:35:03,065
Speaker 4:  I reported on this while it was happening, right. 'cause Harris showed up

1634
01:35:03,065 --> 01:35:06,865
Speaker 4:  on SNL. There was a bunch of like MAGA tweets and then Brendan was

1635
01:35:06,865 --> 01:35:09,785
Speaker 4:  Brendan before he was auditioning for the job. He was like, I will arrest

1636
01:35:10,025 --> 01:35:13,145
Speaker 4:  everyone when I made the FCC Commissioner. Great.

1637
01:35:13,905 --> 01:35:17,555
Speaker 4:  I reported it out at the time. And what I heard from

1638
01:35:17,665 --> 01:35:20,915
Speaker 4:  sources in the FCC was the only

1639
01:35:21,465 --> 01:35:25,195
Speaker 4:  program that regularly deals with this is Saturday Night Live.

1640
01:35:25,225 --> 01:35:29,155
Speaker 4:  They are the best at it because politicians are always showing up on Saturday

1641
01:35:29,155 --> 01:35:32,595
Speaker 4:  Night Live. Oh, interesting. Like they have the infrastructure for dealing

1642
01:35:32,595 --> 01:35:36,495
Speaker 4:  with the equal time rule. 'cause no one else gives a shit. So of

1643
01:35:36,495 --> 01:35:40,335
Speaker 4:  course they filed the equal time notice of the FCC and of course they offered

1644
01:35:40,335 --> 01:35:43,935
Speaker 4:  Donald Trump time the next day because they're good at it. Like, this is

1645
01:35:43,975 --> 01:35:47,775
Speaker 4:  a thing Saturday Night Live is the best at of all of the shows on all of

1646
01:35:47,775 --> 01:35:51,595
Speaker 4:  the broadcast networks. And so sure enough, Brendan,

1647
01:35:52,265 --> 01:35:55,595
Speaker 4:  there's, you know, his emails were FOIA because he was threatening this in

1648
01:35:55,625 --> 01:35:58,755
Speaker 4:  this, this investigation before he was ever made the FCC Commissioner. And

1649
01:35:58,775 --> 01:36:02,595
Speaker 4:  he emailed Fox News producers, here are the, here's the

1650
01:36:02,725 --> 01:36:06,315
Speaker 4:  equal time request from NBC Saturday Live.

1651
01:36:06,505 --> 01:36:09,635
Speaker 4:  Like he, he know, I know he knows it 'cause I'm reading his email saying

1652
01:36:09,635 --> 01:36:13,595
Speaker 4:  he knows it. They complied. Former FCC chair Jessica Rosen

1653
01:36:13,595 --> 01:36:16,955
Speaker 4:  war. So looked at this and dismissed this 'cause of they had complied, you

1654
01:36:16,955 --> 01:36:19,435
Speaker 4:  know, and I know they complied 'cause Trump was on the air during the NASCAR

1655
01:36:19,435 --> 01:36:23,425
Speaker 4:  race. The next day, E equal time was, was given. Well

1656
01:36:23,425 --> 01:36:27,225
Speaker 4:  here we are. May, 2025 election has been won. We have accepted a

1657
01:36:27,225 --> 01:36:30,705
Speaker 4:  jet from the government of Qatar. Like we're definitively passed

1658
01:36:31,505 --> 01:36:34,545
Speaker 4:  Saturday Night Live booking Kamala Harris having any effect on anything that

1659
01:36:34,545 --> 01:36:38,505
Speaker 4:  happens in the country. And Brendan is running around trying to punish

1660
01:36:39,145 --> 01:36:42,625
Speaker 4:  NBC for violating the equal time rule finding ways legally

1661
01:36:43,205 --> 01:36:47,025
Speaker 4:  to reopen this investigation. Just to put more pressure

1662
01:36:47,285 --> 01:36:51,105
Speaker 4:  on NBC for its alleged coverage

1663
01:36:51,305 --> 01:36:54,625
Speaker 4:  of Donald Trump being lost. This is just speech police stuff.

1664
01:36:55,335 --> 01:36:59,245
Speaker 4:  Like it's, it's over, like the election is over.

1665
01:36:59,645 --> 01:37:03,485
Speaker 4:  Whatever harm you may have felt, the Trump campaign felt that Carr

1666
01:37:03,485 --> 01:37:07,045
Speaker 4:  felt for his, his boss and Buddy Donald. It's over.

1667
01:37:07,275 --> 01:37:11,085
Speaker 4:  They, they won but like the harm has passed. Right.

1668
01:37:11,145 --> 01:37:14,005
Speaker 4:  The, the opportunity for that to sway the election has come and gone and

1669
01:37:14,005 --> 01:37:17,885
Speaker 4:  it didn't work. And he knows, we can see

1670
01:37:17,885 --> 01:37:21,735
Speaker 4:  his emails, we reported on his emails that he knows

1671
01:37:21,805 --> 01:37:24,815
Speaker 4:  that they didn't break the rule and yet he's trying to reopen. I mean this

1672
01:37:24,815 --> 01:37:28,735
Speaker 4:  is just moronic, right? Like he's doing it to be a cop, to be,

1673
01:37:28,915 --> 01:37:32,455
Speaker 4:  to chill the speech of these news divisions that are reporting on Donald Trump.

1674
01:37:32,905 --> 01:37:36,815
Speaker 4:  Maybe 'cause NBC owns MS NBC even though Comcast is spinning that off into

1675
01:37:36,815 --> 01:37:40,535
Speaker 4:  a new company called NT I haven't even talked about yet. But like that's

1676
01:37:40,535 --> 01:37:44,445
Speaker 4:  why he is doing it so stupid. So like I will,

1677
01:37:44,725 --> 01:37:48,085
Speaker 4:  I will, we'll put it in the show notes. We have his emails where he's telling

1678
01:37:48,225 --> 01:37:51,525
Speaker 4:  Fox producers, they complied with the rules and yet he's gonna reopen this

1679
01:37:51,805 --> 01:37:54,725
Speaker 4:  investigation just 'cause he wants to be a cop. He's not even trying to hide

1680
01:37:54,725 --> 01:37:56,885
Speaker 5:  The fact that this is what it's about. It's very straightforward.

1681
01:37:57,115 --> 01:37:59,885
Speaker 4:  It's very straightforward. So that's one that's very stupid.

1682
01:38:01,005 --> 01:38:04,645
Speaker 4:  Here's the very disappointing one. We've talked a lot about how

1683
01:38:04,645 --> 01:38:08,445
Speaker 4:  Brendan threatens deals that are in the pipelines and tries to

1684
01:38:09,205 --> 01:38:11,965
Speaker 4:  companies to comply with whatever he wants. Whether it's their coverage,

1685
01:38:11,965 --> 01:38:15,845
Speaker 4:  which he says is biased, whether it's DEI policies, which he says are not

1686
01:38:15,845 --> 01:38:19,045
Speaker 4:  racist enough, whatever It is, he gets what he wants by holding up deals.

1687
01:38:19,665 --> 01:38:23,445
Speaker 4:  So last week Verizon completed its acquisition of

1688
01:38:23,685 --> 01:38:27,245
Speaker 4:  Frontier, another fiber provider. And it got that through the

1689
01:38:27,285 --> 01:38:31,235
Speaker 4:  FCC by sending a letter to Brandon Carr saying it was walking

1690
01:38:31,235 --> 01:38:32,315
Speaker 4:  back all of its DI efforts.

1691
01:38:33,885 --> 01:38:37,575
Speaker 4:  Super disappointing, right? That Verizon just rolled and said we're gonna

1692
01:38:37,575 --> 01:38:41,085
Speaker 4:  give up on our DEI efforts. This is Brendan at work,

1693
01:38:41,615 --> 01:38:44,725
Speaker 4:  right? He, he, he went and threatened the business over something has nothing

1694
01:38:44,725 --> 01:38:48,085
Speaker 4:  to do with the business and Verizon caved like, just

1695
01:38:48,325 --> 01:38:50,805
Speaker 4:  straightforwardly caved and said this is a government regulation that we're

1696
01:38:50,805 --> 01:38:54,645
Speaker 4:  gonna deal with even though the FCC telling us who to hire

1697
01:38:54,665 --> 01:38:58,125
Speaker 4:  and fire is kind of a straightforward infringement of its like rights as

1698
01:38:58,165 --> 01:39:01,435
Speaker 4:  a business. And the thing that is particularly disappointing to me

1699
01:39:01,975 --> 01:39:05,515
Speaker 4:  is that whenever the FCC has tried to do something good to Verizon,

1700
01:39:05,865 --> 01:39:09,115
Speaker 4:  they have fought it tooth and nail. They're the primary legal antagonist

1701
01:39:09,215 --> 01:39:12,235
Speaker 4:  who net neutrality for a decade. Every court case,

1702
01:39:12,925 --> 01:39:16,915
Speaker 4:  every challenge. Verizon was there. Whenever you talk about

1703
01:39:17,805 --> 01:39:21,395
Speaker 4:  increasing broadband access, Verizon fights that stuff tooth and nail. Verizon

1704
01:39:21,395 --> 01:39:25,375
Speaker 4:  takes money and promises Fios will be rolled out in New Jersey and

1705
01:39:25,375 --> 01:39:28,215
Speaker 4:  they just never do it. And then they get sued and they fight it tooth and

1706
01:39:28,215 --> 01:39:32,015
Speaker 4:  nail. We had the, the CEO of Verizon Wireless on decoder. I don't know

1707
01:39:32,235 --> 01:39:36,175
Speaker 4:  why enterprise software CEOs or telecom CEOs show up

1708
01:39:36,175 --> 01:39:39,735
Speaker 4:  on the show, but they just, they came on the show and I asked him, are you

1709
01:39:39,735 --> 01:39:43,295
Speaker 4:  gonna fight Brendan Carr in these weird regulations? As hard as you have

1710
01:39:43,295 --> 01:39:46,215
Speaker 4:  fought net neutral, you just run the clip. I wanna be very clear, very explicit

1711
01:39:46,215 --> 01:39:49,495
Speaker 4:  about this. When the government passed net neutrality rules, it wasn't, we

1712
01:39:49,655 --> 01:39:52,535
Speaker 4:  have to follow the rules of the land. It was we are going to file lawsuits

1713
01:39:52,795 --> 01:39:56,055
Speaker 4:  for a decade to get out of these rules 'cause we think they're dumb. And

1714
01:39:56,055 --> 01:39:59,495
Speaker 4:  in this case you're saying Brendan Carr who has been openly censorious,

1715
01:39:59,755 --> 01:40:03,255
Speaker 4:  openly chilling of speech, openly hostile to companies because they have

1716
01:40:03,255 --> 01:40:06,135
Speaker 4:  diversity initiatives. You're saying you just have to follow his rules. We

1717
01:40:06,135 --> 01:40:06,255
Speaker 4:  have

1718
01:40:06,255 --> 01:40:09,135
Speaker 10:  To follow the rules of the land. I don't think those are his rules. Those

1719
01:40:09,135 --> 01:40:10,815
Speaker 10:  are rules of the land and administration. Are

1720
01:40:10,815 --> 01:40:12,815
Speaker 4:  You gonna file a decade worth of lawsuits about these rules?

1721
01:40:13,035 --> 01:40:16,175
Speaker 10:  We don't know. We are gonna work constructively with them to follow rules

1722
01:40:16,175 --> 01:40:19,455
Speaker 10:  that are needed. But at the end of the day, look, our role is to ask stakeholders

1723
01:40:19,485 --> 01:40:23,455
Speaker 10:  that we have, you know, my stakeholders are my shareholders, my customers,

1724
01:40:23,755 --> 01:40:27,655
Speaker 10:  my employees and society at large. We have to manage and we are gonna deliver

1725
01:40:27,675 --> 01:40:31,095
Speaker 10:  for those stakeholders what's needed for us. And we will do whatever's needed

1726
01:40:31,095 --> 01:40:33,935
Speaker 10:  with the administration to deliver to all the stakeholders. It's a balance.

1727
01:40:33,935 --> 01:40:36,615
Speaker 10:  When you run a large company our size, you have to balance the different

1728
01:40:36,615 --> 01:40:39,895
Speaker 10:  stakeholders and we will balance those stakeholders. We've done it extremely

1729
01:40:39,895 --> 01:40:43,215
Speaker 10:  well in the last 25 years that we've been Verizon and we'll continue to

1730
01:40:43,215 --> 01:40:43,975
Speaker 10:  do that going forward.

1731
01:40:45,005 --> 01:40:47,015
Speaker 4:  Yeah. To balance the stakeholders guys, we have

1732
01:40:47,735 --> 01:40:51,655
Speaker 5:  A comprehensive view and we have balancing the stakeholders and you can

1733
01:40:51,655 --> 01:40:55,605
Speaker 5:  get out of almost anything that sucks. Yep. That is 60 seconds

1734
01:40:55,665 --> 01:40:58,965
Speaker 5:  of him being like, yeah, when he says those aren't Brendan Carr's rules,

1735
01:40:58,965 --> 01:41:01,965
Speaker 5:  those are the rules of the land. That sucks.

1736
01:41:02,225 --> 01:41:04,685
Speaker 4:  That's a, I mean that's straightforwardly a lie. Yeah. Those are Brendan

1737
01:41:04,685 --> 01:41:08,565
Speaker 4:  Carr's rules. He said, I'm gonna hold up this deal unless you walk back your

1738
01:41:08,705 --> 01:41:12,405
Speaker 4:  DEI initiatives and Verizon instead of fighting, which they

1739
01:41:12,405 --> 01:41:16,125
Speaker 4:  always do when it comes to consumer protection or broadband deployment,

1740
01:41:16,275 --> 01:41:19,965
Speaker 4:  they always fight here. They rolled and

1741
01:41:20,395 --> 01:41:24,245
Speaker 4:  it's, it worked. And the thing that kills me is it doesn't have to work.

1742
01:41:24,955 --> 01:41:28,045
Speaker 4:  Like if you stand up to these bullies in particular, they tend to roll over

1743
01:41:28,945 --> 01:41:32,655
Speaker 4:  and br Brendan getting a win is just like the most disappointing

1744
01:41:32,655 --> 01:41:36,375
Speaker 4:  outcome. And it's particularly funny 'cause it's, Verizon always

1745
01:41:36,375 --> 01:41:39,965
Speaker 4:  fights, Verizon looks at government regulation and is like, no, no, no, no,

1746
01:41:39,965 --> 01:41:43,845
Speaker 4:  no. One decade. And here they rolled. And I just think, I think it

1747
01:41:43,845 --> 01:41:47,645
Speaker 4:  sucks. I am, I am incredibly disappointed that it worked in this way. I'm

1748
01:41:47,645 --> 01:41:51,445
Speaker 4:  also, you know, not for nothing. It's another merger of

1749
01:41:51,795 --> 01:41:54,885
Speaker 4:  ISPs and it will result not in greater service and lower prices.

1750
01:41:56,525 --> 01:41:59,265
Speaker 4:  You, you know how it's gonna go. Yeah. Like we all know how this is gonna

1751
01:41:59,265 --> 01:42:02,185
Speaker 5:  Go. I was wondering about this in the context of there was news this week

1752
01:42:02,185 --> 01:42:05,185
Speaker 5:  that at and t is paying I think five and

1753
01:42:05,255 --> 01:42:09,105
Speaker 5:  $5.75 billion to buy lumens, fiber business

1754
01:42:09,765 --> 01:42:13,465
Speaker 5:  and a bunch of cities all over the place. And the timing of this is so

1755
01:42:13,465 --> 01:42:16,905
Speaker 5:  suspicious to me because it really seems like at t looked at this and said,

1756
01:42:16,925 --> 01:42:20,225
Speaker 5:  oh, all we have to do is write a letter about DEI and this'll get done.

1757
01:42:20,765 --> 01:42:20,985
Speaker 4:  Yep.

1758
01:42:21,325 --> 01:42:24,865
Speaker 5:  And I'm like, I hope that's not how this goes, but it sure seems like at

1759
01:42:24,865 --> 01:42:28,065
Speaker 5:  and t is about to just run the same playbook in order to get this thing done.

1760
01:42:28,865 --> 01:42:32,785
Speaker 4:  I think there's a, a renewed sense that some mergers are allowed to

1761
01:42:32,785 --> 01:42:36,505
Speaker 4:  happen. You know, there was a lot of excitement even in the tech

1762
01:42:36,705 --> 01:42:39,425
Speaker 4:  industry, right. That like deals are back. Yeah. Biden and Le Kahn, were

1763
01:42:39,425 --> 01:42:42,185
Speaker 4:  not gonna let any deals happen and now the deals are back. And that actually

1764
01:42:42,185 --> 01:42:46,015
Speaker 4:  wasn't the case early on. Like Andrew Ferguson, the new

1765
01:42:46,015 --> 01:42:49,855
Speaker 4:  chair of the FTC is kind of a trust buster. He kind of hates big companies.

1766
01:42:49,855 --> 01:42:53,835
Speaker 4:  He hates big tech. Brendan is just like, give me an

1767
01:42:53,835 --> 01:42:57,155
Speaker 4:  ounce of blood. You know, just, just like

1768
01:42:57,715 --> 01:43:01,475
Speaker 4:  bloviating away, moronically like these deals aren't happening. And I think

1769
01:43:01,475 --> 01:43:04,515
Speaker 4:  there's, people are starting to figure out like there's a path forward and

1770
01:43:04,515 --> 01:43:06,995
Speaker 4:  it involves trading your values to get the deals right.

1771
01:43:07,175 --> 01:43:09,715
Speaker 5:  And over and over they're doing it. They're so happy to do it.

1772
01:43:09,945 --> 01:43:13,915
Speaker 4:  Well I think the next one we're gonna see is CBS in

1773
01:43:13,915 --> 01:43:17,435
Speaker 4:  Paramount settling their case with Donald Trump over 60 minutes to get the

1774
01:43:17,435 --> 01:43:21,355
Speaker 4:  Skydance deal done. And that will be a dark and sad day for American journalism.

1775
01:43:21,495 --> 01:43:24,435
Speaker 4:  Yep. To the point where it's like, oh, that's over. You shouldn't trust that

1776
01:43:24,435 --> 01:43:28,155
Speaker 4:  network anymore. That's brutal. As always, Brendan,

1777
01:43:28,415 --> 01:43:31,465
Speaker 4:  if you're listening and I super know you are,

1778
01:43:32,545 --> 01:43:35,495
Speaker 4:  you can come on the show. You know, I got your press release today. I wrote

1779
01:43:35,495 --> 01:43:39,055
Speaker 4:  back to your, your people and invited you on. He put out a press release

1780
01:43:39,055 --> 01:43:41,975
Speaker 4:  about how much more spectrum he is opening for satellite networks. Great.

1781
01:43:41,975 --> 01:43:44,295
Speaker 4:  He loves it. It's his favorite thing to do. Congrats on him. You come on,

1782
01:43:44,295 --> 01:43:47,975
Speaker 4:  you can, you can defend this stuff. The, the, the fanciest CEOs in the industry,

1783
01:43:48,005 --> 01:43:51,405
Speaker 4:  telecom CEOs come on to coder and answer the questions

1784
01:43:52,635 --> 01:43:56,205
Speaker 4:  because they can defend themselves. Sampath the CEO of Verizon Wireless.

1785
01:43:56,345 --> 01:43:59,485
Speaker 4:  He gave maybe the single best answer to the decision making question on a

1786
01:43:59,485 --> 01:44:02,765
Speaker 4:  coder we ever had. Even though I completely disagree with, we have to manage

1787
01:44:02,785 --> 01:44:05,205
Speaker 4:  for our stakeholders. They can defend themselves. They're, they're smart.

1788
01:44:05,865 --> 01:44:08,665
Speaker 4:  I think you're stupid so you're not gonna show up. But if you want to, you

1789
01:44:08,665 --> 01:44:12,085
Speaker 4:  can, we're available, you can be on the show, you can be on decoder. But

1790
01:44:12,085 --> 01:44:15,125
Speaker 4:  for now that has been Brendan Carr's Dummy America's favorite podcast within

1791
01:44:15,165 --> 01:44:15,445
Speaker 4:  a podcast.

1792
01:44:15,785 --> 01:44:19,005
Speaker 5:  That's beautiful. I've decided to take a comprehensive view to Brendan Carr.

1793
01:44:19,955 --> 01:44:21,565
Speaker 5:  I'll get back to you on what that looks like. I have a comprehensive

1794
01:44:21,565 --> 01:44:21,765
Speaker 4:  View

1795
01:44:21,765 --> 01:44:25,565
Speaker 5:  Of data. Can I offer you some breaking news as a pal Clinton? Yes. Mark

1796
01:44:25,725 --> 01:44:29,485
Speaker 5:  Erman from Bloomberg just published a big story about

1797
01:44:29,575 --> 01:44:33,325
Speaker 5:  Apple with two very interesting pieces of information that I think are germane

1798
01:44:33,325 --> 01:44:37,165
Speaker 5:  to what we've been talking about. Thing number one is he reported that Apple

1799
01:44:38,025 --> 01:44:42,005
Speaker 5:  has been working on smart glasses and has like really ramped up its effort

1800
01:44:42,005 --> 01:44:45,525
Speaker 5:  to sell smart glasses and hopes to do so by the end of next year. And he,

1801
01:44:46,305 --> 01:44:50,205
Speaker 5:  he, he cites one person presumably inside of Apple who said

1802
01:44:50,205 --> 01:44:54,045
Speaker 5:  they will be similar to the meta ray bands but Better Made, which is

1803
01:44:54,045 --> 01:44:57,845
Speaker 5:  that's Tim Cook. Terrific. So yeah, presumably he called

1804
01:44:57,845 --> 01:45:01,485
Speaker 5:  Tim Cook and Tim told him that. And here we are. So that's piece of information

1805
01:45:01,485 --> 01:45:05,405
Speaker 5:  number one is that Apple Smart Glasses, which we have been hearing for a

1806
01:45:05,405 --> 01:45:08,205
Speaker 5:  while we're, we're sort of being pushed further and further off may have

1807
01:45:08,205 --> 01:45:11,805
Speaker 5:  been pulled back up in the roadmap. And the other one is that

1808
01:45:12,295 --> 01:45:15,325
Speaker 5:  we've been hearing for a while about the idea of an Apple watch with an integrated

1809
01:45:15,325 --> 01:45:19,205
Speaker 5:  camera and that is apparently shut down and that will no longer

1810
01:45:19,385 --> 01:45:22,765
Speaker 5:  be a thing. Which is a shame because I wanted to know very badly how any

1811
01:45:22,785 --> 01:45:25,325
Speaker 5:  of that was going to work in the real world.

1812
01:45:26,665 --> 01:45:30,525
Speaker 5:  But this is, this is Apple pushing on the same kind of stuff, right? Like

1813
01:45:30,525 --> 01:45:34,485
Speaker 5:  multi multimodal AI assistance is the name of the game here and Apple

1814
01:45:34,535 --> 01:45:38,485
Speaker 5:  seems to be on the smart glasses trend along with

1815
01:45:38,585 --> 01:45:41,565
Speaker 5:  at this point kind of everybody other than open AI and Johnny, I,

1816
01:45:42,245 --> 01:45:45,615
Speaker 6:  Yeah. And I can tell you why they're doing it. It's because these smart glasses

1817
01:45:45,755 --> 01:45:49,735
Speaker 6:  are hitting decent scale. They're doing single digit million sales a year.

1818
01:45:49,795 --> 01:45:53,535
Speaker 6:  The meta raybans are, they may get to double digit

1819
01:45:53,535 --> 01:45:53,815
Speaker 6:  millions.

1820
01:45:53,815 --> 01:45:56,615
Speaker 5:  They're also like good and people like them, which I think is an important

1821
01:45:56,795 --> 01:45:59,855
Speaker 4:  Oh I think everyone's super confused and most of all meta about why people

1822
01:45:59,855 --> 01:46:01,615
Speaker 4:  like them RayBan glasses. Oh

1823
01:46:01,615 --> 01:46:04,615
Speaker 5:  That's true. But sure. But they are good and people do like them. Yeah. People

1824
01:46:04,615 --> 01:46:04,735
Speaker 5:  are

1825
01:46:04,735 --> 01:46:07,255
Speaker 4:  Buying them. They they are. But what people like is having glasses with a

1826
01:46:07,255 --> 01:46:09,255
Speaker 4:  camera in them. Sure. In a shutter button. They don't like

1827
01:46:09,255 --> 01:46:11,615
Speaker 5:  Ai. But that's also, if you're Apple, that's a, that's a core competency,

1828
01:46:11,615 --> 01:46:13,375
Speaker 5:  right? Like that's a reason to do it if you're Apple.

1829
01:46:13,635 --> 01:46:16,135
Speaker 4:  But it, that's not, I don't think they're an AI distribution platform the

1830
01:46:16,135 --> 01:46:20,015
Speaker 4:  way that Meta wants 'em to be. No, but but

1831
01:46:20,015 --> 01:46:23,215
Speaker 4:  they're don't have a display in them yet. And that's what everyone else is

1832
01:46:23,215 --> 01:46:27,025
Speaker 4:  demoing. Right? I'm Apple Smart Glasses. If they don't have a

1833
01:46:27,025 --> 01:46:30,305
Speaker 4:  display, they're, they're one kind of product. If they do have a display,

1834
01:46:30,305 --> 01:46:32,865
Speaker 4:  they're a very different kind of product and no one has cracked that display

1835
01:46:32,865 --> 01:46:33,225
Speaker 4:  problem yet.

1836
01:46:33,225 --> 01:46:33,905
Speaker 5:  Yeah. Wait. Least

1837
01:46:33,905 --> 01:46:37,865
Speaker 4:  Of all Google the one we saw the Android xr it it, It is so a prototype,

1838
01:46:37,885 --> 01:46:38,665
Speaker 4:  It is not a display.

1839
01:46:39,165 --> 01:46:41,505
Speaker 5:  You saw these right? I'm gonna talk to VA bunch about all this stuff on,

1840
01:46:41,505 --> 01:46:44,185
Speaker 5:  on Tuesday and the glasses, but I am curious you, you got to try 'em, right?

1841
01:46:44,185 --> 01:46:44,705
Speaker 4:  Yeah, just

1842
01:46:44,705 --> 01:46:47,745
Speaker 6:  Really quickly. It was like a five minute demo and it was basically Gemini

1843
01:46:47,745 --> 01:46:51,485
Speaker 6:  on your face. The weird thing about these prototypes is the wave guide is

1844
01:46:51,485 --> 01:46:54,805
Speaker 6:  very small and not high resolution but it's in the dead center of the lens.

1845
01:46:55,035 --> 01:46:58,405
Speaker 6:  Whereas I was expecting like a heads up to the side thing.

1846
01:46:58,435 --> 01:46:59,725
Speaker 5:  Yeah, the like glassy, glassy

1847
01:46:59,725 --> 01:47:01,525
Speaker 6:  Kind of like up here. Yeah. 'cause it's literally, yeah it's literally just

1848
01:47:01,525 --> 01:47:05,445
Speaker 6:  a Gemini icon and then it reads back what it's saying to it and what you're

1849
01:47:05,445 --> 01:47:08,725
Speaker 6:  saying, which is like I don't, I don't need that. The, the best thing was

1850
01:47:08,725 --> 01:47:11,325
Speaker 6:  the map stuff where like you could turn your head down and it would show

1851
01:47:11,325 --> 01:47:15,165
Speaker 6:  where you were, your pin would move like it had GPS and tracking and who

1852
01:47:15,165 --> 01:47:18,485
Speaker 6:  knows, maybe the room was like set up for that but, and then the other thing

1853
01:47:18,485 --> 01:47:21,245
Speaker 6:  was like oh you, when you take a picture with it, you see the picture like

1854
01:47:21,245 --> 01:47:24,085
Speaker 6:  you can see how you're framing it because it shows it to you. Whereas the

1855
01:47:24,085 --> 01:47:27,685
Speaker 6:  Met Raybans obviously you have to take your phone out to see what you shot.

1856
01:47:27,945 --> 01:47:31,525
Speaker 6:  That's cool. Yeah, the hardware's not super compelling yet. The software's

1857
01:47:31,525 --> 01:47:35,165
Speaker 6:  very bare bones. I think they will actually start shipping these next year

1858
01:47:35,345 --> 01:47:38,765
Speaker 6:  and they got Warby Parker and Carrying, which makes

1859
01:47:38,915 --> 01:47:42,605
Speaker 6:  Cartier and like Gucci glasses and they got another cool, the name

1860
01:47:42,605 --> 01:47:46,325
Speaker 6:  escapes me another cool glasses company out of Asia that V was very excited

1861
01:47:46,325 --> 01:47:49,845
Speaker 6:  about. They got them on board. So basically Meta has hitched its wagon to

1862
01:47:49,845 --> 01:47:53,405
Speaker 6:  elsewhere Luxottica, which makes Ray Band and Oakley and a bunch of other

1863
01:47:53,405 --> 01:47:57,285
Speaker 6:  brands. And Google tried to get that deal. I reported that they

1864
01:47:57,285 --> 01:48:01,045
Speaker 6:  tried very hard and they failed and now they're going for basically every

1865
01:48:01,045 --> 01:48:05,005
Speaker 6:  other eyewear partner that they can to get Android as the platform instead

1866
01:48:05,005 --> 01:48:08,565
Speaker 6:  of meta. So I think we're gonna see this AI platform, smart glasses

1867
01:48:08,705 --> 01:48:12,565
Speaker 6:  battle between Meta and Google with their partnerships

1868
01:48:13,445 --> 01:48:17,365
Speaker 6:  strategy. Gemini's a better AI meta has a head start. We'll see

1869
01:48:17,365 --> 01:48:21,205
Speaker 6:  how it shakes out. The demo though at IO is like kind of meh. I do wanna

1870
01:48:21,205 --> 01:48:24,965
Speaker 6:  quickly say though a demo that was awesome and that I'm very excited about

1871
01:48:24,965 --> 01:48:28,885
Speaker 6:  and we have a great video about that I did with Veen is Beam. Oh yeah. Which

1872
01:48:28,915 --> 01:48:32,685
Speaker 6:  project? Starline and is basically Google's answer to what if virtual

1873
01:48:32,685 --> 01:48:33,765
Speaker 6:  meetings suck less.

1874
01:48:34,005 --> 01:48:37,045
Speaker 5:  Starline has been one of those ideas that Google has had that seems to like

1875
01:48:37,045 --> 01:48:40,925
Speaker 5:  get cooler every 18 months but also less and

1876
01:48:40,925 --> 01:48:42,445
Speaker 5:  less likely to ever be a real thing.

1877
01:48:42,745 --> 01:48:45,805
Speaker 6:  It was really cool. Like they invited Vera and I to the lab where they made

1878
01:48:45,805 --> 01:48:49,205
Speaker 6:  it. We were the first ones in their like labs hardware prototype room

1879
01:48:49,725 --> 01:48:53,085
Speaker 6:  externally. And got to see like how they moved all the tech into basically

1880
01:48:53,085 --> 01:48:56,765
Speaker 6:  what is it started as like it filled a room five years ago and now it's

1881
01:48:56,765 --> 01:49:00,365
Speaker 6:  basically a DVD player and everything is streamed through Google Cloud. So

1882
01:49:00,365 --> 01:49:03,685
Speaker 6:  they built this proprietary volumetric 3D AI model

1883
01:49:04,195 --> 01:49:07,965
Speaker 6:  that makes you look like a hologram basically through a 2D. Very

1884
01:49:08,015 --> 01:49:11,765
Speaker 6:  fancy. I tried to get them Neli to really explain the panel. It's a highly

1885
01:49:11,765 --> 01:49:14,605
Speaker 6:  customized panel that's doing a bunch of fancy

1886
01:49:15,455 --> 01:49:19,405
Speaker 6:  pixel work, but I couldn't get a lot of detail out of it. But it's a reference

1887
01:49:19,405 --> 01:49:22,965
Speaker 6:  design now that HP will actually start deploying into offices

1888
01:49:23,095 --> 01:49:27,005
Speaker 6:  later this year. So they've got Salesforce, Deloitte, some other

1889
01:49:27,245 --> 01:49:31,175
Speaker 6:  companies that have committed to installs and I asked, I was like, is this

1890
01:49:31,175 --> 01:49:34,415
Speaker 6:  expensive because companies will not buy this if it's not, you know, just

1891
01:49:34,415 --> 01:49:37,895
Speaker 6:  as like cheap as their existing video conferencing software. And they're

1892
01:49:37,895 --> 01:49:41,495
Speaker 6:  like, Nope, that's why we did the AI work. It's on the cloud and it costs

1893
01:49:41,705 --> 01:49:45,175
Speaker 6:  about the same as the video conferencing setups you see in offices today.

1894
01:49:45,435 --> 01:49:49,325
Speaker 6:  So I think there's a real chance we start seeing Beam and people who work

1895
01:49:49,325 --> 01:49:53,085
Speaker 6:  in offices start seeing Beam over the next couple years. And It is, It is

1896
01:49:53,085 --> 01:49:56,765
Speaker 6:  wild. Like go watch the video. It's on our YouTube veering. Got to try it

1897
01:49:56,765 --> 01:50:00,125
Speaker 6:  for the first time. I had done it a couple times 'cause we had it at code

1898
01:50:00,125 --> 01:50:03,445
Speaker 6:  and then it was at IO a few years ago and it was starline and

1899
01:50:04,285 --> 01:50:07,525
Speaker 6:  Veering tried it and it's just this like holy shit moment every time someone

1900
01:50:07,525 --> 01:50:11,085
Speaker 6:  tries it for the first time. I mean, NELI, you remember? I do. And yeah,

1901
01:50:11,085 --> 01:50:13,565
Speaker 6:  it's cool. It's like one of these classic Google things. Like, it reminds

1902
01:50:13,565 --> 01:50:17,525
Speaker 6:  me of Waymo, they just toil away as like with the resources that only

1903
01:50:17,605 --> 01:50:21,525
Speaker 6:  a company like Google can to invent this wild new thing. And then five

1904
01:50:21,585 --> 01:50:25,405
Speaker 6:  to 10 years in they're like, it's ready. And it's like now the cars are driving

1905
01:50:25,405 --> 01:50:29,045
Speaker 6:  themselves. Now we have holograms that we're like putting into

1906
01:50:29,045 --> 01:50:32,605
Speaker 6:  Deloitte offices. Like it's just happening. So that was pretty cool. Yeah.

1907
01:50:32,785 --> 01:50:35,285
Speaker 6:  My understanding of the screen, by the way is the reason they're not talking

1908
01:50:35,285 --> 01:50:38,965
Speaker 6:  about it in detail is it It is more standard

1909
01:50:39,015 --> 01:50:42,845
Speaker 6:  particular display than not. I think there is some,

1910
01:50:42,925 --> 01:50:46,125
Speaker 6:  a lot of custom stuff going on there, but usually when you have the big tech

1911
01:50:46,125 --> 01:50:49,885
Speaker 6:  innovation, you talk about it a lot. Yeah. And the reason they're like, it's

1912
01:50:49,885 --> 01:50:53,845
Speaker 6:  a secret is like, it's kind of a 3D display. Like, and it all the

1913
01:50:53,845 --> 01:50:56,565
Speaker 6:  actual happening is in the camera, but eventually these are gonna be out

1914
01:50:56,565 --> 01:50:59,205
Speaker 6:  there and we can go look at them and figure it out. Yeah. But It is very

1915
01:50:59,365 --> 01:51:03,185
Speaker 6:  exciting to see that out. Yeah. On the glasses, the display is not

1916
01:51:03,185 --> 01:51:07,025
Speaker 6:  solved. No meta Orion, like they spent billions of dollars on the

1917
01:51:07,025 --> 01:51:10,825
Speaker 6:  wrong technology and it didn't scale to make those displays. Right. Turns

1918
01:51:10,825 --> 01:51:13,585
Speaker 6:  out growing crystals doesn't work at scale. Yeah.

1919
01:51:16,555 --> 01:51:20,465
Speaker 6:  We're just crystal farmers out here guys. I don't know. I mean, I, I

1920
01:51:20,465 --> 01:51:23,625
Speaker 6:  don't know who's gonna build those displays. I don dunno where they're gonna

1921
01:51:23,625 --> 01:51:27,345
Speaker 6:  come from. No one, no one's cracked it yet. And it, it's, It is just one

1922
01:51:27,345 --> 01:51:29,385
Speaker 6:  of these things like what is the core technology that's gonna enable this

1923
01:51:29,385 --> 01:51:32,545
Speaker 6:  next generation of products? You need the displays to make the smart glasses

1924
01:51:32,805 --> 01:51:36,025
Speaker 6:  or you don't, or you're Johnny, Ivan, you make a fancy aluminum polka x and

1925
01:51:36,255 --> 01:51:37,425
Speaker 6:  like, that's like,

1926
01:51:39,765 --> 01:51:43,705
Speaker 6:  all right. That's, we are way over. I'm, I'm, I'm so sorry to everyone

1927
01:51:43,765 --> 01:51:47,385
Speaker 6:  for, for the Rams today. I mean, what did we expect? Like this week, the

1928
01:51:47,385 --> 01:51:51,145
Speaker 6:  after, I know we got a lot on the site. Tons and tons of coverage.

1929
01:51:51,585 --> 01:51:54,705
Speaker 6:  V is gonna be back next week to talk more about the glasses demo. She just

1930
01:51:54,705 --> 01:51:57,665
Speaker 6:  published a story. Trying to guess what Johnny is willing to way more coverage

1931
01:51:57,665 --> 01:52:01,625
Speaker 6:  than The Vergecast from her coming up Sundar on Monday.

1932
01:52:01,885 --> 01:52:05,505
Speaker 6:  Get ready. It wasn't, it's not. I will tell it's not as tense as it was last

1933
01:52:05,505 --> 01:52:08,145
Speaker 6:  year, but there's some moments. I think you're gonna like it. All right.

1934
01:52:08,165 --> 01:52:09,505
Speaker 6:  That's it. That's Vergecast. Bye.

1935
01:52:14,925 --> 01:52:18,265
Speaker 11:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1936
01:52:18,265 --> 01:52:21,905
Speaker 11:  you. Give us a call at eight six six VERGE one one.

1937
01:52:22,085 --> 01:52:25,585
Speaker 11:  The Vergecast is a production of The Verge and the Vox Media podcast network.

1938
01:52:25,885 --> 01:52:29,705
Speaker 11:  Our show is produced by Will Poor Eric Gomez and Brandon Keefer. And

1939
01:52:29,705 --> 01:52:30,905
Speaker 11:  that's it. We'll see you next week.

