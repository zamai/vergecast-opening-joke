1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 717f1b09-de8d-4b8c-808f-49878898ef63
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/7215121925693810278/7579113407149821892/s93290-US-4901s-1714348636.mp3
Description: Today on the flagship podcast of forced app divestiture: 
03:07 - The Verge’s David Pierce and Lauren Fiener discuss the latest tech policy bills floating through Congress, including a privacy bill, a generative AI bill, and the TikTok divest-or-ban bill.

TikTok ‘ban’ passes in the House again

TikTok divest-or-ban legislation could suddenly be fast-tracked in the Senate

Lawmakers unveil new bipartisan digital privacy bill after years of impasse 


A real privacy law? House lawmakers are optimistic this time 

New bill would create public datasets to train AI and incentivize innovation.


34:17 - David talks with Nikola Todorovic and Tye Sheridan about their company Wonder Dynamics, which is creating AI-powered production tools for filmmakers. 

1:09:16 - David answers a question from the Vergecast Hotline about messaging apps.

Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (25 ads detected)

2
00:00:02,635 --> 00:00:06,205
Speaker 2:  Welcome To The Vergecast, the flagship podcast of Forced

3
00:00:06,585 --> 00:00:10,525
Speaker 2:  App Divesture. I'm your friend David Pierce and I am stuck

4
00:00:10,525 --> 00:00:14,485
Speaker 2:  in my basement because this is the only camera and microphone that I have

5
00:00:14,595 --> 00:00:18,125
Speaker 2:  that works. So I bought this thing that DJ I Osmo Pocket

6
00:00:18,295 --> 00:00:22,085
Speaker 2:  three just to make these intros. I like being out in the world,

7
00:00:22,205 --> 00:00:26,085
Speaker 2:  I like telling you guys what I'm up to. I like doing fun stuff while we record

8
00:00:26,085 --> 00:00:29,885
Speaker 2:  The Vergecast and then it broke. It was just sitting in my bag for

9
00:00:29,885 --> 00:00:33,645
Speaker 2:  reasons that I cannot figure out. Just something about the gimbal

10
00:00:33,835 --> 00:00:37,445
Speaker 2:  stopped working. So now I turn it on and it like sort of

11
00:00:37,735 --> 00:00:41,565
Speaker 2:  spins, but not really. And then I get this error that

12
00:00:41,565 --> 00:00:45,485
Speaker 2:  just says gimbal protection over and over and over again. I don't

13
00:00:45,485 --> 00:00:48,645
Speaker 2:  know how to fix it. I think I know what's wrong. There's a spot that's a

14
00:00:48,645 --> 00:00:52,085
Speaker 2:  little sticky, but the thing about these gimbal cameras is that they're so

15
00:00:52,885 --> 00:00:55,445
Speaker 2:  delicate and so finicky. That's what makes them cool. It's what makes them

16
00:00:55,445 --> 00:00:59,365
Speaker 2:  work. But it basically seems to have broken just because I like put

17
00:00:59,365 --> 00:01:02,925
Speaker 2:  it in my bag and put a rolled up sweater on top of it.

18
00:01:03,735 --> 00:01:07,365
Speaker 2:  Technology man. Anyway, we have an awesome show coming up for you today.

19
00:01:07,365 --> 00:01:11,325
Speaker 2:  We're gonna do two things. First, we're gonna talk about this weird sort

20
00:01:11,325 --> 00:01:14,725
Speaker 2:  of run of bills and legislation that we've gotten in

21
00:01:15,365 --> 00:01:19,005
Speaker 2:  Congress recently. All of a sudden, there's tons of momentum again on the

22
00:01:19,025 --> 00:01:22,845
Speaker 2:  TikTok ban, on digital privacy legislation on ai.

23
00:01:23,345 --> 00:01:26,685
Speaker 2:  So we're gonna try to figure out if any of this actually means anything.

24
00:01:26,825 --> 00:01:30,645
Speaker 2:  And if any of this is ever actually likely to become law, then

25
00:01:30,815 --> 00:01:34,325
Speaker 2:  we're gonna play you a bit of a conversation I had at the Chicago

26
00:01:34,375 --> 00:01:38,165
Speaker 2:  Humanities Festival. I talked to two filmmakers who are also

27
00:01:38,305 --> 00:01:42,005
Speaker 2:  the co-founders of an AI startup, which I just think is a fascinating tension.

28
00:01:42,035 --> 00:01:45,725
Speaker 2:  What does it mean to be a creative person in this world

29
00:01:45,865 --> 00:01:49,565
Speaker 2:  we live in right now while also trying to build AI tools

30
00:01:49,995 --> 00:01:53,845
Speaker 2:  that threaten to totally upend the way that we do all of this

31
00:01:54,205 --> 00:01:57,165
Speaker 2:  creative work? We had a really fun conversation. I wasn't planning on putting

32
00:01:57,165 --> 00:02:00,805
Speaker 2:  it on this show, but I really enjoyed it and I think you might as well. All

33
00:02:00,805 --> 00:02:03,965
Speaker 2:  of that is coming up in just a second. But I just watched a YouTube video

34
00:02:04,055 --> 00:02:07,565
Speaker 2:  where basically the way to fix this gimbal is to shove a bunch of

35
00:02:07,635 --> 00:02:11,525
Speaker 2:  post-it notes in here and you know I'll try anything at this

36
00:02:11,525 --> 00:02:13,645
Speaker 2:  point. This is The Vergecast. See you in a sec.

37
00:03:25,905 --> 00:03:29,795
Speaker 2:  Welcome back. All right. My camera's broken, I think forever, but

38
00:03:29,795 --> 00:03:33,115
Speaker 2:  my mic still works. So we soldier on. So we've learned over the last few

39
00:03:33,115 --> 00:03:36,995
Speaker 2:  years to be, let's say skeptical, anytime there's a

40
00:03:36,995 --> 00:03:40,635
Speaker 2:  new tech related bill moving through Congress, we get all these hearings

41
00:03:40,635 --> 00:03:44,355
Speaker 2:  that turn out to be mostly just grandstanding and people trying to

42
00:03:44,445 --> 00:03:48,435
Speaker 2:  score points that they can post on TikTok. We get bills that don't make

43
00:03:48,435 --> 00:03:52,395
Speaker 2:  sense and go nowhere. We get bills that do make sense and go nowhere. And

44
00:03:52,395 --> 00:03:56,275
Speaker 2:  so it feels like by and large we're left with decades old or

45
00:03:56,275 --> 00:04:00,075
Speaker 2:  even centuries old laws and precedents trying to govern the

46
00:04:00,235 --> 00:04:04,155
Speaker 2:  internet. It's not great. But recently there's a sense of momentum in

47
00:04:04,355 --> 00:04:08,155
Speaker 2:  Congress that feels new. There's a new privacy bill that has some real

48
00:04:08,155 --> 00:04:11,675
Speaker 2:  steam. There's a new take on the TikTok ban that is moving really fast and

49
00:04:11,675 --> 00:04:15,195
Speaker 2:  there's even some AI legislation that seems to have at least an

50
00:04:15,225 --> 00:04:19,155
Speaker 2:  outside shot of turning into something. But is any of this for

51
00:04:19,155 --> 00:04:23,075
Speaker 2:  real? And if so, what's different now? Why is this all kind of happening

52
00:04:23,135 --> 00:04:26,435
Speaker 2:  all at once? The verges, Lauren Finer has been covering all of these bills

53
00:04:26,575 --> 00:04:30,435
Speaker 2:  and she is here to help us figure out what's next. Lauren Finer. Hello. Hi.

54
00:04:30,695 --> 00:04:31,155
Speaker 2:  How are you?

55
00:04:31,405 --> 00:04:32,155
Speaker 5:  Doing well,

56
00:04:32,465 --> 00:04:35,315
Speaker 2:  Just another chill week in Lauren's life. Everything

57
00:04:35,465 --> 00:04:36,635
Speaker 5:  Just a normal weekend.

58
00:04:37,395 --> 00:04:40,755
Speaker 2:  I like this for you. Yeah, it's really rude of Congress to continue to do

59
00:04:40,755 --> 00:04:41,475
Speaker 2:  things on weekends.

60
00:04:41,585 --> 00:04:42,475
Speaker 5:  Seriously, this

61
00:04:42,475 --> 00:04:45,795
Speaker 2:  Feels like they need to like go hang out with other people for a couple of

62
00:04:45,795 --> 00:04:46,835
Speaker 2:  days and like give you a break.

63
00:04:46,975 --> 00:04:47,635
Speaker 5:  Go socialize.

64
00:04:47,705 --> 00:04:50,795
Speaker 2:  Yeah, so we have a bunch of stuff to talk about, but I feel like the place

65
00:04:50,815 --> 00:04:54,715
Speaker 2:  we should start is probably TikTok, because TikTok is like the thing

66
00:04:54,715 --> 00:04:58,315
Speaker 2:  happening right this minute. So where we left off,

67
00:04:58,715 --> 00:05:02,435
Speaker 2:  I would say on the show is the bill sort of flew through

68
00:05:02,575 --> 00:05:05,955
Speaker 2:  the house subcommittee, flew through the house, hit the Senate, and

69
00:05:06,305 --> 00:05:10,155
Speaker 2:  kind of disappeared. There was a lot of momentum and that all the momentum

70
00:05:10,225 --> 00:05:13,915
Speaker 2:  died. And this was a few weeks ago. We have had a re momentum in, tell me

71
00:05:13,915 --> 00:05:15,075
Speaker 2:  what's been going on. Catch me up.

72
00:05:15,305 --> 00:05:19,195
Speaker 5:  Yeah, basically like you said, we had this huge moment in the

73
00:05:19,195 --> 00:05:23,035
Speaker 5:  house where they overwhelmingly passed this bill that could ban

74
00:05:23,055 --> 00:05:26,995
Speaker 5:  TikTok unless it is divested from its Chinese parent

75
00:05:26,995 --> 00:05:30,955
Speaker 5:  company, bite dance within six months. And then you know,

76
00:05:31,185 --> 00:05:34,915
Speaker 5:  next it's on Senate, the Senate to move. And you know, we got a lot of kind

77
00:05:34,915 --> 00:05:38,875
Speaker 5:  of non-committal responses from senators who were in

78
00:05:38,875 --> 00:05:42,835
Speaker 5:  key positions there. So it kind of looked like maybe this thing is just gonna

79
00:05:42,835 --> 00:05:46,555
Speaker 5:  like linger here until everyone kind of forgets about it and moves on.

80
00:05:46,785 --> 00:05:49,675
Speaker 2:  What was your read on that by the way, before we even get to that, I, I found

81
00:05:49,675 --> 00:05:53,635
Speaker 2:  myself wondering a bunch over the last couple of weeks why the Senate seemed

82
00:05:53,635 --> 00:05:56,755
Speaker 2:  content to just let it go. I mean, you have President Biden saying he would

83
00:05:56,825 --> 00:06:00,685
Speaker 2:  sign the law if it hit his desk. Tons of momentum in the

84
00:06:00,685 --> 00:06:04,565
Speaker 2:  house. This thing was going crazy. Like lots of people even in the

85
00:06:04,565 --> 00:06:08,165
Speaker 2:  house, Democrats were for it. Why was the Senate so happy to just sort of

86
00:06:08,265 --> 00:06:10,045
Speaker 2:  let it wither and disappear?

87
00:06:10,395 --> 00:06:14,085
Speaker 5:  Yeah, I think there's two things like one on just like the more political

88
00:06:14,235 --> 00:06:17,925
Speaker 5:  side, you know, the house is a very different beast from the Senate.

89
00:06:18,025 --> 00:06:21,885
Speaker 5:  The Senate is kind of a more deliberative body in a lot of ways. The house

90
00:06:21,945 --> 00:06:24,965
Speaker 5:  is much larger, you can have a lot more like different personalities.

91
00:06:25,585 --> 00:06:27,365
Speaker 2:  It was a very kind way of putting it by the way.

92
00:06:27,635 --> 00:06:27,925
Speaker 5:  Yeah.

93
00:06:29,545 --> 00:06:33,085
Speaker 5:  So I think like things that move in the house aren't necessarily the same

94
00:06:33,085 --> 00:06:36,765
Speaker 5:  that are gonna move in the Senate. So you know, that's one thing. I think

95
00:06:36,845 --> 00:06:40,445
Speaker 5:  a second thing is that I think there were real concerns with the earlier

96
00:06:40,445 --> 00:06:44,405
Speaker 5:  version of the bill, particularly with the timeline for divestment. You know,

97
00:06:44,485 --> 00:06:48,285
Speaker 5:  I think really, I don't think I really heard from many people at all

98
00:06:48,305 --> 00:06:52,285
Speaker 5:  who thought that six months was a reasonable timeline to have this

99
00:06:52,375 --> 00:06:56,045
Speaker 5:  giant s asset divested from this Chinese parent

100
00:06:56,045 --> 00:06:59,925
Speaker 5:  company. So I think there were real concerns about that and you know,

101
00:06:59,925 --> 00:07:03,485
Speaker 5:  potential constitutional concerns, free speech implications.

102
00:07:03,745 --> 00:07:06,445
Speaker 5:  So I think there were still things that senators really wanted to ponder

103
00:07:06,445 --> 00:07:09,845
Speaker 5:  and maybe hold hearings on and go through the regular process there.

104
00:07:10,465 --> 00:07:13,925
Speaker 5:  So I, that's kind of how I think that was going down at the time.

105
00:07:14,475 --> 00:07:17,725
Speaker 2:  Okay, that makes sense. Okay, so I derailed you so that all happens.

106
00:07:18,415 --> 00:07:20,565
Speaker 2:  Brief pause, what happens next?

107
00:07:21,075 --> 00:07:24,965
Speaker 5:  Yeah, so then the house is trying to pass these foreign aid

108
00:07:24,965 --> 00:07:28,645
Speaker 5:  bills basically directing, you know, military and humanitarian

109
00:07:29,065 --> 00:07:32,885
Speaker 5:  aid to places like Israel, Ukraine, Gaza. And

110
00:07:33,025 --> 00:07:36,725
Speaker 5:  so obviously this is something that is high priority in Congress and

111
00:07:37,205 --> 00:07:40,925
Speaker 5:  House speaker Mike Johnson does something interesting and allows a new

112
00:07:40,925 --> 00:07:44,685
Speaker 5:  version of this TikTok bill to get into

113
00:07:44,685 --> 00:07:48,565
Speaker 5:  this foreign aid package. So, you know, I think anyone who

114
00:07:48,565 --> 00:07:52,445
Speaker 5:  follows Congress might understand that a lot of times when we see bills

115
00:07:52,515 --> 00:07:56,365
Speaker 5:  pass into law, it's through like a larger package that has

116
00:07:56,365 --> 00:08:00,245
Speaker 5:  to pass like the National Defense Authorization Act or some

117
00:08:00,245 --> 00:08:03,685
Speaker 5:  sort of, you know, budget for the government or things like that. So that's

118
00:08:03,685 --> 00:08:07,405
Speaker 5:  kind of what happened here is it got packaged in with these bills that are

119
00:08:07,405 --> 00:08:11,125
Speaker 5:  really high priority for Congress to pass. And

120
00:08:11,305 --> 00:08:14,805
Speaker 5:  not only did they kind of bundle this all together, they also

121
00:08:15,155 --> 00:08:18,805
Speaker 5:  changed the bill a little bit so that the timeline for divestment

122
00:08:18,985 --> 00:08:22,445
Speaker 5:  was extended to basically up to a year. So it's really extended

123
00:08:22,865 --> 00:08:26,605
Speaker 5:  to nine months initially. And then if the president sees that

124
00:08:26,605 --> 00:08:30,565
Speaker 5:  there's progress on a deal, he has discretion to extend it essentially

125
00:08:30,565 --> 00:08:34,365
Speaker 5:  another three months. Okay. So up to a year for this deal to be completed,

126
00:08:34,365 --> 00:08:38,125
Speaker 5:  which seemed to kind of assuage a lot of the concerns

127
00:08:38,125 --> 00:08:41,925
Speaker 5:  around it. In particular from senate commerce committee chair Maria

128
00:08:42,045 --> 00:08:45,925
Speaker 5:  Cantwell who'd previously kind of not said anything really definitive

129
00:08:45,925 --> 00:08:49,805
Speaker 5:  about how she felt on the bill, but afterwards said that she supported

130
00:08:49,805 --> 00:08:53,525
Speaker 5:  the updated text of the legislation and that's significant because she

131
00:08:53,525 --> 00:08:57,045
Speaker 5:  leads the committee that would, that under like a normal

132
00:08:57,195 --> 00:09:01,125
Speaker 5:  process would have been considering this bill before it

133
00:09:01,125 --> 00:09:01,965
Speaker 5:  hit the Senate floor.

134
00:09:02,425 --> 00:09:06,085
Speaker 2:  Got it. Okay. Talk me through the politics of the bundling of all of this

135
00:09:06,125 --> 00:09:08,685
Speaker 2:  a little bit. Because like you said, this is the kind of thing that happens

136
00:09:09,255 --> 00:09:13,165
Speaker 2:  relatively frequently that a bunch of bills that either have a little

137
00:09:13,165 --> 00:09:16,285
Speaker 2:  bit to do with each other or nothing to do with each other end up in kind

138
00:09:16,285 --> 00:09:20,125
Speaker 2:  of an all or nothing package. But what do you make of

139
00:09:20,195 --> 00:09:24,085
Speaker 2:  kind of the way this was packaged back together and the way

140
00:09:24,105 --> 00:09:26,045
Speaker 2:  it has sort of gone through this process again?

141
00:09:26,395 --> 00:09:30,205
Speaker 5:  Yeah, so I think, you know, like I said, there's kind of these two ways

142
00:09:30,235 --> 00:09:33,845
Speaker 5:  that a bill can pass through Congress. One is that

143
00:09:34,115 --> 00:09:37,565
Speaker 5:  it's packaged together with these other bills that, you know, Congress has

144
00:09:37,565 --> 00:09:41,285
Speaker 5:  to make a decision on one way or another. You know, they can negotiate for

145
00:09:41,285 --> 00:09:45,125
Speaker 5:  certain bills to get out of that must pass package. But

146
00:09:45,225 --> 00:09:48,085
Speaker 5:  you know, there's a lot more politics there and like what are you really

147
00:09:48,085 --> 00:09:52,005
Speaker 5:  gonna stake your ground on? And then the other way is to pass like standalone

148
00:09:52,005 --> 00:09:55,845
Speaker 5:  bills like we saw the first time with this TikTok bill and you know,

149
00:09:55,845 --> 00:09:59,645
Speaker 5:  that requires a lot of kind of resources from Congress in terms of floor

150
00:09:59,645 --> 00:10:03,445
Speaker 5:  time. There's a lot of bills, a lot of legislative priorities that

151
00:10:03,625 --> 00:10:07,365
Speaker 5:  are out there. And so for house leadership to make the

152
00:10:07,395 --> 00:10:11,245
Speaker 5:  time for the TikTok bill as a standalone to get a

153
00:10:11,245 --> 00:10:13,045
Speaker 5:  vote was a big deal in the first place.

154
00:10:13,265 --> 00:10:16,925
Speaker 2:  That's one thing by the way that I think I have underrated in the past

155
00:10:16,995 --> 00:10:20,525
Speaker 2:  with a lot of this stuff is just the sheer logistics

156
00:10:20,945 --> 00:10:24,845
Speaker 2:  of passing a law like this is so much weirder and more complicated than it

157
00:10:24,845 --> 00:10:28,765
Speaker 2:  gets credit for. And like you say, floor time and my brain just goes like,

158
00:10:28,765 --> 00:10:32,725
Speaker 2:  oh right, there's only like so many hours in the day to stand there and talk

159
00:10:32,725 --> 00:10:35,805
Speaker 2:  about these things and everybody, you have hundreds of people with wildly

160
00:10:35,805 --> 00:10:39,645
Speaker 2:  different priorities. So literally just to like go through the time of

161
00:10:39,645 --> 00:10:43,085
Speaker 2:  discussing this thing to get to a vote, you just can't do it with everything.

162
00:10:43,315 --> 00:10:43,805
Speaker 2:  Totally.

163
00:10:44,005 --> 00:10:47,445
Speaker 5:  I mean think about like everything that's like most important to you

164
00:10:47,445 --> 00:10:50,765
Speaker 5:  politically and everything that's most important to like your parents politically

165
00:10:50,785 --> 00:10:54,645
Speaker 5:  and all these people around you and like they could be, you know, in theory

166
00:10:54,835 --> 00:10:58,525
Speaker 5:  equally important, but how do you decide what gets the time to

167
00:10:58,805 --> 00:11:02,565
Speaker 5:  actually get a vote? So that's something that, you know, congress is always

168
00:11:02,845 --> 00:11:06,365
Speaker 5:  juggling. So I think that's definitely right. It's a, it's a big deal. So

169
00:11:06,365 --> 00:11:10,245
Speaker 5:  then packaging this with the other bills, you know, when it's a standalone

170
00:11:10,275 --> 00:11:14,205
Speaker 5:  bill then it's on the Senate to decide when and how to move

171
00:11:14,665 --> 00:11:18,005
Speaker 5:  and you know, that's really up in the air. It's pretty easy to

172
00:11:18,475 --> 00:11:22,365
Speaker 5:  just like throw sand in the gears of Congress and just slow things down.

173
00:11:23,225 --> 00:11:26,925
Speaker 5:  But with this package it kind of forces the

174
00:11:26,925 --> 00:11:30,845
Speaker 5:  senate's hand in a certain way where it makes them have to figure out

175
00:11:30,845 --> 00:11:34,685
Speaker 5:  like, are we going to, are we going to say like this is the issue

176
00:11:34,685 --> 00:11:38,565
Speaker 5:  that makes us not vote for this foreign aid package that, you know, a lot

177
00:11:38,565 --> 00:11:42,045
Speaker 5:  of senators really want. And you know, you have to think like

178
00:11:42,395 --> 00:11:46,325
Speaker 5:  this is a bill that passed overwhelmingly in the house. It's

179
00:11:46,325 --> 00:11:50,165
Speaker 5:  a pretty popular measure. So is that the thing that senators are

180
00:11:50,165 --> 00:11:54,045
Speaker 5:  gonna kind of put their foot down on for why this like otherwise

181
00:11:54,395 --> 00:11:57,845
Speaker 5:  popular and wanted package is not going to pass?

182
00:11:58,295 --> 00:12:01,765
Speaker 2:  Right. Yeah, I was, I was looking at it in the, I think the bill, I wrote

183
00:12:01,765 --> 00:12:05,405
Speaker 2:  this down passed 360 to 58 in the house, which for the House of

184
00:12:05,405 --> 00:12:09,285
Speaker 2:  Representatives in the country we live in right now is like pretty nuts to

185
00:12:09,285 --> 00:12:13,125
Speaker 2:  get that broad of vote for just about anything at this

186
00:12:13,125 --> 00:12:13,845
Speaker 2:  moment in time. Right?

187
00:12:14,515 --> 00:12:18,165
Speaker 5:  Yeah, exactly. And you know, it was interesting I think

188
00:12:18,985 --> 00:12:22,925
Speaker 5:  during the debate around the bill New York Democrat Gregory Meeks,

189
00:12:22,925 --> 00:12:26,405
Speaker 5:  who's the top democrat on the house Foreign Affairs Committee, he was saying

190
00:12:26,405 --> 00:12:29,805
Speaker 5:  that he had opposed the earlier standalone TikTok bill.

191
00:12:30,225 --> 00:12:33,645
Speaker 5:  He said quote out of concern that it would be a broad authorization that

192
00:12:33,645 --> 00:12:36,685
Speaker 5:  could be misused far beyond what we in Washington are currently debating

193
00:12:36,705 --> 00:12:40,685
Speaker 5:  beyond just TikTok. But you know, he was supporting this package of

194
00:12:40,685 --> 00:12:44,605
Speaker 5:  legislation and said quote, the bill took a step

195
00:12:44,605 --> 00:12:48,125
Speaker 5:  in the right direction with a more realistic timeframe for a complex divestiture

196
00:12:48,125 --> 00:12:51,765
Speaker 5:  process. Let me say for the record that I believe this bill is about one

197
00:12:51,765 --> 00:12:55,365
Speaker 5:  company that additional authorities provided to the executive branch

198
00:12:55,745 --> 00:12:59,685
Speaker 5:  are to be interpreted narrowly. So you know, he's saying, you

199
00:12:59,685 --> 00:13:03,125
Speaker 5:  know, look, I didn't really support this the first time around, but you know,

200
00:13:03,125 --> 00:13:06,765
Speaker 5:  given that it's part of this larger package, I think it

201
00:13:06,875 --> 00:13:10,725
Speaker 5:  it's a better bill than it was before. And if we interpret it narrowly like,

202
00:13:11,025 --> 00:13:12,325
Speaker 5:  you know, I'm fine with it.

203
00:13:12,965 --> 00:13:16,685
Speaker 2:  I feel like there are two ways to read that. And I'm curious, which you read

204
00:13:16,685 --> 00:13:20,605
Speaker 2:  it as if, if either one or if there's something I'm missing. One is basically

205
00:13:20,835 --> 00:13:24,605
Speaker 2:  like the Senate saying, oh God, we've lost, like we're sort of

206
00:13:24,605 --> 00:13:28,565
Speaker 2:  forced to do this now. This foreign aid bill is important. We, we are

207
00:13:28,815 --> 00:13:32,645
Speaker 2:  going to have to vote on this and letting this foreign aid bill

208
00:13:32,705 --> 00:13:36,565
Speaker 2:  die at the hands of a TikTok ban is just not worth the fallout.

209
00:13:36,775 --> 00:13:40,765
Speaker 2:  Which on, if, if that is the case, fairly brilliant

210
00:13:40,835 --> 00:13:44,805
Speaker 2:  political machinations by Mike Johnson and the house, like kudos to them

211
00:13:44,865 --> 00:13:48,005
Speaker 2:  for pulling off what seems like it might actually work. The other read is

212
00:13:48,005 --> 00:13:51,845
Speaker 2:  that we have now debated this out long enough and, and we kind of know

213
00:13:51,845 --> 00:13:55,325
Speaker 2:  where it's going and it seems increasingly clear that the end point is not

214
00:13:55,365 --> 00:13:59,285
Speaker 2:  a ban, it is a sale, which makes it specific, right. That that becomes a

215
00:13:59,285 --> 00:14:02,805
Speaker 2:  very unique thing because there just isn't anything else like that out there

216
00:14:02,805 --> 00:14:06,645
Speaker 2:  right now. So everybody is able to just say, you know what, whatever the

217
00:14:06,645 --> 00:14:09,845
Speaker 2:  law actually says, it is basically only going to apply to TikTok. Let's just

218
00:14:09,845 --> 00:14:12,725
Speaker 2:  pass the thing and move on with our lives. Do you have a sense of which,

219
00:14:12,795 --> 00:14:16,685
Speaker 2:  like is this, did the Senate get sort of, you know, outmaneuvered or

220
00:14:16,685 --> 00:14:19,445
Speaker 2:  is there a sense that like we really can solve this problem the way that

221
00:14:19,445 --> 00:14:20,285
Speaker 2:  we want to now?

222
00:14:20,805 --> 00:14:24,725
Speaker 5:  I would tend to think it's more that, you know, we really can solve

223
00:14:24,725 --> 00:14:28,005
Speaker 5:  this problem the way that we want to. Okay. 'cause I, I still go back to

224
00:14:28,105 --> 00:14:31,285
Speaker 5:  the initial vote on this bill in the house was so

225
00:14:31,545 --> 00:14:35,085
Speaker 5:  overwhelmingly in favor of this legislation, even with the six month

226
00:14:35,365 --> 00:14:39,125
Speaker 5:  timeline. You know, to have that kind of broad support for something like

227
00:14:39,125 --> 00:14:42,805
Speaker 5:  this really tells me that, you know, this is a popular

228
00:14:42,915 --> 00:14:46,885
Speaker 5:  measure and you know, the people who are leading this legislation are saying,

229
00:14:46,985 --> 00:14:50,765
Speaker 5:  you know, this is not a ban. We want divestment. That's our goal.

230
00:14:51,345 --> 00:14:54,365
Speaker 5:  So I think that this is something that a lot of

231
00:14:55,045 --> 00:14:58,365
Speaker 5:  lawmakers are really interested in and you know, we've seen

232
00:14:58,635 --> 00:15:02,325
Speaker 5:  support for this sort of thing from the Senate Intel committee leaders.

233
00:15:02,905 --> 00:15:06,405
Speaker 5:  Now you have the senate commerce committee leader on board. There's a lot

234
00:15:06,405 --> 00:15:10,205
Speaker 5:  of support for doing something on TikTok in general and

235
00:15:10,505 --> 00:15:14,405
Speaker 5:  you know, it seems like this is a way to do it that more

236
00:15:14,685 --> 00:15:16,885
Speaker 5:  lawmakers are somewhat comfortable with.

237
00:15:17,435 --> 00:15:21,005
Speaker 2:  Okay. I would just point out again that still it's been all this time and

238
00:15:21,005 --> 00:15:24,765
Speaker 2:  we have yet to hear this unbelievably compelling evidence that supposedly

239
00:15:24,765 --> 00:15:28,325
Speaker 2:  exists for why we have to ban TikTok. But we will leave that aside. I think

240
00:15:28,325 --> 00:15:31,005
Speaker 2:  there, there, there really is the question of like, should we ban TikTok

241
00:15:31,025 --> 00:15:34,645
Speaker 2:  and is it going to happen? And I think I assumed those two things

242
00:15:34,755 --> 00:15:38,005
Speaker 2:  were were sort of gonna run together, but it really seems like at this point

243
00:15:38,025 --> 00:15:41,765
Speaker 2:  we, it might happen. I it feels like the odds of this

244
00:15:42,195 --> 00:15:46,045
Speaker 2:  bill becoming law are pretty high at this point. Like this thing is

245
00:15:46,045 --> 00:15:48,685
Speaker 2:  moving fast and it seems like the momentum is gonna be pretty hard to slow

246
00:15:48,685 --> 00:15:49,245
Speaker 2:  down. Right.

247
00:15:49,845 --> 00:15:53,805
Speaker 5:  I would agree with that. Yeah. I mean I, I would go back to like, you

248
00:15:53,805 --> 00:15:57,565
Speaker 5:  know, look at what this bill is situated within. It's this package

249
00:15:57,625 --> 00:16:01,445
Speaker 5:  for foreign aid that's very popular. Is TikTok going to be the

250
00:16:01,445 --> 00:16:05,205
Speaker 5:  thing that senators say, we're gonna like put our foot

251
00:16:05,245 --> 00:16:06,685
Speaker 5:  down about getting this out of here.

252
00:16:06,975 --> 00:16:10,285
Speaker 2:  Right? Yeah. We don't want to help Ukraine because we don't wanna ban TikTok

253
00:16:10,345 --> 00:16:13,725
Speaker 2:  is a, is a, it's a strange political maneuver for anyone to pull at this

254
00:16:13,725 --> 00:16:17,565
Speaker 2:  moment in time. So what's next in this one? I think last time all

255
00:16:17,565 --> 00:16:20,605
Speaker 2:  this happened, tons of momentum and then it died in the Senate. We are now

256
00:16:20,635 --> 00:16:24,565
Speaker 2:  kind of back at that same spot again. Is it possible it's gonna

257
00:16:24,565 --> 00:16:25,365
Speaker 2:  die in the Senate again?

258
00:16:25,845 --> 00:16:29,325
Speaker 5:  I think this one's really different because you know, foreign aid is something

259
00:16:29,325 --> 00:16:33,045
Speaker 5:  that is, has some time urgency behind it. Yeah. And

260
00:16:33,305 --> 00:16:36,765
Speaker 5:  the Senate was supposed to be off this week, but they're actually gonna,

261
00:16:36,785 --> 00:16:40,645
Speaker 5:  you know, come in on their vacation. Aw. And they're going to

262
00:16:40,645 --> 00:16:44,565
Speaker 5:  start doing some like process votes tomorrow. So from

263
00:16:44,565 --> 00:16:48,325
Speaker 5:  what I've read, it seems like they could vote as soon as tomorrow and

264
00:16:48,385 --> 00:16:50,845
Speaker 5:  as late as Wednesday on this measure.

265
00:16:51,395 --> 00:16:54,645
Speaker 2:  Fair enough. Yeah. I should say we're talking on Monday morning and it's

266
00:16:54,645 --> 00:16:58,045
Speaker 2:  like we have like a little less than 24 hours before this publishes and Lord

267
00:16:58,045 --> 00:17:01,285
Speaker 2:  knows everything could change between now and then. But okay. Let's, let's

268
00:17:01,285 --> 00:17:04,765
Speaker 2:  talk about some of the privacy stuff because I think the other thing that

269
00:17:04,765 --> 00:17:07,565
Speaker 2:  you've been writing a bunch about is this new thing, the American Privacy

270
00:17:07,665 --> 00:17:11,045
Speaker 2:  Rights Act. I went into the first story that you wrote about this in which

271
00:17:11,045 --> 00:17:13,645
Speaker 2:  you were like, there's lots of momentum. People are excited this might actually

272
00:17:13,645 --> 00:17:16,565
Speaker 2:  happen. Being like, aw Lauren, you've been, you've been scammed again into

273
00:17:16,565 --> 00:17:20,125
Speaker 2:  believing that the American government is gonna pass privacy

274
00:17:20,155 --> 00:17:23,845
Speaker 2:  legislation. But it seems like there is a, a sense that we're gonna

275
00:17:24,125 --> 00:17:28,045
Speaker 2:  actually maybe get A real privacy law here at some point in the near future.

276
00:17:28,155 --> 00:17:31,165
Speaker 2:  What is going on with this bill? Why is this the one so far?

277
00:17:31,675 --> 00:17:35,445
Speaker 5:  Yeah, look, I mean when it comes to privacy, I'm never gonna say this is

278
00:17:35,455 --> 00:17:39,445
Speaker 5:  definitely probably gonna happen because I think I've just

279
00:17:39,445 --> 00:17:43,045
Speaker 5:  seen this so many times. Yeah. Over years of covering tech policy

280
00:17:43,375 --> 00:17:47,125
Speaker 5:  where you get a bipartisan bill that seems like a pretty great

281
00:17:47,575 --> 00:17:51,205
Speaker 5:  compromise and everyone's like, like at least happy enough and then it goes

282
00:17:51,205 --> 00:17:55,085
Speaker 5:  nowhere. So I think this is one where we really gotta watch it till

283
00:17:55,085 --> 00:17:59,005
Speaker 5:  the end. But there are some promising signs in this legislation

284
00:17:59,135 --> 00:18:02,525
Speaker 5:  which you know, to be clear is still a discussion draft. It hasn't even been

285
00:18:02,645 --> 00:18:06,525
Speaker 5:  formally introduced yet. But I think the most significant part of it

286
00:18:06,585 --> 00:18:10,405
Speaker 5:  is just who's behind this bill? And that's the head

287
00:18:10,425 --> 00:18:14,205
Speaker 5:  of the house Energy and Commerce committee. Kathy McMorris Rogers, who's

288
00:18:14,205 --> 00:18:18,125
Speaker 5:  a Republican from Washington and the head of the Senate commerce committee

289
00:18:18,135 --> 00:18:21,165
Speaker 5:  chair Maria Cantwell, who's a Democrat from Washington. Maria

290
00:18:21,165 --> 00:18:22,285
Speaker 2:  Cantwell everywhere right now.

291
00:18:22,435 --> 00:18:23,925
Speaker 5:  Yeah, exactly. Big

292
00:18:23,925 --> 00:18:25,045
Speaker 2:  Month from Maria Cantwell.

293
00:18:25,345 --> 00:18:29,085
Speaker 5:  It truly is. And I think her sponsorship is

294
00:18:29,265 --> 00:18:33,005
Speaker 5:  the most significant here because last time around when we had

295
00:18:33,405 --> 00:18:37,245
Speaker 5:  a bipartisan privacy bill in 2022, she

296
00:18:37,705 --> 00:18:41,285
Speaker 5:  was not on board. And that seemed to be the thing that ultimately kind of

297
00:18:41,285 --> 00:18:45,045
Speaker 5:  like slowed down momentum behind this bill that had a lot of

298
00:18:45,045 --> 00:18:48,565
Speaker 5:  bipartisan supporters. It passed overwhelmingly out of the house Energy and

299
00:18:48,685 --> 00:18:52,645
Speaker 5:  commerce committee and then just kind of died off. So I think

300
00:18:52,645 --> 00:18:56,165
Speaker 5:  that's really what's most significant here and what's giving people the most,

301
00:18:56,985 --> 00:18:59,805
Speaker 5:  you know, hope that maybe we could see something this time around.

302
00:19:00,275 --> 00:19:03,845
Speaker 2:  Okay. So I, I feel like, I don't know, I don't know what the, what the like

303
00:19:03,895 --> 00:19:07,885
Speaker 2:  least optimistic it is possible to be while still being optimistic. You

304
00:19:07,885 --> 00:19:10,485
Speaker 2:  know what I mean? You're like, you're like at a, we're at a sort of one outta

305
00:19:10,505 --> 00:19:14,085
Speaker 2:  10 on the optimism scale, but we're, we we're at least somewhere and I feel

306
00:19:14,085 --> 00:19:17,365
Speaker 2:  like there is, there is, there's possibility that this could happen. What

307
00:19:17,415 --> 00:19:21,285
Speaker 2:  about this bill jumps out to you? It seems like I, I was reading through

308
00:19:21,285 --> 00:19:24,925
Speaker 2:  it and there's a bunch of stuff that sounds a lot like what we have in the

309
00:19:25,165 --> 00:19:28,765
Speaker 2:  eu, this idea of transparency and you should be able to control your own

310
00:19:28,765 --> 00:19:31,245
Speaker 2:  data and you should know where it's going. Some of the stuff that's going

311
00:19:31,245 --> 00:19:35,005
Speaker 2:  on in California with the California Privacy Act that went in

312
00:19:35,005 --> 00:19:38,645
Speaker 2:  2018 I think it was. And then again in 2020, what's in this

313
00:19:38,715 --> 00:19:41,365
Speaker 2:  bill that jumped out to you? Anything particularly interesting?

314
00:19:41,875 --> 00:19:45,565
Speaker 5:  Yeah, I think the thing that stands out the most is the enforcement mechanisms

315
00:19:45,565 --> 00:19:49,285
Speaker 5:  behind the bill because that's something that Senator Cantwell said that,

316
00:19:49,385 --> 00:19:52,645
Speaker 5:  you know, was kind of a sticking point for her with the earlier privacy bill.

317
00:19:53,065 --> 00:19:56,445
Speaker 5:  The A-D-P-P-A. These all have really complicated acronyms

318
00:19:56,985 --> 00:20:00,525
Speaker 5:  and so this bill basically would let

319
00:20:00,635 --> 00:20:04,605
Speaker 5:  individuals sue companies that they feel like have violated

320
00:20:04,655 --> 00:20:08,405
Speaker 5:  their privacy rights. But it would also give companies a chance

321
00:20:08,505 --> 00:20:12,365
Speaker 5:  to correct the things that they're being told they did

322
00:20:12,365 --> 00:20:16,085
Speaker 5:  wrong. So, you know, that's kind of a way to try and balance,

323
00:20:16,705 --> 00:20:20,525
Speaker 5:  you know, a really strong enforcement mechanism that also, you know,

324
00:20:20,575 --> 00:20:24,405
Speaker 5:  kicks in pretty quickly. Consumers can, you know, sue pretty quickly,

325
00:20:24,585 --> 00:20:28,205
Speaker 5:  but also gives companies a chance to correct any mistakes before

326
00:20:28,435 --> 00:20:30,485
Speaker 5:  getting bogged down with lawsuits.

327
00:20:30,715 --> 00:20:33,005
Speaker 2:  Yeah, that's really interesting. 'cause in the past a lot of the enforcement

328
00:20:33,005 --> 00:20:36,885
Speaker 2:  for this has been kind of hand wavy and some somebody

329
00:20:36,885 --> 00:20:40,365
Speaker 2:  will figure it out, we'll set up an agency kind of thing. And

330
00:20:40,835 --> 00:20:44,245
Speaker 2:  this seems like it was much more directed like who will be in charge of this

331
00:20:44,265 --> 00:20:48,045
Speaker 2:  and what they can do and the penalties will be real. Like even

332
00:20:48,045 --> 00:20:51,765
Speaker 2:  just reading through, there's this like eight page summary of it. I got the

333
00:20:51,765 --> 00:20:54,925
Speaker 2:  sense that like this is, this felt like a real thing more than just sort

334
00:20:54,925 --> 00:20:57,125
Speaker 2:  of some ideas that somebody wrote down.

335
00:20:57,675 --> 00:21:01,245
Speaker 5:  Yeah, I think that's right. I think, you know, enforcement was really a key

336
00:21:01,245 --> 00:21:05,125
Speaker 5:  issue for Senator Cantwell, so I think it's not too surprising to see that

337
00:21:05,405 --> 00:21:08,205
Speaker 5:  that was an area that they seemed to spend a lot of time on.

338
00:21:08,555 --> 00:21:12,485
Speaker 2:  Yeah. Okay. Is there anything we've learned do you think, from the

339
00:21:12,685 --> 00:21:15,965
Speaker 2:  EU and California? Like tho those are the two that I, I just kept seeing

340
00:21:16,225 --> 00:21:19,805
Speaker 2:  inside of this bill and, and the, I went back and re-read the California

341
00:21:19,805 --> 00:21:23,725
Speaker 2:  one and it even uses a lot of the same terms right, about being able to

342
00:21:24,115 --> 00:21:28,085
Speaker 2:  correct info about you that's online, that's wrong. Having

343
00:21:28,085 --> 00:21:31,525
Speaker 2:  the right to opt out, they can't discriminate against you if they opt out.

344
00:21:31,525 --> 00:21:34,085
Speaker 2:  There's also just like a weird thing all of a sudden in the middle that just

345
00:21:34,085 --> 00:21:37,885
Speaker 2:  says civil rights and algorithms, which felt like a total sort of random

346
00:21:38,065 --> 00:21:40,725
Speaker 2:  detour in the middle of a data privacy legislation. But I thought that was

347
00:21:40,725 --> 00:21:44,645
Speaker 2:  very interesting. I don't know, does it feel like the Congress is sort

348
00:21:44,645 --> 00:21:48,405
Speaker 2:  of learning from what has worked and passed elsewhere? I mean I, I, I use

349
00:21:48,405 --> 00:21:52,365
Speaker 2:  worked in a very kind of loose way because none of it seems to really be

350
00:21:52,365 --> 00:21:55,485
Speaker 2:  working, but at least what has actually been passed into law elsewhere.

351
00:21:55,875 --> 00:21:59,765
Speaker 5:  Yeah, I think like at a high level there's been a shift from this

352
00:21:59,955 --> 00:22:03,565
Speaker 5:  like notice and consent framework that's basically like,

353
00:22:03,905 --> 00:22:07,765
Speaker 5:  you know, we're gonna tell consumers, you know, what sort of information

354
00:22:07,765 --> 00:22:11,245
Speaker 5:  we're collecting and then you click yes and then we collect it,

355
00:22:11,975 --> 00:22:14,205
Speaker 2:  Which accomplishes nothing. Right? I really believe that.

356
00:22:14,555 --> 00:22:18,485
Speaker 5:  Yeah. Or and shifted more to this idea of data minimization,

357
00:22:18,485 --> 00:22:22,285
Speaker 5:  which is like, let's like limit how much data is collected in the first place

358
00:22:22,425 --> 00:22:25,925
Speaker 5:  and you have to have a really compelling reason for why you need this information

359
00:22:26,025 --> 00:22:29,845
Speaker 5:  to run your service. And so I think that's something that's reflected in

360
00:22:29,845 --> 00:22:30,805
Speaker 5:  this bill.

361
00:22:31,035 --> 00:22:34,005
Speaker 2:  Yeah, that's really interesting. Who are they going after with this bill?

362
00:22:34,005 --> 00:22:37,845
Speaker 2:  Have we gotten a sense yet? I mean they have some basic outlines here.

363
00:22:37,925 --> 00:22:41,845
Speaker 2:  I think it's like if you have more than 5 million users

364
00:22:41,945 --> 00:22:45,645
Speaker 2:  in the US or over $250 million in revenue, it's, it's clearly

365
00:22:45,845 --> 00:22:49,685
Speaker 2:  a, this is pushed at big companies especially, right? It talk,

366
00:22:49,685 --> 00:22:53,365
Speaker 2:  it talks a lot about data brokers, but it seems like the companies most affected

367
00:22:53,365 --> 00:22:57,165
Speaker 2:  by this are going to be like the handful of big tech platforms. Like

368
00:22:57,245 --> 00:22:58,445
Speaker 2:  who do you think this is really aimed at?

369
00:22:58,765 --> 00:23:02,205
Speaker 5:  I think that's right. I think, you know, there is concern in congress about,

370
00:23:02,305 --> 00:23:06,125
Speaker 5:  you know, overly burdening small companies and startups

371
00:23:06,565 --> 00:23:10,285
Speaker 5:  and that kind of feeds into these other questions about competition and

372
00:23:10,425 --> 00:23:13,965
Speaker 5:  you know, concerns there. So I think there is an awareness that congress

373
00:23:13,965 --> 00:23:17,885
Speaker 5:  doesn't want to overburden small businesses. So I

374
00:23:17,885 --> 00:23:21,445
Speaker 5:  think large companies are really ones that will be targeted here.

375
00:23:21,865 --> 00:23:24,925
Speaker 5:  And data brokers, it seems like there's, you know, this growing awareness

376
00:23:25,105 --> 00:23:28,445
Speaker 5:  in Congress about how data brokers can move

377
00:23:28,445 --> 00:23:32,205
Speaker 5:  information around, you know, around the time of the original TikTok

378
00:23:32,595 --> 00:23:36,445
Speaker 5:  legislation we saw a separate bill in the house pass around data

379
00:23:36,475 --> 00:23:40,245
Speaker 5:  brokers and you know, sales of US data to foreign adversaries.

380
00:23:40,785 --> 00:23:44,725
Speaker 5:  So I think that's something that is really on lawmakers minds

381
00:23:44,725 --> 00:23:45,165
Speaker 5:  these days.

382
00:23:45,555 --> 00:23:49,205
Speaker 2:  Okay. So like you mentioned, we've, we've seen this a few times. Bills come

383
00:23:49,205 --> 00:23:53,005
Speaker 2:  out that there is some enthusiasm about what

384
00:23:53,405 --> 00:23:57,245
Speaker 2:  normally happens. Like where in the process does a privacy bill usually

385
00:23:57,465 --> 00:23:57,685
Speaker 2:  die?

386
00:23:58,185 --> 00:24:02,165
Speaker 5:  In the past it's been, you know, with the A-D-P-P-A, which was the

387
00:24:02,165 --> 00:24:06,005
Speaker 5:  last time we had a comprehensive privacy bill that had

388
00:24:06,005 --> 00:24:06,965
Speaker 5:  bipartisan support.

389
00:24:07,145 --> 00:24:08,925
Speaker 2:  And that was two years ago, is that

390
00:24:08,925 --> 00:24:12,765
Speaker 5:  Right? Yeah, I believe that was in 2022. You know, it passed pretty

391
00:24:13,085 --> 00:24:17,005
Speaker 5:  strongly out of the house Energy and commerce committee and then that was

392
00:24:17,005 --> 00:24:20,845
Speaker 5:  it. So that's the point where we see these things slowed down.

393
00:24:21,125 --> 00:24:25,045
Speaker 5:  I think what's significant here is we have both of the chairs who

394
00:24:25,045 --> 00:24:28,485
Speaker 5:  are in power in their committees on the house and Senate side

395
00:24:28,905 --> 00:24:32,845
Speaker 5:  who are going to be able to move this bill if they want, assuming they

396
00:24:32,845 --> 00:24:36,605
Speaker 5:  can get the votes in their committee. So, you know, we could at this point

397
00:24:36,785 --> 00:24:40,485
Speaker 5:  see both the Senate Commerce Committee and the House Energy and Commerce

398
00:24:40,485 --> 00:24:44,165
Speaker 5:  committee pass this bill once it's introduced and then

399
00:24:44,175 --> 00:24:47,845
Speaker 5:  it'll be up to house and senate leaders to

400
00:24:47,845 --> 00:24:51,685
Speaker 5:  determine again, do they give this bill time on the floor,

401
00:24:51,825 --> 00:24:55,485
Speaker 5:  do they package it in some other legislation? So I think that will be

402
00:24:55,675 --> 00:24:59,565
Speaker 5:  kind of the next point to look out for. And of course, you know, before all

403
00:24:59,565 --> 00:25:02,725
Speaker 5:  that happens, we have to make sure that there's support enough support in

404
00:25:02,725 --> 00:25:06,605
Speaker 5:  the committees for these bills. But I think there really is, you know,

405
00:25:06,615 --> 00:25:10,285
Speaker 5:  widespread support in congress for comprehensive privacy

406
00:25:10,475 --> 00:25:14,285
Speaker 5:  legislation overall. So I, I think, you know, there

407
00:25:14,385 --> 00:25:17,125
Speaker 5:  is some, at least a good amount of political will there.

408
00:25:17,625 --> 00:25:21,605
Speaker 2:  Do you make anything of the timing connection with all of the TikTok

409
00:25:21,605 --> 00:25:24,845
Speaker 2:  stuff? Like are these two things that are happening simultaneously

410
00:25:25,465 --> 00:25:25,885
Speaker 2:  on purpose?

411
00:25:26,555 --> 00:25:30,125
Speaker 5:  Yeah, I, I mean it's a great question. I'm not sure if it's like, you know,

412
00:25:30,345 --> 00:25:34,085
Speaker 5:  on purpose in the sense that they were, you know, moved at this time

413
00:25:34,475 --> 00:25:37,965
Speaker 5:  because of the TikTok legislation. But I think,

414
00:25:38,345 --> 00:25:42,005
Speaker 5:  you know, all of this is kind of like percolating around congress and

415
00:25:42,485 --> 00:25:46,325
Speaker 5:  I think it makes sense in a way that we would start to see privacy

416
00:25:46,715 --> 00:25:50,645
Speaker 5:  legislation come out. You know, when we saw the house vote on the

417
00:25:50,645 --> 00:25:53,765
Speaker 5:  standalone TikTok bill, reporters were asking

418
00:25:54,415 --> 00:25:58,325
Speaker 5:  house committee leaders about, well what about privacy? Because that's

419
00:25:58,325 --> 00:26:01,685
Speaker 5:  something that kind of like underlies a lot of these conversations in the

420
00:26:01,685 --> 00:26:05,325
Speaker 5:  first place. And I, I think I remember Mick Morris Rogers

421
00:26:05,605 --> 00:26:08,125
Speaker 5:  actually kind of saying like, you know, we're still working on that. And

422
00:26:08,345 --> 00:26:11,965
Speaker 5:  at the time I remember thinking like, okay, sure, like, like

423
00:26:12,075 --> 00:26:16,005
Speaker 5:  yeah, we're gonna go back. Maybe they're gonna reintroduce the A DPA, something

424
00:26:16,005 --> 00:26:19,525
Speaker 5:  like that. But then we got this totally new bill with Senator

425
00:26:19,645 --> 00:26:23,485
Speaker 5:  Cantwell and that was kind of a moment of, well okay, maybe

426
00:26:24,035 --> 00:26:28,005
Speaker 5:  some of this has been really happening behind the scenes. So

427
00:26:28,005 --> 00:26:31,845
Speaker 5:  yeah, I think, you know, it is significant that privacy is

428
00:26:31,845 --> 00:26:35,645
Speaker 5:  something that kind of underpins a lot of the other conversations we have

429
00:26:35,745 --> 00:26:39,445
Speaker 5:  around tech policy. But you know, I, I think

430
00:26:39,745 --> 00:26:43,725
Speaker 5:  things often in Congress just move when you can get the political will to

431
00:26:43,725 --> 00:26:44,045
Speaker 5:  do them

432
00:26:44,635 --> 00:26:48,325
Speaker 2:  Well. And that's I think kind of the big open question still to me is,

433
00:26:48,825 --> 00:26:52,765
Speaker 2:  is this moment different in that respect than we've had before? Right. I

434
00:26:52,765 --> 00:26:56,605
Speaker 2:  think the privacy thing in general is so tricky because there is

435
00:26:56,605 --> 00:26:59,285
Speaker 2:  this sort of intellectual understanding that I think most people have come

436
00:26:59,285 --> 00:27:03,165
Speaker 2:  to where we all kind of know there's

437
00:27:03,165 --> 00:27:07,125
Speaker 2:  too much information about us out there and it's bad

438
00:27:07,125 --> 00:27:09,765
Speaker 2:  that there is too much access to it and we kind of have no control over it

439
00:27:09,825 --> 00:27:13,645
Speaker 2:  and it sort of feels intellectually weird. But knowing

440
00:27:14,155 --> 00:27:17,405
Speaker 2:  what that is and what to do about it and what the sort of downstream ramifications

441
00:27:17,405 --> 00:27:21,045
Speaker 2:  are is just complicated enough that

442
00:27:21,265 --> 00:27:24,605
Speaker 2:  it has never really felt like anyone's first priority. It's just kind of

443
00:27:24,605 --> 00:27:28,205
Speaker 2:  a thing that lingers at the back of everybody's mind. But then part of me

444
00:27:28,205 --> 00:27:31,445
Speaker 2:  wonders a, with TikTok becoming this very visceral

445
00:27:31,575 --> 00:27:35,125
Speaker 2:  outpouring of what it looks like when a platform knows you very well, that

446
00:27:35,265 --> 00:27:39,245
Speaker 2:  is one thing that I think has like maybe made it more present

447
00:27:39,305 --> 00:27:43,085
Speaker 2:  to people what's really going on here. But then I also just wonder if this

448
00:27:43,085 --> 00:27:46,845
Speaker 2:  is yet another kind of lark from people

449
00:27:46,905 --> 00:27:50,005
Speaker 2:  who, there's just a, a kind of chorus of folks in Congress who are like,

450
00:27:50,005 --> 00:27:52,805
Speaker 2:  we need privacy legislation. And everybody goes, I agree. And then just kind

451
00:27:52,805 --> 00:27:55,845
Speaker 2:  of goes off to other priorities. But it sounds like even maybe just from

452
00:27:55,845 --> 00:27:59,005
Speaker 2:  like conversations you're having that at least

453
00:27:59,715 --> 00:28:02,885
Speaker 2:  it's moving up the priority list. It might not be the top thing, but at the

454
00:28:02,885 --> 00:28:06,645
Speaker 2:  very least it is a thing people are willingly

455
00:28:06,645 --> 00:28:10,125
Speaker 2:  talking about instead of just sort of waving their hands at and saying, we'll

456
00:28:10,125 --> 00:28:12,765
Speaker 2:  get to that eventually. Is that, is that how it feels just to talk to people

457
00:28:12,765 --> 00:28:13,645
Speaker 2:  right now? I

458
00:28:13,645 --> 00:28:17,045
Speaker 5:  Think basically, I think, you know, I didn't necessarily

459
00:28:17,175 --> 00:28:21,045
Speaker 5:  anticipate that we would get a real conversation around comprehensive

460
00:28:21,045 --> 00:28:24,405
Speaker 5:  privacy this year. You know, it's an election year. I mean

461
00:28:24,405 --> 00:28:25,365
Speaker 2:  There's a lot going on.

462
00:28:25,555 --> 00:28:29,525
Speaker 5:  Yeah. So I think there is this sense that, you know,

463
00:28:29,655 --> 00:28:33,525
Speaker 5:  maybe, maybe this is the time, but you know, I'm hesitant to say that

464
00:28:33,525 --> 00:28:37,245
Speaker 5:  because it's felt like that in the past. But I think there's a certain way

465
00:28:37,245 --> 00:28:40,285
Speaker 5:  you could look at this and say, maybe it would be kind of a good look for

466
00:28:40,525 --> 00:28:44,485
Speaker 5:  Congress to pass a comprehensive privacy bill if they're also passing this

467
00:28:44,545 --> 00:28:48,485
Speaker 5:  TikTok legislation since, you know, I think when TikTok

468
00:28:48,645 --> 00:28:52,525
Speaker 5:  CEO came to testify, some people are talking about, well, aren't all

469
00:28:52,685 --> 00:28:56,485
Speaker 5:  of these problems just like problems with privacy on platforms in

470
00:28:56,485 --> 00:29:00,445
Speaker 5:  general? And some of them are. So I think in a way it would

471
00:29:00,445 --> 00:29:04,365
Speaker 5:  be kind of a good look for Congress to pass comprehensive privacy reform

472
00:29:04,515 --> 00:29:07,845
Speaker 5:  alongside something like that. Is that going to be enough to push them over

473
00:29:07,845 --> 00:29:11,685
Speaker 5:  the edge? You know, I think that's anyone's guess, but at least it

474
00:29:11,685 --> 00:29:15,485
Speaker 5:  seems like these conversations are really happening in a real

475
00:29:15,765 --> 00:29:19,285
Speaker 5:  tangible way, again, around an actual piece of legislation.

476
00:29:19,795 --> 00:29:23,725
Speaker 2:  Yeah, I agree. I think my, my guess is we are a ways away from

477
00:29:23,725 --> 00:29:26,525
Speaker 2:  having an answer to that one, but I think it's gonna be fascinating, especially

478
00:29:26,525 --> 00:29:29,805
Speaker 2:  in an election year when so many people are talking about tech. I think it's,

479
00:29:29,965 --> 00:29:33,725
Speaker 2:  it's gonna come up maybe more than it has even at kind of

480
00:29:33,745 --> 00:29:37,085
Speaker 2:  hot moments in the past. It's gonna be really interesting. Before I let you

481
00:29:37,085 --> 00:29:41,005
Speaker 2:  go, the last one I just wanna touch on briefly is the future of

482
00:29:41,225 --> 00:29:44,605
Speaker 2:  AI Innovation Act, which is another thing that is sort of in the air

483
00:29:44,605 --> 00:29:48,405
Speaker 2:  everybody's talking about, but it's not necessarily exactly clear

484
00:29:48,535 --> 00:29:51,605
Speaker 2:  where we are headed with this. Tell me a little bit about the, the future

485
00:29:51,605 --> 00:29:53,325
Speaker 2:  of AI Innovation Act. What are we doing here?

486
00:29:53,595 --> 00:29:57,445
Speaker 5:  Yeah, so this is a bill that was introduced by a bunch

487
00:29:57,505 --> 00:30:01,125
Speaker 5:  of senators that have been involved in AI discussions. Again, we have Senator

488
00:30:01,245 --> 00:30:01,805
Speaker 5:  Cantwell

489
00:30:02,285 --> 00:30:03,165
Speaker 2:  Cantwell man.

490
00:30:03,435 --> 00:30:07,325
Speaker 5:  Yeah. We have Senator Todd Young who has been involved

491
00:30:07,425 --> 00:30:11,085
Speaker 5:  in some of the bipartisan work with Senator

492
00:30:11,115 --> 00:30:14,405
Speaker 5:  Schumer around ai. So this bill would basically,

493
00:30:14,915 --> 00:30:18,445
Speaker 5:  it's basically about creating standards around AI

494
00:30:18,865 --> 00:30:22,165
Speaker 5:  and just, you know, spurring innovation. So, you know, it's not really dealing

495
00:30:22,165 --> 00:30:25,565
Speaker 5:  with some of these like nitty gritty issues that we're thinking about with

496
00:30:25,565 --> 00:30:28,245
Speaker 5:  like copyright and things like that. But it's really about just trying to

497
00:30:28,245 --> 00:30:32,085
Speaker 5:  like create a framework for AI to be developed

498
00:30:32,105 --> 00:30:35,605
Speaker 5:  in a safe way and also for innovation to take place.

499
00:30:36,115 --> 00:30:39,765
Speaker 2:  Okay. So this is one of those bills that is like, let's, let's make a plan

500
00:30:39,765 --> 00:30:43,125
Speaker 2:  to make a plan kind of thing. And you have to write that in a very, and I,

501
00:30:43,315 --> 00:30:46,365
Speaker 2:  that feels small when I say it, but there was like, there was that executive

502
00:30:46,365 --> 00:30:48,605
Speaker 2:  order from the White House, which was like a plan to make a plan to make

503
00:30:48,605 --> 00:30:52,285
Speaker 2:  a plan. This is at least like a plan to make a plan. So we're like, we're

504
00:30:52,285 --> 00:30:54,565
Speaker 2:  like inching towards a plan. Yeah, it's, it's

505
00:30:54,565 --> 00:30:58,205
Speaker 5:  Something. Yeah. Yeah, I mean it's certainly a start. I mean I think AI is

506
00:30:58,205 --> 00:31:01,805
Speaker 5:  such a huge issue that that touches on so many different areas that

507
00:31:02,155 --> 00:31:05,805
Speaker 5:  it's hard to imagine just like one piece of legislation being the be all

508
00:31:05,805 --> 00:31:09,445
Speaker 5:  end all of this. So, you know, it's certainly, it, it's a reasonable place

509
00:31:09,445 --> 00:31:10,845
Speaker 5:  to start, I would say. Yeah,

510
00:31:10,885 --> 00:31:14,365
Speaker 2:  I still remember the Benedict Evans, who the back when he was a venture capitalist

511
00:31:14,365 --> 00:31:17,805
Speaker 2:  always had this line that we don't regulate cars, we regulate parts of cars,

512
00:31:18,405 --> 00:31:21,085
Speaker 2:  which I think is very smart. And I think AI is gonna end up being the same

513
00:31:21,085 --> 00:31:24,805
Speaker 2:  way. Like instead of if we try to regulate AI as a thing,

514
00:31:25,195 --> 00:31:27,965
Speaker 2:  it's just gonna, it'll never work. And I feel like to some extent we've run

515
00:31:27,965 --> 00:31:30,765
Speaker 2:  into this with, with privacy in the internet. Like we try to regulate the

516
00:31:30,765 --> 00:31:34,325
Speaker 2:  whole internet all at once and it's just impossible. But it's like, okay,

517
00:31:34,325 --> 00:31:37,165
Speaker 2:  we can regulate seat belts and we can regulate steering wheels and we can

518
00:31:37,405 --> 00:31:40,725
Speaker 2:  regulate carburetors and we can regulate gas mileage. And it's like that's

519
00:31:40,775 --> 00:31:44,565
Speaker 2:  maybe what we do with ai and this feels like, you know, setting up the,

520
00:31:44,595 --> 00:31:47,405
Speaker 2:  what is it called, the Artificial Intelligence Safety Institute, I think

521
00:31:47,825 --> 00:31:51,605
Speaker 2:  is like, okay, you need a group that is like in charge of looking

522
00:31:51,625 --> 00:31:55,525
Speaker 2:  at this. And that's not anything, but it is a step towards something. And

523
00:31:55,525 --> 00:31:59,085
Speaker 2:  that I sort of appreciate as a, as a way of saying like, okay,

524
00:31:59,305 --> 00:32:02,805
Speaker 2:  AI is too big for one bill that we shove into a foreign aid package.

525
00:32:03,575 --> 00:32:04,645
Speaker 2:  Let's let's start somewhere else.

526
00:32:05,075 --> 00:32:08,845
Speaker 5:  Yeah. Also, I think there's certain things that, you know, the

527
00:32:09,045 --> 00:32:12,885
Speaker 5:  American people would probably rather see a specialized agency looking

528
00:32:12,905 --> 00:32:16,405
Speaker 5:  at rather than congress just going ahead and dictating a bunch of rules.

529
00:32:16,545 --> 00:32:18,685
Speaker 5:  So I think that is a significant step.

530
00:32:18,835 --> 00:32:22,725
Speaker 2:  Yeah, fair enough. Alright, well here's hoping that you at least get a weekend

531
00:32:22,755 --> 00:32:26,125
Speaker 2:  next weekend, but Lord only knows this TikTok thing is just gonna keep happening.

532
00:32:26,185 --> 00:32:28,725
Speaker 2:  So I I suspect we will be talking again sooner rather than later.

533
00:32:29,025 --> 00:32:29,685
Speaker 5:  Yes, I'm sure.

534
00:32:30,045 --> 00:32:31,365
Speaker 2:  Awesome. Thanks Lauren. Appreciate it.

535
00:32:31,365 --> 00:32:31,925
Speaker 5:  Thanks David.

536
00:32:32,475 --> 00:32:35,325
Speaker 2:  Alright, we gotta take a break and then we're gonna come back and

537
00:35:07,275 --> 00:35:11,055
Speaker 2:  All right, we're back. A couple of weeks ago, The Verge did a series of panels

538
00:35:11,055 --> 00:35:14,895
Speaker 2:  at the Chicago Humanities Festival, all about AI and creativity.

539
00:35:15,275 --> 00:35:19,055
Speaker 2:  Our big question basically was what does it mean to be a creative

540
00:35:19,075 --> 00:35:23,015
Speaker 2:  person in a world filled with ai? And how is AI changing

541
00:35:23,155 --> 00:35:26,655
Speaker 2:  the lives and livelihoods of creative people? We had three

542
00:35:26,895 --> 00:35:30,815
Speaker 2:  really fun conversations and I just wanna play you a chunk of mine. I got

543
00:35:30,815 --> 00:35:34,535
Speaker 2:  on stage with Ty Sheridan, who's an actor you might know as Wade Watts, the

544
00:35:34,535 --> 00:35:38,485
Speaker 2:  protagonist in Ready Player one or from movies like X-Men Apocalypse in

545
00:35:38,485 --> 00:35:42,365
Speaker 2:  the Show Wireless along with Nikola Tvi, who is a writer and a

546
00:35:42,365 --> 00:35:46,205
Speaker 2:  visual effects artist. Ty and Nikola are friends, they're collaborators.

547
00:35:46,545 --> 00:35:50,125
Speaker 2:  And as of a few years ago, they're co-founders. They started a company called

548
00:35:50,125 --> 00:35:54,005
Speaker 2:  Wonder Dynamics that brings a bunch of really cool AI tools to the visual

549
00:35:54,005 --> 00:35:57,685
Speaker 2:  effects business. Basically imagine you film a real

550
00:35:57,735 --> 00:36:01,725
Speaker 2:  scene and with like three button clicks you can comp

551
00:36:01,995 --> 00:36:05,885
Speaker 2:  over a regular person and turn it into an animated character.

552
00:36:06,105 --> 00:36:09,085
Speaker 2:  Visual effects with essentially the touch of a button. It's pretty cool.

553
00:36:09,265 --> 00:36:13,125
Speaker 2:  So both Tie and Nikola are right in the center of this really

554
00:36:13,125 --> 00:36:17,045
Speaker 2:  complicated AI moment. So I wanted to talk to them about how they

555
00:36:17,045 --> 00:36:20,685
Speaker 2:  think about ai, how they're trying to build a company, how human

556
00:36:20,815 --> 00:36:24,725
Speaker 2:  creativity and AI can work together and lots more. We had a really good

557
00:36:24,725 --> 00:36:28,365
Speaker 2:  time chatting actually, and I learned a lot so I thought I'd just play you

558
00:36:28,525 --> 00:36:30,485
Speaker 2:  a bit of that conversation. Let's do it.

559
00:36:32,575 --> 00:36:33,325
Speaker 7:  Hello everybody.

560
00:36:34,185 --> 00:36:37,965
Speaker 2:  So I wanna start by telling the story of Wonder Dynamics.

561
00:36:38,355 --> 00:36:42,285
Speaker 7:  Well, we had originally started developing shows together, so Nico and I

562
00:36:42,285 --> 00:36:45,965
Speaker 7:  were both dabbling writing and you know, Nico had had directed it several

563
00:36:45,965 --> 00:36:48,925
Speaker 7:  films and they showed me, I thought he was a really talented filmmaker and

564
00:36:49,155 --> 00:36:53,125
Speaker 7:  obviously a really smart guy. And we just became kind of pals through our,

565
00:36:53,125 --> 00:36:56,725
Speaker 7:  our mutual love of movies and storytelling. And we had started developing

566
00:36:56,805 --> 00:37:00,725
Speaker 7:  a a, a series that had kind of a, it was like, had an interactive

567
00:37:00,965 --> 00:37:04,885
Speaker 7:  element to it and, and, and there wasn't really a technology that existed.

568
00:37:04,985 --> 00:37:08,365
Speaker 7:  So we just kind of built a prototype and that was really the beginning of

569
00:37:08,365 --> 00:37:10,605
Speaker 7:  us getting into technology.

570
00:37:11,195 --> 00:37:14,805
Speaker 8:  It's really the writing that started it. We, we, we wrote a project that

571
00:37:14,805 --> 00:37:18,245
Speaker 8:  we wanted say, and this is gonna sound super meta, but it was about robotics,

572
00:37:18,295 --> 00:37:22,285
Speaker 8:  right? So we're writing this sci-fi series that we wanted to make. And

573
00:37:22,285 --> 00:37:26,005
Speaker 8:  every time you write something as an indie person especially, you have to

574
00:37:26,005 --> 00:37:29,965
Speaker 8:  think about how do I cut my script to afford it? 'cause you can't go out

575
00:37:29,965 --> 00:37:33,205
Speaker 8:  and write dune and hope to get it funded, right? As a, as a

576
00:37:33,395 --> 00:37:37,285
Speaker 8:  filmmaker that hasn't proven itself, especially in visual effects. Most indie

577
00:37:37,285 --> 00:37:40,005
Speaker 8:  filmmakers and visual effects stay away from visual effects and CG 'cause

578
00:37:40,005 --> 00:37:43,685
Speaker 8:  it's so expensive, right? Every single scene or shot you add that has a visual

579
00:37:43,685 --> 00:37:47,405
Speaker 8:  effects element add so much more to your budget and you can't really

580
00:37:47,585 --> 00:37:51,445
Speaker 8:  afford it. So China really started like that. We, we, we, we wrote this and

581
00:37:51,445 --> 00:37:54,925
Speaker 8:  we realized, okay, this would cost about $150 million to make and there's

582
00:37:54,925 --> 00:37:58,365
Speaker 8:  no way we can get that money. And so we started looking at technology, but

583
00:37:58,365 --> 00:38:01,405
Speaker 8:  funny enough we were studying technology as we were riding and it really,

584
00:38:01,435 --> 00:38:05,245
Speaker 8:  this started, you know, from us going really deep in robotics

585
00:38:05,425 --> 00:38:08,445
Speaker 8:  and self-driving cars technology, which is all about understanding the world

586
00:38:08,505 --> 00:38:11,725
Speaker 8:  and space around you, right? That's, that's movement of a GI, which is the

587
00:38:11,885 --> 00:38:14,765
Speaker 8:  computer vision. It's like understanding what a chair is, understanding how

588
00:38:14,765 --> 00:38:17,645
Speaker 8:  much distance I have from something. So that's how it started. Then we started

589
00:38:17,755 --> 00:38:20,525
Speaker 8:  getting deeper, deeper into it and then we started building the tech and

590
00:38:20,725 --> 00:38:24,045
Speaker 8:  realized, okay, this is a bit bigger than just our movies. Let's turn it

591
00:38:24,045 --> 00:38:27,725
Speaker 8:  into a platform and let's really start a company to be focused towards that.

592
00:38:27,745 --> 00:38:31,525
Speaker 8:  And then we'll make our movies down the line. But we, we, we were lucky that

593
00:38:31,525 --> 00:38:35,445
Speaker 8:  we discovered it early before Gene tv I was even a thing we'd

594
00:38:35,445 --> 00:38:38,445
Speaker 8:  internally call it visual ai. There was no, there was no term at the time

595
00:38:38,745 --> 00:38:41,805
Speaker 8:  and we knew it was gonna be. So we built in stealth mode for about close

596
00:38:41,805 --> 00:38:42,325
Speaker 8:  to four years.

597
00:38:42,755 --> 00:38:46,525
Speaker 2:  Just real quick, give me the like 15 second explanation of what Wonder

598
00:38:46,525 --> 00:38:48,925
Speaker 2:  Studio does right now, just for folks who haven't seen it before. Yeah,

599
00:38:48,925 --> 00:38:52,205
Speaker 8:  So essentially if you have seen movies like Avatar, you always have this

600
00:38:52,205 --> 00:38:56,125
Speaker 8:  motion capture suits, the people performing, and then you have a lot of equipment

601
00:38:56,125 --> 00:38:59,525
Speaker 8:  you need to do on set to pick up that data, performance data, et cetera.

602
00:38:59,785 --> 00:39:03,685
Speaker 8:  So we essentially sped up that process and, and let

603
00:39:03,685 --> 00:39:06,965
Speaker 8:  you extract your scene in a 3D space so you can get your performance

604
00:39:07,235 --> 00:39:10,645
Speaker 8:  capture your camera move, capture your lighting capture and get to your final

605
00:39:10,805 --> 00:39:14,045
Speaker 8:  VFX shot much, much quicker than traditional. Okay.

606
00:39:14,045 --> 00:39:17,965
Speaker 2:  Yeah. The, the demo I've seen is basically it's like a, a person coming out

607
00:39:17,965 --> 00:39:20,485
Speaker 2:  of, I think it's a library or something sort of coming down the stairs of

608
00:39:20,485 --> 00:39:24,165
Speaker 2:  the building and then it wipes and all of a sudden it's, it's a cartoon character

609
00:39:24,165 --> 00:39:27,205
Speaker 2:  doing the same movements in the same place the same way as, and, and you're

610
00:39:27,205 --> 00:39:31,045
Speaker 2:  essentially like automating that out of having tie

611
00:39:31,115 --> 00:39:34,845
Speaker 2:  wear a mocap suit, which I'm assuming is not a lot of fun by the way.

612
00:39:35,465 --> 00:39:38,765
Speaker 7:  It depends on how sweaty you are. You know, if you're a naturally sweaty

613
00:39:38,765 --> 00:39:42,645
Speaker 7:  person, no, it's not that much fun. Fair enough. Also, those volumes,

614
00:39:42,645 --> 00:39:45,285
Speaker 7:  you know, they have all the infrared cameras and sensors in there, they're

615
00:39:45,285 --> 00:39:48,085
Speaker 7:  very warm, so Yeah. Yeah. And the extremely,

616
00:39:48,085 --> 00:39:51,645
Speaker 8:  Extremely expensive as very expensive. That's why you have a lot of, you

617
00:39:51,645 --> 00:39:55,405
Speaker 8:  know, 99% of filmmakers cannot afford production side of things, but post-production

618
00:39:55,405 --> 00:39:59,005
Speaker 8:  is really where, where it gets gets extremely, extremely expensive. Right?

619
00:39:59,005 --> 00:40:01,805
Speaker 8:  So that, that was our mission is like how do we get indie filmmakers to have

620
00:40:01,805 --> 00:40:05,325
Speaker 8:  access to this, which traditionally has always been reserved to high budgets

621
00:40:05,325 --> 00:40:06,005
Speaker 8:  and big studios.

622
00:40:06,395 --> 00:40:10,165
Speaker 2:  Yeah, and I think the timing that you guys were dealing with is really

623
00:40:10,445 --> 00:40:14,405
Speaker 2:  interesting because on the one hand you got this massive technical

624
00:40:14,405 --> 00:40:17,045
Speaker 2:  development, right? Like the, the state of the art and everything you're

625
00:40:17,045 --> 00:40:20,805
Speaker 2:  talking about has gone up exponentially over the last few years. On the other

626
00:40:20,835 --> 00:40:24,605
Speaker 2:  hand, we're now talking about this stuff in a cultural way.

627
00:40:24,605 --> 00:40:28,405
Speaker 2:  Like we've been doing all day in a very different way than we were before.

628
00:40:28,405 --> 00:40:32,125
Speaker 2:  You have access to, you know, mountains of data that comes

629
00:40:32,155 --> 00:40:35,445
Speaker 2:  from, in some cases artists who would prefer that their data not be there.

630
00:40:35,625 --> 00:40:38,125
Speaker 2:  And so I'm, I'm really curious as you guys were thinking about like product

631
00:40:38,525 --> 00:40:42,005
Speaker 2:  building especially coming from this sort of creative background. Like what,

632
00:40:42,005 --> 00:40:45,845
Speaker 2:  what should we build? What can we build? Like sort of where, where

633
00:40:45,845 --> 00:40:48,725
Speaker 2:  does the right answer for something like this begin and end? Like are those

634
00:40:48,725 --> 00:40:49,605
Speaker 2:  conversations you're having

635
00:40:50,105 --> 00:40:53,125
Speaker 8:  Ly? Yeah, a lot. Yeah, A lot. And especially from beginning I think we're

636
00:40:53,125 --> 00:40:56,845
Speaker 8:  in a different situation because you know, obviously we've been super fortunate

637
00:40:56,845 --> 00:40:59,565
Speaker 8:  to have some leaders in a space like Joe Ruso, Steven Spielberg, and others

638
00:40:59,565 --> 00:41:02,845
Speaker 8:  who've been on our board obviously ties an actor. So we spent a lot of time

639
00:41:02,865 --> 00:41:06,245
Speaker 8:  and I was a visual effects artist and a supervisor. So

640
00:41:06,865 --> 00:41:10,645
Speaker 8:  we purposely from day one build it with, we're not gonna be

641
00:41:10,645 --> 00:41:14,245
Speaker 8:  generating art, which is gonna extract information, we call it

642
00:41:14,245 --> 00:41:18,165
Speaker 8:  observing nature. So we're observing how the actor is moving and performing.

643
00:41:18,195 --> 00:41:21,525
Speaker 8:  It's still driven by a, by a shot from a cinematographer.

644
00:41:22,025 --> 00:41:25,565
Speaker 8:  The character is still built by an artist. So we are very different than

645
00:41:25,565 --> 00:41:28,765
Speaker 8:  a lot of gen AI companies that you see, which, which are generating pixels

646
00:41:28,785 --> 00:41:32,645
Speaker 8:  by other art for us, you mostly get information in that 3D space. So

647
00:41:32,645 --> 00:41:36,085
Speaker 8:  we're not, not really producing art, we're speeding you up and giving you

648
00:41:36,085 --> 00:41:39,845
Speaker 8:  that scene in 3D space so you can manipulate it. So, and also you'll never

649
00:41:39,845 --> 00:41:43,525
Speaker 8:  see us replace the actor with another human. We only

650
00:41:43,825 --> 00:41:47,245
Speaker 8:  are driving characters that don't exist in the real world. So that's a little

651
00:41:47,245 --> 00:41:50,805
Speaker 8:  bit of a, you know, a thing we had to stop ourself a lot of time. 'cause

652
00:41:50,805 --> 00:41:54,645
Speaker 8:  you get, you get very driven with this innovation and you can,

653
00:41:54,665 --> 00:41:57,965
Speaker 8:  you can go that route and say, you know, I'm, I'm just gonna build, I'm gonna

654
00:41:57,965 --> 00:42:01,205
Speaker 8:  build whatever I want, right? Because I can with this technology. But I think

655
00:42:01,205 --> 00:42:04,485
Speaker 8:  for us it was really important to be ethical in that it's one of those things

656
00:42:04,485 --> 00:42:07,685
Speaker 8:  in a tech company, we call ourself a story company first. 'cause this is

657
00:42:07,765 --> 00:42:11,525
Speaker 8:  a technology to help storytellers, right? A story will

658
00:42:11,525 --> 00:42:14,925
Speaker 8:  always come first for us. So, you know, it is a tough one.

659
00:42:15,275 --> 00:42:18,885
Speaker 8:  It's one of those things that we have a lot of our users that say like, you're

660
00:42:18,885 --> 00:42:22,725
Speaker 8:  one of the, you know, rare ethically built company around this. But

661
00:42:23,235 --> 00:42:26,885
Speaker 8:  it's tough because you do have to pull back for innovation. There's a lot

662
00:42:26,885 --> 00:42:30,805
Speaker 8:  of stuff we've done research internally, right, that we had to pull

663
00:42:30,805 --> 00:42:33,725
Speaker 8:  back a little bit. We've been having this conversation for a long time. We've

664
00:42:33,725 --> 00:42:36,805
Speaker 8:  been telling people three years ago, you know, AI and visual effects is coming.

665
00:42:37,225 --> 00:42:40,565
Speaker 8:  You know, educate yourself. This change is gonna seem extremely drastic

666
00:42:40,635 --> 00:42:43,485
Speaker 8:  overnight, but it's not overnight. This research has been going on in academia,

667
00:42:44,105 --> 00:42:47,605
Speaker 8:  you know, Ivy League schools for, for, for decades now, right? It's just

668
00:42:47,605 --> 00:42:50,845
Speaker 8:  that we are now seeing it extremely accelerated and a lot of innovation you

669
00:42:50,845 --> 00:42:54,725
Speaker 8:  see that comes out, gets viral. It's not from yesterday. It's been built

670
00:42:54,785 --> 00:42:58,525
Speaker 8:  on top on top on top right on that side. So, you know, I, I think

671
00:42:58,825 --> 00:43:02,805
Speaker 8:  the conversation is getting more serious now about, you

672
00:43:02,805 --> 00:43:06,445
Speaker 8:  know, these moral issues for us. I don't

673
00:43:06,975 --> 00:43:10,845
Speaker 8:  think we should see the future where we don't have people on set,

674
00:43:11,025 --> 00:43:14,685
Speaker 8:  we don't have artists working on it, but just someone kind of typing away.

675
00:43:15,045 --> 00:43:18,925
Speaker 8:  I do think it will happen. We wanna be a part of the good side

676
00:43:18,925 --> 00:43:22,245
Speaker 8:  of that story, which is you always gonna have performance arts, you always

677
00:43:22,245 --> 00:43:26,205
Speaker 8:  gonna, I think a lot of people underestimate how hard this thing is. Not

678
00:43:26,305 --> 00:43:30,285
Speaker 8:  ai, I'm talking generally filmmaking. It's extremely hard. All of these

679
00:43:30,285 --> 00:43:33,965
Speaker 8:  artists have incredible high skill sets of that, right?

680
00:43:34,065 --> 00:43:36,565
Speaker 8:  It takes decades to learn some of these things and some of these tools, how

681
00:43:36,565 --> 00:43:40,365
Speaker 8:  to use them properly. So I think this kind of promise, it's

682
00:43:40,365 --> 00:43:44,045
Speaker 8:  one click solution promise, it's a one line away. We're not there yet. I'm

683
00:43:44,045 --> 00:43:47,805
Speaker 8:  talking AI general, not us. Yeah. So I think, I don't think we're gonna replace

684
00:43:48,155 --> 00:43:51,885
Speaker 8:  artists, all of these actors, you know, overnight. I don't think us, I think

685
00:43:51,885 --> 00:43:55,805
Speaker 8:  people will try, they will try, but I think us as audience members, we

686
00:43:55,805 --> 00:43:58,685
Speaker 8:  love that human connection. I don't think all of a sudden everybody's gonna

687
00:43:58,685 --> 00:44:02,285
Speaker 8:  be, you know, I don't have a favorite actor anymore because all the actors

688
00:44:02,445 --> 00:44:06,005
Speaker 8:  I watch are ai, right? I don't think that's gonna happen. Right? So I think

689
00:44:06,005 --> 00:44:08,885
Speaker 8:  that human connection is really important. Sometimes we like art we watch

690
00:44:08,885 --> 00:44:12,405
Speaker 8:  because of the meaning put by a person behind it,

691
00:44:12,745 --> 00:44:15,285
Speaker 8:  not generated something. So I think if you ask me where the future's gonna

692
00:44:15,285 --> 00:44:18,205
Speaker 8:  go, I think it's gonna be combination. There's gonna be some prompt thing

693
00:44:18,205 --> 00:44:21,325
Speaker 8:  that's gonna be a part of it. There's gonna be performance art that's gonna

694
00:44:21,325 --> 00:44:23,845
Speaker 8:  be a part of it, right? Because it's very, think of it this way, we always

695
00:44:23,845 --> 00:44:27,485
Speaker 8:  say this, how do you describe a performance with words?

696
00:44:27,755 --> 00:44:31,725
Speaker 8:  It's very hard. That's why we have directors that say cut five times or

697
00:44:32,205 --> 00:44:36,085
Speaker 8:  I joke title always 20 times with him, right? So 30. But

698
00:44:36,315 --> 00:44:39,725
Speaker 8:  some things we cannot, even, even body movement, we don't have English word

699
00:44:40,265 --> 00:44:44,245
Speaker 8:  for some body movements, right? So I think the performance art will always

700
00:44:44,305 --> 00:44:47,685
Speaker 8:  be there, especially in these, in these kind of cases. So, but let's see

701
00:44:47,685 --> 00:44:51,485
Speaker 8:  where we go is IIII always say it's important that these tools

702
00:44:51,505 --> 00:44:55,325
Speaker 8:  are also built with people in the industry. 'cause if we just let it go on

703
00:44:55,365 --> 00:44:59,125
Speaker 8:  a tech side, it's a very different goal. It's a goal of lemme

704
00:44:59,125 --> 00:45:02,925
Speaker 8:  build a tool that's used by talkers in billions in numbers

705
00:45:03,305 --> 00:45:07,285
Speaker 8:  so I can get higher value for my company, right? I say we need to

706
00:45:07,285 --> 00:45:11,245
Speaker 8:  build alongside artist community. 'cause if you put an artist be behind building

707
00:45:11,245 --> 00:45:14,165
Speaker 8:  something, they will think about it because they'll think about it from their

708
00:45:14,165 --> 00:45:14,925
Speaker 8:  perspective as well.

709
00:45:15,605 --> 00:45:17,445
Speaker 2:  Ty, one of the things you were saying when we were talking the other day

710
00:45:17,445 --> 00:45:21,045
Speaker 2:  that I've been thinking a lot about since is the, the question of basically

711
00:45:21,385 --> 00:45:25,205
Speaker 2:  the upside of these tools is that it gives access to the kind of creative

712
00:45:25,205 --> 00:45:28,725
Speaker 2:  process to more people who, for reasons of

713
00:45:28,975 --> 00:45:32,285
Speaker 2:  money or accessibility or literally where they live in the world,

714
00:45:32,795 --> 00:45:36,605
Speaker 2:  they just couldn't make Hollywood movies, right? Like Hollywood was a, a

715
00:45:36,605 --> 00:45:39,805
Speaker 2:  small group of people in a small place that did this stuff. And now we're

716
00:45:39,805 --> 00:45:42,685
Speaker 2:  getting to the point where these tools and these platforms and these audiences

717
00:45:42,685 --> 00:45:46,125
Speaker 2:  are available to many more people. And I think that's awesome. And I think

718
00:45:46,125 --> 00:45:49,605
Speaker 2:  like anything that I think so gets us there is really great. Is there a

719
00:45:49,915 --> 00:45:53,325
Speaker 2:  flip side to that? Like is is if we democratize all of this stuff

720
00:45:53,705 --> 00:45:57,125
Speaker 2:  and make it all so automated that anybody can use it? I think that's very

721
00:45:57,125 --> 00:46:00,845
Speaker 2:  cool. And also just it feels like it changes what that stuff is in a way

722
00:46:00,845 --> 00:46:03,605
Speaker 2:  that I haven't quite sorted out in my

723
00:46:03,685 --> 00:46:06,965
Speaker 7:  Brain. I think it increases our potential to find original voices. You know,

724
00:46:07,085 --> 00:46:10,845
Speaker 7:  I think what happens when you have a group of people that are surrounded

725
00:46:10,845 --> 00:46:14,205
Speaker 7:  by like-minded individuals, they start to think the same things. It's the

726
00:46:14,205 --> 00:46:18,005
Speaker 7:  same with Hollywood. You know, you have people, you know in one place. It's

727
00:46:18,005 --> 00:46:21,405
Speaker 7:  a very localized industry. And if you want to get a project greenlit, that's

728
00:46:21,405 --> 00:46:23,965
Speaker 7:  where you have to go. You have to go to the executives that are in Hollywood.

729
00:46:23,985 --> 00:46:25,405
Speaker 7:  And I think this

730
00:46:25,405 --> 00:46:27,525
Speaker 2:  Is why we get 600 Marvel movies every day. That's

731
00:46:27,525 --> 00:46:27,685
Speaker 7:  Right.

732
00:46:29,395 --> 00:46:32,565
Speaker 7:  Yeah. So I I I think for us, like, you know, that was the real

733
00:46:33,365 --> 00:46:36,565
Speaker 7:  exciting mission with, with, with our company and the exciting potential

734
00:46:36,865 --> 00:46:40,645
Speaker 7:  is that hey, some, some, a group of kids growing up in

735
00:46:40,765 --> 00:46:44,605
Speaker 7:  the middle of, of, you know, some small town in in Norway or Kenya can,

736
00:46:44,705 --> 00:46:47,445
Speaker 7:  can go out and make a science fiction film that, you know, today it could

737
00:46:47,445 --> 00:46:50,965
Speaker 7:  only be made in Hollywood and do it on a budget. And that's super exciting

738
00:46:50,965 --> 00:46:54,485
Speaker 7:  for us because we don't know where the next James Cameron or Steven Spielberg's

739
00:46:54,485 --> 00:46:58,005
Speaker 7:  gonna come from, you know? And I think giving, you know, thinking even about

740
00:46:58,025 --> 00:47:02,005
Speaker 7:  us, you know, not having a lot of tools growing up, I mean, we would die

741
00:47:02,025 --> 00:47:05,405
Speaker 7:  to have stuff like the tools like this. And I'm not even just saying what,

742
00:47:05,405 --> 00:47:08,645
Speaker 7:  what we're building, but I think the tools that are, that are coming, you

743
00:47:08,645 --> 00:47:12,565
Speaker 7:  know, this, this next generation of filmmakers, I I do, I think the biggest

744
00:47:12,565 --> 00:47:15,965
Speaker 7:  potential there is really to discover original voices and, and yeah. And

745
00:47:15,965 --> 00:47:19,645
Speaker 7:  to, to kind of pave a new path for, for storytelling. And I think

746
00:47:19,715 --> 00:47:23,325
Speaker 7:  Hollywood traditionally prides itself on, you know, stories of

747
00:47:23,435 --> 00:47:26,765
Speaker 7:  inclusivity and, and trying to recognize certain

748
00:47:27,085 --> 00:47:30,365
Speaker 7:  cultures or, or, or, or people that are maybe misunderstood or

749
00:47:30,745 --> 00:47:34,605
Speaker 7:  underrepresented. And I think the way to, to truly do that in, in,

750
00:47:34,605 --> 00:47:38,205
Speaker 7:  in the best and most authentic way is, is to really democratize the tools

751
00:47:38,265 --> 00:47:41,925
Speaker 7:  and, and allow people from all walks of life to go and make these films regardless

752
00:47:42,025 --> 00:47:44,405
Speaker 7:  of their socioeconomical status. Yeah.

753
00:47:44,625 --> 00:47:48,565
Speaker 2:  It also makes me think that maybe rather than like, you know, your your

754
00:47:48,565 --> 00:47:52,485
Speaker 2:  close personal friend Steven Spielberg making tons of movies with all

755
00:47:52,485 --> 00:47:55,485
Speaker 2:  of these new tools very soon what actually might happen is that like

756
00:47:55,955 --> 00:47:59,845
Speaker 2:  YouTubers and tiktoks and the people who are making this stuff who have

757
00:47:59,845 --> 00:48:03,645
Speaker 2:  never had access to these kinds of tools at any cost might be able to

758
00:48:04,105 --> 00:48:06,645
Speaker 2:  do this kind of stuff. Like, one thing I've been thinking a lot about is,

759
00:48:07,385 --> 00:48:10,965
Speaker 2:  is dubbing, which is a thing that is AI is all over, like text to speech

760
00:48:10,965 --> 00:48:13,285
Speaker 2:  is starting to get really good speech to text is starting to get really good.

761
00:48:13,465 --> 00:48:17,325
Speaker 2:  And the idea that you could have something that you make that is actually

762
00:48:17,355 --> 00:48:21,245
Speaker 2:  localized successfully in two languages all over the world, I think is

763
00:48:21,245 --> 00:48:24,965
Speaker 2:  very cool and, and super exciting. And we're a long way from the tech of

764
00:48:24,965 --> 00:48:28,645
Speaker 2:  that being sort of perfect. But it's already like the stuff that

765
00:48:28,845 --> 00:48:31,605
Speaker 2:  Squid game was a couple of years ago where you see their mouth moving and

766
00:48:31,605 --> 00:48:33,925
Speaker 2:  you hear the words and they're obviously different. Those two things are

767
00:48:33,925 --> 00:48:35,205
Speaker 2:  coming closer together. And I think that's very,

768
00:48:35,315 --> 00:48:36,845
Speaker 7:  Yeah. It's also from an actor's perspective,

769
00:48:37,225 --> 00:48:40,365
Speaker 8:  You just revealed, you watched the US version and ethics, so the squid games

770
00:48:40,365 --> 00:48:41,445
Speaker 8:  come on, you have to watch. I

771
00:48:41,445 --> 00:48:43,485
Speaker 2:  Did. I'm not proud of Thatit. Listen, you

772
00:48:43,565 --> 00:48:44,805
Speaker 8:  Gotta watch original with the stuffed

773
00:48:46,075 --> 00:48:49,045
Speaker 7:  Yeah, I mean from, from, from an actor's perspective, do you, you know, you

774
00:48:49,045 --> 00:48:52,965
Speaker 7:  really want someone else in some other country dubbing your voice in a,

775
00:48:53,025 --> 00:48:55,365
Speaker 7:  in a foreign language. I mean, I I would rather,

776
00:48:55,435 --> 00:48:57,685
Speaker 8:  It's a whole different panel now. We're getting in a whole different panel

777
00:48:57,705 --> 00:48:57,925
Speaker 8:  now.

778
00:48:58,745 --> 00:49:01,245
Speaker 7:  No, I think this No, think it's a good, it's a, it's a, it's a good example.

779
00:49:01,365 --> 00:49:05,205
Speaker 7:  I mean, I think, you know, we can recognize where, you know, AI is start,

780
00:49:05,225 --> 00:49:09,085
Speaker 7:  is starting to be used, you know, to in, in, in some ways to stay true

781
00:49:09,185 --> 00:49:12,845
Speaker 7:  to the performance of, of, of the artist, the original artist. And I think,

782
00:49:13,045 --> 00:49:16,165
Speaker 7:  I think that's awesome. I mean, obviously I, I have a biased perspective

783
00:49:16,525 --> 00:49:18,045
Speaker 7:  probably being an actor, but, well,

784
00:49:18,045 --> 00:49:20,805
Speaker 2:  But, and again, this is like, I've realized a lot of what I'm gonna do here

785
00:49:20,805 --> 00:49:23,445
Speaker 2:  is just like throw weird tensions at you and make you sort them out in front

786
00:49:23,445 --> 00:49:27,085
Speaker 2:  of me. But the thing that I struggle with on that one is like, they're doing

787
00:49:27,085 --> 00:49:29,805
Speaker 2:  this a lot with podcasts too, right? And I host a podcast and we, we get

788
00:49:29,805 --> 00:49:33,005
Speaker 2:  a lot of people who are like, I, I don't speak English. I would like to listen

789
00:49:33,065 --> 00:49:36,485
Speaker 2:  to your stupid podcast. Can I do that? And there are companies out there

790
00:49:36,485 --> 00:49:40,045
Speaker 2:  that are like, we will automatically w you into 60 languages. And on the

791
00:49:40,045 --> 00:49:43,645
Speaker 2:  one hand, super cool. On the other hand, I, I don't know if

792
00:49:43,675 --> 00:49:47,485
Speaker 2:  they're good or I'm going to say what is right. Like, so it,

793
00:49:47,485 --> 00:49:50,525
Speaker 2:  it, there's, it sort of crosses this invisible line where I go from like,

794
00:49:50,545 --> 00:49:54,485
Speaker 2:  oh, access is awesome to like, this feels weird because it's, you're

795
00:49:54,485 --> 00:49:58,285
Speaker 2:  now no longer getting the thing that I did. And that somewhere in there it

796
00:49:58,285 --> 00:49:59,965
Speaker 2:  just starts to, it starts to feel odd.

797
00:50:00,035 --> 00:50:02,445
Speaker 8:  There's definitely those lines. But I think what you were saying earlier

798
00:50:02,825 --> 00:50:06,765
Speaker 8:  is what I think is gonna happen, right? I don't think studios are gonna have

799
00:50:06,825 --> 00:50:09,925
Speaker 8:  issues. I think what the people don't realize, there's so much sacrifice

800
00:50:09,925 --> 00:50:13,045
Speaker 8:  on every project we see, turning things from page to screen is really, really

801
00:50:13,045 --> 00:50:16,285
Speaker 8:  hard. There's always a lot of sacrifice that comes from it. So I think studios

802
00:50:16,285 --> 00:50:19,925
Speaker 8:  are just gonna keep pushing things higher. But I think indie filmmakers,

803
00:50:20,025 --> 00:50:23,325
Speaker 8:  as you mentioned, like, you know, YouTubers, if you remember the days when

804
00:50:23,385 --> 00:50:27,245
Speaker 8:  you know DSLRs came out, right? And you have, people are saying,

805
00:50:27,705 --> 00:50:31,005
Speaker 8:  oh my God, everybody's gonna be able to make a movie now, right? With it.

806
00:50:31,145 --> 00:50:34,125
Speaker 8:  But that doesn't mean those movies were necessarily great because storytelling

807
00:50:34,125 --> 00:50:37,605
Speaker 8:  is, is still hard. Coming up with a good story is still extremely hard,

808
00:50:37,775 --> 00:50:41,605
Speaker 8:  right? So we're gonna open access with these innovations and AI right

809
00:50:42,005 --> 00:50:45,605
Speaker 8:  across the board, which is really exciting. I think what YouTube did, and

810
00:50:45,685 --> 00:50:48,765
Speaker 8:  I always say this, YouTube is a good example of distribution changing, right?

811
00:50:48,785 --> 00:50:52,605
Speaker 8:  So we had open distribution. I think what's happening now with

812
00:50:52,605 --> 00:50:56,405
Speaker 8:  generative AI is that we're changing production. So open production,

813
00:50:56,455 --> 00:50:59,565
Speaker 8:  right? So meaning these kids could come up and express themself and put on

814
00:50:59,565 --> 00:51:02,565
Speaker 8:  YouTube and if they're good people like them, they will find them. Right?

815
00:51:02,565 --> 00:51:05,405
Speaker 8:  There's billions of people watching it. I think now with production, we're

816
00:51:05,405 --> 00:51:08,445
Speaker 8:  gonna have these tools where it was gonna be easier direct line for them

817
00:51:08,445 --> 00:51:11,765
Speaker 8:  to express themself. But again, there's no guarantee. That's good. Right?

818
00:51:11,825 --> 00:51:14,925
Speaker 8:  If you go into masses on the flip side of that, which you were saying, you

819
00:51:14,925 --> 00:51:18,525
Speaker 8:  know, kinda mentioning with Marvel movies, it's, it's a hard thing. I understand

820
00:51:18,525 --> 00:51:21,885
Speaker 8:  both sides. It's a really hard thing for studios. 'cause if you spend

821
00:51:22,225 --> 00:51:26,085
Speaker 8:  $250 million, you need to make 500 to break even, right?

822
00:51:26,085 --> 00:51:28,645
Speaker 8:  And you're spending another a hundred problem in marketing, right? Right.

823
00:51:28,825 --> 00:51:32,485
Speaker 8:  So you can't really take a big risk. And I always say it would

824
00:51:32,485 --> 00:51:36,245
Speaker 8:  almost be impossible to get 2001 space audits at Green Lit today.

825
00:51:36,245 --> 00:51:40,005
Speaker 8:  It's too artsy, right? And I think that's what for us was really excited.

826
00:51:40,155 --> 00:51:43,445
Speaker 8:  Once you get these tools and you bring the price down, you can take higher

827
00:51:43,445 --> 00:51:47,405
Speaker 8:  risks and you can make, you know, visually spectacular films that the

828
00:51:47,405 --> 00:51:50,725
Speaker 8:  young audience would wanna watch, but make a grounded story really about

829
00:51:50,725 --> 00:51:54,165
Speaker 8:  the character, push some narrative push, push some subject matter. That's

830
00:51:54,165 --> 00:51:55,285
Speaker 8:  extremely, extremely important. Right? And if

831
00:51:55,285 --> 00:51:57,925
Speaker 7:  You're, and if you're not spending $200 million, if you're spending 20 on

832
00:51:57,925 --> 00:52:00,885
Speaker 7:  each, you, you can make way more films and employ more artists.

833
00:52:01,355 --> 00:52:04,565
Speaker 2:  When we were talking before, one of you brought up Netflix as, as another

834
00:52:04,565 --> 00:52:08,245
Speaker 2:  sort of wrench in the engine of like what people think that we make and how

835
00:52:08,245 --> 00:52:12,205
Speaker 2:  it changes what things were like. We got different kinds of TV

836
00:52:12,375 --> 00:52:16,365
Speaker 2:  shows because Netflix existed and what that platform wanted. So to some extent,

837
00:52:16,365 --> 00:52:20,285
Speaker 2:  do you think AI is just sort of another in this like long series

838
00:52:20,305 --> 00:52:23,125
Speaker 2:  of changes that Hollywood has been going through forever? Or is there something

839
00:52:23,125 --> 00:52:24,085
Speaker 2:  different about ai?

840
00:52:24,885 --> 00:52:28,125
Speaker 8:  I think it's different because of scale. Okay. I think it's different because

841
00:52:28,155 --> 00:52:31,725
Speaker 8:  it's gonna distribute. It is much, much more. It's not gonna be as localized,

842
00:52:32,065 --> 00:52:35,525
Speaker 8:  but I don't know, I'm, I'm optimistic. I think the beauty of it, the

843
00:52:36,005 --> 00:52:39,085
Speaker 8:  positive side is that democratization we're talking about the negative side

844
00:52:39,085 --> 00:52:41,685
Speaker 8:  is like how much is it gonna affect it? But if you look at through history

845
00:52:41,685 --> 00:52:45,485
Speaker 8:  of Hollywood, kind of al always found its way, right? It

846
00:52:45,485 --> 00:52:48,845
Speaker 8:  always found its way to survive. You know, early days of television, people

847
00:52:48,845 --> 00:52:52,365
Speaker 8:  said movies were over, right? Right. And now that cycle went and kind of

848
00:52:52,525 --> 00:52:56,405
Speaker 8:  repeated itself now, you know, 80 years later with the streaming platform.

849
00:52:56,585 --> 00:53:00,165
Speaker 8:  And then it's coming in a slow recovery on that. So I think storytelling

850
00:53:00,475 --> 00:53:04,365
Speaker 8:  find its way on that. But I do think it's gonna be a big shift. A

851
00:53:04,365 --> 00:53:07,605
Speaker 8:  lot of people in production side, especially below the line, a lot of artists

852
00:53:07,655 --> 00:53:11,165
Speaker 8:  below the line, you know, Ty, I talk a lot about like, what if

853
00:53:11,565 --> 00:53:15,205
Speaker 8:  shooting in person is gonna be like black and mo, black and white movies

854
00:53:15,255 --> 00:53:19,205
Speaker 8:  today you don't see many of them, but you see some, right? But if

855
00:53:19,205 --> 00:53:22,485
Speaker 8:  that happens and go going around. So it's interesting. Nobody really has

856
00:53:22,485 --> 00:53:26,365
Speaker 8:  an answer of, of what that's gonna end up being. But yes, I, again,

857
00:53:26,365 --> 00:53:29,485
Speaker 8:  to repeat like we've been telling people for three, four years, start learning

858
00:53:29,485 --> 00:53:33,085
Speaker 8:  these things, start learning these tools, it's the only way to kind of survive

859
00:53:33,085 --> 00:53:36,525
Speaker 8:  through it. I, I think it's jean's out of the bottom. It's really hard to

860
00:53:36,525 --> 00:53:39,125
Speaker 8:  stop some of these research and some of these innovation. So

861
00:53:39,125 --> 00:53:42,205
Speaker 7:  I think, yeah, I think that was the big thing during the strikes that that

862
00:53:42,205 --> 00:53:45,805
Speaker 7:  didn't happen. You know, people, there was a lot of controversy. People were

863
00:53:45,805 --> 00:53:49,205
Speaker 7:  saying, oh, AI is either bad or good and people were jumping on these, this,

864
00:53:49,345 --> 00:53:53,165
Speaker 7:  you know, spectrum on one extreme end or the other. And no one

865
00:53:53,165 --> 00:53:57,045
Speaker 7:  was really coming together to talk about it and, you know, in an open

866
00:53:57,065 --> 00:54:00,765
Speaker 7:  way and say, Hey, let's educate ourselves on this. And I, I think that's

867
00:54:00,765 --> 00:54:03,845
Speaker 7:  truly what everyone needs to do. It's what the industry needs to do. It's

868
00:54:03,845 --> 00:54:07,205
Speaker 7:  what artists need to do. Artists need to understand exactly how

869
00:54:07,555 --> 00:54:11,245
Speaker 7:  it's gonna change the course of their careers. You know, what benefits it

870
00:54:11,365 --> 00:54:14,805
Speaker 7:  poses, what threats it poses to their careers. I think we, we really, you

871
00:54:14,805 --> 00:54:18,525
Speaker 7:  know, as, as creators have to come together and, and really openly educate

872
00:54:18,525 --> 00:54:20,685
Speaker 7:  ourselves on, on what it means for the future.

873
00:54:21,305 --> 00:54:25,125
Speaker 2:  Do you think there's room to influence that as creatives? I think to

874
00:54:25,125 --> 00:54:28,245
Speaker 2:  to some extent that there's a version of that conversation that basically

875
00:54:28,245 --> 00:54:32,125
Speaker 2:  sounds like, you know, we're, we're screwed. The old way is dead. All

876
00:54:32,125 --> 00:54:35,045
Speaker 2:  we can do is just sort of hunker down and try to move forward. But there's

877
00:54:35,045 --> 00:54:37,925
Speaker 2:  another way that says, and I think this is like talking to folks, especially

878
00:54:37,925 --> 00:54:41,085
Speaker 2:  during the writer strike, there's this question of like, how can we make

879
00:54:41,115 --> 00:54:45,085
Speaker 2:  this better? Like we do, we agree that there are things that chat

880
00:54:45,245 --> 00:54:48,525
Speaker 2:  GPT can do for scripts that are useful, but we have to do that in a way that

881
00:54:48,525 --> 00:54:52,485
Speaker 2:  also works for us as people who have jobs and do creative work. Like can

882
00:54:52,485 --> 00:54:56,285
Speaker 2:  we Yeah. Are, are I my worry with a lot of things about AI is that that tech

883
00:54:56,285 --> 00:54:59,725
Speaker 2:  is getting better faster than we are at dealing with it. And so it's just

884
00:54:59,725 --> 00:55:02,565
Speaker 2:  gonna sort of leave us behind and we're gonna end up at this place of all

885
00:55:02,565 --> 00:55:04,565
Speaker 2:  we can do is just sort of give up and try

886
00:55:04,565 --> 00:55:08,085
Speaker 8:  To survive. No, I I I think it's not getting as fast

887
00:55:08,705 --> 00:55:12,565
Speaker 8:  as you would read online. Okay. It's, it's flashy, but in

888
00:55:12,835 --> 00:55:16,605
Speaker 8:  practic, practically on film, it's still not completely

889
00:55:16,625 --> 00:55:19,845
Speaker 8:  usable. It'll get there. That's one of the reasons we built it that way.

890
00:55:20,005 --> 00:55:22,805
Speaker 8:  'cause we knew there's no way you can push every shot just with ai you'll

891
00:55:22,805 --> 00:55:25,725
Speaker 8:  get 60% there. So you need to give this data to the artist to be able to

892
00:55:25,725 --> 00:55:28,965
Speaker 8:  manipulate it all the way there. So it's still a bit like, you know,

893
00:55:29,295 --> 00:55:30,645
Speaker 8:  flashiness and excitements.

894
00:55:30,895 --> 00:55:34,525
Speaker 7:  We're kind of seeing it now in a way with some films that I think are dominating

895
00:55:34,685 --> 00:55:38,405
Speaker 7:  the box office. I think, you know, audiences are seeing films

896
00:55:38,435 --> 00:55:42,285
Speaker 7:  that they're not enjoying. And yeah, sure, you can argue that that's

897
00:55:42,405 --> 00:55:46,205
Speaker 7:  happened always. But I think ultimately, you know, people will,

898
00:55:46,635 --> 00:55:50,045
Speaker 7:  good stories attract people, you know, and people go and they see a good

899
00:55:50,045 --> 00:55:52,525
Speaker 7:  story and they tell someone about it and then that person goes and they see

900
00:55:52,525 --> 00:55:56,285
Speaker 7:  it and they tell someone else. And so I don't think that will ever stop.

901
00:55:56,585 --> 00:55:59,725
Speaker 7:  You know? And so if you have, let's say a studio or a production company

902
00:55:59,785 --> 00:56:03,725
Speaker 7:  or anyone that thinks, oh well we can just, we can just use AI and

903
00:56:03,725 --> 00:56:07,485
Speaker 7:  replace everyone. Well, if they're not making good stories, it doesn't, it's

904
00:56:07,485 --> 00:56:09,725
Speaker 7:  not gonna matter, right? These, these stories won't be successful because

905
00:56:09,725 --> 00:56:12,885
Speaker 7:  they won't connect with people in an emotional, in a real way. And that's

906
00:56:12,885 --> 00:56:15,845
Speaker 7:  what, that's what artists are good at. That's why we need artists, you know,

907
00:56:16,165 --> 00:56:19,765
Speaker 7:  regardless of what tools they have. And that's, that's always been true since

908
00:56:19,985 --> 00:56:23,885
Speaker 7:  the dawn of time, since we've been telling stories, since we've been, you

909
00:56:23,885 --> 00:56:27,165
Speaker 7:  know, telling stories through art. And I don't think that'll ever change,

910
00:56:27,665 --> 00:56:31,085
Speaker 8:  You know, thinking about making a movie, like, I don't know how many people

911
00:56:31,085 --> 00:56:34,765
Speaker 8:  in this room made a hundred million dollar movie or made a movie,

912
00:56:35,215 --> 00:56:37,605
Speaker 8:  right? How many people would like to make a movie? Ty

913
00:56:37,605 --> 00:56:39,645
Speaker 2:  Should erase stand just that Ty should erase.

914
00:56:40,115 --> 00:56:40,405
Speaker 7:  Yeah,

915
00:56:40,705 --> 00:56:44,525
Speaker 8:  But I'm saying if you ask, you know, a really large group of people, how

916
00:56:44,525 --> 00:56:48,445
Speaker 8:  many would like to be able to make a movie? Most people say, oh, that'd be

917
00:56:48,445 --> 00:56:51,245
Speaker 8:  fine. I would like to do that. Right? I would like to express my feeling

918
00:56:51,305 --> 00:56:55,285
Speaker 8:  and tell people my story. The, the amount of people that get to like make

919
00:56:55,365 --> 00:56:59,045
Speaker 8:  $150 million movie. It's a handful. It's handful of these directors that

920
00:56:59,045 --> 00:57:02,565
Speaker 8:  you get to work. So I think our industry will just grow because of it's not

921
00:57:02,565 --> 00:57:05,965
Speaker 8:  gonna shrink, it will grow. 'cause it's gonna be very, very global. We're

922
00:57:05,965 --> 00:57:09,885
Speaker 8:  talking Hollywood and I always say like storytelling extremely, extremely

923
00:57:09,905 --> 00:57:13,445
Speaker 8:  is extremely important. It should not be dependent on your

924
00:57:13,445 --> 00:57:17,405
Speaker 8:  socioeconomical status. That's why we started this. And I think that's what's

925
00:57:17,405 --> 00:57:20,845
Speaker 8:  important about ai. But yes, you gotta build it responsibly, otherwise you

926
00:57:20,845 --> 00:57:22,325
Speaker 8:  can, it can, it can hurt us long way.

927
00:57:22,935 --> 00:57:26,005
Speaker 2:  Let's just talk about sort of Hollywood more broadly right now. And I'm,

928
00:57:26,025 --> 00:57:29,805
Speaker 2:  I'm curious, especially sort of specific AI stuff, you're

929
00:57:29,925 --> 00:57:33,245
Speaker 2:  starting to see work its way into how these things get made. We've talked

930
00:57:33,245 --> 00:57:35,805
Speaker 2:  about dubbing a little bit. We've talked about what you're doing at one with

931
00:57:35,805 --> 00:57:39,685
Speaker 2:  the VXX stuff. Like are there other things that are starting

932
00:57:39,705 --> 00:57:43,405
Speaker 2:  to sort of appear in the filmmaking process and be part of the conversation

933
00:57:43,405 --> 00:57:47,285
Speaker 2:  in Hollywood that are either cool or early versions of

934
00:57:47,285 --> 00:57:47,805
Speaker 2:  something cool?

935
00:57:48,355 --> 00:57:51,925
Speaker 7:  Yeah, I think Hollywood is, it's, it's odd 'cause it's

936
00:57:51,925 --> 00:57:55,725
Speaker 7:  hypocritical, but Hollywood I think is notoriously standoffish to new technology

937
00:57:55,795 --> 00:57:59,405
Speaker 7:  even though it's, it's an entire industry predicated on the

938
00:57:59,425 --> 00:58:03,405
Speaker 7:  advent of a camera, right? And then thereafter, you know, new, new, new

939
00:58:03,405 --> 00:58:07,285
Speaker 7:  stories and the best movies were making technological advancements,

940
00:58:07,465 --> 00:58:09,885
Speaker 7:  you know, and they wouldn't be possible without those technological advancements.

941
00:58:09,945 --> 00:58:13,885
Speaker 7:  But I think now, you know, there are certain decision makers in

942
00:58:13,885 --> 00:58:17,165
Speaker 7:  Hollywood, and I'm not saying it's everyone, but there are a lot of decision

943
00:58:17,165 --> 00:58:20,325
Speaker 7:  makers in Hollywood who know nothing about technology. They know nothing

944
00:58:20,325 --> 00:58:23,605
Speaker 7:  about how visual effects work really, you know, at, at a very high level

945
00:58:23,605 --> 00:58:26,605
Speaker 7:  maybe. But there, there are people who, who make a lot of decisions and,

946
00:58:26,625 --> 00:58:30,605
Speaker 7:  and I think they don't understand technology at a certain depth. So I

947
00:58:30,605 --> 00:58:33,965
Speaker 7:  think sometimes it's also kind of a, a risk management

948
00:58:34,485 --> 00:58:38,365
Speaker 7:  business too. You know, you have thousands of of artists working

949
00:58:38,665 --> 00:58:42,565
Speaker 7:  in certain pipelines. You can't just, you know, decide to, to use some

950
00:58:42,705 --> 00:58:46,685
Speaker 7:  AI tool and change that entire pipeline overnight. No one's ever gonna go

951
00:58:46,685 --> 00:58:50,245
Speaker 7:  for that. And that's, that's one reason why, you know, we built Wonder Studio

952
00:58:50,345 --> 00:58:54,245
Speaker 7:  the way we did so that it plugged in directly to the pipeline that artists

953
00:58:54,245 --> 00:58:57,845
Speaker 7:  were used to, that that productions were using could actually utilize this

954
00:58:57,845 --> 00:59:01,685
Speaker 7:  tool. It's, it's not a black box, right? So yeah, I think

955
00:59:01,865 --> 00:59:04,685
Speaker 7:  the industry's kinda been battered over the past few years. I mean I think

956
00:59:04,685 --> 00:59:08,645
Speaker 7:  it's fair to say, you know, we had Covid, which was a major dent in

957
00:59:08,645 --> 00:59:12,285
Speaker 7:  the global box office and you know, then we had the streaming wars and

958
00:59:12,585 --> 00:59:16,045
Speaker 7:  you know, digital distribution has been, the landscape of digital distribution

959
00:59:16,045 --> 00:59:19,685
Speaker 7:  has been changing and distribution at large has been changing and

960
00:59:19,945 --> 00:59:23,645
Speaker 7:  now the strikes and AI and so it's just, it's kind of, yeah,

961
00:59:23,715 --> 00:59:27,565
Speaker 7:  it's been a rumble tumble past few years. Yeah, I think people are, are

962
00:59:27,565 --> 00:59:31,165
Speaker 7:  really trying to understand like, you know, what, how can we come out of

963
00:59:31,165 --> 00:59:35,005
Speaker 7:  this? And, and you know, are, are we actually on stable ground now and

964
00:59:35,025 --> 00:59:38,925
Speaker 7:  can we go in green light films and if so, how much risk can we take? What

965
00:59:38,975 --> 00:59:42,325
Speaker 7:  tools can we utilize? So I think it's, it's really like we're in a discovery

966
00:59:42,415 --> 00:59:45,485
Speaker 7:  phase right now and I think people are trying new things. I think people

967
00:59:45,485 --> 00:59:49,325
Speaker 7:  are really starting to test a lot of things behind closed doors. So

968
00:59:49,445 --> 00:59:53,165
Speaker 7:  I do think this will be the year where we really start to see a lot of new

969
00:59:53,505 --> 00:59:57,445
Speaker 7:  use cases and, and, and we've seen, we've seen a few, but I do think there'll

970
00:59:57,565 --> 01:00:00,685
Speaker 7:  be more significant, you know, examples that we'll see in the next year.

971
01:00:00,765 --> 01:00:04,645
Speaker 8:  A lot of the tech is not, not there yet. A lot of tech is really good to

972
01:00:04,675 --> 01:00:08,525
Speaker 8:  push it to something that you can put on social media, but film's different.

973
01:00:08,525 --> 01:00:11,645
Speaker 8:  You gotta get it up to eight K, it needs to be crisp. And also, you know,

974
01:00:11,645 --> 01:00:15,525
Speaker 8:  we said like this kind of age of generative AI has been last year, year and

975
01:00:15,525 --> 01:00:19,125
Speaker 8:  a half. But you know, storytelling and film is all about collaboration

976
01:00:19,505 --> 01:00:23,285
Speaker 8:  and back and forth. Like you turn in your shot to a director or editor

977
01:00:23,505 --> 01:00:25,845
Speaker 8:  and they're gonna give you back notes and sometimes you're gonna have, they're

978
01:00:25,845 --> 01:00:28,405
Speaker 8:  called passes, sometimes you're gonna have up to 20 of them and the changes

979
01:00:28,425 --> 01:00:32,245
Speaker 8:  are gonna be so specific, right? And so these generative AI tools

980
01:00:32,245 --> 01:00:36,205
Speaker 8:  just haven't gotten there yet that you can edit the fine, the finest

981
01:00:36,305 --> 01:00:39,965
Speaker 8:  pixels of that. And that's the limitation of 2D space a little bit in that

982
01:00:39,965 --> 01:00:42,045
Speaker 8:  side because you can generate something that's really cool but it's a little

983
01:00:42,045 --> 01:00:44,885
Speaker 8:  bit off how do I fix it? I think it'll get better. Also, we haven't seen

984
01:00:44,885 --> 01:00:48,445
Speaker 8:  really performance, so it hasn't been used. I mean a lot of these text video,

985
01:00:48,635 --> 01:00:52,605
Speaker 8:  they don't yet know how to generate performance completely on that side.

986
01:00:52,665 --> 01:00:56,165
Speaker 8:  So I think a lot of, a lot of filmmakers are still kinda exploring it. So

987
01:00:56,165 --> 01:00:58,685
Speaker 8:  the promise is big, but we're not completely there yet.

988
01:00:59,205 --> 01:01:01,925
Speaker 2:  I read somewhere that actors are starting to put in their contracts that

989
01:01:01,925 --> 01:01:05,565
Speaker 2:  they have to use VFX to make them look however many years younger

990
01:01:05,785 --> 01:01:08,645
Speaker 2:  or you know, remove the lines. I thought you were saying there's a budget

991
01:01:08,645 --> 01:01:10,925
Speaker 2:  just for like wrinkle removal on Tom Cruise's face.

992
01:01:11,645 --> 01:01:12,605
Speaker 8:  I can't, I can't confirm that

993
01:01:14,285 --> 01:01:17,245
Speaker 8:  I did used, I did used to do that early in my career. There you go. It was

994
01:01:17,245 --> 01:01:21,045
Speaker 8:  just, it was just removing people's under eyes and and

995
01:01:21,045 --> 01:01:22,005
Speaker 8:  stuff like that. Yeah,

996
01:01:22,005 --> 01:01:24,885
Speaker 2:  Budget but not Tom Cruise. He's ageless. Tom Cruise Don't

997
01:01:24,885 --> 01:01:25,885
Speaker 8:  Come after me. Yeah, no, never.

998
01:01:26,435 --> 01:01:27,325
Speaker 2:  Okay, so

999
01:01:27,485 --> 01:01:28,685
Speaker 7:  I thought Tom Cruise is ai

1000
01:01:30,785 --> 01:01:31,205
Speaker 2:  That's,

1001
01:01:31,285 --> 01:01:33,405
Speaker 8:  There is an AI meta that's metaphysics, right?

1002
01:01:33,795 --> 01:01:35,245
Speaker 7:  Yeah. Yes, that is,

1003
01:01:35,245 --> 01:01:38,445
Speaker 2:  That's true. Alright, if, let's, let's, we're gonna run outta time here soon.

1004
01:01:38,445 --> 01:01:40,045
Speaker 2:  So let's take some questions if folks have questions.

1005
01:01:40,705 --> 01:01:43,565
Speaker 9:  Hi, can you quantify the

1006
01:01:44,345 --> 01:01:48,005
Speaker 9:  access that you're talking about, the democratization? What is the pricing

1007
01:01:48,175 --> 01:01:51,885
Speaker 9:  model of your product and what are the price, the cost

1008
01:01:51,885 --> 01:01:55,245
Speaker 9:  savings that you promise your clients when you go in for a pitch and across

1009
01:01:55,675 --> 01:01:59,125
Speaker 9:  what tasks I'm curious about? Good question. Kinda the material

1010
01:01:59,605 --> 01:02:00,805
Speaker 9:  benefits of the, of the product.

1011
01:02:01,155 --> 01:02:04,925
Speaker 8:  Yeah, no, that's a great question. When I say democratization, I mean

1012
01:02:05,265 --> 01:02:08,725
Speaker 8:  AI in general where it's going. We're not saying we're the only player out

1013
01:02:08,725 --> 01:02:11,085
Speaker 8:  there that's just gonna democratize the entire plan.

1014
01:02:11,085 --> 01:02:12,485
Speaker 2:  You're just in it for the money. Yeah, that's,

1015
01:02:12,745 --> 01:02:16,685
Speaker 8:  That's a very big promise. I'm talking in the general, for us, we,

1016
01:02:16,705 --> 01:02:20,605
Speaker 8:  we really see savings up to 60, 70% going aspect.

1017
01:02:20,625 --> 01:02:24,085
Speaker 8:  The, the thing that we're really saving people on production side, you need

1018
01:02:24,085 --> 01:02:27,125
Speaker 8:  a lot of these hardware, not just the motion capture suit. You need to track

1019
01:02:27,125 --> 01:02:30,485
Speaker 8:  your camera, which is additional hardware. You need all the sensors, which

1020
01:02:30,485 --> 01:02:33,445
Speaker 8:  is more price on the hardware. And then in post-production, there's a lot

1021
01:02:33,445 --> 01:02:37,405
Speaker 8:  of these monotone tasks that really take, take we weeks sometimes that

1022
01:02:37,525 --> 01:02:41,165
Speaker 8:  we brought down to two hours on the side. So we've seen quite a bit of

1023
01:02:41,805 --> 01:02:45,485
Speaker 8:  significant change. Again, it really depends, you know, and

1024
01:02:45,665 --> 01:02:48,245
Speaker 8:  we just spoke earlier in visual effects. You have something called easy,

1025
01:02:48,245 --> 01:02:51,245
Speaker 8:  medium and hard shots. And usually when you bid a project you say, okay,

1026
01:02:51,445 --> 01:02:55,205
Speaker 8:  I have 2000 shots, 500 is gonna be easy, about a thousand

1027
01:02:55,305 --> 01:02:59,085
Speaker 8:  is gonna be medium and then 500 is gonna be hard. So what we did

1028
01:02:59,105 --> 01:03:02,005
Speaker 8:  is kind of brought down some of those hard to be medium and medium to be

1029
01:03:02,005 --> 01:03:05,525
Speaker 8:  easy. So that really where, where the savings come on. And what we've seen

1030
01:03:05,555 --> 01:03:09,325
Speaker 8:  also what's really interesting is a lot of our users are, because

1031
01:03:09,665 --> 01:03:13,645
Speaker 8:  our tool goes from $20 a month to, to a hundred dollars a

1032
01:03:13,645 --> 01:03:17,165
Speaker 8:  month. So we really built this to be affordable. Traditionally if you use

1033
01:03:17,175 --> 01:03:20,965
Speaker 8:  mocap sometimes you're gonna spend $30,000 a day to do that, right? So there's

1034
01:03:20,965 --> 01:03:24,725
Speaker 8:  a drastic, drastic change there. But you know, we were talking earlier, it's

1035
01:03:24,725 --> 01:03:28,605
Speaker 8:  like our goal is to open it up free as well as a part of that. The only reason

1036
01:03:28,665 --> 01:03:32,165
Speaker 8:  as a startup you can't do that because you, these computers, you know, we

1037
01:03:32,165 --> 01:03:35,245
Speaker 8:  made it as a web browser so everybody can, accessibility was a big one for

1038
01:03:35,245 --> 01:03:38,885
Speaker 8:  us. So everybody can access it, but that means you have the cloud cost.

1039
01:03:39,345 --> 01:03:43,245
Speaker 8:  But I always say cloud's not a actual cloud up there that's free. It's, you

1040
01:03:43,245 --> 01:03:46,525
Speaker 8:  know, Amazon has a workstation somewhere that you have to pay by the hour,

1041
01:03:46,525 --> 01:03:50,325
Speaker 8:  right? So that, that is a cost on us. You know, even Chad, GPT, they're spending

1042
01:03:50,325 --> 01:03:53,445
Speaker 8:  millions of dollars a second when someone's prompting someone's, you know,

1043
01:03:53,445 --> 01:03:57,285
Speaker 8:  someone's gotta pay for that. So for us as a startup, we picked it

1044
01:03:57,425 --> 01:04:00,965
Speaker 8:  in a route where we're not gonna go out to raise hundreds of millions of

1045
01:04:00,965 --> 01:04:04,365
Speaker 8:  dollars just to get higher numbers. So we can claim we have, you know,

1046
01:04:04,605 --> 01:04:08,085
Speaker 8:  hundreds of millions of users, right? We did end up close to a million of

1047
01:04:08,085 --> 01:04:11,885
Speaker 8:  users in our platform, but the hard part there is like, okay,

1048
01:04:11,985 --> 01:04:15,925
Speaker 8:  we can't open it up yet until we bring the cost down for us. So we can, we

1049
01:04:15,925 --> 01:04:19,205
Speaker 8:  can make sure that this can run for a long time. Totally. All right,

1050
01:04:19,205 --> 01:04:22,245
Speaker 10:  Let's do a couple more. Do you guys have any thoughts around

1051
01:04:22,985 --> 01:04:26,765
Speaker 10:  the, the end game for materials being created

1052
01:04:26,855 --> 01:04:30,645
Speaker 10:  these days? Like at the movie theater for instance, are we gonna

1053
01:04:30,705 --> 01:04:34,685
Speaker 10:  see changes in how we view these new products At

1054
01:04:34,685 --> 01:04:37,805
Speaker 10:  some point it seems like going to the movies has

1055
01:04:38,075 --> 01:04:41,565
Speaker 10:  fundamentally changed. I don't know if it was the pandemic or what have

1056
01:04:41,565 --> 01:04:45,485
Speaker 10:  you, but now we are at these inflection points with the ai,

1057
01:04:46,145 --> 01:04:50,005
Speaker 10:  the new ways that we inhabit spaces. Do you have any thoughts around

1058
01:04:50,005 --> 01:04:50,285
Speaker 10:  that?

1059
01:04:51,155 --> 01:04:55,085
Speaker 7:  Yeah, I, I mean I think yeah we will continue to see that change, you know,

1060
01:04:55,085 --> 01:04:58,925
Speaker 7:  will it ever go away entirely? I don't think so. I think people will go

1061
01:04:58,925 --> 01:05:02,765
Speaker 7:  to the, go to the movies and they may be sitting in a virtual cinema but

1062
01:05:03,195 --> 01:05:06,645
Speaker 7:  they might be watching movies till, I don't think that'll ever go away. I

1063
01:05:06,645 --> 01:05:10,285
Speaker 7:  think that's a very special experience. But I do think with new advancements,

1064
01:05:10,285 --> 01:05:14,245
Speaker 7:  you know, we will see new mediums that are extremely compelling. I, I

1065
01:05:14,245 --> 01:05:17,805
Speaker 7:  especially think interactive for games is gonna be huge obviously.

1066
01:05:18,195 --> 01:05:21,805
Speaker 7:  Yeah, we'll continue to see new mediums as technology continues to evolve.

1067
01:05:22,265 --> 01:05:25,445
Speaker 7:  But I don't know if we're we'll ever lose, you know, there there's gonna

1068
01:05:25,445 --> 01:05:29,085
Speaker 7:  be some form of nostalgia of like, oh I love going to see a movie in a theater

1069
01:05:29,685 --> 01:05:33,085
Speaker 7:  surrounded by other people on a huge screen. And I don't, I hope we'll never

1070
01:05:33,085 --> 01:05:33,445
Speaker 7:  lose that.

1071
01:05:34,545 --> 01:05:38,525
Speaker 11:  I'm wondering if you have a POV on like other industries outside of

1072
01:05:38,525 --> 01:05:41,685
Speaker 11:  Hollywood, whether it's like Nollywood or Bollywood, are they embracing

1073
01:05:41,905 --> 01:05:45,845
Speaker 11:  AI more and like maybe seeing that as an opportunity to even like skyrocket

1074
01:05:45,845 --> 01:05:46,685
Speaker 11:  closer to Hollywood?

1075
01:05:46,915 --> 01:05:50,725
Speaker 8:  Yeah, definitely. We've seen like one of our first users have been from,

1076
01:05:50,835 --> 01:05:54,805
Speaker 8:  from Nollywood and, and different ones and we've seen it definitely happen

1077
01:05:55,105 --> 01:05:59,085
Speaker 8:  to me that's, that's exciting 'cause I think we'll get, we'll get more chances

1078
01:05:59,325 --> 01:06:02,965
Speaker 8:  'cause especially in like CG and visual effects, a lot of these smaller markets

1079
01:06:02,965 --> 01:06:06,045
Speaker 8:  really wanna do the same thing that H is doing, but they either didn't have

1080
01:06:06,045 --> 01:06:08,765
Speaker 8:  the know-how or the budgets needed to do that, right? Or

1081
01:06:08,765 --> 01:06:10,925
Speaker 7:  The production equipment. They just don't have the hardware.

1082
01:06:11,155 --> 01:06:14,005
Speaker 8:  Yeah, yeah, exactly. So that's where, that's where it's really exciting.

1083
01:06:14,105 --> 01:06:17,925
Speaker 8:  We, we, we've seen a lot and I think there's a, a stat somewhere

1084
01:06:18,125 --> 01:06:22,045
Speaker 8:  Ollywood is making more films per month than something about like

1085
01:06:22,045 --> 01:06:25,125
Speaker 8:  Hollywood than one more industry combined. Wow. Yeah. Yeah. It's, it's quite

1086
01:06:25,405 --> 01:06:28,965
Speaker 8:  fascinating and if you start looking into it, it's, it's crazy quantity of

1087
01:06:28,965 --> 01:06:31,845
Speaker 8:  it. But again, as you mentioned earlier, like if we have good dubbing and

1088
01:06:31,845 --> 01:06:34,965
Speaker 8:  stuff that comes out, we might probably see more of those films, right? Because

1089
01:06:36,045 --> 01:06:39,325
Speaker 8:  I always joke my girlfriend doesn't like to watch films with subtitles, which

1090
01:06:39,325 --> 01:06:43,085
Speaker 8:  is again terrible for me, right? 'cause I love to watch foreign

1091
01:06:43,085 --> 01:06:43,765
Speaker 8:  films. I

1092
01:06:43,765 --> 01:06:45,765
Speaker 2:  Can't look at my phone and read subtitles at the same time.

1093
01:06:46,005 --> 01:06:49,725
Speaker 8:  Yeah, I know, I know. She said, she says she doesn't like to read movies

1094
01:06:49,865 --> 01:06:52,845
Speaker 8:  and I just, I can't hear her say that every, every, every time. But I do

1095
01:06:52,845 --> 01:06:55,885
Speaker 8:  think like a lot of audience actually doesn't like to read sub. So if we

1096
01:06:55,885 --> 01:06:59,445
Speaker 8:  figure out a way, how do we bring these foreign films more and make people

1097
01:06:59,445 --> 01:07:03,365
Speaker 8:  comfortable to watch it, it will it, it will help on that side.

1098
01:07:03,365 --> 01:07:03,565
Speaker 8:  Yeah.

1099
01:07:03,565 --> 01:07:06,725
Speaker 7:  But our, our, we have, we have users all over the world from

1100
01:07:07,255 --> 01:07:10,525
Speaker 7:  Japan to China to England, Norway,

1101
01:07:11,065 --> 01:07:14,565
Speaker 7:  US, Mexico, south America. Yeah, we have users kind of globally,

1102
01:07:14,985 --> 01:07:18,645
Speaker 8:  But I think general outside of our platform, yeah we're seeing a lot more

1103
01:07:19,145 --> 01:07:22,325
Speaker 8:  AI usage on that side. Everybody's trying to figure out, what's exciting

1104
01:07:22,325 --> 01:07:25,245
Speaker 8:  is sometimes you think you see things going on YouTube that just some keep

1105
01:07:25,245 --> 01:07:28,805
Speaker 8:  came up with a new pipeline. It's extremely promising. And I was like, wow,

1106
01:07:28,805 --> 01:07:32,365
Speaker 8:  that's very interesting. This is, you know, if this kid goes out you could

1107
01:07:32,365 --> 01:07:36,005
Speaker 8:  raise $10 million on idea on a startup because this is a really good

1108
01:07:36,205 --> 01:07:39,325
Speaker 8:  pipeline but it's one kid sitting at the computer and researching things.

1109
01:07:39,645 --> 01:07:42,925
Speaker 8:  I think that's the exciting part and we, we said it's kinda like equal starting

1110
01:07:42,925 --> 01:07:46,845
Speaker 8:  point for everyone. That's the fear of it. But that's also to

1111
01:07:46,845 --> 01:07:49,965
Speaker 8:  me, the exciting part of it is meaning like, oh everybody has a chance to

1112
01:07:49,965 --> 01:07:53,405
Speaker 8:  figure out how to do this. And obviously, you know, the traditional kind

1113
01:07:53,405 --> 01:07:55,685
Speaker 8:  of gatekeepers will be worried about that

1114
01:07:55,985 --> 01:07:59,525
Speaker 7:  And may and maybe at some point, you know, maybe those gatekeepers even get

1115
01:07:59,765 --> 01:08:03,125
Speaker 7:  replaced by people who are telling better stories, who are coming up with

1116
01:08:03,125 --> 01:08:07,005
Speaker 7:  more efficient ways to, to tell those and you know, it's better for the world

1117
01:08:07,095 --> 01:08:07,445
Speaker 7:  maybe

1118
01:08:07,825 --> 01:08:10,685
Speaker 2:  Here's hoping, alright, I could talk to you guys about this for hours and

1119
01:08:10,685 --> 01:08:13,125
Speaker 2:  I also didn't ask you a single question about Ready Player one, so you should

1120
01:08:13,125 --> 01:08:13,525
Speaker 2:  be grateful.

1121
01:08:14,025 --> 01:08:14,645
Speaker 12:  All right Nico, this

1122
01:08:14,645 --> 01:08:15,405
Speaker 7:  One's great. We gotta

1123
01:08:15,405 --> 01:08:18,085
Speaker 2:  Go. Ty, Nico, Nicola, thank you so much for doing this. I think this is really

1124
01:08:18,085 --> 01:08:18,245
Speaker 2:  fun.

1125
01:08:18,245 --> 01:08:20,245
Speaker 12:  Thank you. Of course. Thanks for having us, for having us. Thanks everyone.

1126
01:08:22,115 --> 01:08:25,245
Speaker 2:  Alright, we gotta take one more break and then we'll be back to do a question

1127
01:08:25,245 --> 01:08:27,245
Speaker 2:  from the Vergecast hotline. We'll be right back.

1128
01:10:48,335 --> 01:10:51,915
Speaker 2:  We are back. Let's get to the hotline as always. The number is 8 6 6

1129
01:10:52,195 --> 01:10:55,675
Speaker 2:  VERGE one one and the email is vergecast at The Verge dot com. We love all

1130
01:10:55,675 --> 01:10:58,475
Speaker 2:  your questions and we try to answer at least one on the show every week.

1131
01:10:58,725 --> 01:11:01,395
Speaker 2:  Thank you. by the way, to everybody who keeps reaching out about the books

1132
01:11:01,455 --> 01:11:05,235
Speaker 2:  Palmer, that is like the most intriguing device according to The

1133
01:11:05,355 --> 01:11:09,155
Speaker 2:  Vergecast hotline in a long time. I'm getting one and I'm going to have a

1134
01:11:09,155 --> 01:11:13,035
Speaker 2:  lot to say with Alex on this show very soon. Get ready this

1135
01:11:13,125 --> 01:11:14,565
Speaker 2:  week we have a question from Braden.

1136
01:11:15,505 --> 01:11:19,005
Speaker 17:  Hey David, this is Braden from Utah. Why has

1137
01:11:19,085 --> 01:11:22,765
Speaker 17:  iMessage not improved? I mean that like I remember when

1138
01:11:23,035 --> 01:11:26,845
Speaker 17:  like I 10 happened and Game Pigeon was a thing where you could play

1139
01:11:26,845 --> 01:11:30,645
Speaker 17:  games through iMessage with your friends. It was the coolest

1140
01:11:30,695 --> 01:11:34,525
Speaker 17:  thing. And I still play game pitching games with my friends and family

1141
01:11:34,625 --> 01:11:38,365
Speaker 17:  to this day, but I remember in 2016, that's eight years

1142
01:11:38,545 --> 01:11:42,485
Speaker 17:  ago, I was like, dang, this is cool now. But think of

1143
01:11:42,485 --> 01:11:46,205
Speaker 17:  the things people will make with this, how there will be other things to

1144
01:11:46,205 --> 01:11:50,125
Speaker 17:  do in iMessage. And currently like

1145
01:11:50,385 --> 01:11:53,765
Speaker 17:  the list has not grown at all. It is game pigeon

1146
01:11:54,295 --> 01:11:58,005
Speaker 17:  gifts and then weird apps that want to be in your messages that don't belong

1147
01:11:58,005 --> 01:12:01,405
Speaker 17:  there. And it's just like, don't you feel like iMessage

1148
01:12:01,875 --> 01:12:05,565
Speaker 17:  held more promises and just haven't fulfilled? Does The Vergecast

1149
01:12:05,595 --> 01:12:09,445
Speaker 17:  play game pigeon games together? What are your thoughts? I wanna hear 'em.

1150
01:12:10,235 --> 01:12:13,365
Speaker 2:  This is actually something I've done a lot of reporting about over the last

1151
01:12:13,365 --> 01:12:17,245
Speaker 2:  couple of years. Not necessarily with iMessage specifically, but with

1152
01:12:17,245 --> 01:12:21,125
Speaker 2:  messaging apps in general. And I think you can look at the whole world of

1153
01:12:21,125 --> 01:12:24,725
Speaker 2:  messaging apps to kind of answer the question. So the thing that turns out

1154
01:12:24,725 --> 01:12:28,525
Speaker 2:  to be true about messaging apps is that a, you can't

1155
01:12:28,525 --> 01:12:32,165
Speaker 2:  really build a business on a messaging app. There's just no real

1156
01:12:32,445 --> 01:12:36,405
Speaker 2:  evidence that lots of people will pay you lots of money for a

1157
01:12:36,405 --> 01:12:40,045
Speaker 2:  way to talk to their friends, right? Like the idea of a texting app is so

1158
01:12:40,045 --> 01:12:43,765
Speaker 2:  commoditized at this point that there's really no business to be had there.

1159
01:12:44,265 --> 01:12:48,165
Speaker 2:  So if you are a messaging company, let's say Snapchat

1160
01:12:48,225 --> 01:12:52,125
Speaker 2:  or WhatsApp or Telegram or iMessage,

1161
01:12:52,125 --> 01:12:56,045
Speaker 2:  in this case, you have to figure out a way to be successful that

1162
01:12:56,175 --> 01:12:59,965
Speaker 2:  isn't just messaging, right? If you and I messaging each other

1163
01:13:00,225 --> 01:13:03,205
Speaker 2:  is never going to be a business, and I, I believe that it's not, you have

1164
01:13:03,205 --> 01:13:05,885
Speaker 2:  to figure out something else to do. So you essentially have two options.

1165
01:13:06,225 --> 01:13:10,125
Speaker 2:  You can either try to build stuff around messaging, which

1166
01:13:10,125 --> 01:13:13,325
Speaker 2:  is like what Snapchat does with things like

1167
01:13:14,045 --> 01:13:17,165
Speaker 2:  Snapchat originals and Spotlight and all the creator stuff that they've been

1168
01:13:17,165 --> 01:13:20,725
Speaker 2:  doing. And stories frankly is a way around messaging stuff

1169
01:13:21,145 --> 01:13:24,805
Speaker 2:  so that you come to the app to do messaging and you

1170
01:13:24,875 --> 01:13:28,285
Speaker 2:  stay to do everything else and that's where all the money is, right? That's

1171
01:13:28,285 --> 01:13:32,005
Speaker 2:  essentially what WeChat is. WeChat's whole idea was to build

1172
01:13:32,065 --> 01:13:35,765
Speaker 2:  an entire universe around the place that people go to communicate with their

1173
01:13:35,765 --> 01:13:38,965
Speaker 2:  friends. That's a real business. You can do it. It's a tricky thing to get

1174
01:13:38,965 --> 01:13:42,925
Speaker 2:  right from a UI perspective. I think Snap has been through this in

1175
01:13:43,045 --> 01:13:46,565
Speaker 2:  a bunch of complicated ways, trying to say, okay, you came here for a messaging

1176
01:13:46,565 --> 01:13:50,085
Speaker 2:  app, how do we get you into all of the things that we can do to make money?

1177
01:13:50,345 --> 01:13:54,245
Speaker 2:  But that is one way to do it. That's also I would say what WhatsApp is

1178
01:13:54,245 --> 01:13:57,125
Speaker 2:  doing with things like business messaging. WhatsApp is really interested

1179
01:13:57,265 --> 01:14:01,165
Speaker 2:  in getting you to get menus or

1180
01:14:01,605 --> 01:14:04,485
Speaker 2:  business hours or things like that through WhatsApp. That's a thing people

1181
01:14:04,485 --> 01:14:08,205
Speaker 2:  are doing kind of informally anyway. And they would really, really,

1182
01:14:08,205 --> 01:14:12,125
Speaker 2:  really like to bring that into the WhatsApp ecosystem. That's

1183
01:14:12,125 --> 01:14:14,565
Speaker 2:  also a way they can make money, they can charge businesses for those services.

1184
01:14:14,635 --> 01:14:18,525
Speaker 2:  They can take a portion of money that gets moved around inside of these things.

1185
01:14:18,745 --> 01:14:22,565
Speaker 2:  But again, all of this is kind of ancillary to, this is where you message

1186
01:14:22,565 --> 01:14:26,245
Speaker 2:  your friends. The other way to do it is to try to figure out a way to

1187
01:14:26,585 --> 01:14:30,285
Speaker 2:  insert experiences into the place where you message with your friends.

1188
01:14:30,545 --> 01:14:34,285
Speaker 2:  And this is where it gets really messy. I think about like

1189
01:14:34,745 --> 01:14:38,245
Speaker 2:  Google Aloe is probably the best example of this. If you remember that this

1190
01:14:38,245 --> 01:14:42,165
Speaker 2:  messaging app Google made from years ago and its big idea was that you

1191
01:14:42,165 --> 01:14:46,085
Speaker 2:  could bring an AI system at that time, you know, Google assistant

1192
01:14:46,635 --> 01:14:50,565
Speaker 2:  into your chat with your friends. So you could say, you know, we're

1193
01:14:50,685 --> 01:14:54,245
Speaker 2:  planning to go to dinner or we're planning to go to the movies, let's buy

1194
01:14:54,245 --> 01:14:57,965
Speaker 2:  movie tickets. And you could actually buy movie tickets inside of your

1195
01:14:57,965 --> 01:15:01,645
Speaker 2:  chat with your friends. Cool idea. In theory, I think

1196
01:15:01,675 --> 01:15:05,005
Speaker 2:  this is also where things like games like Braden was talking about or this

1197
01:15:05,005 --> 01:15:08,765
Speaker 2:  idea that we can have experiences together inside of our chat

1198
01:15:08,765 --> 01:15:12,285
Speaker 2:  window seems cool. Emphasis on seams

1199
01:15:13,065 --> 01:15:16,965
Speaker 2:  in reality, this ends up being a pain. It's very hard for these apps

1200
01:15:16,965 --> 01:15:20,685
Speaker 2:  to find a way to sort of insert themselves or add these services or even

1201
01:15:20,685 --> 01:15:24,565
Speaker 2:  make you aware of these services without getting in the way of just you talking

1202
01:15:24,585 --> 01:15:28,405
Speaker 2:  to your friends. And if you think about it, the idea of screwing up

1203
01:15:28,405 --> 01:15:32,325
Speaker 2:  your chat interface with your friends is like totally untenable.

1204
01:15:32,325 --> 01:15:36,005
Speaker 2:  People just won't do it, right? You get to the point where I wanna have

1205
01:15:36,005 --> 01:15:39,845
Speaker 2:  messages back and forth with my friends and if you change that or screw

1206
01:15:39,845 --> 01:15:43,325
Speaker 2:  it up, disaster. And I think in a funny way, apple has

1207
01:15:43,605 --> 01:15:47,565
Speaker 2:  been trying to almost do it without fully doing it

1208
01:15:47,585 --> 01:15:50,685
Speaker 2:  for a really long time. There was the iMessage app store where you could

1209
01:15:50,685 --> 01:15:54,645
Speaker 2:  send stuff, but that had an incredible discovery problem. Like most people,

1210
01:15:54,725 --> 01:15:58,605
Speaker 2:  I don't think knew the iMessage app store existed. It was buried in a

1211
01:15:58,675 --> 01:16:02,565
Speaker 2:  menu in the iOS keyboard when you were in iMessage

1212
01:16:02,635 --> 01:16:06,085
Speaker 2:  that didn't really work. Now there's this new menu where you can send stuff

1213
01:16:06,195 --> 01:16:09,325
Speaker 2:  that works a little bit better, but that menu is really slow and kind of

1214
01:16:09,325 --> 01:16:12,965
Speaker 2:  awkward. But I think what Apple would really like to do is

1215
01:16:13,075 --> 01:16:17,045
Speaker 2:  make messages a full on internet platform in a

1216
01:16:17,045 --> 01:16:20,285
Speaker 2:  very real way. You should be able to send somebody a

1217
01:16:20,835 --> 01:16:24,565
Speaker 2:  link to tic-tac toe and it just expands. And you can play

1218
01:16:24,685 --> 01:16:28,405
Speaker 2:  tic-tac toe inside of the iMessage window right there without ever leaving

1219
01:16:28,405 --> 01:16:31,245
Speaker 2:  the app. If you think of it right now, a lot of this stuff is just links,

1220
01:16:31,245 --> 01:16:34,005
Speaker 2:  right? Like if I wanna play a game with you, I send you a link, you tap on

1221
01:16:34,005 --> 01:16:37,045
Speaker 2:  the link and we go play the game. That's actually a pretty good user experience.

1222
01:16:37,345 --> 01:16:41,045
Speaker 2:  So to bring that into the messaging system is a pretty

1223
01:16:41,115 --> 01:16:44,685
Speaker 2:  tall order. And to do it in a way that doesn't disrupt anything else or

1224
01:16:45,045 --> 01:16:48,765
Speaker 2:  threaten to take your data or break encryption in these encrypted

1225
01:16:48,765 --> 01:16:52,285
Speaker 2:  messaging apps. It's just hard. People want private messaging, they want

1226
01:16:52,285 --> 01:16:54,965
Speaker 2:  simple messaging and they wanna be able to talk to their friends and to add

1227
01:16:54,965 --> 01:16:58,725
Speaker 2:  things to that is borderline impossible. That said,

1228
01:16:58,955 --> 01:17:02,085
Speaker 2:  they're gonna keep trying. They are just going to keep trying. There are

1229
01:17:02,145 --> 01:17:06,085
Speaker 2:  too many people spending too much time in messaging apps for these companies

1230
01:17:06,105 --> 01:17:10,085
Speaker 2:  to not try and find ways to insert themselves. And so you'll see more stuff

1231
01:17:10,085 --> 01:17:13,965
Speaker 2:  like this that inside of things like Slack and Discord, you're

1232
01:17:13,965 --> 01:17:17,925
Speaker 2:  already starting to see these fuller web-like experiences you can

1233
01:17:17,925 --> 01:17:21,685
Speaker 2:  have just inside of a chat window. I think that's where we're headed

1234
01:17:21,745 --> 01:17:24,965
Speaker 2:  in certain ways, but I also think that's gonna make basic

1235
01:17:25,155 --> 01:17:28,525
Speaker 2:  messaging apps like Signal and WhatsApp and frankly

1236
01:17:28,645 --> 01:17:32,045
Speaker 2:  iMessage all the more valuable to people. 'cause that's where you go to just

1237
01:17:32,195 --> 01:17:36,165
Speaker 2:  talk and that is like the main thing we wanna do on the internet. Think

1238
01:17:36,165 --> 01:17:39,765
Speaker 2:  about how annoying Gmail ads are. You're like, I just wanna look at my emails

1239
01:17:39,785 --> 01:17:42,445
Speaker 2:  and you're showing me a bunch of ads at the top of my inbox for some reason.

1240
01:17:42,985 --> 01:17:46,925
Speaker 2:  Add that to your text messages with friends and family, which

1241
01:17:46,925 --> 01:17:50,165
Speaker 2:  are even more intimate and even more important. And that just sucks. People

1242
01:17:50,165 --> 01:17:53,445
Speaker 2:  will run away from that. And if you're Apple, again,

1243
01:17:53,805 --> 01:17:57,365
Speaker 2:  iMessage is one of the core things that keeps people using

1244
01:17:57,755 --> 01:18:01,205
Speaker 2:  iPhones. Being a blue bubble is a very important thing. And so if you make

1245
01:18:01,205 --> 01:18:04,845
Speaker 2:  people hate iMessage to the point where they'll start using WhatsApp, that's

1246
01:18:05,005 --> 01:18:08,925
Speaker 2:  a huge, huge risk for Apple. So Apple is in this tricky spot, just like

1247
01:18:08,925 --> 01:18:11,925
Speaker 2:  everybody who makes a messaging app of saying messaging is so important,

1248
01:18:11,985 --> 01:18:14,725
Speaker 2:  we wanna build a business out of it. It's where people are, it's what people

1249
01:18:14,725 --> 01:18:18,685
Speaker 2:  wanna do. But if we do it even slightly wrong, people will

1250
01:18:18,745 --> 01:18:22,325
Speaker 2:  run away. And that kills a huge amount of any kind of

1251
01:18:22,875 --> 01:18:26,365
Speaker 2:  moat and advantage that we have to keep people in that tension

1252
01:18:26,745 --> 01:18:30,605
Speaker 2:  mostly means we get to keep our simple good messaging apps, but

1253
01:18:30,635 --> 01:18:34,565
Speaker 2:  that tension may not last forever. And I think it's gonna be really interesting

1254
01:18:34,565 --> 01:18:38,325
Speaker 2:  because it is abundantly clear to everyone who makes an app

1255
01:18:38,395 --> 01:18:41,805
Speaker 2:  that if you can get people to message inside of it, they will live inside

1256
01:18:41,965 --> 01:18:44,965
Speaker 2:  of your app and that becomes very powerful. You don't wanna screw it up,

1257
01:18:45,065 --> 01:18:48,365
Speaker 2:  but it's a powerful thing to have going for you. So we'll see. I would bet

1258
01:18:48,365 --> 01:18:51,965
Speaker 2:  we're gonna see a lot more experimentation inside of these apps,

1259
01:18:51,965 --> 01:18:55,765
Speaker 2:  especially these kind of just outside of messaging

1260
01:18:55,765 --> 01:18:59,725
Speaker 2:  things where like, I can send you a game in the way

1261
01:18:59,925 --> 01:19:03,605
Speaker 2:  that I send you a gif, or I can send you an interactive

1262
01:19:03,635 --> 01:19:06,525
Speaker 2:  poll inside of a group chat. That's the kind of stuff that's already coming

1263
01:19:06,585 --> 01:19:10,085
Speaker 2:  to WhatsApp and I think you'll start to see that in more places. But

1264
01:19:10,485 --> 01:19:14,325
Speaker 2:  I sort of hope we never get these like massively interactive full web

1265
01:19:14,325 --> 01:19:18,045
Speaker 2:  experiences. I do not need a chat bot in my messaging system.

1266
01:19:18,325 --> 01:19:22,245
Speaker 2:  I just, I just don't. This thing where you can talk to an AI chat bot,

1267
01:19:22,395 --> 01:19:26,325
Speaker 2:  like you talk to a person, fine, do not put an AI bot in my group

1268
01:19:26,375 --> 01:19:30,285
Speaker 2:  chats. Meta Apple, Google, whoever else. I don't

1269
01:19:30,285 --> 01:19:33,165
Speaker 2:  want that. I don't think anybody does, but they're probably gonna do it.

1270
01:19:33,575 --> 01:19:37,485
Speaker 2:  We'll see. Alright, that is it for The Vergecast today.

1271
01:19:37,485 --> 01:19:40,405
Speaker 2:  Thanks to everybody who came on the show, and thank you as always for listening.

1272
01:19:40,515 --> 01:19:43,805
Speaker 2:  There's lots more on everything we talked about at The Verge dot com. We'll

1273
01:19:43,805 --> 01:19:45,045
Speaker 2:  put a bunch of Lauren's stuff

1274
01:20:14,835 --> 01:20:15,285
Speaker 2:  send us

