1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 667ac264-ab4b-4d7b-bd70-f18170765fb0
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-4219485121670256839/-4523873305035685200/s93290-US-5879s-1737714029.mp3
Description: Nilay, David, and The Verge's Allison Johnson run down all the biggest news from the latest Samsung Unpacked. The S25 Edge had everyone excited, but the other new Galaxy S25 models feel a little familiar. Then, The Verge's Lauren Feiner updates us on the many goings-on in the first days of the new Trump administration, from the TikTok ban delay to the executive orders on citizenship and AI. Finally, in the lightning round, David and Nilay talk about Netflix's price increase, smart-home standards, and more.

2
00:01:37,625 --> 00:01:41,335
Speaker 1:  Hello and welcome to Vergecast the flagship podcast of whatever the hell

3
00:01:41,335 --> 00:01:44,495
Speaker 1:  is going on right now. Does that feel right, David? Yeah,

4
00:01:44,535 --> 00:01:47,895
Speaker 5:  I, I don't know the answer and there are thousands of answers all at the

5
00:01:47,895 --> 00:01:48,135
Speaker 5:  same time.

6
00:01:48,615 --> 00:01:49,655
Speaker 1:  Whatever it is, we're here for you.

7
00:01:51,325 --> 00:01:54,455
Speaker 1:  It's gonna be great. We're super into it. We have a lot talk about this week.

8
00:01:55,255 --> 00:01:58,655
Speaker 1:  I think we're talking on what day day three of a flurry of

9
00:01:58,975 --> 00:02:02,815
Speaker 1:  Trump and big tech merger. We'll get to that later. That's basically a

10
00:02:02,815 --> 00:02:05,735
Speaker 1:  lightning round. We're, we're just gonna have to get through that every week

11
00:02:05,975 --> 00:02:08,615
Speaker 5:  Like I was going through and putting the rundown together. And it's an unusual

12
00:02:08,875 --> 00:02:12,175
Speaker 5:  number of things that we just have to say out loud in a row.

13
00:02:12,605 --> 00:02:15,775
Speaker 5:  There's like, there's just, this is kind of the moment that we're in is a

14
00:02:15,775 --> 00:02:18,415
Speaker 5:  lot of things are happening and I don't know what to make of any of them,

15
00:02:18,595 --> 00:02:20,935
Speaker 5:  but it feels useful to say all of them out loud.

16
00:02:21,365 --> 00:02:24,295
Speaker 1:  Yeah. So we're gonna do that in like two minutes and then David's gonna talk

17
00:02:24,295 --> 00:02:26,335
Speaker 1:  about Netflix increasing prices for an hour and a half.

18
00:02:26,795 --> 00:02:29,535
Speaker 5:  Oh, good. We, we, we have the same idea. This is great.

19
00:02:30,045 --> 00:02:33,175
Speaker 1:  Okay. That's, that's how that's gonna go. Well let me just start by saying

20
00:02:33,175 --> 00:02:36,095
Speaker 1:  it's, it's my first Friday virtual house of 2025. It's nice, it's nice to

21
00:02:36,095 --> 00:02:39,975
Speaker 1:  be back with all of you. There's quite a lot going on. And

22
00:02:39,975 --> 00:02:43,815
Speaker 1:  we should start, I think with Galaxy unpacked of all the things. I think

23
00:02:43,815 --> 00:02:47,295
Speaker 1:  that's right. We looked around and it is Samsung

24
00:02:47,475 --> 00:02:51,415
Speaker 1:  Galaxy unpacked. That's where we should start this vergecast because there's

25
00:02:51,415 --> 00:02:54,095
Speaker 1:  new phones and like truly what is The Verge except new phones.

26
00:02:54,405 --> 00:02:58,015
Speaker 5:  Yeah. If you listen, you, you boil any of the other stuff that's happening

27
00:02:58,245 --> 00:03:01,845
Speaker 5:  down far enough and you get to new phones, which I think is like the whole

28
00:03:01,845 --> 00:03:02,805
Speaker 5:  theory of The Verge.

29
00:03:02,955 --> 00:03:06,245
Speaker 1:  Yeah. So this works out. It's actually, I will say this, this is very true.

30
00:03:06,245 --> 00:03:10,165
Speaker 1:  We have a lot of very smart, very capable reporters on our policy desk reporters,

31
00:03:10,165 --> 00:03:13,125
Speaker 1:  senators, and every now and again they're like, yeah, we work for the phones

32
00:03:13,125 --> 00:03:16,845
Speaker 1:  website. And every now and again they also point out that only the phone

33
00:03:16,845 --> 00:03:20,445
Speaker 1:  website can do some of the policy coverage they do. 'cause you have to understand

34
00:03:20,445 --> 00:03:23,725
Speaker 1:  the phones to do the policy coverage. It all makes sense in the end. It's

35
00:03:23,725 --> 00:03:26,325
Speaker 1:  gonna be a little whiplash here, here at the top. But let's start with Samsung

36
00:03:26,425 --> 00:03:30,365
Speaker 1:  Galaxy Impact. 'cause it was a big one, not

37
00:03:30,485 --> 00:03:33,365
Speaker 1:  a big one. There's a lot to say about Gemini and what Gemini means to people.

38
00:03:34,265 --> 00:03:37,045
Speaker 1:  And then it was also apparently very exciting.

39
00:03:38,065 --> 00:03:41,885
Speaker 1:  And to tell us about the excitement level, I wanna actually bring

40
00:03:41,885 --> 00:03:45,805
Speaker 1:  in Allison Johnson who is, I believe we've called her in from

41
00:03:45,805 --> 00:03:47,605
Speaker 1:  the side of the road. Hello, Allison, are you here?

42
00:03:48,095 --> 00:03:48,445
Speaker 6:  Hello.

43
00:03:48,985 --> 00:03:49,485
Speaker 1:  How you doing?

44
00:03:50,485 --> 00:03:53,445
Speaker 6:  Great. It's beautiful in California. Who knew?

45
00:03:54,785 --> 00:03:58,565
Speaker 6:  But I'm on my way back to Seattle, so it's gonna be short-lived.

46
00:03:58,765 --> 00:03:59,045
Speaker 6:  I think

47
00:03:59,415 --> 00:04:03,245
Speaker 1:  We'll see how that goes. I feel like in the Annals of VERGE casting, pull

48
00:04:03,245 --> 00:04:07,165
Speaker 1:  over in your car and call us to talk about Samsung Galaxy impact is, this

49
00:04:07,165 --> 00:04:10,405
Speaker 1:  is a high water moment. Literally what has happened here. Yeah.

50
00:04:10,515 --> 00:04:10,805
Speaker 6:  Yeah.

51
00:04:10,985 --> 00:04:14,925
Speaker 1:  So you were there. They, they announced the Galaxy Edge, which is the

52
00:04:14,925 --> 00:04:18,685
Speaker 1:  really thin one. I hope people saw the video you made from there.

53
00:04:19,105 --> 00:04:22,885
Speaker 1:  The vibes were high. Just a ton of excitement at Samsung Galaxy

54
00:04:22,885 --> 00:04:23,965
Speaker 1:  Impact. Tell us what was going on.

55
00:04:24,155 --> 00:04:27,045
Speaker 6:  Yeah, it was kind of a, a rollercoaster ride because

56
00:04:28,625 --> 00:04:32,565
Speaker 6:  the, the bulk of unpacked, you know, The S 25 Ultra,

57
00:04:32,705 --> 00:04:36,285
Speaker 6:  the regular, the Plus, they are very, very minor

58
00:04:36,315 --> 00:04:40,285
Speaker 6:  updates, like in the most extreme way. 'cause that's been the case

59
00:04:40,285 --> 00:04:44,005
Speaker 6:  for a long time. But this is, it's gonna be a review

60
00:04:44,065 --> 00:04:47,805
Speaker 6:  of one UI seven, let's put it that way. So

61
00:04:48,705 --> 00:04:52,615
Speaker 6:  we kind of get through the presentation where like, these phones

62
00:04:52,955 --> 00:04:55,855
Speaker 6:  are mildly interesting, I'm not gonna lie.

63
00:04:57,435 --> 00:05:00,735
Speaker 6:  And then right at the end it was like a, and one more thing,

64
00:05:01,275 --> 00:05:05,095
Speaker 6:  the Galaxy S 25 Edge, and then it was like curtains

65
00:05:05,615 --> 00:05:08,655
Speaker 6:  and then run out to the, they had this

66
00:05:10,085 --> 00:05:14,015
Speaker 6:  area set up behind kind of where they did the presentation and had

67
00:05:14,035 --> 00:05:17,775
Speaker 6:  the, had an some sort of phone there, The S

68
00:05:17,775 --> 00:05:21,655
Speaker 6:  25 Edge phone. So it was, yeah, it was full

69
00:05:21,795 --> 00:05:22,815
Speaker 6:  on. And

70
00:05:22,835 --> 00:05:26,735
Speaker 5:  If I understand correctly, the the salient things that we know about

71
00:05:26,735 --> 00:05:30,495
Speaker 5:  this phone are that it is a phone and it's very

72
00:05:30,495 --> 00:05:32,935
Speaker 5:  thin. Yes. Is that, am I missing anything?

73
00:05:33,585 --> 00:05:37,175
Speaker 6:  Those are the two things I confirmed viewing it from

74
00:05:37,495 --> 00:05:41,255
Speaker 6:  a distance. Okay. No, it was a really funny experience,

75
00:05:41,325 --> 00:05:44,935
Speaker 6:  kind of like coming off of this announcement of like some pretty

76
00:05:45,255 --> 00:05:49,135
Speaker 6:  familiar devices and then I think we were all just thirsty for something

77
00:05:49,135 --> 00:05:52,775
Speaker 6:  different. And so they had this, this slim

78
00:05:52,785 --> 00:05:56,535
Speaker 6:  phone out there and it was a full on

79
00:05:56,565 --> 00:06:00,165
Speaker 6:  like see us from 10 years ago kind of

80
00:06:00,365 --> 00:06:03,165
Speaker 6:  scrum where we're all trying to get in and get photos of it.

81
00:06:04,985 --> 00:06:08,085
Speaker 6:  So yeah. But that's, we were not allowed to get close to it. We were not

82
00:06:08,085 --> 00:06:11,525
Speaker 6:  allowed to touch it. We weren't allowed to breathe too close to it.

83
00:06:12,205 --> 00:06:15,805
Speaker 6:  But it's very thin. It's, you can say that

84
00:06:17,065 --> 00:06:19,925
Speaker 6:  it has two cameras instead of three. We, we could tell that.

85
00:06:20,425 --> 00:06:24,325
Speaker 1:  Did they say why it's so thin or why that's important?

86
00:06:24,475 --> 00:06:28,285
Speaker 1:  Like if, if Samsung was like, this phone is twice

87
00:06:28,385 --> 00:06:31,125
Speaker 1:  as thick as last year's phone, but the battery lasts for four days,

88
00:06:32,265 --> 00:06:35,685
Speaker 1:  I'd be like, let's talk, let's get in there. Yeah. It's way thinner

89
00:06:36,425 --> 00:06:39,905
Speaker 1:  to me. It's like we, we all lived through the Johnny Ive

90
00:06:39,905 --> 00:06:43,785
Speaker 1:  experience once we're just doing it again. What, what's

91
00:06:43,785 --> 00:06:44,145
Speaker 1:  happening here?

92
00:06:44,805 --> 00:06:48,785
Speaker 6:  It feels a little bit of that. It feels like a style thing. I

93
00:06:49,015 --> 00:06:52,225
Speaker 6:  just, from the very, very little we know of it,

94
00:06:53,585 --> 00:06:57,465
Speaker 6:  supposedly it'll be lower price than, well than

95
00:06:57,465 --> 00:07:00,665
Speaker 6:  the Ultra, which is like the most expensive phone you can buy anyway.

96
00:07:02,655 --> 00:07:06,225
Speaker 6:  Yeah. It's kind of a, a curiosity. I think it's, it's sort of

97
00:07:07,085 --> 00:07:10,305
Speaker 6:  an answer to like, phones are all the same, you know,

98
00:07:10,775 --> 00:07:14,665
Speaker 6:  folding phones aside, what do we do that's different? And I guess they

99
00:07:14,665 --> 00:07:18,385
Speaker 6:  can make the phones slimmer now. So that's,

100
00:07:18,385 --> 00:07:19,545
Speaker 6:  that's what we're gonna have.

101
00:07:21,025 --> 00:07:24,705
Speaker 5:  I will say to Samsung's credit, and I've only seen this on the video, but

102
00:07:24,765 --> 00:07:28,585
Speaker 5:  the, the way that they showed this thing off is extremely

103
00:07:28,585 --> 00:07:32,345
Speaker 5:  cool. It was on this, this big table and the phone

104
00:07:32,345 --> 00:07:35,945
Speaker 5:  itself looked like it's, it's sort of suspended from a little chain. And

105
00:07:35,945 --> 00:07:39,865
Speaker 5:  then there are like thin slabs of what looked like stone in

106
00:07:39,865 --> 00:07:43,425
Speaker 5:  the, it's this very like earthy cool, like

107
00:07:44,265 --> 00:07:46,665
Speaker 5:  thousands of years ago kind of vibe on this table. Yeah. It

108
00:07:46,665 --> 00:07:49,305
Speaker 1:  Looked like they were painting cabinet doors. What are you talking about?

109
00:07:50,765 --> 00:07:52,825
Speaker 6:  Oh, right. That's how you would hang a like cabinet.

110
00:07:52,885 --> 00:07:55,425
Speaker 1:  It looked like at any moment someone was gonna come up with like a spray

111
00:07:55,425 --> 00:07:55,665
Speaker 1:  paint

112
00:07:55,805 --> 00:07:59,625
Speaker 6:  Can. Yeah. So the, the

113
00:07:59,625 --> 00:08:01,665
Speaker 6:  other little slabs, it was like the,

114
00:08:01,725 --> 00:08:03,905
Speaker 1:  Is that it? So that's the whole thing. It's it's thin they didn't

115
00:08:04,935 --> 00:08:05,865
Speaker 6:  It's thin. Go

116
00:08:05,865 --> 00:08:06,585
Speaker 1:  Ahead. You go ahead Allison.

117
00:08:07,045 --> 00:08:10,825
Speaker 6:  No, they were saying that the, the other little slabs were the,

118
00:08:10,965 --> 00:08:14,865
Speaker 6:  the previous phones, The S 24 and I don't think The

119
00:08:14,985 --> 00:08:18,945
Speaker 6:  S 20 fives. It was like thick phone. Thick phone, slim

120
00:08:18,945 --> 00:08:22,825
Speaker 6:  phone for comparison. And yeah, I think that's

121
00:08:22,825 --> 00:08:26,705
Speaker 6:  the whole vibe. It's just a, a phone that's slimmer and they're like,

122
00:08:26,875 --> 00:08:27,745
Speaker 6:  isn't that cool?

123
00:08:32,115 --> 00:08:32,975
Speaker 6:  So we'll see. Sure.

124
00:08:32,995 --> 00:08:35,415
Speaker 5:  Can I throw a thing at you that might be a bit of a problem for Samsung?

125
00:08:36,135 --> 00:08:39,575
Speaker 5:  I didn't clock that. These three things were different thicknesses. Like

126
00:08:39,575 --> 00:08:42,735
Speaker 5:  even even looking at them in the video and even in the pictures now, I thought

127
00:08:42,735 --> 00:08:43,975
Speaker 5:  they were all the same thing.

128
00:08:44,605 --> 00:08:44,895
Speaker 6:  Yeah.

129
00:08:45,675 --> 00:08:45,895
Speaker 1:  Wow.

130
00:08:47,485 --> 00:08:50,855
Speaker 1:  Okay. So we should just mention there's a, there's a big rumor

131
00:08:51,485 --> 00:08:54,615
Speaker 1:  that in September, apple will do an iPhone error

132
00:08:55,355 --> 00:08:59,325
Speaker 1:  that will be very thin. Do we think that this is Samsung

133
00:08:59,715 --> 00:09:03,445
Speaker 1:  just showing off like vaporware to get ahead of Apple, which is a, you know,

134
00:09:03,445 --> 00:09:07,285
Speaker 1:  thing people believe, or is it just something about the modern

135
00:09:07,375 --> 00:09:10,885
Speaker 1:  smartphone supply chain is like, you can make thin phones now,

136
00:09:11,425 --> 00:09:13,285
Speaker 1:  so everyone's just gonna do that. It

137
00:09:13,285 --> 00:09:16,885
Speaker 6:  Feels like a little bit of both. I think Samsung absolutely

138
00:09:17,185 --> 00:09:20,965
Speaker 6:  is always thinking about Apple and they're like, we can sneak

139
00:09:20,965 --> 00:09:24,765
Speaker 6:  this one in ahead of whatever, you know,

140
00:09:25,025 --> 00:09:28,205
Speaker 6:  Tim Cook is gonna do. I do think it's a supply chain

141
00:09:28,775 --> 00:09:32,685
Speaker 6:  thing of like, well we can do this now. So this is what we have.

142
00:09:33,165 --> 00:09:37,045
Speaker 6:  I I think it's interesting that they, they call it an S 25,

143
00:09:37,385 --> 00:09:41,285
Speaker 6:  so it should ship this year in 2025. You

144
00:09:41,285 --> 00:09:45,085
Speaker 6:  would, you would presume. But that's, yeah, that's all we got.

145
00:09:45,565 --> 00:09:49,325
Speaker 1:  What year did Zoolander come out? I feel like we're in on the Zoolander curve.

146
00:09:49,685 --> 00:09:50,245
Speaker 6:  Oh no,

147
00:09:50,785 --> 00:09:54,445
Speaker 1:  Do Zoolander is 2001. So we're, we're we're exactly

148
00:09:54,445 --> 00:09:57,725
Speaker 1:  25 years later and we don't remember that there was a movie

149
00:09:58,305 --> 00:10:02,085
Speaker 1:  in which one of the main jokes is that phones were super tiny and so now

150
00:10:02,085 --> 00:10:04,605
Speaker 1:  we're just gonna do, we're gonna do marketing that's like, this one's really

151
00:10:04,605 --> 00:10:08,405
Speaker 1:  thin and it's, we're gonna be like, yeah, does it bend a lot like that? The

152
00:10:08,405 --> 00:10:10,285
Speaker 1:  iPhone six was really thin and it bent a lot.

153
00:10:10,435 --> 00:10:11,205
Speaker 6:  Yeah. Yeah.

154
00:10:11,205 --> 00:10:12,925
Speaker 1:  That's Did they say anything about durability?

155
00:10:13,315 --> 00:10:17,125
Speaker 6:  They said they did not say one word about this phone except play

156
00:10:17,125 --> 00:10:20,685
Speaker 6:  like a sizzle reel of, of some like metal

157
00:10:20,985 --> 00:10:24,645
Speaker 6:  pieces coming together. We really just coasted on vibes,

158
00:10:25,465 --> 00:10:28,445
Speaker 1:  But the vibes were high. Like I, that was the thing that really struck, I

159
00:10:28,445 --> 00:10:30,925
Speaker 1:  watched your video, you're like, this is, people are stoked about this phone.

160
00:10:30,925 --> 00:10:34,005
Speaker 1:  Yeah. Did anybody say why while you were there?

161
00:10:34,445 --> 00:10:35,885
Speaker 6:  I think it really was from, were

162
00:10:35,885 --> 00:10:37,725
Speaker 1:  They just like, it was a thing to take a picture of

163
00:10:38,025 --> 00:10:41,445
Speaker 6:  It? I think it was a thing to take a picture of. We were all kind of starved

164
00:10:41,445 --> 00:10:44,925
Speaker 6:  for like, I think in the pres end of the presentation was like,

165
00:10:45,225 --> 00:10:49,005
Speaker 6:  is this it? Like these s 25 phones, like

166
00:10:49,595 --> 00:10:52,885
Speaker 6:  they're not much to write home about. And then it was like,

167
00:10:53,625 --> 00:10:57,525
Speaker 6:  no, here's this other thing you can all take pictures of. And it was so crowded.

168
00:10:57,525 --> 00:11:01,365
Speaker 6:  Like they had security for the thin phone and they were like pushing us

169
00:11:01,365 --> 00:11:05,205
Speaker 6:  back. I saw Dieter, he was like trying

170
00:11:05,205 --> 00:11:09,085
Speaker 6:  to get in line too and he just gave up. I was like, yeah,

171
00:11:09,155 --> 00:11:09,605
Speaker 6:  this is,

172
00:11:11,555 --> 00:11:12,525
Speaker 6:  this is old school.

173
00:11:13,575 --> 00:11:16,725
Speaker 1:  Lemme just ask you for a vibe comparison. Unscientific,

174
00:11:17,315 --> 00:11:21,205
Speaker 1:  they had the thin phone and then they had the new headset. The

175
00:11:21,455 --> 00:11:25,325
Speaker 1:  Wuhan. Yeah. Which is not a great name, but it's their headset,

176
00:11:25,325 --> 00:11:29,285
Speaker 1:  right? It's their Apple Vision Pro competitor with Google, like

177
00:11:29,565 --> 00:11:31,165
Speaker 1:  embedded, they're gonna do a whole thing.

178
00:11:32,895 --> 00:11:34,265
Speaker 1:  What were people more excited about?

179
00:11:35,065 --> 00:11:38,825
Speaker 6:  A hundred percent. The skinny phone. It was like, not

180
00:11:38,825 --> 00:11:42,785
Speaker 6:  even, it wasn't even hard to, it wasn't even hard to get up and

181
00:11:42,785 --> 00:11:45,905
Speaker 6:  take a picture of the headset and there were just a lot of people kind of

182
00:11:46,065 --> 00:11:49,745
Speaker 6:  standing around going, oh, it looks like a vision pro. And

183
00:11:49,805 --> 00:11:53,765
Speaker 6:  you're kinda like, yeah. And we, same rules. We couldn't

184
00:11:54,085 --> 00:11:56,885
Speaker 6:  approach it. We couldn't touch it. You would get yelled at If you moved your

185
00:11:56,885 --> 00:12:00,805
Speaker 6:  camera too close to it. But yeah, the, the slim phone,

186
00:12:01,265 --> 00:12:03,285
Speaker 6:  the Edge won the day. I think

187
00:12:03,825 --> 00:12:07,125
Speaker 1:  It says a lot. I I would just say that that might be the entire story of

188
00:12:07,125 --> 00:12:10,565
Speaker 1:  the tech industry is they are desperate to get a computer on your face

189
00:12:11,145 --> 00:12:15,125
Speaker 1:  and then you show up with like skinny phone and everyone rushes to skinny

190
00:12:15,125 --> 00:12:15,325
Speaker 1:  phone.

191
00:12:15,375 --> 00:12:16,765
Speaker 6:  We're like, yes, this is it.

192
00:12:17,185 --> 00:12:20,925
Speaker 5:  We talked a little bit about the, the iPhone air on the

193
00:12:21,155 --> 00:12:22,205
Speaker 5:  show last Friday.

194
00:12:23,805 --> 00:12:27,205
Speaker 5:  I continue to think there is nothing particularly interesting or compelling

195
00:12:27,205 --> 00:12:30,125
Speaker 5:  about like an eight tenths of a millimeter thinner phone.

196
00:12:31,145 --> 00:12:34,805
Speaker 5:  But you use all of these phones all the time. You, you, you can sense eight

197
00:12:34,805 --> 00:12:38,685
Speaker 5:  tenths of a mil millimeter in a way that very few can Is this exciting?

198
00:12:38,715 --> 00:12:42,285
Speaker 5:  Like is the idea if 2025 is going to be the year all of our phones get thinner,

199
00:12:42,285 --> 00:12:45,565
Speaker 5:  is that like a good cool, exciting thing? Are we happy about this? I'm,

200
00:12:45,665 --> 00:12:49,005
Speaker 6:  I'm not entirely sure. The thing I come back to is

201
00:12:49,665 --> 00:12:53,525
Speaker 6:  the Pixel nine Pro fold, like the second pixel fold

202
00:12:53,865 --> 00:12:57,165
Speaker 6:  was one of those things you read about on paper and they were like, we changed

203
00:12:57,165 --> 00:12:57,885
Speaker 6:  the dimensions.

204
00:12:59,485 --> 00:13:02,925
Speaker 6:  I was like, sure, sure. And I went and picked it up at the event and

205
00:13:02,925 --> 00:13:06,685
Speaker 6:  immediately was like, oh no, this is right. Like this feels so much

206
00:13:06,885 --> 00:13:10,205
Speaker 6:  better, much better. I don't know if a thin phone is gonna have that same

207
00:13:10,205 --> 00:13:13,725
Speaker 6:  kind of like, you pick it up and use it and you're like, oh, this is

208
00:13:13,725 --> 00:13:17,525
Speaker 6:  different. Like it, I don't know. I'm not, I'm

209
00:13:17,525 --> 00:13:21,485
Speaker 6:  not like committing one way or the other. I'll reserve some,

210
00:13:22,035 --> 00:13:25,685
Speaker 6:  some hope. I guess I'm, I'm mostly

211
00:13:26,205 --> 00:13:29,845
Speaker 6:  skeptical, but I think it, it'll be a, a thing. We have to,

212
00:13:30,265 --> 00:13:33,965
Speaker 6:  to pick up the phone and live with it and see if it, it cracks in half If

213
00:13:33,965 --> 00:13:35,525
Speaker 6:  you sit in a sit on it.

214
00:13:36,805 --> 00:13:40,455
Speaker 1:  Yeah. All right. We will let you get back in the car. Alright. You flew,

215
00:13:40,565 --> 00:13:44,295
Speaker 1:  made you pull over. Thank you so much for joining us Alison. I, I

216
00:13:44,925 --> 00:13:48,255
Speaker 1:  eagerly await the th the thin phone review. Oh,

217
00:13:48,755 --> 00:13:50,215
Speaker 6:  Me too. Thanks for having me.

218
00:13:51,745 --> 00:13:54,695
Speaker 1:  Right as we were hanging up in Allison there, the police came.

219
00:13:55,555 --> 00:13:58,095
Speaker 1:  So this is what you do for podcasting.

220
00:13:58,095 --> 00:14:00,935
Speaker 5:  This is the last time anyone will hear from Allison on The Vergecast. This

221
00:14:00,935 --> 00:14:04,855
Speaker 1:  Is how devoted Allison is. The Vergecast, I feel like left unsaid

222
00:14:04,855 --> 00:14:07,655
Speaker 1:  in that conversation is the fact that the other phones are tremendously boring.

223
00:14:08,045 --> 00:14:11,775
Speaker 5:  It's a weird year. So one of the things about unpacked is that

224
00:14:12,265 --> 00:14:15,815
Speaker 5:  every year Samsung shows up to unpacked and launches what are basically

225
00:14:16,175 --> 00:14:19,095
Speaker 5:  guaranteed to be the bestselling Android phones of the year. Yeah. Every

226
00:14:19,095 --> 00:14:21,135
Speaker 5:  year. It's just, it just is what it is. It is clockwork.

227
00:14:23,045 --> 00:14:26,655
Speaker 5:  This is maybe the least interesting phone announcement

228
00:14:27,175 --> 00:14:31,135
Speaker 5:  I can remember, period. Like for all the

229
00:14:31,135 --> 00:14:35,055
Speaker 5:  pomp and circumstance, these phones literally aren't

230
00:14:35,295 --> 00:14:39,175
Speaker 5:  upgrades. Like they, the, so there's The S 25, The

231
00:14:39,255 --> 00:14:43,095
Speaker 5:  S 25 plus and The S 25 Ultra. And they are in,

232
00:14:43,235 --> 00:14:47,175
Speaker 5:  in every meaningful and meaningless way, the same as

233
00:14:47,175 --> 00:14:51,015
Speaker 5:  The S 24 with a slightly newer revved chip

234
00:14:51,235 --> 00:14:55,035
Speaker 5:  from Qualcomm. They bumped up the ram a little bit in,

235
00:14:55,095 --> 00:14:58,355
Speaker 5:  in, I think all of them, but at least a couple of them. The prices are the

236
00:14:58,355 --> 00:15:01,635
Speaker 5:  same. The cameras are the same except for one camera on the Ultra

237
00:15:02,415 --> 00:15:05,755
Speaker 5:  the, they look the same. Like they just shipped the same phones again. Yeah.

238
00:15:05,755 --> 00:15:09,155
Speaker 5:  And I'm like, I'm sort of blown away by that fact and

239
00:15:09,835 --> 00:15:12,595
Speaker 5:  e especially, you know, they had this big arena event, they talked a ton

240
00:15:12,595 --> 00:15:16,035
Speaker 5:  about AI and then like, oh, we're actually rolling all that stuff back to

241
00:15:16,035 --> 00:15:19,955
Speaker 5:  The S 20 fours again. And so part of me is like, why ship

242
00:15:19,975 --> 00:15:23,835
Speaker 5:  new phones? Just have a software event. Just, just be like AI

243
00:15:23,835 --> 00:15:27,155
Speaker 5:  and be like, oh, we are the first company ever who made our old phones,

244
00:15:27,695 --> 00:15:31,675
Speaker 5:  new phones with ai. Like that would've made more sense to me

245
00:15:31,675 --> 00:15:33,755
Speaker 5:  than we just made more of these with a new name

246
00:15:34,215 --> 00:15:37,635
Speaker 1:  Or, or lower the price than that's 24. We're one year into this cycle.

247
00:15:38,165 --> 00:15:41,115
Speaker 1:  We're actually do, taking the radical step of making their phones a little

248
00:15:41,115 --> 00:15:43,795
Speaker 1:  bit cheaper and adding more software to them. Now I know why they can't do

249
00:15:43,795 --> 00:15:47,715
Speaker 1:  it. There are carrier executives who are furiously sending me emails

250
00:15:47,715 --> 00:15:51,235
Speaker 1:  right now. Yeah. Because the carriers need the full spectrum

251
00:15:51,855 --> 00:15:55,835
Speaker 1:  of all the price points from all their partners. And Samsung maybe more than

252
00:15:55,835 --> 00:15:59,755
Speaker 1:  any company, just does what the carriers want. Actually, maybe Motorola

253
00:15:59,755 --> 00:16:03,275
Speaker 1:  does it more like Motorola made the phone for like the dish network 5G. True.

254
00:16:04,135 --> 00:16:07,555
Speaker 1:  But like Samsung has to be on a cadence of all these, like

255
00:16:07,705 --> 00:16:11,355
Speaker 1:  there's all this machinery that is built around the

256
00:16:11,355 --> 00:16:14,795
Speaker 1:  notion of a new phone coming out every year. Yes. We're just at the point

257
00:16:14,795 --> 00:16:18,525
Speaker 1:  where the phone doesn't matter to the machinery. Like

258
00:16:18,865 --> 00:16:21,845
Speaker 1:  to just unlock the T-Mobile's marketing spend for Q4,

259
00:16:22,715 --> 00:16:24,685
Speaker 1:  something has to be called a new phone.

260
00:16:25,275 --> 00:16:28,605
Speaker 5:  Yeah, I think that's right. It just has to be a thing. You have to, it doesn't

261
00:16:28,605 --> 00:16:32,005
Speaker 5:  it and it matters much less what that thing is, or honestly in

262
00:16:32,005 --> 00:16:35,765
Speaker 5:  Samsung's case, if it's any good, like it, it's the same thing as the

263
00:16:35,765 --> 00:16:39,645
Speaker 5:  iPhone in so many ways. Like people who have iPhones at

264
00:16:39,645 --> 00:16:43,005
Speaker 5:  this point will buy the newest iPhone when their current iPhone

265
00:16:43,695 --> 00:16:47,565
Speaker 5:  stops being viable. That is the upgrade path, right? You almost

266
00:16:47,565 --> 00:16:51,405
Speaker 5:  everybody gets the latest iPhone uses it until they

267
00:16:51,405 --> 00:16:54,525
Speaker 5:  can't anymore and then gets the latest iPhone. And so if you're, if you're

268
00:16:54,525 --> 00:16:57,605
Speaker 5:  Samsung, that is, it's the same path and it, it has I think

269
00:16:58,515 --> 00:17:02,485
Speaker 5:  made phones boring, all of them. But

270
00:17:02,515 --> 00:17:06,485
Speaker 5:  this just felt like a real, I don't know, low moment. Or maybe

271
00:17:06,485 --> 00:17:09,125
Speaker 5:  just like saying the quiet part loud of like, yeah, there's nothing left

272
00:17:09,125 --> 00:17:11,965
Speaker 5:  to do here anymore. So here's the same phone again.

273
00:17:12,195 --> 00:17:15,805
Speaker 1:  It's thinner now. Yeah, yeah. Even like the YouTubers who are have a lot

274
00:17:15,805 --> 00:17:19,365
Speaker 1:  of incentives to pretend thing. Nothing. Yeah. Just total flat across the

275
00:17:19,365 --> 00:17:23,285
Speaker 1:  board. The thing that really struck me about all of this is the

276
00:17:23,425 --> 00:17:26,645
Speaker 1:  AI of it. You wrote about Gemini sort of being the winner in the assistant

277
00:17:26,645 --> 00:17:29,965
Speaker 1:  wars because it's just bundled in the Samsung phones now. But I was watching

278
00:17:30,065 --> 00:17:33,805
Speaker 1:  the, the demos of ai, like we're year two

279
00:17:33,955 --> 00:17:37,405
Speaker 1:  into it, essentially. Like here's the AI stuff you can do with our phone

280
00:17:38,415 --> 00:17:41,325
Speaker 1:  maybe year 10 If you, you know, count Google for years.

281
00:17:42,515 --> 00:17:45,405
Speaker 1:  Like not shipping it, it's talking about it. It's, yeah, but it's like year

282
00:17:45,405 --> 00:17:49,245
Speaker 1:  two of like we're shipping it on phones, right? And we're sort of past now

283
00:17:49,245 --> 00:17:52,885
Speaker 1:  the Apple intelligence moment where I think the newest version of the software,

284
00:17:53,105 --> 00:17:55,805
Speaker 1:  the newest version of iOS that's in release candidate right now, it's on

285
00:17:55,805 --> 00:17:59,445
Speaker 1:  by default. So we've like, we've hit the point, right? It's, this is just

286
00:17:59,445 --> 00:18:03,125
Speaker 1:  part of phones now and I'm watching the demos and there are no new ideas.

287
00:18:04,015 --> 00:18:04,235
Speaker 5:  No.

288
00:18:04,745 --> 00:18:05,035
Speaker 1:  Like

289
00:18:05,435 --> 00:18:07,395
Speaker 5:  Everyone has the same like four ideas,

290
00:18:08,285 --> 00:18:11,115
Speaker 1:  Right? They they did the, you can search your photos, which is very smart.

291
00:18:11,255 --> 00:18:15,235
Speaker 1:  I'm very excited about that one. Great idea. Yep. But they spent

292
00:18:15,275 --> 00:18:19,075
Speaker 1:  a long time being like, so what happens is you say a

293
00:18:19,075 --> 00:18:22,275
Speaker 1:  name, location, person and clothing and then the AI

294
00:18:22,985 --> 00:18:26,915
Speaker 1:  understands you and it's like, yeah, we, people have fallen in love with

295
00:18:26,975 --> 00:18:27,395
Speaker 1:  robots,

296
00:18:27,715 --> 00:18:29,795
Speaker 5:  Right? Yeah. Like we got that one pretty covered at

297
00:18:29,795 --> 00:18:31,555
Speaker 1:  This point. We're like, we're really good on that

298
00:18:31,795 --> 00:18:34,675
Speaker 5:  Actually. Yeah. People are doing a lot of illegal stuff with those things.

299
00:18:34,675 --> 00:18:36,315
Speaker 5:  Like we, we know, we understand. Yeah.

300
00:18:36,985 --> 00:18:40,915
Speaker 1:  Then there's the, like you can change a setting by just

301
00:18:40,915 --> 00:18:44,875
Speaker 1:  talking, which Samsung to its credit dog

302
00:18:44,875 --> 00:18:48,315
Speaker 1:  with shoes, Bixby, that was the whole point of Bixby, right?

303
00:18:49,075 --> 00:18:52,655
Speaker 1:  And no one cared. So like getting rid of it and then doing it again with

304
00:18:52,655 --> 00:18:56,265
Speaker 1:  Gemini. I don't, but you can do it now. Maybe it'll work better

305
00:18:56,725 --> 00:18:59,905
Speaker 1:  if it works better. Maybe maybe people will use it. But that's the other

306
00:18:59,905 --> 00:19:03,025
Speaker 1:  demo. And then everything else is kind of the same. Like it does

307
00:19:03,345 --> 00:19:06,465
Speaker 1:  transcriptions, it can record phone calls. Like

308
00:19:07,685 --> 00:19:11,065
Speaker 1:  we can just process language a little bit better than we could before. Right?

309
00:19:11,285 --> 00:19:15,265
Speaker 1:  Did you catch anything that was like, oh, this is a step change because we

310
00:19:15,265 --> 00:19:15,985
Speaker 1:  have this capability.

311
00:19:16,525 --> 00:19:20,105
Speaker 5:  The one thing that feels meaningfully new

312
00:19:20,565 --> 00:19:24,185
Speaker 5:  is this multi-step, multi app thing that

313
00:19:24,205 --> 00:19:27,985
Speaker 5:  Gemini can now do where it, it can, it can essentially

314
00:19:29,405 --> 00:19:33,185
Speaker 5:  direct actions to multiple apps at a time. It can just do several things

315
00:19:33,495 --> 00:19:37,305
Speaker 5:  with one prompt. It doesn't work very well because

316
00:19:37,325 --> 00:19:39,825
Speaker 5:  of course it doesn't, I'll give you an example. So right before we're getting

317
00:19:39,825 --> 00:19:43,745
Speaker 5:  on here, one thing I think Gemini in particular

318
00:19:43,805 --> 00:19:47,625
Speaker 5:  is actually very good at is as a YouTube research tool

319
00:19:48,295 --> 00:19:51,305
Speaker 5:  because it, it has an extension into YouTube and you can just say like, find

320
00:19:51,325 --> 00:19:55,185
Speaker 5:  me popular YouTube videos about whatever. And it,

321
00:19:55,185 --> 00:19:57,465
Speaker 5:  it does a pretty good job of that in a way that like regular YouTube search

322
00:19:57,625 --> 00:20:01,065
Speaker 5:  actually doesn't. So I use it for that all the time when I'm just like, I

323
00:20:01,065 --> 00:20:04,665
Speaker 5:  wanna watch a bunch of videos about how GPUs work. Gemini is actually very

324
00:20:04,665 --> 00:20:08,585
Speaker 5:  good for that. So I was like, okay, cool. Two step thing I'm gonna

325
00:20:08,645 --> 00:20:12,465
Speaker 5:  say Gemini, go find a bunch of videos about

326
00:20:12,885 --> 00:20:16,825
Speaker 5:  how GPUs work on YouTube and then put them all into a note in

327
00:20:16,825 --> 00:20:20,765
Speaker 5:  Google. Keep two steps, right? That's the thing that this stuff

328
00:20:20,765 --> 00:20:23,245
Speaker 5:  is supposed to be able to do now. And it plugs into WhatsApp. So you can

329
00:20:23,245 --> 00:20:27,165
Speaker 5:  be like, find me a restaurant to go to and then, and then text it to Anna.

330
00:20:27,425 --> 00:20:30,005
Speaker 5:  That's where we're going tonight. This is like, these are the kinds of things

331
00:20:30,005 --> 00:20:32,725
Speaker 5:  they're starting to put together. And again, this is the idea everybody has.

332
00:20:32,875 --> 00:20:36,765
Speaker 5:  Yeah, this is open AI's operator, this is series app intents. Like this is

333
00:20:36,865 --> 00:20:40,445
Speaker 5:  the agentic thing everybody is talking about. But anyway, So I do this in

334
00:20:40,445 --> 00:20:44,125
Speaker 5:  Gemini, the first thing it does is pull back five titles of videos

335
00:20:44,225 --> 00:20:46,805
Speaker 5:  and it's like, do you wanna save these to keep? And I go, sure, sounds great.

336
00:20:47,165 --> 00:20:50,625
Speaker 5:  It says Okay, I've created your notes. The notes it created were the titles

337
00:20:50,625 --> 00:20:54,585
Speaker 5:  of the videos and nothing else. Five individual Google

338
00:20:54,585 --> 00:20:56,385
Speaker 5:  keep notes with just titles.

339
00:20:56,495 --> 00:20:58,785
Speaker 1:  Wait, wait, you made individual notes, you made

340
00:20:58,785 --> 00:21:01,905
Speaker 5:  Individual notes. So then I'm like, okay, let's try this a different way.

341
00:21:02,045 --> 00:21:05,905
Speaker 5:  So I go and I'm like, okay, can you just find me a bunch of YouTube videos

342
00:21:05,995 --> 00:21:09,865
Speaker 5:  about GPUs? And it's like, sure. And I was like, okay, next step.

343
00:21:09,925 --> 00:21:13,145
Speaker 5:  So we're already out of the interesting multi-step thing here. And I'm like,

344
00:21:13,145 --> 00:21:17,085
Speaker 5:  what if we just do the simple thing? Great, you found all these videos looks

345
00:21:17,085 --> 00:21:20,885
Speaker 5:  good. Can you put all that into a note for me in Google

346
00:21:20,885 --> 00:21:24,405
Speaker 5:  Keep? And it's like, sure. Done. So now what it does is create a list with

347
00:21:24,405 --> 00:21:28,365
Speaker 5:  a title like videos about how GPUs work and five empty bullets.

348
00:21:29,715 --> 00:21:33,405
Speaker 5:  Like we've done nothing here. And I think like

349
00:21:33,745 --> 00:21:37,525
Speaker 5:  the, the, the point I keep making and I, I said a snarky thing about

350
00:21:37,525 --> 00:21:41,165
Speaker 5:  Apple intelligence on threads a few days ago that was like, I can't wait

351
00:21:41,165 --> 00:21:44,885
Speaker 5:  till iOS 21 when the only feature is getting rid of Apple intelligence and

352
00:21:44,885 --> 00:21:47,685
Speaker 5:  it's the best upgrade in the years. And, and people like raped me over the

353
00:21:47,685 --> 00:21:51,365
Speaker 5:  calls for it and I just, I keep coming back to none of this is actually useful

354
00:21:52,065 --> 00:21:55,445
Speaker 5:  for much of anything. Yeah. And even the things that are useful aren't useful

355
00:21:55,685 --> 00:21:59,245
Speaker 5:  reliably enough to be worth trying to use. Like the, the

356
00:21:59,245 --> 00:22:03,165
Speaker 5:  Samsung Bixby thing was a good idea, right?

357
00:22:03,165 --> 00:22:06,805
Speaker 5:  The idea of like, most people don't know how to find the Bluetooth switch

358
00:22:06,945 --> 00:22:10,885
Speaker 5:  on their phone. And so to be able to just ask my phone how to turn on Bluetooth

359
00:22:10,905 --> 00:22:14,765
Speaker 5:  is like a good and useful idea and it just didn't work enough.

360
00:22:15,025 --> 00:22:18,605
Speaker 5:  So people would just go find Bluetooth on their phone and If you can't, If

361
00:22:18,605 --> 00:22:22,565
Speaker 5:  you can't beat the reliability of swiping up through

362
00:22:22,565 --> 00:22:26,405
Speaker 5:  a settings menu until I find the thing labeled Bluetooth, you have nothing.

363
00:22:26,465 --> 00:22:30,325
Speaker 5:  And we're now 10 years into that problem and I don't think we have meaningfully

364
00:22:30,545 --> 00:22:34,525
Speaker 5:  solved it. So like this, this thing Google keeps demoing, which is like

365
00:22:34,545 --> 00:22:38,205
Speaker 5:  the, the multi-step multimodal multi action stuff is

366
00:22:38,445 --> 00:22:42,365
Speaker 5:  like, it's cool. It's the, the idea is very cool. It feels like the right

367
00:22:42,365 --> 00:22:45,725
Speaker 5:  thing. My phone can just sort of take in all the information that I need

368
00:22:45,725 --> 00:22:48,885
Speaker 5:  it to and then give it back to me in ways that are useful and natural. That

369
00:22:48,885 --> 00:22:52,845
Speaker 5:  all makes sense. It just doesn't work and I'm so stuck on it. None

370
00:22:52,845 --> 00:22:56,645
Speaker 5:  of this matters until it works and we're just not there yet.

371
00:22:57,275 --> 00:23:00,645
Speaker 1:  Yeah. The piece of the puzzle that I think just really alludes me

372
00:23:01,585 --> 00:23:01,805
Speaker 1:  is

373
00:23:03,595 --> 00:23:07,275
Speaker 1:  I like, I do know how to turn Bluetooth on on my phone. Like I'm

374
00:23:07,275 --> 00:23:10,795
Speaker 1:  cursed with this knowledge as, as is are most of our listeners.

375
00:23:11,575 --> 00:23:15,355
Speaker 1:  And it is so much faster for me to learn how to use the

376
00:23:15,355 --> 00:23:19,275
Speaker 1:  thing than to fight the other tool that will do it for me

377
00:23:19,875 --> 00:23:20,435
Speaker 5:  A hundred percent.

378
00:23:20,815 --> 00:23:24,395
Speaker 1:  And maybe that, and like I understand that that again, I think The

379
00:23:24,515 --> 00:23:27,915
Speaker 1:  Vergecast audience and The Verge staff is like a uniquely

380
00:23:28,315 --> 00:23:32,115
Speaker 1:  positioned group of people in that we also all like figuring

381
00:23:32,175 --> 00:23:36,075
Speaker 1:  it out and most people would just like to move on with her day. But I,

382
00:23:36,945 --> 00:23:40,485
Speaker 1:  the danger is you're gonna bounce off of it doing it badly

383
00:23:40,945 --> 00:23:44,805
Speaker 1:  and then you will never try again. Right. Which is with Siri and Alexa

384
00:23:45,145 --> 00:23:48,165
Speaker 1:  in history, why they're perceived to be bad products.

385
00:23:48,265 --> 00:23:52,245
Speaker 5:  Yep. And I think these companies have estimate underestimated how high

386
00:23:52,275 --> 00:23:56,085
Speaker 5:  that bar is for people of like how quickly

387
00:23:56,225 --> 00:23:59,605
Speaker 5:  it will feel like I'm fighting this thing and I will just go back to the

388
00:24:00,095 --> 00:24:04,005
Speaker 5:  definitely less efficient and hacker way that I can figure

389
00:24:04,005 --> 00:24:04,605
Speaker 5:  out how to do.

390
00:24:04,955 --> 00:24:07,965
Speaker 1:  Okay, so here's the flip side of that, which I have been thinking about a

391
00:24:07,965 --> 00:24:11,885
Speaker 1:  lot. There is the section of the

392
00:24:12,085 --> 00:24:14,325
Speaker 1:  audience that's like, please shut up about ai. We don't care about it. We

393
00:24:14,325 --> 00:24:18,085
Speaker 1:  well, I I can hear you already. You're screaming at me and coming back

394
00:24:18,085 --> 00:24:21,525
Speaker 1:  through the internet as you listen to this, I, it resonates in my heart.

395
00:24:22,525 --> 00:24:25,085
Speaker 1:  I hear you. It's the same group of people that whenever we write about semi

396
00:24:25,185 --> 00:24:28,805
Speaker 1:  ai feature on The Verge dot com, they, they're, you're in our comments. You're

397
00:24:28,805 --> 00:24:31,405
Speaker 1:  like, I hate this. Make it go away. I just wanna turn off Apple intelligence.

398
00:24:31,465 --> 00:24:35,165
Speaker 1:  I'm there with you. It's all fraud. Great. Then

399
00:24:35,165 --> 00:24:38,965
Speaker 1:  there's the people who ferociously read everything we write about ai.

400
00:24:40,995 --> 00:24:44,565
Speaker 1:  Dude, they're out there, they, they love it. And then there's the companies

401
00:24:45,105 --> 00:24:48,965
Speaker 1:  who are all telling us that their own usage statistics

402
00:24:49,235 --> 00:24:52,805
Speaker 1:  suggest people are using the hell outta these tools. Right? So the example

403
00:24:52,805 --> 00:24:56,525
Speaker 1:  I'll give, which is old now, but it's just the one that is in my brain all

404
00:24:56,525 --> 00:24:56,765
Speaker 1:  the time.

405
00:24:58,865 --> 00:25:02,845
Speaker 1:  The CEO of Adobe Han Ryan came on decoder and you can go listen

406
00:25:02,845 --> 00:25:06,645
Speaker 1:  to it. He's like, the usage of generative fill in Photoshop is as high

407
00:25:06,645 --> 00:25:10,325
Speaker 1:  as layers. Which is basically I opened Photoshop.

408
00:25:10,475 --> 00:25:12,605
Speaker 5:  Yeah. That's what Photoshop is for the

409
00:25:12,655 --> 00:25:15,445
Speaker 1:  Right. Like that's where he is at. He's like, the take rate on generative

410
00:25:15,445 --> 00:25:19,325
Speaker 1:  fill is as high as layers. So yes, everyone's yelling, everyone's

411
00:25:19,325 --> 00:25:22,045
Speaker 1:  mad, everyone hates it. And then everyone's just using the hell out of it.

412
00:25:22,265 --> 00:25:26,005
Speaker 1:  We can see it on our site. People reading about it like crazy. And then you,

413
00:25:26,405 --> 00:25:29,525
Speaker 1:  I wanna bring this to Gemini specifically on these phones. Gemini's gonna

414
00:25:29,525 --> 00:25:32,205
Speaker 1:  be the default on the Galaxy phones. Samsung's not gonna do its own thing.

415
00:25:32,505 --> 00:25:35,845
Speaker 1:  And Google's like, Gemini is more popular than Chatt now 'cause it's just

416
00:25:35,845 --> 00:25:39,005
Speaker 1:  there and people are using it. Right. And I dunno how to square that with,

417
00:25:39,515 --> 00:25:43,285
Speaker 1:  it's not very good. Like most of the time it's not very good. The cha

418
00:25:43,385 --> 00:25:45,965
Speaker 1:  pt the day we're recording this on, on Thursday, it was down this morning.

419
00:25:46,505 --> 00:25:49,765
Speaker 1:  And there are people being like, I'm not getting any work done because I,

420
00:25:49,765 --> 00:25:53,005
Speaker 1:  I've just like rely on this so much. And I'm like, well you're, I hope your

421
00:25:53,005 --> 00:25:56,765
Speaker 1:  work is good. But like you can see it's, it's, it's wound

422
00:25:56,865 --> 00:26:00,565
Speaker 1:  its way into so many different places. Even though I think

423
00:26:00,705 --> 00:26:04,605
Speaker 1:  the core criticism, boy you hit a wall of this not being very good

424
00:26:04,825 --> 00:26:05,725
Speaker 1:  is still very real.

425
00:26:06,555 --> 00:26:09,565
Speaker 5:  Yeah. I think like generative fill is actually a really interesting one because

426
00:26:09,565 --> 00:26:13,365
Speaker 5:  it's, it's the kind of feature that is ai but it's not

427
00:26:13,565 --> 00:26:17,485
Speaker 5:  in the way that we mostly talk about ai. Like everybody wants us to think

428
00:26:17,485 --> 00:26:21,245
Speaker 5:  about AI as these like, you know, all knowing all

429
00:26:21,245 --> 00:26:25,135
Speaker 5:  purpose omnipresent companions that can just do

430
00:26:25,135 --> 00:26:28,375
Speaker 5:  everything. I, I think that is like completely the wrong

431
00:26:29,435 --> 00:26:33,215
Speaker 5:  way. We should be developing these things like AI as a

432
00:26:33,215 --> 00:26:37,135
Speaker 5:  tool, which is basically a, a smarter thing to plug into

433
00:26:37,135 --> 00:26:40,895
Speaker 5:  generative fill makes a lot of sense. And so it totally tracks me this working.

434
00:26:40,895 --> 00:26:43,365
Speaker 5:  And I think even in chat gt, like I'm very

435
00:26:44,315 --> 00:26:47,925
Speaker 5:  compelled by the stories I hear about people who use it as like a force

436
00:26:48,245 --> 00:26:50,845
Speaker 5:  multiplier for writing code. I think that's real, even if it doesn't write

437
00:26:50,845 --> 00:26:54,685
Speaker 5:  very good code. Like I have talked to enough people who

438
00:26:54,685 --> 00:26:57,685
Speaker 5:  are like, it is so much faster for me to take

439
00:26:58,885 --> 00:27:02,845
Speaker 5:  middling code and make it good than it is to start

440
00:27:02,845 --> 00:27:06,445
Speaker 5:  from scratch and write the like incredibly basic stuff that is

441
00:27:06,805 --> 00:27:10,605
Speaker 5:  required in order to write good code. Like all good code

442
00:27:10,605 --> 00:27:13,645
Speaker 5:  involves lots of bad code and So I can do the bad code automatically and

443
00:27:13,645 --> 00:27:16,605
Speaker 5:  then I just have to do the good code. Like that kind of thing I think is,

444
00:27:16,665 --> 00:27:20,645
Speaker 5:  is really powerful. The problem is for like regular people

445
00:27:20,665 --> 00:27:24,365
Speaker 5:  in their day-to-day lives, these companies have not yet

446
00:27:24,685 --> 00:27:28,605
Speaker 5:  identified a ton of good versions of that. And this is

447
00:27:28,605 --> 00:27:31,445
Speaker 5:  why we keep landing on, like, it'll help you write email and summarize email.

448
00:27:31,705 --> 00:27:33,205
Speaker 5:  That's like, that's the only

449
00:27:34,025 --> 00:27:34,445
Speaker 1:  Normal

450
00:27:34,465 --> 00:27:37,845
Speaker 5:  Person use case any of these companies can think of for what this stuff can

451
00:27:37,845 --> 00:27:41,805
Speaker 5:  do. So I think while in the background, all this interesting stuff is

452
00:27:41,805 --> 00:27:44,765
Speaker 5:  happening in, in like back offices are being

453
00:27:45,485 --> 00:27:49,365
Speaker 5:  reinvented by AI that can do some of this work for them. All this

454
00:27:49,365 --> 00:27:52,645
Speaker 5:  stuff is happening, but the idea that like there's a button on my phone that

455
00:27:52,645 --> 00:27:55,285
Speaker 5:  I'm going to press and it's gonna do things for me a hundred times a day,

456
00:27:55,625 --> 00:27:59,405
Speaker 5:  we just haven't figured out yet. And we haven't, we never figured it out

457
00:27:59,405 --> 00:28:03,285
Speaker 5:  with Siri. We never figured out with Alexa. Like the take rate is high

458
00:28:03,875 --> 00:28:07,845
Speaker 5:  because it's so present. But I think the, the thing these

459
00:28:08,085 --> 00:28:11,125
Speaker 5:  companies know and have figured out and are pushing so hard towards is that

460
00:28:11,125 --> 00:28:14,965
Speaker 5:  If you don't stick it in front of people's face, they won't by and

461
00:28:14,965 --> 00:28:16,245
Speaker 5:  large go looking for it.

462
00:28:16,535 --> 00:28:19,605
Speaker 1:  Right. There's nothing compelling about it that makes you go be like, I need

463
00:28:19,605 --> 00:28:20,245
Speaker 1:  to get this app.

464
00:28:20,575 --> 00:28:20,925
Speaker 5:  Right.

465
00:28:21,285 --> 00:28:25,205
Speaker 1:  I I buy it Except chae is the most popular app,

466
00:28:25,885 --> 00:28:27,875
Speaker 1:  right? The fastest growing consumer product around.

467
00:28:29,035 --> 00:28:32,995
Speaker 5:  I I have, I have many theories about chat g pt, but I, I continue

468
00:28:33,215 --> 00:28:36,475
Speaker 5:  to think that the novelty of chat GPT

469
00:28:37,535 --> 00:28:40,755
Speaker 5:  is a huge part of its appeal. And that's, that's, that's not nothing but

470
00:28:40,755 --> 00:28:44,585
Speaker 5:  like we are still so deep in the,

471
00:28:44,585 --> 00:28:48,065
Speaker 5:  like it's wild that this thing works at all that I think

472
00:28:48,395 --> 00:28:52,365
Speaker 5:  we're not yet really having to reckon with the use cases here. Like

473
00:28:52,715 --> 00:28:56,165
Speaker 5:  chat, GT's search stuff doesn't really seem to be

474
00:28:56,355 --> 00:29:00,285
Speaker 5:  setting the universe on fire. This operator thing that they launched

475
00:29:00,745 --> 00:29:03,845
Speaker 5:  to the $200 a month folks today

476
00:29:05,235 --> 00:29:06,545
Speaker 5:  seems interesting. But like,

477
00:29:08,295 --> 00:29:11,695
Speaker 5:  I dunno, people like chat GBT in theory, I think more than they are spending

478
00:29:12,055 --> 00:29:15,735
Speaker 5:  a lot of like useful day-to-day phone time

479
00:29:15,925 --> 00:29:16,335
Speaker 5:  with it.

480
00:29:16,675 --> 00:29:20,355
Speaker 1:  All right. So that's my prompt to the audience. 'cause I, I

481
00:29:20,595 --> 00:29:23,435
Speaker 1:  disagree with you. Okay. I think we are just old guys who are good at writing.

482
00:29:24,585 --> 00:29:28,075
Speaker 1:  Like I Sure. I think, I think we are just like out of the audience we will

483
00:29:28,135 --> 00:29:31,995
Speaker 1:  do voice disguises If you don't wanna admit it, but call

484
00:29:32,155 --> 00:29:35,995
Speaker 1:  the hotline and and tell us how you're using chat v 'cause I'm

485
00:29:35,995 --> 00:29:39,875
Speaker 1:  very curious. I think there's, there's just a weird split here.

486
00:29:40,025 --> 00:29:43,165
Speaker 1:  There's like a very vocal minority. It's like screw this all, I hate it.

487
00:29:43,195 --> 00:29:46,205
Speaker 1:  There's a lot of people who are just using this stuff and then there's like,

488
00:29:46,255 --> 00:29:49,725
Speaker 1:  again, there's, there's sort of the middle. I think a lot of people are in

489
00:29:49,725 --> 00:29:51,885
Speaker 1:  the middle where you're indifferent 'cause it's not very good. Yeah.

490
00:29:51,905 --> 00:29:54,685
Speaker 5:  And I, but the caveat is, I, I don't want to hear stories about how it makes

491
00:29:54,685 --> 00:29:58,025
Speaker 5:  you better at your job because there, there are lots of those and I think

492
00:29:58,025 --> 00:30:00,825
Speaker 5:  that's, that's fine and good and all of that is really interesting. I'm talking

493
00:30:00,825 --> 00:30:04,465
Speaker 5:  about like stuff that you would press the side button on your phone to do

494
00:30:04,465 --> 00:30:06,865
Speaker 5:  with ai. Like that's, that's the stuff I want to know.

495
00:30:07,365 --> 00:30:11,025
Speaker 1:  All right. Like I'm said voices. We'll we will, we will protect you. The

496
00:30:11,105 --> 00:30:14,705
Speaker 1:  Verge has always protected its sources. And now in this time of

497
00:30:14,705 --> 00:30:17,745
Speaker 1:  increased threat against the First Amendment, I promise you, you can admit

498
00:30:17,745 --> 00:30:21,025
Speaker 1:  to us that you use ai. Yeah. And I'll protect you

499
00:30:21,045 --> 00:30:22,065
Speaker 5:  And we will love you anyway.

500
00:30:22,445 --> 00:30:25,705
Speaker 1:  We should break. But before we do, I just actually want, I I want you to

501
00:30:25,705 --> 00:30:28,225
Speaker 1:  talk about Gemini for one second. 'cause you wrote this piece about how it's

502
00:30:28,225 --> 00:30:31,865
Speaker 1:  won, which is a big claim, especially 'cause se like real

503
00:30:32,205 --> 00:30:35,985
Speaker 1:  ai Siri isn't here yet, but Google was behind and

504
00:30:36,045 --> 00:30:39,975
Speaker 1:  now they've, I mean they're the default on the most popular

505
00:30:40,005 --> 00:30:43,975
Speaker 1:  Android phone, which is a big deal. Do you think Gemini is good

506
00:30:43,995 --> 00:30:47,015
Speaker 1:  as an assistant or do you think it's just won by default of the placement?

507
00:30:47,415 --> 00:30:50,735
Speaker 5:  I think it won by default of the placement, but I actually don't think the

508
00:30:50,735 --> 00:30:54,535
Speaker 5:  difference between those two things is all that big. Like I I,

509
00:30:54,565 --> 00:30:58,455
Speaker 5:  I've been thinking so much in the last couple of weeks about the

510
00:30:58,455 --> 00:31:02,415
Speaker 5:  Google search trial, which essentially amounted to If you have

511
00:31:02,735 --> 00:31:06,255
Speaker 5:  a good enough product that people want to use it and you can put it in front

512
00:31:06,255 --> 00:31:10,135
Speaker 5:  of everybody. Like everybody, you almost immediately

513
00:31:10,135 --> 00:31:13,575
Speaker 5:  build an insurmountable lead. Because what you need is you need data on how

514
00:31:13,575 --> 00:31:15,975
Speaker 5:  people are using your product that you can use to make the product better,

515
00:31:15,975 --> 00:31:18,855
Speaker 5:  which makes more people use it, which means you get more data, which means

516
00:31:18,855 --> 00:31:22,615
Speaker 5:  more people like on and on and on. I mean, like, I, I will never forget

517
00:31:22,705 --> 00:31:26,135
Speaker 5:  Satya Nadella sitting up there being like, I will give Bing to Apple for

518
00:31:26,135 --> 00:31:29,935
Speaker 5:  free. Like they can just have it. We don't just

519
00:31:29,935 --> 00:31:33,895
Speaker 5:  take all, keep all the money. I just want the query data. Like

520
00:31:33,915 --> 00:31:36,975
Speaker 5:  that's real. And we're going through that with AI too. This stuff gets better

521
00:31:36,995 --> 00:31:40,895
Speaker 5:  the more people use it. And what Google has is a

522
00:31:41,845 --> 00:31:45,735
Speaker 5:  unstoppable distribution advantages because it has things like Chrome

523
00:31:45,765 --> 00:31:49,575
Speaker 5:  that it can just stick it in the address bar. It has things like a

524
00:31:49,575 --> 00:31:53,535
Speaker 5:  very popular Google app. It has Google Maps, it has Gmail. Like Google can

525
00:31:53,535 --> 00:31:57,495
Speaker 5:  both use Gemini underneath all of those things and get you

526
00:31:57,495 --> 00:32:01,335
Speaker 5:  to interact with Gemini on top of them. Then you make it the

527
00:32:01,335 --> 00:32:05,295
Speaker 5:  side button on essentially all of the Android phones that matter at

528
00:32:05,295 --> 00:32:08,935
Speaker 5:  this point. And you've built yet another place where you're

529
00:32:08,935 --> 00:32:12,695
Speaker 5:  essentially untouchable and then throw in the fact that

530
00:32:13,075 --> 00:32:16,375
Speaker 5:  Amazon just seems unable to get its shit together with this new Alexa. Like

531
00:32:16,375 --> 00:32:19,295
Speaker 5:  what, whatever that is going to be. It, it is, it does not seem like it's

532
00:32:19,295 --> 00:32:21,535
Speaker 5:  gonna be very good or ship anytime soon at this point.

533
00:32:23,525 --> 00:32:27,295
Speaker 5:  Siri, TBD. But nothing about the history of Siri suggests that it's going

534
00:32:27,295 --> 00:32:28,935
Speaker 5:  to be great. Nothing

535
00:32:28,955 --> 00:32:32,135
Speaker 1:  But the present of Apple intelligence suggests well right. That Siri

536
00:32:32,135 --> 00:32:34,815
Speaker 5:  Will be great. Yeah. There is nothing, there is nothing that leads me to

537
00:32:34,815 --> 00:32:38,615
Speaker 5:  believe there is some radical reinvention of that assistant coming anytime

538
00:32:38,615 --> 00:32:42,605
Speaker 5:  soon. So Google is in a place where it's, it's

539
00:32:42,625 --> 00:32:46,445
Speaker 5:  pretty good. I think Gemini by all accounts is at least in

540
00:32:46,455 --> 00:32:50,205
Speaker 5:  range of Chachi, BT and Claude, which I think have been

541
00:32:50,555 --> 00:32:53,565
Speaker 5:  kind of the, the two gold standards for a while now. And the difference between

542
00:32:53,565 --> 00:32:57,525
Speaker 5:  the two is like personal taste and some one's better at some things

543
00:32:57,565 --> 00:32:59,925
Speaker 5:  and the other's better at other things. But the thing neither of those have

544
00:32:59,925 --> 00:33:02,885
Speaker 5:  is the distribution. They don't have a gadget they can be part of, they don't

545
00:33:02,885 --> 00:33:06,285
Speaker 5:  have web browsers that they can be built into by default. Google has

546
00:33:06,925 --> 00:33:10,845
Speaker 5:  a pretty good product and every other advantage you could possibly want

547
00:33:10,985 --> 00:33:14,725
Speaker 5:  in this space and that that just feeds

548
00:33:14,725 --> 00:33:17,525
Speaker 5:  itself. And so we're hitting a point where Google is going to get this like

549
00:33:17,785 --> 00:33:21,525
Speaker 5:  escape velocity before anybody else can build themselves

550
00:33:21,665 --> 00:33:24,365
Speaker 5:  the things that they need that Google already has

551
00:33:24,705 --> 00:33:28,565
Speaker 1:  Or make a dollar from them. Like the other thing Google has is like a cash

552
00:33:28,565 --> 00:33:29,445
Speaker 1:  machine in search,

553
00:33:29,575 --> 00:33:32,885
Speaker 5:  Right? That is now just shoves Gemini into,

554
00:33:33,965 --> 00:33:37,765
Speaker 1:  I, this is gonna, we're gonna figure this out over the next four years.

555
00:33:37,825 --> 00:33:41,045
Speaker 1:  But this is where like the weirdness of the Trump administration just runs

556
00:33:41,045 --> 00:33:44,605
Speaker 1:  right into the reality of the tech products, especially in now in Trump too.

557
00:33:45,275 --> 00:33:49,245
Speaker 1:  Like Sam Altman's standing on stage of the White House, not stage, just around,

558
00:33:49,245 --> 00:33:50,765
Speaker 1:  they were just in a room I

559
00:33:50,765 --> 00:33:53,285
Speaker 5:  Kept saying it looked like an SNL set. Yeah. It's just like

560
00:33:53,825 --> 00:33:54,245
Speaker 1:  Here's,

561
00:33:54,265 --> 00:33:56,525
Speaker 5:  Here's a wall and like a podium. Yeah.

562
00:33:57,125 --> 00:33:59,325
Speaker 1:  Standing there. They were just like, here's, here's a, here's a sitting room

563
00:33:59,325 --> 00:33:59,805
Speaker 1:  that we're in

564
00:34:01,495 --> 00:34:05,445
Speaker 1:  announcing these quote raised $500 billion to build data centers.

565
00:34:06,665 --> 00:34:10,445
Speaker 1:  Google isn't talking about raising money to do ai. Like they have the data

566
00:34:10,445 --> 00:34:14,285
Speaker 1:  centers. And actually what Char talks about most of all is bringing the cost

567
00:34:14,285 --> 00:34:17,885
Speaker 1:  of inference down. He's like, we're great at this. And so you're, you can

568
00:34:17,885 --> 00:34:20,565
Speaker 1:  just see they're starting from like two radically different positions.

569
00:34:22,325 --> 00:34:25,445
Speaker 1:  I don't know if any of those open AI announcements are real. Certainly Elon

570
00:34:25,445 --> 00:34:29,405
Speaker 1:  Musk does not think they're real. Donald Trump thinks you're real. Like there's

571
00:34:29,405 --> 00:34:32,645
Speaker 1:  a lot of that just like weirdness embedded in there. But you're just, look,

572
00:34:32,785 --> 00:34:36,485
Speaker 1:  I'm just looking at this much more broadly and abstractly from who are the

573
00:34:36,485 --> 00:34:40,405
Speaker 1:  competitors? How are they funded? How long will they last? Who can sustain?

574
00:34:40,465 --> 00:34:44,245
Speaker 1:  And Google is just in a position where, yeah, maybe it's not the best

575
00:34:44,305 --> 00:34:47,405
Speaker 1:  one. There's an argument that it's it's very good at some things compared

576
00:34:47,405 --> 00:34:50,805
Speaker 1:  to the others. Yeah. But maybe it's not the best one overall,

577
00:34:51,545 --> 00:34:55,005
Speaker 1:  but it's in so many different places that are already popular and Google

578
00:34:55,145 --> 00:34:59,085
Speaker 1:  can just lose money on it in a way that Sam Altman has to

579
00:34:59,085 --> 00:35:02,885
Speaker 1:  go raise half a trillion dollars of weird laundered

580
00:35:02,885 --> 00:35:06,725
Speaker 1:  money for Middle Eastern states through SoftBank and then announce data centers

581
00:35:06,725 --> 00:35:09,805
Speaker 1:  that may or not. Like he's gotta do a whole other thing. And Google just

582
00:35:09,805 --> 00:35:12,645
Speaker 1:  has to be like, would you like to buy ads on on YouTube? Right. And that's

583
00:35:12,645 --> 00:35:13,005
Speaker 1:  different,

584
00:35:13,375 --> 00:35:17,205
Speaker 5:  Right? The, I mean like in a very real way, Sam Altman has to build

585
00:35:17,205 --> 00:35:21,125
Speaker 5:  Google and all Google has to do is convince people that Gemini is

586
00:35:21,125 --> 00:35:25,085
Speaker 5:  as good as chat GBT. And that thing is not nothing like, I think the, maybe

587
00:35:25,105 --> 00:35:28,965
Speaker 5:  the biggest thing chat GT is going for it right now is like a huge

588
00:35:29,055 --> 00:35:32,845
Speaker 5:  perception advantage. Like it, it was first it blew people's

589
00:35:32,845 --> 00:35:36,005
Speaker 5:  minds. It it is the thing when people think about this space, like it is,

590
00:35:36,005 --> 00:35:38,365
Speaker 5:  it is the closest to the like Kleenex

591
00:38:58,595 --> 00:39:01,885
Speaker 1:  Alright, we're back. Lauren Finer is here. Hello Lauren. Hi

592
00:39:03,085 --> 00:39:04,885
Speaker 1:  S sorry that you're our policy reporter.

593
00:39:06,015 --> 00:39:07,765
Speaker 12:  Thank you. I'll accept the condolences.

594
00:39:08,665 --> 00:39:12,365
Speaker 1:  You have been very busy. The Trump administration

595
00:39:12,365 --> 00:39:16,165
Speaker 1:  kicked off on day one with a flurry of executive orders, some of which

596
00:39:16,165 --> 00:39:19,685
Speaker 1:  are fully nonsensical, some of which have already been put on hold by courts.

597
00:39:19,685 --> 00:39:23,445
Speaker 1:  Like literally as we're speaking. The birthright citizenship order was,

598
00:39:24,045 --> 00:39:26,325
Speaker 1:  I think the court was like, this is stupid. I don't think I've ever seen

599
00:39:26,405 --> 00:39:29,845
Speaker 1:  a judge just say this is stupid. And he was like, this is stupid. So there's

600
00:39:29,865 --> 00:39:33,565
Speaker 1:  all this activity happening. Trump, as everyone remembers from the first

601
00:39:33,565 --> 00:39:36,805
Speaker 1:  Trump administration, talks a lot. And then you have to evaluate whether

602
00:39:36,805 --> 00:39:39,605
Speaker 1:  what he's saying is real. So I would start with

603
00:39:40,285 --> 00:39:42,765
Speaker 1:  Stargate announcement. We were just talking about OpenAI. They announced

604
00:39:42,765 --> 00:39:45,885
Speaker 1:  this $500 billion plan to build data centers

605
00:39:46,675 --> 00:39:50,125
Speaker 1:  with Oracle, with Nvidia.

606
00:39:51,425 --> 00:39:55,405
Speaker 1:  It was very odd. SoftBank is involved, Maan was in the White House next

607
00:39:55,405 --> 00:39:58,925
Speaker 1:  to Larry Allison and Sam Altman. Sam Altman did the thing that people have

608
00:39:58,925 --> 00:40:00,805
Speaker 1:  to do when they announced anything with Trump. They're like, we can only

609
00:40:00,805 --> 00:40:03,605
Speaker 1:  do this because of you. There's some amount of like, you were already planning

610
00:40:03,605 --> 00:40:07,165
Speaker 1:  on doing this and there's some amount of like, is this real? Like did you

611
00:40:07,165 --> 00:40:11,045
Speaker 1:  just do a foxcon? I got huge foxcon vibes from this. And then on

612
00:40:11,045 --> 00:40:11,725
Speaker 1:  top of all of that

613
00:40:13,385 --> 00:40:17,165
Speaker 1:  is Elon Musk, who is a senior White House advisor in charge of Doge

614
00:40:18,005 --> 00:40:21,605
Speaker 1:  tweeting. They don't have the money. Lauren, can you

615
00:40:22,515 --> 00:40:25,645
Speaker 1:  just like over under how long do you think Elon's gonna last? Is a government

616
00:40:25,645 --> 00:40:27,725
Speaker 1:  advisor criticizing the president's steals?

617
00:40:28,235 --> 00:40:32,045
Speaker 12:  Yeah, I mean it seems like there's gotta be an, an

618
00:40:32,045 --> 00:40:36,005
Speaker 12:  expiration date on this. I mean, it's hard, it's been hard enough

619
00:40:36,075 --> 00:40:39,565
Speaker 12:  that, you know, it's two big egos in a room and now,

620
00:40:39,905 --> 00:40:43,845
Speaker 12:  you know, Musk is really clashing with things that Trump himself is

621
00:40:43,845 --> 00:40:47,645
Speaker 12:  putting out there as these like big accomplishments. And I feel like that's

622
00:40:47,645 --> 00:40:50,405
Speaker 12:  really where we're gonna see a potential for a break.

623
00:40:50,795 --> 00:40:54,165
Speaker 1:  Yeah, I mean the, again, when I say get Fox Sun vibes,

624
00:40:55,105 --> 00:40:58,045
Speaker 1:  we have some new listeners. It, it, it has occurred to me several times over

625
00:40:58,045 --> 00:41:01,815
Speaker 1:  these past few days that Trump won was eight years ago. And that we have

626
00:41:01,815 --> 00:41:04,495
Speaker 1:  listeners who were in their twenties now who were teenagers then. So If you

627
00:41:04,495 --> 00:41:08,255
Speaker 1:  don't remember the things that happened in Trump won, lemme just catch you

628
00:41:08,255 --> 00:41:12,015
Speaker 1:  up. They took golden shovels to my hometown in Rac, Wisconsin and announced

629
00:41:12,015 --> 00:41:15,895
Speaker 1:  an LCD factory and Trump ceremony obviously, you know, broke

630
00:41:15,895 --> 00:41:18,415
Speaker 1:  ground with a golden shovel. They, they never built the factory.

631
00:41:20,095 --> 00:41:22,285
Speaker 1:  We'll put some links in the show notes. We did a lot of coverage on this.

632
00:41:22,665 --> 00:41:26,565
Speaker 1:  It was, it was a substantial portion of my life. They built a dome, there's

633
00:41:26,565 --> 00:41:30,445
Speaker 1:  a dome, they built a dome. They, they continue to claim as a da data

634
00:41:30,445 --> 00:41:32,565
Speaker 1:  center and is actually just a party zone.

635
00:41:34,185 --> 00:41:35,205
Speaker 5:  What's the difference really?

636
00:41:35,605 --> 00:41:39,245
Speaker 1:  There's a, there are some servers in a shipping container outside the,

637
00:41:39,425 --> 00:41:42,725
Speaker 1:  the dome anyway, but that it's like real vibes here, right? Like we're just

638
00:41:42,725 --> 00:41:46,205
Speaker 1:  gonna make big announcements about building stuff in America's back and eventually

639
00:41:46,205 --> 00:41:48,925
Speaker 1:  all you get is a dome in Wisconsin and Microsoft's saying it will build a

640
00:41:48,925 --> 00:41:52,845
Speaker 1:  data center there. And that's all that happened. This to me had

641
00:41:52,845 --> 00:41:56,445
Speaker 1:  all that right? OpenAI and Microsoft are like headed towards this breakup.

642
00:41:56,445 --> 00:41:59,605
Speaker 1:  They're announcing this other funding. We're gonna stand in the White House

643
00:41:59,605 --> 00:42:02,685
Speaker 1:  and announce this big deal. And then right next to it you have

644
00:42:03,475 --> 00:42:07,435
Speaker 1:  Doge and Elon being like, no, that's fake. I like,

645
00:42:07,455 --> 00:42:11,075
Speaker 1:  I'm better. Like I hate Sam Alman. Like he's like jealous. Like, I don't

646
00:42:11,075 --> 00:42:13,995
Speaker 1:  know how else to describe it. He's jealous of Sam Altman getting to stand

647
00:42:13,995 --> 00:42:15,115
Speaker 1:  in the White House to make an announcement

648
00:42:15,295 --> 00:42:18,635
Speaker 5:  And they've been like beefing and subtweeting each other ever since. I mean

649
00:42:18,635 --> 00:42:22,555
Speaker 5:  it, it is a remarkably like juvenile thing that's happening here. But

650
00:42:22,595 --> 00:42:26,475
Speaker 5:  I think at the risk of asking a really stupid question, if

651
00:42:26,545 --> 00:42:30,195
Speaker 5:  this is a real thing and, and I think one thing that we have learned

652
00:42:30,415 --> 00:42:34,395
Speaker 5:  is that Sam Altman's ability to raise money is not to

653
00:42:34,395 --> 00:42:38,235
Speaker 5:  be questioned. That man can raise any amount of money for anything at

654
00:42:38,235 --> 00:42:42,075
Speaker 5:  any time. Just seems to be the case. What is the point of

655
00:42:42,475 --> 00:42:46,075
Speaker 5:  standing at a podium in the White House and doing this with Masa Sun and

656
00:42:46,325 --> 00:42:50,035
Speaker 5:  Larry Ellison Like just, just go spend the $500 billion

657
00:42:50,255 --> 00:42:54,075
Speaker 5:  on open AI infrastructure. Like why, why does the Trump White House have

658
00:42:54,275 --> 00:42:55,035
Speaker 5:  anything to do with this at all?

659
00:42:55,555 --> 00:42:59,035
Speaker 12:  I mean I think Ellison makes sense for sure because

660
00:42:59,465 --> 00:43:03,395
Speaker 12:  he's been a Trump ally for a while now and I think, you know, maybe

661
00:43:03,845 --> 00:43:07,835
Speaker 12:  Trump kind of wants to get his buddies in a room and make a deal.

662
00:43:08,695 --> 00:43:12,275
Speaker 12:  So I think he just likes the optics of having all of them up there, having

663
00:43:12,275 --> 00:43:16,035
Speaker 12:  people working together for him in a sense

664
00:43:16,255 --> 00:43:18,755
Speaker 12:  on this project that he is championing.

665
00:43:18,905 --> 00:43:22,355
Speaker 1:  Allison also, by the way, C of Oracle, which is

666
00:43:22,505 --> 00:43:25,715
Speaker 1:  technically operating TikTok illegally right now.

667
00:43:26,135 --> 00:43:29,195
Speaker 5:  Is it? Well it's what's legal Neil to find legal,

668
00:43:29,605 --> 00:43:33,555
Speaker 1:  Right? And at that same Stargate Press conference, Trump was like, Larry,

669
00:43:33,555 --> 00:43:36,955
Speaker 1:  are you gonna buy it? Let's negotiate in front of the media. And so there's

670
00:43:36,955 --> 00:43:38,515
Speaker 1:  just a, there's a weirdness here.

671
00:43:40,055 --> 00:43:43,915
Speaker 1:  We should talk about Doge for maybe one second. Like the

672
00:43:43,915 --> 00:43:47,875
Speaker 1:  executive order creating Doge Renames an existing part

673
00:43:47,875 --> 00:43:51,715
Speaker 1:  of the White House called the United States Digital Service to the United States

674
00:43:51,825 --> 00:43:54,835
Speaker 1:  Doge Service, which means it's a nesting acronym, I want to kill myself.

675
00:43:56,415 --> 00:44:00,075
Speaker 1:  And it basically says what we're gonna put Doge people throughout the

676
00:44:00,075 --> 00:44:03,195
Speaker 1:  government and you have to tell them all your unclassified information. Weird

677
00:44:03,195 --> 00:44:06,915
Speaker 1:  spot for Elon to be in to run that. He kicked out the VA

678
00:44:07,025 --> 00:44:10,835
Speaker 1:  ramaswamy, which is very funny before he even started.

679
00:44:11,775 --> 00:44:14,995
Speaker 1:  If you don't know about The S ds, the digital service, they're basically

680
00:44:15,015 --> 00:44:18,995
Speaker 1:  the product management organization of the government. Like they built

681
00:44:18,995 --> 00:44:22,955
Speaker 1:  healthcare.gov and they ran the VA website. I have no idea what Elon is

682
00:44:22,955 --> 00:44:26,755
Speaker 1:  gonna do to those websites. Like those are very important websites.

683
00:44:26,855 --> 00:44:30,395
Speaker 5:  The S DS has had a lot of very smart people over the years doing interesting

684
00:44:30,425 --> 00:44:33,445
Speaker 5:  work. I suspect it will be different now.

685
00:44:33,795 --> 00:44:37,725
Speaker 1:  Yeah. So we'll see. And then, you know, Elon is doing like Nazi sluts and

686
00:44:37,725 --> 00:44:41,525
Speaker 1:  so like how long will this last? Like how long can this last

687
00:44:41,595 --> 00:44:44,725
Speaker 1:  when he is the focal point for all of his stuff and he is attacking the president

688
00:44:44,985 --> 00:44:48,205
Speaker 1:  and he might hurt the VA website, like

689
00:44:48,905 --> 00:44:52,445
Speaker 1:  I'm giving it 90 days. That's my call. Lauren, what do you think?

690
00:44:53,495 --> 00:44:54,115
Speaker 12:  I'm gonna say

691
00:44:55,905 --> 00:44:58,445
Speaker 12:  may I think after the first a hundred days he's out.

692
00:44:58,995 --> 00:45:02,725
Speaker 1:  Okay. But he's, I think, I think he's only got until July 4th, 2026. So it's

693
00:45:02,725 --> 00:45:04,565
Speaker 1:  that's that's his own deadline. It's in the eo.

694
00:45:04,985 --> 00:45:05,485
Speaker 12:  Oh, okay.

695
00:45:06,645 --> 00:45:10,455
Speaker 5:  Okay. But just just to logic this out for a second,

696
00:45:11,635 --> 00:45:15,355
Speaker 5:  I I I feel like the over under on this has to be like five days.

697
00:45:15,505 --> 00:45:19,475
Speaker 5:  Like I I, if there's no recrimination or

698
00:45:19,475 --> 00:45:23,075
Speaker 5:  change from what has happened so far, including a Nazi salute on

699
00:45:23,075 --> 00:45:25,075
Speaker 5:  inauguration day two of them, in fact

700
00:45:27,665 --> 00:45:31,325
Speaker 5:  what's, what's, what problem is there going to be that that undoes this?

701
00:45:31,525 --> 00:45:35,415
Speaker 5:  I think like we have already seen what this looks

702
00:45:35,415 --> 00:45:39,095
Speaker 5:  like. We like this is just what it's going to be and if

703
00:45:39,335 --> 00:45:42,135
Speaker 5:  everyone is comfortable with it now, why would they get uncomfortable with

704
00:45:42,135 --> 00:45:43,135
Speaker 5:  it later?

705
00:45:43,605 --> 00:45:46,535
Speaker 1:  Hang on. I feel like I have to very pedantically note, he

706
00:45:47,625 --> 00:45:51,345
Speaker 1:  did a thing that looked like a Nazi salute. That Nazis believe it was a Nazi

707
00:45:51,465 --> 00:45:54,625
Speaker 1:  salute governments around the world. The German press believes it's Nazi

708
00:45:54,745 --> 00:45:58,575
Speaker 1:  salute. His fans say it's not, the

709
00:45:58,575 --> 00:46:01,855
Speaker 1:  point is to make us argue about the ambiguity. That's the point of the troll.

710
00:46:01,915 --> 00:46:02,295
Speaker 1:  We don't

711
00:46:02,295 --> 00:46:05,895
Speaker 5:  Know what was in Elon Musk's heart, but the thing he did looked more like

712
00:46:05,935 --> 00:46:09,095
Speaker 5:  a Nazi salute than any other thing that exists on Earth. Yeah. Than

713
00:46:09,095 --> 00:46:12,615
Speaker 1:  He supports the Chairman Fire party. Like you can just add it up. But the

714
00:46:12,615 --> 00:46:15,775
Speaker 1:  point is to make everyone yell about the equal sign. You know what I mean?

715
00:46:15,805 --> 00:46:19,095
Speaker 1:  Like, anyway, but that is how he gains attention.

716
00:46:20,075 --> 00:46:23,255
Speaker 1:  But the person that wants the most attention is his nominal boss.

717
00:46:24,775 --> 00:46:27,515
Speaker 1:  So you, you might be right about five days, I'm just saying it might be more

718
00:46:27,515 --> 00:46:31,475
Speaker 1:  like 90 'cause Trump is busy just like lecturing the World Economic

719
00:46:31,475 --> 00:46:31,675
Speaker 1:  Forum.

720
00:46:31,895 --> 00:46:35,035
Speaker 5:  He might just like forget Elon is there for a while and then, and then this

721
00:46:35,035 --> 00:46:35,355
Speaker 5:  will happen.

722
00:46:35,505 --> 00:46:38,555
Speaker 1:  Wait, So I just wanna be clear. I said 90 Lauren said a hundred and you say

723
00:46:38,555 --> 00:46:39,515
Speaker 1:  five. I'm

724
00:46:39,515 --> 00:46:42,475
Speaker 5:  Gonna, I'm I'm gonna go 15. I'm putting the over hundred at 15 days. Like

725
00:46:42,475 --> 00:46:46,395
Speaker 5:  I think if, if we get to Valentine's Day and Elon Musk is still has an

726
00:46:46,395 --> 00:46:47,955
Speaker 5:  office in the West Wing, I will be surprised.

727
00:46:48,225 --> 00:46:51,675
Speaker 12:  Okay. I think he's had remarkable staying power so far. So I

728
00:46:52,055 --> 00:46:54,115
Speaker 12:  I'm just bet that he's gonna continue to surprise us.

729
00:46:55,835 --> 00:46:59,715
Speaker 1:  Alright, that's Elon. That's Doge. We'll see if how that turns into reality.

730
00:46:59,735 --> 00:47:02,995
Speaker 1:  It just, I think for The Verge cast audience, for The Verge audience, you

731
00:47:03,195 --> 00:47:07,035
Speaker 1:  should go listen to the administering the, the administrator

732
00:47:07,035 --> 00:47:10,355
Speaker 1:  that, well outgoing administrator of the digital service on decoder. Like

733
00:47:10,355 --> 00:47:13,475
Speaker 1:  that's a software company. There's a, a tiny little software consultancy

734
00:47:13,475 --> 00:47:17,075
Speaker 1:  in the government that makes all the websites go. And now it's Doge. So

735
00:47:17,715 --> 00:47:20,955
Speaker 1:  I, I think I say this a lot. Like you can't know how something is changing

736
00:47:20,955 --> 00:47:24,035
Speaker 1:  unless you know what it was. It's a go listen. If you're interested, go listen

737
00:47:24,035 --> 00:47:27,715
Speaker 1:  to that episode. 'cause how Doge changes is gonna be, I've seen a lot of

738
00:47:28,035 --> 00:47:31,985
Speaker 1:  USDS people say, actually this is the right place for it to go because they're

739
00:47:31,985 --> 00:47:35,625
Speaker 1:  the software consultancy. And this is like, software helps you find

740
00:47:35,625 --> 00:47:39,545
Speaker 1:  efficiency. I suspect those people are coping like they're,

741
00:47:39,545 --> 00:47:41,305
Speaker 1:  they're processing. But anyway, go listen to that.

742
00:47:41,305 --> 00:47:44,865
Speaker 5:  Wouldn't that also mean that the, the job of Doge would be to decide if the

743
00:47:44,865 --> 00:47:48,505
Speaker 5:  government should use like Microsoft Teams or Slack for its communication

744
00:47:48,505 --> 00:47:50,065
Speaker 5:  systems? Like is that, is that where we're headed?

745
00:47:50,265 --> 00:47:53,345
Speaker 1:  I don't, I've, I've met a few PMs in my life. I I don't think that's how

746
00:47:53,345 --> 00:47:54,545
Speaker 1:  they spend their time. But

747
00:47:55,225 --> 00:47:57,705
Speaker 5:  Wait, Lauren, before we move away from this, can I just ask you one like

748
00:47:57,725 --> 00:48:00,265
Speaker 5:  big broad question that I have been having all week and

749
00:48:01,575 --> 00:48:04,345
Speaker 5:  there's this flurry of executive orders and, and my

750
00:48:04,955 --> 00:48:08,865
Speaker 5:  impression of what this moment looks like in a

751
00:48:08,865 --> 00:48:12,785
Speaker 5:  presidency is some mix of like real

752
00:48:12,825 --> 00:48:16,705
Speaker 5:  policymaking and some like Michael Scott in the office

753
00:48:16,705 --> 00:48:20,265
Speaker 5:  yelling, I declare bankruptcy. Like how,

754
00:48:20,565 --> 00:48:23,825
Speaker 5:  how real should we be thinking about all of this stuff is he's talking about

755
00:48:23,825 --> 00:48:27,785
Speaker 5:  everything from like the, the Paris Climate agreement to AI safety, to

756
00:48:27,785 --> 00:48:31,745
Speaker 5:  all this stuff on electric cars. Like how real are you tracking this stuff

757
00:48:31,745 --> 00:48:32,425
Speaker 5:  in your head right now?

758
00:48:33,135 --> 00:48:36,865
Speaker 12:  Yeah, I think that's definitely the perfect analogy for it. I think it's

759
00:48:36,865 --> 00:48:39,745
Speaker 12:  a mix. I think, you know, there's some things that are going to have real

760
00:48:39,745 --> 00:48:43,545
Speaker 12:  implications or at least we really have to see a big fight

761
00:48:43,655 --> 00:48:47,545
Speaker 12:  play out over it to know where it's gonna end up. And then I think there's

762
00:48:47,545 --> 00:48:50,705
Speaker 12:  other stuff that's either like, all right, well this kind of already existed,

763
00:48:50,895 --> 00:48:54,265
Speaker 12:  like the executive order basically saying the first amendment is a thing.

764
00:48:54,925 --> 00:48:58,465
Speaker 12:  And then other things that, you know, are just obviously

765
00:48:59,165 --> 00:49:02,745
Speaker 12:  not inconsistent with the constitution, like ending

766
00:49:02,745 --> 00:49:06,705
Speaker 12:  birthright citizenship. So I think a lot of it

767
00:49:06,885 --> 00:49:10,745
Speaker 12:  is, you know, just shouting something, just declaring that things

768
00:49:10,795 --> 00:49:14,665
Speaker 12:  exist but, you know, mixed in, there are things that will at least

769
00:49:14,665 --> 00:49:18,425
Speaker 12:  have to be dealt with in the courts in and, you know, in

770
00:49:18,425 --> 00:49:20,705
Speaker 12:  people's lives. So I think it is definitely a mix.

771
00:49:21,215 --> 00:49:21,505
Speaker 5:  Okay.

772
00:49:22,135 --> 00:49:25,185
Speaker 1:  This brings us inevitably to TikTok. I just wanna write That's

773
00:49:25,185 --> 00:49:28,225
Speaker 5:  True. Oh yeah, there was a TikTok EO wasn't there? Yes,

774
00:49:28,675 --> 00:49:29,025
Speaker 1:  There

775
00:49:29,045 --> 00:49:29,265
Speaker 12:  Was

776
00:49:31,055 --> 00:49:34,115
Speaker 1:  It. It's there's too many It, well let's just focus, can

777
00:49:34,295 --> 00:49:36,195
Speaker 5:  AI summarize The S the all the executive orders

778
00:49:36,195 --> 00:49:37,675
Speaker 1:  From, well I think AI wrote a lot of them.

779
00:49:39,775 --> 00:49:40,835
Speaker 1:  I'm kind of not joking.

780
00:49:42,535 --> 00:49:45,755
Speaker 1:  The TikTok one I think synthesizes everything you're saying Lauren, right?

781
00:49:45,755 --> 00:49:49,275
Speaker 1:  Like it's, some of it is just yelling, some of it is just pointing out things

782
00:49:49,275 --> 00:49:52,555
Speaker 1:  that already are true. Some of it is gonna get litigated. Some of it is like,

783
00:49:52,555 --> 00:49:54,635
Speaker 1:  does the president even have this power? I guess we're gonna find out,

784
00:49:56,235 --> 00:50:00,115
Speaker 1:  describe what's going on with with the TikTok EO and, I mean

785
00:50:00,115 --> 00:50:01,955
Speaker 1:  TikTok is running, but like, in a weird way.

786
00:50:02,675 --> 00:50:06,565
Speaker 12:  Yeah. So the TikTok executive order is basically Trump

787
00:50:06,565 --> 00:50:10,085
Speaker 12:  saying that for 75 days, I'm gonna tell my enforcers

788
00:50:10,675 --> 00:50:14,525
Speaker 12:  back off, don't enforce this law. And like actually even the

789
00:50:14,545 --> 00:50:18,245
Speaker 12:  day preceding my presidency when the band took effect, like, don't

790
00:50:18,425 --> 00:50:21,045
Speaker 12:  go after the companies for that day

791
00:50:22,545 --> 00:50:25,885
Speaker 12:  and basically saying to the companies like, don't worry,

792
00:50:27,025 --> 00:50:30,765
Speaker 12:  we got you. We're not gonna prosecute you If you bring TikTok

793
00:50:30,765 --> 00:50:34,365
Speaker 12:  back online. Which sounds great, except that

794
00:50:34,745 --> 00:50:38,645
Speaker 12:  it doesn't really mean anything when you have a federal law on the

795
00:50:38,645 --> 00:50:42,365
Speaker 12:  books that the Supreme Court has upheld that all of these

796
00:50:42,765 --> 00:50:46,565
Speaker 12:  companies risk facing billions of dollars in fines, if they violate,

797
00:50:47,275 --> 00:50:50,245
Speaker 1:  It's like $5,000 per user. You have

798
00:50:50,285 --> 00:50:54,045
Speaker 1:  170 million user. Like it's just billions of dollars. And it's weird because

799
00:50:54,045 --> 00:50:58,005
Speaker 1:  Oracle, If you recall, when TikTok was trying to save itself during

800
00:50:58,005 --> 00:51:00,725
Speaker 1:  the first Trump administration, when he threatened to ban it, they built

801
00:51:00,725 --> 00:51:04,565
Speaker 1:  this thing called Project Texas, where Oracle ran its data

802
00:51:04,565 --> 00:51:08,405
Speaker 1:  centers and it was all walled off from China, or by chance or however they

803
00:51:08,405 --> 00:51:11,325
Speaker 1:  were, project Texas to me, always felt like a kids museum.

804
00:51:12,305 --> 00:51:14,205
Speaker 1:  You know, like you could go there and they'd be like, here's how content

805
00:51:14,205 --> 00:51:18,045
Speaker 1:  moderation works. And like nothing was real. So Oracle runs it, Akamai, who's

806
00:51:18,045 --> 00:51:21,565
Speaker 1:  their CDN, you know, moves the data actually around the internet and delivers

807
00:51:21,565 --> 00:51:25,445
Speaker 1:  content people. And then Apple and Google obviously distribute it in their

808
00:51:25,445 --> 00:51:28,405
Speaker 1:  stores. And by the way, so does Amazon. So does the Samsung store. It's not

809
00:51:28,405 --> 00:51:31,565
Speaker 1:  in the Samsung store, in case you're wondering. So it's really interesting

810
00:51:31,565 --> 00:51:35,005
Speaker 1:  to me that Oracle, which hosts the data project, Texas Akamai, which delivers

811
00:51:35,005 --> 00:51:38,165
Speaker 1:  the data, have decided this is fine, they're gonna take the risk. TikTok

812
00:51:38,165 --> 00:51:42,045
Speaker 1:  is back, and then the app stores have decided it's not worth

813
00:51:42,045 --> 00:51:44,685
Speaker 1:  the risk. Like If you search for TikTok in the app store today, it's not

814
00:51:44,685 --> 00:51:47,845
Speaker 1:  there. I, I have not yet figured out why

815
00:51:48,995 --> 00:51:52,245
Speaker 1:  some people think it's okay. I, I actually think Oracle and Akamai have more

816
00:51:52,315 --> 00:51:55,485
Speaker 1:  liability because it, TikTok is already on the phones,

817
00:51:56,175 --> 00:52:00,085
Speaker 1:  right? So like Apple isn't delivering it to more people, I guess software

818
00:52:00,085 --> 00:52:03,605
Speaker 1:  updates or something. But every time all of those people

819
00:52:03,795 --> 00:52:07,485
Speaker 1:  open the TikTok app, like Oracle's liable for it.

820
00:52:07,595 --> 00:52:08,085
Speaker 12:  Exactly.

821
00:52:08,505 --> 00:52:11,945
Speaker 1:  And so that's where I get to like Larry Ellison standing in the room, right?

822
00:52:11,945 --> 00:52:15,825
Speaker 1:  Like Larry Ellison delivered this win to Donald Trump and that

823
00:52:15,825 --> 00:52:18,585
Speaker 1:  means he's a little bit more secure that he won't be prosecuted.

824
00:52:19,505 --> 00:52:23,215
Speaker 12:  Right. And Larry Ellison appears to be one of the only people

825
00:52:23,435 --> 00:52:27,055
Speaker 12:  in the US to be comfortable enough relying on

826
00:52:27,125 --> 00:52:30,975
Speaker 12:  Trump's promise here in this executive order that doesn't

827
00:52:30,975 --> 00:52:34,695
Speaker 12:  really hold the force of law more than the actual law on the book

828
00:52:34,795 --> 00:52:36,935
Speaker 12:  status to believe it.

829
00:52:37,275 --> 00:52:41,015
Speaker 1:  You went and talked to some lawyers about this. I think I want you to explain

830
00:52:41,015 --> 00:52:44,815
Speaker 1:  their read. My, my read was, well, If you read the

831
00:52:44,815 --> 00:52:47,935
Speaker 1:  executive order at the bottom, it's got the same boilerplate that every executive

832
00:52:47,935 --> 00:52:51,815
Speaker 1:  order has on it, which is this does not create liability for

833
00:52:51,815 --> 00:52:54,495
Speaker 1:  the United States. And you're like, well, If you make a promise that you

834
00:52:54,495 --> 00:52:57,855
Speaker 1:  won't enforce a law that you know, kind of like squint that looks like a

835
00:52:58,015 --> 00:53:00,495
Speaker 1:  contract and at the bottom of the contract you're like, you cannot rely on

836
00:53:00,495 --> 00:53:03,415
Speaker 1:  this contract. Well you shouldn't rely on it.

837
00:53:03,845 --> 00:53:07,775
Speaker 12:  Exactly. I mean, I think it's pretty straightforward from the

838
00:53:07,775 --> 00:53:11,655
Speaker 12:  legal analysis that I've seen where it's just, it's basically

839
00:53:11,655 --> 00:53:15,615
Speaker 12:  just, you know, this does not negate the fact that there is a

840
00:53:15,635 --> 00:53:19,415
Speaker 12:  law that finds all of these service providers for TikTok

841
00:53:20,005 --> 00:53:23,815
Speaker 12:  $5,000 per user for accessing the app. And that can

842
00:53:23,815 --> 00:53:27,645
Speaker 12:  amount to, you know, hundreds of billions of dollars in fines

843
00:53:27,645 --> 00:53:31,205
Speaker 12:  for these companies. And even if you're like one of the richest companies

844
00:53:31,205 --> 00:53:34,885
Speaker 12:  in the world, that's a lot of money. And while it

845
00:53:34,885 --> 00:53:38,725
Speaker 12:  might insulate you in the long run from actually having to

846
00:53:38,725 --> 00:53:41,925
Speaker 12:  pay those fines because you can make a decent

847
00:53:42,785 --> 00:53:46,685
Speaker 12:  due process argument that there's some sort of, you were

848
00:53:46,715 --> 00:53:50,525
Speaker 12:  operating under this kind of promise from the executive branch

849
00:53:50,635 --> 00:53:54,605
Speaker 12:  that you wouldn't be prosecuted for this. I think ultimately

850
00:53:54,955 --> 00:53:58,565
Speaker 12:  it's a big risk to take when you know the penalties are that large.

851
00:53:59,285 --> 00:54:02,525
Speaker 1:  I keep trying to figure out why Apple and Google aren't like, do like

852
00:54:03,005 --> 00:54:06,525
Speaker 1:  distributing this app. And I, I guess, you know, if Oracle doesn't play ball,

853
00:54:06,535 --> 00:54:10,125
Speaker 1:  there is no TikTok if Apple and Google don't play ball well, people still

854
00:54:10,125 --> 00:54:13,165
Speaker 1:  have the app, right? And like, so maybe they're just like, look, this isn't

855
00:54:13,165 --> 00:54:15,765
Speaker 1:  worth it. The people who have it already have it and all, all that's really

856
00:54:15,765 --> 00:54:17,405
Speaker 1:  happening is people aren't gonna download this app for a while.

857
00:54:17,655 --> 00:54:21,405
Speaker 12:  Right. And I imagine what Trump would notice more is like,

858
00:54:21,945 --> 00:54:25,405
Speaker 12:  can people see content on TikTok or not?

859
00:54:25,665 --> 00:54:29,205
Speaker 12:  Versus like, can a marginal few who don't already have it on

860
00:54:29,335 --> 00:54:33,285
Speaker 12:  their phones? I mean, there's 170 million people in the US who already have

861
00:54:33,285 --> 00:54:36,365
Speaker 12:  been using TikTok can like a marginal few, like

862
00:54:37,145 --> 00:54:40,805
Speaker 12:  get it now. And you know, eventually if Apple and Google

863
00:54:41,015 --> 00:54:44,365
Speaker 12:  don't bring it back to their app stores, the app is going to degrade

864
00:54:44,945 --> 00:54:48,765
Speaker 12:  and you know, it won't work as well as it you'd expect it to.

865
00:54:48,985 --> 00:54:52,525
Speaker 12:  But how long will that take? We don't really know. It might be a long enough

866
00:54:52,525 --> 00:54:54,845
Speaker 12:  time that it doesn't really matter in the end. Well,

867
00:54:54,845 --> 00:54:58,765
Speaker 1:  Yeah, yeah. 75 days. Like I, I know when I know when

868
00:54:58,765 --> 00:55:02,205
Speaker 1:  something ends. Right. Two really interesting things about that. One,

869
00:55:02,955 --> 00:55:06,685
Speaker 1:  some people did delete TikTok from their phones. They are very sad 'cause

870
00:55:06,685 --> 00:55:10,605
Speaker 1:  they can't get it back. There's a weird black market for phones with TikTok

871
00:55:10,605 --> 00:55:13,205
Speaker 1:  on them on eBay where they're selling for thousands of dollars. And then

872
00:55:13,345 --> 00:55:17,045
Speaker 1:  we had the very weird experience of

873
00:55:17,225 --> 00:55:20,645
Speaker 1:  our posts saying TikTok is not available in app stores. Like a little quick

874
00:55:20,645 --> 00:55:24,485
Speaker 1:  post ranked in Google. It was the number one search result for

875
00:55:24,485 --> 00:55:28,365
Speaker 1:  TikTok app store. Weird and flawed of comments.

876
00:55:28,765 --> 00:55:31,445
Speaker 1:  Thousands of comments from people who thought, well we were a social network

877
00:55:31,985 --> 00:55:35,685
Speaker 1:  and we posting things like what app is this? Where'd everybody come from?

878
00:55:36,615 --> 00:55:40,365
Speaker 1:  Which is a lot, was a lot like, we were like, is this a bot attack? And then

879
00:55:40,385 --> 00:55:44,325
Speaker 1:  our like head of community is like, it's teens. It's teens who

880
00:55:44,325 --> 00:55:47,405
Speaker 1:  have never seen a website before and we just trying to find TikTok. Yes.

881
00:55:48,115 --> 00:55:51,325
Speaker 12:  Yeah. I have seen that some people have found a workaround for

882
00:55:51,845 --> 00:55:55,325
Speaker 12:  downloading TikTok if they somehow deleted it.

883
00:55:55,565 --> 00:55:56,805
Speaker 1:  What's the, wait, what's the workaround?

884
00:55:57,395 --> 00:56:00,765
Speaker 12:  From what I saw, it has to do with like logging out of your

885
00:56:01,025 --> 00:56:04,485
Speaker 12:  iCloud account and then like logging in like you're in another country and

886
00:56:04,725 --> 00:56:08,485
Speaker 12:  downloading it. So I'm not endorsing this, but that's something that I've

887
00:56:08,485 --> 00:56:08,645
Speaker 12:  seen.

888
00:56:09,195 --> 00:56:12,925
Speaker 1:  There's a, you know, there's like a lot of like controversial science around

889
00:56:12,925 --> 00:56:16,045
Speaker 1:  like cell phone addiction. Like, and you can have a lot of arguments and

890
00:56:16,045 --> 00:56:16,845
Speaker 1:  it's like, well,

891
00:56:19,705 --> 00:56:21,815
Speaker 1:  would you, like, do you see what's happening here?

892
00:56:23,495 --> 00:56:24,555
Speaker 12:  The teens will find a way,

893
00:56:24,775 --> 00:56:27,995
Speaker 1:  The teens, it was just like really, like someone was like, this post is getting

894
00:56:28,395 --> 00:56:31,675
Speaker 1:  thousands of comments. Like we must, it must be a bot attack.

895
00:56:32,695 --> 00:56:35,955
Speaker 1:  It is, it is currently the most commented post in our new commenting system.

896
00:56:36,175 --> 00:56:39,075
Speaker 1:  Wow. More comments than my endorsement of comma.

897
00:56:42,185 --> 00:56:45,675
Speaker 1:  Like, I got a note that was like, your record has been broken. It's teens

898
00:56:45,675 --> 00:56:49,475
Speaker 1:  making friends in the TikTok post. Very good. I hope

899
00:56:49,525 --> 00:56:52,795
Speaker 1:  maybe Trump was right to save it after all. Well, no, it helped us,

900
00:56:53,535 --> 00:56:56,795
Speaker 1:  you know, this is a new Trump too is very transactional. All right. We should

901
00:56:56,795 --> 00:57:00,755
Speaker 1:  keep TikTok shut down so teens can make friends in our comments on

902
00:57:00,755 --> 00:57:04,595
Speaker 1:  quick post. So there's, there's, that's the shape of it,

903
00:57:04,595 --> 00:57:08,075
Speaker 1:  right? We got seven, five days. There's also the politics of, of TikTok

904
00:57:08,685 --> 00:57:11,955
Speaker 1:  where they, you know, they took the app down. They said, we are thankful

905
00:57:11,975 --> 00:57:15,795
Speaker 1:  for President Trump working on a solution. They brought the app up. They

906
00:57:15,795 --> 00:57:19,755
Speaker 1:  said, thanks to President Trump we're back. They thought they were

907
00:57:19,755 --> 00:57:23,675
Speaker 1:  getting 90 days. 'cause in the bill, there's this provision to extend

908
00:57:23,675 --> 00:57:27,515
Speaker 1:  the deadline by 90 days. If there's a sale in the works, there is no

909
00:57:27,515 --> 00:57:31,315
Speaker 1:  sale in the works as far as we know. They didn't even get 90 days. They got

910
00:57:31,315 --> 00:57:34,315
Speaker 1:  75 days. Trump shaved 15 days off their deadline,

911
00:57:35,315 --> 00:57:39,275
Speaker 1:  which is very funny. And now, you know, he, he's given a number of

912
00:57:39,275 --> 00:57:42,795
Speaker 1:  press conferences since, and he talks like a real estate developer.

913
00:57:43,295 --> 00:57:46,995
Speaker 1:  He, he has, you know, shoved all of this into his

914
00:57:47,275 --> 00:57:50,755
Speaker 1:  internal framework. He's an 80 something year old real estate developer.

915
00:57:50,755 --> 00:57:53,875
Speaker 1:  He's got one framework and it's real estate. And he is like, oh, TikTok needs

916
00:57:53,955 --> 00:57:54,315
Speaker 1:  a permit,

917
00:57:56,005 --> 00:57:59,635
Speaker 1:  which is not the case. But he keeps saying, I have to give them a permit

918
00:57:59,695 --> 00:58:03,115
Speaker 1:  to operate. If I don't give them a permit, it's worth nothing. If I give

919
00:58:03,115 --> 00:58:05,515
Speaker 1:  them a permit, it's worth a trillion. They have to do a deal with me.

920
00:58:06,305 --> 00:58:10,095
Speaker 12:  Right. So there's like a, a bunch of things in there. I think

921
00:58:10,185 --> 00:58:13,975
Speaker 12:  first of all, the 75 days is interesting 'cause it kind of just goes

922
00:58:13,995 --> 00:58:17,615
Speaker 12:  to show how like not a thing this

923
00:58:17,615 --> 00:58:21,535
Speaker 12:  executive order is, because in the law it says that the

924
00:58:21,535 --> 00:58:25,095
Speaker 12:  president can give TikTok 90 days if

925
00:58:25,095 --> 00:58:28,695
Speaker 12:  there's like very much a deal happening. If the

926
00:58:28,695 --> 00:58:32,095
Speaker 12:  president is certifying that there's a deal happening and

927
00:58:32,515 --> 00:58:35,695
Speaker 12:  you know, then they can get 90 days. And it was like a little ambiguous,

928
00:58:35,695 --> 00:58:39,535
Speaker 12:  like could that 90 days be granted once the ban takes

929
00:58:39,555 --> 00:58:43,455
Speaker 12:  effect or not. But it seems like it's like ambiguous enough that it

930
00:58:43,455 --> 00:58:46,495
Speaker 12:  might've been available to Trump. Trump didn't do that. He did something

931
00:58:46,495 --> 00:58:50,135
Speaker 12:  totally different, pulled out 75 days and said

932
00:58:50,245 --> 00:58:52,935
Speaker 12:  just like, let's not enforce this for 75 days.

933
00:58:54,155 --> 00:58:57,175
Speaker 12:  And then, you know, we also have that like,

934
00:58:58,375 --> 00:59:01,955
Speaker 12:  it, it just is a completely different way of going about this. So

935
00:59:02,435 --> 00:59:06,275
Speaker 12:  I, it just kind of shows that this is not really something that's like

936
00:59:06,275 --> 00:59:09,795
Speaker 12:  following the law itself. And nothing in the law says like

937
00:59:10,225 --> 00:59:14,075
Speaker 12:  permits. And we don't know that, you know, it's not just like

938
00:59:14,125 --> 00:59:17,875
Speaker 12:  Trump can decide to sell it and then the deal will happen. The main,

939
00:59:18,655 --> 00:59:22,405
Speaker 12:  you know, point of blockage here is China. And we still

940
00:59:22,405 --> 00:59:26,205
Speaker 12:  don't know if China will really ultimately agree to a deal that would

941
00:59:26,305 --> 00:59:28,925
Speaker 12:  put TikTok in compliance with the law.

942
00:59:29,555 --> 00:59:32,965
Speaker 5:  Yeah. And there's been a lot of reporting that China is like open to it,

943
00:59:33,205 --> 00:59:36,285
Speaker 5:  thinking about it, but there's like, it's, it's all so squishy that it's

944
00:59:36,285 --> 00:59:39,925
Speaker 5:  hard to make anything out of right now. It, especially when you have

945
00:59:40,955 --> 00:59:44,005
Speaker 5:  President Trump now running around saying that what he actually wants is

946
00:59:44,045 --> 00:59:47,125
Speaker 5:  a joint venture with the United States of America in which it owns half of

947
00:59:47,125 --> 00:59:50,885
Speaker 5:  TikTok, which a, what? And BI can't imagine

948
00:59:51,025 --> 00:59:53,285
Speaker 5:  is a thing that is going to be palatable to the Chinese government.

949
00:59:53,945 --> 00:59:57,885
Speaker 12:  And it also doesn't seem like it would put it in compliance with

950
00:59:58,005 --> 01:00:01,485
Speaker 12:  this law anyway. It just kind of does this like other third thing

951
01:00:02,325 --> 01:00:06,285
Speaker 12:  because under the law, it seems like it most likely ance

952
01:00:06,285 --> 01:00:09,765
Speaker 12:  could not be involved. You know, we still couldn't have like more than

953
01:00:09,865 --> 01:00:13,565
Speaker 12:  20% ownership by a Chinese entity. So I

954
01:00:13,575 --> 01:00:17,565
Speaker 12:  don't know how exactly this works. Is the US government owning a

955
01:00:17,565 --> 01:00:21,405
Speaker 12:  speech platform? Like it's not really clear if Trump himself knows the answer

956
01:00:21,405 --> 01:00:22,205
Speaker 12:  to these questions.

957
01:00:22,745 --> 01:00:26,565
Speaker 1:  We said this a lot in sort of the Elon Twitter takeover

958
01:00:26,625 --> 01:00:30,045
Speaker 1:  era where criticism of the current administration is

959
01:00:30,545 --> 01:00:34,485
Speaker 1:  in no way praise of the previous administration. And I feel like

960
01:00:34,585 --> 01:00:38,405
Speaker 1:  that's just very clear on all sides here. Like

961
01:00:38,785 --> 01:00:42,325
Speaker 1:  the, this is pure Trump chaos, right? Like I invented an executive order

962
01:00:42,625 --> 01:00:44,765
Speaker 1:  at the end of seven, five days. If this thing isn't sold,

963
01:00:47,105 --> 01:00:50,245
Speaker 1:  he doesn't get to pull the 90 day lever because the

964
01:00:50,655 --> 01:00:54,415
Speaker 1:  expiration date has hit, right? You can't delay a

965
01:00:54,575 --> 01:00:58,375
Speaker 1:  deadline 75 days after it's passed by 90 days. Like the

966
01:00:58,375 --> 01:01:01,815
Speaker 1:  law isn't written that way. So we're, you know, by the time you're listening

967
01:01:01,815 --> 01:01:04,975
Speaker 1:  to this, we're down to 73 days, 72 days. Like

968
01:01:05,995 --> 01:01:08,895
Speaker 1:  you gotta sell TikTok. Like that's what you have to do.

969
01:01:09,225 --> 01:01:10,095
Speaker 1:  April 4th

970
01:01:10,095 --> 01:01:10,495
Speaker 5:  Is the day,

971
01:01:10,555 --> 01:01:14,335
Speaker 1:  By the way. Right? So by April 4th, TikTok has to be sold. That's a huge

972
01:01:14,365 --> 01:01:18,135
Speaker 1:  transaction that two world superpowers have to agree upon

973
01:01:18,155 --> 01:01:22,095
Speaker 1:  in some way that Trump has to, right? That they're gonna give

974
01:01:22,095 --> 01:01:25,655
Speaker 1:  him a big novelty permit to hand over. Like he runs City Hall. Like, I don't

975
01:01:25,655 --> 01:01:29,295
Speaker 1:  know what he is gonna do inside of that. Like, like

976
01:01:29,435 --> 01:01:33,295
Speaker 1:  TikTok itself has to change ownership. It does feel like

977
01:01:33,315 --> 01:01:37,215
Speaker 1:  TikTok misplayed this completely. And this is what I mean about like, it's

978
01:01:37,215 --> 01:01:40,775
Speaker 1:  not praise for any other administration. Like TikTok thought they could roll

979
01:01:40,775 --> 01:01:44,375
Speaker 1:  the Biden administration, which totally blew this. Like the Biden administration

980
01:01:44,375 --> 01:01:47,615
Speaker 1:  should have been pressuring TikTok to sell every single day. Yeah. And they

981
01:01:47,615 --> 01:01:50,855
Speaker 1:  just didn't, they did not communicate well. The Biden administration, the

982
01:01:50,855 --> 01:01:53,895
Speaker 1:  failure of Biden administration is they fundamentally like did not communicate

983
01:01:53,895 --> 01:01:56,815
Speaker 1:  very well, but they didn't. So then

984
01:01:57,515 --> 01:02:01,375
Speaker 1:  TikTok thought it could run and Biden signed it on the day, which set off

985
01:02:01,375 --> 01:02:04,375
Speaker 1:  the clock, which landed it on the day that it did, which is effectively in

986
01:02:04,375 --> 01:02:06,135
Speaker 1:  between administrations. So he,

987
01:02:07,945 --> 01:02:11,505
Speaker 1:  I, I'm sure they could count, but he signed it on day knowing

988
01:02:11,655 --> 01:02:15,545
Speaker 1:  what day the ban would hit. And he landed it right in the middle creating

989
01:02:15,575 --> 01:02:19,285
Speaker 1:  this weird vacuum where he was like, I'm handing enforcement over to the

990
01:02:19,285 --> 01:02:22,325
Speaker 1:  Trump administration. And Trump was handed this ability to just say like,

991
01:02:22,325 --> 01:02:26,125
Speaker 1:  I'll figure it out. Sure. And then TikTok had no plan

992
01:02:26,205 --> 01:02:29,765
Speaker 1:  B, they have no deal in the works. And the fact that they have no plan

993
01:02:29,885 --> 01:02:33,805
Speaker 1:  BI think is the, like the most essential failure here

994
01:02:34,235 --> 01:02:37,525
Speaker 1:  because you might think Trump is gonna save it. And certainly he saved it

995
01:02:37,605 --> 01:02:41,365
Speaker 1:  for 75 more days, sort of because it's the, the app stores. But it's

996
01:02:41,545 --> 01:02:45,365
Speaker 1:  so obvious that he wants to make this deal, right? Right.

997
01:02:45,365 --> 01:02:48,245
Speaker 1:  That he wants to be like, there's a transaction and now the, the US government

998
01:02:48,245 --> 01:02:52,045
Speaker 1:  owns a 50% or whatever we can, we haven't even talked about how bananas that

999
01:02:52,045 --> 01:02:54,885
Speaker 1:  is, but like there will be a sale.

1000
01:02:55,865 --> 01:02:59,845
Speaker 1:  And so TikTok is maybe not gonna sell to who it wants to on its terms.

1001
01:03:00,395 --> 01:03:04,365
Speaker 1:  They're gonna sell to who Trump wants on Trump's terms. And I don't know

1002
01:03:04,365 --> 01:03:06,645
Speaker 1:  that the Chinese government is super down with that idea.

1003
01:03:07,245 --> 01:03:10,845
Speaker 12:  I think there's an arc, there's a way to see this where by dance

1004
01:03:11,065 --> 01:03:14,925
Speaker 12:  didn't misplay it, they played it the only way they were going to

1005
01:03:14,925 --> 01:03:18,485
Speaker 12:  play it, which is to ride out the legal process. And

1006
01:03:18,825 --> 01:03:22,805
Speaker 12:  you know, if it doesn't work, China's not gonna let them

1007
01:03:22,835 --> 01:03:26,605
Speaker 12:  sell in a way that complies with this law. And you know, the Chinese

1008
01:03:26,605 --> 01:03:30,205
Speaker 12:  government might be okay with that because they might think that this makes

1009
01:03:30,225 --> 01:03:34,085
Speaker 12:  the US government look bad in the eyes of other governments around the world.

1010
01:03:34,345 --> 01:03:38,245
Speaker 12:  So I think it's possible that that's one, that's still one

1011
01:03:38,245 --> 01:03:41,645
Speaker 12:  way it goes. And I think there's a lot of, I've been surprised just how

1012
01:03:41,645 --> 01:03:45,365
Speaker 12:  quickly people on TikTok have been cheering that it's back and like

1013
01:03:45,635 --> 01:03:49,085
Speaker 12:  kind of taking it for granted all over again. But I think it's a very real

1014
01:03:49,085 --> 01:03:50,845
Speaker 12:  possibility that this goes away again.

1015
01:03:51,515 --> 01:03:55,445
Speaker 5:  Yeah. Yeah. It's, and it'll be even more fascinating as we get towards

1016
01:03:55,445 --> 01:03:59,285
Speaker 5:  that, that deadline in April because there is

1017
01:03:59,305 --> 01:04:02,885
Speaker 5:  now such a sense of, this was all a nonsense

1018
01:04:02,885 --> 01:04:06,765
Speaker 5:  misdirect, this isn't real. Tiktoks never gonna actually go away. Like the,

1019
01:04:06,785 --> 01:04:10,325
Speaker 5:  the we're so back vibes on TikTok are just everywhere.

1020
01:04:10,785 --> 01:04:14,765
Speaker 5:  And I think the, if it does happen and

1021
01:04:14,765 --> 01:04:18,565
Speaker 5:  if it does go away in some meaningful way in April, it's gonna weirdly,

1022
01:04:18,645 --> 01:04:21,285
Speaker 5:  I think hit even harder the second time because

1023
01:04:22,545 --> 01:04:25,885
Speaker 5:  it doesn't seem like people are leaving, all these other apps are desperately

1024
01:04:25,885 --> 01:04:29,605
Speaker 5:  trying to get people to come. Like X launched a videos

1025
01:04:29,605 --> 01:04:33,405
Speaker 5:  thing, blue Sky launched a videos thing, Instagram is out here like

1026
01:04:33,885 --> 01:04:37,725
Speaker 5:  redesigning the app and trying to pay people to come make reels like

1027
01:04:38,665 --> 01:04:41,565
Speaker 5:  all these other companies sense a moment. And as at least as far as I can

1028
01:04:41,565 --> 01:04:44,445
Speaker 5:  tell, that moment did not materialize except for the one day everybody went

1029
01:04:44,445 --> 01:04:45,485
Speaker 5:  and made jokes on Red Note.

1030
01:04:47,215 --> 01:04:51,085
Speaker 5:  Which means I think that if it comes crashing down, it will crash even harder

1031
01:04:51,185 --> 01:04:51,765
Speaker 5:  the second time.

1032
01:04:52,225 --> 01:04:56,045
Speaker 12:  Oh, totally. I think people are not prepared for the

1033
01:04:56,045 --> 01:04:59,565
Speaker 12:  possibility that it could go away again. And just like, there were such

1034
01:04:59,565 --> 01:05:03,405
Speaker 12:  short memories the first time because, and you know, I'm not blaming anyone

1035
01:05:03,425 --> 01:05:06,605
Speaker 12:  for feeling this way. 'cause it, we've heard over and over, oh, tiktoks

1036
01:05:06,605 --> 01:05:09,605
Speaker 12:  gonna be banned, tiktoks gonna be banned, and then it doesn't happen. But

1037
01:05:09,645 --> 01:05:13,365
Speaker 12:  I think it's always been a much more real threat than

1038
01:05:13,545 --> 01:05:17,365
Speaker 12:  anyone really processed. It just has been a, you know, a

1039
01:05:17,365 --> 01:05:21,245
Speaker 12:  far amount of time off. And I am really curious what Trump

1040
01:05:21,265 --> 01:05:25,125
Speaker 12:  is gonna do when those 75 days expire and more likely than

1041
01:05:25,145 --> 01:05:29,125
Speaker 12:  not, if there is any kind of deal, it's probably not gonna be

1042
01:05:29,125 --> 01:05:31,885
Speaker 12:  completed. And then does he extend it again?

1043
01:05:33,275 --> 01:05:33,565
Speaker 12:  Well,

1044
01:05:33,565 --> 01:05:35,365
Speaker 1:  He can't. He can't. That's what I mean, like Right.

1045
01:05:35,625 --> 01:05:35,845
Speaker 12:  He

1046
01:05:35,985 --> 01:05:39,875
Speaker 1:  He can't Right. But like he can write another executive order being

1047
01:05:39,875 --> 01:05:43,755
Speaker 1:  like, I hereby give you another 75 days. But triggering

1048
01:05:43,755 --> 01:05:47,505
Speaker 1:  the 90 day deal clause is, I I think that's an immediate

1049
01:05:47,815 --> 01:05:51,105
Speaker 1:  like state attorney general lawsuit. Like Tom Cotton who is a

1050
01:05:51,785 --> 01:05:54,545
Speaker 1:  Republican, like the most Republican.

1051
01:05:55,895 --> 01:05:59,025
Speaker 1:  He's like, I will, I will bring down ruinous liability in these companies.

1052
01:05:59,025 --> 01:06:02,065
Speaker 1:  Like, I want this app gone. I want China outta my country. Right. And you

1053
01:06:02,065 --> 01:06:05,665
Speaker 1:  can agree or disagree with con like strange bedfellows in the second Trump

1054
01:06:05,665 --> 01:06:06,745
Speaker 1:  administration. Right.

1055
01:06:07,485 --> 01:06:10,505
Speaker 5:  But he, and in the TikTok ban, like it's, it's useful to remember that this

1056
01:06:10,505 --> 01:06:14,345
Speaker 5:  is about as bipartisan a thing as we have had in

1057
01:06:14,345 --> 01:06:18,265
Speaker 5:  this government for a very long time. Like part of me wants to respond to

1058
01:06:18,265 --> 01:06:21,505
Speaker 5:  all of what you're saying, Eli, with you're assuming like a rational government

1059
01:06:21,505 --> 01:06:24,265
Speaker 5:  operating in a rational way and there's no evidence that that's what the

1060
01:06:24,265 --> 01:06:26,185
Speaker 5:  second Trump administration is going to be. But

1061
01:06:27,825 --> 01:06:31,615
Speaker 5:  every Republican wanted to ban TikTok and every Democrat wanted to.

1062
01:06:31,715 --> 01:06:35,335
Speaker 5:  So it's, it's really like, it, it's not, it's not a

1063
01:06:35,645 --> 01:06:38,815
Speaker 5:  sort of swinging back in the new administration. Like

1064
01:06:39,125 --> 01:06:42,255
Speaker 5:  everybody in Congress voted for this already.

1065
01:06:42,445 --> 01:06:46,215
Speaker 1:  Wait, let me just, let me just offer you, this is a Lauren Finer quick post.

1066
01:06:46,315 --> 01:06:50,175
Speaker 1:  I'm just gonna read it to you. The repeal TikTok Ban Act was introduced

1067
01:06:50,715 --> 01:06:54,415
Speaker 1:  by Senator Rand Paul and Representative Rohan

1068
01:06:55,175 --> 01:06:56,295
Speaker 5:  Speaking of strange bedfellows

1069
01:06:56,325 --> 01:06:59,215
Speaker 1:  That it's weird. That is a weird combo platter of people.

1070
01:06:59,645 --> 01:07:03,335
Speaker 12:  Yeah. Yeah. And I think another kind of weird combination

1071
01:07:03,355 --> 01:07:06,615
Speaker 12:  that's coming up on the other side of things is like, people who actually

1072
01:07:06,725 --> 01:07:09,855
Speaker 12:  supported TikTok in their case, who wrote

1073
01:07:10,315 --> 01:07:14,215
Speaker 12:  amicus briefs are now saying like, yeah, we don't like this.

1074
01:07:14,275 --> 01:07:17,575
Speaker 12:  But also it's like generally bad to just like, ignore

1075
01:07:18,175 --> 01:07:20,615
Speaker 12:  a law and just like not follow the process.

1076
01:07:22,365 --> 01:07:25,615
Speaker 5:  Yeah. But seem like if you're in Congress, you should be pro laws general.

1077
01:07:25,685 --> 01:07:29,015
Speaker 5:  Like that's like one of their main things is doing laws. No,

1078
01:07:29,015 --> 01:07:32,375
Speaker 1:  But like, it's, it's the EFF that wrote that one. It's like one of the major

1079
01:07:32,375 --> 01:07:36,255
Speaker 1:  digital civil liberties groups is like, this law is bad. The

1080
01:07:36,255 --> 01:07:40,175
Speaker 1:  Supreme Court is bad, all this is bad. But then having a

1081
01:07:40,175 --> 01:07:43,775
Speaker 1:  law that the president can be like, that law doesn't exist equally bad.

1082
01:07:45,785 --> 01:07:49,695
Speaker 12:  Right. And I think people are like banking a lot on Congress just

1083
01:07:49,695 --> 01:07:53,495
Speaker 12:  like realizing like, this is such a mess. But at the same time,

1084
01:07:53,565 --> 01:07:57,455
Speaker 12:  like hundreds of lawmakers voted for this in the first place.

1085
01:07:57,595 --> 01:08:00,815
Speaker 12:  And you know, these are people who have egos. They don't wanna just like

1086
01:08:01,245 --> 01:08:04,815
Speaker 12:  undo their own work. So I think like

1087
01:08:05,245 --> 01:08:09,055
Speaker 12:  relying on something like that happening even after all this is like a,

1088
01:08:09,415 --> 01:08:11,135
Speaker 12:  a really big leap of faith.

1089
01:08:11,765 --> 01:08:14,615
Speaker 1:  Okay. I just wanna say this literally for the tape.

1090
01:08:16,645 --> 01:08:20,305
Speaker 1:  The United States government owning a social network is one of the stupidest

1091
01:08:20,305 --> 01:08:24,185
Speaker 1:  first amendment, first year law school hypotheticals I

1092
01:08:24,185 --> 01:08:28,105
Speaker 1:  can possibly come up with. It will, it will not be possible to

1093
01:08:28,105 --> 01:08:31,825
Speaker 1:  run a social network if the government owns it in this country. Especially

1094
01:08:31,825 --> 01:08:35,705
Speaker 1:  with Trump doing his like big EO that's like, I've circled the

1095
01:08:35,705 --> 01:08:36,145
Speaker 1:  First Amendment,

1096
01:08:37,845 --> 01:08:40,145
Speaker 1:  you know, what's legal under the First Amendment spam?

1097
01:08:41,905 --> 01:08:45,875
Speaker 1:  Just straightforwardly spam is legal under the First Amendment. So the

1098
01:08:45,875 --> 01:08:48,995
Speaker 1:  government can't filter spam on the TikTok it owns,

1099
01:08:49,635 --> 01:08:52,555
Speaker 1:  pretty much brings an end to TikTok. I'm gonna go

1100
01:08:52,795 --> 01:08:54,035
Speaker 13:  Ham on this social network.

1101
01:08:54,535 --> 01:08:54,755
Speaker 1:  I'm

1102
01:08:54,755 --> 01:08:57,035
Speaker 13:  Just, you, you have, you don't even know what I'm capable of

1103
01:08:57,415 --> 01:09:00,035
Speaker 1:  On the government, on social, there's a whole bunch of other stuff. TikTok

1104
01:09:00,495 --> 01:09:03,915
Speaker 1:  the, by the way, the most moderated social network, right? The social network

1105
01:09:03,985 --> 01:09:07,835
Speaker 1:  that gave the world the word segs because people are afraid

1106
01:09:07,835 --> 01:09:10,435
Speaker 1:  to say sex. That's TikTok.

1107
01:09:12,015 --> 01:09:15,555
Speaker 1:  You know, what is, and because there's so many kids on it, do you know what

1108
01:09:15,555 --> 01:09:19,315
Speaker 1:  is legal on the First Amendment? Just pornography, all kinds.

1109
01:09:20,035 --> 01:09:23,565
Speaker 1:  It's available. The United States government cannot, they would have to come

1110
01:09:23,565 --> 01:09:27,335
Speaker 1:  up with some straightforward, like digital

1111
01:09:27,335 --> 01:09:31,095
Speaker 1:  version of a time place and manner restriction. This is all like very technical

1112
01:09:31,335 --> 01:09:34,895
Speaker 1:  First Amendment lawyering that doesn't exist if the government wants to own

1113
01:09:34,895 --> 01:09:38,375
Speaker 1:  a social network and filter anything, and it probably can't.

1114
01:09:38,835 --> 01:09:42,655
Speaker 1:  So I'm just like that. I'll just set that aside. We, it

1115
01:09:42,655 --> 01:09:45,775
Speaker 1:  hasn't happened yet. I don't know if it's gonna happen. But someone should

1116
01:09:45,775 --> 01:09:49,415
Speaker 1:  explain to Donald Trump that the, the government taking interest in TikTok

1117
01:09:49,475 --> 01:09:52,815
Speaker 1:  in this way would actually result in the destruction of TikTok.

1118
01:09:53,135 --> 01:09:56,255
Speaker 12:  I think that actually kind of goes to the idea that I think

1119
01:09:56,925 --> 01:10:00,215
Speaker 12:  like TikTok users are just expecting now that

1120
01:10:00,925 --> 01:10:04,055
Speaker 12:  tiktoks back, it's gonna stick around, it's gonna be kind of more or less

1121
01:10:04,055 --> 01:10:07,615
Speaker 12:  like it's always been. And like under a lot of these

1122
01:10:08,015 --> 01:10:11,935
Speaker 12:  scenarios it would look very different. Like if the US government

1123
01:10:12,005 --> 01:10:15,815
Speaker 12:  owns it and can't moderate content, if like Elon Musk owns it. And

1124
01:10:15,815 --> 01:10:19,415
Speaker 12:  I've seen people commenting If, you know, if Musk buys this, I'm leaving

1125
01:10:19,595 --> 01:10:23,575
Speaker 12:  TikTok, which who knows if people actually would, but you know, it's not

1126
01:10:23,575 --> 01:10:27,135
Speaker 12:  gonna be most likely the same TikTok that

1127
01:10:27,155 --> 01:10:28,135
Speaker 12:  you're using today.

1128
01:10:28,565 --> 01:10:30,535
Speaker 1:  Just to put this in perspective for folks

1129
01:10:32,845 --> 01:10:36,745
Speaker 1:  in what, 2017 after Trump, Trump won became president, he

1130
01:10:36,745 --> 01:10:40,065
Speaker 1:  blocked people from his Twitter account and that was a First Amendment lawsuit

1131
01:10:40,575 --> 01:10:44,505
Speaker 1:  that went for Oh right. Five years. Like just that, like can

1132
01:10:44,505 --> 01:10:48,475
Speaker 1:  the president block people was a Supreme Court level

1133
01:10:48,475 --> 01:10:51,235
Speaker 1:  First Amendment challenge and eventually vacated. 'cause he, he lost, right?

1134
01:10:51,235 --> 01:10:54,315
Speaker 1:  Like he, he lost the election. He wasn't president anymore and he gave up.

1135
01:10:54,815 --> 01:10:58,535
Speaker 1:  But like that is the amount of lawyer just on the block button

1136
01:10:58,535 --> 01:11:01,175
Speaker 1:  alone. That was the amount of lawyering that occurred about the First amendment

1137
01:11:01,175 --> 01:11:04,015
Speaker 1:  in social networks. Can the president block someone from his official account?

1138
01:11:04,715 --> 01:11:08,135
Speaker 1:  The government owns a social network as like a nuclear

1139
01:11:08,225 --> 01:11:12,115
Speaker 1:  Armageddon of First Amendment issues. It's, it's just such a

1140
01:11:12,115 --> 01:11:13,245
Speaker 1:  deeply bad idea. All right.

1141
01:11:13,245 --> 01:11:16,805
Speaker 5:  You've talked me into it. Let's do it. I'm ready. Joint venture, 50% China,

1142
01:11:16,815 --> 01:11:19,405
Speaker 5:  50% us. Let's go. Alright,

1143
01:11:19,505 --> 01:11:23,325
Speaker 1:  I'm in. Alright. That's enough to, we'll see what happens, right?

1144
01:11:23,325 --> 01:11:26,525
Speaker 1:  Yeah. We got April. Yeah, a April. April is upon us already.

1145
01:11:27,945 --> 01:11:31,325
Speaker 1:  But it just, it all, everything about the Trump administration, like it comes

1146
01:11:31,525 --> 01:11:34,365
Speaker 1:  together in the TikTok. Like why is Larry Sen in, in the White House talking

1147
01:11:34,365 --> 01:11:38,275
Speaker 1:  about a data center project? 'cause he's running TikTok and

1148
01:11:38,275 --> 01:11:42,195
Speaker 1:  like he, the TikTok CEO is running around the inauguration being like,

1149
01:11:42,195 --> 01:11:45,995
Speaker 1:  don't ban me please. It's a lot. There's

1150
01:11:45,995 --> 01:11:49,595
Speaker 1:  other stuff. It's, it's a lot. We're gonna end up covering it.

1151
01:11:49,645 --> 01:11:53,315
Speaker 1:  We're gonna find ways to cover it here on The Vergecast and on the site.

1152
01:11:53,375 --> 01:11:57,035
Speaker 1:  In, in ways that split up the beginning of any presidential

1153
01:11:57,035 --> 01:12:00,035
Speaker 1:  administration, as David was saying, is a flurry of activity.

1154
01:12:00,985 --> 01:12:04,255
Speaker 1:  Trump administrations are typically very chaotic activity the whole time,

1155
01:12:05,635 --> 01:12:09,455
Speaker 1:  as we know from the first one. Poor Lauren, I want to hear from everybody

1156
01:12:09,645 --> 01:12:13,375
Speaker 1:  like how you want us to handle it. Like, again, in Trump one, we spent a

1157
01:12:13,375 --> 01:12:16,335
Speaker 1:  lot of time talking about Foxcon because Foxcon was just such a big symbol

1158
01:12:16,395 --> 01:12:19,695
Speaker 1:  of a lot of things all at once. This time

1159
01:12:20,285 --> 01:12:23,775
Speaker 1:  there's a bunch of climate science impacts. There's a bunch of EV impacts.

1160
01:12:24,285 --> 01:12:28,215
Speaker 1:  Brendan Carr is in charge of the FCC. You'll be hearing this man's name a

1161
01:12:28,215 --> 01:12:32,135
Speaker 1:  lot on this show. He wants to censor the internet. He

1162
01:12:32,135 --> 01:12:33,215
Speaker 1:  just straightforwardly does.

1163
01:12:34,925 --> 01:12:38,295
Speaker 1:  There's, as Lauren was saying, the birthright citizenship question, which

1164
01:12:38,335 --> 01:12:41,615
Speaker 1:  a lot of our listeners and readers care about. 'cause they are H one B visa

1165
01:12:41,815 --> 01:12:44,095
Speaker 1:  holders in the United States of America work tech companies. And whether

1166
01:12:44,095 --> 01:12:48,055
Speaker 1:  or not they're children, our citizens is a pretty meaningful question that

1167
01:12:48,055 --> 01:12:50,935
Speaker 1:  I I think we we're gonna end up talking about. And then there's tariffs.

1168
01:12:51,535 --> 01:12:55,415
Speaker 1:  Trump announced what, 10% tariffs on China, Lauren? Yeah. Or he at least

1169
01:12:55,415 --> 01:12:58,575
Speaker 1:  floated it. Sure. So everything is made in China.

1170
01:12:59,285 --> 01:13:02,175
Speaker 1:  Like how will Apple deal with this? What trade will Apple make

1171
01:13:03,155 --> 01:13:06,615
Speaker 1:  to reduce tariffs on the iPhone? Tim Cook managed it the last time.

1172
01:13:07,065 --> 01:13:10,295
Speaker 1:  We're gonna end up having to talk about it because at the same time, I think

1173
01:13:10,295 --> 01:13:14,135
Speaker 1:  it was today, Trump gave a speech where he was like, these

1174
01:13:14,135 --> 01:13:17,935
Speaker 1:  cases they're losing the eu. That's a i'm that's a tax on our great American

1175
01:13:18,175 --> 01:13:22,055
Speaker 1:  companies. That's a lot. That's just, that's a lot of moving pieces

1176
01:13:22,055 --> 01:13:23,935
Speaker 1:  that all like literally he's talking about Apple.

1177
01:13:25,605 --> 01:13:29,265
Speaker 1:  So I wanna hear from all of you again. I, I think

1178
01:13:29,525 --> 01:13:31,945
Speaker 1:  If you want to call us and send us a note about how you want us discover

1179
01:13:31,945 --> 01:13:34,785
Speaker 1:  Trump, I'd like you to begin that note with a formal apology to Lauren.

1180
01:13:36,145 --> 01:13:38,945
Speaker 1:  I think I might might be nice to just send her all those, but I, we have

1181
01:13:38,945 --> 01:13:42,825
Speaker 1:  to strike a balance here. There's a lot. It's gonna be really noisy and

1182
01:13:42,985 --> 01:13:46,865
Speaker 1:  you know, our goal is to always do good rigorous journalism and also to make

1183
01:13:46,865 --> 01:13:50,145
Speaker 1:  sure like we're pointed at places that matter, not just all of the noise.

1184
01:13:50,235 --> 01:13:53,265
Speaker 1:  Right. Because If you do all the noise, you're just gonna burn out. But that

1185
01:13:53,825 --> 01:13:56,555
Speaker 1:  Yeah, I rem like it was eight years ago. It's a long time ago, but it wasn't

1186
01:13:56,555 --> 01:13:59,835
Speaker 1:  so long ago and I wanna make sure we get it right. So let us know.

1187
01:14:00,685 --> 01:14:03,995
Speaker 1:  We're obviously gonna keep covering the TikTok band, but there's just a whole

1188
01:14:03,995 --> 01:14:06,155
Speaker 1:  bunch of other stuff happening. So let us know what you think is important.

1189
01:14:06,385 --> 01:14:07,795
Speaker 5:  Neil, do you still think it's gonna be Walmart?

1190
01:14:08,825 --> 01:14:10,125
Speaker 1:  Oh, the Vice TikTok? Yeah,

1191
01:14:12,435 --> 01:14:14,565
Speaker 5:  It's got like 71 days to pull it off.

1192
01:14:16,795 --> 01:14:20,445
Speaker 1:  There's the Project Liberty with Frank McCourt, which I think is really interesting.

1193
01:14:20,545 --> 01:14:21,965
Speaker 5:  And Mr. Beast now apparently.

1194
01:14:21,965 --> 01:14:23,485
Speaker 1:  Oh, is Mr. Beast in that one? Apparently

1195
01:14:23,515 --> 01:14:24,005
Speaker 5:  He's part of it.

1196
01:14:24,145 --> 01:14:27,285
Speaker 1:  Is he part of that one? That one's interesting just because I think on a

1197
01:14:27,285 --> 01:14:30,125
Speaker 1:  technical level they're interested in like Federating TikTok.

1198
01:14:31,165 --> 01:14:34,695
Speaker 1:  I have no idea what that means. Right. But he's like, we are gonna rebuild

1199
01:14:34,695 --> 01:14:37,855
Speaker 1:  the tech stock and give you your own algorithms and each, it's like federation

1200
01:14:37,995 --> 01:14:38,415
Speaker 1:  noises.

1201
01:14:39,965 --> 01:14:43,935
Speaker 1:  Okay. I'm reading for that one. Like that's as, as much as I can

1202
01:14:44,445 --> 01:14:47,615
Speaker 1:  pick anything that that seems that one has Mr. Wonderful. It's just like,

1203
01:14:47,735 --> 01:14:50,935
Speaker 1:  there's just characters just like opening doors and being like, I'll buy

1204
01:14:50,955 --> 01:14:54,935
Speaker 1:  TikTok. Like I think Amazon's in the mix. I think Jeff Bezos is

1205
01:14:54,935 --> 01:14:56,055
Speaker 1:  sitting on that stage.

1206
01:14:57,845 --> 01:15:01,015
Speaker 1:  Yeah, IIII if I were Walmart, I would be

1207
01:15:01,605 --> 01:15:04,295
Speaker 1:  They were Lauren weren't they in the mix the first time? It was like Microsoft,

1208
01:15:04,295 --> 01:15:06,215
Speaker 1:  Oracle and Walmart were like a consortium, right?

1209
01:15:07,045 --> 01:15:10,855
Speaker 12:  They were, yeah. Yeah. I think the Mr. Beast one might be separate

1210
01:15:10,925 --> 01:15:14,575
Speaker 12:  from Project Liberty. So just a bunch of, bunch of different

1211
01:15:14,575 --> 01:15:15,495
Speaker 5:  Characters. No, they, they at least were talking.

1212
01:15:15,555 --> 01:15:16,335
Speaker 12:  Are they the same? Okay.

1213
01:15:16,495 --> 01:15:19,775
Speaker 5:  I don't know if they're like technically in it, but they, they, they, they

1214
01:15:19,775 --> 01:15:20,615
Speaker 5:  had a confab.

1215
01:15:21,085 --> 01:15:21,895
Speaker 12:  Okay. As, as,

1216
01:15:21,955 --> 01:15:25,935
Speaker 5:  As least you know, maybe there was like lunch lease stock involved

1217
01:15:26,035 --> 01:15:29,975
Speaker 5:  in the, I don't know. I don't know all the details, but I'll do it. Mrs.

1218
01:15:29,975 --> 01:15:32,055
Speaker 5:  Beast and Wonderful are both part of this somehow.

1219
01:15:33,685 --> 01:15:35,375
Speaker 1:  None of it's good. None of it's good.

1220
01:15:36,885 --> 01:15:37,375
Speaker 1:  Yeah, I

1221
01:15:37,575 --> 01:15:40,415
Speaker 5:  I think they don't sell and I think it actually goes dark in April in, in

1222
01:15:40,415 --> 01:15:43,295
Speaker 5:  a way. I agree. That's, that's my current stake in the ground.

1223
01:15:45,015 --> 01:15:48,855
Speaker 12:  I agree. I think there's a possibility that some like crazy

1224
01:15:48,925 --> 01:15:52,815
Speaker 12:  deal comes out of nowhere after it goes dark eventually.

1225
01:15:53,435 --> 01:15:57,055
Speaker 12:  But I do think I, if I had to predict now it does go dark after

1226
01:15:57,355 --> 01:15:58,295
Speaker 12:  the 75 days.

1227
01:15:59,305 --> 01:16:01,875
Speaker 1:  Yeah. I just, I I wanna believe you. I also know that

1228
01:16:02,855 --> 01:16:06,795
Speaker 1:  Donald Trump wants nothing more in his life than to hand a novelty permit

1229
01:16:06,815 --> 01:16:09,475
Speaker 1:  to someone like a, like a big red ribbon.

1230
01:16:09,645 --> 01:16:11,435
Speaker 5:  There will be big scissors for sure.

1231
01:16:11,615 --> 01:16:15,115
Speaker 1:  You know what I'm saying? Yeah. So I, I'm betting on novelty permit. I don't

1232
01:16:15,115 --> 01:16:17,995
Speaker 1:  know who the buyer will be or whether that will actually be effective under

1233
01:16:17,995 --> 01:16:21,275
Speaker 1:  the law. The novelty permit is happening. Alright, we gotta take a break.

1234
01:16:21,815 --> 01:16:25,355
Speaker 1:  Lauren, thank you so much for joining us again. We're very sorry. We'll have

1235
01:16:25,355 --> 01:16:26,075
Speaker 1:  you back very soon.

1236
01:16:26,365 --> 01:16:27,195
Speaker 12:  Thank you so much.

1237
01:17:59,665 --> 01:17:59,885
Speaker 16:  All

1238
01:17:59,885 --> 01:18:03,685
Speaker 1:  Right, we're back. We're already over. It's, it's, we're

1239
01:18:03,895 --> 01:18:05,445
Speaker 1:  again, we, we gotta figure this out.

1240
01:18:05,805 --> 01:18:09,565
Speaker 5:  I don't think every week is going to be like this, but I think we

1241
01:18:09,595 --> 01:18:13,205
Speaker 5:  also kind of spent four years saying that the last time Trump was

1242
01:18:13,505 --> 01:18:17,315
Speaker 5:  in the White House. So I don't know. I'm

1243
01:18:17,315 --> 01:18:17,515
Speaker 5:  tired

1244
01:18:17,895 --> 01:18:21,155
Speaker 1:  The last time this happened, and again, I I recognize this was a long time

1245
01:18:21,175 --> 01:18:25,155
Speaker 1:  ago. We got to the point on the show where I was getting angry

1246
01:18:25,155 --> 01:18:28,155
Speaker 1:  anonymous letters from Foxconn employees telling me to leave the company

1247
01:18:28,155 --> 01:18:31,595
Speaker 1:  alone. And then our, our listeners were turning those letters into songs.

1248
01:18:32,895 --> 01:18:35,915
Speaker 1:  So I just wanna set know

1249
01:18:35,915 --> 01:18:37,475
Speaker 5:  We we have a long road to go. That's what you're saying.

1250
01:18:37,505 --> 01:18:41,155
Speaker 1:  It's like no one's broken out a guitar yet. You know, like we're, it's a

1251
01:18:41,155 --> 01:18:44,795
Speaker 1:  long way. All right. Lightning round unsponsored.

1252
01:18:45,535 --> 01:18:46,955
Speaker 1:  You know, we're starting here off slow.

1253
01:18:48,625 --> 01:18:48,915
Speaker 1:  It's

1254
01:18:48,955 --> 01:18:49,275
Speaker 5:  Q1,

1255
01:18:49,345 --> 01:18:52,955
Speaker 1:  It's, it's q1, it's way goes. Everyone blew their extra

1256
01:18:53,235 --> 01:18:56,795
Speaker 1:  Q4 budgets sponsoring lightning round in 2024.

1257
01:18:57,475 --> 01:19:00,595
Speaker 1:  And and here we are in 25 where it's gonna happen again though. We got some

1258
01:19:00,795 --> 01:19:04,675
Speaker 1:  ideas. All right. Netflix is raising prices again. I said it was a

1259
01:19:04,675 --> 01:19:07,755
Speaker 1:  lightning round, but I think David is best to talk for one full hour. I just,

1260
01:19:07,985 --> 01:19:09,995
Speaker 5:  It's, this is the most

1261
01:19:11,585 --> 01:19:15,235
Speaker 5:  like mafia thing that has happened this week. And that's a pretty high bar.

1262
01:19:16,225 --> 01:19:17,355
Speaker 1:  Like, so the,

1263
01:19:17,535 --> 01:19:20,115
Speaker 5:  The thing that happened in the last

1264
01:19:21,195 --> 01:19:25,025
Speaker 5:  month, really, and I think, I think this is even more true than I realized,

1265
01:19:25,465 --> 01:19:29,425
Speaker 5:  I I spent some time with some Netflix folks talking about how they're thinking

1266
01:19:29,425 --> 01:19:32,905
Speaker 5:  about live ahead of the, the football game kind of between the

1267
01:19:32,905 --> 01:19:35,985
Speaker 5:  Paul Tyson fight and the Christmas Day football games. I spent some time

1268
01:19:35,985 --> 01:19:39,585
Speaker 5:  with the NFL folks or, but the Netflix folks wrote a story about it and came

1269
01:19:39,585 --> 01:19:42,745
Speaker 5:  away from it being like, okay, they're, they're really committed to this

1270
01:19:42,745 --> 01:19:46,665
Speaker 5:  like always on live programming. What I didn't realize is Netflix is

1271
01:19:46,665 --> 01:19:49,145
Speaker 5:  pretty sure it has already won television.

1272
01:19:50,415 --> 01:19:54,305
Speaker 5:  They, they have proven now that this sports thing can work and drives

1273
01:19:54,305 --> 01:19:58,145
Speaker 5:  real audience. They've proven that they can manufacture huge

1274
01:19:58,145 --> 01:20:02,065
Speaker 5:  events out of nowhere, essentially. Like they had a YouTuber fight an old

1275
01:20:02,125 --> 01:20:05,825
Speaker 5:  man and 65 million people watched like just on his face.

1276
01:20:05,855 --> 01:20:09,485
Speaker 5:  That is nuts. And that's not a thing you can do infinitely,

1277
01:20:09,705 --> 01:20:13,205
Speaker 5:  but it is a thing that Netflix continues to prove it can do whenever it wants.

1278
01:20:13,545 --> 01:20:17,485
Speaker 5:  And so now if you're Netflix you say, okay, we have a bunch of big hit shows

1279
01:20:17,515 --> 01:20:20,245
Speaker 5:  that people like 2024 was kind of a down year

1280
01:20:20,305 --> 01:20:22,525
Speaker 1:  For those shows. And I, stranger

1281
01:20:22,525 --> 01:20:26,485
Speaker 5:  Things is huge. Wednesday is huge and coming back, there's a

1282
01:20:26,485 --> 01:20:27,845
Speaker 5:  third one that I can't remember the name of.

1283
01:20:27,955 --> 01:20:30,125
Speaker 1:  Sure. There you go. Keep going. I think

1284
01:20:30,125 --> 01:20:31,765
Speaker 5:  Ryan Reynolds is probably doing stuff.

1285
01:20:32,555 --> 01:20:36,525
Speaker 1:  None of those count, those are all, those are all so wait list as

1286
01:20:36,525 --> 01:20:37,085
Speaker 1:  to not exist.

1287
01:20:37,405 --> 01:20:40,765
Speaker 5:  I think Stranger Things is actually the only one I would, I would put like

1288
01:20:41,065 --> 01:20:44,925
Speaker 5:  in the it in on the list of like biggest things on television.

1289
01:20:45,005 --> 01:20:48,645
Speaker 5:  I think Stranger Things belongs there. Sure. But what Netflix actually has

1290
01:20:48,745 --> 01:20:52,685
Speaker 5:  is just an infinite library of c plus stuff

1291
01:20:52,685 --> 01:20:55,805
Speaker 5:  that people will watch, which actually turns out to be very important because

1292
01:20:56,145 --> 01:21:00,085
Speaker 5:  that's most of what people watch. Yes. Like I will watch as

1293
01:21:00,085 --> 01:21:03,045
Speaker 5:  many seasons of the ultimatum as you wanna watch, as you wanna make Netflix.

1294
01:21:03,155 --> 01:21:06,885
Speaker 5:  Like sure I'm into it. But, so anyway, so Netflix is now in this

1295
01:21:07,045 --> 01:21:10,885
Speaker 5:  position where like it is making money in a time where it's very hard for

1296
01:21:10,885 --> 01:21:13,605
Speaker 5:  entertainment companies to make money. It is more powerful than ever. It's

1297
01:21:13,605 --> 01:21:17,485
Speaker 5:  driving more audience than ever. It is just winning. And so what

1298
01:21:17,485 --> 01:21:21,405
Speaker 5:  they used to do is like make these grand cases for why they had to,

1299
01:21:22,025 --> 01:21:25,645
Speaker 5:  you know, raise the prices because they wanted to reinvest in amazing tech

1300
01:21:25,665 --> 01:21:28,845
Speaker 5:  and now they're just like, yeah, we're raising the prices. It went up a dollar,

1301
01:21:28,845 --> 01:21:29,565
Speaker 5:  whatcha gonna do about

1302
01:21:29,565 --> 01:21:32,645
Speaker 1:  It. I have two things to say about this. One, it occurs to me that you have

1303
01:21:33,245 --> 01:21:36,565
Speaker 1:  a garbage TV and you watch garbage tv. Correct. And I haven't,

1304
01:21:37,935 --> 01:21:40,325
Speaker 1:  those things, haven't those puzzle pieces never been before.

1305
01:21:40,325 --> 01:21:41,605
Speaker 5:  We can't all have Bravia Core

1306
01:21:41,735 --> 01:21:44,045
Speaker 1:  Patel. Right? That's what I'm saying. Yeah. I'm like, I'm gonna watch one

1307
01:21:44,045 --> 01:21:47,965
Speaker 1:  movie a week at 75 glorious megabits of streaming. Right.

1308
01:21:48,025 --> 01:21:51,565
Speaker 1:  And fun. I'm, I'm, I'm worried about you, but it finally clicked

1309
01:21:51,565 --> 01:21:54,445
Speaker 5:  Into me. I, me, I turn off steel or Node islands to watch to make this podcast

1310
01:21:54,625 --> 01:21:55,125
Speaker 5:  and that's it.

1311
01:21:57,635 --> 01:21:59,765
Speaker 1:  It's just generally worried about you.

1312
01:22:02,225 --> 01:22:06,125
Speaker 1:  The other thing is, I think Casey posted this on

1313
01:22:06,125 --> 01:22:09,525
Speaker 1:  Blue Sky this week. He's like, Netflix had better content when it costs 7 99.

1314
01:22:10,445 --> 01:22:13,415
Speaker 1:  Like, and that I think it's straightforwardly true. When they were playing

1315
01:22:13,415 --> 01:22:17,135
Speaker 1:  the big prestige TV game, they were

1316
01:22:17,215 --> 01:22:20,975
Speaker 1:  making better stuff when they're like, we're HBO, like, and now

1317
01:22:21,235 --> 01:22:23,335
Speaker 1:  the quality has just dropped.

1318
01:22:23,925 --> 01:22:27,255
Speaker 5:  Sure. But you know, what happened in the interim is Netflix went from losing

1319
01:22:27,255 --> 01:22:30,935
Speaker 5:  billions of dollars to making billions of dollars. Which it turns out If

1320
01:22:30,935 --> 01:22:34,655
Speaker 5:  you Netflix is a pretty good trade, we made our content

1321
01:22:34,705 --> 01:22:38,535
Speaker 5:  worse and our money bigger is is the thing they are happy with.

1322
01:22:38,835 --> 01:22:42,255
Speaker 5:  But the other thing is there's this big macro trend

1323
01:22:42,845 --> 01:22:46,515
Speaker 5:  away from premium high res

1324
01:22:46,865 --> 01:22:50,715
Speaker 5:  streaming to ads. And, and that that is the push here.

1325
01:22:50,715 --> 01:22:54,595
Speaker 5:  And what Netflix is doing is a thing that I have been saying for a while

1326
01:22:54,595 --> 01:22:58,035
Speaker 5:  that it was doing it is just going to price you out.

1327
01:22:58,465 --> 01:23:02,355
Speaker 5:  Like it it is, it went from, so the ad supported tier, which is now

1328
01:23:02,355 --> 01:23:06,125
Speaker 5:  the base tier of Netflix went from 6 99 a month to 7 99 a month. A dollar

1329
01:23:06,165 --> 01:23:09,925
Speaker 5:  a month is a lot, but whatever the standard

1330
01:23:10,305 --> 01:23:14,005
Speaker 5:  ad free tier, which is standard Netflix, which used to be 7 99 a month,

1331
01:23:14,355 --> 01:23:18,125
Speaker 5:  went from 1549 to 1799 a month. That's a $2 and

1332
01:23:18,125 --> 01:23:22,085
Speaker 5:  50 cent increase. That's a big number. Yeah. And then the premium tier,

1333
01:23:22,135 --> 01:23:25,685
Speaker 5:  which I think is how you get 4K, I don't know, my TV doesn't do any of that

1334
01:23:25,685 --> 01:23:29,365
Speaker 5:  stuff, So I don't even know. It's horrible. Went from 2299 to

1335
01:23:29,365 --> 01:23:33,245
Speaker 5:  24 9 a month. So to have the best possible Netflix experience, just

1336
01:23:33,245 --> 01:23:36,005
Speaker 5:  Netflix is $25 a month. Netflix

1337
01:23:36,955 --> 01:23:40,245
Speaker 5:  does not want you to pay that. Like, I, I cannot be clear about this. Netflix

1338
01:23:40,385 --> 01:23:44,365
Speaker 5:  is trying to price you out of every tier except the ads

1339
01:23:44,395 --> 01:23:48,245
Speaker 5:  ones. And it is going to keep raising those prices as fast as it can

1340
01:23:48,295 --> 01:23:52,165
Speaker 5:  until it does. So, because it makes more money when you do ads and the

1341
01:23:52,165 --> 01:23:55,925
Speaker 5:  more people go to ads, the bigger a scale advertising business Netflix can

1342
01:23:55,925 --> 01:23:59,125
Speaker 5:  build, which is what it needs to go after, who it really wants to be, which

1343
01:23:59,125 --> 01:24:03,055
Speaker 5:  is YouTube. That's where the, those two companies are

1344
01:24:03,055 --> 01:24:06,215
Speaker 5:  on like a collision course with each other. And Netflix needs to get bigger

1345
01:24:06,555 --> 01:24:09,055
Speaker 5:  in order to like really try to play YouTube's

1346
01:24:09,055 --> 01:24:12,335
Speaker 1:  Game. Yeah. And YouTube's growth is on television, which they've said over

1347
01:24:12,335 --> 01:24:15,335
Speaker 1:  and over again to you, to me, to anybody who will listen. Right. We're growing

1348
01:24:15,335 --> 01:24:16,495
Speaker 1:  faster on smart TVs

1349
01:24:16,655 --> 01:24:16,935
Speaker 5:  Anywhere else.

1350
01:24:17,805 --> 01:24:21,775
Speaker 1:  This actually hilariously Netflix raised prices

1351
01:24:21,875 --> 01:24:25,855
Speaker 1:  by $2. YouTube announced this week a price decreased by

1352
01:24:25,855 --> 01:24:29,535
Speaker 1:  $2 ish, but only, but only

1353
01:24:29,675 --> 01:24:33,255
Speaker 1:  If you also buy Google one, which you have to pay more money for. And it's

1354
01:24:33,255 --> 01:24:35,655
Speaker 1:  like, this is the one of the saddest announcements of all time. And the way

1355
01:24:35,655 --> 01:24:38,615
Speaker 1:  that I know it's one of the saddest is in the press release, they didn't

1356
01:24:38,615 --> 01:24:42,495
Speaker 1:  say how much the bundle would be cheaper. And we had to go ask them

1357
01:24:42,875 --> 01:24:45,495
Speaker 1:  and they were like, it's $2. And we're like, all right, it's $2.

1358
01:24:46,165 --> 01:24:48,055
Speaker 5:  Cool, I'll take two bucks. Like sure.

1359
01:24:48,975 --> 01:24:52,135
Speaker 1:  They don't ask so much of other features. You get higher quality audio streaming,

1360
01:24:52,135 --> 01:24:55,375
Speaker 1:  they're gonna let you do picture and picture. They basically, you can run

1361
01:24:55,375 --> 01:24:58,495
Speaker 1:  like their two of their beta features at once. Now, you know, wait,

1362
01:24:58,495 --> 01:25:01,575
Speaker 5:  Can I just say my favorite thing about this is one of the experimental features

1363
01:25:01,575 --> 01:25:05,375
Speaker 5:  that they have is you can automatically download recommended

1364
01:25:05,375 --> 01:25:09,175
Speaker 5:  YouTube shorts to watch offline, which is like,

1365
01:25:09,175 --> 01:25:13,135
Speaker 5:  if I were to make you a list of features that YouTube users do not

1366
01:25:13,165 --> 01:25:16,175
Speaker 5:  want download shorts to my phone without

1367
01:25:16,175 --> 01:25:19,055
Speaker 1:  Asking me just like fill up my storage YouTube shorts.

1368
01:25:20,405 --> 01:25:24,055
Speaker 5:  Yeah. Everybody wants a thousand YouTube shorts just taking up space on their

1369
01:25:24,055 --> 01:25:26,175
Speaker 5:  phone. That's definitely what YouTube users are asking for.

1370
01:25:26,695 --> 01:25:30,335
Speaker 1:  I guess if I could tell one of those platforms, like my subway

1371
01:25:30,445 --> 01:25:33,975
Speaker 1:  ride from Grand Central to the office is about 22 minutes. Like

1372
01:25:34,445 --> 01:25:38,335
Speaker 1:  just make sure I have 22 minutes of garbage to watch. You know,

1373
01:25:40,165 --> 01:25:42,175
Speaker 1:  like I could see it

1374
01:25:42,795 --> 01:25:46,375
Speaker 5:  If there was a button called download garbage, I would take that

1375
01:25:46,765 --> 01:25:47,975
Speaker 5:  like I have,

1376
01:25:47,975 --> 01:25:51,455
Speaker 1:  But no, it would've to be like a dial. Like I, I need about

1377
01:25:51,455 --> 01:25:53,215
Speaker 1:  25 minutes of garbage. How

1378
01:25:53,215 --> 01:25:55,415
Speaker 5:  Much time do you have to kill? You just spin the tile

1379
01:25:55,875 --> 01:25:59,655
Speaker 1:  And you like, just, just make sure it's here. All right. Like I don't, it

1380
01:25:59,655 --> 01:26:03,525
Speaker 1:  should actually, it should be like a system wide setting that all of the

1381
01:26:03,525 --> 01:26:07,505
Speaker 1:  social garbage apps can feed into. So you're like, you know,

1382
01:26:07,565 --> 01:26:11,145
Speaker 1:  I'm gonna bounce between like Instagram and TikTok and YouTube, like

1383
01:26:11,375 --> 01:26:14,465
Speaker 1:  just between the three of you. I'm giving you about 25 minutes of garbage

1384
01:26:14,465 --> 01:26:16,345
Speaker 1:  storage. I figure it out.

1385
01:26:17,655 --> 01:26:20,625
Speaker 5:  Yeah. You know what's great is in a fedi verse world that can happen. You

1386
01:26:20,625 --> 01:26:22,545
Speaker 5:  could film that app on activity, put

1387
01:26:23,125 --> 01:26:26,865
Speaker 1:  20, 25 minutes of garbage finally

1388
01:26:26,925 --> 01:26:28,385
Speaker 1:  an app idea. That's really sticky.

1389
01:26:30,335 --> 01:26:32,585
Speaker 1:  Yeah, no, I agree with you. The, they're, they're on a collision course.

1390
01:26:32,745 --> 01:26:36,705
Speaker 1:  I just, I think YouTube has better economics because they don't pay

1391
01:26:36,705 --> 01:26:40,265
Speaker 1:  anyone. Like I understand some people make money from YouTube Premium, but

1392
01:26:40,595 --> 01:26:44,025
Speaker 1:  again, the math on YouTube is just so staggeringly good. Like

1393
01:26:44,495 --> 01:26:48,465
Speaker 1:  they make much, so much more money than they pay out that net. Netflix

1394
01:26:48,495 --> 01:26:51,185
Speaker 1:  just has a pro, like Netflix has headed to a place where

1395
01:26:52,405 --> 01:26:56,145
Speaker 1:  it makes no content and everyone in America watches like

1396
01:26:56,565 --> 01:27:00,185
Speaker 1:  TV shows made in Korea and Australia because those markets have better

1397
01:27:00,465 --> 01:27:03,145
Speaker 1:  economics for content. Yep. And reselling it to America is cheaper.

1398
01:27:03,655 --> 01:27:07,425
Speaker 5:  Yeah. I mean I think Netflix in a really interesting way

1399
01:27:07,445 --> 01:27:10,985
Speaker 5:  is going to like, eat the business of Hollywood

1400
01:27:11,445 --> 01:27:15,385
Speaker 5:  and then discover how actually small that business is in the scheme of

1401
01:27:15,385 --> 01:27:19,345
Speaker 5:  things. Like, like If you add up all of the movie studios and all

1402
01:27:19,345 --> 01:27:23,305
Speaker 5:  of the TV studios and all of the like interesting distributors, you're

1403
01:27:23,305 --> 01:27:27,075
Speaker 5:  not at Google sized, you're not, you're probably not even at

1404
01:27:27,075 --> 01:27:31,035
Speaker 5:  YouTube size as it comes to the business. So like Netflix I

1405
01:27:31,035 --> 01:27:34,815
Speaker 5:  think is desperately trying to figure out how to do Hollywood

1406
01:27:34,835 --> 01:27:37,975
Speaker 5:  at tech money and scale and

1407
01:27:39,405 --> 01:27:43,175
Speaker 5:  it's, it's gonna, it's gonna get to the end of that quickly and it's going

1408
01:27:43,175 --> 01:27:46,295
Speaker 5:  to be very expensive for us as the viewers. But I think the question

1409
01:27:47,035 --> 01:27:50,735
Speaker 5:  of whether Netflix can get beyond that kind of ceiling of Hollywood

1410
01:27:51,525 --> 01:27:52,895
Speaker 5:  remains to be seen. Right.

1411
01:27:52,895 --> 01:27:56,815
Speaker 1:  And then Google has, you know, a million kids every

1412
01:27:56,815 --> 01:27:58,495
Speaker 1:  day being like, I'm gonna be a YouTuber.

1413
01:27:58,705 --> 01:28:02,615
Speaker 5:  Right. YouTube is the, that's the strange thing about it is it is, it is

1414
01:28:02,655 --> 01:28:06,495
Speaker 5:  a completely different game that is also competing increasingly on the

1415
01:28:06,495 --> 01:28:10,295
Speaker 5:  same screen as Netflix, which I think is a, is a

1416
01:28:10,385 --> 01:28:12,335
Speaker 5:  scary thing if you're Netflix. Right.

1417
01:28:12,545 --> 01:28:14,375
Speaker 1:  Again, IIII say this but, but it's

1418
01:28:14,375 --> 01:28:16,295
Speaker 5:  A lot scarier if you're anybody other than Netflix.

1419
01:28:16,785 --> 01:28:20,135
Speaker 1:  Right. By the way, CNN today after

1420
01:28:20,765 --> 01:28:24,615
Speaker 1:  launching a streaming service and then killing it immediately four years

1421
01:28:24,675 --> 01:28:26,535
Speaker 1:  ago announced a streaming service.

1422
01:28:27,115 --> 01:28:29,925
Speaker 5:  Yeah. By the way, we're way outta practice on disclosures. This is where

1423
01:28:29,925 --> 01:28:33,565
Speaker 5:  we have to tell the people that Comcast is a minority

1424
01:28:33,925 --> 01:28:34,205
Speaker 5:  investor through,

1425
01:28:34,585 --> 01:28:37,445
Speaker 1:  We haven't talked universal about Peacock exists. It sucks you

1426
01:28:37,885 --> 01:28:40,925
Speaker 5:  Eped a Netflix show That actually, actually came up in my recommendations

1427
01:28:40,925 --> 01:28:42,285
Speaker 5:  not that long ago. That was pretty exciting.

1428
01:28:42,425 --> 01:28:44,485
Speaker 1:  Did it have my face on? It was like, hey this

1429
01:28:44,745 --> 01:28:46,125
Speaker 5:  No, it was like the animated poster.

1430
01:28:46,505 --> 01:28:48,925
Speaker 1:  Oh sure. Yeah. No, I, it's called The Future of You should Go watch it. It

1431
01:28:48,925 --> 01:28:52,685
Speaker 1:  was very good. We, Netflix doesn't pay residuals, So I, I'm not biased

1432
01:28:52,705 --> 01:28:55,165
Speaker 1:  by telling you go watch it. I didn't get paid the first time

1433
01:28:56,965 --> 01:29:00,365
Speaker 1:  straightforwardly. It was just like a fun side project we did.

1434
01:29:01,715 --> 01:29:05,525
Speaker 1:  Sure. That's how that was supposed to work. We only made one season disclosure.

1435
01:29:07,395 --> 01:29:10,405
Speaker 1:  Yeah, I think that's it. Oh, we Comcast owns it. Oh. And we talked about

1436
01:29:10,425 --> 01:29:14,365
Speaker 1:  OpenAI some somewhere. Our, our work is RSS

1437
01:29:14,365 --> 01:29:17,045
Speaker 1:  fed into some chat CPT database. Do you ever

1438
01:29:17,045 --> 01:29:17,725
Speaker 5:  Use the search thing?

1439
01:29:17,945 --> 01:29:20,245
Speaker 1:  No. This is why I always forget to do the disclosure.

1440
01:29:20,475 --> 01:29:20,765
Speaker 5:  Yeah.

1441
01:29:21,665 --> 01:29:23,525
Speaker 1:  It just doesn't matter in that way. That

1442
01:29:23,525 --> 01:29:26,965
Speaker 5:  Seems to have kind of come and gone in everybody's mind pretty fast. Yeah.

1443
01:29:28,905 --> 01:29:31,885
Speaker 1:  Is it copyright infringement? Who knows? But there, there's a deal in the

1444
01:29:31,885 --> 01:29:35,445
Speaker 1:  background. Great. We don't pay any attention to it. Yeah. Okay. Two more

1445
01:29:35,445 --> 01:29:39,265
Speaker 1:  lightning round things there, like the same thing but also

1446
01:29:39,265 --> 01:29:39,545
Speaker 1:  different.

1447
01:29:41,185 --> 01:29:45,085
Speaker 1:  The entire tech industry is figuring out how to make light bulbs. Watch

1448
01:29:45,085 --> 01:29:45,285
Speaker 1:  you,

1449
01:29:51,025 --> 01:29:52,145
Speaker 1:  I dunno, I dunno what else to

1450
01:29:52,145 --> 01:29:53,865
Speaker 5:  Optimistic to say about this then? I was hoping for, okay.

1451
01:29:54,255 --> 01:29:58,185
Speaker 1:  Okay, so Jen has the scoop on Sens five Gen

1452
01:29:58,185 --> 01:30:01,665
Speaker 1:  two A has a scoop on fy, which is tech that can turn

1453
01:30:02,325 --> 01:30:05,825
Speaker 1:  ZigBee radios into motion sensors. So light switches,

1454
01:30:06,035 --> 01:30:07,105
Speaker 1:  plugs, whatever,

1455
01:30:08,655 --> 01:30:12,505
Speaker 1:  notably Philips HU bulbs are ZigBee devices. So If you have a lot of Phillips

1456
01:30:12,525 --> 01:30:13,385
Speaker 1:  HU bulbs in your life,

1457
01:30:15,005 --> 01:30:18,785
Speaker 1:  the FY people are saying like, this is a firmware update away. Like there

1458
01:30:18,785 --> 01:30:22,345
Speaker 1:  are, they're ready to go put in the firmware update and something the ZigBee

1459
01:30:22,345 --> 01:30:25,585
Speaker 1:  radios can do motion sensing, which you know, solves a lot of problems like

1460
01:30:25,585 --> 01:30:29,345
Speaker 1:  smart home problems. Like you walk into a room, the lights know you're there,

1461
01:30:29,345 --> 01:30:33,145
Speaker 1:  they turn on. Right. This is very cool also, you're like bulb are

1462
01:30:33,265 --> 01:30:33,545
Speaker 1:  watching you.

1463
01:30:33,775 --> 01:30:36,545
Speaker 5:  Yeah. I feel like this is a

1464
01:30:37,565 --> 01:30:41,505
Speaker 5:  yet another hard to remember name on top of all of the other smart home standards.

1465
01:30:41,505 --> 01:30:44,905
Speaker 5:  So that's exciting. But I would, I would argue if this

1466
01:30:45,475 --> 01:30:49,145
Speaker 5:  works, it's on balance a good thing, right? Because like, like you're saying

1467
01:30:49,175 --> 01:30:53,025
Speaker 5:  this, this sort of ambient awareness of

1468
01:30:53,125 --> 01:30:56,865
Speaker 5:  you in your home has always been a requirement of making the smart homework.

1469
01:30:57,095 --> 01:31:00,825
Speaker 5:  Like if, if my home is very smart but has no idea that I

1470
01:31:00,835 --> 01:31:04,785
Speaker 5:  exist Yeah. It, it can't actually do that much. So like we've signed

1471
01:31:04,785 --> 01:31:08,745
Speaker 5:  up for this already and it's weird because it's your light

1472
01:31:08,745 --> 01:31:12,145
Speaker 5:  bulbs, but if it's not your light bulbs, we everybody would have to eventually

1473
01:31:12,165 --> 01:31:15,625
Speaker 5:  put some other sensor in their living room. So like it might as well be the

1474
01:31:15,625 --> 01:31:16,065
Speaker 5:  light bulbs.

1475
01:31:16,175 --> 01:31:19,105
Speaker 1:  Yeah. And I, I call this that all the time. The way you know, the way my

1476
01:31:19,255 --> 01:31:23,185
Speaker 1:  hack together Smart Home works is like the motion sensor in

1477
01:31:23,185 --> 01:31:27,105
Speaker 1:  my Eco B thermostat is wired into an automation that turns

1478
01:31:27,125 --> 01:31:31,025
Speaker 1:  on lights in the morning when it senses that my family is awake. That's

1479
01:31:31,165 --> 01:31:35,065
Speaker 1:  too much. Like that's No, no. Normal human should

1480
01:31:35,065 --> 01:31:38,945
Speaker 1:  figure out how to do that. But it works for us. Having the

1481
01:31:38,945 --> 01:31:42,025
Speaker 1:  bulbs themselves do it is great. I will say to your point about the smart

1482
01:31:42,025 --> 01:31:44,705
Speaker 1:  home being very complicated, I'm just gonna read you this full sentence from

1483
01:31:45,035 --> 01:31:48,985
Speaker 1:  Jen's piece. There's been speculation that Hugh is working on a

1484
01:31:49,005 --> 01:31:52,345
Speaker 1:  ZigBee sensing technology since its sister company. Whiz

1485
01:31:52,945 --> 01:31:56,625
Speaker 1:  w debuted a similar attack called Space Sense in 2022,

1486
01:31:56,755 --> 01:31:58,505
Speaker 1:  which uses WNS over wifi.

1487
01:32:00,705 --> 01:32:02,085
Speaker 1:  Did you know Wiz ran Space Sense?

1488
01:32:02,345 --> 01:32:05,165
Speaker 5:  The only words in that sentence I understand are like the prepositions.

1489
01:32:05,835 --> 01:32:09,245
Speaker 1:  It's very good. It's, it's, it's all, it's all very good. Okay. So that,

1490
01:32:09,265 --> 01:32:12,725
Speaker 1:  that's right. So Hugh Bulbs might be doing wins over

1491
01:32:13,065 --> 01:32:13,485
Speaker 1:  ZigBee.

1492
01:32:14,045 --> 01:32:15,845
Speaker 5:  I mean that, I've been waiting for that for years. And

1493
01:32:15,845 --> 01:32:16,805
Speaker 1:  Then at the same time it was

1494
01:32:18,765 --> 01:32:21,725
Speaker 1:  Samsung did have its event and so they had a bunch of sort of other things.

1495
01:32:21,835 --> 01:32:25,645
Speaker 1:  They're bringing ambient sensing to smart things. Oh, okay. So

1496
01:32:25,645 --> 01:32:29,245
Speaker 1:  Samsung TVs, speakers, fridges, air conditioners

1497
01:32:29,875 --> 01:32:33,485
Speaker 1:  will become motion sensors for smart things. If you run a smart thing, smart

1498
01:32:33,485 --> 01:32:37,085
Speaker 1:  house, unclear if they're using the Wiz supported

1499
01:32:37,365 --> 01:32:41,285
Speaker 1:  technology space sets or this new FY

1500
01:32:41,285 --> 01:32:45,125
Speaker 1:  thing that is running on, like this idea has been around for a long time.

1501
01:32:45,515 --> 01:32:49,165
Speaker 1:  I've, I feel like I covered it in, in gadget years ago where

1502
01:32:49,385 --> 01:32:53,285
Speaker 1:  If you have wireless radios, they can detect disturbances

1503
01:32:53,305 --> 01:32:57,245
Speaker 1:  and, and Sure. The radio waves that they're sensing. So this

1504
01:32:57,245 --> 01:32:59,245
Speaker 1:  idea has been around for a while. So I think we're just getting to a place

1505
01:32:59,245 --> 01:33:02,365
Speaker 1:  where you can do it at scale. And so there's different ways of doing it.

1506
01:33:02,365 --> 01:33:05,485
Speaker 1:  So Samsung is doing it in a different way for smart things and because they

1507
01:33:05,515 --> 01:33:09,125
Speaker 1:  have TV speakers, fridges, all this other stuff, it will just be in your

1508
01:33:09,125 --> 01:33:12,925
Speaker 1:  house. Now. I think it is weirder if your TV is the motion

1509
01:33:12,925 --> 01:33:16,805
Speaker 1:  sensor for a lot of reasons, but

1510
01:33:17,045 --> 01:33:20,685
Speaker 1:  I will point out Samsung TV's, the Frame TV

1511
01:33:21,525 --> 01:33:25,085
Speaker 1:  famously a motion sensing product because it lights up and does the frame

1512
01:33:25,085 --> 01:33:27,605
Speaker 1:  thing when you walk into the room. So they, you can see they're kind of like

1513
01:33:27,805 --> 01:33:30,365
Speaker 1:  building towards the puzzle pieces I already have.

1514
01:33:30,595 --> 01:33:34,565
Speaker 5:  Yeah. Well it's a fun run because the, the usefulness factor

1515
01:33:34,625 --> 01:33:38,445
Speaker 5:  and the creepiness factor are perfectly correlated on

1516
01:33:38,445 --> 01:33:42,085
Speaker 5:  all of this stuff, right? Like the idea that I could just walk

1517
01:33:42,195 --> 01:33:45,765
Speaker 5:  into my living room and flop down on the couch and the TV turns on and I

1518
01:33:45,765 --> 01:33:49,515
Speaker 5:  don't even have to find the remote nifty, terrifying,

1519
01:33:49,955 --> 01:33:53,915
Speaker 5:  like yeah, it's all, it's all the same thing. Like, do I want

1520
01:33:53,915 --> 01:33:56,355
Speaker 5:  to be able to walk into the room and have my lights automatically turn on?

1521
01:33:56,355 --> 01:33:59,995
Speaker 5:  Like, yeah, that's, that's a like fundamentally good thing in the smart home

1522
01:33:59,995 --> 01:34:02,235
Speaker 5:  that it knows when I'm there and it turns the lights on and it turns them

1523
01:34:02,235 --> 01:34:06,035
Speaker 5:  off and I'm gone. Also, a constant reminder that I am being watched by my

1524
01:34:06,035 --> 01:34:06,235
Speaker 5:  house

1525
01:34:06,895 --> 01:34:09,515
Speaker 1:  By Samsung. Let me just read you another paragraph from Gun Story

1526
01:34:10,895 --> 01:34:14,555
Speaker 1:  for example. Samsung says, motion sensors and a Samsung TV

1527
01:34:14,935 --> 01:34:18,355
Speaker 1:  can quote, detect what kind of exercise you're doing, guide you on your form

1528
01:34:18,355 --> 01:34:21,155
Speaker 1:  and provide the optimal exercise time for maximum results.

1529
01:34:21,815 --> 01:34:24,475
Speaker 5:  Can you imagine your fridge just suddenly going three more

1530
01:34:27,045 --> 01:34:30,875
Speaker 1:  Knees out. I'm gonna keep going. If you sit down

1531
01:34:30,975 --> 01:34:31,635
Speaker 1:  in a chair,

1532
01:34:33,885 --> 01:34:37,155
Speaker 1:  smart things can automatically turn on the nearby reading lamp and adjust

1533
01:34:37,155 --> 01:34:40,835
Speaker 1:  the room to your preferred temperature. Samsung says it can also quote,

1534
01:34:41,835 --> 01:34:45,075
Speaker 1:  identify your miniature pincher jumping onto the couch,

1535
01:34:45,565 --> 01:34:49,195
Speaker 1:  activating the air purifier to remove allergens from the air.

1536
01:34:50,095 --> 01:34:54,055
Speaker 1:  And If you are drying your hair a device for the speaker,

1537
01:34:54,085 --> 01:34:57,765
Speaker 1:  like the music frame can hear the hair dryer and tell the Samsung

1538
01:34:58,135 --> 01:35:01,805
Speaker 1:  robot vacuum to come vacuum up the hair. You have shed

1539
01:35:02,585 --> 01:35:05,205
Speaker 5:  Utopia and dystopia. That is the smart home story.

1540
01:35:05,445 --> 01:35:09,165
Speaker 1:  I, I just want to point out, these were Samsung's own

1541
01:35:09,205 --> 01:35:12,885
Speaker 1:  examples. Someone was like, all right, I've got a little dog in an air purifier.

1542
01:35:15,155 --> 01:35:18,805
Speaker 5:  Yeah. Like what's, maybe brush your dog. I don't know. Air purifier doesn't

1543
01:35:18,805 --> 01:35:20,045
Speaker 5:  need to do that much work. I don't

1544
01:35:20,045 --> 01:35:23,125
Speaker 1:  Think it's, I'm just telling you the, the, we're we're headed towards a place

1545
01:35:23,575 --> 01:35:27,175
Speaker 1:  where like having to demonstrate that

1546
01:35:27,175 --> 01:35:31,055
Speaker 1:  linking up everything in your house in a smart home, it, all these companies

1547
01:35:31,055 --> 01:35:35,015
Speaker 1:  are gonna start making up just absolutely absurd scenarios. Yeah. Just

1548
01:35:35,015 --> 01:35:36,655
Speaker 1:  the lights are watching you. It's, it's

1549
01:35:36,655 --> 01:35:40,215
Speaker 5:  The, it's the, the hairdryer one that really does it for me. Like can you

1550
01:35:40,215 --> 01:35:44,135
Speaker 5:  imagine just, you like plug in your hair dryer and it's, it's like set

1551
01:35:44,195 --> 01:35:48,055
Speaker 5:  to on and so it, it automatically goes on for a second and your robot vacuum

1552
01:35:48,075 --> 01:35:51,615
Speaker 5:  has to be like, ah, shit, here we go again. And just like comes rolling in

1553
01:35:51,615 --> 01:35:52,575
Speaker 5:  from the other room.

1554
01:35:53,445 --> 01:35:57,095
Speaker 1:  Well no David, because obviously Samsung's AI platform will be involved.

1555
01:35:57,095 --> 01:36:00,415
Speaker 1:  You'll I see. Yeah. Right. You'll be like, Hey, I've got this hairdryer problem.

1556
01:36:00,635 --> 01:36:03,695
Speaker 1:  Can you get the, can you get every time I do this, can you get, make the

1557
01:36:03,695 --> 01:36:06,575
Speaker 1:  vacuum come in and then we're gonna sit through another in terminal demo

1558
01:36:06,575 --> 01:36:09,895
Speaker 1:  that's like the computer understands language and you know what I mean? Like

1559
01:36:10,125 --> 01:36:12,765
Speaker 1:  it's happening. It's happening.

1560
01:36:14,135 --> 01:36:15,585
Speaker 5:  Yeah. And then we're all so screwed.

1561
01:36:16,145 --> 01:36:19,985
Speaker 1:  I will say, I mean, I, I love a smart home. I just, the lights are watching

1562
01:36:19,985 --> 01:36:23,625
Speaker 1:  you is that's what you have like apple's, like, we're gonna do a door lock.

1563
01:36:23,625 --> 01:36:27,265
Speaker 1:  It's like step one, but to, we're gonna put a motorized

1564
01:36:27,535 --> 01:36:31,425
Speaker 1:  iPad on a stick in your house. You need, you need all this other

1565
01:36:31,425 --> 01:36:33,865
Speaker 1:  stuff to be there. Yeah. Because otherwise your house is gonna be very smart

1566
01:36:33,865 --> 01:36:34,065
Speaker 1:  at all.

1567
01:36:34,125 --> 01:36:36,865
Speaker 5:  And then there's gonna be a software bug and it's gonna beat you to death

1568
01:36:36,865 --> 01:36:40,185
Speaker 5:  when you try to use your hairdryer. That's, that's where we're headed. Welcome

1569
01:36:40,185 --> 01:36:40,585
Speaker 5:  to the future.

1570
01:36:40,945 --> 01:36:44,305
Speaker 1:  I do love that. It, they specify that it was a miniature pincher.

1571
01:36:45,015 --> 01:36:48,105
Speaker 5:  It's very, yeah. Other dogs, whatever, they're, they're not a problem.

1572
01:36:48,765 --> 01:36:52,545
Speaker 1:  Our hair purifier, our air purifier isn't strong enough. All right. That's

1573
01:36:52,545 --> 01:36:56,065
Speaker 1:  it. That was lightning round. For what it's worth, like I said, we want your

1574
01:36:56,225 --> 01:37:00,105
Speaker 1:  feedback on how to manage all this coverage. I know that

1575
01:37:00,105 --> 01:37:03,345
Speaker 1:  it's polarizing. Some of you want a lot of it, some of you would prefer that

1576
01:37:03,345 --> 01:37:06,825
Speaker 1:  we filter all of it on the homepage. We're gonna find ways to do that on

1577
01:37:06,825 --> 01:37:10,665
Speaker 1:  our website. We need to calibrate it on this podcast. So let us

1578
01:37:10,665 --> 01:37:14,545
Speaker 1:  know. We're gonna, we're gonna work it out, but we're, we

1579
01:37:14,545 --> 01:37:18,265
Speaker 1:  can't ignore it because all of the products you make are downstream of some

1580
01:37:18,265 --> 01:37:22,225
Speaker 1:  of these politics. Like that's, they were all sitting there. Yeah, they were

1581
01:37:22,225 --> 01:37:25,385
Speaker 1:  all sitting there. Mark Zuckerberg was sitting there.

1582
01:37:26,845 --> 01:37:27,785
Speaker 1:  He was looking at stuff.

1583
01:37:27,965 --> 01:37:28,185
Speaker 7:  Yep.

1584
01:37:29,085 --> 01:37:31,145
Speaker 1:  That's it everybody. That's The Vergecast Rock and roll.

1585
01:37:36,845 --> 01:37:39,985
Speaker 7:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1586
01:37:39,985 --> 01:37:43,585
Speaker 7:  you. Give us a call at eight six six VERGE one one.

1587
01:37:43,765 --> 01:37:47,345
Speaker 7:  The Vergecast is a production of The Verge and the Vox Media Podcast network.

1588
01:37:47,685 --> 01:37:51,625
Speaker 7:  Our show is produced by Will Poor Eric Gomez and Brandon Keefer. And

1589
01:37:51,625 --> 01:37:52,945
Speaker 7:  that's it. We'll see you next week.

