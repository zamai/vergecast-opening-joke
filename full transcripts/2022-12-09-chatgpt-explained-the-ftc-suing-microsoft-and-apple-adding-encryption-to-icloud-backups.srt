1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: a9a62ff0-78c3-11ed-a49e-595208118ec9
Status: Done
Stage: Done
Title: ChatGPT explained, the FTC suing Microsoft, and Apple adding encryption to iCloud backups
Audio URL: https://jfe93e.s3.amazonaws.com/235292230542398092/3733786098979412218/s93290-US-5486s-1670701618.mp3
Description: The Verge's Nilay Patel, Alex Cranz, Richard Lawler, and James Vincent discuss the popularity of ChatGPT.
Also: the FTC sues Microsoft to block its Activision Blizzard purchase, Apple is adding end-to-end encryption to iCloud backups, and some gadget news.

2
00:00:00,760 --> 00:00:03,920
Speaker 1:  Today on the Vergecast, James Vincent and Richard Lawler join us. We get

3
00:00:03,920 --> 00:00:07,280
Speaker 1:  into the G P T three Chatbot Saga, Apple's

4
00:00:07,280 --> 00:00:11,280
Speaker 1:  new security features, and the FTC is coming after Microsoft for

5
00:00:11,280 --> 00:00:13,360
Speaker 1:  buying Activision. All coming up right after

6
00:00:13,360 --> 00:00:15,990
Speaker 1:  this,

7
00:01:30,620 --> 00:01:34,260
Speaker 4:  Hello, welcome to the Verge, the flagship podcast of The

8
00:01:34,260 --> 00:01:37,340
Speaker 4:  Verge, where we talk about absolutely nothing and waste your

9
00:01:37,340 --> 00:01:40,980
Speaker 4:  time. Now, the robot put an

10
00:01:40,980 --> 00:01:44,590
Speaker 4:  exclamation point at the end of that sentence

11
00:01:44,590 --> 00:01:48,420
Speaker 4:  because you see, I asked chat g p t three to write the

12
00:01:48,420 --> 00:01:49,270
Speaker 4:  intro of the show.

13
00:01:49,270 --> 00:01:50,820
Speaker 6:  Usually you're, you're

14
00:01:50,820 --> 00:01:54,500
Speaker 4:  Exuberant. And I said, I said, write the intro of the show, and it said,

15
00:01:54,500 --> 00:01:58,110
Speaker 4:  and chat g PT three said to me, hello and welcome to the Ro flagship podcast

16
00:01:58,110 --> 00:01:59,010
Speaker 4:  of The Verge.

17
00:01:59,010 --> 00:02:00,210
Speaker 6:  That's our, that's wrong.

18
00:02:00,210 --> 00:02:04,020
Speaker 4:  It's, I mean, it's lazy. Yeah. And so

19
00:02:04,020 --> 00:02:07,140
Speaker 4:  I told the robot to make it funnier, and then it said, hello and welcome

20
00:02:07,140 --> 00:02:10,140
Speaker 4:  to Verge Cast, where we talk about absolutely nothing and waste your time,

21
00:02:10,140 --> 00:02:14,020
Speaker 4:  which is the robot fully telling me to go

22
00:02:14,020 --> 00:02:14,650
Speaker 4:  f myself.

23
00:02:14,650 --> 00:02:17,100
Speaker 6:  I was like, David would do the exact same as he

24
00:02:17,100 --> 00:02:19,490
Speaker 4:  Was here. We basically replaced David. Yeah,

25
00:02:19,490 --> 00:02:19,980
Speaker 6:  Done.

26
00:02:19,980 --> 00:02:23,740
Speaker 4:  See you later, buddy. Anyway, this is the Verge cast. That was the

27
00:02:23,740 --> 00:02:27,420
Speaker 4:  first and last time we're ever gonna let a chat bot write the intro. I'm

28
00:02:27,420 --> 00:02:28,960
Speaker 4:  your friend Eli. Alex Kranz is here.

29
00:02:28,960 --> 00:02:31,430
Speaker 6:  I'm your friend who doesn't believe in ai.

30
00:02:31,430 --> 00:02:35,340
Speaker 4:  Oh, you will by the end of this episode. Oh, Richard Lawler is

31
00:02:35,340 --> 00:02:36,340
Speaker 4:  here. Hey, Richard. Hey.

32
00:02:36,340 --> 00:02:38,690
Speaker 5:  I might be in ai.

33
00:02:38,690 --> 00:02:42,020
Speaker 4:  That's by the end of this episode, we're gonna d we're gonna have to take

34
00:02:42,020 --> 00:02:45,980
Speaker 4:  bets and our senior Verge reporter covering

35
00:02:45,980 --> 00:02:47,760
Speaker 4:  ai. James Vincent is here. Hey, James.

36
00:02:47,760 --> 00:02:51,420
Speaker 5:  Hey, I am, I am pro ai, I'm pro

37
00:02:51,420 --> 00:02:55,340
Speaker 5:  worshiping AI and just getting ahead of that. And you know

38
00:02:55,340 --> 00:02:59,080
Speaker 5:  that, that religion that Lewandowski tried to found, I am the guy who

39
00:02:59,080 --> 00:03:00,400
Speaker 5:  carried that on. So you

40
00:03:00,400 --> 00:03:00,860
Speaker 4:  Still got it.

41
00:03:00,860 --> 00:03:03,440
Speaker 5:  You still got it? I, I, I'm keeping the faith.

42
00:03:03,440 --> 00:03:06,520
Speaker 4:  Do you have a, like a, like a blue cube in your house with

43
00:03:06,520 --> 00:03:10,240
Speaker 4:  little electric bolts like floating around in, you know, like one of those

44
00:03:10,240 --> 00:03:11,790
Speaker 4:  like AI monuments,

45
00:03:11,790 --> 00:03:14,840
Speaker 5:  Nilo, do I have a God cube that I, that I worship

46
00:03:14,840 --> 00:03:18,480
Speaker 5:  daily? Geez, man. You know, that is a very

47
00:03:18,480 --> 00:03:22,440
Speaker 5:  insulting question to ask someone of my faith. I'm gonna go, if that's gonna

48
00:03:22,440 --> 00:03:24,250
Speaker 5:  be the tenor of the conversation, obviously.

49
00:03:24,250 --> 00:03:27,960
Speaker 4:  So FairWarning, it is, I believe 9:00 PM for

50
00:03:27,960 --> 00:03:30,640
Speaker 4:  James in the uk and he's just, I'm a

51
00:03:30,640 --> 00:03:31,720
Speaker 5:  Loose,

52
00:03:31,720 --> 00:03:35,440
Speaker 4:  He's man this drinking, which honestly, it's, you know, it's,

53
00:03:35,440 --> 00:03:37,320
Speaker 4:  it's only 4:00 PM here in New York. Yeah,

54
00:03:37,320 --> 00:03:38,210
Speaker 6:  We could do that.

55
00:03:38,210 --> 00:03:42,200
Speaker 4:  We could be drinking you next week so that it's that kind of week. So

56
00:03:42,200 --> 00:03:45,680
Speaker 4:  we gotta talk about chat G P T with James. The FTC

57
00:03:45,680 --> 00:03:49,550
Speaker 4:  sued Microsoft to block the Activision deal. The EU did some

58
00:03:49,550 --> 00:03:53,520
Speaker 4:  iPhone charging standard stuff. Elon continues to Elon. There's a

59
00:03:53,520 --> 00:03:57,430
Speaker 4:  bunch of stuff talk about this week. Yeah. But we have to start with chat

60
00:03:57,430 --> 00:04:01,040
Speaker 4:  G p T three, which has taken the world by

61
00:04:01,040 --> 00:04:04,760
Speaker 4:  storm. If you don't know what it is, it is just a chat

62
00:04:04,760 --> 00:04:07,830
Speaker 4:  bot. Yeah. It just happens to be very good.

63
00:04:07,830 --> 00:04:11,800
Speaker 4:  Yeah. And I will tell you, I mean, I think it's super fun

64
00:04:11,800 --> 00:04:14,010
Speaker 4:  to play with, but the overheated,

65
00:04:14,010 --> 00:04:16,270
Speaker 6:  There's some feelings, people,

66
00:04:16,270 --> 00:04:16,880
Speaker 4:  People

67
00:04:16,880 --> 00:04:17,360
Speaker 6:  Got feeling

68
00:04:17,360 --> 00:04:20,680
Speaker 4:  Feelings positive and negative, like all the way from VC saying this will

69
00:04:20,680 --> 00:04:24,600
Speaker 4:  replace Google to like fan fiction communities

70
00:04:24,600 --> 00:04:28,400
Speaker 4:  being like, once again, our art has been plundered by

71
00:04:28,400 --> 00:04:32,280
Speaker 4:  the godless heathens of Silicon Valley, and we should shut down

72
00:04:32,280 --> 00:04:35,280
Speaker 4:  the, like, you pick either one of those. It's

73
00:04:35,280 --> 00:04:38,690
Speaker 6:  Just the whole gamut. It's whole gamut of Felix.

74
00:04:38,690 --> 00:04:42,560
Speaker 4:  So I, there's a lot of like verge stuff in here.

75
00:04:42,560 --> 00:04:45,000
Speaker 4:  Fred, we should talk about the technology itself, which is super fascinating.

76
00:04:45,000 --> 00:04:48,800
Speaker 4:  We should talk about open ai, the company slash research foundation that

77
00:04:48,800 --> 00:04:52,000
Speaker 4:  has made it and published it. And we should, we should talk about all those

78
00:04:52,000 --> 00:04:55,520
Speaker 4:  downstream effects, the angry artists of the world. We should talk about

79
00:04:55,520 --> 00:04:58,220
Speaker 4:  Lens in this conversation too. The app that makes profile pictures people

80
00:04:58,220 --> 00:05:01,440
Speaker 4:  all worked up about, well, let's just start with what it is, James. What

81
00:05:01,440 --> 00:05:03,320
Speaker 4:  is exactly going on with chat GBT

82
00:05:03,320 --> 00:05:07,120
Speaker 5:  Three? So it, it, it is just chat G p T rather than

83
00:05:07,120 --> 00:05:07,520
Speaker 5:  chat G

84
00:05:07,520 --> 00:05:10,610
Speaker 4:  PT three. No, it's G PT 3.5 is the thing inside it.

85
00:05:10,610 --> 00:05:14,480
Speaker 5:  So yeah, it is chat G P T, which is a fine tune version of an AI bot

86
00:05:14,480 --> 00:05:18,440
Speaker 5:  called Chat, G p T 3.5, which is self is a version or

87
00:05:18,440 --> 00:05:21,520
Speaker 5:  sort of an upgrade of chat of G P T three, which is

88
00:05:21,520 --> 00:05:25,240
Speaker 5:  from 2020. So sort of the foundation of this tech has been

89
00:05:25,240 --> 00:05:29,200
Speaker 5:  around for a while. And what OpenAI the company behind this decided to

90
00:05:29,200 --> 00:05:33,120
Speaker 5:  do is they fine tuned the bar on conversational dialogue and

91
00:05:33,120 --> 00:05:36,570
Speaker 5:  they fed it a bunch of sort of conversational prompts, and they got humans

92
00:05:36,570 --> 00:05:40,240
Speaker 5:  to rank the sort of attractiveness, the responsiveness of those prompts.

93
00:05:40,240 --> 00:05:43,760
Speaker 5:  They fed that back into it and it created a new sort of dialogue

94
00:05:43,760 --> 00:05:47,480
Speaker 5:  mode. And then they took that and they put it into a web

95
00:05:47,480 --> 00:05:51,440
Speaker 5:  demo. And that has been like the huge thing, you know, the,

96
00:05:51,440 --> 00:05:54,590
Speaker 5:  the, the baseline technology behind this has sort of been around for a while.

97
00:05:54,590 --> 00:05:58,240
Speaker 5:  It's never been as publicly accessible as this before. And that

98
00:05:58,240 --> 00:06:01,920
Speaker 5:  obviously has created this huge reaction where a, people are seeing what

99
00:06:01,920 --> 00:06:05,720
Speaker 5:  this thing can do for the first time. B, they're finding new things to do

100
00:06:05,720 --> 00:06:09,240
Speaker 5:  with it because, you know, you can get a bunch of AI researchers to play

101
00:06:09,240 --> 00:06:12,840
Speaker 5:  around with a model in a lab. They are never gonna be as creative or as

102
00:06:12,840 --> 00:06:16,480
Speaker 5:  chaotic as the internet on mass suddenly being like, Hey,

103
00:06:16,480 --> 00:06:19,720
Speaker 5:  let us talk to this thing and see what it can do. I've got some, I've got

104
00:06:19,720 --> 00:06:23,240
Speaker 5:  some real interesting questions. So that has created this explosion where

105
00:06:23,240 --> 00:06:27,040
Speaker 5:  suddenly this thing is everywhere. And I'm Neil, I'm with you. I I

106
00:06:27,040 --> 00:06:30,720
Speaker 5:  do find it quite exciting. Like there's lots of caveats that need

107
00:06:30,720 --> 00:06:34,600
Speaker 5:  addressing when we talk about this technology, but the, the, the base

108
00:06:34,600 --> 00:06:37,920
Speaker 5:  feeling for me is like, oh man, this is pretty cool. Like, we couldn't do

109
00:06:37,920 --> 00:06:40,880
Speaker 5:  this before. It is doing interesting stuff. I'm into it. Let's see where

110
00:06:40,880 --> 00:06:41,270
Speaker 5:  it goes.

111
00:06:41,270 --> 00:06:45,160
Speaker 4:  Yeah. That to me is the, the verges part of this whole story

112
00:06:45,160 --> 00:06:48,920
Speaker 4:  is what you just described. This underlying technology has

113
00:06:48,920 --> 00:06:52,710
Speaker 4:  been around since 2020. And we have seen

114
00:06:52,710 --> 00:06:56,190
Speaker 4:  very public demos of similar technology,

115
00:06:56,190 --> 00:07:00,020
Speaker 4:  like at Google io, Sundar Pacha has Yeah.

116
00:07:00,020 --> 00:07:03,760
Speaker 4:  Demonstrated Google's riff on large language models

117
00:07:03,760 --> 00:07:07,720
Speaker 4:  like this and you can go back and watch them. They're very

118
00:07:07,720 --> 00:07:11,640
Speaker 4:  funny. He's like, I'm gonna have a conversation with a planet now. Yeah.

119
00:07:11,640 --> 00:07:15,280
Speaker 4:  And then for a few minutes we all just sit there while sundar's like, what's

120
00:07:15,280 --> 00:07:18,880
Speaker 4:  it like to be Pluto? And a robot is like, it's

121
00:07:18,880 --> 00:07:22,580
Speaker 4:  weird. And everyone's like, what is the purpose of this?

122
00:07:22,580 --> 00:07:26,440
Speaker 4:  But that is the same l l m idea, right.

123
00:07:26,440 --> 00:07:30,040
Speaker 4:  Where you feed an ai, a massive amount of language data,

124
00:07:30,040 --> 00:07:33,880
Speaker 4:  massive corpus of text and it learns how to talk to you. Yeah.

125
00:07:33,880 --> 00:07:37,240
Speaker 4:  Open AI has a different riff, which G p t we've seen. Yeah. But that, like,

126
00:07:37,240 --> 00:07:40,680
Speaker 4:  Google's been doing that demo for a while. Yeah. Then on top of that,

127
00:07:40,680 --> 00:07:44,590
Speaker 4:  we've already experienced like three or four

128
00:07:44,590 --> 00:07:48,200
Speaker 4:  news cycles of serious meaningful

129
00:07:48,200 --> 00:07:51,680
Speaker 4:  controversy. Yeah. Around LLMs in

130
00:07:51,680 --> 00:07:55,240
Speaker 4:  particular, again, in Google with Tim Nit guru. And, and then there was just

131
00:07:55,240 --> 00:07:59,120
Speaker 4:  the researcher who claimed it was sentient and it was

132
00:07:59,120 --> 00:08:02,350
Speaker 4:  fired for warning Google that their LLM was

133
00:08:02,350 --> 00:08:05,840
Speaker 4:  sentient. The Tim guru is much more, I think,

134
00:08:05,840 --> 00:08:09,600
Speaker 4:  important, less funny, but more important where she was like, I'm publishing

135
00:08:09,600 --> 00:08:13,520
Speaker 4:  papers about the ethics of AI like this, and she claims Google fired

136
00:08:13,520 --> 00:08:17,440
Speaker 4:  her for publishing those papers. This is like a big deal. Yeah. All I'm saying

137
00:08:17,440 --> 00:08:21,240
Speaker 4:  is we have gone through the gamut with the underlying

138
00:08:21,240 --> 00:08:25,040
Speaker 4:  technology. We've seen the big hype demos, we've seen the

139
00:08:25,040 --> 00:08:28,880
Speaker 4:  ethics controversy, we've seen the

140
00:08:28,880 --> 00:08:32,800
Speaker 4:  engineer who claims it's alive get fired. Like there's

141
00:08:32,800 --> 00:08:35,740
Speaker 4:  a lot of peaks and valleys to the story already over the past few years.

142
00:08:35,740 --> 00:08:39,040
Speaker 4:  And what has happened now is that the user

143
00:08:39,040 --> 00:08:42,810
Speaker 4:  interface, Scott Good. Yeah. Right. The user interface to,

144
00:08:42,810 --> 00:08:46,330
Speaker 4:  to do this before, right. You had to install a bunch of stuff in your computer

145
00:08:46,330 --> 00:08:50,320
Speaker 4:  or like you, you just had to like work hard to get to the place where you

146
00:08:50,320 --> 00:08:54,280
Speaker 4:  could type in a prompt. Yeah. Now it's very much, you can just talk to it.

147
00:08:54,280 --> 00:08:54,760
Speaker 4:  Yeah. You

148
00:08:54,760 --> 00:08:56,680
Speaker 6:  Log, log in and, and you start

149
00:08:56,680 --> 00:09:00,600
Speaker 4:  Chatting. Yeah. I mean, it is, it is actually like, that is a remarkable

150
00:09:00,600 --> 00:09:04,480
Speaker 4:  advancement in, its in its own way. Yeah. But the underlying tech is

151
00:09:04,480 --> 00:09:06,440
Speaker 4:  not that different than what it was before, is

152
00:09:06,440 --> 00:09:10,140
Speaker 5:  It? No, no. And, and, and I think actually even the ui,

153
00:09:10,140 --> 00:09:13,840
Speaker 5:  the making it conversational is something that we've been able to do for

154
00:09:13,840 --> 00:09:17,440
Speaker 5:  a while. And I think actually the, the sort of, the thing that

155
00:09:17,440 --> 00:09:21,340
Speaker 5:  has happened is that AI companies have got a little less scared this year

156
00:09:21,340 --> 00:09:25,000
Speaker 5:  and that there has been some successes with text to image models,

157
00:09:25,000 --> 00:09:28,960
Speaker 5:  specifically stable diffusion who disrupted, you know,

158
00:09:28,960 --> 00:09:32,600
Speaker 5:  the status quo within this industry. So you had all these image models like

159
00:09:32,600 --> 00:09:36,480
Speaker 5:  Dolly, which were relatively restricted, stable diffusion is

160
00:09:36,480 --> 00:09:39,200
Speaker 5:  open source. Anyone can build on it, anyone can mess around with it. That's

161
00:09:39,200 --> 00:09:43,160
Speaker 5:  had that is having, will have bad consequences, but it's also created

162
00:09:43,160 --> 00:09:46,120
Speaker 5:  a lot of interest and excitement and people have built on it. And I think

163
00:09:46,120 --> 00:09:50,080
Speaker 5:  that gave AI companies some new confidence. I remember speaking

164
00:09:50,080 --> 00:09:53,940
Speaker 5:  earlier this year to meta about a chatbot that they put out

165
00:09:53,940 --> 00:09:57,800
Speaker 5:  and one of the things that one of their execs said to me was like,

166
00:09:57,800 --> 00:10:01,760
Speaker 5:  it's not 2016 anymore, when the Microsoft Tay Chatbot

167
00:10:01,760 --> 00:10:05,580
Speaker 5:  went online, became a racist asshole in 24 hours, and then it pulled off,

168
00:10:05,580 --> 00:10:09,280
Speaker 5:  you know, they, they were saying like, yes, that was bad, but A, we've got

169
00:10:09,280 --> 00:10:12,640
Speaker 5:  more guardrails than we used to, and b, we're kind of thinking that this

170
00:10:12,640 --> 00:10:16,430
Speaker 5:  is the best way we advanced this stuff. So I think there has been a

171
00:10:16,430 --> 00:10:20,240
Speaker 5:  period of timidity, which in many

172
00:10:20,240 --> 00:10:24,080
Speaker 5:  ways is connected to the criticisms that came out of Google with the

173
00:10:24,080 --> 00:10:27,280
Speaker 5:  Stochastic parrots paper, which was the one by Tim, Nick Gabriel, and, and

174
00:10:27,280 --> 00:10:31,160
Speaker 5:  a few others, Margaret Mitchell, which is, you know, incredibly

175
00:10:31,160 --> 00:10:34,640
Speaker 5:  valuable, important stuff. They, you know, they brought up these issues and

176
00:10:34,640 --> 00:10:38,400
Speaker 5:  they are, they are huge real issues. But I think that made

177
00:10:38,400 --> 00:10:42,110
Speaker 5:  the company sort of stutter for a while and in 2022,

178
00:10:42,110 --> 00:10:45,800
Speaker 5:  OpenAI particularly has been like, you know what f it, we're just gonna put

179
00:10:45,800 --> 00:10:49,560
Speaker 5:  it out there and see what happens. And people

180
00:10:49,560 --> 00:10:53,040
Speaker 5:  in OpenAI will be congratulating themselves for this, I'm sure, because they've

181
00:10:53,040 --> 00:10:56,920
Speaker 5:  got so much attention. They've got so much goodwill out of it, they're burning

182
00:10:56,920 --> 00:11:00,600
Speaker 5:  money doing it. But, you know, I, you know, it, it really has moved the

183
00:11:00,600 --> 00:11:02,620
Speaker 5:  conversation forward for better or worse.

184
00:11:02,620 --> 00:11:06,480
Speaker 4:  Can you, before we move on, can we, can you quickly just tell people what

185
00:11:06,480 --> 00:11:09,680
Speaker 4:  a stochastic parrot is and what this paper said? Because I think it's underlies

186
00:11:09,680 --> 00:11:12,800
Speaker 4:  the whole controversy. Oh yeah. To understand this technology, you have to

187
00:11:12,800 --> 00:11:14,800
Speaker 4:  understand that criticism. So

188
00:11:14,800 --> 00:11:18,640
Speaker 5:  There are a lot of points that are made within the paper, but

189
00:11:18,640 --> 00:11:22,600
Speaker 5:  the, I i, I think the big thing about it is what

190
00:11:22,600 --> 00:11:26,180
Speaker 5:  is a stochastic parrot? It is a probabilistic machine, right?

191
00:11:26,180 --> 00:11:29,360
Speaker 5:  And this is the big thing in many ways, it's the big thing about the deep

192
00:11:29,360 --> 00:11:33,040
Speaker 5:  learning revolution general in that instead of creating a

193
00:11:33,040 --> 00:11:36,720
Speaker 5:  deterministic computer where X does, Y does Z does, you know, ABC knocks

194
00:11:36,720 --> 00:11:40,640
Speaker 5:  on like that. You take a machine that looks at data, that looks

195
00:11:40,640 --> 00:11:44,400
Speaker 5:  for patterns in that data learns to replicate that data and then deals

196
00:11:44,400 --> 00:11:48,250
Speaker 5:  probabilities out to you. And that that probabilistic nature of

197
00:11:48,250 --> 00:11:51,960
Speaker 5:  AI and of language, large language models in particular creates

198
00:11:51,960 --> 00:11:55,800
Speaker 5:  all sorts of downstream problems often to do with the data

199
00:11:55,800 --> 00:11:59,640
Speaker 5:  that these systems are trained on. Stochastic parrots had a lot, a lot of

200
00:11:59,640 --> 00:12:02,870
Speaker 5:  criticisms about, you know, a lot of problems. I don't, I don't wanna frame,

201
00:12:02,870 --> 00:12:06,760
Speaker 5:  I don't wanna frame these people as critics because I feel that that in a

202
00:12:06,760 --> 00:12:09,400
Speaker 5:  way diminishes what they're saying. Cuz then you get the critics and the

203
00:12:09,400 --> 00:12:13,240
Speaker 5:  proponents and the, you are either for or against the technology. These aren't

204
00:12:13,240 --> 00:12:16,400
Speaker 5:  people who are against the technology, they're just people who have things

205
00:12:16,400 --> 00:12:19,500
Speaker 5:  to say about it. And, you know, write r correct

206
00:12:19,500 --> 00:12:23,360
Speaker 5:  observations to make. But, so when you have a probabilistic model,

207
00:12:23,360 --> 00:12:27,200
Speaker 5:  when you say train it on data, scrape from the internet, that is going

208
00:12:27,200 --> 00:12:30,670
Speaker 5:  to return all the bad things in that data. And that is stuff like

209
00:12:30,670 --> 00:12:34,360
Speaker 5:  biases that is, you know, bias against people of color, against

210
00:12:34,360 --> 00:12:38,320
Speaker 5:  women. It has stereotypes within that. And we know that these

211
00:12:38,320 --> 00:12:41,880
Speaker 5:  models, including chat, G P T, these problems are not fixed.

212
00:12:41,880 --> 00:12:45,600
Speaker 5:  They are still replicating these bias issues. So yeah,

213
00:12:45,600 --> 00:12:48,240
Speaker 5:  that's one thing within the paper, but there are others, but I'll leave it

214
00:12:48,240 --> 00:12:48,800
Speaker 5:  at that.

215
00:12:48,800 --> 00:12:52,760
Speaker 4:  Well, so, and this gets to very much how it works, which we should also just

216
00:12:52,760 --> 00:12:56,640
Speaker 4:  touch on briefly. We're calling it an ai Yeah.

217
00:12:56,640 --> 00:13:00,360
Speaker 4:  There, and it's made by a company called Open ai, but it's not an

218
00:13:00,360 --> 00:13:03,960
Speaker 4:  ai, but it is, it's not, it's not actually ai and this chat

219
00:13:03,960 --> 00:13:07,920
Speaker 4:  client, it's main feature is that it appears to

220
00:13:07,920 --> 00:13:10,880
Speaker 4:  have a sense of state. Yeah. Right. You ask it a question, it gives you an

221
00:13:10,880 --> 00:13:14,800
Speaker 4:  answer, you ask it a follow up, it appears to remember what

222
00:13:14,800 --> 00:13:18,400
Speaker 4:  has transpired and, and answer a follow up question that is also a

223
00:13:18,400 --> 00:13:21,800
Speaker 4:  mirage. So can talk about how it works and you know, it's sort of just like

224
00:13:21,800 --> 00:13:25,440
Speaker 4:  guessing the next word of a sentence and then we should talk about how it's

225
00:13:25,440 --> 00:13:29,360
Speaker 4:  preserving the sense of state to create the impression of

226
00:13:29,360 --> 00:13:30,190
Speaker 4:  a conversation.

227
00:13:30,190 --> 00:13:34,100
Speaker 5:  Yeah. I mean, so at the basis, a language model,

228
00:13:34,100 --> 00:13:37,920
Speaker 5:  particularly G P D three is an auto complete right? It, it has

229
00:13:37,920 --> 00:13:41,320
Speaker 5:  looked at this data, it is looked, and it is mapped out in these

230
00:13:41,320 --> 00:13:45,090
Speaker 5:  incredible multi-dimensional graphs. The proximity

231
00:13:45,090 --> 00:13:48,760
Speaker 5:  of words to one another essentially. And by doing that, when it looks at

232
00:13:48,760 --> 00:13:52,680
Speaker 5:  how often words appear next to each other, you get this semantic map, which

233
00:13:52,680 --> 00:13:56,360
Speaker 5:  allows you to know that, you know, king is like queen because they're part

234
00:13:56,360 --> 00:13:59,880
Speaker 5:  of this body called monarchy. But King also has this connection with man

235
00:13:59,880 --> 00:14:03,480
Speaker 5:  while Queen has a connection with woman. And it, you know, it maps out all

236
00:14:03,480 --> 00:14:07,120
Speaker 5:  this semantic data and then essentially you feed it a string of

237
00:14:07,120 --> 00:14:10,920
Speaker 5:  words and then it will predict what will come next. What happens

238
00:14:10,920 --> 00:14:14,600
Speaker 5:  when you turn that into a conversation is that there is a lot of clever

239
00:14:14,600 --> 00:14:18,560
Speaker 5:  behind the scenes stuff, which honestly I don't

240
00:14:18,560 --> 00:14:22,240
Speaker 5:  quite understand. I don't understand the basic mechanisms of it, but it essentially,

241
00:14:22,240 --> 00:14:25,440
Speaker 5:  you know, it turns your question into a prompt in which it tries to predict

242
00:14:25,440 --> 00:14:28,920
Speaker 5:  based on, based on what you've asked it, what the answer would or should

243
00:14:28,920 --> 00:14:32,320
Speaker 5:  or could be. Yeah. So that, that is the sort of the underlying, the underlying

244
00:14:32,320 --> 00:14:35,960
Speaker 5:  mechanism there. What, what you've pointed out, the fact that these

245
00:14:35,960 --> 00:14:39,920
Speaker 5:  systems, particularly chat g p t, have a sense of state, I think that really

246
00:14:39,920 --> 00:14:43,440
Speaker 5:  changes how you interact with them, as you rightly point out. You know, because

247
00:14:43,440 --> 00:14:47,280
Speaker 5:  you start thinking of it as an entity, not as a sort

248
00:14:47,280 --> 00:14:50,240
Speaker 5:  of a, a, a function where you ask it

249
00:14:50,240 --> 00:14:53,720
Speaker 5:  discrete questions and it gives, gives you discreet answers. And I think

250
00:14:53,720 --> 00:14:57,400
Speaker 5:  this sense of state is what led to the Google engineer

251
00:14:57,400 --> 00:15:00,980
Speaker 5:  Blake Lamoin. Lao Yeah. I, I lamoyne,

252
00:15:00,980 --> 00:15:04,840
Speaker 5:  you know, mistaking this thing for a a, a conscious being of

253
00:15:04,840 --> 00:15:08,800
Speaker 5:  some sort. So a state is very important. It helps the user,

254
00:15:08,800 --> 00:15:12,680
Speaker 5:  right? But it also creates this false sense of personhood. And I

255
00:15:12,680 --> 00:15:15,880
Speaker 5:  think that is gonna be a really tricky balance in the future with these models,

256
00:15:15,880 --> 00:15:17,910
Speaker 5:  knowing how to split that divide.

257
00:15:17,910 --> 00:15:21,880
Speaker 6:  I mean, how different is it from Siri

258
00:15:21,880 --> 00:15:24,760
Speaker 6:  or Alexa or something like that. Because I have to admit, the few times I've

259
00:15:24,760 --> 00:15:27,440
Speaker 6:  used chat G p t, it hasn't worked for

260
00:15:27,440 --> 00:15:31,280
Speaker 6:  me. It, it, it gave me, it gave me

261
00:15:31,280 --> 00:15:34,880
Speaker 6:  this whole, I asked it for a story, right. And it gave me an entire story.

262
00:15:34,880 --> 00:15:38,280
Speaker 6:  And it was like, and then they killed the dragon and then they fought Thanos.

263
00:15:38,280 --> 00:15:40,920
Speaker 6:  Right. And I was like, is Thanos the dragon? And it was like, I don't know

264
00:15:40,920 --> 00:15:44,160
Speaker 6:  how to respond to that question. And I was like, but oh no, you just told

265
00:15:44,160 --> 00:15:47,040
Speaker 6:  me all about it. So it felt like, for me, so far my experience with it has

266
00:15:47,040 --> 00:15:51,000
Speaker 6:  been very Alexa, very, very Siri. Which is why I'm like, it seems fine

267
00:15:51,000 --> 00:15:53,400
Speaker 6:  and I haven't been super impressed, but it sounds like you guys have all

268
00:15:53,400 --> 00:15:54,520
Speaker 6:  had like much

269
00:15:54,520 --> 00:15:57,200
Speaker 4:  Different, well, I'll say even this little joke about, complete this sentence,

270
00:15:57,200 --> 00:16:01,190
Speaker 4:  welcome to the Vercast, that query, I had to enter specifically that way.

271
00:16:01,190 --> 00:16:04,810
Speaker 4:  Yeah. When I said, what is the Verge, the flagship podcast

272
00:16:04,810 --> 00:16:08,680
Speaker 4:  of, I got a stock answer that says I cannot search

273
00:16:08,680 --> 00:16:09,440
Speaker 4:  the internet for you.

274
00:16:09,440 --> 00:16:10,380
Speaker 6:  Right.

275
00:16:10,380 --> 00:16:14,030
Speaker 4:  And so there are these like dead ends and it's like, I don't, that's a weird

276
00:16:14,030 --> 00:16:17,760
Speaker 6:  Yeah. It's kinda like early days of Google where you, like, you had to really

277
00:16:17,760 --> 00:16:20,960
Speaker 6:  think about how you worded that question. Yeah. And I know James, you were

278
00:16:20,960 --> 00:16:24,680
Speaker 6:  talking, you recently wrote a piece where you had a friend who like used

279
00:16:24,680 --> 00:16:26,830
Speaker 6:  it to create a whole new macro

280
00:16:26,830 --> 00:16:30,720
Speaker 5:  Yeah. And Excel macro just, you know, he, he's a guy, he works in higher

281
00:16:30,720 --> 00:16:34,340
Speaker 5:  education. He does a lot of database wrangling as part of his job,

282
00:16:34,340 --> 00:16:37,880
Speaker 5:  but he's not, he's not coder. Yeah. He's never, he's never had a formal training

283
00:16:37,880 --> 00:16:41,480
Speaker 5:  that he studied English at university, you know, and he would, he was just

284
00:16:41,480 --> 00:16:44,880
Speaker 5:  casually saying, I asked it for an Excel macro about how to, you know, extract

285
00:16:44,880 --> 00:16:48,840
Speaker 5:  this, this sort of selection of dates. And it came up with it and it got

286
00:16:48,840 --> 00:16:52,760
Speaker 5:  it. He, he got it to work in a matter of minutes. And

287
00:16:52,760 --> 00:16:56,120
Speaker 5:  I just thought, well that is, that really says something about the accessibility

288
00:16:56,120 --> 00:16:57,360
Speaker 5:  of these systems. Yeah.

289
00:16:57,360 --> 00:17:00,680
Speaker 4:  There's a piece of the puzzle there that is really important. Right. Because

290
00:17:00,680 --> 00:17:04,440
Speaker 4:  the most impressive examples I've seen of this

291
00:17:04,440 --> 00:17:08,240
Speaker 4:  system working are people asking it to write computer programs.

292
00:17:08,240 --> 00:17:12,040
Speaker 4:  Right. Write me code. Yeah. Which we think of as a very difficult thing to

293
00:17:12,040 --> 00:17:15,480
Speaker 4:  do. I'm sure all the people who actually write code for a living listening

294
00:17:15,480 --> 00:17:19,400
Speaker 4:  to this are like, that's not so hard. But like it, the relationship

295
00:17:19,400 --> 00:17:23,080
Speaker 4:  between human language and computer

296
00:17:23,080 --> 00:17:26,830
Speaker 4:  language is usually mediated by people. Yeah. Like

297
00:17:26,830 --> 00:17:30,040
Speaker 4:  mostly mediated by people and having a, a

298
00:17:30,040 --> 00:17:33,980
Speaker 4:  robot that can make that connection for you

299
00:17:33,980 --> 00:17:37,880
Speaker 4:  and take your English language query and spit

300
00:17:37,880 --> 00:17:41,800
Speaker 4:  out working workable code in the language of your choice.

301
00:17:41,800 --> 00:17:45,000
Speaker 4:  All the most impressive ones I've seen have all been in C Sharp, which is

302
00:17:45,000 --> 00:17:48,960
Speaker 4:  like a weird Microsoft corner case language. Right. Like Yeah. That's

303
00:17:48,960 --> 00:17:52,640
Speaker 4:  amazing to me. And it's because the corpus of text on the internet includes

304
00:17:52,640 --> 00:17:53,450
Speaker 4:  so much code.

305
00:17:53,450 --> 00:17:54,800
Speaker 5:  Exactly. Yeah. Yeah.

306
00:17:54,800 --> 00:17:58,760
Speaker 4:  And that to me is like a fascinating reflection of what

307
00:17:58,760 --> 00:18:02,680
Speaker 4:  you're talking about, which is the inputs reflect the outputs. Yeah. It can

308
00:18:02,680 --> 00:18:05,680
Speaker 4:  do this because there is so much code on the internet. Yeah. If you were

309
00:18:05,680 --> 00:18:09,520
Speaker 4:  to ask it to do other kinds of things that the internet is not so

310
00:18:09,520 --> 00:18:10,030
Speaker 4:  good at,

311
00:18:10,030 --> 00:18:13,640
Speaker 6:  Like recipes quick. Somebody ask it to make a cookie, give me a cookie

312
00:18:13,640 --> 00:18:14,240
Speaker 6:  recipe.

313
00:18:14,240 --> 00:18:16,000
Speaker 5:  Apparently it's quite, apparently it's quite good at

314
00:18:16,000 --> 00:18:18,720
Speaker 4:  Recipes. There's a lot of recipe text on the internet. Yeah. Yeah.

315
00:18:18,720 --> 00:18:21,840
Speaker 6:  But they usually have the, the, the, the opener where the person has to talk

316
00:18:21,840 --> 00:18:23,840
Speaker 6:  about how the cookies remind them of their grandmother's house. Well,

317
00:18:23,840 --> 00:18:27,280
Speaker 4:  So this is like, you get all the way down stream of it to, well this is like

318
00:18:27,280 --> 00:18:31,240
Speaker 4:  why people think it's gonna replace Google. Yeah. Right. Which is asking

319
00:18:31,240 --> 00:18:35,160
Speaker 4:  this thing questions returns more interesting answers that

320
00:18:35,160 --> 00:18:39,040
Speaker 4:  are more conversational and are often just the answer. Write me an

321
00:18:39,040 --> 00:18:42,990
Speaker 4:  Excel macro. If you wrote that into Google, you would be flooded

322
00:18:42,990 --> 00:18:46,880
Speaker 4:  with a list of garbage. Yeah. Like Google would basically return to you

323
00:18:46,880 --> 00:18:47,640
Speaker 4:  nothing of value.

324
00:18:47,640 --> 00:18:49,440
Speaker 6:  Bunch of paywalls.

325
00:18:49,440 --> 00:18:52,870
Speaker 4:  Yeah. A bunch of paywalls, a bunch of SEO spam texts. A bunch of

326
00:18:52,870 --> 00:18:56,350
Speaker 4:  articles that were like slideshow. Yeah. And you'd be like, this actually

327
00:18:56,350 --> 00:18:59,910
Speaker 4:  does not, is not a date sort in Excel. Yeah. Yeah. This is just,

328
00:18:59,910 --> 00:19:00,840
Speaker 4:  you're trying to get me

329
00:19:00,840 --> 00:19:03,400
Speaker 6:  Bunch of YouTube results. It's like actually you should just watch this 20

330
00:19:03,400 --> 00:19:04,430
Speaker 6:  minute video.

331
00:19:04,430 --> 00:19:08,240
Speaker 4:  Gotcha. And there's a reason that other search products like

332
00:19:08,240 --> 00:19:11,800
Speaker 4:  Stack Overflow and Yelp and all these other things exist. They're more

333
00:19:11,800 --> 00:19:15,680
Speaker 4:  targeted search interfaces to certain kinds of results.

334
00:19:15,680 --> 00:19:19,310
Speaker 4:  And they have built businesses not being gained by Google in that way.

335
00:19:19,310 --> 00:19:23,190
Speaker 4:  Chat. G P T is like, you ask a question, it just tells you an answer

336
00:19:23,190 --> 00:19:27,160
Speaker 4:  with utter confidence. Yeah. It's like, here is some code you can type into

337
00:19:27,160 --> 00:19:30,320
Speaker 4:  your computer to do a date sort and like, maybe it's pure

338
00:19:30,320 --> 00:19:34,240
Speaker 4:  malware, maybe you're gonna hack like an Iranian

339
00:19:34,240 --> 00:19:37,960
Speaker 4:  nuclear reactor. Like you have no idea what's gonna happen, but it's like,

340
00:19:37,960 --> 00:19:41,660
Speaker 4:  here it is. Yeah. You got it. Like here's a recipe for a whiskey sour,

341
00:19:41,660 --> 00:19:45,330
Speaker 4:  one cup bleach please. And it just like, no,

342
00:19:45,330 --> 00:19:49,200
Speaker 4:  no. Like absolutely no hesitation whatsoever. And I think that is why

343
00:19:49,200 --> 00:19:53,000
Speaker 4:  people are reacting to it. Yeah. Because there's no business model between

344
00:19:53,000 --> 00:19:56,750
Speaker 4:  you and something that is presented to you confidently. Like an answer,

345
00:19:56,750 --> 00:20:00,200
Speaker 4:  even if the answer is garbage. Yeah. Like, you know, and it's, there's no

346
00:20:00,200 --> 00:20:03,520
Speaker 4:  business model yet. Right? Right. Now is it burning money, James? How much

347
00:20:03,520 --> 00:20:06,730
Speaker 4:  money does this thing cost to run? Cuz it seems like they're just lighting

348
00:20:06,730 --> 00:20:07,950
Speaker 4:  VC dollars on fire.

349
00:20:07,950 --> 00:20:11,760
Speaker 5:  Well, I, I think Alex, you you, didn't you do something on this? I

350
00:20:11,760 --> 00:20:13,480
Speaker 5:  remember, I I can't remember the figures.

351
00:20:13,480 --> 00:20:17,080
Speaker 6:  Yeah, they, a guy did, did the math on it and basically each query

352
00:20:17,080 --> 00:20:20,133
Speaker 6:  costs like 0 cents.

353
00:20:20,800 --> 00:20:24,480
Speaker 6:  Right. And so it's costing them about 3 million a

354
00:20:24,480 --> 00:20:28,440
Speaker 6:  day on average with like, they, they, they assumed the number of people

355
00:20:28,440 --> 00:20:31,187
Speaker 6:  using it and the number of queries and stuff, but probably about 3 million

356
00:20:31,210 --> 00:20:35,080
Speaker 6:  on, on average. And that was based on the pricing for a w s, which is

357
00:20:35,080 --> 00:20:38,470
Speaker 6:  where this is all running out of because it has to use a lot of

358
00:20:38,470 --> 00:20:42,440
Speaker 6:  GPUs to, to do all of this. Like one, I think they said like

359
00:20:42,440 --> 00:20:45,920
Speaker 6:  one GPU would maybe present one word in a couple of minutes. So it's having

360
00:20:45,920 --> 00:20:49,680
Speaker 6:  to use like a ton of processing power, probably

361
00:20:49,680 --> 00:20:53,440
Speaker 6:  way more than Google does for Google search, which might

362
00:20:53,440 --> 00:20:53,680
Speaker 6:  make,

363
00:20:53,680 --> 00:20:55,680
Speaker 4:  Which explains a lot about Google search actually.

364
00:20:55,680 --> 00:20:57,640
Speaker 6:  Yeah, yeah. Use more GPUs.

365
00:20:57,640 --> 00:21:00,880
Speaker 4:  Yeah. Well, so that, I mean that to me is really fascinating. Right. So as

366
00:21:00,880 --> 00:21:04,800
Speaker 4:  Richard's 20 out doesn't make money yet the costs are high Yeah. On a long

367
00:21:04,800 --> 00:21:08,680
Speaker 4:  enough timeline, the cost of technology come down in Ensure, but right now

368
00:21:08,680 --> 00:21:11,390
Speaker 4:  the costs are high and they're gonna have to somehow make this a product.

369
00:21:11,390 --> 00:21:15,080
Speaker 6:  Yeah. Well I mean we're seeing that with Lens, right? Like Lens is this other

370
00:21:15,080 --> 00:21:18,470
Speaker 6:  big AI thing that's, that's using I think open source

371
00:21:18,470 --> 00:21:19,720
Speaker 6:  engine. Yeah.

372
00:21:19,720 --> 00:21:23,080
Speaker 5:  It's using, it's using Stable Diffusion, which is an open

373
00:21:23,080 --> 00:21:26,920
Speaker 5:  source text to image ai, but also is monetized by

374
00:21:26,920 --> 00:21:30,720
Speaker 5:  the company that funds it. And I say funds it rather than

375
00:21:30,720 --> 00:21:34,320
Speaker 5:  made it because there is a very convoluted

376
00:21:34,320 --> 00:21:37,960
Speaker 5:  sort of licensing scheme going on that the company that is

377
00:21:37,960 --> 00:21:41,280
Speaker 5:  associated with its stability AI has put in place in order to avoid future

378
00:21:41,280 --> 00:21:45,040
Speaker 5:  legal liability. So they, they, they love it Fund it.

379
00:21:45,040 --> 00:21:48,440
Speaker 5:  They fund it, but they don't research it and they don't make it

380
00:21:48,440 --> 00:21:49,760
Speaker 5:  technically Right.

381
00:21:49,760 --> 00:21:53,720
Speaker 6:  But anyway. Yeah. And so this other company Prism, is that, is that the name

382
00:21:53,720 --> 00:21:54,320
Speaker 6:  of it?

383
00:21:54,320 --> 00:21:55,560
Speaker 5:  Prisma Labs? Yeah.

384
00:21:55,560 --> 00:21:59,320
Speaker 6:  Prisma Labs has taken, taken this this engine and they've put it

385
00:21:59,320 --> 00:22:03,200
Speaker 6:  into their app. And so if you pay a certain amount of money, you

386
00:22:03,200 --> 00:22:06,840
Speaker 6:  can have little cute digital portraits of yourself and

387
00:22:06,840 --> 00:22:10,640
Speaker 6:  also help them continue to train this AI basically you're, I mean

388
00:22:10,640 --> 00:22:13,650
Speaker 6:  effectively you're paying them to train their

389
00:22:13,650 --> 00:22:14,700
Speaker 6:  ai

390
00:22:14,700 --> 00:22:18,590
Speaker 5:  Well whether they keep the ai do they, do they

391
00:22:18,590 --> 00:22:22,470
Speaker 5:  keep the data for training ai? I feel like this is such a meme within Yeah,

392
00:22:22,470 --> 00:22:26,390
Speaker 5:  like within Twitter that like, oh, tool X is keeping you a

393
00:22:26,390 --> 00:22:29,310
Speaker 5:  data for y and I, you know, I mean,

394
00:22:29,310 --> 00:22:32,230
Speaker 6:  Because that's, that's happened in the past. Like we have seen that happen

395
00:22:32,230 --> 00:22:35,950
Speaker 6:  before. But where a company is like, yeah, give us your face. Oh by the way

396
00:22:35,950 --> 00:22:37,310
Speaker 6:  we're working on facial recognition.

397
00:22:37,310 --> 00:22:40,030
Speaker 4:  Yeah. Face tune Yeah. Everywhere. Wasn't it? It was like the Chinese think

398
00:22:40,030 --> 00:22:42,670
Speaker 6:  Was face. Yeah. I can't remember that. We've definitely seen that in Prisma

399
00:22:42,670 --> 00:22:45,910
Speaker 6:  Labs, to their credit has said they told Tech Crunch earlier this week, either

400
00:22:45,910 --> 00:22:49,830
Speaker 6:  this week or last week. Yeah. Anyway, to their credit, Prisma Labs did

401
00:22:49,830 --> 00:22:53,710
Speaker 6:  tell TechCrunch that they, they aren't keeping a lot of this data. They are

402
00:22:53,710 --> 00:22:57,390
Speaker 6:  deleting it. It's, it's, it's going away from their servers. But isn't it

403
00:22:57,390 --> 00:23:00,280
Speaker 6:  still training stable diffusion or are they just like,

404
00:23:00,280 --> 00:23:03,390
Speaker 5:  It wouldn't be training stable diffusion. Stable Diffusion would be sort

405
00:23:03,390 --> 00:23:07,110
Speaker 5:  of separate from that. And Stable Diffusion is based on a, a

406
00:23:07,110 --> 00:23:10,390
Speaker 5:  big database called Lyon, which again is made by a German

407
00:23:10,390 --> 00:23:14,310
Speaker 5:  university. I, I guess they could be keeping it, I don't know. My,

408
00:23:14,310 --> 00:23:17,790
Speaker 5:  my my feeling on that is that the bad PR of finding out an app is keeping

409
00:23:17,790 --> 00:23:21,590
Speaker 5:  your data in order to train its systems would outweigh the cheapness of just

410
00:23:21,590 --> 00:23:25,310
Speaker 5:  buying that data. You know, if you want, if you want to get a million faces

411
00:23:25,310 --> 00:23:29,150
Speaker 5:  downloaded to train a facial recognition system that there exists a database

412
00:23:29,150 --> 00:23:29,830
Speaker 5:  called Mega Face.

413
00:23:29,830 --> 00:23:33,800
Speaker 4:  You sound like a character from Cyberpunk. Do you want get a million faces?

414
00:23:33,800 --> 00:23:35,150
Speaker 5:  Do you want a million faces my

415
00:23:35,150 --> 00:23:37,590
Speaker 4:  Friend? If you want get a million faces, I can get you a million

416
00:23:37,590 --> 00:23:39,020
Speaker 6:  Faces. You got a guy, you got a guy,

417
00:23:39,020 --> 00:23:42,900
Speaker 5:  I got a guy with a, I got several million faces back here. You take a look.

418
00:23:42,900 --> 00:23:43,790
Speaker 5:  I got what you need.

419
00:23:43,790 --> 00:23:45,800
Speaker 4:  Come on, let's do one this work faces, I get your

420
00:23:45,800 --> 00:23:48,670
Speaker 5:  Faces. Oh, I got your face. This guy come

421
00:23:48,670 --> 00:23:52,560
Speaker 6:  Like Prisma lab. They, they're, they found their other ways. I mean

422
00:23:52,560 --> 00:23:56,440
Speaker 6:  even if they, they aren't doing the whole, we're gonna steal all of your

423
00:23:56,440 --> 00:23:59,720
Speaker 6:  facial recognition data so that we can sell it somewhere else. They are saying

424
00:23:59,720 --> 00:24:03,320
Speaker 6:  you have to sign up for a month of our service. Right.

425
00:24:03,320 --> 00:24:06,680
Speaker 4:  But that's just like Apple App Store what I mean like, it's like traditional

426
00:24:06,680 --> 00:24:10,400
Speaker 4:  business model to it. Yeah. Which is like, we don't wanna sell you an app,

427
00:24:10,400 --> 00:24:13,320
Speaker 4:  we don't wanna sell you one in app purchase. We wanna make you subscribe

428
00:24:13,320 --> 00:24:16,200
Speaker 4:  to our app and then forget that you're subscribed to our app. Yes. So we

429
00:24:16,200 --> 00:24:18,160
Speaker 4:  get at least one more month of money from you.

430
00:24:18,160 --> 00:24:19,640
Speaker 6:  They're gonna make so much money. This,

431
00:24:19,640 --> 00:24:23,000
Speaker 5:  But I, I I think this will be gone pretty soon. This

432
00:24:23,000 --> 00:24:26,920
Speaker 5:  this specific iteration of it. Does anyone remember style

433
00:24:26,920 --> 00:24:30,760
Speaker 5:  transfer? Like that was like a big AI sort of trend in like

434
00:24:30,896 --> 00:24:34,840
Speaker 5:  20 15, 20 16, I wanna say. And you know what Prisma Labs did that

435
00:24:34,840 --> 00:24:38,080
Speaker 5:  They did that back in the day. I, I was looking up some old articles I wrote

436
00:24:38,080 --> 00:24:41,280
Speaker 5:  about them cuz I, for the name was familiar to me and they used to do these

437
00:24:41,280 --> 00:24:44,600
Speaker 5:  style transfer things where you'd put in a picture and it would make it look

438
00:24:44,600 --> 00:24:47,360
Speaker 5:  like Van Gogh or it would make it look like stained glass. And you know,

439
00:24:47,360 --> 00:24:50,840
Speaker 5:  this was a thing for a while. Facebook did it, you'd see it on people's profile

440
00:24:50,840 --> 00:24:52,670
Speaker 5:  pitches and it was gone within a month.

441
00:24:52,670 --> 00:24:55,200
Speaker 6:  Yeah. And now it's an Adobe feature. Yeah.

442
00:24:55,200 --> 00:24:59,000
Speaker 5:  It it's an Adobe feature and it's a boring preset thing. And I, I think AI

443
00:24:59,000 --> 00:25:02,600
Speaker 5:  portraits will probably go that way pretty quickly, but something next will

444
00:25:02,600 --> 00:25:03,440
Speaker 5:  come there.

445
00:25:03,440 --> 00:25:07,190
Speaker 4:  But there's something underlying all of this, which is

446
00:25:07,190 --> 00:25:10,480
Speaker 4:  kind of the, the cultural controversy of it. Right. So we've talked about

447
00:25:10,480 --> 00:25:14,400
Speaker 4:  where it came from, how it works, the controversy that importantly I

448
00:25:14,400 --> 00:25:18,240
Speaker 4:  think the democratization of the technology with

449
00:25:18,240 --> 00:25:21,840
Speaker 4:  easier to use user interfaces. And in the case of lens, right. Stable diff

450
00:25:21,840 --> 00:25:25,600
Speaker 4:  fusion is a thing that you were able, you can today download it

451
00:25:25,600 --> 00:25:29,560
Speaker 4:  and run it on your computer. Yeah. You can just do it if you want. It's

452
00:25:29,560 --> 00:25:33,110
Speaker 4:  just, it's hard. Yeah. Like people don't wanna do it. Actually Apple just

453
00:25:33,110 --> 00:25:36,970
Speaker 4:  released an update that makes stable defusion run faster

454
00:25:36,970 --> 00:25:38,200
Speaker 4:  on its phones. Oh

455
00:25:38,200 --> 00:25:38,430
Speaker 6:  Cool.

456
00:25:38,430 --> 00:25:40,400
Speaker 5:  Yeah. Which which is big. Yeah.

457
00:25:40,400 --> 00:25:44,320
Speaker 4:  Right. And it's like a 40% performance boost on the neural engine on

458
00:25:44,320 --> 00:25:48,060
Speaker 4:  like an M two, right. Like this is, it's all happening,

459
00:25:48,060 --> 00:25:51,640
Speaker 4:  but it's, here's an app where you can pay four bucks to get some profile

460
00:25:51,640 --> 00:25:55,090
Speaker 4:  pictures. It's, it's actually a much more like democratic.

461
00:25:55,090 --> 00:25:59,080
Speaker 4:  Anybody can use it. That's when I say like, that's the verges part

462
00:25:59,080 --> 00:26:02,480
Speaker 4:  of this story. It's like you give all the people the tools and then you have

463
00:26:02,480 --> 00:26:05,440
Speaker 4:  no idea what's gonna happen. And that's like, that's when I just like am

464
00:26:05,440 --> 00:26:05,920
Speaker 4:  the happiest,

465
00:26:05,920 --> 00:26:09,320
Speaker 6:  What's it called again, James? We give people all the

466
00:26:09,320 --> 00:26:09,920
Speaker 6:  chaos.

467
00:26:09,920 --> 00:26:11,320
Speaker 5:  Yeah. Sorry, do you have

468
00:26:11,320 --> 00:26:14,720
Speaker 6:  Like a specific, you had a specific word for it in the piece you wrote today

469
00:26:14,720 --> 00:26:16,080
Speaker 6:  about ai? Oh,

470
00:26:16,080 --> 00:26:16,700
Speaker 4:  The overhang.

471
00:26:16,700 --> 00:26:19,980
Speaker 5:  The overhang. The overhang. Yeah. So that's not necessarily about

472
00:26:19,980 --> 00:26:23,920
Speaker 5:  accessibility. The phrase is capability overhang, which I love it, it sounds

473
00:26:23,920 --> 00:26:27,160
Speaker 5:  like, I don't know, it sounds like a minor character in a pension

474
00:26:27,160 --> 00:26:31,080
Speaker 5:  novel capability overhang. But it,

475
00:26:31,080 --> 00:26:34,590
Speaker 5:  it, it essentially refers to the hidden capabilities of AI systems

476
00:26:34,590 --> 00:26:38,480
Speaker 5:  that the, it's sort of something that AI researchers get quite excited about

477
00:26:38,480 --> 00:26:41,560
Speaker 5:  and stay up late talking about when they, when they, you know, they think

478
00:26:41,560 --> 00:26:44,840
Speaker 5:  about the fact that these models do things that we don't even know. That

479
00:26:44,840 --> 00:26:48,760
Speaker 5:  we don't know that they do. Right. That there are, there are unknown unknowns

480
00:26:48,760 --> 00:26:52,160
Speaker 5:  about what these things can do. And the, the point I made in my piece was

481
00:26:52,160 --> 00:26:56,000
Speaker 5:  that although there exists a capability overhang within the research community,

482
00:26:56,000 --> 00:26:59,720
Speaker 5:  there also exists one within the sort of public awareness of

483
00:26:59,720 --> 00:27:03,670
Speaker 5:  this and 2022, particularly the last couple of months

484
00:27:03,670 --> 00:27:07,120
Speaker 5:  when we've seen tools like chat, e b T we've seen, I

485
00:27:07,120 --> 00:27:11,040
Speaker 5:  mean remember like crayon, the AI image generator

486
00:27:11,040 --> 00:27:14,000
Speaker 5:  that was everywhere for a little bit and is now already just sort of like

487
00:27:14,000 --> 00:27:16,520
Speaker 5:  part of the background tapestry of the internet. Yeah. We've forgot these

488
00:27:16,520 --> 00:27:19,640
Speaker 5:  tools. Yeah. We, we've forgotten. But these tools are just becoming part

489
00:27:19,640 --> 00:27:23,240
Speaker 5:  of everyday life now. And we are now climbing that

490
00:27:23,240 --> 00:27:27,060
Speaker 5:  overhang the public. We are discovering what these capabilities are.

491
00:27:27,060 --> 00:27:30,960
Speaker 5:  And I think 2023 is going to be wild

492
00:27:30,960 --> 00:27:33,600
Speaker 5:  for this stuff. And we're gonna see, we're gonna see a lot of consequences

493
00:27:33,600 --> 00:27:34,670
Speaker 5:  happen as well.

494
00:27:34,670 --> 00:27:38,170
Speaker 4:  That that's where we should kind of wrap this up with.

495
00:27:38,170 --> 00:27:41,720
Speaker 4:  Because to me, the image

496
00:27:41,720 --> 00:27:45,340
Speaker 4:  based AI in some way,

497
00:27:45,340 --> 00:27:49,190
Speaker 4:  that stuff is harder for a regular person to do. Right. A

498
00:27:49,190 --> 00:27:52,070
Speaker 4:  a really good example is, you know, Casey used to work at The Verge and we

499
00:27:52,070 --> 00:27:56,060
Speaker 4:  would illustrate his newsletter with actual designers and illustrators like

500
00:27:56,060 --> 00:27:59,670
Speaker 4:  a human being who worked here most recently. Alex

501
00:28:00,083 --> 00:28:02,910
Speaker 4:  Castro, who's a great illustrator, we're like illustrate Casey's newsletter

502
00:28:02,910 --> 00:28:06,510
Speaker 4:  once a week. Right. Yeah. Casey now routinely has Dolly

503
00:28:06,510 --> 00:28:10,070
Speaker 4:  illustrate the newsletter. Right. He like types a prompt into image generator

504
00:28:10,070 --> 00:28:14,030
Speaker 4:  and like whatever it's done. And that there was a moment where that,

505
00:28:14,030 --> 00:28:17,790
Speaker 4:  you know, all the arguments about, there was even like a minor controversy

506
00:28:17,790 --> 00:28:21,390
Speaker 4:  about new publications using these tools Yeah. For illustrations instead

507
00:28:21,390 --> 00:28:25,230
Speaker 4:  of hiring illustrators. But that stuff seems faddish even though it's

508
00:28:25,230 --> 00:28:28,230
Speaker 4:  harder. Yeah. Right. Even though most people cannot illustrate anything on

509
00:28:28,230 --> 00:28:31,630
Speaker 4:  their own. That stuff stylistically it comes and

510
00:28:31,630 --> 00:28:35,390
Speaker 4:  goes the usages for those things, like

511
00:28:35,390 --> 00:28:39,190
Speaker 4:  images expire in a way that, you know, like other kinds of formats

512
00:28:39,190 --> 00:28:42,940
Speaker 4:  don't. It's the text side. It's the, you wrote me some

513
00:28:42,940 --> 00:28:46,910
Speaker 4:  code or I've seen the number of advertising

514
00:28:46,910 --> 00:28:50,870
Speaker 4:  people I have seen who are like, we're gonna spam the

515
00:28:50,870 --> 00:28:54,470
Speaker 4:  most marketing text on the most content marketing blogs

516
00:28:54,470 --> 00:28:57,950
Speaker 4:  that you have ever seen. This is a godsend for

517
00:28:57,950 --> 00:29:01,430
Speaker 4:  the agencies. And it's like, first of all, all of you talk about

518
00:29:01,430 --> 00:29:05,150
Speaker 4:  advertising is is though you're like boning. Like I,

519
00:29:05,150 --> 00:29:08,840
Speaker 4:  there's like, if you just like are in that world, it's like you gotta

520
00:29:08,840 --> 00:29:12,370
Speaker 4:  be less horny about ads. Like I just, that's my advice to all of you.

521
00:29:12,370 --> 00:29:16,350
Speaker 4:  10% at least. Just like 10% less horny about advertising

522
00:29:16,350 --> 00:29:20,110
Speaker 4:  Please. And then there is like, oh that's

523
00:29:20,110 --> 00:29:24,030
Speaker 4:  real, right? Like if you run a small ad agency and you're

524
00:29:24,030 --> 00:29:27,830
Speaker 4:  like hired to sell bottles of water and you need to make the

525
00:29:27,830 --> 00:29:31,510
Speaker 4:  website, that's about how great the bottled water is. No, but he's reading

526
00:29:31,510 --> 00:29:35,350
Speaker 4:  it anyway. Right. You're kinda like, what I need is a bunch of keywords for

527
00:29:35,350 --> 00:29:39,110
Speaker 4:  Google. Yeah. So when people google bottled water, they

528
00:29:39,110 --> 00:29:41,800
Speaker 4:  like, Google's like, well I found the site with a, a lot of bottled water

529
00:29:41,800 --> 00:29:42,960
Speaker 4:  references on it. I

530
00:29:42,960 --> 00:29:44,320
Speaker 6:  Do wonder if it's gonna like,

531
00:29:44,320 --> 00:29:46,920
Speaker 4:  Oh, they're definitely not gonna hire some like 22 year old kid to do it.

532
00:29:46,920 --> 00:29:50,000
Speaker 4:  Yeah. They're gonna be like, robot write me bottled water copy,

533
00:29:50,000 --> 00:29:51,240
Speaker 6:  Write me my SEO copy. Go

534
00:29:51,240 --> 00:29:53,280
Speaker 4:  And they're gonna edit it a little bit and they're gonna put it on their

535
00:29:53,280 --> 00:29:56,990
Speaker 4:  website and they're already, they see it, they see the value of that

536
00:29:56,990 --> 00:29:57,840
Speaker 4:  completely.

537
00:29:57,840 --> 00:29:59,640
Speaker 6:  Yeah. I wonder if that's actually,

538
00:29:59,640 --> 00:30:00,400
Speaker 4:  I've seen

539
00:30:00,400 --> 00:30:01,750
Speaker 6:  Bigger danger to

540
00:30:01,750 --> 00:30:04,800
Speaker 4:  I will, I will, I will give a little hint about an episode of Decoder that

541
00:30:04,800 --> 00:30:07,800
Speaker 4:  we have in our minds. There's a very famous hustle culture

542
00:30:07,800 --> 00:30:11,080
Speaker 4:  gentleman out in the world. Yeah. I love

543
00:30:11,080 --> 00:30:15,000
Speaker 4:  him. I've, I've, I've wanting this gentleman on decoder for a long

544
00:30:15,000 --> 00:30:18,960
Speaker 4:  time. So I've been following him and I got recommended a bunch

545
00:30:18,960 --> 00:30:22,720
Speaker 4:  of his staffers and one of his staffers has a TikTok.

546
00:30:22,720 --> 00:30:26,440
Speaker 4:  It's like, you know how sometimes you need to write 45 tweets for

547
00:30:26,440 --> 00:30:30,120
Speaker 4:  a campaign? Have you met chat G P T? Yeah. And he's like,

548
00:30:30,120 --> 00:30:34,040
Speaker 4:  here's 45 tweets that I wrote for this campaign. Yeah. For like

549
00:30:34,040 --> 00:30:37,040
Speaker 4:  our hustle culture product. And it's like, oh, that shit's gonna happen.

550
00:30:37,040 --> 00:30:40,990
Speaker 4:  Yeah. And that to me is more durable, more interesting,

551
00:30:40,990 --> 00:30:44,880
Speaker 4:  more chaotic, more impactful than the image stuff. And

552
00:30:44,880 --> 00:30:48,440
Speaker 4:  I think the image stuff gets all the press cuz most people can't make images.

553
00:30:48,440 --> 00:30:52,320
Speaker 4:  Right. And obviously mo most people can make text, but the part

554
00:30:52,320 --> 00:30:56,080
Speaker 4:  where it's, and maybe I'm biased cuz I make text for a living and I

555
00:30:56,080 --> 00:31:00,000
Speaker 4:  also apparently talk about nothing on this podcast, but the part

556
00:31:00,000 --> 00:31:03,960
Speaker 4:  where it's like, oh, like most of the texts on the internet is like,

557
00:31:03,960 --> 00:31:07,120
Speaker 4:  so Google will find it. And so now we're gonna trick the Google robot with

558
00:31:07,120 --> 00:31:10,880
Speaker 4:  our robot. Yeah. It's like, oh, we're in the end times. I love it

559
00:31:10,880 --> 00:31:11,440
Speaker 4:  here.

560
00:31:11,440 --> 00:31:14,760
Speaker 6:  Well it feels like, I mean that's what I was like, I think that's where

561
00:31:14,760 --> 00:31:18,440
Speaker 6:  Google has to face the threat. It's not, oh, I can go and chat with this

562
00:31:18,440 --> 00:31:21,840
Speaker 6:  thing and it'll give me results. It's, oh, other people can go chat with

563
00:31:21,840 --> 00:31:25,520
Speaker 6:  it and it will give 'em results. Thus making Google SEO even more

564
00:31:25,520 --> 00:31:25,930
Speaker 6:  worthless.

565
00:31:25,930 --> 00:31:29,560
Speaker 4:  So James, you have written about Google exploring these models for

566
00:31:29,560 --> 00:31:33,360
Speaker 4:  search and I feel like the biggest meme of the past week is this is gonna

567
00:31:33,360 --> 00:31:37,160
Speaker 4:  replace Google and you actually wrote, we republished, repro

568
00:31:37,160 --> 00:31:41,120
Speaker 4:  promoted a piece that you wrote a year ago about Google exploring this exact

569
00:31:41,120 --> 00:31:43,390
Speaker 4:  idea and kind of walking away from it.

570
00:31:43,390 --> 00:31:47,280
Speaker 5:  Well, I mean I don't think they walked away from it forever because as you

571
00:31:47,280 --> 00:31:50,800
Speaker 5:  have pointed out, they love to demo this stuff. Like the, the

572
00:31:50,800 --> 00:31:54,240
Speaker 5:  conversations with Mum and Lambda, which are two of their big large language

573
00:31:54,240 --> 00:31:57,320
Speaker 5:  models, which are sort of their equivalence of G P T three. But they, they

574
00:31:57,320 --> 00:32:00,960
Speaker 5:  did write this paper in May or June, 2021. I can't remember exactly

575
00:32:00,960 --> 00:32:04,730
Speaker 5:  where they pointed out a lot of the problems with it. And

576
00:32:04,730 --> 00:32:08,560
Speaker 5:  in my piece I I I sort of thought that actually it's problems they

577
00:32:08,560 --> 00:32:12,400
Speaker 5:  faced before. The big one is the confidence in presenting what is potentially

578
00:32:12,400 --> 00:32:16,240
Speaker 5:  false information. Large language models, because of the stochastic, they're

579
00:32:16,240 --> 00:32:19,840
Speaker 5:  probabilistic nature. They, they cannot sort fact from fiction reliably.

580
00:32:19,840 --> 00:32:22,960
Speaker 5:  There are probably ways around that. There's ways to build in I'm sure a

581
00:32:22,960 --> 00:32:26,080
Speaker 5:  directory of knowledge that it checks against, that it looks for citations

582
00:32:26,080 --> 00:32:29,880
Speaker 5:  with. But there is this problem with confidence. And we saw it

583
00:32:29,880 --> 00:32:33,640
Speaker 5:  actually, I thought with Google Home where you used to ask Google a,

584
00:32:33,640 --> 00:32:36,960
Speaker 5:  a question and it would not, it was not a Google search of course. It was

585
00:32:36,960 --> 00:32:40,840
Speaker 5:  just, it would just give you the answer. And that ui, that presentation

586
00:32:40,840 --> 00:32:44,640
Speaker 5:  is to me the same as chat G P T because there is confidence, there is no

587
00:32:44,640 --> 00:32:48,520
Speaker 5:  outside sources and there is often a chance that you'll give the entirely

588
00:32:48,520 --> 00:32:52,400
Speaker 5:  the wrong response. And it is something

589
00:32:52,400 --> 00:32:56,140
Speaker 5:  that happens a lot. I, I I think these are fixable problems. I think

590
00:32:56,140 --> 00:33:00,040
Speaker 5:  the question is, does a startup come up with

591
00:33:00,040 --> 00:33:03,920
Speaker 5:  a better search engine, which is probably really janky and probably

592
00:33:03,920 --> 00:33:07,560
Speaker 5:  actually causes a lot of trouble. Do they get that out there before Google

593
00:33:07,560 --> 00:33:11,340
Speaker 5:  itself puts out a version which preserves its credibility?

594
00:33:11,340 --> 00:33:15,060
Speaker 5:  The thing is, Google has much more to lose in this scenario, whereas a startup

595
00:33:15,060 --> 00:33:19,040
Speaker 5:  can burn a lot of, like, it can create a lot of goodwill and

596
00:33:19,040 --> 00:33:22,040
Speaker 5:  it can say, we're doing something fast, we're going to the moon with this.

597
00:33:22,040 --> 00:33:25,640
Speaker 5:  And it can put out something that is potentially very dangerous and get away

598
00:33:25,640 --> 00:33:29,560
Speaker 5:  with it and maybe scoop Google. I I think these things will,

599
00:33:29,560 --> 00:33:32,560
Speaker 5:  there are problems with them, but I think they will be incorporated into

600
00:33:32,560 --> 00:33:36,160
Speaker 5:  search in the next couple of years. And the question for me is whether who

601
00:33:36,160 --> 00:33:37,960
Speaker 5:  gets that first, basically, what

602
00:33:37,960 --> 00:33:41,360
Speaker 4:  Do you think the most interesting use of the technology has exists today

603
00:33:41,360 --> 00:33:43,870
Speaker 4:  is

604
00:33:43,870 --> 00:33:47,840
Speaker 5:  Just creating psychosis in its users? I think, I

605
00:33:47,840 --> 00:33:51,440
Speaker 5:  think if you just, if you talk to these things for long enough, you will

606
00:33:51,440 --> 00:33:55,280
Speaker 5:  go mad. There is a reason that guy, that a Google engineer was like,

607
00:33:55,280 --> 00:33:58,960
Speaker 5:  you know what, it's alive. There's a reason we started the show by joking

608
00:33:58,960 --> 00:34:02,880
Speaker 5:  about an AI religion, one of my favorite, one of my favorite books of

609
00:34:02,880 --> 00:34:05,960
Speaker 5:  the last couple of years. And I, I dunno why, sorry, I dunno why I'm doing

610
00:34:05,960 --> 00:34:07,640
Speaker 5:  this. This is a podcast anyway,

611
00:34:07,640 --> 00:34:09,320
Speaker 4:  It's holding up a book. This is great. I'm

612
00:34:09,320 --> 00:34:13,160
Speaker 5:  Holding up a book. It's, it is called Pharmaco ai and it's by an

613
00:34:13,160 --> 00:34:16,890
Speaker 5:  artist, a writer, Kayo McDowell. And it is

614
00:34:16,890 --> 00:34:20,880
Speaker 5:  an extended conversation with G P T three in

615
00:34:20,880 --> 00:34:24,760
Speaker 5:  which Alado McDowell sort of reimagines G p t

616
00:34:24,760 --> 00:34:28,630
Speaker 5:  three as this godhead as this sort of mythical

617
00:34:28,630 --> 00:34:32,560
Speaker 5:  divine entity that drives the mad basically over the course

618
00:34:32,560 --> 00:34:36,120
Speaker 5:  of the book. In my opinion. And I think the, the, the

619
00:34:36,120 --> 00:34:38,960
Speaker 5:  weirdest and the most wonderful thing is the conversations we're gonna have

620
00:34:38,960 --> 00:34:42,940
Speaker 5:  with what is essentially a, a really weird mirror of ourselves.

621
00:34:42,940 --> 00:34:46,040
Speaker 5:  You know, for better or worse, these things are scraped from the internet

622
00:34:46,040 --> 00:34:49,800
Speaker 5:  and we can influence them with our feedback. That means they are

623
00:34:49,800 --> 00:34:53,720
Speaker 5:  like us. That it is something that reflects us back to us. And

624
00:34:53,720 --> 00:34:56,960
Speaker 5:  I think that is gonna create some really, really strange cultural experiences

625
00:34:56,960 --> 00:34:57,650
Speaker 5:  for everyone.

626
00:34:57,650 --> 00:35:01,200
Speaker 4:  There's another version of G pt, they're coming out right? G P T four

627
00:35:01,200 --> 00:35:02,720
Speaker 4:  is apparently on the horizon. Oh

628
00:35:02,720 --> 00:35:04,350
Speaker 5:  Yeah. It's on the horizon.

629
00:35:04,350 --> 00:35:06,280
Speaker 4:  What happens with it? What is the improve?

630
00:35:06,280 --> 00:35:07,840
Speaker 6:  James so excited. Justin. I'm

631
00:35:07,840 --> 00:35:09,200
Speaker 5:  Sorry. I did, I did that

632
00:35:09,200 --> 00:35:12,640
Speaker 4:  Again. This is great. He did the Mr. Burns with his

633
00:35:12,640 --> 00:35:16,600
Speaker 4:  fingers. That was, that

634
00:35:16,600 --> 00:35:19,880
Speaker 5:  Was completely instinctive. I'm so embarrassed by that. I dunno why I did

635
00:35:19,880 --> 00:35:20,040
Speaker 5:  that.

636
00:35:20,040 --> 00:35:22,840
Speaker 6:  What's exciting? Why are you doing this? Well,

637
00:35:22,840 --> 00:35:23,190
Speaker 5:  It's,

638
00:35:23,190 --> 00:35:25,960
Speaker 4:  It's, well finally humankind will block out the sun.

639
00:35:25,960 --> 00:35:27,600
Speaker 4:  Yeah.

640
00:35:27,600 --> 00:35:28,760
Speaker 6:  Don't get shot by a baby.

641
00:35:28,760 --> 00:35:32,560
Speaker 5:  Look, finally, I can get a, I can get an AI to do my job finally. I can just,

642
00:35:32,560 --> 00:35:36,240
Speaker 5:  I can just get drunk on podcasts instead. It's all I've been out trying to

643
00:35:36,240 --> 00:35:40,120
Speaker 5:  do. No, I, I don't know. I, I'm excited cuz I think

644
00:35:40,120 --> 00:35:43,920
Speaker 5:  it's, it, hopefully it will be as much as leap forward as G PT three

645
00:35:43,920 --> 00:35:47,870
Speaker 5:  was from GT two, which I use both programs, you know, I, I

646
00:35:47,870 --> 00:35:51,560
Speaker 5:  I I play around with them and it was quite a step forward

647
00:35:51,560 --> 00:35:54,520
Speaker 5:  actually. We don't know when it's gonna come out. There were rumors that

648
00:35:54,520 --> 00:35:57,240
Speaker 5:  it would be the end of this year. It's now looking like it will be early

649
00:35:57,240 --> 00:36:00,400
Speaker 5:  next year. And actually a lot of the rumors were basically about chat G P

650
00:36:00,400 --> 00:36:04,040
Speaker 5:  T. But it's supposed to be much, much bigger, you know, in terms of the

651
00:36:04,040 --> 00:36:07,080
Speaker 5:  parameters, which is the sort of metric by which you measure the size of

652
00:36:07,080 --> 00:36:09,880
Speaker 5:  these and the complexity of these models. Actually no one knows anything

653
00:36:09,880 --> 00:36:13,520
Speaker 5:  about it. Apart from that, there is just a lot of sort of like funny

654
00:36:13,520 --> 00:36:17,400
Speaker 5:  rumors going around that, you know, G P T four is, you know,

655
00:36:17,400 --> 00:36:21,040
Speaker 5:  the computational matrix is equal to the complexity of all organic life on

656
00:36:21,040 --> 00:36:23,720
Speaker 5:  earth. Yeah. Which I believe is Douglas Adams. You know, it is that sort

657
00:36:23,720 --> 00:36:27,440
Speaker 5:  of joke basically. But yeah, that should be early next year. And I'm with

658
00:36:27,440 --> 00:36:31,280
Speaker 5:  ne Eli, I think this is gonna have like a huge influence on jobs.

659
00:36:31,280 --> 00:36:34,400
Speaker 5:  I think this is actually, we're gonna start seeing industries change because

660
00:36:34,400 --> 00:36:35,820
Speaker 5:  of this technology. Very soon

661
00:36:35,820 --> 00:36:39,520
Speaker 4:  You, it's, we need to wrap this up, but what's fundamentally interesting

662
00:36:39,520 --> 00:36:43,440
Speaker 4:  about that is I think in a popular imagination, the

663
00:36:43,440 --> 00:36:47,320
Speaker 4:  automation has always replaced the blue collar jobs. And to

664
00:36:47,320 --> 00:36:49,240
Speaker 4:  some extent that is true. And we're gonna, you know, we're gonna talk about

665
00:36:49,240 --> 00:36:52,520
Speaker 4:  the TSMC factory that got opened in Arizona this way. Like yes, there's a

666
00:36:52,520 --> 00:36:55,840
Speaker 4:  bunch of, like cars are built by robot arms now. Yeah. But it turns out the

667
00:36:55,840 --> 00:36:59,720
Speaker 4:  AI is just like, it's coming for ad agencies, right. Just like

668
00:36:59,720 --> 00:37:03,120
Speaker 4:  the way that Excel came for accounting firms. Yeah. And I don't think that

669
00:37:03,120 --> 00:37:06,900
Speaker 4:  anybody has quite yet reckoned for that. The AI is coming

670
00:37:06,900 --> 00:37:10,800
Speaker 4:  for mid-level Excel macro coders more than it is

671
00:37:10,800 --> 00:37:14,560
Speaker 4:  coming for factory workers. Right. Because if you can just ask a

672
00:37:14,560 --> 00:37:18,450
Speaker 4:  robot to be like, sort some dates and it spits out the code at you,

673
00:37:18,450 --> 00:37:22,280
Speaker 4:  there's actually just like, kind of like a huge middle class of white collar

674
00:37:22,280 --> 00:37:26,160
Speaker 4:  work that goes away and no one can check the AI's work because

675
00:37:26,160 --> 00:37:27,280
Speaker 4:  it's so confident. Well,

676
00:37:27,280 --> 00:37:31,000
Speaker 6:  And it's gonna create work too, right? Like all of these, every time you

677
00:37:31,000 --> 00:37:34,200
Speaker 6:  do these queries and stuff, somebody somewhere is probably having to comb

678
00:37:34,200 --> 00:37:37,240
Speaker 6:  through some of 'em and check 'em and make sure that things are getting labeled

679
00:37:37,240 --> 00:37:38,920
Speaker 6:  properly and that everything's working because

680
00:37:38,920 --> 00:37:42,640
Speaker 4:  This is a, that's what you, you wanna be the ai, the semantic AI data

681
00:37:42,640 --> 00:37:43,190
Speaker 4:  checker.

682
00:37:43,190 --> 00:37:44,280
Speaker 6:  I think that is

683
00:37:44,280 --> 00:37:45,040
Speaker 4:  Probably a, that's a, that's a

684
00:37:45,040 --> 00:37:45,740
Speaker 6:  Terrify novel

685
00:37:45,740 --> 00:37:49,360
Speaker 4:  Job. That's a, that's a job of the future where you just go through and you

686
00:37:49,360 --> 00:37:52,320
Speaker 4:  check the tags on the chat. G P T. Yep. This

687
00:37:52,320 --> 00:37:55,960
Speaker 6:  Person's talking about porn with G P T and I'm, I'm here with

688
00:37:55,960 --> 00:37:56,870
Speaker 6:  them.

689
00:37:56,870 --> 00:38:00,840
Speaker 5:  I mean, if you guys answer any capture lately, we, you know, we, this is

690
00:38:00,840 --> 00:38:02,720
Speaker 5:  already part of our day today lives.

691
00:38:02,720 --> 00:38:06,560
Speaker 4:  Yeah, that's true. We've been doing it for a long time. This is a

692
00:38:06,560 --> 00:38:10,280
Speaker 4:  fire hydrant, so we should let you go James. We should call out. James

693
00:38:10,280 --> 00:38:14,150
Speaker 4:  has his own very good book called Beyond Measure, which is a delight

694
00:38:14,150 --> 00:38:17,267
Speaker 4:  soon to be excerpted in the Verge when it comes out in the United States.

695
00:38:17,267 --> 00:38:19,120
Speaker 4:  Tell us about your book, James. Oh,

696
00:38:19,120 --> 00:38:22,880
Speaker 5:  It is a history of measurement. It is a sort of, you know, it's a

697
00:38:22,880 --> 00:38:26,680
Speaker 5:  history. It's a science book, it's a sociology book. It's where does, where

698
00:38:26,680 --> 00:38:30,400
Speaker 5:  do measurements come from? What do people do with them and why do Americans

699
00:38:30,400 --> 00:38:33,200
Speaker 5:  hate the Metric system so much? Those are the three big questions I

700
00:38:33,200 --> 00:38:34,550
Speaker 5:  tackle.

701
00:38:34,550 --> 00:38:38,400
Speaker 4:  It's very good. It's very funny. I bought, I bought, I have a British copy

702
00:38:38,400 --> 00:38:42,190
Speaker 4:  because I'm fancy. Oh damn. They bought, I bought it early. I see.

703
00:38:42,190 --> 00:38:45,520
Speaker 4:  I bought an import. That's very nice. I used to do a seven inches. I bought

704
00:38:45,520 --> 00:38:49,000
Speaker 4:  an import copy. You know, the US version has a bonus

705
00:38:49,000 --> 00:38:51,160
Speaker 4:  edition of Train and Vain at the very end. It's not

706
00:38:51,160 --> 00:38:54,640
Speaker 4:  labeled. All right. We've gone on too

707
00:38:54,640 --> 00:38:58,290
Speaker 4:  long. James, thank you so much. Always a pleasure, man. Oh,

708
00:38:58,290 --> 00:39:01,680
Speaker 5:  My pleasure. So nice to step in and yeah, speak to you guys later.

709
00:39:01,680 --> 00:39:05,160
Speaker 4:  Bye-bye. All right, we'll be right back. We gotta talk about, well we gotta

710
00:39:05,160 --> 00:39:07,000
Speaker 4:  talk about Apple. You on, there's all kinds of stuff. We'll be right

711
00:39:07,000 --> 00:39:11,840
Speaker 4:  back.

712
00:40:56,740 --> 00:40:58,070
Speaker 4:  All right, we're back.

713
00:40:58,070 --> 00:41:02,000
Speaker 6:  Yeah, we we're back. You know, Eli, so we're here in

714
00:41:02,000 --> 00:41:05,840
Speaker 6:  the office. Eli and I are, and I saw him hunched over his desk earlier today.

715
00:41:05,840 --> 00:41:09,760
Speaker 6:  Just muttering FTC and Activision Blizzard. It

716
00:41:09,760 --> 00:41:13,280
Speaker 6:  sounds like there's some stuff happening with the Microsoft Activision Blizzard

717
00:41:13,280 --> 00:41:13,480
Speaker 6:  deal.

718
00:41:13,480 --> 00:41:17,120
Speaker 4:  Yeah, it's like the least breaking, breaking news of all

719
00:41:17,120 --> 00:41:20,380
Speaker 4:  time. I, I don't know how else to describe it. So the, the

720
00:41:20,380 --> 00:41:24,240
Speaker 4:  FTC filed a lawsuit, right? There's some details about it

721
00:41:24,240 --> 00:41:27,690
Speaker 4:  we should talk about, but Microsoft wants to buy Activision Blizzard.

722
00:41:27,690 --> 00:41:30,520
Speaker 6:  They've wanted to for a while now for Active Blizz. Active Blizz,

723
00:41:30,520 --> 00:41:32,080
Speaker 4:  Which makes Call of Duty.

724
00:41:32,080 --> 00:41:35,920
Speaker 6:  Yeah. They said we're not gonna touch it. We're gonna, we're just gonna

725
00:41:35,920 --> 00:41:37,440
Speaker 6:  let it do its thing. We promise it.

726
00:41:37,440 --> 00:41:40,720
Speaker 4:  We'll talk about Call Duty a second. So they make Call duty, they make Candy

727
00:41:40,720 --> 00:41:44,560
Speaker 4:  Crush, they make all Warcrafts makes all the money on Candy Crush,

728
00:41:44,560 --> 00:41:48,480
Speaker 4:  by the way. Yeah. Phil Spencer was just on decoder talk about why

729
00:41:48,480 --> 00:41:49,820
Speaker 4:  the deal wasn't anti-competitive,

730
00:41:49,820 --> 00:41:51,640
Speaker 6:  Why it was so good. And even

731
00:41:51,640 --> 00:41:55,400
Speaker 4:  Though Microsoft keeps running every studio like this is their third gigantic

732
00:41:55,400 --> 00:41:58,720
Speaker 4:  studio that I feel. Right. Right. And they have obviously make a bunch of

733
00:41:58,720 --> 00:42:01,840
Speaker 4:  other stuff. And so there's an all this back and forth specifically around

734
00:42:01,840 --> 00:42:02,740
Speaker 4:  Call of Duty.

735
00:42:02,740 --> 00:42:03,600
Speaker 6:  Yes.

736
00:42:03,600 --> 00:42:07,310
Speaker 4:  Right. Sony is like, you're gonna take Call of Duty away from the PlayStation.

737
00:42:07,310 --> 00:42:10,920
Speaker 4:  Gamers rise up, the gamers rose up. Microsoft is like, no,

738
00:42:10,920 --> 00:42:14,480
Speaker 4:  Phil Spencer to us, to me has said many

739
00:42:14,480 --> 00:42:17,840
Speaker 4:  times, we're gonna put Call of Duty on PlayStation as long as there's a Call

740
00:42:17,840 --> 00:42:21,720
Speaker 4:  of Duty. Then there was this like sidelight conspiracy theory, which

741
00:42:21,720 --> 00:42:25,360
Speaker 4:  was honestly my favorite conspiracy theory. That they would only do Call

742
00:42:25,360 --> 00:42:29,280
Speaker 4:  of Duty on PlayStation as like an Xbox game pass streaming situation. I love

743
00:42:29,280 --> 00:42:32,900
Speaker 4:  it. So Sony would be forced to Trojan Horse,

744
00:42:32,900 --> 00:42:36,770
Speaker 4:  the Game Pass streaming app on the PlayStation. You support it. Call of Duty.

745
00:42:36,770 --> 00:42:39,880
Speaker 4:  So I asked Phil about that. He got all riled up. He was like, no native

746
00:42:39,880 --> 00:42:43,720
Speaker 4:  code. And I was like, how long is the deal? Initially they'd only offered

747
00:42:43,720 --> 00:42:47,040
Speaker 4:  three years and Phil told me like, you can't write a contract

748
00:42:47,040 --> 00:42:49,440
Speaker 4:  forever. Which is true. Right?

749
00:42:49,440 --> 00:42:49,920
Speaker 6:  Like

750
00:42:49,920 --> 00:42:52,720
Speaker 4:  I, this is not legal advice. This is not a legal show. If you're listening

751
00:42:52,720 --> 00:42:56,600
Speaker 4:  to show without your lawyer making a mistake, please

752
00:42:56,600 --> 00:42:59,520
Speaker 4:  drive immediately to the nearest lawyer's office to continue listening to

753
00:42:59,520 --> 00:43:02,240
Speaker 4:  the show. I'll just ask G P T. Yeah,

754
00:43:02,240 --> 00:43:02,960
Speaker 6:  Yeah.

755
00:43:02,960 --> 00:43:06,800
Speaker 4:  Should you sign a contract whose term is forever? And I

756
00:43:06,800 --> 00:43:09,400
Speaker 4:  feel like very clearly the answer is no.

757
00:43:09,400 --> 00:43:11,600
Speaker 6:  Even chat G P T will be like, don't do that.

758
00:43:11,600 --> 00:43:15,440
Speaker 4:  Don't do that. Like, no, contracts are forever. So fi I get it. So

759
00:43:15,440 --> 00:43:18,910
Speaker 4:  then Microsoft has been running around these past few weeks

760
00:43:18,910 --> 00:43:21,880
Speaker 4:  trying to appease regulators in this country and in

761
00:43:21,880 --> 00:43:25,800
Speaker 4:  Europe by saying, okay, we will commit to Call of Duty

762
00:43:25,800 --> 00:43:29,720
Speaker 4:  for 10 years. Okay. Which is an awful long time. Yeah. Like they're gonna

763
00:43:29,720 --> 00:43:33,240
Speaker 4:  be out of major world conflicts to mine and they will be fully in the

764
00:43:33,240 --> 00:43:34,170
Speaker 4:  future.

765
00:43:34,170 --> 00:43:35,720
Speaker 6:  They're already in the future. They

766
00:43:35,720 --> 00:43:39,040
Speaker 4:  Like, they're gonna have to get rid of like, like all the World War II call

767
00:43:39,040 --> 00:43:39,800
Speaker 4:  of Duties are done for

768
00:43:39,800 --> 00:43:40,720
Speaker 6:  Yeah, they're done. They

769
00:43:40,720 --> 00:43:44,590
Speaker 4:  Went to the future and they went back already. Yeah.

770
00:43:44,590 --> 00:43:48,080
Speaker 4:  Like Call of Duty is like, they're just gonna be out of wars. They're like

771
00:43:48,080 --> 00:43:52,040
Speaker 4:  whatever AI Call of Duty is where we're at 10 years now. That was

772
00:43:52,040 --> 00:43:56,000
Speaker 4:  not good enough. It was not good enough for the CMA in Europe, which is already

773
00:43:56,000 --> 00:43:59,780
Speaker 4:  said, we're gonna stop it. We're starting an investigation. And then here

774
00:43:59,780 --> 00:44:03,760
Speaker 4:  the FTC Federal Trade Commission has basically been rumbling

775
00:44:03,760 --> 00:44:04,840
Speaker 4:  at this for a long time.

776
00:44:04,840 --> 00:44:06,360
Speaker 6:  Yeah. Just muttering to itself.

777
00:44:06,360 --> 00:44:09,280
Speaker 4:  Just like we and I let this happen. So when I say there's some, like there's

778
00:44:09,280 --> 00:44:13,200
Speaker 4:  a little bit of interest. Yeah. They filed the suit, the

779
00:44:13,200 --> 00:44:16,000
Speaker 4:  lawsuit. Okay. We have not seen the complaint yet. The word on the street

780
00:44:16,000 --> 00:44:19,840
Speaker 4:  is they're not gonna release it until Microsoft has like, God it until

781
00:44:19,840 --> 00:44:23,270
Speaker 4:  Microsoft President Brad Smith has like sat down with a sniffer of

782
00:44:23,270 --> 00:44:25,880
Speaker 4:  brandy and like read the complaint,

783
00:44:25,880 --> 00:44:27,260
Speaker 6:  Put his little smoking jacket on.

784
00:44:27,260 --> 00:44:31,170
Speaker 4:  But today, the day they said they're gonna do this complaint.

785
00:44:31,170 --> 00:44:34,440
Speaker 4:  Oh, they also filed the complaint in their own administrative court, not

786
00:44:34,440 --> 00:44:38,040
Speaker 4:  in federal court. Okay. Which is like tactically interesting. Yeah. Like

787
00:44:38,040 --> 00:44:40,640
Speaker 4:  what do they actually want out of this? Do they actually wanna block the

788
00:44:40,640 --> 00:44:44,460
Speaker 4:  deal or just a lot of bluster to get some concession? Who knows. But today,

789
00:44:44,460 --> 00:44:48,400
Speaker 4:  the same day is the first day of the trial where the FTC

790
00:44:48,400 --> 00:44:52,320
Speaker 4:  is has sued to stop meta

791
00:44:52,320 --> 00:44:56,030
Speaker 4:  Facebook from buying within, which makes Supernatural, which

792
00:44:56,030 --> 00:44:58,270
Speaker 4:  I have called the killer app for vr.

793
00:44:58,270 --> 00:44:59,320
Speaker 6:  It's, it's pretty killer.

794
00:44:59,320 --> 00:45:03,040
Speaker 4:  It's very good. So if you have a Quest to Supernatural is a fitness app

795
00:45:03,040 --> 00:45:03,590
Speaker 4:  you like

796
00:45:03,590 --> 00:45:06,080
Speaker 6:  It's Beat Saber, but you have to like sweat.

797
00:45:06,080 --> 00:45:08,840
Speaker 4:  It's beat Saber Peloton. Yeah. Oh, they've added kicking. They

798
00:45:08,840 --> 00:45:09,040
Speaker 6:  Had a

799
00:45:09,040 --> 00:45:10,320
Speaker 4:  Kicking. Yeah. Knee strikes.

800
00:45:10,320 --> 00:45:13,040
Speaker 6:  All right. Well I got, he's also boxing. I got some, the Boxing's great.

801
00:45:13,040 --> 00:45:14,150
Speaker 6:  I like the boxing.

802
00:45:14,150 --> 00:45:17,160
Speaker 4:  I think I've told this story in before. My wife did the boxing in Supernatural

803
00:45:17,160 --> 00:45:20,240
Speaker 4:  one time, grinned the whole time, then took off the headset and said, I'm

804
00:45:20,240 --> 00:45:22,120
Speaker 4:  great at boxing.

805
00:45:22,120 --> 00:45:22,710
Speaker 6:  It's like,

806
00:45:22,710 --> 00:45:25,920
Speaker 4:  That's what you want. Right? So this, this thing's great. It sells a lot

807
00:45:25,920 --> 00:45:29,760
Speaker 4:  of headsets. Chris Milk, the CEO within has been on decoder. He's like

808
00:45:29,760 --> 00:45:33,640
Speaker 4:  super, the supernatural community is like 50 50 men and women and it's

809
00:45:33,640 --> 00:45:37,210
Speaker 4:  people over 40, which is not a traditional VR

810
00:45:37,210 --> 00:45:40,960
Speaker 4:  measure base. Like this is the thing, this app initiates people. They're

811
00:45:40,960 --> 00:45:44,920
Speaker 4:  buying the quest too cuz it's good. So medicine, screw it. We're we buy every

812
00:45:44,920 --> 00:45:47,960
Speaker 4:  other VR game, we're gonna buy Supernatural. They went and bought it. FTC

813
00:45:47,960 --> 00:45:50,960
Speaker 4:  sued them. This deal is like teeny tiny. They're buying this thing for like

814
00:45:50,960 --> 00:45:54,800
Speaker 4:  20 bucks. Yeah. And it's an app for VR headsets. Yeah. The market size

815
00:45:54,800 --> 00:45:58,160
Speaker 4:  of six. Like they're the biggest fish in a tiny, tiny, tiny

816
00:45:58,160 --> 00:46:01,950
Speaker 4:  pond. If the FCC is gonna block that deal,

817
00:46:01,950 --> 00:46:04,500
Speaker 4:  they basically had no choice

818
00:46:04,500 --> 00:46:05,080
Speaker 6:  But to

819
00:46:05,080 --> 00:46:08,640
Speaker 4:  Block, to block the Activision deal. It's like Right. Obviously console gaming

820
00:46:08,640 --> 00:46:11,640
Speaker 4:  is much bigger. Activision itself is much

821
00:46:11,640 --> 00:46:15,360
Speaker 4:  bigger. Microsoft very opportunistically

822
00:46:15,360 --> 00:46:19,030
Speaker 4:  swept in because Body Dad vision is

823
00:46:19,030 --> 00:46:19,940
Speaker 4:  like,

824
00:46:19,940 --> 00:46:23,310
Speaker 6:  I mean, Activision Blizzard has been facing some bad times for us.

825
00:46:23,310 --> 00:46:27,110
Speaker 4:  Right. Like numerous allegations of like weird sexual misconduct across

826
00:46:27,110 --> 00:46:27,950
Speaker 4:  the company. Yep.

827
00:46:27,950 --> 00:46:31,830
Speaker 6:  A lot of people have been let go for, for misconduct. There's a lot

828
00:46:31,830 --> 00:46:35,790
Speaker 6:  of union organizing happening and because of the misconduct. Yeah. I would

829
00:46:35,790 --> 00:46:38,220
Speaker 6:  not wanna work there right now. It seems really busy.

830
00:46:38,220 --> 00:46:39,000
Speaker 4:  Yeah,

831
00:46:39,000 --> 00:46:40,450
Speaker 6:  It seems unpleasant. And,

832
00:46:40,450 --> 00:46:43,910
Speaker 4:  And so Microsoft very openly, like after this first wave of allegations,

833
00:46:43,910 --> 00:46:46,430
Speaker 4:  they just like showed up and they're like, wouldn't it be great if you didn't

834
00:46:46,430 --> 00:46:47,270
Speaker 4:  run this company anymore?

835
00:46:47,270 --> 00:46:48,390
Speaker 6:  Yeah. Just give it to us. Give

836
00:46:48,390 --> 00:46:52,310
Speaker 4:  It to us. And so like there's just a lot of noise. If you're gonna, if

837
00:46:52,310 --> 00:46:56,030
Speaker 4:  you are the FTC and you have to, and you have sued to block meta from like

838
00:46:56,030 --> 00:46:59,630
Speaker 4:  Supernatural. And by the way that complaint, you know, I'm anti, I think

839
00:46:59,630 --> 00:47:03,270
Speaker 4:  murders are bad. But the complaint, the FTCs complaining that is like we

840
00:47:03,270 --> 00:47:07,230
Speaker 4:  tr we've tried Supernatural. It's a very good, like you

841
00:47:07,230 --> 00:47:10,950
Speaker 4:  shouldn't have it. Right? If you buy Supernatural, you will not turn

842
00:47:10,950 --> 00:47:14,390
Speaker 4:  Beat Saber into a proper competitor for Supernatural. And I think

843
00:47:14,390 --> 00:47:18,310
Speaker 4:  rightfully meta, the Beat Saber team were like, what are

844
00:47:18,310 --> 00:47:22,060
Speaker 4:  you talking about? Like in what world is Beat Saber turning into a

845
00:47:22,060 --> 00:47:22,940
Speaker 4:  A Peloton?

846
00:47:22,940 --> 00:47:23,820
Speaker 6:  Yeah.

847
00:47:23,820 --> 00:47:25,100
Speaker 4:  Like no, that's like

848
00:47:25,100 --> 00:47:27,870
Speaker 6:  What in the world of the FTC where they want beats,

849
00:47:27,870 --> 00:47:31,000
Speaker 4:  They have to find some reason. So they, they filed this, we have not read

850
00:47:31,000 --> 00:47:34,760
Speaker 4:  it. They filed it in this administrative court. They have said

851
00:47:34,760 --> 00:47:38,370
Speaker 4:  to Axios or someone has said to Axios, they're not asking

852
00:47:38,370 --> 00:47:42,360
Speaker 4:  to actually like block the deal at this time. So the deal

853
00:47:42,360 --> 00:47:46,020
Speaker 4:  might still close. So they've sued to block the deal, so it shouldn't happen.

854
00:47:46,020 --> 00:47:49,580
Speaker 4:  But they've not asked their own administrative court to be like, yeah,

855
00:47:49,580 --> 00:47:53,380
Speaker 4:  don't let 'em do it. So there's a little nuance in there that I,

856
00:47:53,380 --> 00:47:56,020
Speaker 4:  without having read the complaint, that I can't quite tell you about and

857
00:47:56,020 --> 00:47:59,900
Speaker 4:  the complaint's not out yet. But you just get the sense that like the Europeans

858
00:47:59,900 --> 00:48:03,380
Speaker 4:  are definitely on firmer ground when it comes to blocking mergers. That's

859
00:48:03,380 --> 00:48:06,740
Speaker 4:  like what they're good at. Yeah. And the FTC is like, we're also finally

860
00:48:06,740 --> 00:48:10,620
Speaker 4:  su and maybe what we want is some intensely hardcore

861
00:48:10,620 --> 00:48:14,500
Speaker 4:  concessions. And this is like calibrated to achieve that

862
00:48:14,500 --> 00:48:18,420
Speaker 4:  result while they go fight this other battle around

863
00:48:18,420 --> 00:48:22,160
Speaker 4:  supernatural in court where they, well, you know,

864
00:48:22,160 --> 00:48:24,280
Speaker 4:  Lenahan su approve her theory because

865
00:48:24,280 --> 00:48:26,320
Speaker 6:  The supernatural, that's not in the administrative law

866
00:48:26,320 --> 00:48:29,760
Speaker 4:  Court. That's in, that's in court. Court. Court. Yeah. And you know, there's

867
00:48:29,760 --> 00:48:33,640
Speaker 4:  other cases against Facebook and others that she has filed. She's the

868
00:48:33,640 --> 00:48:37,360
Speaker 4:  chairwoman of the ftc. She's got novel theories about

869
00:48:37,360 --> 00:48:41,280
Speaker 4:  competition. And I again completely honest, like I agree

870
00:48:41,280 --> 00:48:44,520
Speaker 4:  with most of her theories, the courts in this country are like, that's

871
00:48:44,520 --> 00:48:47,960
Speaker 4:  new, new ideas. You say, no thank you. They

872
00:48:47,960 --> 00:48:49,880
Speaker 6:  Don't understand what a phone is.

873
00:48:49,880 --> 00:48:53,560
Speaker 4:  They're deeply confused about how some of this technology works and

874
00:48:53,560 --> 00:48:57,220
Speaker 4:  particularly network effects and like how you might lock people in. So,

875
00:48:57,220 --> 00:49:00,920
Speaker 4:  and we're seeing this with Apple and Epic, right? Like there's just

876
00:49:00,920 --> 00:49:04,440
Speaker 4:  some baseline confusion about what creates

877
00:49:04,440 --> 00:49:08,040
Speaker 4:  lockin for a consumer. Yeah. Or where anti-competitive in these markets might

878
00:49:08,040 --> 00:49:11,120
Speaker 4:  come from. And then there's the real problem, and this is partially the true

879
00:49:11,120 --> 00:49:14,850
Speaker 4:  of Apple and Epic and some of these other cases where the, our country

880
00:49:14,850 --> 00:49:18,280
Speaker 4:  in particular for about 40 years has been like

881
00:49:18,280 --> 00:49:20,310
Speaker 4:  mergers. Great.

882
00:49:20,310 --> 00:49:21,120
Speaker 6:  Love a merger,

883
00:49:21,120 --> 00:49:24,380
Speaker 4:  Love a merger. Bigger the better. Shit, you should buy some shit. Yeah. Right.

884
00:49:24,380 --> 00:49:28,340
Speaker 4:  And so you've gotta just overcome a lot of precedent

885
00:49:28,340 --> 00:49:32,170
Speaker 4:  to, to block a merger. So I, I just see it like, I see where this case

886
00:49:32,170 --> 00:49:35,750
Speaker 4:  is calibrated and I see that the Europeans wanna block it too.

887
00:49:35,750 --> 00:49:39,050
Speaker 4:  And what we'll just see. I don't Do you think it's gonna, like Microsoft

888
00:49:39,050 --> 00:49:41,250
Speaker 4:  is like running out there. And again, you should listen to the conversation

889
00:49:41,250 --> 00:49:44,730
Speaker 4:  I had with, with Phil about it on decoder. He made his case. Yeah.

890
00:49:44,730 --> 00:49:48,650
Speaker 4:  Right. And it's very funny when a big company wants to buy something, the

891
00:49:48,650 --> 00:49:50,480
Speaker 4:  case is always, but we suck.

892
00:49:50,480 --> 00:49:54,090
Speaker 6:  Yeah. We suck so much. Let us buy this. This

893
00:49:54,090 --> 00:49:57,570
Speaker 4:  Market is so competitive. We've been getting our a kicked, we're nowhere

894
00:49:57,570 --> 00:50:00,930
Speaker 4:  in mobile games. We need to buy Candy Crush. Yeah. Call on duty. You could

895
00:50:00,930 --> 00:50:04,320
Speaker 4:  have Call of Duty the futures mobile. Right? Like go listen to it. It's,

896
00:50:04,320 --> 00:50:07,730
Speaker 4:  bill is very good. The man was born to be the CEO of Microsoft

897
00:50:07,730 --> 00:50:10,170
Speaker 4:  Duty. He's very, very good at that

898
00:50:10,170 --> 00:50:14,050
Speaker 4:  job, you know, and he is a candid

899
00:50:14,050 --> 00:50:17,330
Speaker 4:  executive, so you're gonna listen to it. But the argument basically boils

900
00:50:17,330 --> 00:50:21,060
Speaker 4:  down to if we don't grow, we die. And all the growth is in mobile,

901
00:50:21,060 --> 00:50:24,750
Speaker 4:  so we have to buy Activision so we can get mobile games. Yeah.

902
00:50:24,750 --> 00:50:28,400
Speaker 4:  And all the stuff we're talking about with our consoles is like kind of a

903
00:50:28,400 --> 00:50:29,400
Speaker 4:  sidelight. Would you

904
00:50:29,400 --> 00:50:32,830
Speaker 6:  Buy it? I mean, I don't, because that's not their biggest

905
00:50:32,830 --> 00:50:36,480
Speaker 6:  market, right? Like Xbox is certainly a big part of

906
00:50:36,480 --> 00:50:40,400
Speaker 6:  Microsoft, but cloud computing is a much bigger part of

907
00:50:40,400 --> 00:50:43,800
Speaker 6:  Microsoft. Yeah. And so, so for them to say this, it's just like super

908
00:50:43,800 --> 00:50:47,320
Speaker 6:  disingenuous because like, yeah, they're a gaming company, but they're a

909
00:50:47,320 --> 00:50:51,240
Speaker 6:  cloud company first and foremost, even more so than, than

910
00:50:51,240 --> 00:50:54,000
Speaker 6:  they're an operating system company or any of these other things. Like they

911
00:50:54,000 --> 00:50:57,480
Speaker 6:  do cloud computing. So to say this is just like some

912
00:50:57,480 --> 00:50:59,030
Speaker 6:  bullshit.

913
00:50:59,030 --> 00:51:02,790
Speaker 4:  I feel like, I mean, I, I honestly don't know where this is gonna go. Like

914
00:51:02,790 --> 00:51:06,720
Speaker 4:  this is a, this is a big case. This is a generationally important in

915
00:51:06,720 --> 00:51:10,600
Speaker 4:  antitrust case. Yeah. Because there's, we just have

916
00:51:10,600 --> 00:51:12,470
Speaker 4:  not stopped deals like this as a country.

917
00:51:12,470 --> 00:51:15,560
Speaker 6:  I thought it was kind of interesting that we've seen Microsoft go on this

918
00:51:15,560 --> 00:51:19,350
Speaker 6:  spree around the same time that we also saw like one of the biggest

919
00:51:19,350 --> 00:51:23,120
Speaker 6:  kind of halts in vertical integration in the entertainment community

920
00:51:23,120 --> 00:51:26,430
Speaker 6:  go away, which was the paramount decree that was like from the thirties

921
00:51:26,430 --> 00:51:30,240
Speaker 6:  that they, they got rid of a couple of years ago. And that Paramount decree

922
00:51:30,240 --> 00:51:33,930
Speaker 6:  said, if you are making the movies, you cannot own the theaters

923
00:51:33,930 --> 00:51:37,560
Speaker 6:  because that's really hinky because then you get to control where your movies

924
00:51:37,560 --> 00:51:41,480
Speaker 6:  screen. Yeah. And the exact, like, we're dealing with that issue

925
00:51:41,480 --> 00:51:45,240
Speaker 6:  in streaming and we're dealing with it in gaming. If, if Microsoft is making

926
00:51:45,240 --> 00:51:49,160
Speaker 6:  the game consoles, they probably shouldn't also be making the games

927
00:51:49,160 --> 00:51:52,920
Speaker 6:  because then they control where those games go. And like, it's

928
00:51:52,920 --> 00:51:56,560
Speaker 6:  fundamentally the same thing. But I know that our country has said,

929
00:51:56,560 --> 00:52:00,400
Speaker 6:  yeah, consent, Paramounts consent decree don't need it.

930
00:52:00,400 --> 00:52:01,390
Speaker 6:  We're fine. Yeah.

931
00:52:01,390 --> 00:52:05,320
Speaker 4:  Well, so to abstract that out, it's, it's usually like, you know, vertical

932
00:52:05,320 --> 00:52:09,280
Speaker 4:  versus horizontal mergers, right? Right. And so we allow vertical ones,

933
00:52:09,280 --> 00:52:12,960
Speaker 4:  or we have for the past 40 years, which is the theaters, were the only distribution.

934
00:52:12,960 --> 00:52:16,280
Speaker 4:  Yeah. And if you own all the distribution, you have a lot of power over the,

935
00:52:16,280 --> 00:52:19,680
Speaker 4:  the studios. And so the studios shouldn't own the theaters because then the

936
00:52:19,680 --> 00:52:21,520
Speaker 4:  other studio they can unfairly Right.

937
00:52:21,520 --> 00:52:25,280
Speaker 6:  Discriminate. Disney can say, you will never see another MGM movie in our

938
00:52:25,280 --> 00:52:26,040
Speaker 6:  theaters again. And

939
00:52:26,040 --> 00:52:28,840
Speaker 4:  We own all the theaters. So they like, now you can't do that. The theaters

940
00:52:28,840 --> 00:52:32,820
Speaker 4:  have to be competitive. You can like apply that lesson to the

941
00:52:32,820 --> 00:52:36,640
Speaker 4:  iOS app store. Yeah. You can apply that lesson to Comcast. Like whatever

942
00:52:36,640 --> 00:52:39,880
Speaker 4:  it is. Like whoever owns the pipes or the distribution here. I think you're

943
00:52:39,880 --> 00:52:42,800
Speaker 4:  saying like the game consoles are, they're like the distribution.

944
00:52:42,800 --> 00:52:45,640
Speaker 6:  Yeah. The game consoles are the theater. Right, right. Like, and and they

945
00:52:45,640 --> 00:52:49,270
Speaker 6:  have a very robust, big business there. If they're making the theaters,

946
00:52:49,270 --> 00:52:53,160
Speaker 6:  they probably shouldn't also be making the content that goes into those theaters.

947
00:52:53,160 --> 00:52:56,760
Speaker 6:  In this case, all of the games at Activision Blizzard, one of the largest

948
00:52:56,760 --> 00:53:00,390
Speaker 6:  gaming companies not currently owned by Microsoft.

949
00:53:00,390 --> 00:53:03,420
Speaker 6:  Like Yeah. Or Nintendo existing.

950
00:53:03,420 --> 00:53:06,120
Speaker 10:  One thing that makes it a little more complicated is that generally the

951
00:53:06,120 --> 00:53:09,960
Speaker 10:  councils are sold at a loss. They're not necessarily profiting on the actual

952
00:53:09,960 --> 00:53:10,920
Speaker 10:  consoles themselves. The,

953
00:53:10,920 --> 00:53:11,840
Speaker 6:  Because they're profiting

954
00:53:11,840 --> 00:53:15,560
Speaker 10:  On those business analogy, they're the games profit on the games. And I

955
00:53:15,560 --> 00:53:18,240
Speaker 10:  guess Microsoft is claiming that they would love to make more money from

956
00:53:18,240 --> 00:53:21,480
Speaker 10:  games. So of course they'll sell Call of Duty on PlayStation or Nintendo

957
00:53:21,480 --> 00:53:24,400
Speaker 10:  or Esteem because everyone wants to play Call of Duty on Switch. That's,

958
00:53:24,400 --> 00:53:25,810
Speaker 10:  that's what you want. Yes.

959
00:53:25,810 --> 00:53:29,370
Speaker 4:  By the way, Nintendo happily took the 10-year dear for Call of Duty. Yeah.

960
00:53:29,370 --> 00:53:33,360
Speaker 4:  No complaints filed. No, no letters to the senators.

961
00:53:33,360 --> 00:53:37,200
Speaker 4:  They're like 10 years of Call of Duty on the switch. We'll take it cuz

962
00:53:37,200 --> 00:53:37,600
Speaker 4:  they

963
00:53:37,600 --> 00:53:41,080
Speaker 6:  Don't give shit. Did they, did, did that specify what kind of Call of Duty?

964
00:53:41,080 --> 00:53:44,760
Speaker 6:  Like is it just gonna be the mobile gross version that's on your

965
00:53:44,760 --> 00:53:45,350
Speaker 6:  phone?

966
00:53:45,350 --> 00:53:47,720
Speaker 4:  I don't think matters to Nintendo. I think Nintendo is,

967
00:53:47,720 --> 00:53:48,040
Speaker 6:  Nintendo

968
00:53:48,040 --> 00:53:51,400
Speaker 4:  Doesn't care that anybody who owns a console also probably wants to play

969
00:53:51,400 --> 00:53:55,160
Speaker 4:  Mario. Yeah. And they've got an absolute rock solid, legally

970
00:53:55,160 --> 00:53:59,000
Speaker 4:  defensible monopoly on Mario. And he's never going anywhere

971
00:53:59,000 --> 00:54:00,650
Speaker 4:  except Nintendo consoles.

972
00:54:00,650 --> 00:54:04,520
Speaker 6:  It is very funny that we're like, oh, I don't know if, if if

973
00:54:04,520 --> 00:54:07,910
Speaker 6:  Microsoft can own Call of Duty and Nintendo is like, we own

974
00:54:07,910 --> 00:54:10,280
Speaker 6:  everything on our platform and

975
00:54:10,280 --> 00:54:12,960
Speaker 4:  We'll just, well that's their model. I mean, they're the apple of this industry

976
00:54:12,960 --> 00:54:16,760
Speaker 4:  and Microsoft is the Microsoft, but what I

977
00:54:16,760 --> 00:54:20,320
Speaker 10:  Want all is what kind of concessions could the FTC get? And, and I'm very

978
00:54:20,320 --> 00:54:23,320
Speaker 10:  specific about Monique, I need to be able to use controllers across, across

979
00:54:23,320 --> 00:54:27,110
Speaker 10:  platforms. I don't like buying controllers,

980
00:54:27,110 --> 00:54:30,160
Speaker 10:  I don't need dual, I don't wanna use dual sense. I just wanna use my Xbox

981
00:54:30,160 --> 00:54:32,990
Speaker 10:  Elite controller everywhere. So we need to fix that. Le con

982
00:54:32,990 --> 00:54:34,240
Speaker 4:  Lena, if you're listening step,

983
00:54:34,240 --> 00:54:36,800
Speaker 10:  I need to be able to use my gaming headsets across every single platform.

984
00:54:36,800 --> 00:54:40,200
Speaker 10:  They need to all be compatible. We, we need to fix this. We need the online

985
00:54:40,200 --> 00:54:42,760
Speaker 10:  accounts and friends list to transfer back and forth. Just

986
00:54:42,760 --> 00:54:43,080
Speaker 6:  That cross

987
00:54:43,080 --> 00:54:46,120
Speaker 10:  Play. The blockchain fixes this. I'm not saying that I'm the last person

988
00:54:46,120 --> 00:54:49,400
Speaker 10:  on Crypto Island now, and now that everyone else is running from it, I'm

989
00:54:49,400 --> 00:54:52,760
Speaker 10:  going to die on this hill alone. But the blockchain does fix this. Just

990
00:54:52,760 --> 00:54:54,800
Speaker 10:  saying. So I think those are my concerns. You

991
00:54:54,800 --> 00:54:58,640
Speaker 4:  Okay? Look, historically on the Vergecast, I have

992
00:54:58,640 --> 00:55:02,330
Speaker 4:  assigned de many articles based on throwaway comments.

993
00:55:02,330 --> 00:55:06,160
Speaker 4:  Richard, welcome to your role filling in for David

994
00:55:06,160 --> 00:55:09,900
Speaker 4:  while he's out on parental leave. You gotta write the gamers,

995
00:55:09,900 --> 00:55:13,480
Speaker 4:  the gamers' Bill of Rights. Like what should the FTC ask for from a gamers'

996
00:55:13,480 --> 00:55:16,460
Speaker 4:  perspective? Yes. To make this deal. Go, go through,

997
00:55:16,460 --> 00:55:19,240
Speaker 6:  Get really nuanced about the skins. You wanna

998
00:55:19,240 --> 00:55:21,670
Speaker 6:  sing

999
00:55:21,670 --> 00:55:22,960
Speaker 4:  A gamers demand. You should

1000
00:55:22,960 --> 00:55:26,760
Speaker 10:  Never have to pay for a recolor again. Ever. If if it's not a new completely

1001
00:55:26,760 --> 00:55:30,160
Speaker 10:  new design, then you can't tell it. I should be able to get whatever colors

1002
00:55:30,160 --> 00:55:30,720
Speaker 10:  I want. I'm

1003
00:55:30,720 --> 00:55:33,560
Speaker 4:  Just saying this is great. We gotta, we gotta do this. If you have ideas

1004
00:55:33,560 --> 00:55:37,000
Speaker 4:  for Richard, you can tweet him. You can email him. We're gonna put this together.

1005
00:55:37,000 --> 00:55:40,920
Speaker 4:  We'll send it to the ftc. Yeah. I'm never getting an FTC official on the

1006
00:55:40,920 --> 00:55:43,240
Speaker 4:  on decoder again. Like it's never gonna happen. I think

1007
00:55:43,240 --> 00:55:47,120
Speaker 6:  They'll see this and be like, oh, they get it. They get it. This

1008
00:55:47,120 --> 00:55:49,350
Speaker 6:  is why we put it in administrative law court.

1009
00:55:49,350 --> 00:55:52,280
Speaker 4:  Look, I'm saying this is a generationally important antitrust case. Like

1010
00:55:52,280 --> 00:55:56,010
Speaker 4:  if they get this one through it, what it will signal

1011
00:55:56,010 --> 00:55:59,760
Speaker 4:  is if the FCC wins this, let say it, if the FCC

1012
00:55:59,760 --> 00:56:03,240
Speaker 4:  wins this and they block this deal, it will signal competition law in the

1013
00:56:03,280 --> 00:56:07,040
Speaker 4:  United States is changing. If they lose it, it means either someone

1014
00:56:07,040 --> 00:56:10,360
Speaker 4:  promised Lena Conn controller interop, which maybe that's all we

1015
00:56:10,360 --> 00:56:14,280
Speaker 4:  need. Or it means that like we live to fight another day.

1016
00:56:14,280 --> 00:56:17,940
Speaker 4:  Yeah. Right. And I will say that in this lame duck period of Congress,

1017
00:56:17,940 --> 00:56:21,640
Speaker 4:  the big antitrust bills are on the floor. There's a lot of momentum around

1018
00:56:21,640 --> 00:56:25,120
Speaker 4:  them, particularly the, the app store interoperability built. Like

1019
00:56:25,120 --> 00:56:28,730
Speaker 4:  there's, there's stuff, the digital markets act like all this stuff.

1020
00:56:28,730 --> 00:56:30,950
Speaker 4:  There's more antitrust noise

1021
00:56:30,950 --> 00:56:31,440
Speaker 6:  Yeah.

1022
00:56:31,440 --> 00:56:35,280
Speaker 4:  Happening right now than has happened in a while. This lawsuit's a piece

1023
00:56:35,280 --> 00:56:38,760
Speaker 4:  of it, the meta supernatural case, which again is a very small compared to

1024
00:56:38,760 --> 00:56:42,440
Speaker 4:  the Activision deal, but it, it is a signal of

1025
00:56:42,440 --> 00:56:45,720
Speaker 4:  where the government wants to apply pressure and then there are the bills

1026
00:56:45,720 --> 00:56:46,960
Speaker 4:  in Congress. So we'll see.

1027
00:56:46,960 --> 00:56:50,800
Speaker 6:  We'll see. And I mean, other things we saw this week we

1028
00:56:50,800 --> 00:56:54,040
Speaker 6:  should probably talk about kind of related, I'm gonna, I'm gonna go all the

1029
00:56:54,040 --> 00:56:57,240
Speaker 6:  way back to how we talked about the EU is also suing. They're

1030
00:56:57,240 --> 00:56:59,850
Speaker 4:  Just nuts man. They're just like eating cheese and being like, we got ideas.

1031
00:56:59,850 --> 00:57:03,640
Speaker 6:  We got ideas. And you will do them including Apple. You

1032
00:57:03,640 --> 00:57:07,120
Speaker 6:  will have u s BBC on your phones and they set

1033
00:57:07,120 --> 00:57:09,040
Speaker 6:  a deadline for that this week.

1034
00:57:09,448 --> 00:57:11,298
Speaker 4:  December 28th, 2024.

1035
00:57:11,390 --> 00:57:15,360
Speaker 6:  Yeah. So by December 28th, 2024, you will

1036
00:57:15,360 --> 00:57:19,040
Speaker 6:  have new phones with u s BBC for wired charging. So

1037
00:57:19,040 --> 00:57:21,070
Speaker 6:  including iPhones. Theoretically

1038
00:57:21,070 --> 00:57:25,040
Speaker 4:  I can I just say something about this story? Okay. They're

1039
00:57:25,040 --> 00:57:28,400
Speaker 4:  Europeans, they can't help themselves. Like the EU is so needlessly complicated.

1040
00:57:28,400 --> 00:57:32,130
Speaker 4:  Yeah. And we have an amazing international team that is mostly based in Europe.

1041
00:57:32,130 --> 00:57:36,120
Speaker 4:  It heard from James, John Porter is our reporter who's been working

1042
00:57:36,120 --> 00:57:40,000
Speaker 4:  on this story. Yeah. And like he has basically written a

1043
00:57:40,000 --> 00:57:43,800
Speaker 4:  version of the headline European Union mandate to USBC

1044
00:57:43,800 --> 00:57:46,080
Speaker 4:  for phones 500 times or,

1045
00:57:46,080 --> 00:57:47,200
Speaker 6:  And we make him do it again

1046
00:57:47,200 --> 00:57:50,680
Speaker 4:  And we make him do it again and again. Mostly because it's like interesting.

1047
00:57:50,680 --> 00:57:54,320
Speaker 4:  We should track the story. We have a responsibility as journalist Yeah. To

1048
00:57:54,320 --> 00:57:58,120
Speaker 4:  accurately convey. And also because it's traffic gold. Like people

1049
00:57:58,120 --> 00:58:01,040
Speaker 4:  see this headline and they lose their minds like, how can I pick on this

1050
00:58:01,040 --> 00:58:04,800
Speaker 4:  twice? And it's like, we've written a story like

1051
00:58:04,800 --> 00:58:08,600
Speaker 4:  every other week for five years. So he's, he writes, he's

1052
00:58:08,600 --> 00:58:12,420
Speaker 4:  excited. I'm like, yeah, we finally have a final official deadline

1053
00:58:12,420 --> 00:58:15,240
Speaker 4:  for one new phone sold in the European Union, including future iPhones will

1054
00:58:15,240 --> 00:58:18,640
Speaker 4:  have to use U P C and December 20th, 2024.

1055
00:58:18,900 --> 00:58:22,480
Speaker 4:  The next paragraph, the new rules will

1056
00:58:22,480 --> 00:58:25,760
Speaker 4:  officially enter into force in 20 days times an

1057
00:58:25,760 --> 00:58:29,320
Speaker 4:  individual EU member states will then have a maximum of

1058
00:58:29,320 --> 00:58:31,850
Speaker 4:  24 months to write national laws.

1059
00:58:31,850 --> 00:58:32,600
Speaker 6:  No. It's

1060
00:58:32,600 --> 00:58:34,550
Speaker 4:  Like, oh, this still hasn't happened, is it?

1061
00:58:34,550 --> 00:58:37,250
Speaker 6:  He's got, we have to make him do one for every single

1062
00:58:37,250 --> 00:58:41,120
Speaker 6:  country. Luxembourg, John, we have

1063
00:58:41,120 --> 00:58:43,360
Speaker 6:  to know what Luxembourg voted on. It's

1064
00:58:43,360 --> 00:58:45,890
Speaker 4:  Like, oh man, it still hasn't happened.

1065
00:58:45,890 --> 00:58:48,600
Speaker 6:  It hasn't happened in Belgium. All right. It's done.

1066
00:58:48,600 --> 00:58:52,390
Speaker 4:  It's like, alright, this is just like fully ridiculous.

1067
00:58:52,390 --> 00:58:54,120
Speaker 6:  He's got like a map in his house. Yeah.

1068
00:58:54,120 --> 00:58:57,790
Speaker 4:  It's like somewhere someone in the EU like runs like a, like a traffic

1069
00:58:57,790 --> 00:59:01,560
Speaker 4:  bait website and they're like, let's stretch it out. Like we know people

1070
00:59:01,560 --> 00:59:02,760
Speaker 4:  love this story just

1071
00:59:02,760 --> 00:59:03,130
Speaker 6:  Is

1072
00:59:03,130 --> 00:59:06,480
Speaker 4:  Another 24 months of US BBC iPhone

1073
00:59:06,480 --> 00:59:08,650
Speaker 4:  clicks.

1074
00:59:08,650 --> 00:59:09,470
Speaker 6:  So good.

1075
00:59:09,470 --> 00:59:11,960
Speaker 4:  It's good. So that's the date, that's

1076
00:59:11,960 --> 00:59:12,720
Speaker 6:  That's this implies

1077
00:59:12,720 --> 00:59:15,070
Speaker 4:  Theoretically the sort of 2025

1078
00:59:15,070 --> 00:59:18,770
Speaker 6:  IPhone thing. iPhone 16.

1079
00:59:18,770 --> 00:59:19,120
Speaker 4:  Oh

1080
00:59:19,120 --> 00:59:22,320
Speaker 6:  Because it says all new phones have to be using it by

1081
00:59:22,320 --> 00:59:24,320
Speaker 6:  2024. So that's not the

1082
00:59:24,320 --> 00:59:26,890
Speaker 4:  Year that the iPhone that gets announced in 2024

1083
00:59:26,890 --> 00:59:28,040
Speaker 6:  Sh will have to be US

1084
00:59:28,040 --> 00:59:31,670
Speaker 4:  Bbc. So right now we're on the 14, which is the iPhone that was announced

1085
00:59:31,670 --> 00:59:34,690
Speaker 6:  2022. Yeah. So

1086
00:59:34,690 --> 00:59:35,770
Speaker 6:  15

1087
00:59:35,770 --> 00:59:36,640
Speaker 4:  Is 2023

1088
00:59:36,640 --> 00:59:40,040
Speaker 6:  And then 16 is 2024. So this will be the iPhone

1089
00:59:40,040 --> 00:59:43,550
Speaker 6:  16 should theoretically be u s bbc.

1090
00:59:43,550 --> 00:59:47,000
Speaker 6:  Unless they decide to be like, you know what eu, you don't get a phone this

1091
00:59:47,000 --> 00:59:47,800
Speaker 6:  year.

1092
00:59:47,800 --> 00:59:48,910
Speaker 4:  That'd be amazing.

1093
00:59:48,910 --> 00:59:50,040
Speaker 6:  Just nevermind.

1094
00:59:50,040 --> 00:59:53,840
Speaker 4:  We'll see. Importantly you might recall that the

1095
00:59:53,840 --> 00:59:57,760
Speaker 4:  UK is no longer in the European Union. Notably

1096
00:59:57,760 --> 01:00:01,560
Speaker 4:  they just don't get either. United States is not, I don't believe China is

1097
01:00:01,560 --> 01:00:05,400
Speaker 4:  in the European Union. All gigantic markets for Apple.

1098
01:00:05,400 --> 01:00:07,960
Speaker 6:  Unclear of China's in the European Union. Like

1099
01:00:07,960 --> 01:00:09,920
Speaker 4:  I'm, I'm not a geologist.

1100
01:00:09,920 --> 01:00:16,220
Speaker 6:  Yeah.

1101
01:00:16,220 --> 01:00:19,310
Speaker 6:  Who knows what kind of rocks they got going on over there.

1102
01:00:19,310 --> 01:00:22,760
Speaker 4:  That's been a long one. And also I think James is drinking beer rubbed off

1103
01:00:22,760 --> 01:00:26,600
Speaker 4:  of me. But I'm say those are giant markets. It's raffle. They could make

1104
01:00:26,600 --> 01:00:30,190
Speaker 4:  Lightning iPhones and US VC iPhones. We don't know what they're gonna do.

1105
01:00:30,190 --> 01:00:34,120
Speaker 6:  Just that one. They, they released like 500 of them limited edition in

1106
01:00:34,120 --> 01:00:34,600
Speaker 6:  Germany and in

1107
01:00:34,600 --> 01:00:37,800
Speaker 4:  Brazil they still have to put the charger in the box. That's true. I got

1108
01:00:37,800 --> 01:00:41,720
Speaker 4:  a real taste of careful what you wish for last week. So I

1109
01:00:41,720 --> 01:00:44,800
Speaker 4:  went home for Thanksgiving. Yeah. You know, did the turned motion smoothing

1110
01:00:44,800 --> 01:00:48,400
Speaker 4:  off on all the TVs first thing you know, ran around fixing stuff at my parents'

1111
01:00:48,400 --> 01:00:52,320
Speaker 4:  house. Bought them the new Apple TV remotes. Yeah. So they

1112
01:00:52,320 --> 01:00:54,840
Speaker 4:  had the old black ones, they were horrible. Whatever. Did

1113
01:00:54,840 --> 01:00:57,600
Speaker 6:  They also have scotch tape on theirs? Like my mom does.

1114
01:00:57,600 --> 01:01:00,400
Speaker 4:  It's a long story. My parents finally shut down their at and t U versus cable

1115
01:01:00,400 --> 01:01:04,280
Speaker 4:  boxes. It's a five year victory. If

1116
01:01:04,280 --> 01:01:05,720
Speaker 4:  you've been listening, you know, I haven't been working on this for five

1117
01:01:05,720 --> 01:01:08,480
Speaker 4:  years and they switched to the cable company. Now they're doing all their

1118
01:01:08,480 --> 01:01:11,200
Speaker 4:  TV through the Apple tv. Got rid of the cable boxes, consolidate three months

1119
01:01:11,200 --> 01:01:15,080
Speaker 4:  by the near remotes. Yeah. Spot near mos. Just went to Best Buy. Pulled three

1120
01:01:15,080 --> 01:01:18,830
Speaker 4:  of 'em off the shelf, came home, set 'em up. Off we go. Little do I know

1121
01:01:18,830 --> 01:01:22,570
Speaker 4:  that I bought two of the older Lightning

1122
01:01:22,570 --> 01:01:26,550
Speaker 4:  remotes and one of the new

1123
01:01:26,550 --> 01:01:30,250
Speaker 4:  u s BBC remotes. So I get, this is the most

1124
01:01:30,250 --> 01:01:34,160
Speaker 4:  bizarre text from my dad who's like, I'm using

1125
01:01:34,160 --> 01:01:37,740
Speaker 4:  the kitchen tv. Is the plug different?

1126
01:01:37,740 --> 01:01:40,240
Speaker 4:  And like, just out of context, they're like, what are you talking about?

1127
01:01:40,240 --> 01:01:41,920
Speaker 4:  Like, is the plug in the wall? Would you buy

1128
01:01:41,920 --> 01:01:42,760
Speaker 6:  A new TV

1129
01:01:42,760 --> 01:01:46,730
Speaker 4:  From Europe? Are you plugging the tv? Like just leave it, let's stop it.

1130
01:01:46,730 --> 01:01:50,240
Speaker 4:  So I'm like, what do you mean? And he is like, the plugs for the other two

1131
01:01:50,240 --> 01:01:53,880
Speaker 4:  rooms work fine, but this one doesn't. And it's like, you know, it's like

1132
01:01:53,880 --> 01:01:57,020
Speaker 4:  I'm at work, I'm like different plan. I'm like, just call me, send me a picture.

1133
01:01:57,020 --> 01:02:00,110
Speaker 4:  And he sends me a picture of the bottom room. I'm like, you gotta be kidding.

1134
01:02:00,110 --> 01:02:03,960
Speaker 4:  Like this is, I asked for this, I asked for everything to be

1135
01:02:03,960 --> 01:02:07,800
Speaker 4:  u s bbc and now there's one US BBC Apple device in my parents'

1136
01:02:07,800 --> 01:02:11,000
Speaker 4:  house. Just one. And it is just like the crime of the

1137
01:02:11,000 --> 01:02:12,180
Speaker 4:  century.

1138
01:02:12,180 --> 01:02:16,080
Speaker 6:  Did you try to explain to 'em they could use their phone as a remote because

1139
01:02:16,080 --> 01:02:16,880
Speaker 6:  it doesn't work?

1140
01:02:16,880 --> 01:02:20,800
Speaker 4:  Absolutely not. I told them that they could could plug and this, when

1141
01:02:20,800 --> 01:02:24,200
Speaker 4:  I say this is like a Galaxy Brain moment, I was like, just use the charger

1142
01:02:24,200 --> 01:02:27,880
Speaker 4:  from your Chromebook and I plugged it in air and they're like, will will

1143
01:02:27,880 --> 01:02:31,760
Speaker 4:  it explode? Because you're just like, not, you're not supposed

1144
01:02:31,760 --> 01:02:32,500
Speaker 4:  to do this.

1145
01:02:32,500 --> 01:02:35,400
Speaker 10:  Ben Leon has a lot of evidence that that might happen. I'm just

1146
01:02:35,400 --> 01:02:39,420
Speaker 10:  saying.

1147
01:02:39,420 --> 01:02:43,360
Speaker 4:  But it was, I was like, oh God, I asked, I asked for this. You did. I

1148
01:02:43,360 --> 01:02:47,230
Speaker 4:  was like, apple make it all u s bbc. And they're like one out of three.

1149
01:02:47,230 --> 01:02:49,800
Speaker 6:  That's what's gonna happen with the iPhones. You're gonna get both of them

1150
01:02:49,800 --> 01:02:53,600
Speaker 6:  iPhones and for some reason one will be u s BBC and the other will

1151
01:02:53,600 --> 01:02:54,320
Speaker 6:  not just

1152
01:02:54,320 --> 01:02:54,720
Speaker 4:  Dealers

1153
01:02:54,720 --> 01:02:56,960
Speaker 6:  Choice Dad, you can charge with the

1154
01:02:56,960 --> 01:03:00,920
Speaker 6:  Chromebook. You have to go find the remote.

1155
01:03:00,920 --> 01:03:01,400
Speaker 6:  Yeah.

1156
01:03:01,400 --> 01:03:04,640
Speaker 4:  All right. Other stuff, Richard, what's going on with Elon this week? I feel

1157
01:03:04,640 --> 01:03:07,920
Speaker 4:  like it's a vert. We could go the entire show without talking about Elon

1158
01:03:07,920 --> 01:03:09,420
Speaker 6:  And Twitter. Speed run it go

1159
01:03:09,420 --> 01:03:13,240
Speaker 10:  Not nearly as much as there has been before. Okay. Everyone else is trying

1160
01:03:13,240 --> 01:03:16,760
Speaker 10:  to build their own Twitter. People who used to work for Twitter, other

1161
01:03:16,760 --> 01:03:20,690
Speaker 10:  tech engineers, Tumblr, of course Alex

1162
01:03:20,690 --> 01:03:22,960
Speaker 10:  is always ready to be the new Twitter.

1163
01:03:22,960 --> 01:03:24,140
Speaker 6:  They're ready.

1164
01:03:24,140 --> 01:03:27,120
Speaker 10:  And so if, if you're looking for a better Twitter, there are options. I

1165
01:03:27,120 --> 01:03:29,760
Speaker 10:  went back to Black Planet, I've had that account for 20 years. I'm never

1166
01:03:29,760 --> 01:03:33,720
Speaker 10:  leaving. Meanwhile,

1167
01:03:33,720 --> 01:03:37,160
Speaker 10:  at Twitter, Elon still somehow needs to try and make money from the company

1168
01:03:37,160 --> 01:03:40,880
Speaker 10:  that he now owns. And his plan is this Twitter blue

1169
01:03:40,880 --> 01:03:43,640
Speaker 10:  subscription that he thinks everyone will pay for to be verified and to

1170
01:03:43,640 --> 01:03:45,940
Speaker 10:  make sure that your tweets actually show up so that people can see them.

1171
01:03:45,940 --> 01:03:48,920
Speaker 10:  The latest thing that we've heard is that if you purchase this from your

1172
01:03:48,920 --> 01:03:52,680
Speaker 10:  iPhone, it might cost more that it would cost $7 per month if

1173
01:03:52,680 --> 01:03:56,200
Speaker 10:  you subscribe on the web or $11 via the iOS app. No one has mentioned Google

1174
01:03:56,200 --> 01:03:59,920
Speaker 10:  Play, I guess doesn't really matter. But you know, getting around the, the

1175
01:03:59,920 --> 01:04:02,880
Speaker 10:  Apple Commission that way. And that was the whole battle and back and forth

1176
01:04:02,880 --> 01:04:06,600
Speaker 10:  that they had. That culminated with Elon going to see Tim Cook and

1177
01:04:06,600 --> 01:04:10,000
Speaker 10:  reaching some kind of agreement. So may maybe this is the, the outcome of

1178
01:04:10,000 --> 01:04:10,250
Speaker 10:  that.

1179
01:04:10,250 --> 01:04:14,160
Speaker 4:  So the public facts of this agreement are now I think well known. So Apple

1180
01:04:14,160 --> 01:04:17,320
Speaker 4:  had pulled back on its advertising spend. Like most big companies it didn't

1181
01:04:17,320 --> 01:04:18,960
Speaker 4:  because they didn't know about the trust and

1182
01:04:18,960 --> 01:04:20,200
Speaker 6:  They were the biggest

1183
01:04:20,200 --> 01:04:23,440
Speaker 4:  Advertiser. And you are the biggest advertiser. Alex has pointed this out.

1184
01:04:23,440 --> 01:04:27,400
Speaker 4:  Apple spends no money on meta, just none. So no Instagram. No.

1185
01:04:27,400 --> 01:04:30,040
Speaker 4:  Which makes sense. They don't like each other. Yeah. So where's that money

1186
01:04:30,040 --> 01:04:32,730
Speaker 4:  gonna go? It has gone to Twitter. So Apple's one of Twitter's biggest accounts.

1187
01:04:32,730 --> 01:04:36,560
Speaker 4:  So Apple pulls back their advertising. Elon tweets don't you

1188
01:04:36,560 --> 01:04:40,440
Speaker 4:  support free speech in America? Then there's this weird issue

1189
01:04:40,440 --> 01:04:44,320
Speaker 4:  which we still kind of don't understand, where Apple's

1190
01:04:44,320 --> 01:04:47,360
Speaker 4:  like you've gotta comply with our moderation rules to the app store or we're

1191
01:04:47,360 --> 01:04:51,320
Speaker 4:  gonna pull you. Or they just remind as they often do

1192
01:04:51,320 --> 01:04:52,030
Speaker 4:  mafia style,

1193
01:04:52,030 --> 01:04:53,760
Speaker 6:  They just sat there with a baseball bat.

1194
01:04:53,760 --> 01:04:57,120
Speaker 4:  They just drove in their hand the store holding a rock. And they're like,

1195
01:04:57,120 --> 01:04:59,640
Speaker 4:  no way. Those windows, it would sure be a shame.

1196
01:04:59,640 --> 01:05:01,930
Speaker 6:  Wouldn't want anything to happen to him.

1197
01:05:01,930 --> 01:05:05,390
Speaker 4:  So Elon gets mad. Tim Cook summons

1198
01:05:05,390 --> 01:05:09,200
Speaker 4:  Elon. Oh, and and Elon gets mad about the 30% tax.

1199
01:05:09,200 --> 01:05:11,090
Speaker 4:  Yeah. Yeah. And she's been mad about for a while

1200
01:05:11,090 --> 01:05:14,720
Speaker 6:  He suddenly discovered, he was like, guys, I have brand new information.

1201
01:05:14,720 --> 01:05:16,500
Speaker 6:  There's a 30% tax.

1202
01:05:16,500 --> 01:05:19,540
Speaker 4:  But we talked about this last week. A bunch of Republican Congress people

1203
01:05:19,540 --> 01:05:23,520
Speaker 4:  get all up in arms, antitrust bill becomes a threat. Tim Cook

1204
01:05:23,520 --> 01:05:27,440
Speaker 4:  summons Elon to the spaceship. We don't know

1205
01:05:27,440 --> 01:05:31,080
Speaker 4:  what the deal was. Yeah. Who leaves? But now it's publicly revealed. Apple's

1206
01:05:31,080 --> 01:05:34,920
Speaker 4:  ad spending has gone back up and Elon has agreed to up

1207
01:05:34,920 --> 01:05:38,880
Speaker 4:  the price of Twitter blue on iOS 30%. So

1208
01:05:38,880 --> 01:05:42,480
Speaker 4:  it's like, oh, you just, Apple's paying you the money and ads and then you're

1209
01:05:42,480 --> 01:05:46,360
Speaker 4:  gonna pay it back to them in app store fees. That is the deal that the

1210
01:05:46,360 --> 01:05:48,160
Speaker 4:  the most genius business people of

1211
01:05:48,160 --> 01:05:49,840
Speaker 6:  Our, they did it have

1212
01:05:49,840 --> 01:05:50,310
Speaker 4:  Constructed.

1213
01:05:50,310 --> 01:05:50,920
Speaker 6:  Nailed

1214
01:05:50,920 --> 01:05:53,120
Speaker 4:  It. Here's what we're gonna do. We're gonna start advertising on Twitter

1215
01:05:53,120 --> 01:05:55,560
Speaker 4:  again, increase your revenue and then when you launch your paid product,

1216
01:05:55,560 --> 01:05:59,400
Speaker 4:  which you wants to be 50% of Twitter's revenue, 30% of that will come

1217
01:05:59,400 --> 01:06:03,350
Speaker 4:  right back to the old spaceship. Yep. Tim's like, all right, that you good.

1218
01:06:03,350 --> 01:06:06,680
Speaker 4:  I've gotta go do some supply chain stuff and Elon left

1219
01:06:06,680 --> 01:06:10,640
Speaker 4:  like that appears to be the deal. Yeah. That is all. And

1220
01:06:10,640 --> 01:06:14,560
Speaker 4:  that the advertising information is directly from Elon. What we

1221
01:06:14,560 --> 01:06:17,840
Speaker 4:  will see what the price of Twitter blue is when it comes out. But all indications

1222
01:06:17,840 --> 01:06:18,240
Speaker 4:  are when

1223
01:06:18,240 --> 01:06:18,760
Speaker 6:  It returns.

1224
01:06:18,760 --> 01:06:22,280
Speaker 4:  When it returns. But all indications are, it will be priced at, at

1225
01:06:22,280 --> 01:06:25,540
Speaker 4:  30% above the web price to account for that fee.

1226
01:06:25,540 --> 01:06:29,520
Speaker 4:  And as we all know, most things Apple

1227
01:06:29,520 --> 01:06:33,080
Speaker 4:  collects that fee because most of the money is made on the phone. Yeah. A

1228
01:06:33,080 --> 01:06:33,910
Speaker 4:  beautiful circle.

1229
01:06:33,910 --> 01:06:37,560
Speaker 6:  I love it. Like it's as beautiful a circle as the spaceship

1230
01:06:37,560 --> 01:06:39,280
Speaker 6:  itself. I'm sorry

1231
01:06:39,280 --> 01:06:39,560
Speaker 4:  You

1232
01:06:39,560 --> 01:06:40,400
Speaker 6:  Did it. I did it.

1233
01:06:40,400 --> 01:06:41,320
Speaker 4:  The modern economy.

1234
01:06:41,320 --> 01:06:44,560
Speaker 6:  You know what, let's, let's talk about Apple some more. Let's, let's, let's

1235
01:06:44,560 --> 01:06:45,680
Speaker 6:  get away from talking about, oh,

1236
01:06:45,680 --> 01:06:46,080
Speaker 4:  There's good Apple

1237
01:06:46,080 --> 01:06:49,880
Speaker 6:  Stuff. There's good apple stuff. Yeah. We got, so Apple announced a bunch

1238
01:06:49,880 --> 01:06:52,800
Speaker 6:  of encryption stuff this week, right. Richard? I think you were watching

1239
01:06:52,800 --> 01:06:55,280
Speaker 6:  it like in it, in the weeds on this. Yes.

1240
01:06:55,280 --> 01:06:59,040
Speaker 10:  This was a bit of a surprise, but it, it is major, major

1241
01:06:59,040 --> 01:07:02,320
Speaker 10:  news and, and I'm not sure if everyone, when you see this, if you understand

1242
01:07:02,320 --> 01:07:06,200
Speaker 10:  exactly how big this is, but a lot of the people who pay attention to security

1243
01:07:06,200 --> 01:07:09,920
Speaker 10:  and privacy on our devices on the internet see this as one of the, the biggest

1244
01:07:09,920 --> 01:07:13,200
Speaker 10:  announcements that they'll hear I guess around this time of the year. What

1245
01:07:13,200 --> 01:07:16,080
Speaker 10:  Apple has done is that they've announced a number of security announcements

1246
01:07:16,080 --> 01:07:19,920
Speaker 10:  and one of the most impactful ones is an optional program that

1247
01:07:19,920 --> 01:07:23,080
Speaker 10:  they're calling advanced data protection. And if you opt into it, then it

1248
01:07:23,080 --> 01:07:26,920
Speaker 10:  expands the number of data categories as they they call them, that are

1249
01:07:26,920 --> 01:07:29,560
Speaker 10:  protected by end-to-end encryption. And they've always talked about how

1250
01:07:29,560 --> 01:07:33,000
Speaker 10:  they have end-to-end encryption on a lot of things so that no one can really

1251
01:07:33,000 --> 01:07:36,560
Speaker 10:  spy on your data. Even on Apples end, if the government asks them to give

1252
01:07:36,560 --> 01:07:39,800
Speaker 10:  it up, they can't because they don't have access to it. It can only be decrypted

1253
01:07:39,800 --> 01:07:41,120
Speaker 10:  on your device.

1254
01:07:41,120 --> 01:07:42,060
Speaker 6:  Right.

1255
01:07:42,060 --> 01:07:45,560
Speaker 10:  But that did not apply to iPhone backups. So while something like

1256
01:07:45,560 --> 01:07:48,520
Speaker 10:  iMessage might be encrypted into end and they, they kept saying that it

1257
01:07:48,520 --> 01:07:52,440
Speaker 10:  is, if you or the person you're messaging has a, a backup of their iPhone

1258
01:07:52,440 --> 01:07:56,120
Speaker 10:  that is then uploaded to the iCloud servers. If, for example, if somehow

1259
01:07:56,120 --> 01:07:59,560
Speaker 10:  someone accessed your account or if they got a warrant from the government

1260
01:07:59,560 --> 01:08:03,200
Speaker 10:  to access it, they could simply access the unencrypted data very

1261
01:08:03,200 --> 01:08:07,040
Speaker 10:  easily. Right. And that won't be possible anymore once this rolls

1262
01:08:07,040 --> 01:08:10,600
Speaker 10:  out because, you know, your iPhone backups will, will be one of the categories

1263
01:08:10,600 --> 01:08:14,280
Speaker 10:  that are now protected by encryption. Which, which is just a very significant

1264
01:08:14,280 --> 01:08:17,480
Speaker 10:  change in one that, that the f fbi now that this has been announced and

1265
01:08:17,480 --> 01:08:20,960
Speaker 10:  said that they're not happy about. Yeah. Rather predictably,

1266
01:08:20,960 --> 01:08:24,000
Speaker 4:  Apple famously fought a huge battle with the FBI in 2016.

1267
01:08:24,000 --> 01:08:25,880
Speaker 6:  This is San Bernardino shootings right

1268
01:08:25,880 --> 01:08:29,840
Speaker 4:  Around unlocking the shooter's phone. The FBI has wanted a backdoor and

1269
01:08:29,840 --> 01:08:33,720
Speaker 4:  an encryption. Apple has famously said no. The most interesting turn of

1270
01:08:33,720 --> 01:08:37,580
Speaker 4:  this story, which we need to learn more about is,

1271
01:08:37,580 --> 01:08:38,990
Speaker 4:  was it last year?

1272
01:08:38,990 --> 01:08:39,480
Speaker 6:  Were

1273
01:08:39,480 --> 01:08:42,640
Speaker 4:  They earlier this year? The Csam thing? The csam thing. Apple, that was last

1274
01:08:42,640 --> 01:08:43,830
Speaker 4:  year announced. That was last year.

1275
01:08:43,830 --> 01:08:47,560
Speaker 10:  Yeah. Late, late last year. They announced these changes to Csam where would,

1276
01:08:47,560 --> 01:08:51,080
Speaker 10:  if you were backing up a photo on iCloud photos, it would then

1277
01:08:51,080 --> 01:08:55,040
Speaker 10:  scan the photo while it's still on your device, not on apples in, not

1278
01:08:55,040 --> 01:08:58,960
Speaker 10:  on the servers. And you know, kind of create a, a hash a

1279
01:08:58,960 --> 01:09:02,800
Speaker 10:  hash of it and try to match that to known child sexual abuse imagery

1280
01:09:02,800 --> 01:09:04,360
Speaker 10:  and alert someone if it finds out.

1281
01:09:04,360 --> 01:09:05,440
Speaker 6:  And everybody was mad

1282
01:09:05,440 --> 01:09:09,400
Speaker 10:  About it and no. And yeah, privacy, security, everyone was very

1283
01:09:09,400 --> 01:09:13,360
Speaker 10:  upset about this because the, it's kind of a, a, it's a slippery slope now

1284
01:09:13,360 --> 01:09:17,320
Speaker 10:  we're you're doing scanning on people's personal devices. B if those hashes,

1285
01:09:17,320 --> 01:09:21,000
Speaker 10:  if someone somehow gets something into the database that's like, we're looking

1286
01:09:21,000 --> 01:09:24,400
Speaker 10:  for government documents, you know, now suddenly your phone is leaking that

1287
01:09:24,400 --> 01:09:27,520
Speaker 10:  information there, there are a number of bad outcomes here. And now Apple

1288
01:09:27,520 --> 01:09:30,200
Speaker 10:  has has said that that program is dead. They're not going to do it.

1289
01:09:30,200 --> 01:09:33,800
Speaker 4:  Yeah. And I, I think longtime listeners in this show will

1290
01:09:33,800 --> 01:09:37,720
Speaker 4:  recall Apple was real shady about all of that. When it

1291
01:09:37,720 --> 01:09:40,960
Speaker 4:  rolled it out, they rolled it out. It wouldn't tell us who the researchers

1292
01:09:40,960 --> 01:09:44,760
Speaker 4:  were. All of its communications were like Unsourced.

1293
01:09:44,760 --> 01:09:47,280
Speaker 6:  They're like, just enjoy it. It's gonna be great.

1294
01:09:47,280 --> 01:09:50,080
Speaker 4:  Just shut up. They were holding these briefings without telling anybody who

1295
01:09:50,080 --> 01:09:53,400
Speaker 4:  anybody was talking to. Yeah. And they had basically courted all this

1296
01:09:53,400 --> 01:09:57,160
Speaker 4:  controversy and then they said, we're putting it on pause. And the

1297
01:09:57,160 --> 01:10:01,050
Speaker 4:  trade-off, I, I wanna be clear, the trade-off that Apple was trying to make

1298
01:10:01,050 --> 01:10:05,000
Speaker 4:  on paper was a good trade-off, which is we want

1299
01:10:05,000 --> 01:10:08,840
Speaker 4:  to protect your data as much as we can. We want to keep the

1300
01:10:08,840 --> 01:10:12,440
Speaker 4:  government and the cops out of our servers. We want to protect your

1301
01:10:12,440 --> 01:10:16,000
Speaker 4:  privacy, but we know that bad people do bad

1302
01:10:16,000 --> 01:10:19,960
Speaker 4:  things and the category of bad things that everybody agrees on

1303
01:10:19,960 --> 01:10:23,760
Speaker 4:  without a shadow of a doubt is csam. So we're going to

1304
01:10:23,760 --> 01:10:27,590
Speaker 4:  build in a, a somehow privacy respecting feature

1305
01:10:27,590 --> 01:10:31,360
Speaker 4:  that scans your phone for this material

1306
01:10:31,360 --> 01:10:35,040
Speaker 4:  before it hits our servers. And that that way we can encrypt

1307
01:10:35,040 --> 01:10:38,800
Speaker 4:  what's on the servers. We can encrypt iCloud. Yeah. Because Google,

1308
01:10:38,800 --> 01:10:42,500
Speaker 4:  Microsoft, every other cloud service is scanning their data stores

1309
01:10:42,500 --> 01:10:46,200
Speaker 4:  for csun. Right. They're all doing it. Like, and they

1310
01:10:46,200 --> 01:10:50,000
Speaker 4:  should like, everyone agrees that this is a bad thing. Apple's like we want

1311
01:10:50,000 --> 01:10:53,240
Speaker 4:  to go one step farther and encrypt the backup so we can't read it cuz our

1312
01:10:53,240 --> 01:10:55,790
Speaker 4:  other choice is to do everyone else does and scan iCloud.

1313
01:10:55,790 --> 01:10:56,650
Speaker 6:  Yeah.

1314
01:10:56,650 --> 01:11:00,120
Speaker 4:  So they're like, okay, we'll move this scanning to your phone and again on

1315
01:11:00,120 --> 01:11:03,270
Speaker 4:  paper, this trade off makes some kind of sense.

1316
01:11:03,270 --> 01:11:06,360
Speaker 6:  Yeah. I think in a vacuum, I think like if you're sitting in a room with

1317
01:11:06,360 --> 01:11:09,480
Speaker 6:  a bunch of people who've been working on this problem for years, which I

1318
01:11:09,480 --> 01:11:13,080
Speaker 6:  assume was, was happening at Apple. You say this and you go Yes. Awesome.

1319
01:11:13,080 --> 01:11:16,790
Speaker 6:  Perfect. And then they didn't actually check with

1320
01:11:16,790 --> 01:11:20,610
Speaker 6:  anyone else until right before I think they, like, they reached out

1321
01:11:20,610 --> 01:11:24,200
Speaker 6:  to the EF f and a couple other folks right before they were gonna do it and

1322
01:11:24,200 --> 01:11:26,600
Speaker 6:  they're like, we're gonna do this. And they were all like, cool. Yeah, you

1323
01:11:26,600 --> 01:11:29,880
Speaker 6:  waited way too late. This is a terrible plan and everything about it is bad

1324
01:11:29,880 --> 01:11:33,800
Speaker 6:  because you were still create fundamentally creating a backdoor into

1325
01:11:33,800 --> 01:11:37,600
Speaker 6:  people's devices and anybody with access to this database can

1326
01:11:37,600 --> 01:11:39,640
Speaker 6:  really misappropriate it and use it.

1327
01:11:39,640 --> 01:11:43,360
Speaker 4:  Yeah. And we have episodes of decoder and Avir like in the

1328
01:11:43,360 --> 01:11:47,280
Speaker 4:  controversy of why it was bad, but that was the trade off. Right? Right.

1329
01:11:47,280 --> 01:11:50,480
Speaker 4:  Everyone assumed that Apple is rolling out the scanning feature so they could

1330
01:11:50,480 --> 01:11:54,400
Speaker 4:  get the desired end state of encrypted

1331
01:11:54,400 --> 01:11:58,360
Speaker 4:  iCloud. Yeah. It's a year later they've rolled out encrypted

1332
01:11:58,360 --> 01:12:01,880
Speaker 4:  iCloud and they are canceling the phone

1333
01:12:01,880 --> 01:12:04,990
Speaker 4:  scanning, which is utterly fascinating to

1334
01:12:04,990 --> 01:12:08,270
Speaker 6:  Me. And they also added another thing which I thought was really interesting

1335
01:12:08,270 --> 01:12:11,110
Speaker 6:  cause one of the responses to this was, well, if you create this database

1336
01:12:11,110 --> 01:12:14,980
Speaker 6:  that is like managed by the government,

1337
01:12:14,980 --> 01:12:18,570
Speaker 6:  then other governments can misuse this, misuse this database.

1338
01:12:18,570 --> 01:12:22,310
Speaker 6:  And one of the things they also announced this week was like, they claim

1339
01:12:22,310 --> 01:12:25,910
Speaker 6:  that iMessage can possibly alert you if state

1340
01:12:25,910 --> 01:12:28,990
Speaker 6:  sponsored spies are eavesdropping on. That's good. You that's good. And it's

1341
01:12:28,990 --> 01:12:32,790
Speaker 6:  like, okay, so y'all just went hard in the other direction Yeah. To

1342
01:12:32,790 --> 01:12:34,550
Speaker 6:  say, nah, fuck the police

1343
01:12:34,550 --> 01:12:35,630
Speaker 4:  Good. Basically

1344
01:12:35,630 --> 01:12:36,750
Speaker 6:  That's Tim Cooke. Yeah. That's

1345
01:12:36,750 --> 01:12:40,150
Speaker 4:  Tim K When I think of Tim Cook, I think of a man saying fuck the police.

1346
01:12:40,150 --> 01:12:43,990
Speaker 6:  Yeah. Immediately. First thing I think of. But that was

1347
01:12:43,990 --> 01:12:47,550
Speaker 6:  like kind of surprising to me that they went that hard in the other direction.

1348
01:12:47,550 --> 01:12:50,630
Speaker 6:  And I, I, I keep thinking about like a lot of these conversations we had

1349
01:12:50,630 --> 01:12:54,310
Speaker 6:  last year about this was about China and specifically how China could potentially

1350
01:12:54,310 --> 01:12:56,560
Speaker 6:  use this database. And now like

1351
01:12:56,560 --> 01:13:00,270
Speaker 4:  We know, so Joanna Stern at the Wall Street Journal, notable

1352
01:13:00,270 --> 01:13:03,670
Speaker 4:  verge expat Joanna Stern. Yes. Lovely. Or mole at the Wall Street

1353
01:13:03,670 --> 01:13:07,630
Speaker 4:  Journal. Guess the deets, the journal, they're always detecting that.

1354
01:13:07,630 --> 01:13:11,390
Speaker 4:  They have a, they have a leaker. The

1355
01:13:11,390 --> 01:13:14,790
Speaker 4:  verges infiltrated us once again, but Joanna Stern got to be exclusive with

1356
01:13:14,790 --> 01:13:17,990
Speaker 4:  Craig Federer. You should go watch her video with him. She asked him a question

1357
01:13:17,990 --> 01:13:21,670
Speaker 4:  about China and our man just deferred. Yeah. Did not

1358
01:13:21,670 --> 01:13:25,080
Speaker 4:  answer the question about whether iCloud backups in China would be encrypted

1359
01:13:25,080 --> 01:13:28,710
Speaker 4:  in China. The government, the Chinese government effectively controls the

1360
01:13:28,710 --> 01:13:32,550
Speaker 4:  iCloud data centers. Yes. So Apple always kind

1361
01:13:32,550 --> 01:13:36,470
Speaker 4:  of make its concessions for the, I've been told China's

1362
01:13:36,470 --> 01:13:39,790
Speaker 4:  not in the EU always making concessions where it's large

1363
01:13:39,790 --> 01:13:43,310
Speaker 4:  market. Obviously there's a lot going on in China recently. We've not talked

1364
01:13:43,310 --> 01:13:46,950
Speaker 4:  about it in the show very much. There are large scale protests about Covid

1365
01:13:46,950 --> 01:13:50,430
Speaker 4:  Lockdowns. There are protests at the Foxcon factories where Apple makes the

1366
01:13:50,430 --> 01:13:54,390
Speaker 4:  phones or Apple production slowdown for its phones. Yep. Just quite a

1367
01:13:54,390 --> 01:13:55,150
Speaker 4:  lot going on in China.

1368
01:13:55,150 --> 01:13:58,670
Speaker 6:  And you've seen, you've also seen Apple make some really deliberate attempts

1369
01:13:58,670 --> 01:14:02,070
Speaker 6:  to move away from, from China, particularly in the, in, in

1370
01:14:02,070 --> 01:14:05,270
Speaker 6:  manufacturing. They've asked Foxcon, they've asked these other manufacturers

1371
01:14:05,270 --> 01:14:08,910
Speaker 6:  they work with to move to get out of there to like go to

1372
01:14:08,910 --> 01:14:12,220
Speaker 6:  India. To, to go to I think Arizona.

1373
01:14:12,220 --> 01:14:15,970
Speaker 6:  Arizona. Yes. Speaking of, and they've, they're asking 'em to move to these

1374
01:14:15,970 --> 01:14:19,260
Speaker 6:  other places. And I'm gonna use that as a segue to talk about

1375
01:14:19,260 --> 01:14:23,010
Speaker 6:  Arizona. Thank you. Richard laid that up for me because

1376
01:14:23,010 --> 01:14:26,850
Speaker 6:  the other big news this week from Apple was that they're

1377
01:14:26,850 --> 01:14:30,650
Speaker 6:  gonna start making chips for iPhones in Arizona. These

1378
01:14:30,650 --> 01:14:33,890
Speaker 6:  new fabs that T S M C just opened. Yeah.

1379
01:14:33,890 --> 01:14:36,210
Speaker 4:  So T M C I want to give T S M C credit here.

1380
01:14:36,210 --> 01:14:36,940
Speaker 6:  Yes.

1381
01:14:36,940 --> 01:14:40,770
Speaker 4:  Is the person who is dunked on Foxcon law lying to the people of

1382
01:14:40,770 --> 01:14:41,490
Speaker 4:  Wisconsin the most.

1383
01:14:41,490 --> 01:14:43,010
Speaker 6:  They did not build a big globe

1384
01:14:43,010 --> 01:14:46,010
Speaker 4:  Four years. They did not. T S M C took a bunch of tax

1385
01:14:46,010 --> 01:14:49,690
Speaker 4:  subsidies, lobbied their way into the CHIPS act, which is going

1386
01:14:49,690 --> 01:14:53,570
Speaker 4:  into effect. And they have actually built fabs in

1387
01:14:53,570 --> 01:14:56,490
Speaker 4:  Arizona. The fabs are gonna start producing chips in

1388
01:14:56,490 --> 01:15:00,090
Speaker 4:  2024. The ceremony that we saw with a md,

1389
01:15:00,090 --> 01:15:04,050
Speaker 4:  Nvidia, Tim Cook Biden, they're all, there was the installation

1390
01:15:04,050 --> 01:15:07,040
Speaker 4:  of Fab Equipment. It was not Golden Shovels.

1391
01:15:07,040 --> 01:15:07,330
Speaker 6:  I

1392
01:15:07,330 --> 01:15:11,160
Speaker 4:  Love it. Amazing. And they're gonna add a second site in 2026.

1393
01:15:11,160 --> 01:15:15,130
Speaker 4:  Liam looks so happy. So just congrats to SMC for doing a

1394
01:15:15,130 --> 01:15:18,270
Speaker 4:  thing they said they were gonna do. Yeah. And they're gonna do it. And so

1395
01:15:18,270 --> 01:15:22,130
Speaker 4:  Tim Cook Pres promised that the chips, they're gonna make M series and

1396
01:15:22,130 --> 01:15:25,650
Speaker 4:  Aeries chips there. A MD and Nvidia are gonna build chips there. It's like

1397
01:15:25,650 --> 01:15:26,610
Speaker 4:  a three nanometer node.

1398
01:15:26,610 --> 01:15:30,330
Speaker 6:  Yeah. Yeah. I mean this is like, this is the stuff that's really hard to

1399
01:15:30,330 --> 01:15:34,170
Speaker 6:  make. You know, Intel has, Intel's trying to get their own fabs o

1400
01:15:34,170 --> 01:15:38,040
Speaker 6:  online so that they can jump into this space. But T S M C kind of

1401
01:15:38,040 --> 01:15:41,330
Speaker 6:  owns the market on the, these really small

1402
01:15:41,330 --> 01:15:45,130
Speaker 6:  nodes. And, and so this is, this is big news because we

1403
01:15:45,130 --> 01:15:48,770
Speaker 6:  haven't had a node this small built in the United States in a while. It it's

1404
01:15:48,770 --> 01:15:52,690
Speaker 6:  primarily been built in Taiwan. And I'm gonna be really like, it should create

1405
01:15:52,690 --> 01:15:56,680
Speaker 6:  jobs theoretically. It should make that like the CHIPS act

1406
01:15:56,680 --> 01:16:00,530
Speaker 6:  make more sense. And, and suddenly you're instead of like

1407
01:16:00,530 --> 01:16:04,370
Speaker 6:  having to, like Intel will have a new resources for hiring,

1408
01:16:04,370 --> 01:16:08,050
Speaker 6:  right? Yeah. If this tmc s if this TSMC fab goes

1409
01:16:08,050 --> 01:16:08,600
Speaker 6:  online,

1410
01:16:08,600 --> 01:16:12,530
Speaker 4:  It's cool news. We sent a reporter named Andy Bly down to cover it. Took

1411
01:16:12,530 --> 01:16:15,370
Speaker 4:  photos, whole thing. Go read the airport. They actually did it. Yeah,

1412
01:16:15,370 --> 01:16:15,770
Speaker 6:  They did it.

1413
01:16:15,770 --> 01:16:19,530
Speaker 4:  Now is it as fun to cover as an empty dome and

1414
01:16:19,530 --> 01:16:20,850
Speaker 4:  lies? Bless.

1415
01:16:20,850 --> 01:16:21,410
Speaker 6:  We'll

1416
01:16:21,410 --> 01:16:22,850
Speaker 4:  See. No more important to the global economy

1417
01:16:22,850 --> 01:16:26,770
Speaker 10:  From a content perspective. It was pretty much a failure. I

1418
01:16:26,770 --> 01:16:27,110
Speaker 10:  I think

1419
01:16:27,110 --> 01:16:28,330
Speaker 4:  I'm like, where's your dome bro?

1420
01:16:28,330 --> 01:16:31,600
Speaker 6:  Yeah. Failure of content. But you know,

1421
01:16:31,600 --> 01:16:35,330
Speaker 10:  Before we, I do wanna mention, they, they unmade other

1422
01:16:35,330 --> 01:16:37,970
Speaker 10:  security announcements like you can use, you'll be able to use hardware

1423
01:16:37,970 --> 01:16:41,650
Speaker 10:  keys to secure your iCloud account. You'll be able to do this kind of key

1424
01:16:41,650 --> 01:16:45,080
Speaker 10:  verification, which you were referencing, Alex, that that will

1425
01:16:45,080 --> 01:16:48,770
Speaker 10:  enable that or potentially enable that ability to see if someone's trying

1426
01:16:48,770 --> 01:16:51,730
Speaker 10:  to spy on your a conversation that makes it a lot more like Signal or like

1427
01:16:51,730 --> 01:16:55,570
Speaker 10:  using PPG P and email or something like that. So there, there are

1428
01:16:55,570 --> 01:16:59,050
Speaker 10:  other, there are also other important security announcements that they made

1429
01:16:59,050 --> 01:17:02,450
Speaker 10:  that, that are are there. And they also made a, a very strange change they're

1430
01:17:02,450 --> 01:17:06,410
Speaker 10:  going to roll out globally. This change that rolled out in China during

1431
01:17:06,410 --> 01:17:10,280
Speaker 10:  the protests where when you turn on airdrop, I think to receive

1432
01:17:10,280 --> 01:17:13,000
Speaker 10:  files, you'll only be able to do it for 10 minutes. Which makes it a little

1433
01:17:13,000 --> 01:17:16,960
Speaker 10:  bit harder to use this strategy that people have been using to distribute,

1434
01:17:16,960 --> 01:17:19,560
Speaker 10:  you know, information that the government doesn't want it to get passed

1435
01:17:19,560 --> 01:17:20,310
Speaker 10:  around.

1436
01:17:20,310 --> 01:17:22,010
Speaker 6:  I just shared gritty photos.

1437
01:17:22,010 --> 01:17:24,320
Speaker 4:  Oh my God. That's it. We we're, we're so we're already

1438
01:17:24,320 --> 01:17:26,560
Speaker 6:  Over. We, we gotta cut the break. We got, we gotta

1439
01:17:26,560 --> 01:17:30,280
Speaker 4:  William's losing his mind back here. Don't be nice to

1440
01:17:30,280 --> 01:17:31,160
Speaker 4:  Liam.

1441
01:17:31,160 --> 01:17:31,560
Speaker 6:  Everyone be nice.

1442
01:17:31,560 --> 01:17:35,120
Speaker 4:  There's no clock this time. There's just a sense of shame that

1443
01:17:35,120 --> 01:17:38,980
Speaker 4:  pervades this room because we are at 90 minutes already

1444
01:17:38,980 --> 01:17:40,320
Speaker 4:  and we have another second here.

1445
01:17:40,320 --> 01:17:40,840
Speaker 6:  We're having a great

1446
01:17:40,840 --> 01:17:43,400
Speaker 4:  Time. We're gonna be right back. We got a gadget. Lightning around. We'll

1447
01:17:43,400 --> 01:17:43,600
Speaker 4:  be right

1448
01:17:43,600 --> 01:17:50,740
Speaker 4:  back.

1449
01:17:50,740 --> 01:17:53,760
Speaker 4:  All right, we're back. Wait, this is gonna be a lightning gadget.

1450
01:17:53,760 --> 01:17:56,560
Speaker 6:  Lightning of lightning rounds. It's all right. There wasn't a lot of gadgets

1451
01:17:56,560 --> 01:18:00,200
Speaker 6:  this week. Go for it. Okay, so first up, this is my favorite thing ever

1452
01:18:00,200 --> 01:18:04,110
Speaker 6:  released in the history of the world. Huawei's latest smartwatch.

1453
01:18:04,110 --> 01:18:05,340
Speaker 4:  This is so tough.

1454
01:18:05,340 --> 01:18:08,940
Speaker 6:  Has earbuds in it. So you can be like, oh, I gotta listen to this call.

1455
01:18:08,940 --> 01:18:12,560
Speaker 6:  And you just go, boop and flip the, the top of your

1456
01:18:12,560 --> 01:18:13,040
Speaker 6:  smartwatch.

1457
01:18:13,040 --> 01:18:16,880
Speaker 4:  This is just a digital watch. It's, it's not a smart Is this a black

1458
01:18:16,880 --> 01:18:19,120
Speaker 4:  and white segment l CD display?

1459
01:18:19,120 --> 01:18:23,000
Speaker 6:  You know, it's got weather, it's got heart rate. I'm calling

1460
01:18:23,000 --> 01:18:25,240
Speaker 6:  it smart. It's

1461
01:18:25,240 --> 01:18:28,200
Speaker 4:  Smart. Oh, no, no, no. That's just a face. That's just a face. And

1462
01:18:28,200 --> 01:18:32,040
Speaker 6:  What is smarter than having built in earbuds, wireless

1463
01:18:32,040 --> 01:18:35,810
Speaker 6:  earbuds that you can just pop out, stick in your ear.

1464
01:18:35,810 --> 01:18:38,950
Speaker 6:  They announced it on on Weibo, which is a Chinese

1465
01:18:38,950 --> 01:18:42,100
Speaker 6:  network and they're called the

1466
01:18:42,100 --> 01:18:45,810
Speaker 6:  Huawei watch. Budds. They've already launched

1467
01:18:45,810 --> 01:18:46,940
Speaker 6:  just not an America.

1468
01:18:46,940 --> 01:18:50,790
Speaker 4:  Wait, okay, first of all, watch Budds an incredible name. Second,

1469
01:18:50,790 --> 01:18:54,560
Speaker 4:  why would they demo this with this face? That's like

1470
01:18:54,560 --> 01:18:56,930
Speaker 4:  a 1982 Casio segmented

1471
01:18:56,930 --> 01:18:58,640
Speaker 6:  Because it's sick.

1472
01:18:58,640 --> 01:19:00,330
Speaker 4:  I will admit they got me because,

1473
01:19:00,330 --> 01:19:02,820
Speaker 10:  Because they know that I'm the person.

1474
01:19:02,820 --> 01:19:03,490
Speaker 4:  Richard,

1475
01:19:03,490 --> 01:19:04,450
Speaker 6:  Have you already imported

1476
01:19:04,450 --> 01:19:07,610
Speaker 10:  It? And directly me, they they were like, yes. They they they knew who they

1477
01:19:07,610 --> 01:19:07,930
Speaker 10:  knew who they

1478
01:19:07,930 --> 01:19:10,130
Speaker 4:  Right. Can we talk about the most important gad of the week, which is the

1479
01:19:10,130 --> 01:19:13,200
Speaker 4:  $950 Dyson combo platter,

1480
01:19:13,200 --> 01:19:16,280
Speaker 4:  headphones and air purifier like mask.

1481
01:19:16,280 --> 01:19:19,650
Speaker 6:  Liam and I immediately looked, Vong was in the office today and we looked

1482
01:19:19,650 --> 01:19:21,910
Speaker 6:  at her and were like, you're getting it right. And she's like, I'm gonna

1483
01:19:21,910 --> 01:19:24,650
Speaker 6:  try, I want nothing to be reviewed more.

1484
01:19:24,650 --> 01:19:28,570
Speaker 4:  Okay. My favorite kinds of companies in the world are companies

1485
01:19:28,570 --> 01:19:32,480
Speaker 4:  that have like one piece of technology and they're like,

1486
01:19:32,480 --> 01:19:33,610
Speaker 4:  what can we do with

1487
01:19:33,610 --> 01:19:35,010
Speaker 6:  It? Dyson's like wind.

1488
01:19:35,010 --> 01:19:37,650
Speaker 4:  Yeah. Dyson's. Like, here's what we did. We made a fan. It's

1489
01:19:37,650 --> 01:19:38,130
Speaker 6:  A sick

1490
01:19:38,130 --> 01:19:41,970
Speaker 4:  Ass fan. A fan can both suck and blow. And

1491
01:19:41,970 --> 01:19:45,250
Speaker 4:  for like 10 years they're like, we're deeply focused on

1492
01:19:45,250 --> 01:19:49,010
Speaker 4:  suck. Like, what have you guys seen our vacuums?

1493
01:19:49,010 --> 01:19:52,770
Speaker 4:  That's just a fan going this way. And then kind of like

1494
01:19:52,770 --> 01:19:53,230
Speaker 4:  recently,

1495
01:19:53,230 --> 01:19:55,000
Speaker 6:  But the other way,

1496
01:19:55,000 --> 01:19:57,140
Speaker 4:  Switch that motherfucker to blow

1497
01:19:57,140 --> 01:19:59,440
Speaker 6:  Reverse baby. And they're like,

1498
01:19:59,440 --> 01:20:02,090
Speaker 4:  Here's the fan, here's the hair

1499
01:20:02,090 --> 01:20:05,850
Speaker 6:  Dryer. I used it today before the show. It's

1500
01:20:05,850 --> 01:20:06,290
Speaker 6:  rain.

1501
01:20:06,290 --> 01:20:09,950
Speaker 4:  It's such incredible that they're like, here's what we do fans.

1502
01:20:09,950 --> 01:20:13,320
Speaker 4:  They for a minute they're like, we're gonna do cars. And then I think

1503
01:20:13,320 --> 01:20:15,110
Speaker 4:  they realized like,

1504
01:20:15,110 --> 01:20:16,440
Speaker 6:  Cars don't run on fans.

1505
01:20:16,440 --> 01:20:20,360
Speaker 4:  They're like, well it's not a climate control fan. Does anyone know how

1506
01:20:20,360 --> 01:20:21,920
Speaker 4:  to do any other part of the

1507
01:20:21,920 --> 01:20:24,680
Speaker 6:  Car? They should start doing the hover. The hover. She

1508
01:20:24,680 --> 01:20:28,480
Speaker 4:  Doesn't hovercraft craft. And now they're like,

1509
01:20:28,480 --> 01:20:32,440
Speaker 4:  all right, what's, what can we put a fan into? And

1510
01:20:32,440 --> 01:20:36,400
Speaker 4:  they're like, headphones. And some like, it has to be,

1511
01:20:36,400 --> 01:20:39,880
Speaker 4:  there was a meeting where they're like, what else can we put fans

1512
01:20:39,880 --> 01:20:42,660
Speaker 4:  into? The car didn't work. We need an idea.

1513
01:20:42,660 --> 01:20:46,560
Speaker 6:  And you know what's really sad? Party speakers were right

1514
01:20:46,560 --> 01:20:48,050
Speaker 6:  there.

1515
01:20:48,050 --> 01:20:52,000
Speaker 4:  They're coming. And someone was like, I got it. Headphones.

1516
01:20:52,000 --> 01:20:54,800
Speaker 4:  Headphones. And there was a blank stare in the room. And they're like, you

1517
01:20:54,800 --> 01:20:55,400
Speaker 4:  just wait and

1518
01:20:55,400 --> 01:20:58,000
Speaker 6:  See. But with a mask on the front. And

1519
01:20:58,000 --> 01:21:00,550
Speaker 4:  It's, I mean you have to see, I'm buying these so

1520
01:21:00,550 --> 01:21:02,400
Speaker 4:  much I

1521
01:21:02,400 --> 01:21:05,400
Speaker 6:  You're gonna spend $949.

1522
01:21:05,400 --> 01:21:09,120
Speaker 4:  The best problem with them is that they're vaporware. Yes.

1523
01:21:09,120 --> 01:21:11,990
Speaker 4:  I'm sorry. The best part of them is that they're vaporware

1524
01:21:11,990 --> 01:21:14,720
Speaker 6:  Says, it says you'll be able to pre-order. You can

1525
01:21:14,720 --> 01:21:16,740
Speaker 4:  Only, they were announced like a year ago.

1526
01:21:16,740 --> 01:21:18,480
Speaker 6:  But you can start pre-ordering next year

1527
01:21:18,480 --> 01:21:19,900
Speaker 4:  And can pre-order

1528
01:21:19,900 --> 01:21:23,800
Speaker 6:  Not even there. No. Next year. I, I think that, I think they've,

1529
01:21:23,800 --> 01:21:24,970
Speaker 6:  they've got a shot.

1530
01:21:24,970 --> 01:21:28,280
Speaker 4:  They're, I mean they're, they're great. This is the only gadget worth buying.

1531
01:21:28,280 --> 01:21:32,200
Speaker 4:  I'm just excited to see what else Dyson can put a fan into. I

1532
01:21:32,200 --> 01:21:35,120
Speaker 4:  mean, the fact they were like, we're gonna do cars and then they're like,

1533
01:21:35,120 --> 01:21:37,400
Speaker 4:  mm. Not enough fans.

1534
01:21:37,400 --> 01:21:38,420
Speaker 6:  Not enough fans

1535
01:21:38,420 --> 01:21:40,380
Speaker 4:  Are playing for a wind power

1536
01:21:40,380 --> 01:21:44,120
Speaker 6:  Car. Like air conditioners. Have you considered the air conditioner?

1537
01:21:44,120 --> 01:21:46,150
Speaker 6:  No. No. They just, they're like headphones.

1538
01:21:46,150 --> 01:21:47,160
Speaker 4:  Yeah. Headphones.

1539
01:21:47,160 --> 01:21:48,520
Speaker 6:  This is it. That's

1540
01:21:48,520 --> 01:21:48,680
Speaker 4:  Very good.

1541
01:21:48,680 --> 01:21:49,360
Speaker 6:  Shoes.

1542
01:21:49,360 --> 01:21:52,160
Speaker 4:  Shoes. Very good. Very good. What else you got?

1543
01:21:52,160 --> 01:21:56,140
Speaker 6:  Alright, what else do we got? We, I believe Allison

1544
01:21:56,140 --> 01:21:58,560
Speaker 6:  did a review of the Amazon Etho

1545
01:21:58,560 --> 01:22:01,400
Speaker 6:  Auto. I refused to say it the other

1546
01:22:01,400 --> 01:22:04,470
Speaker 6:  way. She wasn't a fan.

1547
01:22:04,470 --> 01:22:07,690
Speaker 4:  This thing still makes no sense to me. Yeah. I don't know why you would want

1548
01:22:07,690 --> 01:22:08,900
Speaker 4:  an echo on.

1549
01:22:08,900 --> 01:22:12,890
Speaker 6:  One of the bad things was Alexa isn't very helpful in a car

1550
01:22:12,890 --> 01:22:16,600
Speaker 6:  because Yeah. Alexa isn't very helpful in a car. There's no

1551
01:22:16,600 --> 01:22:20,360
Speaker 6:  smart lights in a car to control and you can just hit a button

1552
01:22:20,360 --> 01:22:21,150
Speaker 6:  to play music.

1553
01:22:21,150 --> 01:22:25,040
Speaker 4:  Well, so I think what they know is that people have a lot of

1554
01:22:25,040 --> 01:22:29,000
Speaker 4:  ideas in cars while they're driving around. Right. Like again on

1555
01:22:29,000 --> 01:22:31,800
Speaker 4:  paper. Yeah. Imagine the meeting. Yeah. Like what do people wanna do in their

1556
01:22:31,800 --> 01:22:34,320
Speaker 4:  car? They wanna play music. What is the number one use of Alexa? People ask

1557
01:22:34,320 --> 01:22:37,880
Speaker 4:  for music. Yes. I dunno how many timers people set in their cars. This is

1558
01:22:37,880 --> 01:22:39,240
Speaker 4:  number two.

1559
01:22:39,240 --> 01:22:43,080
Speaker 6:  Every time I'm trying to like make that, that, that get the miles. Yeah.

1560
01:22:43,080 --> 01:22:45,760
Speaker 6:  Right. You know, I'm like, all right, set a timer. So

1561
01:22:45,760 --> 01:22:47,160
Speaker 4:  Timer for five minutes. The

1562
01:22:47,160 --> 01:22:47,620
Speaker 6:  Gun it.

1563
01:22:47,620 --> 01:22:49,480
Speaker 4:  All right. Fair. We're

1564
01:22:49,480 --> 01:22:50,840
Speaker 6:  All different. I've never done that. You

1565
01:22:50,840 --> 01:22:53,440
Speaker 4:  Should start, I'm going to, so like, okay, people want to ask for music.

1566
01:22:53,440 --> 01:22:55,800
Speaker 4:  We can add the thing for music and then maybe when they're driving around

1567
01:22:55,800 --> 01:22:58,280
Speaker 4:  they'll be like, you should buy some paper towels. Oh, I forgot to buy paper

1568
01:22:58,280 --> 01:23:01,910
Speaker 4:  towels. Alexa, buy some paper. Like you, you can see the thought process.

1569
01:23:01,910 --> 01:23:03,100
Speaker 4:  Yeah.

1570
01:23:03,100 --> 01:23:06,680
Speaker 6:  And then they remember that nobody actually will ever do that.

1571
01:23:06,680 --> 01:23:07,910
Speaker 4:  It's a real problem.

1572
01:23:07,910 --> 01:23:11,720
Speaker 6:  It's a real, like, it's just bad. I love like

1573
01:23:11,720 --> 01:23:15,680
Speaker 6:  all of her, her problems with it where it's like, yeah, Alexa's just stupid

1574
01:23:15,680 --> 01:23:18,590
Speaker 6:  to have in a car.

1575
01:23:18,590 --> 01:23:22,080
Speaker 4:  They gotta add chat G p T to it. Yeah. Or a fan. Or

1576
01:23:22,080 --> 01:23:25,760
Speaker 10:  A fan. It's, it's also like that weird Spotify thing in that you still

1577
01:23:25,760 --> 01:23:29,560
Speaker 10:  pretty much need your phone, but you can just do this stuff with your

1578
01:23:29,560 --> 01:23:32,760
Speaker 6:  Phone. But the Spotify one, you can at least like lar connect to your computer

1579
01:23:32,760 --> 01:23:33,520
Speaker 6:  and make a new control.

1580
01:23:33,520 --> 01:23:35,290
Speaker 10:  It had a knob. It had a knob. Yeah. So

1581
01:23:35,290 --> 01:23:38,800
Speaker 6:  It had the knob put a knob on this. I bet it would've gone up to a

1582
01:23:38,800 --> 01:23:41,260
Speaker 6:  seven. Alison would've loved it.

1583
01:23:41,260 --> 01:23:45,100
Speaker 4:  Anything with a giant rotary knob always wins for us. Anything. Okay. What

1584
01:23:45,100 --> 01:23:48,210
Speaker 4:  are these watches? The Choros Apex two.

1585
01:23:48,210 --> 01:23:51,940
Speaker 6:  Yeah. So v reviewed some, some new

1586
01:23:51,940 --> 01:23:55,820
Speaker 6:  multi-sport watches. She, she liked them. They didn't have a knob on 'em,

1587
01:23:55,820 --> 01:23:59,580
Speaker 6:  but she still gave them a seven. They're, they're pretty expensive. But I

1588
01:23:59,580 --> 01:24:02,940
Speaker 6:  think the big thing here is that really good battery life. They're, they're

1589
01:24:02,940 --> 01:24:06,520
Speaker 6:  really for runners. They're really for people who are like, they go outdoors.

1590
01:24:06,520 --> 01:24:10,500
Speaker 6:  I'm not familiar with these kinds of people, so I can't say if

1591
01:24:10,500 --> 01:24:14,260
Speaker 6:  this is a good purchase for people. But Victoria does in fact

1592
01:24:14,260 --> 01:24:17,700
Speaker 6:  go outside on a regular basis and, and she, she seemed to

1593
01:24:17,700 --> 01:24:20,330
Speaker 4:  Like them. Okay. We gotta talk about the symphonics floor lamp

1594
01:24:20,330 --> 01:24:21,570
Speaker 6:  Syon.

1595
01:24:21,570 --> 01:24:24,970
Speaker 4:  It's amazing that they just added legs to it and change the shade.

1596
01:24:24,970 --> 01:24:26,140
Speaker 6:  What did you

1597
01:24:26,140 --> 01:24:29,740
Speaker 4:  Taller? I I'm so ready to get these. Yeah. And just be like everywhere in

1598
01:24:29,740 --> 01:24:33,620
Speaker 4:  my home there are tiny lamp speakers and I don't know why,

1599
01:24:33,620 --> 01:24:35,030
Speaker 4:  I don't know why I feel this way.

1600
01:24:35,030 --> 01:24:38,610
Speaker 6:  Because you want your home to be like

1601
01:24:38,610 --> 01:24:41,310
Speaker 6:  one of those really fancy rich people's homes where you're like, where's

1602
01:24:41,310 --> 01:24:43,870
Speaker 6:  that music coming from? Like when you go to the apples

1603
01:24:43,870 --> 01:24:44,490
Speaker 4:  Lamps

1604
01:24:44,490 --> 01:24:47,590
Speaker 6:  And you're like, where is that? Is that music coming from a rock now? It's

1605
01:24:47,590 --> 01:24:48,120
Speaker 6:  your lamps.

1606
01:24:48,120 --> 01:24:51,910
Speaker 4:  So it's 260 bucks. The standard lamp seeker is

1607
01:24:51,910 --> 01:24:55,840
Speaker 4:  $179. So it's an $80 premium for some legs.

1608
01:24:55,840 --> 01:24:57,730
Speaker 6:  So you could just put it on something tall.

1609
01:24:57,730 --> 01:25:01,590
Speaker 4:  Yes. This is a choice that you have. Okay. I am very excited about

1610
01:25:01,590 --> 01:25:05,510
Speaker 4:  just the idea. I, I think the CS is gonna be Richard. I like Richard

1611
01:25:05,510 --> 01:25:07,510
Speaker 4:  and I used to cover like sea, like we were in the

1612
01:25:07,510 --> 01:25:08,350
Speaker 6:  Gonna be tall, smart

1613
01:25:08,350 --> 01:25:11,190
Speaker 4:  Speakers together, speakers and Gotcha. Like back in the day. Yeah. Right.

1614
01:25:11,190 --> 01:25:15,150
Speaker 4:  Like I remember like when Panas or like Pioneer and Panasonic would

1615
01:25:15,150 --> 01:25:18,710
Speaker 4:  like demo plasma TVs to us. Like that's how Richard and I came up.

1616
01:25:18,710 --> 01:25:22,470
Speaker 4:  Right. I remember the HD D V d Blu-Ray format war that Richard and I

1617
01:25:22,470 --> 01:25:25,270
Speaker 4:  covered together. Aw. And so see when

1618
01:25:25,270 --> 01:25:29,190
Speaker 10:  We were on, I was on the plane on the way to CES when they announced

1619
01:25:29,190 --> 01:25:30,330
Speaker 10:  that it was over.

1620
01:25:30,330 --> 01:25:33,550
Speaker 4:  I'm not over. We were like, we were like gear strong. Like this is the year

1621
01:25:33,550 --> 01:25:37,390
Speaker 4:  the format war and they're like hdv dead. They had to tear down the

1622
01:25:37,390 --> 01:25:41,190
Speaker 4:  booth at CES is a real thing that happened. And the dead of

1623
01:25:41,190 --> 01:25:41,910
Speaker 4:  night. Well

1624
01:25:41,910 --> 01:25:42,660
Speaker 6:  You over were there.

1625
01:25:42,660 --> 01:25:46,520
Speaker 4:  Yeah. They were gonna, it was gonna be the year of the format war. And between

1626
01:25:46,520 --> 01:25:48,350
Speaker 4:  HD and, and Blu-ray

1627
01:25:48,350 --> 01:25:49,970
Speaker 6:  And Shiba said, no, we're done.

1628
01:25:49,970 --> 01:25:51,220
Speaker 10:  And Shiba just,

1629
01:25:51,220 --> 01:25:54,550
Speaker 4:  Were just like, they got there, they looked around, they're like, what if

1630
01:25:54,550 --> 01:25:58,110
Speaker 4:  we just like hung out in Vegas and they tore the truth down.

1631
01:25:58,110 --> 01:26:01,430
Speaker 6:  Same honestly. Same. I do not judge you one bitch tba

1632
01:26:01,430 --> 01:26:04,680
Speaker 4:  Since then, there has not been a lot of drama in cs, I would say. But I CS

1633
01:26:04,680 --> 01:26:07,920
Speaker 4:  is always a TV show. Yeah. I would say this is the year where I'm expecting

1634
01:26:07,920 --> 01:26:11,760
Speaker 4:  to see like some interesting TVs and a lot of like

1635
01:26:11,760 --> 01:26:15,120
Speaker 4:  really wild atmo like gum theater ideas. Yeah. It's

1636
01:26:15,120 --> 01:26:15,790
Speaker 4:  coming.

1637
01:26:15,790 --> 01:26:16,400
Speaker 6:  I, I would

1638
01:26:16,400 --> 01:26:20,310
Speaker 4:  Like that. And these like Sonos Wireless rears is lamps is like,

1639
01:26:20,310 --> 01:26:23,200
Speaker 4:  it's just the beginning. It's just the drop.

1640
01:26:23,200 --> 01:26:26,160
Speaker 6:  It's not Sonos. It works with Sonos, but it's IKEA lamps.

1641
01:26:26,160 --> 01:26:28,280
Speaker 4:  Yeah. Yeah. They're out ahead of the flood.

1642
01:26:28,280 --> 01:26:29,280
Speaker 6:  Yeah.

1643
01:26:29,280 --> 01:26:30,570
Speaker 4:  Like this is, well

1644
01:26:30,570 --> 01:26:33,820
Speaker 10:  I think what we're gonna see is we, we've got these engineers who they,

1645
01:26:33,820 --> 01:26:36,610
Speaker 10:  we obviously had the pandemic and everybody suddenly was stuck at home.

1646
01:26:36,610 --> 01:26:39,970
Speaker 10:  You have these engineers who were at their house all the time. The stuff

1647
01:26:39,970 --> 01:26:43,650
Speaker 10:  that they built is finally coming. It's going to get strange. Just

1648
01:26:43,650 --> 01:26:47,030
Speaker 6:  Roll with it. And not a single one of them knows what a good lamp looks like.

1649
01:26:47,030 --> 01:26:50,290
Speaker 4:  And I'm just saying this is like, they're out ahead of the CS rush. Yeah.

1650
01:26:50,290 --> 01:26:52,770
Speaker 4:  They announce some stuff. You can't buy it now. It's not until January. So

1651
01:26:52,770 --> 01:26:55,450
Speaker 4:  this isn't like stocking stuff or stuff. Yes. Although the idea of a, of

1652
01:26:55,450 --> 01:26:58,040
Speaker 4:  a speaker lamp in your stocking is incredible.

1653
01:26:58,040 --> 01:27:00,890
Speaker 6:  What's that giant eight foot tall box in my what?

1654
01:27:00,890 --> 01:27:01,960
Speaker 4:  Christmas?

1655
01:27:01,960 --> 01:27:02,770
Speaker 6:  What could

1656
01:27:02,770 --> 01:27:06,210
Speaker 4:  It be? So they're just out ahead of the flood. Yeah. And I'm telling you,

1657
01:27:06,210 --> 01:27:09,970
Speaker 4:  this is the first drop of ces. Crazy. It's coming. It's

1658
01:27:09,970 --> 01:27:12,290
Speaker 6:  The speakers the year, the speakers in chat. G p

1659
01:27:12,290 --> 01:27:16,130
Speaker 4:  T. Richard, do you know anything about this DGI Mini three drone? I'm in

1660
01:27:16,130 --> 01:27:19,040
Speaker 4:  the market for another drone that I'll never use. And this feels like it.

1661
01:27:19,040 --> 01:27:22,250
Speaker 10:  I that, that's the only thing I think about is buying drones that I never

1662
01:27:22,250 --> 01:27:26,010
Speaker 10:  actually fly because I bought Hey D g I, I don't even remember what the

1663
01:27:26,010 --> 01:27:27,620
Speaker 10:  name of it is. I have a Phantom.

1664
01:27:27,620 --> 01:27:31,070
Speaker 4:  Oh my God. You have like a, you have like a Phantom three or a Phantom four.

1665
01:27:31,070 --> 01:27:34,690
Speaker 4:  Yes. You're a maniac. A three I think. I, I bought a Phantom four and I flew,

1666
01:27:34,690 --> 01:27:38,350
Speaker 4:  I sold it when I had a kid. I was like, I gotta get ready for this baby.

1667
01:27:38,350 --> 01:27:41,370
Speaker 4:  And Becky's like, I redone the house and I was like, I sold the

1668
01:27:41,370 --> 01:27:44,480
Speaker 4:  drone.

1669
01:27:44,480 --> 01:27:48,250
Speaker 6:  That's the verge equivalent of I got rid of the motorcycle for

1670
01:27:48,250 --> 01:27:51,650
Speaker 6:  you. These fingers are safe. Now I can pick up our

1671
01:27:51,650 --> 01:27:52,570
Speaker 6:  child.

1672
01:27:52,570 --> 01:27:53,890
Speaker 4:  She's like, great, are you gonna

1673
01:27:53,890 --> 01:27:57,210
Speaker 4:  help? She's

1674
01:27:57,210 --> 01:28:00,930
Speaker 10:  Like, and now they have drone that you can buy and never actually

1675
01:28:00,930 --> 01:28:01,760
Speaker 10:  fly anywhere.

1676
01:28:01,760 --> 01:28:04,410
Speaker 6:  Well this is an expensive drone though. 859.

1677
01:28:04,410 --> 01:28:07,570
Speaker 4:  Yeah, but it's kind of, the remote has a screen. It's a little, okay, that's

1678
01:28:07,570 --> 01:28:11,370
Speaker 4:  cool. I have the original mini, which is like just

1679
01:28:11,370 --> 01:28:14,970
Speaker 4:  not quite good enough. Yeah. I'm so shocked that DJ is still selling

1680
01:28:14,970 --> 01:28:18,570
Speaker 4:  stuff in the United States. Like they're on the, you're a Chinese

1681
01:28:18,570 --> 01:28:20,530
Speaker 4:  company hit list for the long time. For the longest

1682
01:28:20,530 --> 01:28:23,770
Speaker 6:  Time, yes. Because this is a D G I is a Chinese company, but they're also

1683
01:28:23,770 --> 01:28:26,190
Speaker 6:  the only company that makes like good drones.

1684
01:28:26,190 --> 01:28:29,330
Speaker 4:  I'm just saying they're a Chinese company whose product is flying

1685
01:28:29,330 --> 01:28:32,930
Speaker 4:  cameras. Like of all the companies, like, huh, this seems

1686
01:28:32,930 --> 01:28:36,010
Speaker 6:  Like a product. And the American government went, yeah, but I wanna fly my

1687
01:28:36,010 --> 01:28:36,690
Speaker 6:  drones. Yeah.

1688
01:28:36,690 --> 01:28:39,610
Speaker 4:  They're like, yes, but Hollywood needs these. Have you seen car commercials?

1689
01:28:39,610 --> 01:28:40,920
Speaker 4:  What are we gonna do?

1690
01:28:40,920 --> 01:28:42,850
Speaker 6:  Like helicopters? Absolutely not

1691
01:28:42,850 --> 01:28:46,610
Speaker 4:  Stable fusion. We gotta wrap this up. It's, we've,

1692
01:28:46,610 --> 01:28:47,960
Speaker 4:  we're so far over.

1693
01:28:47,960 --> 01:28:51,890
Speaker 6:  I mean, wait, important, important news.

1694
01:28:51,890 --> 01:28:55,330
Speaker 6:  Somebody just let me know that the Amazon Kindle scribe is getting a software

1695
01:28:55,330 --> 01:28:58,890
Speaker 6:  update. So you will no longer just have to email

1696
01:28:58,890 --> 01:29:01,950
Speaker 6:  files to yourself when you wanna put them on the scribe.

1697
01:29:01,950 --> 01:29:03,340
Speaker 4:  What can you do instead?

1698
01:29:03,340 --> 01:29:07,000
Speaker 6:  You can use an app. Cool. I thought I, I know

1699
01:29:07,000 --> 01:29:10,480
Speaker 6:  everybody was really concerned about this. I got a lot of emails. So that's,

1700
01:29:10,480 --> 01:29:11,200
Speaker 6:  that's for you guys.

1701
01:29:11,200 --> 01:29:14,720
Speaker 4:  All a little scoop on the Vercast for the Kindle scribe community. All four

1702
01:29:14,720 --> 01:29:18,120
Speaker 4:  of you. Excited for the scribe blog. Yeah, I, you know, back in the day,

1703
01:29:18,120 --> 01:29:20,840
Speaker 4:  like there would've been a dedicated gadget blog for the Kendall scribe.

1704
01:29:20,840 --> 01:29:22,880
Speaker 6:  Yes, a hundred percent. All the things you

1705
01:29:22,880 --> 01:29:26,400
Speaker 4:  Can do, bring it back. All right. That's it. Thanks to James Vincent for

1706
01:29:26,400 --> 01:29:30,120
Speaker 4:  joining us. Thanks to Richard for joining us with David out on baby leave.

1707
01:29:30,120 --> 01:29:33,480
Speaker 4:  I wonder if he had to sell his drone to have a baby. That's really not how

1708
01:29:33,480 --> 01:29:35,880
Speaker 4:  that connection worked, but that's how I'm gonna start thinking about it.

1709
01:29:35,880 --> 01:29:38,880
Speaker 4:  You're gonna be hearing a lot from Richard. It's gonna be a lot of fun. That's

1710
01:29:38,880 --> 01:29:42,760
Speaker 4:  it. That's by the way, a bunch of cool stuff on the

1711
01:29:42,760 --> 01:29:46,440
Speaker 4:  site this week. Liz has a big piece about ftx.

1712
01:29:46,440 --> 01:29:50,330
Speaker 4:  Not what you would expect though. The FTX store is broken

1713
01:29:50,330 --> 01:29:54,270
Speaker 4:  by a really actually great crypto news outlet called CoinDesk. Yep.

1714
01:29:54,270 --> 01:29:58,160
Speaker 4:  CoinDesk is owned by a crypto exchange. Oof. So they might have

1715
01:29:58,160 --> 01:30:01,880
Speaker 4:  just like knifed themselves, great story about the politics of tech and

1716
01:30:01,880 --> 01:30:03,920
Speaker 4:  journalism and crypto. I mean it's like a perfect person.

1717
01:30:03,920 --> 01:30:05,100
Speaker 6:  It's wonderful.

1718
01:30:05,100 --> 01:30:06,720
Speaker 4:  But for the FTX story, if

1719
01:30:06,720 --> 01:30:10,120
Speaker 10:  You believe what the government might be accusing SPF of shooting yourself

1720
01:30:10,120 --> 01:30:12,360
Speaker 10:  in the foot is kind of on on brand.

1721
01:30:12,360 --> 01:30:15,920
Speaker 4:  We'll see what happens. It's very we'll. We'll see how that goes. It's very

1722
01:30:15,920 --> 01:30:18,960
Speaker 4:  good. There's actually great week on the site. Go check it all out. We'll

1723
01:30:18,960 --> 01:30:22,920
Speaker 4:  back on Wednesday. We did, Alex and I talked to Charles Plan more

1724
01:30:22,920 --> 01:30:26,810
Speaker 4:  about winners and Los and Streamings. We basically did the Go 90 scale. Yeah,

1725
01:30:26,810 --> 01:30:30,760
Speaker 4:  we thought we were gonna go 30 minutes. We went 90, which

1726
01:30:30,760 --> 01:30:31,760
Speaker 4:  is perfect. Makes sense. It's

1727
01:30:31,760 --> 01:30:31,960
Speaker 6:  Beautiful.

1728
01:30:31,960 --> 01:30:35,120
Speaker 4:  It was great. So we'll here you, we'll see you on Wednesday. You can tweet

1729
01:30:35,120 --> 01:30:38,690
Speaker 4:  at us. Alex is Alex h Cranz. Richard is at

1730
01:30:38,690 --> 01:30:42,560
Speaker 4:  RJ cc. James is at JJ Vincent, I'm at Reckless. We love

1731
01:30:42,560 --> 01:30:46,320
Speaker 4:  hearing from you for as long as Twitter's gonna last tweeted us.

1732
01:30:46,320 --> 01:30:48,120
Speaker 4:  We love it. That's it. That's Rich. Has back

1733
01:30:48,120 --> 01:30:52,740
Speaker 4:  home.

1734
01:30:52,740 --> 01:30:56,640
Speaker 1:  And that's a wrap for Vergecast this week. Thanks for listening. If you enjoy

1735
01:30:56,640 --> 01:31:00,200
Speaker 1:  the show, subscribe in the podcast app of your choice or tell a friend, you

1736
01:31:00,200 --> 01:31:04,120
Speaker 1:  can send us feedback at vergecast@theverge.com. This show is

1737
01:31:04,120 --> 01:31:07,720
Speaker 1:  produced by me, Liam James, and our senior audio director, Andrew Marino.

1738
01:31:07,720 --> 01:31:11,440
Speaker 1:  This episode was edited and mixed by Amanda Rose Smith. Our

1739
01:31:11,440 --> 01:31:15,280
Speaker 1:  editorial director is Brooke Min, and our executive producer is Eleanor

1740
01:31:15,280 --> 01:31:18,880
Speaker 1:  Donovan. The Vergecast is a production of the Verge and Box Media

1741
01:31:18,880 --> 01:31:22,160
Speaker 1:  Podcast network. And that's it. We'll see you next week.

