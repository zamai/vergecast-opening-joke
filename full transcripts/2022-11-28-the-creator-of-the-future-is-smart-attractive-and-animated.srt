1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 4b89dd00-6f26-11ed-9ee2-2538705d6798
Status: Done
Stage: Done
Title: The creator of the future is smart, attractive... and animated
Audio URL: https://jfe93e.s3.amazonaws.com/-7386924605804331894/-4754743803979134890/s93290-US-2001s-1669644535.mp3
Description: As we spend more time in digital spaces, our avatars are becoming part of our personality. And digital creators and influencers are becoming part of our culture. Producer Gina Pollack join's The Verge's David Pierce with stories about why advertisers love digital creators, why a dancing hot dog will never leave your brain, and what the creator industry is learning from mascots. Next time you’re scrolling through your Instagram feed, keep your eyes peeled — not everyone’s as human as they look.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (4 ads detected)

2
00:00:41,610 --> 00:00:45,300
Speaker 1:  Welcome to the Verge Cast, the flagship podcast of the Uncanny Valley.

3
00:00:45,300 --> 00:00:48,920
Speaker 1:  I'm your friend, David Pierce. And for the next three weeks in this feed,

4
00:00:48,920 --> 00:00:52,520
Speaker 1:  we're gonna be doing a mini series that we made all about the creator

5
00:00:52,520 --> 00:00:56,360
Speaker 1:  economy, emphasis on economy. We're gonna look into how all

6
00:00:56,360 --> 00:01:00,160
Speaker 1:  kinds of influencers, the brands they represent, and the people who manage

7
00:01:00,160 --> 00:01:04,040
Speaker 1:  them make and spend their money. Today we're starting with

8
00:01:04,040 --> 00:01:07,720
Speaker 1:  a story from producer Gina Pollock about a topic I know

9
00:01:07,720 --> 00:01:11,580
Speaker 1:  nothing about and am super excited to get into, which is virtual

10
00:01:11,580 --> 00:01:15,520
Speaker 1:  influencers. Maybe you've seen them out there, they look just like the other

11
00:01:15,520 --> 00:01:19,370
Speaker 1:  models and cool kids posing and promoting brands on Instagram or TikTok

12
00:01:19,370 --> 00:01:23,200
Speaker 1:  or wherever else, but there's something different about

13
00:01:23,200 --> 00:01:26,450
Speaker 1:  them. And then you look a little closer and you realize they're not human

14
00:01:26,450 --> 00:01:29,800
Speaker 1:  at all. They're actually these hyper realistic 3D

15
00:01:29,800 --> 00:01:33,580
Speaker 1:  illustrations designed to look like humans ish,

16
00:01:33,580 --> 00:01:37,380
Speaker 1:  but not exactly. Managers, artists and tech companies are making big,

17
00:01:37,380 --> 00:01:41,240
Speaker 1:  big money with these digital characters and they could become an even bigger

18
00:01:41,240 --> 00:01:45,120
Speaker 1:  deal as virtual reality is like the metaverse become more mainstream. And

19
00:01:45,120 --> 00:01:48,440
Speaker 1:  this phenomenon brings up all kinds of questions. Could digital

20
00:01:48,440 --> 00:01:52,080
Speaker 1:  influencers change the future of the creator economy or replace

21
00:01:52,080 --> 00:01:55,840
Speaker 1:  human influencers entirely? Or is this just a passing

22
00:01:55,840 --> 00:01:59,520
Speaker 1:  trend? So Gina, I don't even,

23
00:01:59,520 --> 00:02:02,640
Speaker 1:  let's, let's just get into this here. Welcome to the show.

24
00:02:02,640 --> 00:02:05,410
Speaker 3:  Thank you. I'm very excited to talk about this.

25
00:02:05,410 --> 00:02:09,400
Speaker 1:  So at the very beginning, this story was about humans and you came back and

26
00:02:09,400 --> 00:02:13,320
Speaker 1:  said, actually humans, that's old hat passe. Tell me how you found

27
00:02:13,320 --> 00:02:14,590
Speaker 1:  out about this phenomenon.

28
00:02:14,590 --> 00:02:18,200
Speaker 3:  Yeah, so a couple years ago I was doing another story

29
00:02:18,200 --> 00:02:21,640
Speaker 3:  about Instagram influencers and I talked to this agent named

30
00:02:21,887 --> 00:02:25,280
Speaker 3:  Jennifer Powell. She was showing me like her roster of

31
00:02:25,280 --> 00:02:28,720
Speaker 3:  clients and one of them looked a little different to

32
00:02:28,720 --> 00:02:32,660
Speaker 3:  me. And her name is Shoe Do. Okay.

33
00:02:32,660 --> 00:02:36,440
Speaker 3:  And I want you to open and look at her page

34
00:02:36,440 --> 00:02:38,990
Speaker 3:  because I feel like you have to see it.

35
00:02:38,990 --> 00:02:42,680
Speaker 1:  Okay. You sent me her Instagram link. She's 237,000

36
00:02:42,680 --> 00:02:44,660
Speaker 1:  followers. Good for her.

37
00:02:44,660 --> 00:02:48,400
Speaker 3:  And I would say to describe her, she's very tall, she has very

38
00:02:48,400 --> 00:02:51,910
Speaker 3:  short hair, she has really dark, perfect

39
00:02:51,910 --> 00:02:55,880
Speaker 3:  skin emphasis on Perfect. And I think the celebrity

40
00:02:55,880 --> 00:02:58,360
Speaker 3:  that she reminds me of most is Looped and Jango.

41
00:02:58,360 --> 00:03:01,200
Speaker 1:  That's exactly who I was about to say a hundred percent.

42
00:03:01,200 --> 00:03:05,120
Speaker 3:  She's like Lupita Jango, but somehow even more

43
00:03:05,120 --> 00:03:08,320
Speaker 3:  symmetrical, which is shocking because Lupita is perfect

44
00:03:08,320 --> 00:03:09,360
Speaker 3:  looking.

45
00:03:09,360 --> 00:03:10,050
Speaker 1:  Yeah,

46
00:03:10,050 --> 00:03:10,720
Speaker 3:  Already

47
00:03:10,720 --> 00:03:12,190
Speaker 1:  That's a high water. Yeah.

48
00:03:12,190 --> 00:03:15,920
Speaker 3:  Yeah. So when I was first working on that story, I was

49
00:03:15,920 --> 00:03:19,230
Speaker 3:  surprised that she wasn't a real person and also

50
00:03:19,230 --> 00:03:23,000
Speaker 3:  confused like who is making her. And I ended up

51
00:03:23,000 --> 00:03:26,560
Speaker 3:  talking to Cameron James Wilson, who is her

52
00:03:26,560 --> 00:03:30,480
Speaker 3:  creator, and he was a fashion photographer for

53
00:03:30,480 --> 00:03:33,720
Speaker 3:  a long time. He was going through a difficult time in his personal life and

54
00:03:33,720 --> 00:03:37,160
Speaker 3:  was kind of stuck at home. This was way before the pandemic. And he started

55
00:03:37,160 --> 00:03:40,760
Speaker 3:  playing around with virtual imaging and creating

56
00:03:40,760 --> 00:03:44,640
Speaker 3:  these 3D models. And he said he just

57
00:03:44,640 --> 00:03:48,530
Speaker 3:  wanted to create the perfect woman. And that's how

58
00:03:48,530 --> 00:03:50,630
Speaker 3:  he landed on sh Do

59
00:03:50,630 --> 00:03:54,600
Speaker 4:  I think sh Do Represents what I see as Beautiful. And

60
00:03:54,600 --> 00:03:58,360
Speaker 4:  eventually she kind of became almost her own personality, her own

61
00:03:58,360 --> 00:04:02,240
Speaker 4:  individual self and became the world's first

62
00:04:02,240 --> 00:04:03,800
Speaker 4:  digital supermodel, which

63
00:04:03,800 --> 00:04:07,690
Speaker 1:  Yikes, like interesting and yikes simultaneously.

64
00:04:07,690 --> 00:04:09,750
Speaker 1:  Is that, is that a fair response to that?

65
00:04:09,750 --> 00:04:13,480
Speaker 3:  Yeah, it's weird because it was this white man who

66
00:04:13,480 --> 00:04:17,360
Speaker 3:  wanted to create this perfect, beautiful

67
00:04:17,360 --> 00:04:20,600
Speaker 3:  black woman. So that created some

68
00:04:20,600 --> 00:04:22,460
Speaker 3:  controversy. Of course,

69
00:04:22,460 --> 00:04:26,160
Speaker 1:  All of that aside, I'm kind of like, if you had just shown me

70
00:04:26,160 --> 00:04:30,040
Speaker 1:  this Instagram and not given me any other information ahead of

71
00:04:30,040 --> 00:04:33,360
Speaker 1:  it, I would've a hundred percent just believed this was a real person and

72
00:04:33,360 --> 00:04:36,800
Speaker 1:  moved on. And even I'm like poking through the comments on her page now and

73
00:04:36,800 --> 00:04:40,720
Speaker 1:  people are responding like she's a real person. This is kind of

74
00:04:40,720 --> 00:04:41,850
Speaker 1:  blowing my mind.

75
00:04:41,850 --> 00:04:45,840
Speaker 3:  So when he actually first created her, it didn't say in the profile

76
00:04:45,840 --> 00:04:49,400
Speaker 3:  that she wasn't a real person. And he made

77
00:04:49,400 --> 00:04:53,160
Speaker 3:  this image where there was this orange background and she's wearing

78
00:04:53,160 --> 00:04:57,040
Speaker 3:  this orange lipstick and he matched the shade of the lipstick to a

79
00:04:57,040 --> 00:05:00,590
Speaker 3:  Fenty Beauty lipstick. And Fenty

80
00:05:00,590 --> 00:05:04,440
Speaker 3:  repost it on their Instagram and it went super viral. People were like,

81
00:05:04,440 --> 00:05:07,520
Speaker 3:  who is this model? Oh wow, this is amazing. She's

82
00:05:07,520 --> 00:05:11,070
Speaker 3:  beautiful. And then people started commenting, wait a second,

83
00:05:11,070 --> 00:05:14,380
Speaker 3:  this is not a real per, is this a real person? This is not a real person.

84
00:05:14,380 --> 00:05:17,770
Speaker 3:  And then Fenty Beauty actually took it down

85
00:05:17,770 --> 00:05:20,690
Speaker 3:  because they didn't know it wasn't a real person.

86
00:05:20,690 --> 00:05:24,240
Speaker 1:  Oh wow. Clearly real or not, this thing is

87
00:05:24,240 --> 00:05:27,960
Speaker 1:  working right. She has tons of followers, lots of comments,

88
00:05:27,960 --> 00:05:31,240
Speaker 1:  there's a lot of engagement on it. Everybody like treats her like they would

89
00:05:31,240 --> 00:05:35,160
Speaker 1:  be fans of any other supermodel, like real or digital. Sheu seems to be working

90
00:05:35,160 --> 00:05:37,030
Speaker 1:  on Instagram, which is kind of nuts.

91
00:05:37,030 --> 00:05:40,760
Speaker 3:  Yeah. And Sheu has worked with huge brands and fashion

92
00:05:40,760 --> 00:05:41,460
Speaker 3:  magazines.

93
00:05:41,460 --> 00:05:45,300
Speaker 4:  She was, was actually paid to model

94
00:05:45,300 --> 00:05:49,260
Speaker 4:  for Vogue Australia with Al Harper's Vogue,

95
00:05:49,260 --> 00:05:50,960
Speaker 4:  cosmopolitan.

96
00:05:50,960 --> 00:05:54,320
Speaker 3:  She's worked with all these major luxury brands,

97
00:05:54,320 --> 00:05:58,040
Speaker 4:  Luton, Bama, Lexus, Tiffany,

98
00:05:58,040 --> 00:06:01,440
Speaker 4:  Todds, Faragamo, the list goes

99
00:06:01,440 --> 00:06:02,410
Speaker 4:  on.

100
00:06:02,410 --> 00:06:06,180
Speaker 3:  So if she was a real model, she would be considered very successful.

101
00:06:06,180 --> 00:06:09,640
Speaker 1:  Wow. So how much is this a totally new

102
00:06:09,640 --> 00:06:10,590
Speaker 1:  thing?

103
00:06:10,590 --> 00:06:14,320
Speaker 3:  I talked to actually a guy named Christopher Tra, and he runs a

104
00:06:14,320 --> 00:06:18,160
Speaker 3:  website called Virtual Humans, which is kind of like an encyclopedia of digital

105
00:06:18,160 --> 00:06:21,870
Speaker 3:  influencers. And he was talking about how he was actually

106
00:06:21,870 --> 00:06:25,790
Speaker 3:  inspired by the early internet, like the late nineties

107
00:06:25,790 --> 00:06:29,440
Speaker 5:  When I was a kid. There was no social network that was focused on

108
00:06:29,440 --> 00:06:33,240
Speaker 5:  being yourself. It was all using handles and pseudonyms. So for a good 10

109
00:06:33,240 --> 00:06:37,090
Speaker 5:  years I was playing video games, browsing forums, and engaging in chat

110
00:06:37,090 --> 00:06:38,770
Speaker 5:  through pseudonyms.

111
00:06:38,770 --> 00:06:42,630
Speaker 3:  He was just really into this idea of inhabiting another

112
00:06:42,630 --> 00:06:46,520
Speaker 3:  body. So you're pulling the strings, but the body isn't yours and the

113
00:06:46,520 --> 00:06:48,930
Speaker 3:  personality isn't necessarily the same as your personality.

114
00:06:48,930 --> 00:06:49,940
Speaker 1:  Totally.

115
00:06:49,940 --> 00:06:53,640
Speaker 3:  But the next wave of social media was really more

116
00:06:53,640 --> 00:06:57,180
Speaker 3:  encouraging us to be ourselves. Like Facebook and Instagram,

117
00:06:57,180 --> 00:07:01,080
Speaker 3:  I'm creating an image of myself and then I'm supposed to, you know, like

118
00:07:01,080 --> 00:07:04,760
Speaker 3:  update my friends on what I'm really doing. And Christopher says, this whole

119
00:07:04,760 --> 00:07:08,720
Speaker 3:  trend of digital models is kind of like a rewind back to the beginning

120
00:07:08,720 --> 00:07:11,460
Speaker 3:  of the internet where we could all be different people.

121
00:07:11,460 --> 00:07:15,250
Speaker 5:  Now the internet is coming full circle to this idea because

122
00:07:15,250 --> 00:07:19,010
Speaker 5:  avatars are taking form and they're competing with humans now

123
00:07:19,010 --> 00:07:22,680
Speaker 5:  in what was supposedly this human media feed,

124
00:07:22,680 --> 00:07:26,560
Speaker 5:  right? People are now leveraging the same technology that is used

125
00:07:26,560 --> 00:07:30,400
Speaker 5:  to create video games and characters in video games to now create characters

126
00:07:30,400 --> 00:07:34,040
Speaker 5:  outside of video games for all sorts of purposes. That could be

127
00:07:34,040 --> 00:07:38,000
Speaker 5:  modeling, that could be influencing, that could be live streaming. So

128
00:07:38,000 --> 00:07:41,760
Speaker 5:  the form could be a digital model, but if you take this digital

129
00:07:41,760 --> 00:07:45,720
Speaker 5:  model and you post it redundantly on social media, that starts to look like

130
00:07:45,720 --> 00:07:47,030
Speaker 5:  a virtual influencer.

131
00:07:47,030 --> 00:07:50,800
Speaker 1:  That transition actually makes complete sense to me in that sense. Now you're

132
00:07:50,800 --> 00:07:53,880
Speaker 1:  making me wonder like, do I go on Instagram every day and half the things

133
00:07:53,880 --> 00:07:57,640
Speaker 1:  I see are digital characters that I don't even realize are digital

134
00:07:57,640 --> 00:08:00,550
Speaker 1:  characters? Like how big is this space right now?

135
00:08:00,550 --> 00:08:04,270
Speaker 3:  I wouldn't say it's at that level yet. There are definitely

136
00:08:04,270 --> 00:08:08,200
Speaker 3:  a lot of these, but you kind of have to look for them. And I didn't happen

137
00:08:08,200 --> 00:08:12,000
Speaker 3:  to be following any of them. I just, once you kind of look

138
00:08:12,000 --> 00:08:15,250
Speaker 3:  into it, then you find more and more and more and they're all over the world.

139
00:08:15,250 --> 00:08:19,120
Speaker 3:  So the first one that I saw was a character called Lil

140
00:08:19,120 --> 00:08:23,040
Speaker 3:  Makayla. And her creators basically looked

141
00:08:23,040 --> 00:08:26,960
Speaker 3:  at what influencers were doing. They took Lil Makayla and mimicked

142
00:08:26,960 --> 00:08:30,760
Speaker 3:  that entire lifestyle and that entire ethos of being an

143
00:08:30,760 --> 00:08:34,560
Speaker 3:  influencer. Oh wow. And that made her go super, super viral. Like,

144
00:08:34,560 --> 00:08:37,760
Speaker 3:  oh, they're hanging out with their friends, they're shopping, they're at

145
00:08:37,760 --> 00:08:39,880
Speaker 3:  the pink wall taking a picture. Yeah.

146
00:08:39,880 --> 00:08:42,600
Speaker 1:  Whoa. She has almost 3 million followers on Instagram.

147
00:08:42,600 --> 00:08:46,590
Speaker 3:  She's kind of like the most Gen Z optimized

148
00:08:46,590 --> 00:08:50,340
Speaker 3:  cool girl that could possibly exist. Like

149
00:08:50,340 --> 00:08:54,120
Speaker 3:  she is ethnically ambiguous. Apparently

150
00:08:54,120 --> 00:08:57,960
Speaker 3:  she's 19, 19 years old and she has these two buns that she wears

151
00:08:57,960 --> 00:09:01,200
Speaker 3:  on top of her head. And you're kind of like, Ooh, I wanna hang out with her.

152
00:09:01,200 --> 00:09:01,920
Speaker 3:  I wanna be her friend.

153
00:09:01,920 --> 00:09:05,560
Speaker 1:  And Mikayla's been around for a while, right? Yeah. Like I'm impressed

154
00:09:05,560 --> 00:09:09,480
Speaker 1:  how she remains as popular as she has been for

155
00:09:09,480 --> 00:09:12,720
Speaker 1:  what it looks like a long time. I mean, she's posted 1,250 times on Instagram.

156
00:09:12,720 --> 00:09:15,180
Speaker 1:  Yeah. Which is not a small amount of work.

157
00:09:15,180 --> 00:09:19,090
Speaker 3:  She was created in 2016 by an LA company called

158
00:09:19,090 --> 00:09:23,000
Speaker 3:  br. It's B R U D. It might be brewed, we'll never know because they

159
00:09:23,000 --> 00:09:26,090
Speaker 3:  don't respond to emails from journalists. So

160
00:09:26,090 --> 00:09:29,840
Speaker 3:  unclear. But I think they kind of created this persona

161
00:09:29,840 --> 00:09:33,600
Speaker 3:  for her where she talks about being a 19 year old robot living in

162
00:09:33,600 --> 00:09:34,250
Speaker 3:  la

163
00:09:34,250 --> 00:09:38,240
Speaker 6:  If you're late to the party, Hey, I'm McKayla, I'm a 19 year

164
00:09:38,240 --> 00:09:41,150
Speaker 6:  old robot living in LA making music and

165
00:09:41,150 --> 00:09:45,110
Speaker 6:  well, I guess just keep watching and catch up.

166
00:09:45,110 --> 00:09:47,760
Speaker 1:  It's gotta be tough. It's hard out there for a 19 year old robot.

167
00:09:47,760 --> 00:09:51,720
Speaker 3:  It's hard out there. But she's actually not a robot because she's not

168
00:09:51,720 --> 00:09:55,660
Speaker 3:  artificially intelligent. She is just voiced by an actor.

169
00:09:55,660 --> 00:09:59,320
Speaker 3:  She has a YouTube page where she does the kind of confessional

170
00:09:59,320 --> 00:10:01,030
Speaker 3:  videos that you'd see on the YouTube page.

171
00:10:01,030 --> 00:10:04,480
Speaker 6:  Wait, I'm dead at the fact that my mind just made me the person being broken

172
00:10:04,480 --> 00:10:08,200
Speaker 6:  up with. Does that mean I'm the toxic front? No. To self discuss the next

173
00:10:08,200 --> 00:10:09,660
Speaker 6:  Tuesday's therapy session.

174
00:10:09,660 --> 00:10:11,400
Speaker 3:  And she even has a music

175
00:10:11,400 --> 00:10:19,170
Speaker 3:  video.

176
00:10:19,170 --> 00:10:20,120
Speaker 1:  Oh my gosh.

177
00:10:20,120 --> 00:10:24,040
Speaker 3:  She's a very complete being as far as a not

178
00:10:24,040 --> 00:10:24,750
Speaker 3:  real person.

179
00:10:24,750 --> 00:10:27,940
Speaker 1:  Yeah. She's a real like multihyphenate over here.

180
00:10:27,940 --> 00:10:31,880
Speaker 3:  She became really famous because the media loved her because she was

181
00:10:31,880 --> 00:10:35,690
Speaker 3:  the first digital in influencer. She was the first one

182
00:10:35,690 --> 00:10:39,480
Speaker 3:  to be doing exactly what human beings were doing. And she followed that playbook.

183
00:10:39,480 --> 00:10:43,070
Speaker 3:  Exactly. And by she, I mean her creators, but again, it's like

184
00:10:43,070 --> 00:10:46,880
Speaker 3:  hard to not personify them. Right? Because it's easier for me to

185
00:10:46,880 --> 00:10:49,310
Speaker 3:  say she than to say it.

186
00:10:49,310 --> 00:10:52,880
Speaker 1:  Yeah. I'm struggling with this too, cuz it's like McKayla is obviously not

187
00:10:52,880 --> 00:10:56,720
Speaker 1:  a person and so saying she feels wrong, but also I'm looking at her Instagram

188
00:10:56,720 --> 00:11:00,410
Speaker 1:  and it is the Instagram of a 19 year old girl

189
00:11:00,410 --> 00:11:01,350
Speaker 1:  in la.

190
00:11:01,350 --> 00:11:05,240
Speaker 6:  Okay, we're done here. Send me your questions for next time. And what? That's

191
00:11:05,240 --> 00:11:08,390
Speaker 6:  right. Don't forget to recharge your hearts.

192
00:11:08,390 --> 00:11:12,220
Speaker 1:  Okay. So let, let's get back to the economics of all of this because

193
00:11:12,220 --> 00:11:15,800
Speaker 1:  I'm assuming like shoot, you and Lo Makayla don't

194
00:11:15,800 --> 00:11:19,760
Speaker 1:  have bank accounts and there's no them to pay for their

195
00:11:19,760 --> 00:11:23,230
Speaker 1:  services. So how, how do these virtual

196
00:11:23,230 --> 00:11:25,960
Speaker 1:  humans actually make money, right?

197
00:11:25,960 --> 00:11:29,600
Speaker 3:  For example, Jennifer represents shoe do, but

198
00:11:29,600 --> 00:11:33,200
Speaker 3:  really she's representing Cameron, so that's a little different from another

199
00:11:33,200 --> 00:11:36,680
Speaker 3:  influencer that she represents. Name sincerely Jules. Sincerely Jules is

200
00:11:36,680 --> 00:11:40,520
Speaker 3:  her client, but here you have her client creating another client and

201
00:11:40,520 --> 00:11:44,400
Speaker 3:  the first one is not appearing as themselves. Aside from that,

202
00:11:44,400 --> 00:11:47,350
Speaker 3:  it's very similar as Christopher explained.

203
00:11:47,350 --> 00:11:50,840
Speaker 5:  When you think about virtual influencers, how does it work that they can

204
00:11:50,840 --> 00:11:54,120
Speaker 5:  also do brand deals? Well the Playbook, playbook was written

205
00:11:54,120 --> 00:11:58,060
Speaker 5:  exhaustively by human influencers and by brands who are willing

206
00:11:58,060 --> 00:12:01,740
Speaker 5:  to pay. Honestly, human influencers to a lot of brands are just

207
00:12:01,740 --> 00:12:05,260
Speaker 5:  mannequins. And so I've met brands who say they really care about working

208
00:12:05,260 --> 00:12:08,420
Speaker 5:  with, you know, the right influencers and all of this. What they really mean

209
00:12:08,420 --> 00:12:11,900
Speaker 5:  is, I care about reaching your fandom in the right way and I will pay you

210
00:12:11,900 --> 00:12:15,260
Speaker 5:  to do it for me. So when you think about how a human

211
00:12:15,260 --> 00:12:18,920
Speaker 5:  influencer set the playbook and built the pipeline

212
00:12:18,920 --> 00:12:22,500
Speaker 5:  for brand dollars to flow through to them. Now when you

213
00:12:22,500 --> 00:12:26,400
Speaker 5:  simulate a human influencer, the infrastructure is already in place

214
00:12:26,400 --> 00:12:29,580
Speaker 5:  and brands can now flow their dollars in through virtual

215
00:12:29,580 --> 00:12:32,690
Speaker 5:  influencers and achieve the same end.

216
00:12:32,690 --> 00:12:36,460
Speaker 3:  Brands know how to work with them, they pay them a certain

217
00:12:36,460 --> 00:12:40,260
Speaker 3:  amount of money for a post. They have marketing tools that tell

218
00:12:40,260 --> 00:12:44,020
Speaker 3:  them what the return on investment was. Jennifer says the way it's generally

219
00:12:44,020 --> 00:12:44,870
Speaker 3:  calculated

220
00:12:44,870 --> 00:12:48,500
Speaker 8:  Is there's no great way to quantify, but if I was

221
00:12:48,500 --> 00:12:51,460
Speaker 8:  going to, it'd be like a thousand for every hundred thousand

222
00:12:51,460 --> 00:12:55,300
Speaker 8:  followers perhaps. I mean that's kind of a rule of

223
00:12:55,300 --> 00:12:59,100
Speaker 8:  thumb ish. But then people can ask for whatever they

224
00:12:59,100 --> 00:12:59,560
Speaker 8:  want

225
00:12:59,560 --> 00:13:03,140
Speaker 3:  For sincerely, Jules who I was talking about, she has 7 million followers.

226
00:13:03,140 --> 00:13:06,960
Speaker 3:  So the minimum that a brand would pay for a post from her would be $70,000.

227
00:13:06,960 --> 00:13:10,500
Speaker 3:  But it varies because they might be like, oh, we want five posts, or we want

228
00:13:10,500 --> 00:13:14,260
Speaker 3:  one post and an Instagram story, or we want a tweet and we want you to appear

229
00:13:14,260 --> 00:13:17,890
Speaker 3:  at the store. But that's kind of the baseline that Jennifer told me.

230
00:13:17,890 --> 00:13:21,860
Speaker 8:  I really did put the same sort of like price tags on shoe

231
00:13:21,860 --> 00:13:25,740
Speaker 8:  do as a human talent. But the

232
00:13:25,740 --> 00:13:29,620
Speaker 8:  difference is, is the production costs. And so I have to

233
00:13:29,620 --> 00:13:33,020
Speaker 8:  factor that in and that's almost like how I figured out a day rate. And then

234
00:13:33,020 --> 00:13:36,700
Speaker 8:  I figure out a usage. And it's the same

235
00:13:36,700 --> 00:13:40,620
Speaker 8:  like a human model. When you thought of them shooting a fragrance, then

236
00:13:40,620 --> 00:13:44,580
Speaker 8:  you have to talk about exclusivity and you have to talk about a term

237
00:13:44,580 --> 00:13:48,100
Speaker 8:  that they're going to be associated with the brand and such. And we do the

238
00:13:48,100 --> 00:13:51,700
Speaker 8:  same thing with Sudu. We don't wanna create conflicts or anything for Sudu

239
00:13:51,700 --> 00:13:55,360
Speaker 8:  because she is intellectual property. She is,

240
00:13:55,360 --> 00:13:58,960
Speaker 8:  her image is very much like an actual human.

241
00:13:58,960 --> 00:14:02,740
Speaker 1:  But you were saying Cameron has like a whole, this is like an

242
00:14:02,740 --> 00:14:03,460
Speaker 1:  agency thing

243
00:14:03,460 --> 00:14:07,140
Speaker 3:  Now, right? Like so just like bred the company that made Lil McKayla

244
00:14:07,140 --> 00:14:11,020
Speaker 3:  created Bermuda to be her like Arch Ral friend. That's

245
00:14:11,020 --> 00:14:14,670
Speaker 3:  kind of also what Cameron did. And

246
00:14:14,670 --> 00:14:18,580
Speaker 3:  we just have to listen to him explain it for himself because

247
00:14:18,580 --> 00:14:19,580
Speaker 3:  he's the best at doing

248
00:14:19,580 --> 00:14:23,340
Speaker 4:  It. Danny is a Caucasian model with red

249
00:14:23,340 --> 00:14:26,980
Speaker 4:  hair and freckles. Kofi is a

250
00:14:26,980 --> 00:14:30,740
Speaker 4:  gorgeous tool, muscular black model. He's

251
00:14:30,740 --> 00:14:34,580
Speaker 4:  very athletic into body building. I imagined

252
00:14:34,580 --> 00:14:38,560
Speaker 4:  when I created him that him and Chu would be kind of like

253
00:14:38,560 --> 00:14:42,340
Speaker 4:  the Adam and Eve of the digital world, the new Barbie in

254
00:14:42,340 --> 00:14:45,490
Speaker 4:  can, an alien model called

255
00:14:45,490 --> 00:14:49,300
Speaker 4:  Gaia. And she's really tall with blue Skin

256
00:14:49,300 --> 00:14:52,880
Speaker 4:  Voice is non-binary and is a virtual

257
00:14:52,880 --> 00:14:56,120
Speaker 4:  drag queen. Another character that we created is

258
00:14:56,120 --> 00:14:59,200
Speaker 4:  Bran. She's a a mixed race plus size

259
00:14:59,200 --> 00:15:03,000
Speaker 4:  model with stretch marks. And then Jay

260
00:15:03,000 --> 00:15:06,800
Speaker 4:  Young, who is a Korean male model. And we are

261
00:15:06,800 --> 00:15:10,320
Speaker 4:  currently developing him even further.

262
00:15:10,320 --> 00:15:14,240
Speaker 1:  There's just a lot going on there that I need to spend a lot

263
00:15:14,240 --> 00:15:17,880
Speaker 1:  of time sorting through. But he really sees them as people

264
00:15:17,880 --> 00:15:21,560
Speaker 1:  with personalities and imperfections and you know, he talks about

265
00:15:21,560 --> 00:15:25,280
Speaker 1:  stretch marks and the different features that different versions of them

266
00:15:25,280 --> 00:15:29,040
Speaker 1:  have. It's like, it's really interesting that the goal here is to

267
00:15:29,040 --> 00:15:32,680
Speaker 1:  go sort of extremely human rather than

268
00:15:32,680 --> 00:15:36,560
Speaker 1:  like kind of science fiction futuristic robot, which I

269
00:15:36,560 --> 00:15:39,180
Speaker 1:  would think would be one way a lot of people would go with this.

270
00:15:39,180 --> 00:15:43,160
Speaker 3:  The trend now in marketing for female clothes

271
00:15:43,160 --> 00:15:47,120
Speaker 3:  and female products has gone from, okay, everybody has to be really

272
00:15:47,120 --> 00:15:50,810
Speaker 3:  skinny and look like a six foot tall, 100 pound woman.

273
00:15:50,810 --> 00:15:54,400
Speaker 3:  To now, like, we wanna see real women brands like Dove or like

274
00:15:54,400 --> 00:15:58,360
Speaker 3:  Airy for example, have different sized models and models again

275
00:15:58,360 --> 00:16:02,280
Speaker 3:  with stretch marks and they're not Photoshopping. And so he can then create

276
00:16:02,280 --> 00:16:05,200
Speaker 3:  a character, this character brand that he's talking about, who, who's perfect

277
00:16:05,200 --> 00:16:08,710
Speaker 3:  for those brands. And then he can create a non-binary virtual

278
00:16:08,710 --> 00:16:12,680
Speaker 3:  drag queen. So it's like whatever the trend is in

279
00:16:12,680 --> 00:16:16,260
Speaker 3:  the larger culture, he can kind of roll with that

280
00:16:16,260 --> 00:16:20,080
Speaker 3:  and create the perfect person in air

281
00:16:20,080 --> 00:16:22,140
Speaker 3:  quotes to represent that trend.

282
00:16:22,140 --> 00:16:25,960
Speaker 1:  And I guess it's true that if the audience is gonna respond to

283
00:16:25,960 --> 00:16:29,390
Speaker 1:  these characters in roughly the same way that they would respond to

284
00:16:29,390 --> 00:16:33,360
Speaker 1:  actual human influencers, you've basically like replaced central

285
00:16:33,360 --> 00:16:36,920
Speaker 1:  casting with a bunch of animators in a way that like, it's a little bit

286
00:16:36,920 --> 00:16:40,440
Speaker 1:  cynical in a way. Yeah, but it's, it, it almost makes total sense to

287
00:16:40,440 --> 00:16:43,640
Speaker 3:  Me. It's like you're cutting out the middle man and the middle man is the

288
00:16:43,640 --> 00:16:44,360
Speaker 3:  human. The

289
00:16:44,360 --> 00:16:45,480
Speaker 1:  Literal man. Yeah,

290
00:16:45,480 --> 00:16:49,440
Speaker 3:  The literal man in the middle. You're like, we, we just don't need

291
00:16:49,440 --> 00:16:50,030
Speaker 3:  you.

292
00:16:50,030 --> 00:16:53,600
Speaker 1:  It's so much safer in so many

293
00:16:53,600 --> 00:16:57,360
Speaker 1:  ways. Like your, your virtual influencer is never gonna have a rebellious

294
00:16:57,360 --> 00:16:58,870
Speaker 1:  phase. That's not like a thing,

295
00:16:58,870 --> 00:17:02,640
Speaker 3:  A real model. You have to fly her from Milan to New York.

296
00:17:02,830 --> 00:17:06,720
Speaker 3:  I mean, that's time consuming. You have to coordinate all of that and

297
00:17:06,720 --> 00:17:10,480
Speaker 3:  pay for those expenses. And you can't have somebody doing two photo

298
00:17:10,480 --> 00:17:14,080
Speaker 3:  shoots at once. But obviously with these digital models, you

299
00:17:14,080 --> 00:17:14,790
Speaker 3:  can,

300
00:17:14,790 --> 00:17:18,400
Speaker 5:  Brands can now work with this character over a lifetime because that

301
00:17:18,400 --> 00:17:21,480
Speaker 5:  character's never going to age out. It's never going to die for all these

302
00:17:21,480 --> 00:17:25,440
Speaker 5:  various benefits from a technological standpoint, all the way down to the

303
00:17:25,440 --> 00:17:28,990
Speaker 5:  fact that you can better control what a virtual influencer says

304
00:17:28,990 --> 00:17:30,630
Speaker 5:  over a human influencer.

305
00:17:30,630 --> 00:17:34,480
Speaker 1:  Your model will never die. Is is honestly, it's a really good sales

306
00:17:34,480 --> 00:17:38,440
Speaker 1:  page. All right, we're gonna take a quick break and we

307
00:17:38,440 --> 00:17:40,240
Speaker 1:  will be back with more on digital

308
00:17:40,240 --> 00:17:46,200
Speaker 1:  influencers.

309
00:19:31,350 --> 00:19:35,320
Speaker 1:  All right, we're back. Now you've got me wondering how this actually

310
00:19:35,320 --> 00:19:39,090
Speaker 1:  goes down. Like I'm, I'm thinking about if I wanna take a picture of Charlie

311
00:19:39,090 --> 00:19:42,530
Speaker 1:  DME with a Dunkin Donuts, that's pretty

312
00:19:42,530 --> 00:19:46,330
Speaker 1:  straightforward. How do you do one of these virtual

313
00:19:46,330 --> 00:19:47,520
Speaker 1:  photo shoots?

314
00:19:47,520 --> 00:19:51,210
Speaker 3:  It's definitely a production. Cameron told me all about this and it's

315
00:19:51,210 --> 00:19:54,610
Speaker 3:  like when you think about a regular photo

316
00:19:54,610 --> 00:19:55,230
Speaker 3:  shoot,

317
00:19:55,230 --> 00:19:58,530
Speaker 4:  You would probably have hair stylist, a makeup

318
00:19:58,530 --> 00:20:02,280
Speaker 4:  artist, a clothing stylist, a photographer,

319
00:20:02,280 --> 00:20:06,250
Speaker 4:  a photography assistant. You'd have the model, you'd have to hire

320
00:20:06,250 --> 00:20:07,760
Speaker 4:  the studio.

321
00:20:07,760 --> 00:20:11,210
Speaker 3:  It's a physical shoot that's happening, especially when you're talking about

322
00:20:11,210 --> 00:20:14,930
Speaker 3:  modeling doing that digitally. It's this whole other

323
00:20:14,930 --> 00:20:18,370
Speaker 3:  process and it's kind of hard to conceptualize because it's not an actual

324
00:20:18,370 --> 00:20:19,110
Speaker 3:  shoot.

325
00:20:19,110 --> 00:20:23,050
Speaker 4:  The problem with virtual shoots is time. You know, it might take two

326
00:20:23,050 --> 00:20:26,740
Speaker 4:  weeks or more to kind of work on a 3D editorial

327
00:20:26,740 --> 00:20:30,610
Speaker 4:  in the studio. Will, we might have to buy certain digital

328
00:20:30,610 --> 00:20:31,790
Speaker 4:  assets.

329
00:20:31,790 --> 00:20:35,530
Speaker 3:  The one where shoe do is wearing Louis Vuitton and

330
00:20:35,530 --> 00:20:39,410
Speaker 3:  standing in New York on Broadway. That background was a panel

331
00:20:39,410 --> 00:20:43,010
Speaker 3:  created by an illustrator that was given to

332
00:20:43,010 --> 00:20:46,770
Speaker 3:  Cameron, who then takes shoe do and puts her

333
00:20:46,770 --> 00:20:49,920
Speaker 3:  on that background. And he uses a real

334
00:20:49,920 --> 00:20:53,280
Speaker 3:  model to pose in the same pose that Shooter's

335
00:20:53,280 --> 00:20:54,130
Speaker 3:  doing.

336
00:20:54,130 --> 00:20:58,040
Speaker 4:  If you imagine an influencer receives a product and they can

337
00:20:58,040 --> 00:21:01,880
Speaker 4:  just pop that product on and take a photo with it. When it

338
00:21:01,880 --> 00:21:05,720
Speaker 4:  comes to us, you know, we have to get a model

339
00:21:05,720 --> 00:21:09,600
Speaker 4:  involved. We have to put that product on, we have to do a whole photo

340
00:21:09,600 --> 00:21:13,380
Speaker 4:  shoot, then we have to replace the model with shoot and,

341
00:21:13,380 --> 00:21:17,040
Speaker 4:  and kind of match her in in 3D to the real

342
00:21:17,040 --> 00:21:20,880
Speaker 4:  one. In some circumstances we have to recreate

343
00:21:20,880 --> 00:21:23,240
Speaker 4:  all of the clothes virtually. There

344
00:21:23,240 --> 00:21:26,960
Speaker 3:  Are these companies that create digital

345
00:21:26,960 --> 00:21:30,920
Speaker 3:  clothing based on photographs. So I thought maybe Louis Vuitton is

346
00:21:30,920 --> 00:21:34,840
Speaker 3:  gonna send their jacket to this company and they're gonna

347
00:21:34,840 --> 00:21:38,400
Speaker 3:  just digitize it. But it's actually just a photo of the jacket. You're taking

348
00:21:38,400 --> 00:21:42,320
Speaker 3:  the jacket, you're taking a picture of it, and then the company is

349
00:21:42,320 --> 00:21:45,840
Speaker 3:  recreating that jacket digitally and then it becomes a digital

350
00:21:45,840 --> 00:21:49,730
Speaker 3:  product that then can be put onto Shoe do

351
00:21:49,730 --> 00:21:53,320
Speaker 3:  by Cameron. Wow. In his digital software. And

352
00:21:53,320 --> 00:21:56,040
Speaker 3:  that was really surprising to me cause I was like, I can't believe there's

353
00:21:56,040 --> 00:21:59,840
Speaker 3:  not only one company that does that, but multiple companies do that.

354
00:21:59,840 --> 00:22:03,340
Speaker 3:  Cameron also works with a stylist named Tom.

355
00:22:03,340 --> 00:22:07,320
Speaker 4:  His specialty is a digital fashion. He's actually a

356
00:22:07,320 --> 00:22:11,230
Speaker 4:  virtual stylist. I've coined that kind of term

357
00:22:11,230 --> 00:22:14,880
Speaker 4:  a couple of years ago when he was working

358
00:22:14,880 --> 00:22:18,800
Speaker 4:  constantly on these kind of photo shoots, styling them in 3d that

359
00:22:18,800 --> 00:22:21,690
Speaker 4:  is very specific to working with digital fashion

360
00:22:21,690 --> 00:22:25,140
Speaker 3:  Instead of a regular stylist where he's working with the brands and finding

361
00:22:25,140 --> 00:22:28,420
Speaker 3:  the right clothes that match the right jewelry.

362
00:22:28,420 --> 00:22:32,320
Speaker 3:  Tom is getting those assets from the

363
00:22:32,320 --> 00:22:33,630
Speaker 3:  digital fashion companies.

364
00:22:33,630 --> 00:22:37,600
Speaker 1:  This is the dumbest thing to have just thought of. But there's the moment

365
00:22:37,600 --> 00:22:40,190
Speaker 1:  in Clueless where she's standing there looking at her closet.

366
00:22:40,190 --> 00:22:42,880
Speaker 10:  I mean, I get up, I brush my teeth and

367
00:22:42,880 --> 00:22:44,820
Speaker 3:  I pick out my skull clothes

368
00:22:44,820 --> 00:22:48,440
Speaker 1:  And she basically scrolls through all of the

369
00:22:48,440 --> 00:22:51,880
Speaker 1:  available different clothes, find something that's gonna match, and then

370
00:22:51,880 --> 00:22:55,280
Speaker 1:  basically hits a button and her closet spins until it finds the right stuff.

371
00:22:55,280 --> 00:22:58,120
Speaker 1:  She pulls it off and puts it on. And I feel like you're describing that exact

372
00:22:58,120 --> 00:23:01,900
Speaker 1:  process, but it all happens digitally. Yeah. For a virtual influencer.

373
00:23:01,900 --> 00:23:05,120
Speaker 3:  And then someone in the metaverse could buy that

374
00:23:05,120 --> 00:23:09,100
Speaker 3:  digital clothing item using cryptocurrency

375
00:23:09,100 --> 00:23:12,920
Speaker 3:  and put it on their avatar. Oh wow. Or they could just buy it

376
00:23:12,920 --> 00:23:14,910
Speaker 3:  to have it like an nft,

377
00:23:14,910 --> 00:23:18,760
Speaker 8:  A person could buy a digital dress that

378
00:23:18,760 --> 00:23:22,360
Speaker 8:  they can put on their avatar and they can also buy the

379
00:23:22,360 --> 00:23:26,240
Speaker 8:  same physical dress so they can put, you know,

380
00:23:26,240 --> 00:23:29,010
Speaker 8:  on their human self.

381
00:23:29,010 --> 00:23:30,480
Speaker 3:  It just keeps going, you

382
00:23:30,480 --> 00:23:33,800
Speaker 1:  Know? Yeah. Well and it kind of comes back to that same point, right? Where

383
00:23:33,800 --> 00:23:37,480
Speaker 1:  if we are in a time where we treat these digital

384
00:23:37,480 --> 00:23:41,200
Speaker 1:  things and these real world things the same. And if I'm being

385
00:23:41,200 --> 00:23:44,760
Speaker 1:  totally honest, I, I am like deeply suspicious of the idea that people are

386
00:23:44,760 --> 00:23:48,360
Speaker 1:  gonna gonna like care about their Metaverse Louis Vuitton jacket, the way

387
00:23:48,360 --> 00:23:52,080
Speaker 1:  that they care about their real world Louis Vuitton jacket. But it does seem

388
00:23:52,080 --> 00:23:55,960
Speaker 1:  like we're headed in some version of that direction. And if, if

389
00:23:55,960 --> 00:23:59,810
Speaker 1:  the metaverse is gonna be a thing, if buying, you know, clothes in Fortnite

390
00:23:59,810 --> 00:24:02,640
Speaker 1:  is gonna be a thing, like that's, that's where a lot of this is gonna head

391
00:24:02,640 --> 00:24:03,840
Speaker 1:  in some way, shape or form.

392
00:24:03,840 --> 00:24:07,760
Speaker 3:  Yeah. And Jennifer mentioned that too cuz she was like, my kids play Fortnite

393
00:24:07,760 --> 00:24:11,680
Speaker 3:  all the time and they're always buying these skins. I think the skins are

394
00:24:11,680 --> 00:24:12,560
Speaker 3:  like different costumes.

395
00:24:12,560 --> 00:24:14,700
Speaker 1:  Yeah, they're, they're basically costumes.

396
00:24:14,700 --> 00:24:17,640
Speaker 8:  Not everybody is gonna buy into it and that's okay. But there are people

397
00:24:17,640 --> 00:24:21,560
Speaker 8:  that are gonna be engaged in the Metaverses no matter what they are. I

398
00:24:21,560 --> 00:24:25,080
Speaker 8:  mean, my kids live on Fortnite and that really is in fact,

399
00:24:25,080 --> 00:24:28,880
Speaker 8:  metaverse and the kids that this generation that's coming up, that's what

400
00:24:28,880 --> 00:24:32,490
Speaker 8:  they know they can buy things with in Metaverse and they get sold

401
00:24:32,490 --> 00:24:36,120
Speaker 8:  to in Fortnight all the time. So I think it's just

402
00:24:36,120 --> 00:24:37,900
Speaker 8:  gonna continue happening.

403
00:24:37,900 --> 00:24:41,880
Speaker 3:  And that's like actual cold money coming out of Jennifer's bank accounts.

404
00:24:41,880 --> 00:24:44,700
Speaker 3:  So she's like, it is real. They are buying these things.

405
00:24:44,700 --> 00:24:45,200
Speaker 1:  Yep.

406
00:24:45,200 --> 00:24:48,720
Speaker 3:  Again, like you're right, who's gonna buy a $3,000

407
00:24:48,720 --> 00:24:52,180
Speaker 3:  Louis Vuitton bag that's digital. I'm not really sure,

408
00:24:52,180 --> 00:24:55,760
Speaker 3:  but it's not as crazy an idea as maybe it

409
00:24:55,760 --> 00:24:56,470
Speaker 3:  sounds.

410
00:24:56,470 --> 00:24:59,800
Speaker 1:  Yeah, I I think that's exactly right. And speaking of the expense of this

411
00:24:59,800 --> 00:25:03,000
Speaker 1:  all, one of the things you're making me think of with this process is this

412
00:25:03,000 --> 00:25:06,640
Speaker 1:  feels like it's such new behavior that it might be both

413
00:25:06,640 --> 00:25:10,560
Speaker 1:  like more efficient and way less efficient. Like what does this cost? Do

414
00:25:10,560 --> 00:25:13,440
Speaker 1:  you have a sense, if I wanted to do a virtual photo shoot or a real life

415
00:25:13,440 --> 00:25:17,150
Speaker 1:  photo shoot, is it the same price? Is it more one way or the other?

416
00:25:17,150 --> 00:25:20,240
Speaker 3:  I think it definitely depends on the digital character you're using. Like

417
00:25:20,240 --> 00:25:23,840
Speaker 3:  she do, for example, is so intricate that she would be more

418
00:25:23,840 --> 00:25:27,680
Speaker 3:  expensive than a human because then, you know, you're

419
00:25:27,680 --> 00:25:31,120
Speaker 3:  working with a digital stylist and it takes all these different teams of

420
00:25:31,120 --> 00:25:35,000
Speaker 3:  people to make that happen. So there are all kinds of ways

421
00:25:35,000 --> 00:25:38,440
Speaker 3:  that you can make this even more expensive if you want

422
00:25:38,440 --> 00:25:39,220
Speaker 3:  to.

423
00:25:39,220 --> 00:25:42,760
Speaker 1:  And is it the same on the other end too? Like I, I feel like the, the big

424
00:25:42,760 --> 00:25:46,350
Speaker 1:  sort of unasked question through all of this is like, does this stuff

425
00:25:46,350 --> 00:25:50,040
Speaker 1:  work? Does it work more than having human models? Does it work less than

426
00:25:50,040 --> 00:25:52,520
Speaker 1:  having human models but it's worth it for the other stuff? Like what, what

427
00:25:52,520 --> 00:25:53,500
Speaker 1:  do you hear from these folks?

428
00:25:53,500 --> 00:25:57,160
Speaker 3:  The difference is actually in the shock

429
00:25:57,160 --> 00:26:00,680
Speaker 3:  value. It really is more about the novelty. It's about the

430
00:26:00,680 --> 00:26:04,040
Speaker 3:  click factor. What Christopher and Jennifer both

431
00:26:04,040 --> 00:26:07,950
Speaker 3:  explained is that if you want to

432
00:26:07,950 --> 00:26:11,830
Speaker 3:  have a simple like person posing with

433
00:26:11,830 --> 00:26:15,760
Speaker 3:  a product and you wanna generate foot traffic to that store or you wanna

434
00:26:15,760 --> 00:26:19,040
Speaker 3:  sell that product, you're probably gonna go with a human

435
00:26:19,040 --> 00:26:19,810
Speaker 3:  influencer.

436
00:26:19,810 --> 00:26:23,790
Speaker 5:  If it's about those purposes, they're gonna filter that money down

437
00:26:23,790 --> 00:26:27,560
Speaker 5:  into agencies who are capable of delivering foot traffic

438
00:26:27,560 --> 00:26:31,340
Speaker 5:  and delivering app installs in a very scientific

439
00:26:31,340 --> 00:26:34,840
Speaker 5:  way. You know, a tracked way, a very return on investment

440
00:26:34,840 --> 00:26:38,600
Speaker 5:  mindset. However, often with virtual influencers, the

441
00:26:38,600 --> 00:26:42,380
Speaker 5:  real spend they activate is on pr, on the backend.

442
00:26:42,380 --> 00:26:46,240
Speaker 3:  Now journalists are gonna say, wow, did you know

443
00:26:46,240 --> 00:26:49,800
Speaker 3:  she's not real? That's still so many years

444
00:26:49,800 --> 00:26:53,320
Speaker 3:  later a huge story. Totally. It's just like a different kind of

445
00:26:53,320 --> 00:26:57,030
Speaker 3:  advertising. It's still surprising. I don't know how long

446
00:26:57,030 --> 00:27:00,760
Speaker 3:  that novelty can last. It's possible that it could wear

447
00:27:00,760 --> 00:27:04,720
Speaker 3:  off, but it definitely right now seems to still

448
00:27:04,720 --> 00:27:08,640
Speaker 3:  generate enough PR for it to be worth it for these brands to do

449
00:27:08,640 --> 00:27:12,320
Speaker 3:  it because they are doing it. And like as we get into more animation

450
00:27:12,320 --> 00:27:16,280
Speaker 3:  and more metaverses and video games kind

451
00:27:16,280 --> 00:27:19,880
Speaker 3:  of merging with those spaces, it's possible that this will

452
00:27:19,880 --> 00:27:23,410
Speaker 3:  become even more lucrative. And whether or not that's gonna happen

453
00:27:23,410 --> 00:27:27,240
Speaker 3:  remains to be seen. Like a lot of the metaverse we don't really

454
00:27:27,240 --> 00:27:28,210
Speaker 3:  know yet.

455
00:27:28,210 --> 00:27:32,080
Speaker 1:  It also leads towards the kind of other questions of what

456
00:27:32,080 --> 00:27:35,240
Speaker 1:  other kinds of digital influencers are there gonna be, right? Cause I feel

457
00:27:35,240 --> 00:27:38,880
Speaker 1:  like we're at a moment now where everything, at least some of those you've

458
00:27:38,880 --> 00:27:41,520
Speaker 1:  been talking about are very like human looking and it makes sense that that

459
00:27:41,520 --> 00:27:45,320
Speaker 1:  would be a place we would start. But then I think about like the, remember

460
00:27:45,320 --> 00:27:48,840
Speaker 1:  the dancing hotdog that Snapchat released a bunch of years ago that everybody

461
00:27:48,840 --> 00:27:52,520
Speaker 1:  got really excited about? Like that's just a digital influencer that's like

462
00:27:52,520 --> 00:27:54,840
Speaker 1:  one Instagram page away from making millions of dollars.

463
00:27:54,840 --> 00:27:58,120
Speaker 3:  And when you say dancing hotdog, it's funny because I was going to bring

464
00:27:58,120 --> 00:28:01,800
Speaker 3:  up an example of probably my favorite non-human virtual

465
00:28:01,800 --> 00:28:04,050
Speaker 3:  influencer. Nobody sausage.

466
00:28:04,050 --> 00:28:06,100
Speaker 1:  Oh my God. Is there another dancing hot dog?

467
00:28:06,100 --> 00:28:08,070
Speaker 3:  Yes, there's another dancing hot dog.

468
00:28:08,070 --> 00:28:09,560
Speaker 1:  This is the best news.

469
00:28:09,560 --> 00:28:13,400
Speaker 3:  Maybe technically a sausage, not a hot dog. He doesn't have a bun. He's

470
00:28:13,400 --> 00:28:14,810
Speaker 3:  just a hot dog by himself.

471
00:28:14,810 --> 00:28:18,413
Speaker 1:  Oh yeah. I'm getting his TikTok page where he has 18.2 million

472
00:28:18,413 --> 00:28:22,040
Speaker 1:  followers. Yes. I'm getting a lot of like claymation

473
00:28:22,040 --> 00:28:25,090
Speaker 1:  kind of vibes here. He comes in many colors.

474
00:28:25,090 --> 00:28:29,000
Speaker 3:  He kinda reminds me of Gumby a little bit, who is really good at

475
00:28:29,000 --> 00:28:32,520
Speaker 3:  dancing and also does comedy sort of

476
00:28:32,520 --> 00:28:35,800
Speaker 3:  videos where he's like falling on the ground. It's very

477
00:28:35,800 --> 00:28:39,030
Speaker 3:  entertaining and people seem to love it.

478
00:28:39,030 --> 00:28:42,760
Speaker 1:  That is really something. This now gets into a, a weird

479
00:28:42,760 --> 00:28:46,400
Speaker 1:  mix of things where it's like, we used to have Tony the Tiger with Frost

480
00:28:46,400 --> 00:28:50,160
Speaker 1:  Flakes. There was like Snap, crackle and Pop for the Rice Krispies are

481
00:28:50,160 --> 00:28:54,000
Speaker 1:  virtual influencers, just the new brand mascots.

482
00:28:54,000 --> 00:28:56,610
Speaker 1:  Can you sort of draw a straight line from one to the other?

483
00:28:56,610 --> 00:29:00,160
Speaker 3:  So all of those characters are just mascots. But if

484
00:29:00,160 --> 00:29:04,000
Speaker 3:  you put those characters on Instagram and then give

485
00:29:04,000 --> 00:29:07,840
Speaker 3:  them a personality and a backstory, that's when they become

486
00:29:07,840 --> 00:29:11,200
Speaker 3:  digital influencers. So for example, the Geico

487
00:29:11,200 --> 00:29:15,160
Speaker 3:  Gecko actually does have an Instagram account. Of course

488
00:29:15,160 --> 00:29:17,920
Speaker 3:  he talk and another one is

489
00:29:17,920 --> 00:29:21,800
Speaker 3:  Barbie actually. So you know, we all know Barbie as the toy, but

490
00:29:21,800 --> 00:29:24,760
Speaker 3:  kids would know Barbie as a 3D

491
00:29:24,760 --> 00:29:28,660
Speaker 3:  character who has a personality and talks to them.

492
00:29:28,660 --> 00:29:32,120
Speaker 3:  And Barbie on Instagram has 2 million followers.

493
00:29:32,120 --> 00:29:35,960
Speaker 1:  Amazing. What does the like next five years of this look like, do you

494
00:29:35,960 --> 00:29:39,320
Speaker 3:  Think? I think in terms of saying if this has staying power, a lot of

495
00:29:39,320 --> 00:29:43,120
Speaker 3:  these companies are actually becoming very

496
00:29:43,120 --> 00:29:47,080
Speaker 3:  profitable. So the Fabricant, which is one of the companies

497
00:29:47,080 --> 00:29:50,480
Speaker 3:  that makes virtual fashion designs just raised 14 million in

498
00:29:50,480 --> 00:29:53,800
Speaker 3:  investments to build the wardrobe of the

499
00:29:53,800 --> 00:29:57,600
Speaker 3:  Metaverse and then sell those closes NFTs. Okay. And

500
00:29:57,600 --> 00:30:01,390
Speaker 3:  bred the company that created little McKayla was recently

501
00:30:01,390 --> 00:30:05,240
Speaker 3:  acquired by Dapper Loves, which is an NFT startup, and they

502
00:30:05,240 --> 00:30:09,000
Speaker 3:  value bread at 125 million. Wow. So

503
00:30:09,000 --> 00:30:12,260
Speaker 3:  these companies are actually making money

504
00:30:12,260 --> 00:30:15,240
Speaker 3:  and have created a little mini

505
00:30:15,240 --> 00:30:19,190
Speaker 3:  ecosystem that Cameron says, it's just gonna keep getting bigger.

506
00:30:19,190 --> 00:30:23,040
Speaker 4:  I think this industry is growing exponentially and

507
00:30:23,040 --> 00:30:26,880
Speaker 4:  every year it matures. It's kind of proving itself, it's sticking

508
00:30:26,880 --> 00:30:30,790
Speaker 4:  around, it's, it's not just a fad anymore. It, I think we're gonna see

509
00:30:30,790 --> 00:30:34,480
Speaker 4:  lots and lots more businesses being created in this

510
00:30:34,480 --> 00:30:38,440
Speaker 4:  space. Lots of different job opportunities. I think we're gonna see a lot

511
00:30:38,440 --> 00:30:42,320
Speaker 4:  more digital fashion houses, digital modeling agencies,

512
00:30:42,320 --> 00:30:46,080
Speaker 4:  different characters, different avatars. So you really

513
00:30:46,080 --> 00:30:49,730
Speaker 4:  have this growth coming from all directions. And I've seen

514
00:30:49,730 --> 00:30:53,600
Speaker 4:  so, so many people becoming instantly successful and

515
00:30:53,600 --> 00:30:57,110
Speaker 4:  and busy because these skills are so in demand.

516
00:30:57,110 --> 00:31:01,040
Speaker 3:  They definitely believe that this is where the

517
00:31:01,040 --> 00:31:04,920
Speaker 3:  world is going. Like in the next five years, 10 years,

518
00:31:04,920 --> 00:31:08,800
Speaker 3:  the near future, we will be living more

519
00:31:08,800 --> 00:31:12,760
Speaker 3:  in digital spaces. We will be living more in digital worlds. And the

520
00:31:12,760 --> 00:31:16,480
Speaker 3:  line between real life and digital life is

521
00:31:16,480 --> 00:31:20,440
Speaker 3:  going to be blurrier. We'll be able to interact with these characters

522
00:31:20,440 --> 00:31:24,260
Speaker 3:  in the Metaverses and we're all gonna have our own avatars,

523
00:31:24,260 --> 00:31:28,080
Speaker 3:  and that's where they feel the future is going. So I think

524
00:31:28,080 --> 00:31:31,680
Speaker 3:  the creators of them are very aware of that. They're like, whatever

525
00:31:31,680 --> 00:31:35,600
Speaker 3:  technology comes out, that's where we have to be. If the

526
00:31:35,600 --> 00:31:39,000
Speaker 3:  metaverse is the thing that's cool, we gotta be in the metaverse. If people

527
00:31:39,000 --> 00:31:42,600
Speaker 3:  wanna talk to us, we better animate. Like they have to keep

528
00:31:42,600 --> 00:31:46,440
Speaker 3:  evolving to stay relevant and to stay desirable if they wanna keep

529
00:31:46,440 --> 00:31:47,250
Speaker 3:  making money.

530
00:31:47,250 --> 00:31:51,210
Speaker 1:  Another extremely human desire stay relevant

531
00:31:51,210 --> 00:31:54,960
Speaker 1:  or the modeling industry forgets about you, I think is something a lot of

532
00:31:54,960 --> 00:31:58,800
Speaker 1:  real humans can identify with. Eventually. This is coming

533
00:31:58,800 --> 00:32:02,200
Speaker 1:  for us too. You and I are gonna be replaced by AI

534
00:32:02,200 --> 00:32:05,760
Speaker 1:  versions of us who make podcasts together. Yeah, I'm actually okay with that,

535
00:32:05,760 --> 00:32:09,070
Speaker 1:  even as I say it out loud. That sounds great. I'll just, I'll be around.

536
00:32:09,070 --> 00:32:13,040
Speaker 1:  It's no problem. All right, Gina, thank you for exploring this deeply bizarre

537
00:32:13,040 --> 00:32:15,560
Speaker 1:  world with us. This, this is incredibly fun.

538
00:32:15,560 --> 00:32:16,400
Speaker 3:  Thank you so

539
00:32:16,400 --> 00:32:26,710
Speaker 3:  much.

