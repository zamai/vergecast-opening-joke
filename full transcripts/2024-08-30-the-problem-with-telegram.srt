1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: cbe34ed9-5675-42ef-ae7e-accf80d6e8d5
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/8738651179393976956/9204521294220976189/s93290-US-5351s-1725040067.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and David Pierce discuss Telegram CEO being charged in a French criminal investigation over content moderation, Yelp suing Google for antitrust violations, a week in AI-generated nonsense, and more.

Telegram says CEO has ‘nothing to hide’ after being arrested in France 

French authorities arrest Telegram’s CEO

Why the Telegram CEO’s arrest is such a big deal

Telegram CEO charged in French criminal investigation

Telegram CEO Pavel Durov faces court questioning in France.

French prosecutors explain why they arrested Telegram CEO Pavel Durov

How Pavel Durov, Telegram’s Founder, Went From Russia’s Mark Zuckerberg to Wanted Man

Can Tech Executives Be Held Responsible for What Happens on Their Platforms?

How Telegram played itself

Yelp sues Google for antitrust violations


TikTok must face a lawsuit for recommending the viral ‘blackout challenge’

California State Assembly passes sweeping AI safety bill

Mark Zuckerberg responds to GOP pressure, says Biden pushed to ‘censor’ covid post

Google Gemini will let you create AI-generated people again

xAI’s new Grok image generator floods X with controversial AI fakes

X’s Grok directs to government site after sharing false election info

Smart home company Brilliant has found a buyer

ESPN ‘Where to Watch’ feature helps find where to stream sporting events

Plaud’s NotePin is an AI wearable for summarizing meetings and taking voice notes

The maker of the Palma has a new cheaper e-reader

The Dyson Airwrap i.d. is a smarter hair curler

Snapchat finally launched an iPad app

Instagram adds what photos have always needed: words

Apple’s iPhone 16 launch event is set for September


Email us at vergecast@theverge.com or call us at 866-VERGE11, we love hearing from you.
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (45 ads detected)

2
00:01:18,765 --> 00:01:21,105
Speaker 3:  and I I don't know that those are the right decisions.

3
00:01:22,815 --> 00:01:26,705
Speaker 4:  Well, you can't read. We should say that upfront. Neli famously

4
00:01:26,705 --> 00:01:27,225
Speaker 4:  illiterate.

5
00:01:28,065 --> 00:01:31,985
Speaker 3:  I hate reading. Hi, I'm your friend Neli. I I I love reading

6
00:01:32,085 --> 00:01:34,305
Speaker 3:  and I think you should do more of it. David Pierce is here.

7
00:01:34,765 --> 00:01:34,985
Speaker 4:  Hi

8
00:01:35,215 --> 00:01:36,665
Speaker 3:  Alex. Cranz is here. I'm

9
00:01:36,665 --> 00:01:37,985
Speaker 5:  Just thinking about reading Rainbow now.

10
00:01:38,445 --> 00:01:41,785
Speaker 3:  You know, it's, it's fun when you have a small child, you're like in reading

11
00:01:41,785 --> 00:01:45,585
Speaker 3:  again. That's a real thing. And which, what Max wants to read with me most

12
00:01:45,605 --> 00:01:48,585
Speaker 3:  is Calvin and Hobbes, which is, I love it. The

13
00:01:48,585 --> 00:01:49,545
Speaker 4:  Dream that rules.

14
00:01:49,975 --> 00:01:53,705
Speaker 3:  Have you ever tried to read Calvin and Hobbes out loud to another

15
00:01:53,705 --> 00:01:57,025
Speaker 3:  person who doesn't understand the jokes from the literal

16
00:01:57,505 --> 00:01:59,945
Speaker 3:  1980s? She's like, what's a telephone?

17
00:02:01,455 --> 00:02:05,345
Speaker 4:  Kavanaugh Hobbs is definitely a cartoon for adults. Like it is not,

18
00:02:05,345 --> 00:02:09,305
Speaker 4:  yeah. The jokes in it and, and like the messages in it I find like deep

19
00:02:09,365 --> 00:02:12,985
Speaker 4:  and important and profound now. And I'm like very old

20
00:02:13,445 --> 00:02:17,425
Speaker 4:  and it, how, I don't know how to explain any of that to a child.

21
00:02:17,905 --> 00:02:21,305
Speaker 3:  Try, try. I just, if you have a kid and you're like, here's what we can do,

22
00:02:21,305 --> 00:02:24,505
Speaker 3:  we can read comic books together. We were at, I already own all these books

23
00:02:24,505 --> 00:02:27,705
Speaker 3:  and we were at an estate sale like down the street from our house and we

24
00:02:27,725 --> 00:02:30,665
Speaker 3:  bought the full collection again 'cause it was so cheap. And I was like,

25
00:02:31,175 --> 00:02:33,865
Speaker 3:  this is what money's for. Like this is how you make it to buy all the Calvin

26
00:02:33,915 --> 00:02:37,065
Speaker 3:  Hobbs books whenever they're available to you. Yeah. And so she was excited

27
00:02:37,225 --> 00:02:40,865
Speaker 3:  'cause we bought all these what appeared to her to be comic books. I encourage

28
00:02:40,865 --> 00:02:44,745
Speaker 3:  you, if you have children, to try and just try to read any comic strip. Like

29
00:02:44,745 --> 00:02:47,865
Speaker 3:  first of all, that's bananas. True. And then these are like the multilayers,

30
00:02:47,865 --> 00:02:51,825
Speaker 3:  like I'm giggling and she's like, I don't, is the, is the tiger alive?

31
00:02:52,025 --> 00:02:52,825
Speaker 3:  I don't understand.

32
00:02:53,485 --> 00:02:54,985
Speaker 5:  Get her in on Kathy's. See

33
00:02:55,085 --> 00:02:56,025
Speaker 3:  She looked at me. Let's see

34
00:02:56,245 --> 00:02:57,145
Speaker 5:  How she feels about Kathy.

35
00:02:58,405 --> 00:02:58,865
Speaker 3:  Oh man.

36
00:03:00,505 --> 00:03:03,205
Speaker 3:  She looked at me very seriously the other day and she goes, my line never

37
00:03:03,205 --> 00:03:07,025
Speaker 3:  comes alive. And I was like, well, you're a grown up kid. You're in first

38
00:03:07,025 --> 00:03:07,745
Speaker 3:  grade now. Yeah.

39
00:03:07,815 --> 00:03:10,865
Speaker 4:  Working taxes your imagination, max. How about that?

40
00:03:11,455 --> 00:03:15,345
Speaker 3:  Alright. There's a lot going on this week. It's kind of a, we're two weeks

41
00:03:15,345 --> 00:03:19,065
Speaker 3:  out from the iPhone event. It's, I keep joking. It's like summer's over.

42
00:03:19,065 --> 00:03:22,945
Speaker 3:  That's where we are. Yep. I candidly have just like

43
00:03:23,485 --> 00:03:26,905
Speaker 3:  end of summer whatever, like that's my brain. Like

44
00:03:27,215 --> 00:03:30,025
Speaker 3:  what if I, what if I just shut it down for a couple weeks before we ramp

45
00:03:30,025 --> 00:03:33,305
Speaker 3:  up into the gadget season? But there's still a lot of news a ton and all

46
00:03:33,305 --> 00:03:36,825
Speaker 3:  of it is like policy news, like the, the, the legal systems of the world

47
00:03:36,825 --> 00:03:39,625
Speaker 3:  just keep generating PDFs for me to read on a Bos Palmer,

48
00:03:39,825 --> 00:03:43,745
Speaker 4:  I mean literally it's just everyone trying to

49
00:03:44,005 --> 00:03:46,825
Speaker 4:  do the work that they have to do so that they can have a long weekend over

50
00:03:46,825 --> 00:03:50,025
Speaker 4:  Labor Day. Like I'm, I'm convinced that's, this is the same thing that happens

51
00:03:50,455 --> 00:03:53,665
Speaker 4:  like right before Thanksgiving. It's just a bunch of people who are like,

52
00:03:53,705 --> 00:03:57,665
Speaker 4:  I need a minute. Let me, let me file all my paperwork. Let me get

53
00:03:57,665 --> 00:04:01,425
Speaker 4:  all the work done. Let me push publish on the thing and then get out. Like,

54
00:04:01,655 --> 00:04:05,585
Speaker 4:  nothing is going to happen on Friday. Nothing. Everybody's on vacation. They're

55
00:04:05,585 --> 00:04:07,705
Speaker 4:  like, I did it already four day weekend. Let And

56
00:04:07,705 --> 00:04:10,865
Speaker 3:  You're listening to this on a Friday, so get ready. Yeah. We're gonna recapitulate

57
00:04:10,985 --> 00:04:11,585
Speaker 3:  everyone else you've

58
00:04:11,585 --> 00:04:13,185
Speaker 4:  Done work any work today. Stop it.

59
00:04:13,625 --> 00:04:17,585
Speaker 5:  I I do think like the, the theory falls a little short when we

60
00:04:17,705 --> 00:04:21,025
Speaker 5:  consider France. 'cause they don't do Labor Day the same Labor day. I mean

61
00:04:21,025 --> 00:04:22,385
Speaker 3:  Every day in France is Labor Day.

62
00:04:22,665 --> 00:04:23,025
Speaker 5:  That's true.

63
00:04:23,335 --> 00:04:26,145
Speaker 3:  They're like, were you counting on the buses today? Strike.

64
00:04:27,325 --> 00:04:30,505
Speaker 3:  That's not how French people talk. Can I just talk about the, the Boots Bomber

65
00:04:30,505 --> 00:04:34,225
Speaker 3:  for one more second. So governments around the world generate PDFs and it

66
00:04:34,225 --> 00:04:38,145
Speaker 3:  is our job to read them a broad in broad strokes. That's how I think of

67
00:04:38,145 --> 00:04:42,025
Speaker 3:  the virtual policy teams coverage. Yes. David made everybody

68
00:04:42,025 --> 00:04:45,465
Speaker 3:  buy a b Palmer. Right. He came to your house and it with threat of violence

69
00:04:45,575 --> 00:04:48,105
Speaker 3:  made you buy an EIN Android phone. He

70
00:04:48,105 --> 00:04:50,065
Speaker 5:  Just knocked on my door. But it was, it was polite.

71
00:04:50,605 --> 00:04:52,025
Speaker 3:  But it happened. It, yeah,

72
00:04:52,025 --> 00:04:52,225
Speaker 5:  It

73
00:04:52,385 --> 00:04:56,105
Speaker 3:  Happened. The ver the other function of The Verge besides responding to

74
00:04:56,105 --> 00:04:59,225
Speaker 3:  government PDFs is making people buy stupid gadgets.

75
00:05:00,085 --> 00:05:03,865
Speaker 3:  I'm very confident in this dynamic. Like this is how it should be. So I.

76
00:05:03,985 --> 00:05:07,025
Speaker 3:  I buy a BWA and I'm like, this is how I'm gonna do PDFs 'cause so much of

77
00:05:07,025 --> 00:05:10,505
Speaker 3:  my job is reading PDFs. But then you need another piece of software. 'cause

78
00:05:10,505 --> 00:05:14,185
Speaker 3:  it's not good at anything by itself. No, because it's just a low end

79
00:05:14,185 --> 00:05:17,905
Speaker 3:  Android phone with an e this way. So then you gotta get Read Wise Reader

80
00:05:19,365 --> 00:05:22,145
Speaker 3:  and it feels like those two companies should just

81
00:05:23,435 --> 00:05:27,335
Speaker 3:  put it together. Like those are the 200% things. It's not actually

82
00:05:27,355 --> 00:05:31,175
Speaker 3:  the B Palmer that people want it's read wise reader on an eing screen.

83
00:05:31,235 --> 00:05:31,455
Speaker 3:  Yes.

84
00:05:31,575 --> 00:05:35,135
Speaker 4:  A hundred percent could not agree more. And and like

85
00:05:35,905 --> 00:05:39,055
Speaker 4:  every time Alex and I both go through this, every time we talk to a company

86
00:05:39,055 --> 00:05:41,815
Speaker 4:  that makes e-readers, we say, when are you gonna make a decent app for reading

87
00:05:41,815 --> 00:05:44,375
Speaker 4:  things? And every time we talk to a company that makes a reading app, it's

88
00:05:44,375 --> 00:05:48,095
Speaker 4:  like, why don't you make hardware? And at some point somebody's gonna

89
00:05:48,155 --> 00:05:51,855
Speaker 4:  do it and it's gonna be great because you are exactly correct. Didn't

90
00:05:51,875 --> 00:05:52,815
Speaker 5:  Amazon kind of do it

91
00:05:52,815 --> 00:05:56,135
Speaker 3:  Right? This is a no, what I'm arguing for is lockin, but I'm just

92
00:05:56,135 --> 00:05:59,845
Speaker 4:  Pointing out I software that's the the word good is so, so, so important

93
00:05:59,845 --> 00:06:02,765
Speaker 4:  in that That's true. That's true. Read wise reader, good software. There's

94
00:06:02,765 --> 00:06:06,245
Speaker 4:  good software out there. Omnivore free app, good software.

95
00:06:06,835 --> 00:06:09,245
Speaker 4:  Make a, make a thing for it. I mean Omni

96
00:06:09,465 --> 00:06:12,965
Speaker 3:  Or, or work together with the company that's making the thing like if you

97
00:06:12,985 --> 00:06:16,285
Speaker 3:  buy the books, but you would not know that Read Wise Reader exists and is

98
00:06:16,285 --> 00:06:19,405
Speaker 3:  the thing that unlocks the hardware. Right. Right. I feel like they could

99
00:06:19,405 --> 00:06:21,885
Speaker 3:  just close that loop Anyway, read Wise Reader, the thing that you do to it

100
00:06:21,885 --> 00:06:25,605
Speaker 3:  is you upload the PDF and it basically ocr it into a format

101
00:06:25,605 --> 00:06:29,365
Speaker 3:  that looks good on an ING screen, which is great. Only that takes

102
00:06:29,365 --> 00:06:33,205
Speaker 3:  time. And I have no attention span and no patience. And I was like,

103
00:06:33,205 --> 00:06:35,765
Speaker 3:  screw it. I'm just reading this PDF on my iPad and that has happened like

104
00:06:35,765 --> 00:06:36,565
Speaker 3:  five times in a row.

105
00:06:36,765 --> 00:06:40,445
Speaker 4:  Yeah. Kevin Wynn, our features editor is, is constantly dealing with this

106
00:06:40,565 --> 00:06:43,965
Speaker 4:  'cause we're always getting like galley of books and stuff and

107
00:06:44,655 --> 00:06:48,045
Speaker 4:  every time I get like a PDF of a book that's about to be released that I

108
00:06:48,045 --> 00:06:51,645
Speaker 4:  have to read for whatever reason, I first try to upload it into my Kindle

109
00:06:51,645 --> 00:06:54,925
Speaker 4:  where it doesn't work and looks bad. Then I try to put it in read wise where

110
00:06:54,925 --> 00:06:58,725
Speaker 4:  it doesn't work and looks bad. And then I end up reading it like in Dropbox

111
00:06:58,825 --> 00:07:02,605
Speaker 4:  on my iPad and I'm like, how is this the best possible solution

112
00:07:02,605 --> 00:07:04,445
Speaker 4:  here? And yet it

113
00:07:04,445 --> 00:07:07,645
Speaker 3:  Is. and it, every time it's, I I've come this close from

114
00:07:08,255 --> 00:07:12,005
Speaker 3:  going from Books Palmer to, to buying an iPad mini like pilots around the,

115
00:07:12,005 --> 00:07:15,285
Speaker 3:  the world. By the way, people now just send us creep shots of pilots

116
00:07:15,755 --> 00:07:16,525
Speaker 3:  with iPads.

117
00:07:16,805 --> 00:07:17,445
Speaker 4:  It's the best.

118
00:07:17,955 --> 00:07:21,565
Speaker 3:  It's my favorite. Yeah. Very unexpected outcome of hosting a podcast about

119
00:07:21,575 --> 00:07:25,405
Speaker 3:  technology. But pilot, you look, all the pilots look great.

120
00:07:25,725 --> 00:07:28,485
Speaker 3:  Everyone looks happy with their iPad. Okay, let's talk about some actual

121
00:07:28,685 --> 00:07:32,405
Speaker 3:  technique. So we, we gotta talk about Telegram. There's a bunch of other

122
00:07:32,405 --> 00:07:35,325
Speaker 3:  legal stuff that happened this week and then we got a a, a true lightning

123
00:07:35,325 --> 00:07:38,205
Speaker 3:  round where we're gonna try to get through 'em all to wrap this thing up.

124
00:07:38,205 --> 00:07:42,005
Speaker 3:  Yeah. Yep. But we should start with Telegram, which is, I would say

125
00:07:42,025 --> 00:07:45,805
Speaker 3:  not only news of the week, but the ongoing news of of time to

126
00:07:45,805 --> 00:07:49,565
Speaker 3:  come. Yeah. Because the idea that's that social or messaging

127
00:07:49,845 --> 00:07:53,285
Speaker 3:  platform owners face criminal liability for what happens on their platforms.

128
00:07:53,945 --> 00:07:56,805
Speaker 3:  It might feel new and shocking 'cause the French government arrested this

129
00:07:56,805 --> 00:08:00,405
Speaker 3:  guy, but it's also been building for a very, very long time

130
00:08:00,675 --> 00:08:04,005
Speaker 3:  that some amount of responsibility for what happens on our platform will

131
00:08:04,005 --> 00:08:07,805
Speaker 3:  be imposed. And ev government, our government has wanted to do this. Right.

132
00:08:07,805 --> 00:08:10,805
Speaker 3:  I mean Congress is like Jack Dorsey, we're gonna yell at you for a while.

133
00:08:11,995 --> 00:08:15,165
Speaker 3:  This one I think is particularly shocking 'cause of just the facts of what

134
00:08:15,165 --> 00:08:19,045
Speaker 3:  Telegram is. The fact that Paval Duro, the CEO landed

135
00:08:19,145 --> 00:08:22,245
Speaker 3:  in France on a private jet and the French authorities immediately arrested

136
00:08:22,245 --> 00:08:25,965
Speaker 3:  him. Why land your plane where the people are gonna arrest you? His sort

137
00:08:25,965 --> 00:08:28,605
Speaker 3:  of unrepentant attitude about what's going on the platform. David, catch

138
00:08:28,605 --> 00:08:29,565
Speaker 3:  us up on the basics.

139
00:08:29,835 --> 00:08:33,805
Speaker 4:  Sure. So basically last Sunday, Paval Duro,

140
00:08:33,805 --> 00:08:37,445
Speaker 4:  the CEO of Telegram was arrested, like you said, after landing. The

141
00:08:37,585 --> 00:08:38,365
Speaker 4:  PJ in France

142
00:08:39,555 --> 00:08:40,725
Speaker 3:  Just don't land your PJ there.

143
00:08:40,795 --> 00:08:41,965
Speaker 4:  It's, it's, it's an odd move. You're

144
00:08:41,965 --> 00:08:42,445
Speaker 3:  Connecting

145
00:08:43,015 --> 00:08:46,725
Speaker 4:  Don't call it No, just don't land. The PJ is

146
00:08:46,945 --> 00:08:50,525
Speaker 4:  is a good, we can make shirts that say just don't land the PJ that could

147
00:08:50,525 --> 00:08:50,645
Speaker 4:  work,

148
00:08:51,075 --> 00:08:52,085
Speaker 3:  Stay in the sky Pavo.

149
00:08:52,945 --> 00:08:56,845
Speaker 4:  And all we knew at the time was that he was arrested. I believe the

150
00:08:56,845 --> 00:09:00,125
Speaker 4:  French authorities said like in conjunction with an investigation into

151
00:09:00,865 --> 00:09:04,845
Speaker 4:  crimes. It was very vague. But he, he was arrested and over

152
00:09:04,845 --> 00:09:08,685
Speaker 4:  the next several days it, it came out that essentially what

153
00:09:08,685 --> 00:09:12,245
Speaker 4:  was happening is that he was being arrested based on

154
00:09:13,165 --> 00:09:17,045
Speaker 4:  criminal activity that was happening on Telegram. He, he was not as

155
00:09:17,045 --> 00:09:20,605
Speaker 4:  far as I understand, being accused of doing the crimes, but of, of what the

156
00:09:20,605 --> 00:09:24,445
Speaker 4:  French government ended up calling complicity in the, the criminal

157
00:09:24,685 --> 00:09:27,325
Speaker 4:  activity that was happening on Telegram. He has since been charged with a

158
00:09:27,325 --> 00:09:31,045
Speaker 4:  bunch of things, most of which are complicity for things like money

159
00:09:31,205 --> 00:09:35,125
Speaker 4:  laundering and child abuse, child sexual abuse material, all

160
00:09:35,125 --> 00:09:38,845
Speaker 4:  sorts of other internet crimes. And the, the overarching theme seems to be

161
00:09:39,725 --> 00:09:43,645
Speaker 4:  a, a lot of this bad stuff was happening in relative plain sight

162
00:09:43,745 --> 00:09:46,925
Speaker 4:  on Telegram. And b Telegram

163
00:09:47,705 --> 00:09:51,605
Speaker 4:  not only knew it was hosting this stuff and allowed

164
00:09:51,605 --> 00:09:55,565
Speaker 4:  it, but actively resisted working with governments

165
00:09:55,945 --> 00:09:59,925
Speaker 4:  to help. Yes. I think, again, this is the very beginning of what I

166
00:09:59,925 --> 00:10:03,165
Speaker 4:  think is gonna be a very long story, but I think in terms of like the basics

167
00:10:03,165 --> 00:10:06,485
Speaker 4:  of what we know, I think that's kind of where we are at this moment. Yeah.

168
00:10:06,745 --> 00:10:08,125
Speaker 4:  Am I missing anything? No,

169
00:10:08,125 --> 00:10:11,645
Speaker 3:  Those are the basics. Obviously none of us are experts in French law. If

170
00:10:11,645 --> 00:10:14,885
Speaker 3:  you are an expert bull Sure. French law, you know, hit us up. We would love

171
00:10:14,885 --> 00:10:15,245
Speaker 3:  to talk to you.

172
00:10:17,155 --> 00:10:20,495
Speaker 3:  But the, the piece of that puzzle that are really important here that I think

173
00:10:20,495 --> 00:10:24,415
Speaker 3:  we can understand and talk about credibly are one, how Telegram

174
00:10:24,425 --> 00:10:28,415
Speaker 3:  works. Because a lot of people want to impute how other

175
00:10:28,735 --> 00:10:32,495
Speaker 3:  platform works onto Telegram and they're actually really different. So

176
00:10:32,685 --> 00:10:35,775
Speaker 3:  it's a very common misconception that Telegram is encrypted and it is just

177
00:10:35,775 --> 00:10:39,175
Speaker 3:  not, it's not even like Secret Chats

178
00:10:39,665 --> 00:10:43,175
Speaker 3:  one-to-one chats You can push a button and make them encrypted, but that

179
00:10:43,175 --> 00:10:46,975
Speaker 3:  is not the default. Right. Telegram groups are not encrypted. Like

180
00:10:47,055 --> 00:10:50,855
Speaker 3:  there's nothing inherently secure about Telegram. Like I will tell you,

181
00:10:51,015 --> 00:10:54,855
Speaker 3:  I know a lot of activists who know, who know that there

182
00:10:54,855 --> 00:10:58,455
Speaker 3:  is CSAM on iMessage that there is is bad stuff on

183
00:10:58,455 --> 00:11:02,135
Speaker 3:  signal. And the defense is these are end-to-end encrypted.

184
00:11:03,155 --> 00:11:07,015
Speaker 3:  No one can see the data except the participants. So Apple knows it's there.

185
00:11:07,565 --> 00:11:11,295
Speaker 3:  Everybody, people have told them, they've actually built some tools to try

186
00:11:11,295 --> 00:11:14,415
Speaker 3:  to detect it and gotten yelled at Yeah. For essentially breaking the encryption.

187
00:11:14,415 --> 00:11:18,175
Speaker 3:  We've, we've talked about that straight at length, but fundamentally it can't

188
00:11:18,175 --> 00:11:22,055
Speaker 3:  see it. So it has this out like these companies have these outs,

189
00:11:22,055 --> 00:11:25,895
Speaker 3:  like people are using, doing bad things on our platforms, but we can't see

190
00:11:25,895 --> 00:11:29,655
Speaker 3:  it. So we don't know. And you should find other ways to catch them. Right?

191
00:11:29,655 --> 00:11:33,055
Speaker 3:  And this is a lot of the argument about end-to-end encryption is

192
00:11:33,595 --> 00:11:37,095
Speaker 3:  the cops wanna see the bad stuff and they want the companies to build them

193
00:11:37,095 --> 00:11:40,695
Speaker 3:  back doors to give the bad stuff to them. And So, I think there's a

194
00:11:40,705 --> 00:11:44,575
Speaker 3:  temptation to assume Telegram is operating the same way and it

195
00:11:44,595 --> 00:11:48,295
Speaker 3:  is not. Right. And I think that is actually the first most important thing

196
00:11:48,795 --> 00:11:52,255
Speaker 3:  is all this stuff is just happening out in the wide open in Telegram

197
00:11:52,855 --> 00:11:56,125
Speaker 3:  in these huge channels, in these unencrypted chats.

198
00:11:56,525 --> 00:11:59,765
Speaker 3:  Telegram can see almost everything that happens unless you press secret chat,

199
00:11:59,765 --> 00:12:03,005
Speaker 3:  which no one is pressing and it only happens between people anyway. And so

200
00:12:03,005 --> 00:12:06,885
Speaker 3:  it is a very unusual service in the sense that it,

201
00:12:06,945 --> 00:12:10,885
Speaker 3:  it knows te it has the technical ability to know what is happening on service.

202
00:12:10,985 --> 00:12:13,445
Speaker 3:  And lots of people can see what's happening on service, which

203
00:12:13,675 --> 00:12:15,805
Speaker 5:  Have y'all used Telegram a whole lot

204
00:12:16,545 --> 00:12:17,325
Speaker 4:  In bits and pieces

205
00:12:17,605 --> 00:12:19,365
Speaker 3:  Today? Today felt like the wrong day to start.

206
00:12:19,845 --> 00:12:23,765
Speaker 5:  I would say no. I like, I remember using it. I, I used it when I

207
00:12:23,765 --> 00:12:26,565
Speaker 5:  went to Taipei at one point just 'cause it was easy to communicate with my

208
00:12:26,565 --> 00:12:30,485
Speaker 5:  friends. And when I got back Covid hit was living in New York and

209
00:12:30,485 --> 00:12:33,325
Speaker 5:  so all the protests and stuff were happening. So there was a ton of organizing

210
00:12:33,755 --> 00:12:37,635
Speaker 5:  happening on Telegram most miserable experience I've ever

211
00:12:37,635 --> 00:12:41,395
Speaker 5:  had using a chat program. Just awful. Because it is just like,

212
00:12:42,065 --> 00:12:45,155
Speaker 5:  it's a chat room. It's just, okay, you're now in a chat room and you're gonna

213
00:12:45,155 --> 00:12:49,075
Speaker 5:  go to bed and wake up to 200 new messages in a chat room

214
00:12:49,815 --> 00:12:50,475
Speaker 5:  and you have no

215
00:12:50,475 --> 00:12:52,875
Speaker 4:  Control over that. It actually, you make a good point. Let me just quickly

216
00:12:52,905 --> 00:12:55,555
Speaker 4:  like explain the rough structure of Telegram because like you're saying,

217
00:12:55,615 --> 00:12:58,315
Speaker 4:  Eli it is, it is actually instructive for what's

218
00:12:58,315 --> 00:12:59,835
Speaker 3:  Happening here. It tells you a lot about the story.

219
00:12:59,835 --> 00:13:03,315
Speaker 4:  Yeah, yeah. So there are, there are basically three levels of

220
00:13:03,395 --> 00:13:06,515
Speaker 4:  Telegram which exists, I would say somewhere in the, like if you imagine

221
00:13:06,535 --> 00:13:10,355
Speaker 4:  the overlapping Venn diagram of like WhatsApp and

222
00:13:10,505 --> 00:13:14,395
Speaker 4:  Discord and like WeChat. Like Telegram

223
00:13:14,395 --> 00:13:17,835
Speaker 4:  is somewhere in the middle of those three things. So all the way at the top

224
00:13:17,835 --> 00:13:21,435
Speaker 4:  you have public channels, right? Which is basically like the

225
00:13:21,435 --> 00:13:24,835
Speaker 4:  equivalent of an Instagram feed, right? You, you post something,

226
00:13:25,185 --> 00:13:29,075
Speaker 4:  lots of people can see it. It's a, it's essentially just a public, like

227
00:13:29,075 --> 00:13:33,035
Speaker 4:  one-way feed the middle thing, which I think is probably telegrams

228
00:13:33,035 --> 00:13:36,875
Speaker 4:  like most, I don't know if not most used and sort of most

229
00:13:37,135 --> 00:13:40,355
Speaker 4:  unique thing is the group chats. And the group chats can have, I think the

230
00:13:40,355 --> 00:13:43,835
Speaker 4:  number is up to 200,000 people in them, which is a crazy number,

231
00:13:44,375 --> 00:13:48,075
Speaker 4:  but is is a huge part of the reason that Telegram has been used for things

232
00:13:48,075 --> 00:13:52,035
Speaker 4:  like political organizing and for huge, like government communications.

233
00:13:52,065 --> 00:13:55,835
Speaker 4:  It's because You can have that many people literally in a space together.

234
00:13:56,215 --> 00:13:59,875
Speaker 4:  It becomes abject chaos. But it can be really useful. And then

235
00:14:00,135 --> 00:14:04,115
Speaker 4:  at the bottom of that is one-to-one chat. So You can go all

236
00:14:04,115 --> 00:14:07,275
Speaker 4:  the way from, you and I are texting each other to

237
00:14:07,945 --> 00:14:11,795
Speaker 4:  like one to millions inside of Telegram.

238
00:14:12,115 --> 00:14:15,235
Speaker 4:  And there's also like, there's, there's app stuff going on in there. There

239
00:14:15,235 --> 00:14:17,795
Speaker 4:  are all kinds of like plug iny things You can do inside of Telegram. It has

240
00:14:17,795 --> 00:14:21,595
Speaker 4:  some of that like WeChat operating system dynamic

241
00:14:21,735 --> 00:14:23,795
Speaker 4:  to it. But most of what

242
00:14:23,795 --> 00:14:24,835
Speaker 5:  Happens truly terrible emojis.

243
00:14:25,015 --> 00:14:28,315
Speaker 4:  Oh, it's not a good looking app in the same way that WeChat is not a good

244
00:14:28,315 --> 00:14:31,675
Speaker 4:  looking app, but like it does a lot. Yeah. And so it's, it's very useful

245
00:14:31,855 --> 00:14:35,035
Speaker 4:  for those reasons. And so the reason

246
00:14:35,865 --> 00:14:39,515
Speaker 4:  most people use it, and for our purposes, I think the reason it's particularly

247
00:14:39,875 --> 00:14:42,835
Speaker 4:  interesting right now because of what's going on with POV is

248
00:14:43,575 --> 00:14:47,235
Speaker 4:  really probably the top two things, right? The thing where one person can

249
00:14:47,395 --> 00:14:50,915
Speaker 4:  communicate with a lot of people very quickly and the sort of giant

250
00:14:51,115 --> 00:14:54,965
Speaker 4:  teaming group chats of up to 200,000 people, right? So like there's nothing

251
00:14:54,965 --> 00:14:56,965
Speaker 4:  else that works quite that same way on

252
00:14:56,965 --> 00:15:00,805
Speaker 3:  The internet. And to be clear, if you have a one to 200,000 person

253
00:15:00,855 --> 00:15:04,645
Speaker 3:  group chat, not, it doesn't matter if that's encrypted definitionally, that

254
00:15:04,645 --> 00:15:07,685
Speaker 3:  thing is leaking, right? Yeah. It's, it, it, it, you're not trying to keep

255
00:15:07,685 --> 00:15:11,365
Speaker 3:  that secret. Like that's not the point of talking to 200,000 people. So

256
00:15:11,555 --> 00:15:14,325
Speaker 3:  there's all these arguments for why it would and would not be encrypted.

257
00:15:14,545 --> 00:15:18,525
Speaker 3:  But I I think the first thing to just be very clear on is it's

258
00:15:18,525 --> 00:15:20,885
Speaker 3:  not like the other platforms that are used for political organizing. It's

259
00:15:20,885 --> 00:15:23,845
Speaker 3:  not like the other platforms that are encrypted or claim to be encrypted.

260
00:15:24,425 --> 00:15:28,285
Speaker 3:  It is inherently this broadcast medium. And then the other piece, which is

261
00:15:28,285 --> 00:15:31,965
Speaker 3:  tremendously relevant to all of this is that it has

262
00:15:32,145 --> 00:15:35,765
Speaker 3:  no, the has no content moderation, which is not like a technical

263
00:15:36,125 --> 00:15:39,285
Speaker 3:  decision that is a policy decision. They've just decided

264
00:15:39,835 --> 00:15:42,645
Speaker 3:  this is gonna be fine, it's a free for all. We don't care. We support free

265
00:15:42,645 --> 00:15:46,525
Speaker 3:  speech and the policy decision's a flow from that get all the way to like,

266
00:15:46,525 --> 00:15:50,085
Speaker 3:  how do we treat governments and also just implicate things like ISIS lives

267
00:15:50,105 --> 00:15:53,365
Speaker 3:  on Telegram. This is just gonna be the app that ISIS uses and people have

268
00:15:53,365 --> 00:15:56,565
Speaker 3:  known this for a long time. We are going to accept an enormous

269
00:15:57,105 --> 00:16:00,125
Speaker 3:  amount of pornography, including some of the worst pornography, including

270
00:16:00,125 --> 00:16:03,565
Speaker 3:  some child sexual abuse material, perhaps lots of child sexual abuse material.

271
00:16:03,565 --> 00:16:07,005
Speaker 3:  And everyone can just see it and researchers can just file a report after

272
00:16:07,005 --> 00:16:10,885
Speaker 3:  report about this, the prevalence of this material on Telegram and the

273
00:16:10,885 --> 00:16:14,645
Speaker 3:  company is gonna do nothing about it. And then that brings you to the actual

274
00:16:14,645 --> 00:16:18,325
Speaker 3:  policy decisions, which is the way they've structured the data on

275
00:16:18,325 --> 00:16:21,885
Speaker 3:  Telegram means that if a government wants to issue a warrant or a subpoena,

276
00:16:22,315 --> 00:16:26,085
Speaker 3:  they actually have to issue like 20 right? In countries across the world

277
00:16:26,585 --> 00:16:29,525
Speaker 3:  in order to get the data and prove the case against the bad people. And telegrams

278
00:16:29,525 --> 00:16:33,045
Speaker 3:  are very proud of this. And all platforms have, you know, kind of like their

279
00:16:33,045 --> 00:16:36,925
Speaker 3:  transparency reports or you know, in other

280
00:16:36,925 --> 00:16:40,805
Speaker 3:  times, like they've had what's called canaries where they, they

281
00:16:40,825 --> 00:16:43,525
Speaker 3:  say they have a sentence on their website, it's like, we've never done this.

282
00:16:43,745 --> 00:16:47,085
Speaker 3:  And then they silently change it, they get rid of that sentence so people

283
00:16:47,085 --> 00:16:50,405
Speaker 3:  know it happened, but even they haven't disclosed anything. Oh wow. Telegrams

284
00:16:50,405 --> 00:16:53,925
Speaker 3:  one is like, we've just never given up any data. Everyone can see all the

285
00:16:53,925 --> 00:16:56,965
Speaker 3:  bad things on our platforms. They wanna catch the bad people and we have

286
00:16:56,965 --> 00:17:00,445
Speaker 3:  made it so you cannot, you can't do it. Right? Like it's so hard to do it.

287
00:17:00,605 --> 00:17:04,525
Speaker 3:  No government has ever managed to do this. And I think that is the

288
00:17:04,595 --> 00:17:07,645
Speaker 3:  liability. Again, I don't really know French law,

289
00:17:09,105 --> 00:17:12,205
Speaker 3:  but I, I know how our government thinks about it. I know how researchers

290
00:17:12,205 --> 00:17:15,005
Speaker 3:  think about it. You know, academics think about it. And once you get to,

291
00:17:15,005 --> 00:17:17,845
Speaker 3:  you know it's happening and you won't even help the cops stop it.

292
00:17:18,985 --> 00:17:22,965
Speaker 3:  You end up with maybe you're responsible for yourself and that is,

293
00:17:22,985 --> 00:17:26,955
Speaker 3:  that's a lot of steps. You gotta get through a lot of steps. I

294
00:17:26,955 --> 00:17:30,355
Speaker 3:  don't think there's any networks in the United States that are anywhere close

295
00:17:30,355 --> 00:17:34,155
Speaker 3:  to that, even though the usual morons are screaming

296
00:17:34,155 --> 00:17:37,515
Speaker 3:  about free speech on Telegram. That's just not how it works here. Like no

297
00:17:37,515 --> 00:17:41,395
Speaker 3:  one is that stupid, but here it feels like there

298
00:17:41,395 --> 00:17:45,275
Speaker 3:  was an such an active effort to frustrate authorities from

299
00:17:45,595 --> 00:17:49,445
Speaker 3:  prosecuting just straightforwardly criminal behavior. The

300
00:17:49,555 --> 00:17:52,965
Speaker 3:  dude landed his PJ in France and got arrested. Yeah, I think we gotta find

301
00:17:52,965 --> 00:17:55,485
Speaker 3:  out the case. We gotta figure again, not an expert in French law. Like there's,

302
00:17:55,485 --> 00:17:59,245
Speaker 3:  there's a whole bunch of stuff, but the, you know, the crimes are

303
00:17:59,245 --> 00:18:03,165
Speaker 3:  happening and you won't even help us stop it. That's where you, you

304
00:18:03,165 --> 00:18:05,605
Speaker 3:  get that collapse of liability too. We're just gonna arrest the CE of the

305
00:18:05,605 --> 00:18:06,485
Speaker 3:  social network. Well,

306
00:18:06,485 --> 00:18:09,925
Speaker 4:  And it goes back to the questions of encryption too, because I think you,

307
00:18:10,025 --> 00:18:13,805
Speaker 4:  you look at something like iMessage, right? Where Apple's

308
00:18:13,965 --> 00:18:17,845
Speaker 4:  stance, right or wrong believe it or don't believe it is privacy is

309
00:18:17,845 --> 00:18:21,125
Speaker 4:  more important than everything else. We can't see it and neither can you,

310
00:18:21,125 --> 00:18:21,365
Speaker 4:  right?

311
00:18:21,595 --> 00:18:23,645
Speaker 3:  Yeah. We don't even know it's happening. Right? You believe it's happening,

312
00:18:23,645 --> 00:18:26,205
Speaker 3:  but there's no way for us to know it's happening. Right?

313
00:18:26,205 --> 00:18:29,805
Speaker 4:  Agree or disagree. That is a stance, that is a whole stance. What

314
00:18:29,805 --> 00:18:33,645
Speaker 4:  Telegram has said is, is the opposite, right? Like all the things you're

315
00:18:33,645 --> 00:18:37,445
Speaker 4:  saying about we can see it and we're doing nothing about it. They've like

316
00:18:37,465 --> 00:18:40,085
Speaker 4:  touted this as part of the point over the years. And one of the things that

317
00:18:40,085 --> 00:18:43,965
Speaker 4:  Paval Durra has said many to times is like, You can't build a,

318
00:18:44,365 --> 00:18:48,045
Speaker 4:  a private, secure, safe place for people to talk except for

319
00:18:48,045 --> 00:18:51,525
Speaker 4:  terrorists. And like, again, bunch of really interesting arguments behind

320
00:18:51,525 --> 00:18:55,485
Speaker 4:  that sentence, but it's not encrypted. And this is the thing that it comes

321
00:18:55,485 --> 00:18:59,325
Speaker 4:  back to, right? Is it would be a different argument if Telegram were an encrypted

322
00:18:59,325 --> 00:19:02,605
Speaker 4:  app on which all of this stuff was kind of loosely known to be happening,

323
00:19:02,605 --> 00:19:06,565
Speaker 4:  but you couldn't see it. But like You can see it. It's just, it's just

324
00:19:06,565 --> 00:19:09,165
Speaker 4:  there, there have been all these researchers in the last few days who are,

325
00:19:09,165 --> 00:19:12,685
Speaker 4:  are coming out and saying like, the the new Defy

326
00:19:13,025 --> 00:19:16,445
Speaker 4:  AI apps is one that you see a lot on on Telegram and

327
00:19:16,885 --> 00:19:20,125
Speaker 4:  Willow Remus at the Washington Post Twitter. Great story about researcher

328
00:19:20,125 --> 00:19:22,445
Speaker 4:  who was like, I just went and found a bunch of 'em just right there. They're

329
00:19:22,445 --> 00:19:26,245
Speaker 4:  just sitting there. It's not like a secret. This stuff is known. And so

330
00:19:26,345 --> 00:19:29,725
Speaker 4:  it seems like that disconnect between, it's just sitting right there.

331
00:19:30,345 --> 00:19:34,045
Speaker 4:  The the evidence is that is in front of us. Like all I have to do is look

332
00:19:34,045 --> 00:19:37,845
Speaker 4:  with my eyes and it's right there. And yet you are willingly

333
00:19:37,875 --> 00:19:41,765
Speaker 4:  obfuscating my ability to do anything about it. That is, I feel like

334
00:19:41,765 --> 00:19:44,725
Speaker 4:  the bridge that I've never seen another company cross this way. Yeah,

335
00:19:44,755 --> 00:19:48,565
Speaker 5:  Well I think there's the, the one example is like kim.com, right? Where he

336
00:19:48,565 --> 00:19:52,045
Speaker 5:  was kind of egregiously ignoring the fact that his,

337
00:19:52,345 --> 00:19:56,085
Speaker 5:  his mega upload site was full of pirated stuff.

338
00:19:56,665 --> 00:19:59,645
Speaker 5:  And then one day they came, the New Zealand authorities came and said, yeah,

339
00:19:59,665 --> 00:20:03,565
Speaker 5:  you can't do that anymore. And now he's being extradited like on his

340
00:20:03,565 --> 00:20:07,285
Speaker 5:  extradition tour every couple of years gets sent to a new country to be

341
00:20:07,285 --> 00:20:10,925
Speaker 5:  extradited, So, I. Think there's like examples of this before.

342
00:20:11,625 --> 00:20:15,465
Speaker 5:  And Pavel, I like, why did this guy come to France?

343
00:20:15,705 --> 00:20:16,545
Speaker 5:  'cause hadn't he said

344
00:20:16,545 --> 00:20:19,505
Speaker 3:  Before, this is the biggest question I have on this. Why land the plane in

345
00:20:19,505 --> 00:20:20,025
Speaker 3:  France? Like

346
00:20:20,025 --> 00:20:23,065
Speaker 5:  Did, he'd said before like, I'm not gonna go to places where they wanna arrest

347
00:20:23,065 --> 00:20:26,425
Speaker 5:  me for this stuff and they hate what I'm doing. France.

348
00:20:26,815 --> 00:20:28,505
Speaker 5:  Like, did he miss the Olympics?

349
00:20:28,835 --> 00:20:31,425
Speaker 3:  Maybe he thought they were just distracted. Like they're coming off the Olympics.

350
00:20:31,455 --> 00:20:33,545
Speaker 3:  He's like, I'm gonna stop and get a baguette and a cigarette, you know, and

351
00:20:33,545 --> 00:20:37,385
Speaker 3:  see how it goes. Two things about kim.com. One, we had the,

352
00:20:37,385 --> 00:20:41,105
Speaker 3:  the best headline in virtual history kim.com mega uploaded to the

353
00:20:41,105 --> 00:20:44,065
Speaker 3:  United States. Oh yeah, it's very good on copyright charges, which people

354
00:20:44,065 --> 00:20:46,465
Speaker 3:  thought was a body shaming joke. And we're like, no, that's the name of the

355
00:20:46,465 --> 00:20:46,625
Speaker 3:  company.

356
00:20:48,785 --> 00:20:51,645
Speaker 3:  I'm sorry I, it was just very funny. That whole sequence of events was very

357
00:20:51,645 --> 00:20:55,445
Speaker 3:  funny. Two copyright law, the only one where we

358
00:20:55,445 --> 00:20:59,405
Speaker 3:  just accept the speech regulation. It is wild to me. You're like, you

359
00:20:59,405 --> 00:21:03,245
Speaker 3:  hired a bunch of Disney movies, jail, the entire international community

360
00:21:03,245 --> 00:21:06,405
Speaker 3:  is gonna put you in jail all together at the same time. Yeah. Everyone's

361
00:21:06,405 --> 00:21:10,045
Speaker 3:  like, huh, that's not a pro. Like Elon Musk is not like justice for kim.com

362
00:21:11,135 --> 00:21:15,085
Speaker 3:  right? Right. And then it's like we're talking about ISIS and, and CS a

363
00:21:15,225 --> 00:21:18,445
Speaker 3:  and censorship. We're, we're having a free competition. And it's like, what

364
00:21:18,445 --> 00:21:22,365
Speaker 3:  are you talking about? So there's just a huge disconnect in how we perceive

365
00:21:22,365 --> 00:21:26,205
Speaker 3:  speech regulations. Even in this case where tub are facing criminal

366
00:21:26,205 --> 00:21:30,125
Speaker 3:  prosecution around the world for, you know, for violating our speech norms.

367
00:21:30,145 --> 00:21:33,965
Speaker 3:  One just happens to be Disney or like Hollywood and that seems to be fine.

368
00:21:34,345 --> 00:21:38,045
Speaker 3:  So that's just weird. Like just on its face, Alex. I agree there's a weirdness

369
00:21:38,045 --> 00:21:40,965
Speaker 3:  there. But the other piece of the puzzle is that even United States where

370
00:21:40,965 --> 00:21:44,325
Speaker 3:  we have strong prohibitions against speech regulations in the First Amendment

371
00:21:44,905 --> 00:21:48,245
Speaker 3:  and things like Section two 30, which insulate

372
00:21:49,005 --> 00:21:52,965
Speaker 3:  platform owners for the speech that happens on our platforms, we make a

373
00:21:52,965 --> 00:21:56,725
Speaker 3:  big exception for criminal behavior on your platform. It's like part of

374
00:21:56,725 --> 00:22:00,565
Speaker 3:  section two 30 is no effect on criminal prosecution.

375
00:22:00,565 --> 00:22:04,405
Speaker 3:  Like it's the heading. It's like you don't get insulated from

376
00:22:04,405 --> 00:22:08,285
Speaker 3:  crimes, right? If the people are doing crimes in your platforms and we

377
00:22:08,285 --> 00:22:11,805
Speaker 3:  can connect it to you, we're going to, you are the crimes that's you

378
00:22:12,215 --> 00:22:16,085
Speaker 3:  fossa and cesta. The, the the, the exception to to two 30

379
00:22:16,635 --> 00:22:20,485
Speaker 3:  that made basically made talking about sex

380
00:22:20,485 --> 00:22:24,445
Speaker 3:  work illegal on these platforms. It was an anti-trafficking law, but it

381
00:22:24,445 --> 00:22:27,965
Speaker 3:  landed with basically sex workers can't do business on platforms anymore.

382
00:22:29,395 --> 00:22:32,925
Speaker 3:  That is a criminal, that's a criminal statute. We're like, you, this is illegal

383
00:22:32,985 --> 00:22:36,525
Speaker 3:  now you'll be responsible for this, you'll be liable for this. And the platforms

384
00:22:36,525 --> 00:22:40,085
Speaker 3:  all took it down and most people are sort of fine with that. There's a

385
00:22:40,495 --> 00:22:43,245
Speaker 3:  large controversy brewing about that that we don't have time to talk about.

386
00:22:43,305 --> 00:22:47,205
Speaker 3:  But that was the tradeoff that was made in that policy here. I think

387
00:22:47,205 --> 00:22:50,925
Speaker 3:  there's just a deep confusion about a

388
00:22:50,925 --> 00:22:54,845
Speaker 3:  French law. No, no one knows, says French law and B like how much do

389
00:22:54,845 --> 00:22:58,125
Speaker 3:  you have to know? When do you have to know it? Is Mark Zuckerberg gonna go

390
00:22:58,125 --> 00:23:02,045
Speaker 3:  to jail? 'cause all of this stuff exists on meta platforms, but

391
00:23:02,045 --> 00:23:05,405
Speaker 3:  they actively try to shut it down. They work with law enforcement. So I was

392
00:23:05,405 --> 00:23:09,295
Speaker 3:  like you in jail if you do slightly less than meta, do you go to

393
00:23:09,295 --> 00:23:13,015
Speaker 3:  jail? If you're Elon Musk, do you go to jail? Right? Like, I, I don't think

394
00:23:13,015 --> 00:23:16,735
Speaker 3:  we understand the gradation. And then on top of that, every country

395
00:23:17,115 --> 00:23:21,015
Speaker 3:  on the planet right now is grappling with how to regulate social media and

396
00:23:21,015 --> 00:23:23,935
Speaker 3:  the bad things that happen on social media. And they're all gonna come to

397
00:23:23,935 --> 00:23:27,775
Speaker 3:  different wildly different answers. And I think all these platform

398
00:23:27,775 --> 00:23:30,095
Speaker 3:  owners are kind of like, okay, this is the tip of the spear. Like this is

399
00:23:30,095 --> 00:23:33,375
Speaker 3:  the beginning of the end, even though

400
00:23:34,085 --> 00:23:37,575
Speaker 3:  it's so out of bounds, even though it's so far few, like no one else is doing

401
00:23:37,575 --> 00:23:38,575
Speaker 3:  a Telegram is doing it this way.

402
00:23:39,035 --> 00:23:42,415
Speaker 5:  Do you guys consider Telegram social media? I always considered it like a

403
00:23:42,445 --> 00:23:43,295
Speaker 5:  chat platform.

404
00:23:44,285 --> 00:23:44,975
Speaker 3:  What is a photo?

405
00:23:45,665 --> 00:23:45,985
Speaker 4:  Yeah, it's,

406
00:23:46,495 --> 00:23:50,385
Speaker 5:  Yeah, I basically wanted to just do that at you. What is a social

407
00:23:50,385 --> 00:23:50,705
Speaker 5:  media

408
00:23:51,185 --> 00:23:55,065
Speaker 4:  Platform? Like it's it's instagramy enough in the

409
00:23:55,065 --> 00:23:58,745
Speaker 4:  way that people use it in some ways that I think, I think it

410
00:23:58,745 --> 00:23:59,705
Speaker 4:  counts. Okay.

411
00:24:00,165 --> 00:24:04,095
Speaker 3:  I'm all the way at like, anything where You can post Yeah.

412
00:24:04,235 --> 00:24:08,215
Speaker 3:  To like, and that post might go to people you don't know is

413
00:24:08,215 --> 00:24:11,985
Speaker 3:  a social media platform. Yeah. Right. And that's a, that's a very

414
00:24:11,985 --> 00:24:14,545
Speaker 3:  loose, so You can, You can find exceptions to that one all over the place.

415
00:24:14,605 --> 00:24:17,345
Speaker 3:  But that's kind of my, my broad definition.

416
00:24:17,765 --> 00:24:19,425
Speaker 5:  Can you post, I I like that. Well

417
00:24:19,425 --> 00:24:21,985
Speaker 3:  Can you post and, and can the person on the other, like, do you have to know

418
00:24:21,985 --> 00:24:25,265
Speaker 3:  the person on the other, other end of it? So like by this definition, like

419
00:24:25,265 --> 00:24:28,105
Speaker 3:  Discord is a social media platform, right? Twitch is a social media platform.

420
00:24:29,185 --> 00:24:32,145
Speaker 3:  Twitch is obviously, it's in my mind obviously a social, YouTube is a social

421
00:24:32,145 --> 00:24:35,545
Speaker 3:  media platform. Yeah. By this definition. Yeah. So I. Think there's, that's

422
00:24:35,545 --> 00:24:39,375
Speaker 3:  a, that's a broad one, but this basic idea of like if you run

423
00:24:39,455 --> 00:24:42,975
Speaker 3:  a platform or, or someone can talk to a lot of strangers, what

424
00:24:42,975 --> 00:24:46,655
Speaker 3:  responsibility do you as a platform owner have? And pretty much the only

425
00:24:46,655 --> 00:24:49,975
Speaker 3:  responsibility we've decided any of these companies truly have

426
00:24:51,135 --> 00:24:54,915
Speaker 3:  is making sure there's not copyright ment. Like is making sure

427
00:24:55,105 --> 00:24:57,915
Speaker 3:  that Hollywood and the music industry are protected. That is pretty much

428
00:24:57,935 --> 00:25:01,705
Speaker 3:  it. Right? And then there's a, there's a litany of, of

429
00:25:01,705 --> 00:25:05,025
Speaker 3:  really bad things that they've decided that they will keep off on their own.

430
00:25:06,445 --> 00:25:09,745
Speaker 3:  And that is it, that's the answer. Like across the world, like that's the

431
00:25:09,905 --> 00:25:13,225
Speaker 3:  baseline answer is copyright infringement and then, and then there's TikTok,

432
00:25:13,225 --> 00:25:14,465
Speaker 3:  which is like, what if we did it anyway?

433
00:25:15,135 --> 00:25:15,425
Speaker 4:  Yeah.

434
00:25:15,645 --> 00:25:19,425
Speaker 3:  But like I would just point out that that always is where

435
00:25:19,465 --> 00:25:21,545
Speaker 3:  these arguments break down. If you're okay with

436
00:25:23,075 --> 00:25:26,925
Speaker 3:  sending Kim come to jail for copyright infringement, you,

437
00:25:26,985 --> 00:25:30,045
Speaker 3:  you, you might have to reconsider how you feel about the other stuff or you

438
00:25:30,045 --> 00:25:32,445
Speaker 3:  might have to reconsider feel about copyright infringement. I just think

439
00:25:32,445 --> 00:25:35,845
Speaker 3:  this stuff is so openly horrible that actively

440
00:25:36,395 --> 00:25:37,925
Speaker 3:  thwarting the authorities

441
00:25:39,465 --> 00:25:41,405
Speaker 3:  should probably light you in some hot water.

442
00:25:41,905 --> 00:25:45,645
Speaker 4:  But I think the, the question of who, who is you in that

443
00:25:46,485 --> 00:25:50,405
Speaker 4:  sentence, right? I think is is doing a lot of work in this question because

444
00:25:50,405 --> 00:25:54,285
Speaker 4:  like what what we see most of the time is companies get in

445
00:25:54,285 --> 00:25:57,805
Speaker 4:  trouble, right? Like companies get fined or

446
00:25:58,675 --> 00:26:02,485
Speaker 4:  everybody, like the government sends things to lawyers and,

447
00:26:02,505 --> 00:26:06,365
Speaker 4:  and it's all this sort of entity to entity thing. And So I think the fact

448
00:26:06,365 --> 00:26:09,685
Speaker 4:  that it was like a a a dude who got arrested for it

449
00:26:10,425 --> 00:26:14,045
Speaker 4:  is so unusual. I mean it's like if, if somebody did a bad tweet and

450
00:26:14,195 --> 00:26:18,125
Speaker 4:  Jack Dorsey got arrested, like back in the day, instead of him going

451
00:26:18,125 --> 00:26:18,285
Speaker 4:  to

452
00:26:18,385 --> 00:26:20,845
Speaker 3:  Do, you know how different our lives would've been if Dorsey could have been

453
00:26:21,045 --> 00:26:21,685
Speaker 3:  arrested for bad tweets?

454
00:26:21,685 --> 00:26:23,525
Speaker 4:  This is what I mean. And I think it, it's it's,

455
00:26:23,525 --> 00:26:24,445
Speaker 3:  We live in a utopia.

456
00:26:24,565 --> 00:26:28,445
Speaker 4:  I I think if you, if you like there's that kidding

457
00:26:28,505 --> 00:26:30,045
Speaker 4:  the jails would just be full of Jack Dorsey,

458
00:26:31,585 --> 00:26:31,805
Speaker 3:  But,

459
00:26:33,105 --> 00:26:33,445
Speaker 4:  But I think

460
00:26:33,515 --> 00:26:34,605
Speaker 5:  He'd be on this extradition tour.

461
00:26:35,115 --> 00:26:37,925
Speaker 3:  Yeah, exactly. Just send him around the world from jail to jail around the

462
00:26:37,925 --> 00:26:38,005
Speaker 3:  world.

463
00:26:39,145 --> 00:26:42,685
Speaker 4:  And I think if you, if you cast this out and in a certain way that it goes

464
00:26:42,685 --> 00:26:46,125
Speaker 4:  like, you know, we talk about chilling effects of laws all the time. Like

465
00:26:46,125 --> 00:26:49,605
Speaker 4:  yeah, how are you gonna feel right now if you're a CEO of a company that

466
00:26:49,625 --> 00:26:53,205
Speaker 4:  is dealing with any of this, you're like, oh my God. Suddenly not only is

467
00:26:53,205 --> 00:26:56,645
Speaker 4:  my company at risk, but literally I personally could go to jail man. Which

468
00:26:56,685 --> 00:27:00,045
Speaker 4:  I guarantee you is not something most people are thinking about. Yeah.

469
00:27:00,065 --> 00:27:03,285
Speaker 3:  My access to high quality wine and cheese is at risk today.

470
00:27:03,725 --> 00:27:04,245
Speaker 4:  Yeah, exactly.

471
00:27:04,585 --> 00:27:05,685
Speaker 3:  We cannot go to France.

472
00:27:06,125 --> 00:27:09,645
Speaker 5:  I was like, who are tho who are those CEOs? It's like Mark Zuckerberg. What?

473
00:27:09,835 --> 00:27:11,165
Speaker 5:  Elon Musk? They're

474
00:27:11,165 --> 00:27:13,525
Speaker 3:  Fine. Well it's, it's a lot of them, right? Like, you know, what

475
00:27:13,525 --> 00:27:14,685
Speaker 4:  If Andy Jassy could go to jail

476
00:27:15,035 --> 00:27:15,765
Speaker 3:  That something

477
00:27:15,965 --> 00:27:16,365
Speaker 4:  Happened on Twich,

478
00:27:17,585 --> 00:27:19,965
Speaker 3:  But well all of like, yes you

479
00:27:20,045 --> 00:27:21,565
Speaker 5:  Should for Amazon Kindle app you

480
00:27:21,565 --> 00:27:25,485
Speaker 3:  Should likes should go to jail for what happens on YouTube is like a

481
00:27:25,485 --> 00:27:29,445
Speaker 3:  pretty weird jump, right? Yeah. At the same time, all of

482
00:27:29,445 --> 00:27:33,085
Speaker 3:  those companies actively cooperate with law enforcement all the time.

483
00:27:33,265 --> 00:27:36,965
Speaker 3:  For sure. In ways, in ways good and bad. I think on our lightning round list

484
00:27:36,965 --> 00:27:40,845
Speaker 3:  is like Mark Zuckerberg sending a letter to Jim Jordan in Congress

485
00:27:40,845 --> 00:27:44,085
Speaker 3:  this week saying, I, we caved to the Biden administration too much. I think

486
00:27:44,085 --> 00:27:48,005
Speaker 3:  that letter is like political theater, but like, that's what I mean by active

487
00:27:48,005 --> 00:27:50,805
Speaker 3:  cooperation. The governor on both sides, he's saying we cooperated with the

488
00:27:50,805 --> 00:27:53,845
Speaker 3:  Biden administration and now he's cooperating with Jim Jordan saying we cooperated

489
00:27:53,845 --> 00:27:57,765
Speaker 3:  with the Biden administration. Like the, the big companies

490
00:27:58,225 --> 00:28:02,165
Speaker 3:  in this country at least are actively engaged with various governments

491
00:28:02,165 --> 00:28:05,725
Speaker 3:  around the world in various ways. And I think that insulates them

492
00:28:06,275 --> 00:28:09,965
Speaker 3:  from Mark Zuckerberg getting arrested in France. Here you just have this

493
00:28:09,965 --> 00:28:13,885
Speaker 3:  other thing. It is so different like architecturally and how the

494
00:28:13,885 --> 00:28:17,565
Speaker 3:  app works user experience wise and you know, the the one to

495
00:28:17,565 --> 00:28:21,445
Speaker 3:  200,000 kind of opportunities you have moderation wise. And that

496
00:28:21,445 --> 00:28:24,725
Speaker 3:  they do literally nothing with the worst content anyone can possibly imagine.

497
00:28:25,345 --> 00:28:29,245
Speaker 3:  And then policy wise in that they have structured it to

498
00:28:29,245 --> 00:28:31,365
Speaker 3:  make it impossible for the authorities to even arrest the people they can

499
00:28:31,365 --> 00:28:34,005
Speaker 3:  see with their own eyes, right? Yeah. And all of that and

500
00:28:34,015 --> 00:28:36,525
Speaker 4:  Kinda is the middle finger to them in the process, right?

501
00:28:36,525 --> 00:28:38,885
Speaker 3:  And you just, you just like stack all that up. You're like, that is a lot

502
00:28:38,885 --> 00:28:40,805
Speaker 3:  of decisions that land you in French jail.

503
00:28:42,325 --> 00:28:46,205
Speaker 5:  I I wonder how it's gonna affect Elon 'cause he'd really been trying to take

504
00:28:46,665 --> 00:28:50,365
Speaker 5:  the same path with x that Davs been doing of just being like

505
00:28:50,665 --> 00:28:53,245
Speaker 5:  the free speech absolution. Is that how you say it?

506
00:28:53,265 --> 00:28:56,285
Speaker 3:  Oh, I, I completely disagree with that. I mean, I understand the argument.

507
00:28:56,285 --> 00:28:57,965
Speaker 3:  That's what he wants us all would think, but

508
00:28:57,965 --> 00:29:00,285
Speaker 5:  He's just a full, he always like, ah, it's, it's all about free speech. And

509
00:29:00,345 --> 00:29:03,765
Speaker 5:  he fundamentally is very different in how he actually does that. 'cause he's

510
00:29:03,765 --> 00:29:07,285
Speaker 5:  always like, oh, you want me to take that down? Done. But

511
00:29:07,465 --> 00:29:10,705
Speaker 3:  Like, yeah, I mean his, his definition of free speech and we can You can

512
00:29:10,705 --> 00:29:14,585
Speaker 3:  go look at the tweet is if

513
00:29:14,585 --> 00:29:17,905
Speaker 3:  people want speech regulations, they should pass laws and that's how democracy

514
00:29:18,005 --> 00:29:21,135
Speaker 3:  is supposed to work. And I will follow with the law. And you're like, you're,

515
00:29:21,235 --> 00:29:25,055
Speaker 3:  that's the weirdest definition of free speech that exists because you're

516
00:29:25,055 --> 00:29:27,015
Speaker 3:  asking for government speech regulations, right.

517
00:29:27,035 --> 00:29:30,855
Speaker 4:  And what they have followed historically has been draconian speech laws

518
00:29:30,855 --> 00:29:31,535
Speaker 4:  in other countries.

519
00:29:32,045 --> 00:29:35,415
Speaker 3:  Yeah. Right? Like India has draconian speech laws X complies with them, right?

520
00:29:35,435 --> 00:29:39,015
Speaker 3:  Brazil has new draconian speech laws and he doesn't like the government,

521
00:29:39,015 --> 00:29:42,805
Speaker 3:  so he won't, it's like, it's this weird balance of like,

522
00:29:43,165 --> 00:29:47,085
Speaker 3:  actually I'll do whatever I want, but like if I don't want to get

523
00:29:47,085 --> 00:29:49,565
Speaker 3:  into a fight with the government or I like the government there, I'll do

524
00:29:49,565 --> 00:29:53,245
Speaker 3:  what they say, which is just not a position. Right. That is just pure

525
00:29:53,725 --> 00:29:56,365
Speaker 3:  opportunism. Yeah. And I think the noise around

526
00:29:57,585 --> 00:30:01,125
Speaker 3:  dur of getting arrested in France is like useful for him and his

527
00:30:01,635 --> 00:30:05,605
Speaker 3:  ongoing quest to be perceived as a free speech martyr. But I,

528
00:30:05,985 --> 00:30:09,285
Speaker 3:  if you look at his posts lately, You can tell he is like in an

529
00:30:09,285 --> 00:30:12,965
Speaker 3:  intellectual spiral because his position has no center,

530
00:30:13,175 --> 00:30:16,085
Speaker 3:  right? There's no, there's actually no ideological commitment to anything.

531
00:30:16,545 --> 00:30:19,325
Speaker 3:  So if you're like, I would wish to deify the government, but I might get

532
00:30:19,565 --> 00:30:22,685
Speaker 3:  arrested also, some of this stuff is horrible. Also, I'm still like, I'm

533
00:30:22,695 --> 00:30:25,765
Speaker 3:  suing advertisers for antitrust. There's nothing there.

534
00:30:26,635 --> 00:30:30,245
Speaker 3:  Like there, there's literally no center, there's no intellectual center to

535
00:30:30,245 --> 00:30:33,845
Speaker 3:  that position. And You can kind of see him be like, oh crap, this is what

536
00:30:33,845 --> 00:30:37,485
Speaker 3:  free speech might actually mean. I don't wanna, I don't wanna over

537
00:30:37,485 --> 00:30:41,365
Speaker 3:  intellectualize Elon's, you know, ketamine musings or

538
00:30:41,445 --> 00:30:44,805
Speaker 3:  whatever the fuck it is. But it's like You can, You can see it's starting

539
00:30:44,835 --> 00:30:47,205
Speaker 3:  like the pressure welcome to hell is basically what I'm saying. Like yeah,

540
00:30:47,315 --> 00:30:51,045
Speaker 3:  yeah. The internal contradictions of owning a platform like X and all of

541
00:30:51,045 --> 00:30:53,845
Speaker 3:  the demands that are placed on that. Right. I think it's it's starting to

542
00:30:53,845 --> 00:30:55,365
Speaker 3:  wear on 'em a little bit. Yeah.

543
00:30:55,515 --> 00:30:59,045
Speaker 4:  Yeah. I mean, and I think we should probably move on from this, but the,

544
00:30:59,065 --> 00:31:02,845
Speaker 4:  the thing that keeps coming up to me as I, I read these stories and talk

545
00:31:02,845 --> 00:31:05,285
Speaker 4:  to people about this, there's been a ton of good reporting on this by the

546
00:31:05,285 --> 00:31:09,085
Speaker 4:  way. Good, good week for the journalists out there is

547
00:31:09,265 --> 00:31:13,165
Speaker 4:  how much that the, the sort of legal push against

548
00:31:13,165 --> 00:31:16,605
Speaker 4:  the tech industry is still going along the lines of we have to protect the

549
00:31:16,685 --> 00:31:20,645
Speaker 4:  children. Like so much of this is about the CS a m stuff

550
00:31:20,645 --> 00:31:23,605
Speaker 4:  that's happening on Telegram and the laws that have been passed recently.

551
00:31:24,605 --> 00:31:28,085
Speaker 4:  I think it was, I think it was Britain that re that passed a law

552
00:31:28,635 --> 00:31:32,565
Speaker 4:  last year that explicitly allows the executives

553
00:31:32,685 --> 00:31:35,485
Speaker 4:  of tech companies to be held personally responsible for

554
00:31:36,465 --> 00:31:40,045
Speaker 4:  CS a m stuff and child safety stuff on their platform in general if they're

555
00:31:40,555 --> 00:31:43,845
Speaker 4:  told about it and don't do anything about it. Like that's, that's now a law

556
00:31:43,845 --> 00:31:47,765
Speaker 4:  that exists. And, and this idea of we have to protect

557
00:31:47,925 --> 00:31:51,885
Speaker 4:  children being like the most bipartisan thing you'll find anywhere on

558
00:31:51,985 --> 00:31:55,325
Speaker 4:  any line of poli political reasoning.

559
00:31:56,105 --> 00:31:59,365
Speaker 4:  That's how we're getting towards all of this. And it's just, it's just fascinating

560
00:31:59,365 --> 00:32:02,445
Speaker 4:  that that continues to be the thing that is like, the deeper this goes, the

561
00:32:02,445 --> 00:32:03,405
Speaker 4:  more it's about child safety.

562
00:32:03,625 --> 00:32:06,765
Speaker 5:  That's, that's always the argument for censorship or like, that's always

563
00:32:06,765 --> 00:32:09,965
Speaker 5:  like the first line of arguments for censorship is we must think of the children.

564
00:32:10,195 --> 00:32:13,125
Speaker 3:  Well no, it's Hollywood first and then child safety. Yeah.

565
00:32:13,125 --> 00:32:13,565
Speaker 5:  It's it's be

566
00:32:13,565 --> 00:32:15,645
Speaker 3:  Right. Like it's never, it's never not Mickey Mouse first.

567
00:32:15,925 --> 00:32:19,365
Speaker 5:  Yeah. Mickey Mouse and then child safety almost always. Like, there's a great

568
00:32:19,365 --> 00:32:23,045
Speaker 5:  Simpsons episode about that exact thing and then all of the big fights in

569
00:32:23,045 --> 00:32:26,885
Speaker 5:  the the nineties over TV ratings and stuff like that where you

570
00:32:26,885 --> 00:32:30,565
Speaker 5:  had like Clinton Yeah. And then all these Republicans getting together being

571
00:32:30,565 --> 00:32:32,205
Speaker 5:  like, won't anyone think of these babies?

572
00:32:32,455 --> 00:32:35,605
Speaker 4:  Right. You can get anything done if You can convince people. It's about keeping

573
00:32:35,845 --> 00:32:36,245
Speaker 4:  children safe.

574
00:32:36,575 --> 00:32:40,245
Speaker 3:  Right. And I, I agree with you and you know, tipper Gore

575
00:32:41,145 --> 00:32:44,365
Speaker 3:  put the explicit lyrics label on all the CDs and

576
00:32:45,115 --> 00:32:48,645
Speaker 3:  Gore, tipper Gore, very weird. Tipper Gore versus

577
00:32:48,805 --> 00:32:51,845
Speaker 3:  NWA. Just imagine how far we've come.

578
00:32:53,035 --> 00:32:57,005
Speaker 3:  It's super weird. But the the reason that you do

579
00:32:57,285 --> 00:33:01,205
Speaker 3:  that is if You can make the counter argument seem morally

580
00:33:01,205 --> 00:33:05,085
Speaker 3:  indefensible you, you win. Right? Right. And but here I just

581
00:33:05,085 --> 00:33:08,805
Speaker 3:  wanna come back to it is really there. It it, oh yeah. The the, the

582
00:33:08,865 --> 00:33:12,725
Speaker 3:  CS a m is there, it's very bad. And so like sometimes

583
00:33:13,665 --> 00:33:17,245
Speaker 3:  you just have to say it's there and it's bad and you have to do something

584
00:33:17,245 --> 00:33:21,125
Speaker 3:  about it. Right. And I that's, I if you've listened to this show or

585
00:33:21,155 --> 00:33:25,045
Speaker 3:  decoder or list read us for years, like the free speech debate online

586
00:33:25,305 --> 00:33:28,525
Speaker 3:  has raged and we have struggled with it and we've done endless episodes about

587
00:33:28,525 --> 00:33:32,425
Speaker 3:  it, but like, you need an ideological center. Right. That's what I'm saying.

588
00:33:33,285 --> 00:33:37,185
Speaker 3:  and it's real, it's there like ISIS is there, like you don't

589
00:33:37,185 --> 00:33:41,065
Speaker 3:  have to overthink it, you just have to make it go away and hold the people

590
00:33:41,065 --> 00:33:44,665
Speaker 3:  responsible for it accountable. And I think here it's like, well that's an

591
00:33:44,665 --> 00:33:48,345
Speaker 3:  easy line. There are lots of gray areas and lots of much harder lines. That

592
00:33:48,345 --> 00:33:51,185
Speaker 3:  one feels pretty easy to me. Yeah. Alright, we should wrap it up. I promise

593
00:33:51,185 --> 00:33:55,145
Speaker 3:  we'll talk about doing other more fun crimes the next session we'll be right

594
00:33:55,145 --> 00:33:55,345
Speaker 3:  back.

595
00:37:50,675 --> 00:37:51,395
Speaker 3:  I was gonna are these,

596
00:37:51,545 --> 00:37:52,875
Speaker 11:  They're they're less

597
00:37:53,275 --> 00:37:57,155
Speaker 3:  Bummer crimes. I don't they're more fun crimes. I would say that

598
00:37:57,155 --> 00:38:00,915
Speaker 3:  not about whimsical crimes in episode. The, the, the grok

599
00:38:01,015 --> 00:38:04,315
Speaker 3:  AI being so out of control that You can just make like

600
00:38:04,905 --> 00:38:08,795
Speaker 3:  Hillary Clinton do. GTA is legitimately very funny

601
00:38:08,795 --> 00:38:12,715
Speaker 3:  to me. Okay, yeah, fair enough. I'll give you that. Like I

602
00:38:12,715 --> 00:38:16,355
Speaker 3:  have a lot of deep reservations about deep fakes and what is a photo and

603
00:38:16,355 --> 00:38:19,955
Speaker 3:  lies on the internet. And then I'm like, it's also hilarious to have Elon

604
00:38:19,975 --> 00:38:23,475
Speaker 3:  Moscow on an ayahuasca journey at the end of it, have his deep fake character

605
00:38:23,495 --> 00:38:26,795
Speaker 3:  say, I'm starting in soup kitchen, which is a real video I watched today.

606
00:38:31,395 --> 00:38:31,915
Speaker 3:  s like, oh

607
00:38:34,965 --> 00:38:35,125
Speaker 3:  you,

608
00:38:36,895 --> 00:38:40,545
Speaker 3:  it's all very good with an Elon voice too. Yes.

609
00:38:40,725 --> 00:38:44,465
Speaker 3:  All of it's great. Don't look at it, respect the truth.

610
00:38:44,815 --> 00:38:46,425
Speaker 3:  Photography is sast.

611
00:38:48,225 --> 00:38:51,865
Speaker 3:  I laughed. I did law. Let's start with the, the big one, David, you, you

612
00:38:51,865 --> 00:38:55,345
Speaker 3:  said this is the story of segment two. It's Yelp sues Google for antitrust

613
00:38:55,345 --> 00:38:55,985
Speaker 3:  violations.

614
00:38:56,645 --> 00:39:00,465
Speaker 4:  So, okay. Did you guys think that Yelp had already sued

615
00:39:00,465 --> 00:39:00,665
Speaker 4:  Google?

616
00:39:01,305 --> 00:39:01,545
Speaker 3:  Because

617
00:39:01,625 --> 00:39:04,945
Speaker 4:  I definitely thought Yelp sued Google like a long time ago. Yeah,

618
00:39:04,945 --> 00:39:08,185
Speaker 3:  Yeah. Yelp at this point is a front for suing Google, right? Is that not

619
00:39:08,185 --> 00:39:08,305
Speaker 3:  how it

620
00:39:08,305 --> 00:39:11,345
Speaker 4:  Basically, it's, I think everyone who works at Yelp is just lawyers now.

621
00:39:12,045 --> 00:39:15,865
Speaker 4:  Yes. But anyway, so very recently, earlier this summer, a couple weeks

622
00:39:15,885 --> 00:39:19,705
Speaker 4:  ago Google lost its antitrust search

623
00:39:19,705 --> 00:39:23,385
Speaker 4:  trial. We're still in the remedies phase and then there will be appeals and

624
00:39:23,385 --> 00:39:26,585
Speaker 4:  then God only knows. But for now it lost. And so Yelp

625
00:39:27,175 --> 00:39:30,945
Speaker 4:  clearly emboldened by that fact filed a suit of its own

626
00:39:31,855 --> 00:39:35,665
Speaker 4:  that basically brings up something that got thrown out

627
00:39:35,665 --> 00:39:39,105
Speaker 4:  of the last case against Google, which I find really interesting. And

628
00:39:39,105 --> 00:39:43,065
Speaker 4:  essentially the argument is that Google prioritizes

629
00:39:43,245 --> 00:39:46,505
Speaker 4:  its own stuff in Google search results and thus

630
00:39:46,615 --> 00:39:50,345
Speaker 4:  Deprioritizes Yelp and TripAdvisor and other companies like it and has

631
00:39:50,345 --> 00:39:51,265
Speaker 4:  thus killed them.

632
00:39:52,895 --> 00:39:56,025
Speaker 4:  This is an argument we've been hearing forever. This is an argument Yelp

633
00:39:56,025 --> 00:39:59,985
Speaker 4:  has been making loudly forever. Literally. I assumed Yelp had already

634
00:40:00,055 --> 00:40:03,305
Speaker 4:  sued Google. Apparently it hadn't. And now

635
00:40:04,015 --> 00:40:07,825
Speaker 4:  what Yelp seems to be thinking like Jeremy Stoppelman, the CEO said

636
00:40:08,645 --> 00:40:12,585
Speaker 4:  in as many words, the wins on antitrust have shifted dramatically. So

637
00:40:12,585 --> 00:40:16,265
Speaker 4:  this company clearly senses this is the moment to finally

638
00:40:16,535 --> 00:40:20,025
Speaker 4:  pick the fight. It's been wanting to pick for many years, which I think is

639
00:40:20,025 --> 00:40:23,705
Speaker 4:  very odd given that that particular fight has been

640
00:40:23,805 --> 00:40:27,745
Speaker 4:  thrown out of court in the same search trial that Google just

641
00:40:27,815 --> 00:40:30,945
Speaker 4:  lost. So I'm confused, but also I think it's fascinating. And

642
00:40:31,765 --> 00:40:35,425
Speaker 4:  yet again, like Google is just up against it, man.

643
00:40:35,495 --> 00:40:39,105
Speaker 4:  Like the, the epic stuff is happening. The search stuff is happening. This

644
00:40:39,105 --> 00:40:41,985
Speaker 4:  is happen. Like it's just, it just keeps happening for Google.

645
00:40:42,095 --> 00:40:46,065
Speaker 3:  There's an ad tech trial. Yeah. I I say that like people know it is Google

646
00:40:46,165 --> 00:40:49,985
Speaker 3:  has yet another antitrust trial coming up over its ad tech stack,

647
00:40:50,515 --> 00:40:51,505
Speaker 3:  which is the money.

648
00:40:51,815 --> 00:40:55,745
Speaker 5:  Yeah. I mean, wasn't this like Google strategy? They would spend

649
00:40:55,745 --> 00:40:59,525
Speaker 5:  20 years fucking around and then now

650
00:40:59,525 --> 00:41:02,005
Speaker 5:  they're in their find out phase and that's why they have all the lawyers

651
00:41:02,555 --> 00:41:04,045
Speaker 5:  that they pay a lot of money to.

652
00:41:04,395 --> 00:41:08,365
Speaker 3:  Yeah, I mean there's something I I I agree with Jeremy Stoppelman and

653
00:41:08,385 --> 00:41:12,085
Speaker 3:  the winds of antitrust have definitely changed. I actually can point to a

654
00:41:12,085 --> 00:41:15,805
Speaker 3:  very, very odd example of this. Luther Lowe,

655
00:41:15,865 --> 00:41:19,605
Speaker 3:  who is Yelp's old policy person recently left

656
00:41:19,765 --> 00:41:22,725
Speaker 3:  Yelp. He's doing all this stuff. He, I think he's a Y Combinator now, and

657
00:41:22,725 --> 00:41:26,685
Speaker 3:  they did like a meet and greet in dc you know, like a little thing.

658
00:41:27,105 --> 00:41:30,965
Speaker 3:  And JD Vance before he was vice presidential candidate, showed up at the

659
00:41:30,965 --> 00:41:33,885
Speaker 3:  meet and greet with all the startups, talk about how he used to be an ad

660
00:41:33,885 --> 00:41:37,005
Speaker 3:  tech and he thought big tech was censoring everyone. And the woke mob was

661
00:41:37,005 --> 00:41:39,725
Speaker 3:  outta control. And then in the middle of all that nonsense, he was like,

662
00:41:39,725 --> 00:41:43,645
Speaker 3:  I think Lena Khan's doing a good job. Yes. Weird. Like weird, right?

663
00:41:43,645 --> 00:41:47,245
Speaker 3:  Like that's how much the wind of antitrust has changed. He's like, the only

664
00:41:47,245 --> 00:41:50,125
Speaker 3:  person that Biden administration is doing a good job is Lena Khan because

665
00:41:50,125 --> 00:41:53,485
Speaker 3:  we should break up. And, and, and really his point was like, because this

666
00:41:53,485 --> 00:41:56,645
Speaker 3:  woke will, should is outta control. And like maybe that doesn't all put together

667
00:41:56,645 --> 00:41:59,165
Speaker 3:  like that puzzle, you put all the pieces together, you're like, that doesn't

668
00:41:59,165 --> 00:42:02,445
Speaker 3:  look like anything. But he still said it, right? Like that's the, you got

669
00:42:02,445 --> 00:42:05,165
Speaker 3:  the spirit. That's where mindset, the mindset is, is these companies have

670
00:42:05,165 --> 00:42:08,445
Speaker 3:  too much control and we should take that control away from them and do something

671
00:42:08,445 --> 00:42:12,365
Speaker 3:  else. What the something else is. I think all of us are very

672
00:42:12,365 --> 00:42:16,325
Speaker 3:  curious to find out. It's clearly going to be something else. But you look

673
00:42:16,325 --> 00:42:20,165
Speaker 3:  at Google, which is just taking it on the chin lately,

674
00:42:20,165 --> 00:42:23,605
Speaker 3:  right? In the search case, in the epic case, they lost there's, and that

675
00:42:23,625 --> 00:42:27,605
Speaker 3:  the judge in that case has been very loud like, you did it, I'm gonna do

676
00:42:27,605 --> 00:42:31,325
Speaker 3:  something about it. Yeah. And then the ad tech case, which I, you know, I,

677
00:42:31,345 --> 00:42:35,005
Speaker 3:  we don't know what's gonna happen. Every trial is kind of a coin flip, but

678
00:42:35,005 --> 00:42:38,915
Speaker 3:  we're gonna get a documents about Google's money in that case. Like it's

679
00:42:38,915 --> 00:42:41,715
Speaker 3:  gonna, some stuff is gonna come out because it's gonna be the business side

680
00:42:41,715 --> 00:42:45,275
Speaker 3:  of Google talking about the money. Yeah. And I, I think Google's image is

681
00:42:45,275 --> 00:42:48,915
Speaker 3:  very cuddly as though the money just appears from nowhere. And that's not

682
00:42:48,915 --> 00:42:49,075
Speaker 3:  true.

683
00:42:49,495 --> 00:42:53,355
Speaker 4:  And that's also the trial. Let's not forget that Google wrote a, just without

684
00:42:53,575 --> 00:42:56,355
Speaker 4:  any questions, just wrote a check for the maximum possible this, oh my God.

685
00:42:56,375 --> 00:42:56,795
Speaker 3:  God, I forgot

686
00:42:57,175 --> 00:43:01,075
Speaker 4:  To avoid having a jury trial like innocent,

687
00:43:01,075 --> 00:43:03,875
Speaker 4:  you know, innocent until proving guilty. But let's a tough look. Can we just

688
00:43:03,875 --> 00:43:07,275
Speaker 3:  Sit on for one second? This is a real thing. We have a picture of the check

689
00:43:07,275 --> 00:43:10,995
Speaker 3:  on the website. I believe they just wrote a check to the

690
00:43:10,995 --> 00:43:14,795
Speaker 3:  Department of Justice for the maximum, like a cashier's check. Yeah. I believe

691
00:43:14,795 --> 00:43:18,675
Speaker 3:  it's drawn on Wells Fargo and they're like, here's all of the money

692
00:43:18,695 --> 00:43:21,675
Speaker 3:  we could possibly owe you. We would like to settle the case like out of the

693
00:43:21,675 --> 00:43:24,315
Speaker 3:  blue. Yeah, it's incredible

694
00:43:24,665 --> 00:43:25,875
Speaker 4:  Just to not have a jury trial

695
00:43:26,845 --> 00:43:30,825
Speaker 5:  And I mean, it's at Wells Fargo, so it'll take about what, 12 years before

696
00:43:30,825 --> 00:43:31,505
Speaker 5:  it gets cashed

697
00:43:33,445 --> 00:43:35,185
Speaker 3:  The blockchain. I gotta double check solves this Alex,

698
00:43:35,525 --> 00:43:36,385
Speaker 5:  Oh, sorry, it's sorry.

699
00:43:37,015 --> 00:43:40,505
Speaker 3:  Have you heard of Lightning? Okay, so back to Yelp.

700
00:43:41,265 --> 00:43:44,385
Speaker 3:  I think the winds have changed is an opportunity

701
00:43:45,165 --> 00:43:48,985
Speaker 3:  for maybe not to, you know, win like epic, right? Yelp doesn't have

702
00:43:49,425 --> 00:43:52,145
Speaker 3:  a Fortnite in the background to fund all this stuff,

703
00:43:53,205 --> 00:43:56,905
Speaker 3:  but I think maybe to claw out a settlement outta Google Yeah. To, to somehow

704
00:43:56,965 --> 00:44:00,345
Speaker 3:  put some pressure on Google to open up and make this go away to appease the

705
00:44:00,545 --> 00:44:04,065
Speaker 3:  Europeans in some way. Whatever you think of Google's relationship, the internet,

706
00:44:04,065 --> 00:44:07,145
Speaker 3:  it is changing. And I think that as we've said a million times, that means

707
00:44:07,145 --> 00:44:07,985
Speaker 3:  the Internet's gonna change.

708
00:44:08,405 --> 00:44:12,385
Speaker 4:  So you think Yelp wants to be, to Google what, like

709
00:44:12,405 --> 00:44:16,065
Speaker 4:  the Delta emulator folks have been to Apple? Yes. Which is just,

710
00:44:16,415 --> 00:44:19,705
Speaker 4:  just sort of the one in there, just like prying it open slightly

711
00:44:21,225 --> 00:44:24,565
Speaker 4:  and they're in a position of weakness and gonna have to start to make policy

712
00:44:24,565 --> 00:44:27,405
Speaker 4:  changes that are good for you, but also good for everybody. And so Yelp is

713
00:44:27,405 --> 00:44:30,685
Speaker 4:  like, we don't, we don't wanna break up Google, we just, we just think this

714
00:44:30,685 --> 00:44:34,645
Speaker 4:  is a chance to like extract some of what we want from the company because

715
00:44:34,645 --> 00:44:36,165
Speaker 4:  they're gonna decide it's easier to do that.

716
00:44:36,635 --> 00:44:39,685
Speaker 5:  It's kind of necessary for Yelp at this point because

717
00:44:40,955 --> 00:44:44,445
Speaker 5:  they have been struggling for a while, right? Like they, they, they have

718
00:44:44,445 --> 00:44:48,005
Speaker 5:  resorted to basically bullying restaurants and stuff and making sure that

719
00:44:48,005 --> 00:44:50,445
Speaker 5:  they're on their, their platform and everything. Here's

720
00:44:50,445 --> 00:44:54,085
Speaker 3:  What we can do. We can shake down millions of local restaurants or we can

721
00:44:54,085 --> 00:44:56,125
Speaker 3:  get a check from Google, which is Yeah.

722
00:44:56,395 --> 00:44:59,125
Speaker 5:  Yeah. I mean it kind of feels like what it's for you, y'all,

723
00:44:59,315 --> 00:45:02,645
Speaker 4:  There's so many companies You can describe with that sentence. Good god.

724
00:45:03,115 --> 00:45:07,085
Speaker 3:  Well, so there was somebody who was on thread say, who posted that? I, I've

725
00:45:07,085 --> 00:45:11,005
Speaker 3:  been more cantankerous lately. Can I, can I just describe my Joker

726
00:45:11,005 --> 00:45:14,945
Speaker 3:  moment broadly? Yes. Because I love that these companies are

727
00:45:14,945 --> 00:45:17,265
Speaker 3:  having these fights. I love that the Internet's gonna be potentially more

728
00:45:17,265 --> 00:45:19,705
Speaker 3:  open. I love that. At least we're in the season of change. All that's great.

729
00:45:20,385 --> 00:45:24,145
Speaker 3:  I do not for one second believe any of this is truly idealistic, right? I

730
00:45:24,145 --> 00:45:28,025
Speaker 3:  think this is a version of capitalism at play where a lot of

731
00:45:28,025 --> 00:45:31,105
Speaker 3:  very self-interested parties are looking out for their own self-interest

732
00:45:31,105 --> 00:45:34,945
Speaker 3:  and they're colliding in weird ways. Great. The reason I say it that way

733
00:45:35,405 --> 00:45:38,545
Speaker 3:  is because a long time ago I used to cover net neutrality like every other

734
00:45:38,545 --> 00:45:42,145
Speaker 3:  day. And the big player in net neutrality was Netflix,

735
00:45:42,525 --> 00:45:46,345
Speaker 3:  if you'll recall. They, Netflix and Reddit

736
00:45:46,855 --> 00:45:50,425
Speaker 3:  were like out in front, like black out the internet, fight for net neutrality.

737
00:45:50,445 --> 00:45:53,585
Speaker 3:  We don't let Comcast throttle us disclosure, Comcast is an investor in this

738
00:45:53,585 --> 00:45:56,625
Speaker 3:  company and boy did they not like my net neutrality coverage,

739
00:45:57,815 --> 00:46:00,305
Speaker 3:  just putting it all out there. And then one year

740
00:46:01,945 --> 00:46:04,425
Speaker 3:  Reed Hastings on stage of the code conference, So I believe Peter Kafka.

741
00:46:04,565 --> 00:46:07,105
Speaker 3:  And Peter said, you, you've been really quiet about net neutrality lately.

742
00:46:07,405 --> 00:46:09,665
Speaker 3:  And Reid Hastings looked at him and said, yeah, we're so big it doesn't matter

743
00:46:09,665 --> 00:46:11,905
Speaker 3:  anymore. And I turned into the joker.

744
00:46:12,305 --> 00:46:13,505
Speaker 4:  That's honest. Wow.

745
00:46:14,605 --> 00:46:18,505
Speaker 3:  And like, fine. Right? Like fine. And

746
00:46:18,505 --> 00:46:22,065
Speaker 3:  So I like the idea that Yelp is a idealistic freedom fighter or even Delta,

747
00:46:22,255 --> 00:46:25,585
Speaker 3:  even though it's a smaller company, I do think they're pressing their regulatory

748
00:46:25,585 --> 00:46:28,905
Speaker 3:  advantage to open up these platforms. Oh yeah. But I don't think that like

749
00:46:28,905 --> 00:46:32,545
Speaker 3:  they're the, they're gonna get theirs, right? And then yeah, all bets are

750
00:46:32,545 --> 00:46:35,785
Speaker 3:  off. And I, I, I think all of this change is like, who's gonna get theirs

751
00:46:35,885 --> 00:46:38,545
Speaker 3:  at the end of this? And so like, we're gonna shake down a bunch of restaurants

752
00:46:38,545 --> 00:46:42,185
Speaker 3:  or shake down Google is kind of the best, in my mind, like the best way to

753
00:46:42,185 --> 00:46:45,185
Speaker 3:  understand it. Like everyone's just trying to get paid whether or not the

754
00:46:45,185 --> 00:46:48,145
Speaker 3:  legal system can shift the money around in a more equitable way. At the end

755
00:46:48,145 --> 00:46:51,985
Speaker 3:  of all this totally remains to be seen. I don't, I truly

756
00:46:52,005 --> 00:46:52,465
Speaker 3:  do not know.

757
00:46:52,685 --> 00:46:55,745
Speaker 4:  In a funny way, what you're arguing for is capitalism working as intended.

758
00:46:55,895 --> 00:46:56,185
Speaker 4:  Yeah.

759
00:46:56,495 --> 00:46:57,425
Speaker 3:  That might be everyone

760
00:46:57,625 --> 00:47:01,265
Speaker 4:  Fighting for money is what it should be. What we've landed on is nobody can

761
00:47:01,265 --> 00:47:04,945
Speaker 4:  fight for money because Google took it all. Like Yeah, that's that's true.

762
00:47:04,945 --> 00:47:07,785
Speaker 3:  Capitalism has never been tried. David, I think that's what I'm trying to

763
00:47:07,785 --> 00:47:07,985
Speaker 3:  tell

764
00:47:07,985 --> 00:47:11,785
Speaker 4:  You. Yeah. What if it was just a street fight

765
00:47:11,805 --> 00:47:13,545
Speaker 4:  for money? Let's find out. Is

766
00:47:13,545 --> 00:47:16,025
Speaker 5:  Postmodern capitalism just Google opening its wallet?

767
00:47:17,255 --> 00:47:18,225
Speaker 3:  Yeah. You have

768
00:47:18,295 --> 00:47:18,865
Speaker 5:  Like, sorry,

769
00:47:19,045 --> 00:47:21,825
Speaker 3:  You come all the way. That's the horseshoe theory of internet capitalism

770
00:47:21,885 --> 00:47:25,665
Speaker 3:  is Google is the planned economy directors handing money to people.

771
00:47:28,165 --> 00:47:31,365
Speaker 3:  I'm not saying that's not what everybody wants. Okay. So that's Google. We'll

772
00:47:31,365 --> 00:47:34,565
Speaker 3:  see what happens with Yelp and Google. But You can see, like, as we keep

773
00:47:34,565 --> 00:47:37,645
Speaker 3:  saying whatever happens to Google is the internet. Yeah. And You can just

774
00:47:37,645 --> 00:47:41,325
Speaker 3:  see the internet sort of like fracturing around around that already. This

775
00:47:41,325 --> 00:47:44,245
Speaker 3:  other one I think is really interesting. It's a weird ruling.

776
00:47:45,425 --> 00:47:49,325
Speaker 3:  TikTok has to face a lawsuit for recommending the blackout challenge.

777
00:47:49,545 --> 00:47:53,205
Speaker 3:  And this is a very, very sad story. There is a, a 10-year-old who

778
00:47:53,355 --> 00:47:57,045
Speaker 3:  died doing the blackout challenge. The parents sued

779
00:47:57,045 --> 00:48:00,565
Speaker 3:  TikTok. You know, one of those cases where it's like

780
00:48:00,705 --> 00:48:04,085
Speaker 3:  TikTok didn't make the blackout challenge videos. They didn't,

781
00:48:05,175 --> 00:48:08,945
Speaker 3:  they're they're not telling you to do it. Right? The people are

782
00:48:08,945 --> 00:48:12,345
Speaker 3:  seeing the content on the platform, they're taking the action. A

783
00:48:12,905 --> 00:48:16,265
Speaker 3:  horrible, extremely depressing thing happened. And now we're gonna try to

784
00:48:16,265 --> 00:48:20,185
Speaker 3:  hold TikTok liable. And throughout most of this, we

785
00:48:20,185 --> 00:48:23,345
Speaker 3:  are not holding platforms liable for what the recommendation algorithms do,

786
00:48:23,485 --> 00:48:26,825
Speaker 3:  except we just had a Supreme Court ruling which said

787
00:48:27,635 --> 00:48:31,425
Speaker 3:  maybe we should like, more or less, maybe we should. So one of the weirdest

788
00:48:31,425 --> 00:48:35,345
Speaker 3:  things about this case is that it's, it's like a, it's like

789
00:48:35,345 --> 00:48:39,145
Speaker 3:  a puzzle the Supreme Court put itself in, so the,

790
00:48:39,145 --> 00:48:42,825
Speaker 3:  there were content moderation laws passed in Texas and Florida. They went

791
00:48:42,825 --> 00:48:45,665
Speaker 3:  up to the Supreme Court. The Supreme Court baseless said, nah, these are,

792
00:48:46,155 --> 00:48:50,145
Speaker 3:  these are weird. You're just obviously conservative states being mad at Facebook,

793
00:48:50,405 --> 00:48:53,905
Speaker 3:  but you didn't think about what would happen to other platforms, right. Or

794
00:48:53,905 --> 00:48:56,745
Speaker 3:  smaller platforms. You have to go think about this before you, you met this,

795
00:48:56,745 --> 00:48:59,585
Speaker 3:  like go back and figure it out. The one thing the Supreme Court was very

796
00:48:59,585 --> 00:49:03,185
Speaker 3:  clear about was that it is protected first party

797
00:49:03,205 --> 00:49:05,945
Speaker 3:  speech when they curate other people's content.

798
00:49:06,905 --> 00:49:10,855
Speaker 3:  Right? So section two 30 says you Facebook are

799
00:49:10,855 --> 00:49:13,735
Speaker 3:  not responsible for what people post on the platform for. Mostly

800
00:49:14,955 --> 00:49:18,815
Speaker 3:  that's their speech, not your speech. The way you moderate that speech

801
00:49:18,815 --> 00:49:22,745
Speaker 3:  and presented to other people is your speech. So that's

802
00:49:23,135 --> 00:49:26,905
Speaker 3:  protected by the First Amendment. You cannot, the government cannot set content

803
00:49:26,905 --> 00:49:30,025
Speaker 3:  moderation rules because the content moderation itself is speech

804
00:49:30,555 --> 00:49:34,425
Speaker 3:  Right, right. Okay. That feels right. Generally that feels right. That's

805
00:49:34,425 --> 00:49:36,785
Speaker 3:  how you get a marketed different platforms. That's how you end up with a

806
00:49:36,785 --> 00:49:40,425
Speaker 3:  truth social and, and x and then whatever kindergarten

807
00:49:40,425 --> 00:49:43,505
Speaker 3:  subreddit rules you wanna participate in, right? Like that's how you get

808
00:49:43,505 --> 00:49:45,985
Speaker 3:  the whole range of expression. The marketing can sort of decide how much

809
00:49:45,985 --> 00:49:49,345
Speaker 3:  content moderation they want. The court, the third circuit

810
00:49:49,595 --> 00:49:53,465
Speaker 3:  evaluating TikTok case says, well give quote, given the Supreme

811
00:49:53,465 --> 00:49:56,065
Speaker 3:  Court's observation that platforms engage in protected first party speech

812
00:49:56,065 --> 00:49:59,225
Speaker 3:  on the First Amendment. When they curate compilations of other people's content

813
00:49:59,725 --> 00:50:03,665
Speaker 3:  via their expressive algorithms, it follows that doing so

814
00:50:03,775 --> 00:50:07,625
Speaker 3:  amounts to first Sparty speech under section two 30. So the

815
00:50:07,625 --> 00:50:10,905
Speaker 3:  algorithm is your speech. That means you're liable for your speech. So if

816
00:50:10,905 --> 00:50:14,865
Speaker 3:  you recommend a bunch of blackout challenge videos, that's your speech, right?

817
00:50:14,865 --> 00:50:17,905
Speaker 3:  That's a choice you've made. Now you're liable for your speech. And it's

818
00:50:17,905 --> 00:50:20,505
Speaker 3:  calling that curation, which is really fascinating. So this is just like

819
00:50:20,545 --> 00:50:24,505
Speaker 3:  a long, a long chain of reasoning that ends with, okay, some of

820
00:50:24,505 --> 00:50:27,685
Speaker 3:  this belongs to you, so the government can tell you what to do, but it belongs

821
00:50:27,685 --> 00:50:31,275
Speaker 3:  to you. So now other people can get mad at you for it. And that, I don't

822
00:50:31,275 --> 00:50:33,715
Speaker 3:  know how that's gonna shake out. I don't know if TikTok is the right plaintiff

823
00:50:33,715 --> 00:50:37,155
Speaker 3:  for that. They're TikTok after all. We just talked about Save the Children

824
00:50:37,415 --> 00:50:41,275
Speaker 3:  is a theme. Yeah. They, their TikTok versus the United States government

825
00:50:41,275 --> 00:50:44,755
Speaker 3:  versus China. That's all in the mix there. Whether or not TikTok can like

826
00:50:44,885 --> 00:50:46,595
Speaker 3:  mount a challenge to this

827
00:50:48,775 --> 00:50:51,825
Speaker 3:  kind of a huge precedent to say these algorithms are not only your speech

828
00:50:51,825 --> 00:50:55,505
Speaker 3:  and can be protected from laws and can be protected from

829
00:50:55,505 --> 00:50:59,465
Speaker 3:  government interference, but also now other people can sue you for your algorithms

830
00:50:59,465 --> 00:50:59,625
Speaker 3:  do.

831
00:50:59,855 --> 00:51:03,785
Speaker 5:  Yeah. I mean, that like breaks the internet, right?

832
00:51:03,855 --> 00:51:07,705
Speaker 5:  Like, 'cause it doesn't just break TikTok, which arguably should be broken

833
00:51:07,705 --> 00:51:11,465
Speaker 5:  because I hate its algorithm right now, but it, it breaks, it

834
00:51:11,465 --> 00:51:15,065
Speaker 5:  breaks Facebook, it breaks Google because Google is using an

835
00:51:15,065 --> 00:51:17,305
Speaker 5:  algorithm to res to recommend search results.

836
00:51:17,695 --> 00:51:21,625
Speaker 3:  Yeah. You're showing me bad stuff is now legally actionable is whoa,

837
00:51:22,725 --> 00:51:26,625
Speaker 3:  off we go. You know, and like, yeah, fine. But we

838
00:51:26,625 --> 00:51:29,025
Speaker 3:  we're, I don't think we're ready for that world either. So the other side

839
00:51:29,025 --> 00:51:31,745
Speaker 3:  of the coin, the non-Google side of the coin also sort of up for grabs because

840
00:51:31,745 --> 00:51:35,105
Speaker 3:  of what's happening in the legal system, which just, I again, say this to

841
00:51:35,105 --> 00:51:37,985
Speaker 3:  you. I'm trying to have the end of summer here. I, I'm trying to drink a

842
00:51:37,985 --> 00:51:40,825
Speaker 3:  pina colada and the PDFs just keep coming nonstop.

843
00:51:40,825 --> 00:51:41,305
Speaker 4:  Killing

844
00:51:41,305 --> 00:51:44,785
Speaker 3:  Me. All right, let's do one more PDF and then we gotta get outta here. The

845
00:51:44,825 --> 00:51:48,465
Speaker 3:  p the pinot is a Wait, this one is, is a tough one 'cause

846
00:51:48,815 --> 00:51:51,865
Speaker 3:  it's not actually the law yet. And I actually don't know if Gavin Newsom's

847
00:51:51,865 --> 00:51:55,625
Speaker 3:  gonna sign this bill, but the California State Assembly passed an AI safety

848
00:51:55,625 --> 00:51:56,945
Speaker 3:  bill. What's going on in here, David?

849
00:51:57,445 --> 00:52:01,185
Speaker 4:  So it's called the Safe and Secure Innovation for Frontier Artificial

850
00:52:01,185 --> 00:52:04,145
Speaker 4:  Intelligence Models Act, which just rolls off the tongue.

851
00:52:05,165 --> 00:52:06,305
Speaker 3:  Is that spell anything or

852
00:52:06,305 --> 00:52:10,025
Speaker 4:  Is that It's S-S-I-F-A-I-M-A. Si.

853
00:52:11,535 --> 00:52:15,025
Speaker 4:  Noma. No. All good laws are acronyms.

854
00:52:15,425 --> 00:52:18,785
Speaker 4:  Everyone needs to remember this. If your law is not a acronym, it's not a

855
00:52:19,065 --> 00:52:21,145
Speaker 4:  good law. It's just, it's so important to me.

856
00:52:21,255 --> 00:52:24,025
Speaker 3:  Just get the ai Ask Chad G to make a name, man. Yeah,

857
00:52:24,195 --> 00:52:27,905
Speaker 4:  Right. Can you like the, the insane things people have done.

858
00:52:28,265 --> 00:52:28,385
Speaker 4:  I

859
00:52:28,385 --> 00:52:32,105
Speaker 5:  Think the people writing this bill weren't fans of ai, though. That's true.

860
00:52:32,105 --> 00:52:34,505
Speaker 5:  They, they weren't gonna ask Chad GT do it this

861
00:52:34,505 --> 00:52:38,025
Speaker 4:  Law. They should have found a way to make it spell out artificial with all

862
00:52:38,105 --> 00:52:42,025
Speaker 4:  the letters and it would've been amazing. But a last, it is mostly

863
00:52:42,025 --> 00:52:45,745
Speaker 4:  just called SB 10 47. It's an AI

864
00:52:45,745 --> 00:52:49,665
Speaker 4:  safety bill that I would say concerns itself as we've been talking

865
00:52:49,665 --> 00:52:52,985
Speaker 4:  about in part with who is responsible for bad things that happen

866
00:52:53,195 --> 00:52:56,385
Speaker 4:  downstream of AI models. and it changes

867
00:52:57,325 --> 00:53:00,865
Speaker 4:  the way that companies need to test and train these things. It changes the

868
00:53:00,865 --> 00:53:03,785
Speaker 4:  way that they need to talk to the government. Like it's, it's kind of a

869
00:53:04,385 --> 00:53:08,145
Speaker 4:  sweeping like here's how we want to oversee and think

870
00:53:08,145 --> 00:53:11,985
Speaker 4:  about AI models. There's been a lot of backing

871
00:53:11,985 --> 00:53:15,825
Speaker 4:  and forthing open AI kind of came out against it. Anthropic

872
00:53:15,965 --> 00:53:19,665
Speaker 4:  was like, we love it. Let's change some stuff. There have been

873
00:53:20,465 --> 00:53:23,065
Speaker 4:  a bunch of politicians who are against it, a bunch of politicians who are

874
00:53:23,065 --> 00:53:26,855
Speaker 4:  for it. I agree. It seems very debatable

875
00:53:26,855 --> 00:53:30,175
Speaker 4:  whether Gavin Newsom will sign this thing as it currently stands,

876
00:53:31,275 --> 00:53:34,735
Speaker 4:  but it, it, it does still feel like a moment that it passed

877
00:53:35,315 --> 00:53:37,215
Speaker 4:  out of the assembly the way that it has.

878
00:53:37,395 --> 00:53:41,255
Speaker 5:  If he signs it. Does that just mean like Google open ai, a lot of these companies

879
00:53:41,455 --> 00:53:42,415
Speaker 5:  relocate like,

880
00:53:42,805 --> 00:53:45,495
Speaker 3:  Well, California's still a huge market. Yeah,

881
00:53:45,685 --> 00:53:49,135
Speaker 4:  Well that, but also one of the things a lot of the companies have said

882
00:53:49,285 --> 00:53:52,215
Speaker 4:  essentially is why are you making a law

883
00:53:53,075 --> 00:53:56,975
Speaker 4:  to make us do what we've already promised to do? Which is essentially make

884
00:53:56,975 --> 00:54:00,095
Speaker 4:  sure our AI is not going to destroy the world. And I would argue that's a

885
00:54:00,095 --> 00:54:03,815
Speaker 4:  very funny line of reasoning to be like we said we're gonna be cool. Why

886
00:54:03,815 --> 00:54:07,415
Speaker 4:  are you gonna make me be cool? Is, is is an odd stance.

887
00:54:08,635 --> 00:54:12,535
Speaker 4:  But my sense is, and again, all of this could change,

888
00:54:12,535 --> 00:54:15,695
Speaker 4:  especially as the, the law, you know, continues to change and depending on

889
00:54:15,695 --> 00:54:18,415
Speaker 4:  how it gets implemented is that a lot of these companies are gonna not like

890
00:54:18,415 --> 00:54:22,095
Speaker 4:  it, but we'll figure out a way to play along. I don't think this is the sort

891
00:54:22,095 --> 00:54:25,095
Speaker 4:  of thing that is like gonna run anybody out of California. Yeah,

892
00:54:25,285 --> 00:54:28,175
Speaker 3:  Yeah. Philanthropic sent a letter basically supporting it.

893
00:54:29,995 --> 00:54:33,885
Speaker 3:  And to your point, they're like, yeah, but then you read the letter and

894
00:54:33,885 --> 00:54:36,365
Speaker 3:  you're like, we should definitely do something. Here's just a line from Philanthropics

895
00:54:36,365 --> 00:54:39,845
Speaker 3:  letter. We believe SB 10 47, particularly after recent amendments,

896
00:54:39,985 --> 00:54:43,605
Speaker 3:  likely presents a feasible compliance burden for companies like ours

897
00:54:43,945 --> 00:54:47,605
Speaker 3:  in light of the importance of averting catastrophic misuse.

898
00:54:49,725 --> 00:54:53,655
Speaker 3:  Yeah. Cool. Yeah. There's a real chance we might destroy the world.

899
00:54:53,755 --> 00:54:56,175
Speaker 3:  So let's do a feasible compliance burden.

900
00:54:58,795 --> 00:55:02,655
Speaker 3:  Whoops. Yeah. I don't know how this plays, you know, the Biden had the AI

901
00:55:02,665 --> 00:55:05,575
Speaker 3:  order that just came out. A bunch of companies said they were gonna join

902
00:55:05,715 --> 00:55:09,655
Speaker 3:  the, the AI model board that make sure their safety testing

903
00:55:09,715 --> 00:55:13,055
Speaker 3:  and releases results. This one feels like

904
00:55:14,735 --> 00:55:18,595
Speaker 3:  in particular, Gavin Newsom has to make a decision and then he that

905
00:55:18,595 --> 00:55:22,235
Speaker 3:  will decide whether a bunch of similar

906
00:55:22,365 --> 00:55:25,115
Speaker 3:  state laws get passed all around the country. 'cause once California does

907
00:55:25,115 --> 00:55:28,875
Speaker 3:  it, it's, it's like fine, like York will do it tomorrow. Right? Right. Because

908
00:55:29,295 --> 00:55:32,435
Speaker 3:  now you're not making him do anything. Like, you're just like off to the

909
00:55:32,445 --> 00:55:34,995
Speaker 3:  races. And then the, and then Alex, your point in the question is like, whether

910
00:55:35,005 --> 00:55:38,715
Speaker 3:  Texas does or doesn't do it, and it doesn't matter because all the researchers

911
00:55:38,715 --> 00:55:41,795
Speaker 3:  are in California anyway, right? Like, and you have to sell to the California

912
00:55:41,795 --> 00:55:45,355
Speaker 3:  market and then there you go. So I, I'm very

913
00:55:45,635 --> 00:55:49,595
Speaker 3:  confused about this one in the sense that what you really need is a federal

914
00:55:49,775 --> 00:55:53,395
Speaker 3:  law. And that doesn't seem likely. And it's also

915
00:55:53,635 --> 00:55:57,515
Speaker 3:  election season and who knows what's gonna happen. But it also seems

916
00:55:57,515 --> 00:56:00,875
Speaker 3:  like Newsom kind of doesn't wanna sign it. Like he hasn't said anything.

917
00:56:00,905 --> 00:56:04,735
Speaker 3:  Like usually when you're the governor of the big state with the

918
00:56:04,735 --> 00:56:07,815
Speaker 3:  first in the nation AI safety bill, you're, you're like pounding the pavement.

919
00:56:07,815 --> 00:56:10,095
Speaker 3:  And here he, he literally has not said anything to anyone Well,

920
00:56:10,095 --> 00:56:14,055
Speaker 5:  Like affects, who gives him money for his presidential

921
00:56:14,295 --> 00:56:15,095
Speaker 5:  campaign in four years.

922
00:56:15,805 --> 00:56:17,975
Speaker 3:  Yeah. I mean, yeah, it's true.

923
00:56:18,655 --> 00:56:21,015
Speaker 4:  I mean, I also think like we're in a moment right now where

924
00:56:22,635 --> 00:56:26,605
Speaker 4:  all of the money on planet Earth is being thrown at ai

925
00:56:27,145 --> 00:56:30,405
Speaker 4:  and there are a lot of very powerful, very rich people

926
00:57:05,845 --> 00:57:08,325
Speaker 4:  wants there to be rules until there are rules.

927
01:01:14,545 --> 01:01:17,885
Speaker 3:  All right, we're back in the break. I disclosed to David

928
01:01:18,675 --> 01:01:22,045
Speaker 3:  what company I was thinking of and Liam immediately said he bought something

929
01:01:22,045 --> 01:01:24,045
Speaker 3:  and now David is writing profile. It's happening, it's

930
01:01:24,045 --> 01:01:24,485
Speaker 4:  Gonna happen. But

931
01:01:24,485 --> 01:01:27,605
Speaker 3:  I'm I'm not telling you who, it's just immediate. It was incredible. If You

932
01:01:27,605 --> 01:01:31,405
Speaker 3:  can guess, if you send us an email and you guess maybe something that'll

933
01:01:31,405 --> 01:01:31,845
Speaker 3:  happen to you,

934
01:01:31,855 --> 01:01:33,725
Speaker 4:  We'll ship you. Whatever Liam bought from me.

935
01:01:36,465 --> 01:01:36,685
Speaker 19:  No.

936
01:01:37,065 --> 01:01:39,645
Speaker 3:  All right. Okay. We're doing it this way. We're doing true lightning round

937
01:01:40,275 --> 01:01:44,025
Speaker 3:  unsponsored. No one can tell us what to do because they haven't paid us any

938
01:01:44,025 --> 01:01:47,785
Speaker 3:  money. I'm, what I'm suggesting here is you could pay us money. I dunno how

939
01:01:47,785 --> 01:01:51,265
Speaker 3:  that works and I probably still won't do it. You say, but you know,

940
01:01:52,375 --> 01:01:54,105
Speaker 3:  live a life of possibility. True.

941
01:01:54,105 --> 01:01:57,945
Speaker 4:  Like I have one Vergecast listener slash startup executive

942
01:01:57,945 --> 01:02:01,625
Speaker 4:  who I will not name who asked me very seriously recently,

943
01:02:01,725 --> 01:02:05,105
Speaker 4:  if we actually want people to sponsor the lightning round or if we enjoy

944
01:02:05,105 --> 01:02:08,065
Speaker 4:  this bit so much that we actually don't want people. And I was like, no.

945
01:02:08,225 --> 01:02:11,785
Speaker 4:  To be very clear, I like money more than I like bits. We, we will take.

946
01:02:12,285 --> 01:02:13,745
Speaker 4:  Please sponsor the lightning round.

947
01:02:13,965 --> 01:02:16,945
Speaker 3:  Can I do an aside about the Influencer Con? I've been thinking a lot about

948
01:02:17,225 --> 01:02:20,885
Speaker 3:  journals, whereas influencers are the whole thing. Here's the problem that

949
01:02:20,885 --> 01:02:24,565
Speaker 3:  we have. When you give other people money,

950
01:02:24,715 --> 01:02:28,365
Speaker 3:  they do what you say. People come to our sales team and they're like, will

951
01:02:28,365 --> 01:02:32,325
Speaker 3:  they do what we say? And we're kinda like, no, no, just

952
01:02:32,325 --> 01:02:35,885
Speaker 3:  like flat. No. There's a real problem with journalism as a whole right now,

953
01:02:36,485 --> 01:02:40,005
Speaker 3:  fighting an uphill fight, but there's always a chance, huh?

954
01:02:40,765 --> 01:02:44,565
Speaker 4:  Hmm. I do think our enthusiasm for a sponsor of the lightning round

955
01:02:44,695 --> 01:02:47,405
Speaker 4:  might make it the greatest ROI in the history of the version.

956
01:02:48,515 --> 01:02:51,005
Speaker 4:  Like we will talk about whoever that sponsor is forever.

957
01:02:52,265 --> 01:02:54,445
Speaker 4:  Unless they suck, in which case we will see tattoo.

958
01:02:54,505 --> 01:02:58,445
Speaker 3:  See, you never know. Go Life's the game of chance. Roll the dice.

959
01:02:58,445 --> 01:03:01,925
Speaker 3:  People sponsor the lightning round. All right, true. Lightning round. I'm

960
01:03:01,925 --> 01:03:03,845
Speaker 3:  gonna read a headline. We're all gonna react to it. We're gonna move on.

961
01:03:04,415 --> 01:03:07,925
Speaker 3:  Right? Good. This is the idea. Yeah. Okay, here we go. Google

962
01:03:08,025 --> 01:03:10,325
Speaker 3:  Gemini will let you create AI generated people. again,

963
01:03:11,285 --> 01:03:14,285
Speaker 4:  Remember the diverse Nazis? No. Yeah. Those are the days we're back.

964
01:03:14,865 --> 01:03:16,165
Speaker 4:  Google says they fixed it.

965
01:03:16,925 --> 01:03:19,805
Speaker 3:  I would say you guys only white Nazis. Now you but you, you all talked at

966
01:03:19,805 --> 01:03:22,005
Speaker 3:  length about what is a photo last week with Reimagine the Pixel.

967
01:03:22,955 --> 01:03:26,725
Speaker 4:  Yeah. I had a bit of a freak out that I still am getting a lot of notes from

968
01:03:26,725 --> 01:03:30,195
Speaker 4:  people about and I feel very good about as time goes on. Yeah.

969
01:03:30,195 --> 01:03:33,915
Speaker 3:  I feel, I feel by the way, yes, we're gonna defend the sanctity of photography

970
01:03:33,915 --> 01:03:34,475
Speaker 3:  here in The Verge.

971
01:03:35,995 --> 01:03:36,835
Speaker 3:  I feel nothing about this.

972
01:03:38,375 --> 01:03:41,075
Speaker 3:  My only question is, it feels like Google's attitude about this has slightly

973
01:03:41,075 --> 01:03:44,805
Speaker 3:  shifted, right? Like they diverse Nazis were a thing and they,

974
01:03:44,805 --> 01:03:47,905
Speaker 3:  they killed it because now they're like, actually they killed it because

975
01:03:47,945 --> 01:03:50,625
Speaker 3:  a bunch of people were like, we should be able to make diverse Nazis were

976
01:03:50,625 --> 01:03:50,945
Speaker 3:  weird

977
01:03:52,565 --> 01:03:55,545
Speaker 3:  and they took it down. 'cause like we don't want, I we, I don't dunno how

978
01:03:55,605 --> 01:03:58,705
Speaker 3:  Gemini is gonna work now, but reimagine, they're just like, yeah, let it

979
01:03:58,705 --> 01:04:01,785
Speaker 3:  ride. Right? Like it's more or less their attitude towards it. Yep.

980
01:04:02,975 --> 01:04:05,585
Speaker 3:  Well, well I just, it feels like something has shifted there. I'm not sure

981
01:04:05,585 --> 01:04:06,065
Speaker 3:  what with Google,

982
01:04:06,165 --> 01:04:09,905
Speaker 4:  Was it six months ago that the Gemini thing first happened? Yeah, it was

983
01:04:09,905 --> 01:04:13,865
Speaker 4:  like earlier this year. Wild. How much this has shifted

984
01:04:13,925 --> 01:04:16,065
Speaker 4:  in six months. Because I totally agree with

985
01:04:16,065 --> 01:04:19,745
Speaker 3:  You. There's more safety rules there. Like Reimagine is not supposed to add

986
01:04:19,785 --> 01:04:22,985
Speaker 3:  a bunch of drugs to like, I don't think that's what they want, but you can't

987
01:04:23,165 --> 01:04:24,145
Speaker 4:  Add people at all

988
01:04:24,165 --> 01:04:27,345
Speaker 3:  And you can't add people. It, it won't touch people. We don't know how this

989
01:04:27,345 --> 01:04:30,545
Speaker 3:  one's gonna work. So there's some shift I'm saying is there's a shift. It's

990
01:04:30,545 --> 01:04:34,345
Speaker 3:  not the next thing we're gonna talk about, which is just a free for all.

991
01:04:34,525 --> 01:04:37,905
Speaker 3:  But there's some little shift inside of Google that says, okay, the market

992
01:04:37,925 --> 01:04:41,305
Speaker 3:  is willing to accept a little bit more crazy. And like here we are seeing

993
01:04:41,575 --> 01:04:45,025
Speaker 3:  free for all X AI's new gr image generator floods X with

994
01:04:45,025 --> 01:04:48,145
Speaker 3:  controversial AI fix. That is a very soft headline

995
01:04:48,755 --> 01:04:51,185
Speaker 4:  Again. Wait, that's okay. Here little, little quick.

996
01:04:51,185 --> 01:04:52,705
Speaker 3:  That's we're almost like a little too nice.

997
01:04:53,005 --> 01:04:56,585
Speaker 4:  No. Little, little quick. How the internet works thing. That's actually the

998
01:04:56,865 --> 01:05:00,345
Speaker 4:  SEO optimized headline that goes to Google. Can I read you the headline that's

999
01:05:00,345 --> 01:05:01,145
Speaker 4:  on our website? Ah,

1000
01:05:01,145 --> 01:05:01,945
Speaker 3:  This makes more sense. Yes.

1001
01:05:02,485 --> 01:05:05,665
Speaker 4:  X's new AI image generator will make anything from Taylor Swift and Lingerie

1002
01:05:05,665 --> 01:05:07,545
Speaker 4:  to Kamala Harris with a gun. Yeah,

1003
01:05:07,565 --> 01:05:11,265
Speaker 3:  We should, I don't know. That's what Google wants. Kamala Harris with a gun

1004
01:05:11,365 --> 01:05:15,225
Speaker 3:  is all anybody's searching nowadays. So let alone the other thing,

1005
01:05:15,655 --> 01:05:19,145
Speaker 3:  it's bananas like again, I've watched

1006
01:05:19,295 --> 01:05:23,265
Speaker 3:  hilarious videos of like Donald Trump and Hillary Clinton in an arm

1007
01:05:23,305 --> 01:05:26,905
Speaker 3:  standoff on X and it and the underlying technology is called

1008
01:05:27,055 --> 01:05:30,705
Speaker 3:  Flux, which is an open source AI system. So even if X added the controls,

1009
01:05:30,705 --> 01:05:33,665
Speaker 3:  they've now sort of like opened everyone's eyes to flux,

1010
01:05:34,595 --> 01:05:36,465
Speaker 3:  which is open source. You can just do whatever you want with.

1011
01:05:36,605 --> 01:05:38,345
Speaker 4:  And Flux is bio icon's. Very good.

1012
01:05:38,895 --> 01:05:42,065
Speaker 3:  It's very good. You can just go watch the thing. It's just very funny that

1013
01:05:42,335 --> 01:05:46,305
Speaker 3:  grok like Elon Musk, Elon Musk, I mean he wants to be a famous person,

1014
01:05:46,315 --> 01:05:50,185
Speaker 3:  right? He released his own completely out of control deepfake

1015
01:05:50,185 --> 01:05:53,985
Speaker 3:  tool on his own platform and the number of DeepFakes of him

1016
01:05:54,245 --> 01:05:54,985
Speaker 3:  are out of control.

1017
01:05:55,215 --> 01:05:57,385
Speaker 4:  What was the one you were saying you were seeing before?

1018
01:05:57,815 --> 01:06:01,705
Speaker 3:  It's Elon Musk. It's a long video. Okay. Where Elon Musk

1019
01:06:02,145 --> 01:06:06,105
Speaker 3:  narrates his ayahuasca journey. There's a, there's like a

1020
01:06:06,105 --> 01:06:08,585
Speaker 3:  nightmare dream sequence in the middle of it. And at the end of it

1021
01:06:10,005 --> 01:06:13,865
Speaker 3:  he commits to do like giving away free food to everyone like soup kitchens.

1022
01:06:13,865 --> 01:06:17,545
Speaker 3:  And also says he's gonna build a public transport system and it is per is

1023
01:06:17,545 --> 01:06:17,825
Speaker 3:  perfect.

1024
01:06:19,615 --> 01:06:21,385
Speaker 3:  Like it is a perfect Elon Musk troll.

1025
01:06:21,415 --> 01:06:22,145
Speaker 4:  Yeah. That's amazing.

1026
01:06:22,955 --> 01:06:26,825
Speaker 3:  Robot Elon Musk ways I deep fakes should be illegal. Like

1027
01:06:27,345 --> 01:06:31,305
Speaker 3:  speaking of laws that are coming, like a lot of people agree that deep fakes

1028
01:06:31,425 --> 01:06:35,305
Speaker 3:  should be legal, especially non-consensual pornography, deep fakes. This

1029
01:06:35,305 --> 01:06:38,785
Speaker 3:  is gonna be a weird fight. It's gonna be a weird fight. A bunch of Scarlet

1030
01:06:38,785 --> 01:06:42,145
Speaker 3:  Johansen does not want her voice being used by open ai, right? Like she's

1031
01:06:42,475 --> 01:06:45,705
Speaker 3:  filed that complaint. I don't know. I I do not know how these are gonna go

1032
01:06:45,705 --> 01:06:49,425
Speaker 3:  because it feels like we just let the cat out of the bag talk about feasible

1033
01:06:49,425 --> 01:06:53,225
Speaker 3:  compliance. Here we go. Like we have a picture of Mickey Mouse smoking a

1034
01:06:53,225 --> 01:06:54,905
Speaker 3:  cigarette on The Verge dot com. It's so good.

1035
01:06:55,125 --> 01:06:55,345
Speaker 4:  Yes.

1036
01:06:56,135 --> 01:06:59,865
Speaker 3:  Next one. Smart home company Brilliant has found a buyer. David, did you

1037
01:06:59,865 --> 01:07:00,625
Speaker 3:  ever have a brilliant

1038
01:07:01,225 --> 01:07:05,065
Speaker 4:  I did briefly. Wow. I knew it. I had, I had one of the earliest,

1039
01:07:05,305 --> 01:07:07,505
Speaker 4:  earliest ones I met with their founders, like before they even launched the

1040
01:07:07,505 --> 01:07:11,465
Speaker 4:  product and they gave me a, a prototype of one. I put it in my wall and

1041
01:07:11,465 --> 01:07:15,305
Speaker 4:  then I had a, I had a balcony at my apartment and it turned on the

1042
01:07:15,305 --> 01:07:19,145
Speaker 4:  balcony light permanently. It just, it wouldn't turn it off.

1043
01:07:19,855 --> 01:07:23,025
Speaker 4:  Just, and that was, that was my experience with the first brilliant, brilliant

1044
01:07:23,075 --> 01:07:26,985
Speaker 4:  stuff. Actually got very good there. They made a most

1045
01:07:27,105 --> 01:07:30,585
Speaker 4:  of their business like going into apartment buildings and stuff, but just

1046
01:07:30,585 --> 01:07:32,465
Speaker 4:  never really made it work.

1047
01:07:32,465 --> 01:07:35,425
Speaker 3:  Yeah. This is the what if you had an iPod touching your wall that controlled

1048
01:07:35,425 --> 01:07:36,465
Speaker 3:  all your smart home stuff, right?

1049
01:07:36,495 --> 01:07:38,145
Speaker 4:  Yeah, exactly. And,

1050
01:07:38,365 --> 01:07:42,305
Speaker 3:  And Jen Tooey, our brilliant smart home reviewer, reported they were

1051
01:07:42,305 --> 01:07:44,825
Speaker 3:  going outta business, which they kind of denied and then they went outta

1052
01:07:45,185 --> 01:07:48,985
Speaker 3:  business. But rescued, rescued by those white Knights of American

1053
01:07:48,985 --> 01:07:50,265
Speaker 3:  capitalism, private equity,

1054
01:07:51,125 --> 01:07:54,865
Speaker 4:  Our people, we love them. The company's now called Rescued Us. The company

1055
01:07:54,865 --> 01:07:57,665
Speaker 4:  is now called Brilliant NextGen. Good. Which is just brutal.

1056
01:07:59,205 --> 01:08:03,185
Speaker 4:  and it seems like what we know so far is that people who

1057
01:08:03,605 --> 01:08:06,905
Speaker 4:  had brilliant stuff, their stuff is going to continue to work.

1058
01:08:08,375 --> 01:08:12,145
Speaker 4:  What happens after that? I don't know the,

1059
01:08:12,325 --> 01:08:16,205
Speaker 4:  the new leadership did say they're gonna stop selling direct to

1060
01:08:16,325 --> 01:08:18,765
Speaker 4:  consumer stuff anywhere other than their website, which I think is a pretty

1061
01:08:18,765 --> 01:08:22,325
Speaker 4:  strong signal that they are going further and further

1062
01:08:22,635 --> 01:08:26,405
Speaker 4:  into the like selling into new construction business. Yeah.

1063
01:08:26,405 --> 01:08:29,205
Speaker 4:  Which makes a certain amount of sense. A lot of these companies, like Amazon

1064
01:08:29,305 --> 01:08:32,925
Speaker 4:  has focused a lot on that with Alexa stuff. There's been this big race to

1065
01:08:33,185 --> 01:08:35,645
Speaker 4:  be this sort of default choice for professional builders.

1066
01:08:35,845 --> 01:08:38,925
Speaker 3:  I know that when I was still in the rental market, what I looked for in an

1067
01:08:38,925 --> 01:08:42,525
Speaker 3:  apartment was a 6-year-old unsupported iPod touch built right into the wall.

1068
01:08:44,635 --> 01:08:45,285
Speaker 4:  It's the dream

1069
01:08:45,905 --> 01:08:49,845
Speaker 3:  So I was like same get these, get these non iPod touch apartments outta

1070
01:08:49,865 --> 01:08:50,325
Speaker 3:  my face.

1071
01:08:51,435 --> 01:08:54,925
Speaker 4:  Yeah. But I think it's, this company has always been, I would say more

1072
01:08:55,475 --> 01:08:59,045
Speaker 4:  sort of B2B than like a thing a regular person would consider buying. Like

1073
01:08:59,225 --> 01:09:03,085
Speaker 4:  you go to Home Depot and you're gonna buy a Lutron, not this, right? Like

1074
01:09:03,085 --> 01:09:04,165
Speaker 4:  I think that's pretty clear.

1075
01:09:05,705 --> 01:09:09,565
Speaker 4:  But it'll be interesting to see if they can get it righted or

1076
01:09:09,565 --> 01:09:13,005
Speaker 4:  if this just becomes kind of a slow death for a smart home company.

1077
01:09:13,315 --> 01:09:13,605
Speaker 4:  Yeah.

1078
01:09:13,685 --> 01:09:16,525
Speaker 3:  I feel like if you have one of these in your wall, you're, you're run at

1079
01:09:16,525 --> 01:09:18,525
Speaker 3:  the clock on that cloud service continuing to work. I kind

1080
01:09:18,525 --> 01:09:19,245
Speaker 4:  Of think that's right.

1081
01:09:19,475 --> 01:09:22,125
Speaker 3:  Yeah. All right. ESPN Where to Watch feature

1082
01:09:23,295 --> 01:09:26,525
Speaker 3:  fines were to stream sporting events. They basically made TV guide but for

1083
01:09:26,525 --> 01:09:26,885
Speaker 3:  streaming

1084
01:09:27,195 --> 01:09:29,725
Speaker 4:  That has ever happened to me in my entire house. This is pretty funny. I

1085
01:09:29,725 --> 01:09:32,525
Speaker 4:  spent like a lot of yesterday trying to find the catch in this and I think

1086
01:09:32,525 --> 01:09:35,685
Speaker 4:  there's like a big galaxy brain strategy, but also this is just like a good

1087
01:09:35,685 --> 01:09:39,645
Speaker 4:  nice thing for the internet. ESPN released a thing that just gives you a

1088
01:09:39,645 --> 01:09:42,205
Speaker 4:  list of all the games that are being played, which is like a thing that every

1089
01:09:42,205 --> 01:09:46,045
Speaker 4:  sports app has, right? Except for Apple Sports, which doesn't, but You

1090
01:09:46,045 --> 01:09:49,725
Speaker 4:  can go and You can see where all the games are, what the score is, whatever.

1091
01:09:49,745 --> 01:09:53,485
Speaker 4:  And now it just has a TV guide thing where You can, it'll say it's playing

1092
01:09:53,485 --> 01:09:57,325
Speaker 4:  on MLB TV or it's playing on TNT or this MLS game is on

1093
01:09:57,325 --> 01:10:01,285
Speaker 4:  Apple TV plus like, it just tells you where to stream sports, which

1094
01:10:01,335 --> 01:10:05,285
Speaker 4:  seems like it a thing that should not be complicated or need to exist or

1095
01:10:05,285 --> 01:10:05,365
Speaker 4:  a

1096
01:10:05,365 --> 01:10:08,005
Speaker 3:  Picture of a pirate ship and a Reddit logo Anywhere in this s

1097
01:10:09,315 --> 01:10:13,205
Speaker 4:  Yeah, there's just one that says parentheses. It's a Russian website against

1098
01:10:13,795 --> 01:10:14,605
Speaker 3:  Like half of these

1099
01:10:16,985 --> 01:10:18,885
Speaker 3:  You'll be extradited. Yeah, exactly.

1100
01:10:20,465 --> 01:10:24,445
Speaker 4:  But I like ESPN did this whole big media day on

1101
01:10:24,445 --> 01:10:27,245
Speaker 4:  Wednesday this week and essentially

1102
01:10:28,155 --> 01:10:31,365
Speaker 4:  made clear that it is making this pivot from being a cable channel to like

1103
01:10:31,405 --> 01:10:35,085
Speaker 4:  a sports lifestyle brand. That was how Sarah Fisher and Axios

1104
01:10:35,475 --> 01:10:38,925
Speaker 4:  explained it and I think that was really right. They're about to launch a

1105
01:10:38,925 --> 01:10:42,685
Speaker 4:  streaming service that shows you all the ESPN stuff like ESPN, the cable

1106
01:10:42,685 --> 01:10:46,445
Speaker 4:  channel's gonna be a streaming service. They're part of venue sports. ESPN's

1107
01:10:46,445 --> 01:10:49,965
Speaker 4:  whole thing is they're just like, we want you to come to ESPN when you wanna

1108
01:10:49,965 --> 01:10:52,525
Speaker 4:  watch sports, even if you go somewhere else because they have the rights.

1109
01:10:52,525 --> 01:10:55,965
Speaker 4:  If you open the ES ESPN app first we win. Right? 'cause that's how you bet.

1110
01:10:55,965 --> 01:10:59,925
Speaker 4:  That's how you get into fantasy all this stuff. And so ESPN is like deep

1111
01:11:00,105 --> 01:11:04,045
Speaker 4:  in the weeds of like, how do we become a destination for

1112
01:11:04,105 --> 01:11:07,925
Speaker 4:  all things sports all the time? And this is both a very good idea in

1113
01:11:07,925 --> 01:11:11,285
Speaker 4:  that vein and also just like a useful page that I'm going to load every single

1114
01:11:11,305 --> 01:11:12,165
Speaker 4:  day for the rest of my life.

1115
01:11:12,485 --> 01:11:15,765
Speaker 5:  I have a question. Question. Was this originally supposed to be a venue launch

1116
01:11:16,655 --> 01:11:19,405
Speaker 5:  party or something? Because wasn't venue like supposed to launch around?

1117
01:11:19,405 --> 01:11:21,805
Speaker 5:  That's around now. Really good point. Before FUBU killed it, it was

1118
01:11:22,445 --> 01:11:25,645
Speaker 3:  Fu they slowed it down. They sued him another them another antitrust lawsuit

1119
01:11:25,705 --> 01:11:29,445
Speaker 3:  in the background of Yeah, the entire internet economy may maybe, but

1120
01:11:29,575 --> 01:11:33,005
Speaker 3:  venue was supposed to have everything in it. Venues the the opposite strategy.

1121
01:11:33,575 --> 01:11:33,925
Speaker 3:  Right?

1122
01:11:33,925 --> 01:11:34,565
Speaker 5:  Right, right. But

1123
01:11:34,565 --> 01:11:37,485
Speaker 3:  Then really ESPN is trying to send you to every other service and venue is

1124
01:11:37,485 --> 01:11:40,085
Speaker 3:  like, we're gonna buy everything and fu Bo's dead. And Fuo was like, yeah

1125
01:11:40,085 --> 01:11:41,005
Speaker 3:  but ESPN, hold on a minute.

1126
01:11:42,435 --> 01:11:45,605
Speaker 5:  ESPN was gonna be a part of this. So then you know, you'd go and you'd look

1127
01:11:45,605 --> 01:11:48,605
Speaker 5:  and be like, oh, what should I watch? Oh, it's all on venue. I sure should

1128
01:11:48,925 --> 01:11:50,405
Speaker 5:  subscribe to venue. It's a TV

1129
01:11:50,645 --> 01:11:52,805
Speaker 3:  Guide. But the answer is one channel. Just

1130
01:11:52,995 --> 01:11:54,245
Speaker 5:  Just one channel every time.

1131
01:11:54,545 --> 01:11:55,685
Speaker 3:  That's, I could see it,

1132
01:11:56,105 --> 01:11:59,125
Speaker 4:  But you know what's so telling about the sports industry is it wouldn't have

1133
01:11:59,125 --> 01:12:02,805
Speaker 4:  been that way. The thing that is ostensibly the streaming service for all

1134
01:12:02,805 --> 01:12:06,205
Speaker 4:  the sports is not actually the streaming service for all the sports. This

1135
01:12:06,205 --> 01:12:07,885
Speaker 4:  is why it's so terrible. But yeah,

1136
01:12:07,885 --> 01:12:09,445
Speaker 5:  It's gonna be like 60% I think.

1137
01:12:09,995 --> 01:12:13,725
Speaker 4:  Yeah. Which is fine. And I think venue, if it eventually launches will be

1138
01:12:13,725 --> 01:12:16,725
Speaker 4:  a useful thing that I'm sure I will give too much money to. But like the

1139
01:12:16,725 --> 01:12:20,405
Speaker 4:  fact that You can go on here and it will tell you the game is on Prime

1140
01:12:20,405 --> 01:12:23,885
Speaker 4:  video or that the game is on Apple TV plus is like a genuine

1141
01:12:25,035 --> 01:12:28,965
Speaker 4:  user interface victory. And I also think it makes a lot of

1142
01:12:28,965 --> 01:12:30,525
Speaker 4:  sense for ESPN as like a big,

1143
01:12:30,705 --> 01:12:33,885
Speaker 3:  How low our standards have fallen. This is what I mean. TV guide,

1144
01:12:34,595 --> 01:12:37,685
Speaker 3:  literally it's TV guide even looks like TV guide. We're moving on David.

1145
01:12:37,805 --> 01:12:39,885
Speaker 3:  I feel like that you're gonna try to convince some people to buy this thing

1146
01:12:39,985 --> 01:12:43,125
Speaker 3:  and I'm telling you already immediately saying you wanna see it.

1147
01:12:43,565 --> 01:12:46,565
Speaker 4:  I have it. It's right here. You have it. The claw, this thing is called the

1148
01:12:46,565 --> 01:12:50,445
Speaker 4:  clawed note pin. Yeah. And it's, we're just in this

1149
01:12:50,445 --> 01:12:53,805
Speaker 4:  phase of everybody launching little tiny voice recorders that

1150
01:12:54,195 --> 01:12:57,845
Speaker 4:  then use chat GPT to summarize whatever they hear. Like

1151
01:12:57,955 --> 01:13:01,805
Speaker 4:  they all have bigger ideas, but that's essentially what it is. This one

1152
01:13:01,825 --> 01:13:05,645
Speaker 4:  is just cool 'cause it's like a little, it's a little guy, it's a wearable.

1153
01:13:05,785 --> 01:13:09,085
Speaker 4:  It comes with a lanyard and a clip

1154
01:13:09,705 --> 01:13:13,365
Speaker 4:  and something else. Oh, and a thing You can wear on your wrist. So the idea

1155
01:13:13,365 --> 01:13:17,125
Speaker 4:  is like, it's a, it's a wearable. You, you tap it, you talk to it, it

1156
01:13:17,485 --> 01:13:20,045
Speaker 4:  summarizes your notes. Is that anything?

1157
01:13:21,365 --> 01:13:21,765
Speaker 4:  I don't know.

1158
01:13:23,345 --> 01:13:26,445
Speaker 4:  But this is like, this is the thing, right? Like Microsoft Recall is a version

1159
01:13:26,445 --> 01:13:29,045
Speaker 4:  of this, the Limitless thing which we've talked about is a version of this.

1160
01:13:29,355 --> 01:13:33,245
Speaker 4:  This idea of like, how do we make it easy for you to input all of

1161
01:13:33,245 --> 01:13:37,085
Speaker 4:  your stuff into an AI system and then it will make something out of that

1162
01:13:37,085 --> 01:13:40,885
Speaker 4:  for you is like a, the big new

1163
01:13:40,885 --> 01:13:43,685
Speaker 4:  product question for a lot of these companies. I don't know if any of it

1164
01:13:43,685 --> 01:13:46,965
Speaker 4:  amounts to anything. I have just been sitting here like yelling thoughts

1165
01:13:46,965 --> 01:13:50,685
Speaker 4:  into this thing all day and it just keeps transcribing them being like, you

1166
01:13:50,715 --> 01:13:54,125
Speaker 4:  need to remember that your meeting is in an hour. I'm like, is this useful?

1167
01:13:54,125 --> 01:13:55,245
Speaker 4:  Like what did we get from this?

1168
01:13:55,675 --> 01:13:59,285
Speaker 3:  This is like, what were those journals of the pens and the the, the Notebooks

1169
01:13:59,285 --> 01:14:03,125
Speaker 3:  had Oh, the live Scribe Dots. Yeah. And I wanted to be a live

1170
01:14:03,525 --> 01:14:07,445
Speaker 3:  a live scribe person. So bad. Same. So bad. And now it's a little

1171
01:14:07,445 --> 01:14:11,205
Speaker 3:  chat PT AI bubble device. I look forward to your review. I

1172
01:14:11,205 --> 01:14:14,005
Speaker 3:  encourage everyone to wait for the inevitable in the review.

1173
01:14:15,465 --> 01:14:18,965
Speaker 4:  The good news is this is a thing AI can actually do, which

1174
01:14:19,355 --> 01:14:23,205
Speaker 4:  like summarize a bunch like voice to text to

1175
01:14:23,405 --> 01:14:26,995
Speaker 4:  summarization. AI is good at all of those steps, unlike

1176
01:14:27,215 --> 01:14:31,155
Speaker 4:  so many other things. Like I watched a very funny video of somebody reviewing

1177
01:14:31,155 --> 01:14:35,115
Speaker 4:  the, the Brilliant Frame ar gla ai glasses. Oh my gosh. And I

1178
01:14:35,115 --> 01:14:38,555
Speaker 4:  got such flashbacks to doing the review of the humane AI pin where you're

1179
01:14:38,555 --> 01:14:41,155
Speaker 4:  just sitting there asking it really basic questions. He was sitting there

1180
01:14:41,155 --> 01:14:45,075
Speaker 4:  holding up like a, I forget what it was, a drink of some

1181
01:14:45,075 --> 01:14:47,835
Speaker 4:  kind, I think. and it kept being like, that's a bag of potato chips. And

1182
01:14:47,835 --> 01:14:50,915
Speaker 4:  he's like, no, it's not. And it's just like, oh yeah, this is where AI is.

1183
01:14:50,975 --> 01:14:53,675
Speaker 4:  But summarizing texts on point. All right,

1184
01:14:53,675 --> 01:14:56,115
Speaker 3:  I look again. Well I look forward to your review And. if you don't have the

1185
01:14:56,115 --> 01:14:59,995
Speaker 3:  chart of wearable bullshit in the review, here's what I'm saying. The

1186
01:15:00,115 --> 01:15:02,675
Speaker 3:  maker of the B Palmer has a new cheaper e-reader.

1187
01:15:03,405 --> 01:15:03,995
Speaker 5:  Don't get it.

1188
01:15:04,455 --> 01:15:07,355
Speaker 3:  You don't get it. Oh wait, you're saying not to get it.

1189
01:15:07,545 --> 01:15:11,395
Speaker 5:  Okay, so it's $150. It's the books six Go and

1190
01:15:11,585 --> 01:15:15,235
Speaker 5:  it's their new small low budget cheap e-reader that also runs

1191
01:15:15,235 --> 01:15:18,275
Speaker 5:  Android. So you think, oh, these are all good things. Oh, it's cheaper than

1192
01:15:18,275 --> 01:15:21,715
Speaker 5:  the book's Palmer. But I get a wider screen. That sounds great. Then you

1193
01:15:21,715 --> 01:15:25,635
Speaker 5:  look at how much Ram it has and then you look at how much Ram the books

1194
01:15:25,775 --> 01:15:26,835
Speaker 5:  Palmer has. Oh my

1195
01:15:26,835 --> 01:15:28,195
Speaker 4:  God. It's only two gigs of Ram,

1196
01:15:28,545 --> 01:15:31,515
Speaker 5:  Only two gigs of Ram. Don't, don't, you're right. Like I've had a couple

1197
01:15:31,515 --> 01:15:35,315
Speaker 5:  of books products where they, where they skimp on the ram and you feel

1198
01:15:35,315 --> 01:15:37,875
Speaker 5:  it. It's real, real rough. Yeah. So,

1199
01:15:37,955 --> 01:15:40,035
Speaker 4:  And that's the sort of thing that like on a Kindle, you don't really have

1200
01:15:40,035 --> 01:15:43,035
Speaker 4:  to worry about Ram 'cause it's kind of only doing one thing. Yeah. But like

1201
01:15:43,455 --> 01:15:46,475
Speaker 4:  you're gonna accidentally have three apps open at the same time on this thing

1202
01:15:46,475 --> 01:15:48,275
Speaker 4:  and it is just going to set itself on fire.

1203
01:15:48,615 --> 01:15:52,395
Speaker 5:  Yep. So don't don't Oh man. Like resist the urge. I know

1204
01:15:53,045 --> 01:15:56,515
Speaker 5:  books put more ram in. Like it's not that you're not Apple, it's cheap.

1205
01:15:56,555 --> 01:15:59,395
Speaker 3:  I mean at this point we are just directly controlling books this business.

1206
01:15:59,485 --> 01:15:59,835
Speaker 3:  Right.

1207
01:15:59,975 --> 01:16:00,955
Speaker 5:  I'm trying, I'm trying.

1208
01:16:00,955 --> 01:16:04,915
Speaker 3:  We'll tell you what to do. All right. Speaking of Ram,

1209
01:16:05,985 --> 01:16:09,675
Speaker 3:  that makes no sense. Sure. The Dyson Air Wrap

1210
01:16:09,935 --> 01:16:13,555
Speaker 3:  ID is a new smarter hair curler, So, I. Put this on here because one is,

1211
01:16:13,555 --> 01:16:17,515
Speaker 3:  Alex has mentioned to us many, many times the Dyson Supersonic, that's

1212
01:16:17,515 --> 01:16:21,115
Speaker 3:  the hair dryer and the air wrap. Legitimate gadgets. Like insane

1213
01:16:21,345 --> 01:16:24,355
Speaker 3:  gadgets in the classic Dyson mold of, we made a fan

1214
01:16:25,385 --> 01:16:28,515
Speaker 3:  what can have a fan. And remember when they got all the way to the car and

1215
01:16:28,515 --> 01:16:30,235
Speaker 3:  they're like, no fan. You

1216
01:16:30,235 --> 01:16:33,675
Speaker 5:  Know, they stopped. They stopped fans. They, they got to, they, they're do

1217
01:16:33,715 --> 01:16:37,235
Speaker 5:  a like a hair serum now. Ooh. And I was like, where is the fan in this hair

1218
01:16:37,235 --> 01:16:38,035
Speaker 5:  serum? Yeah,

1219
01:16:38,035 --> 01:16:41,075
Speaker 3:  I'm calling it right now. Dyson. You're way out your lane. Yeah. It's fans

1220
01:16:41,075 --> 01:16:41,355
Speaker 3:  only.

1221
01:16:41,975 --> 01:16:42,195
Speaker 5:  Yep.

1222
01:16:42,955 --> 01:16:45,395
Speaker 3:  I really wanted the car to have a fan. Anyway, tell me about it.

1223
01:16:46,705 --> 01:16:50,395
Speaker 5:  Okay, so this is the new air wrap. They've released a couple of different

1224
01:16:50,595 --> 01:16:53,395
Speaker 5:  additions at this point. This one's got a couple of different

1225
01:16:54,405 --> 01:16:58,115
Speaker 5:  extensions. One that's like just is gonna suck your hair slightly

1226
01:16:58,115 --> 01:17:01,955
Speaker 5:  differently. I probably should explain what an air rep does after saying

1227
01:17:01,955 --> 01:17:03,355
Speaker 5:  that. It sucks

1228
01:17:03,355 --> 01:17:04,275
Speaker 4:  Your hair. I don't understand.

1229
01:17:05,295 --> 01:17:07,155
Speaker 5:  The air basically sucks your hair. You go

1230
01:17:07,155 --> 01:17:07,955
Speaker 3:  In one of two directions,

1231
01:17:08,575 --> 01:17:12,485
Speaker 5:  It sucks your hair in one of two directions and, and then you like You

1232
01:17:12,485 --> 01:17:16,125
Speaker 5:  can put hot air on it or cold air on it. Yeah. And it's magic

1233
01:17:16,345 --> 01:17:20,285
Speaker 5:  And. if you have ever tried to curl your hair or have thought about it,

1234
01:17:20,395 --> 01:17:23,445
Speaker 5:  it's, it's horrible. And, and this makes it easier. It's for those of us

1235
01:17:23,445 --> 01:17:26,125
Speaker 5:  who don't wanna worry about having hot metal our faces.

1236
01:17:26,435 --> 01:17:29,765
Speaker 3:  Okay. So I'm gonna ask you this question about this one. Yeah. The addition

1237
01:17:29,765 --> 01:17:32,565
Speaker 3:  to this one is that it has Bluetooth, it's

1238
01:17:32,565 --> 01:17:33,045
Speaker 5:  Got an app.

1239
01:17:33,755 --> 01:17:35,605
Speaker 3:  What, what

1240
01:17:35,745 --> 01:17:39,565
Speaker 5:  The, the, I think the app could probably be

1241
01:17:39,635 --> 01:17:43,165
Speaker 5:  like, you could just not get this and turn on use YouTube or

1242
01:17:43,225 --> 01:17:47,165
Speaker 5:  TikTok. I would give you the same thing, but yeah, it's supposed

1243
01:17:47,165 --> 01:17:50,845
Speaker 5:  to kind of guide you through Sure. Doing the hair and making sure you're

1244
01:17:50,845 --> 01:17:53,045
Speaker 5:  holding it for long enough. 'cause a lot of people would just do it once

1245
01:17:53,045 --> 01:17:54,125
Speaker 5:  and be like, it looks good. Is

1246
01:17:54,125 --> 01:17:57,725
Speaker 3:  This like the smart oven theory where you like you have a QR code of a

1247
01:17:57,725 --> 01:18:01,325
Speaker 3:  hairstyle and then it just like does the settings for you? Yeah. Yeah.

1248
01:18:01,325 --> 01:18:03,845
Speaker 5:  You're, yeah, it kinda walks you through it. I haven't gotten to play with

1249
01:18:03,845 --> 01:18:07,325
Speaker 5:  this app obviously, but that's kind of the idea. It's just supposed to help

1250
01:18:07,325 --> 01:18:10,525
Speaker 5:  you make things a little easier through some Bluetooth in,

1251
01:18:11,145 --> 01:18:14,805
Speaker 5:  so yeah. More gadget. But honestly the, the

1252
01:18:14,805 --> 01:18:18,365
Speaker 5:  extensions for it are the cool part. They're doing like one for curly hair

1253
01:18:18,385 --> 01:18:22,245
Speaker 5:  and one for straighter hair. So curly hair people get like more diffusers

1254
01:18:22,245 --> 01:18:24,085
Speaker 5:  and stuff, which is very exciting.

1255
01:18:24,665 --> 01:18:28,045
Speaker 4:  And it's just as stupidly expensive as the air wrap Yeah. Was

1256
01:18:28,045 --> 01:18:29,125
Speaker 5:  Already so, and it's worth it. So

1257
01:18:29,325 --> 01:18:29,605
Speaker 4:  Whatever. Oh,

1258
01:18:30,125 --> 01:18:32,405
Speaker 5:  Somebody in your life is gonna ask for one and get it bought.

1259
01:18:32,485 --> 01:18:35,605
Speaker 3:  I bought one for Becky because Alex has been so high on these things for

1260
01:18:35,605 --> 01:18:39,445
Speaker 3:  so long and for in the spirit of no one knows how to use it.

1261
01:18:40,005 --> 01:18:43,605
Speaker 3:  I bought without the Bluetooth, the Bluetooth one was that yet also, also

1262
01:18:43,605 --> 01:18:47,525
Speaker 3:  if I bought Becky a Bluetooth device, she'd be like, no, no thank you. I'm

1263
01:18:47,525 --> 01:18:51,365
Speaker 3:  never, I'm never using this functionality. But for like a week, both Becky

1264
01:18:51,385 --> 01:18:55,285
Speaker 3:  and Max looked like, like full Texas cheerleaders. Like it's

1265
01:18:55,285 --> 01:18:58,525
Speaker 5:  Great. Just big hair Awesome. Every day. That's awesome. Yeah. And then that's

1266
01:18:58,525 --> 01:18:59,685
Speaker 5:  how you know somebody gets one. Yeah.

1267
01:18:59,685 --> 01:19:03,165
Speaker 3:  And then the the weird thing is our air purifiers like light up every time

1268
01:19:03,165 --> 01:19:06,925
Speaker 3:  we use it. And someone told me it's because of the hot air, like the hair

1269
01:19:06,925 --> 01:19:08,925
Speaker 3:  products you use, it like lights up the air purifiers.

1270
01:19:09,315 --> 01:19:13,125
Speaker 5:  That makes sense. Your hairspray Yeah. Is you're not, don't breathe that

1271
01:19:13,125 --> 01:19:14,845
Speaker 5:  stuff. I mean it smells great, but don't huff it.

1272
01:19:14,845 --> 01:19:18,805
Speaker 3:  Here's what I'm saying. Dyson, find more things to put fans in. That's

1273
01:19:18,805 --> 01:19:21,925
Speaker 3:  your lane. And I, I fully support it.

1274
01:19:23,465 --> 01:19:27,405
Speaker 3:  Get to a car, get to a hovercraft car. This is what I'm, I'm

1275
01:19:27,405 --> 01:19:31,085
Speaker 3:  trying to, this is why your first car failed. It wasn't floating on a bed

1276
01:19:31,085 --> 01:19:34,685
Speaker 3:  of perfectly produced air. The next car, they

1277
01:19:34,685 --> 01:19:37,365
Speaker 5:  Got this, I believe in them. Do you think they were such great hair?

1278
01:19:37,505 --> 01:19:40,285
Speaker 3:  Do you think they were like Electric Motors are kind of like fans, right?

1279
01:19:40,285 --> 01:19:42,765
Speaker 3:  Yes. They're fans without blades and they're like, we'll do Electric Motors

1280
01:19:42,765 --> 01:19:46,285
Speaker 3:  at scale. Yes. I'm Di James Dyson. Sir James Dyson.

1281
01:19:47,275 --> 01:19:49,645
Speaker 3:  Come on the show. I have only one question for you. It's not worth a full

1282
01:19:49,645 --> 01:19:53,245
Speaker 3:  decoder. Did you think electric Cars were fans without Blades? Yes or no

1283
01:19:53,265 --> 01:19:53,485
Speaker 3:  sir.

1284
01:19:55,155 --> 01:19:56,125
Speaker 5:  Just a yes or no.

1285
01:19:56,775 --> 01:19:59,445
Speaker 4:  We're gonna get James Dyson on the show and we're gonna play. Can you put

1286
01:19:59,445 --> 01:19:59,925
Speaker 4:  a fan in that?

1287
01:20:00,975 --> 01:20:01,885
Speaker 3:  We're gonna do that for

1288
01:20:01,985 --> 01:20:05,285
Speaker 4:  One full hour of the first cast and it's gonna be incredible. And I honestly

1289
01:20:05,285 --> 01:20:07,205
Speaker 4:  believe he would say yes to doing that with us

1290
01:20:07,205 --> 01:20:08,565
Speaker 3:  On the first cast. Most things. Yeah. Yeah.

1291
01:20:08,565 --> 01:20:11,805
Speaker 4:  This is my new goal. Can you put a fan in that with James Dyson

1292
01:20:13,445 --> 01:20:14,205
Speaker 3:  Gauntlet Throne?

1293
01:20:14,515 --> 01:20:15,285
Speaker 4:  Yeah, let's go.

1294
01:20:16,125 --> 01:20:17,725
Speaker 3:  Snapchat finally launched an iPad app.

1295
01:20:19,005 --> 01:20:19,295
Speaker 5:  Good,

1296
01:20:19,865 --> 01:20:22,415
Speaker 3:  Right? Huh? Little

1297
01:20:22,415 --> 01:20:25,895
Speaker 5:  Like the 13 year olds, they're like, they can use their

1298
01:20:26,105 --> 01:20:26,775
Speaker 5:  iPads now.

1299
01:20:27,375 --> 01:20:31,295
Speaker 3:  That's great. I feel like this belongs in our Is this anything Hall of Fame?

1300
01:20:32,855 --> 01:20:33,295
Speaker 4:  Snapchat for

1301
01:20:33,295 --> 01:20:36,215
Speaker 3:  IPad? Yeah. Right. It's like, it's a bunch of weird i AI gadgets and it's

1302
01:20:36,215 --> 01:20:38,575
Speaker 3:  like Snapchat, iPad. Like is this anything

1303
01:20:40,225 --> 01:20:41,715
Speaker 3:  what people is Instagram?

1304
01:20:42,585 --> 01:20:42,875
Speaker 4:  Yeah.

1305
01:20:43,135 --> 01:20:44,355
Speaker 3:  And they're never gonna get it

1306
01:20:45,375 --> 01:20:49,275
Speaker 4:  And Snap is like, it, it, it clearly wants you to use

1307
01:20:49,275 --> 01:20:52,635
Speaker 4:  it to like watch Snap originals and stories and stuff.

1308
01:20:53,795 --> 01:20:56,955
Speaker 4:  I don't think this is gonna be like a super kick ass messaging

1309
01:20:57,615 --> 01:21:01,355
Speaker 4:  system. Snapchat for the iPad. Not so sure.

1310
01:21:02,055 --> 01:21:05,795
Speaker 3:  All right. It's for the speaking of Instagram. Another classic all time headline

1311
01:21:05,795 --> 01:21:09,235
Speaker 3:  on The Verge dot com. Instagram adds what? photos have always needed

1312
01:21:09,685 --> 01:21:10,035
Speaker 3:  words.

1313
01:21:14,985 --> 01:21:16,555
Speaker 3:  It's good. I mean, yeah, it's

1314
01:21:16,555 --> 01:21:18,915
Speaker 5:  Good. I look get a lot of photos and think what if it just had a letter on

1315
01:21:18,915 --> 01:21:19,035
Speaker 5:  it?

1316
01:21:19,145 --> 01:21:22,995
Speaker 3:  Basically You can now put text in the ins on photos in the Instagram editor,

1317
01:21:23,245 --> 01:21:25,275
Speaker 3:  which is fine. 'cause you could already do that. And obviously real,

1318
01:21:25,275 --> 01:21:27,275
Speaker 4:  I was gonna say everyone has figured out how to do this anyway,

1319
01:21:27,935 --> 01:21:31,275
Speaker 3:  But the, the fact that Instagram is slowly becoming Canva because Instagram

1320
01:21:31,295 --> 01:21:34,755
Speaker 3:  is slowly becoming Craigslist very real.

1321
01:21:35,135 --> 01:21:38,515
Speaker 4:  Oh, that's okay. So I. The reason I put this in here is I was gonna ask,

1322
01:21:38,975 --> 01:21:42,755
Speaker 4:  are we headed towards a world in which threads and Instagram

1323
01:21:42,935 --> 01:21:46,915
Speaker 4:  are just the same thing? Instagram is gonna get texty and Threads is

1324
01:21:46,915 --> 01:21:50,075
Speaker 4:  gonna get Photo Z and they're just gonna be the same. But what you're saying

1325
01:21:50,075 --> 01:21:53,475
Speaker 4:  is Threads is gonna become Instagram because Instagram is becoming Craigslist.

1326
01:21:54,055 --> 01:21:55,995
Speaker 5:  No threads isn't gonna become Instagram. Yeah,

1327
01:21:55,995 --> 01:21:59,915
Speaker 3:  I think they know that the point of threads is extremely leading questions

1328
01:21:59,915 --> 01:22:03,715
Speaker 3:  about nothing and they're just gonna keep leaning right into I I

1329
01:22:03,715 --> 01:22:07,515
Speaker 3:  come up with lists of questions to ask threads like every day. That's like,

1330
01:22:08,315 --> 01:22:11,035
Speaker 3:  I was walking down the street and I saw a guy wearing headphones. Has anyone

1331
01:22:11,035 --> 01:22:12,675
Speaker 3:  seen one of these before? It's like

1332
01:22:14,295 --> 01:22:18,195
Speaker 3:  all day long. You can, like I could be the most popular

1333
01:22:18,375 --> 01:22:22,315
Speaker 3:  person on threads tomorrow. You can just do it all day long. You

1334
01:22:22,315 --> 01:22:26,155
Speaker 3:  just like look at things like, has anyone ever seen this before? I saw one

1335
01:22:26,595 --> 01:22:30,235
Speaker 3:  today. Today person posted. They were on an airplane and you know, sometimes

1336
01:22:30,235 --> 01:22:32,395
Speaker 3:  their airplane you're boarding and there's condensation coming off the plane

1337
01:22:32,735 --> 01:22:36,155
Speaker 3:  and I don't know who they were just like, I've never seen this on a plane

1338
01:22:36,155 --> 01:22:37,715
Speaker 3:  before and I fly all the time. My plane

1339
01:22:37,735 --> 01:22:38,195
Speaker 4:  Is wet

1340
01:22:38,915 --> 01:22:41,915
Speaker 3:  In all of just like millions of replies of people being like, you've never

1341
01:22:41,915 --> 01:22:43,035
Speaker 3:  been on a plane before. Wow.

1342
01:22:43,035 --> 01:22:45,435
Speaker 5:  That's great. Yeah, a lot of people are asking, she was, she was asking if

1343
01:22:45,435 --> 01:22:48,235
Speaker 5:  she was gonna die and people were like, no, no.

1344
01:22:49,455 --> 01:22:52,395
Speaker 3:  But I'm just telling you the fact that like millions of people will fall

1345
01:22:52,395 --> 01:22:55,515
Speaker 3:  for the worst engagement bait on threads means we've learned nothing.

1346
01:22:56,375 --> 01:22:59,315
Speaker 3:  You know how like we just did the, like you're here because you, you think

1347
01:22:59,315 --> 01:23:03,115
Speaker 3:  people are smarter than Photoshop, you know, and everyone's like, we are,

1348
01:23:03,115 --> 01:23:06,325
Speaker 3:  you know this. I'm like, I wanna show you this picture of condensation on

1349
01:23:06,325 --> 01:23:10,205
Speaker 3:  a plane and millions of people bringing, we know nothing, one of our biggest

1350
01:23:10,205 --> 01:23:13,885
Speaker 3:  tech companies algorithms to its knees over the dumbest

1351
01:23:13,885 --> 01:23:15,125
Speaker 3:  engagement bait in the world.

1352
01:23:15,865 --> 01:23:19,125
Speaker 4:  It is fun just as like an intellectual exercise to look around and just think

1353
01:23:19,125 --> 01:23:21,885
Speaker 4:  like, what engagement bait question could I ask

1354
01:23:22,185 --> 01:23:24,365
Speaker 3:  You can just post a picture of anything. Be like, what is

1355
01:23:24,365 --> 01:23:27,365
Speaker 4:  This? Yeah, yeah. Like my water has bubbles in it. How did that happen?

1356
01:23:27,715 --> 01:23:31,365
Speaker 3:  This is what we're doing from now on. Just, I'm just get ready.

1357
01:23:31,435 --> 01:23:35,325
Speaker 3:  It's coming. Anyway, my my point is I think they know they

1358
01:23:35,325 --> 01:23:39,245
Speaker 3:  want that thing, they want a bunch of calm tweets during NBA games, right?

1359
01:23:39,245 --> 01:23:42,965
Speaker 3:  Yeah. Like you, that's not actually like a visual communication moment, but

1360
01:23:42,965 --> 01:23:45,085
Speaker 3:  it's a place for people to engage and they just want the engagement and they

1361
01:23:45,085 --> 01:23:47,525
Speaker 3:  can put the ads there. Whether or not that's photos or not, I think they're

1362
01:23:47,525 --> 01:23:50,565
Speaker 3:  happy with what they've got. I'm just saying Instagram is becoming a marketing

1363
01:23:50,805 --> 01:23:54,005
Speaker 3:  platform. It already was a marketing platform and increasingly what they're

1364
01:23:54,005 --> 01:23:57,165
Speaker 3:  marketing is small business services. So being able to just put the picture

1365
01:23:57,225 --> 01:24:00,645
Speaker 3:  on your yoga studio with the text. It's like yoga class. Like they're just

1366
01:24:00,645 --> 01:24:04,565
Speaker 3:  gonna do it. And that I think that they're, that's why I was like, it's Canva.

1367
01:24:05,835 --> 01:24:09,355
Speaker 3:  'cause that's what Canva does for people. Great. Cool. Soon it will just

1368
01:24:09,355 --> 01:24:13,155
Speaker 3:  be ai like generate me a photo of a yoga studio. It'll be fine.

1369
01:24:13,465 --> 01:24:17,225
Speaker 3:  Alright, last one. We've gone this whole time without talking about it

1370
01:24:17,805 --> 01:24:20,125
Speaker 3:  and we're just gonna talk about it for one second in the slide around. 'cause

1371
01:24:20,125 --> 01:24:23,485
Speaker 3:  I'm confident next week we'll have a full preview. Apple's iPhone 16 launch

1372
01:24:23,485 --> 01:24:24,685
Speaker 3:  event is September 9th.

1373
01:24:25,065 --> 01:24:25,765
Speaker 5:  It sure is

1374
01:24:27,005 --> 01:24:29,095
Speaker 4:  Glow time, right? That's the, that's that's the

1375
01:24:29,095 --> 01:24:31,895
Speaker 3:  Tagline's. That's gotta be for the Siri, right? The the Siri Effect.

1376
01:24:31,895 --> 01:24:35,645
Speaker 4:  Yeah. Yeah. Which I'm excited about. Okay. I have, I

1377
01:24:35,805 --> 01:24:39,445
Speaker 4:  I, I see this going one of two ways and I'm curious which you guys think

1378
01:24:39,445 --> 01:24:39,685
Speaker 4:  it is.

1379
01:24:41,465 --> 01:24:45,405
Speaker 4:  One, this is just another AI show. There's gonna be some new

1380
01:24:45,605 --> 01:24:48,605
Speaker 4:  hardware they're gonna, but all they're gonna talk about is how like the

1381
01:24:48,605 --> 01:24:52,485
Speaker 4:  new whatever chip inside of the iPhone 16

1382
01:24:53,175 --> 01:24:55,765
Speaker 4:  gives you more AI stuff.

1383
01:24:57,085 --> 01:25:00,975
Speaker 4:  Or this is going to be like a massive

1384
01:25:01,275 --> 01:25:05,215
Speaker 4:  new hardware rev. We're gonna get the camera buttons, we're gonna

1385
01:25:05,215 --> 01:25:08,135
Speaker 4:  get new designs, new colors, all kinds of stuff. Like

1386
01:25:09,095 --> 01:25:13,055
Speaker 4:  I could see this being either incredibly boring or incredibly huge cool,

1387
01:25:13,415 --> 01:25:16,535
Speaker 4:  exciting new iPhone year. And I kind of feel like it's nothing between, and

1388
01:25:16,675 --> 01:25:20,455
Speaker 4:  it all seems to me it depends on how exciting Apple really thinks Apple

1389
01:25:20,455 --> 01:25:21,935
Speaker 4:  Intelligence is. I think

1390
01:25:21,935 --> 01:25:25,695
Speaker 5:  It'll be between, 'cause that's, they, they threaded that needle at WW

1391
01:25:25,855 --> 01:25:28,935
Speaker 5:  DC where they took all the really cool stuff that was gonna happen and they're

1392
01:25:28,935 --> 01:25:32,495
Speaker 5:  like, this is happening ai, so we're gonna be like button

1393
01:25:32,925 --> 01:25:35,655
Speaker 5:  cool new phones AI for the rest of the show.

1394
01:25:35,725 --> 01:25:39,615
Speaker 4:  That is true. Like WW DC was actually like very good

1395
01:25:39,915 --> 01:25:42,735
Speaker 4:  and totally overshadowed by the weirdness of Apple Intelligence.

1396
01:25:43,165 --> 01:25:46,295
Speaker 3:  Yeah. I I I think, you know, they've gotta talk about it. It's the thing

1397
01:25:46,295 --> 01:25:49,935
Speaker 3:  that's coming next year and this phone will be the bleeding edge of it. Yeah.

1398
01:25:49,935 --> 01:25:50,815
Speaker 3:  I think the real question is whether

1399
01:25:52,405 --> 01:25:56,255
Speaker 3:  they bring those features to the base model phone, for lack of a

1400
01:25:56,255 --> 01:26:00,135
Speaker 3:  better word, right? Does the iPhone 16 get Apple Intelligence or just the

1401
01:26:00,135 --> 01:26:03,965
Speaker 3:  16 Pro? Because only the 15 Pro is getting it right. If the 16 is

1402
01:26:03,965 --> 01:26:07,845
Speaker 3:  getting it, the regular 16, that's the big upgrade cycle and they're gonna

1403
01:26:08,165 --> 01:26:11,455
Speaker 3:  reintroduce all of the features. Mm. Because that's,

1404
01:26:12,125 --> 01:26:15,975
Speaker 3:  they know that general consumers do not pay attention to wwe C Right.

1405
01:26:15,975 --> 01:26:19,775
Speaker 3:  That's, that's the tech audience. That's developers. Apple releases new

1406
01:26:19,875 --> 01:26:23,775
Speaker 3:  iPhone. It's gonna have the features coming later this Syria be able to tell

1407
01:26:23,885 --> 01:26:27,175
Speaker 3:  Siri to do whatever You can tell Siri to do. That's your Good Morning America.

1408
01:26:27,525 --> 01:26:27,815
Speaker 3:  Yeah.

1409
01:26:28,145 --> 01:26:31,175
Speaker 5:  We've seen some rumors. I think that, that it was gonna come for the 16,

1410
01:26:31,175 --> 01:26:31,455
Speaker 5:  right?

1411
01:26:31,895 --> 01:26:32,015
Speaker 4:  I

1412
01:26:32,095 --> 01:26:34,935
Speaker 3:  I, I assume so. Yeah. But I'm just saying like, that's my guess. That like

1413
01:26:34,935 --> 01:26:37,655
Speaker 3:  this is, that's how they balance that out. Yeah. I'm excited for a bunch

1414
01:26:37,655 --> 01:26:41,255
Speaker 3:  of other stuff. I think we're also expecting new watches, new watches,

1415
01:26:41,995 --> 01:26:42,895
Speaker 3:  new Air pods,

1416
01:26:43,115 --> 01:26:44,495
Speaker 4:  New headphones, potentially new Air Pods

1417
01:26:44,855 --> 01:26:47,255
Speaker 3:  M four Mac mini maybe Right? Is in the mix that

1418
01:26:47,255 --> 01:26:48,095
Speaker 4:  Yep. Is it weird that

1419
01:26:58,895 --> 01:26:59,655
Speaker 4:  want it so bad to give

1420
01:26:59,655 --> 01:27:00,935
Speaker 5:  It to you. If, if, if that is here

