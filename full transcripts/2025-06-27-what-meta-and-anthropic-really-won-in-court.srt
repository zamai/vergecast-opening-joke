1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: e03e7743-8428-430a-a428-033ed90c0ae6
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-1866450124987257775/7581251673948174220/s93290-US-6258s-1751022302.mp3
Description: It's already the heat of summer, and the news keeps coming. Nilay, David, and Jake start the show with a bunch of tech news, including the latest on Tesla's robotaxi launch, some updates on the Trump Phone, new devices from Fairphone and Unihertz, and Meta's shifting strategy for face computers. After that, The Verge's Adi Roberston joins the show to talk about two important AI lawsuits that were both decided this week — one involving Anthropic and the other involving Meta — and what this particular battle means for who will win the AI war. Finally, in the lightning round, it's time for another round of Brendan Carr is a Dummy, some huge news in the HDMI world, and the end of the Blue Screen of Death.


2
00:01:51,745 --> 00:01:53,015
Speaker 5:  Hello and welcome to GR Cast,

3
00:01:53,395 --> 00:01:54,255
Speaker 1:  The flagship podcast

4
00:01:54,795 --> 00:01:56,815
Speaker 5:  Of being the new editor in chief of Vogue.

5
00:01:58,895 --> 00:02:02,815
Speaker 5:  I just put my name in the ring. I don't, I don't think that would be

6
00:02:02,815 --> 00:02:06,695
Speaker 5:  a good idea And I, I love my current job, but it

7
00:02:06,695 --> 00:02:10,055
Speaker 5:  is true that Anna Wintour is stepping down as editor-in-chief of Vogue, the

8
00:02:10,055 --> 00:02:13,175
Speaker 5:  most legendary editor-in-chief of all time. And

9
00:02:14,955 --> 00:02:16,335
Speaker 5:  hey, you know, hey,

10
00:02:16,565 --> 00:02:18,745
Speaker 6:  NELI, 30 more years of this.

11
00:02:21,005 --> 00:02:24,865
Speaker 5:  Hi, I am your friend, NELI. That's Jay Kakis. Hello. Hello. David Pierce

12
00:02:24,865 --> 00:02:25,185
Speaker 5:  is here.

13
00:02:25,385 --> 00:02:28,545
Speaker 6:  I like imagining Vogue as just like

14
00:02:29,185 --> 00:02:32,745
Speaker 6:  everyone is wearing black on every page of every issue of the magazine

15
00:02:33,365 --> 00:02:36,225
Speaker 6:  and, and there's like Packer's logos in the background. Yeah, that'd

16
00:02:36,225 --> 00:02:40,145
Speaker 5:  Great. That vo that that's my vogue. And, and only iPhone fours are

17
00:02:40,145 --> 00:02:41,625
Speaker 5:  allowed the most beautiful phone of all time.

18
00:02:42,925 --> 00:02:45,265
Speaker 6:  You would last two issues, but they would be glorious.

19
00:02:45,735 --> 00:02:49,585
Speaker 5:  I've only met Anna once or twice, and

20
00:02:49,585 --> 00:02:51,785
Speaker 5:  both times she was utterly charming. I, I have to say.

21
00:02:53,365 --> 00:02:57,095
Speaker 5:  But the second time was at the American

22
00:02:57,095 --> 00:03:00,965
Speaker 5:  Society of Magazine Editors Awards, the ASME awards. And we had a table right

23
00:03:00,965 --> 00:03:04,125
Speaker 5:  next to like a Conde Nast table that Anna was at, and they were terrified

24
00:03:04,125 --> 00:03:08,015
Speaker 5:  of her. Like she was sitting there and all of the Conde Nast people

25
00:03:08,015 --> 00:03:11,895
Speaker 5:  were sitting there just in terror. And then our table was just like drunk

26
00:03:11,895 --> 00:03:15,855
Speaker 5:  and cheering like we were having the most fun. And they, they

27
00:03:15,855 --> 00:03:18,215
Speaker 5:  kept staring. And I was like, don't you know? And I was like, I do know.

28
00:03:18,685 --> 00:03:21,975
Speaker 5:  This is our vibe. It's our time now. You're the legacy media.

29
00:03:22,535 --> 00:03:26,215
Speaker 6:  I, I always like thinking about how she must feel about Devil always

30
00:03:26,345 --> 00:03:29,335
Speaker 6:  wears Prada in the way that Mark Zuckerberg feels about the social network

31
00:03:29,695 --> 00:03:33,655
Speaker 6:  because like, he's a been very upfront about how

32
00:03:34,035 --> 00:03:38,015
Speaker 6:  he did not like the way he was portrayed in that movie. I would say history

33
00:03:38,035 --> 00:03:41,255
Speaker 6:  has proven that movie more right over time than I expected. No,

34
00:03:41,255 --> 00:03:43,815
Speaker 5:  Jesse Isenberg was vastly cooler than Mark Zuckerberg in that movie.

35
00:03:43,975 --> 00:03:44,415
Speaker 6:  I mean, yes,

36
00:03:44,765 --> 00:03:47,775
Speaker 5:  Like history, what history has proven is Mark Zuckerberg is a huge nerd who

37
00:03:47,775 --> 00:03:50,095
Speaker 5:  can't outrun his own essential nature. Yes.

38
00:03:50,285 --> 00:03:53,735
Speaker 6:  Fair. Which I think was a, a large part of what that movie is going for.

39
00:03:54,115 --> 00:03:57,815
Speaker 6:  But like, yeah, I, I think that thing where somebody like very

40
00:03:57,855 --> 00:04:01,815
Speaker 6:  famously portrays you and it changes the way everyone thinks about you has

41
00:04:01,815 --> 00:04:05,775
Speaker 6:  gotta be such a weird phenomenon that like most people think when they close

42
00:04:05,775 --> 00:04:08,575
Speaker 6:  their eyes and think of Anna Winter, they think of Meryl Streep. And that's

43
00:04:08,575 --> 00:04:10,655
Speaker 6:  just weird. That's gotta be so strange.

44
00:04:12,255 --> 00:04:15,975
Speaker 5:  It does. And and you know, I watch several product whenever it's on television.

45
00:04:16,295 --> 00:04:18,175
Speaker 5:  I, it's like, it's a great movie. You should watch it. It

46
00:04:18,235 --> 00:04:18,455
Speaker 6:  Is.

47
00:04:19,155 --> 00:04:23,015
Speaker 5:  And Becky has always reminding me that I think I'm Anne Hathaway, but

48
00:04:23,015 --> 00:04:23,615
Speaker 5:  in fact I,

49
00:04:25,355 --> 00:04:29,055
Speaker 5:  I'm now the Meryl Streep character. Interesting. It's over. But anyway, look,

50
00:04:29,055 --> 00:04:32,535
Speaker 5:  if you know anyone over there and

51
00:04:33,035 --> 00:04:36,975
Speaker 5:  you want to see Vogue get super weird. I I'm open to a

52
00:04:36,975 --> 00:04:39,965
Speaker 5:  conversation. I'm not, you know, just

53
00:04:40,685 --> 00:04:42,485
Speaker 6:  The vogue was a series of feeds, you

54
00:04:42,485 --> 00:04:45,805
Speaker 5:  Know what I mean? Yeah. Like, have you thought about going all in on activity

55
00:04:45,905 --> 00:04:49,885
Speaker 5:  pub in the world's most important fashion magazine? There's a lot

56
00:04:49,885 --> 00:04:53,005
Speaker 5:  there. You can unpack a lot there. The world's most important fashion magazine

57
00:04:53,005 --> 00:04:55,925
Speaker 5:  and the age of Instagram and TikTok is a weird thing to be.

58
00:04:57,165 --> 00:05:01,095
Speaker 5:  They, they have a monopoly on taste. Right. It's the cover of Vogue

59
00:05:01,095 --> 00:05:04,735
Speaker 5:  that's important. The contents of Vogue almost are not. And so like that's

60
00:05:04,735 --> 00:05:08,655
Speaker 5:  the dynamic there. And it's super interesting if you're not Anna

61
00:05:08,655 --> 00:05:12,575
Speaker 5:  Wintour and you don't have that monopoly, whether or not Vogue can

62
00:05:13,075 --> 00:05:16,015
Speaker 5:  be the thing that it is in the age of Instagram, TikTok in the age of all

63
00:05:16,015 --> 00:05:17,855
Speaker 5:  the celebrities just being able to publish their own pictures of themselves.

64
00:05:18,765 --> 00:05:22,605
Speaker 5:  It's a thing. I'm just, I'm just saying it's a thing. All right. We, we can

65
00:05:22,605 --> 00:05:26,265
Speaker 5:  set that aside. But if you know anyone and if you know anyone at Kanye ask

66
00:05:26,325 --> 00:05:29,745
Speaker 5:  has, or wanna have a really weird conversation about the future of Vogue,

67
00:05:31,595 --> 00:05:34,775
Speaker 5:  you know, again, I love this job. Either there's not another job, other job

68
00:05:34,775 --> 00:05:35,895
Speaker 5:  really for me. But

69
00:05:36,155 --> 00:05:39,655
Speaker 6:  I'm trying to decide if it would be more fun to like point the verges

70
00:05:39,855 --> 00:05:43,815
Speaker 6:  resources at getting you elected president in 2028 or getting you the job

71
00:05:43,995 --> 00:05:45,455
Speaker 6:  as editor in chief of Vogue.

72
00:05:45,885 --> 00:05:47,175
Speaker 5:  It's like, it's, they're

73
00:05:47,535 --> 00:05:51,415
Speaker 6:  Very different futures, but I find the project of both of

74
00:05:51,415 --> 00:05:52,255
Speaker 6:  them very fun.

75
00:05:52,615 --> 00:05:55,335
Speaker 5:  I do think I can shape the, the direction of the country in either role.

76
00:05:55,515 --> 00:05:57,015
Speaker 5:  It just in very different way.

77
00:05:57,715 --> 00:05:59,205
Speaker 6:  Yeah. Which is more effective.

78
00:05:59,625 --> 00:06:03,485
Speaker 5:  All right. There's a lot of news this week. A lot of updates. We,

79
00:06:03,485 --> 00:06:07,245
Speaker 5:  we got a, there's a big set of AI copyright decisions that

80
00:06:07,255 --> 00:06:10,285
Speaker 5:  Addie is gonna join us to talk about and unpack. I would say earlier today,

81
00:06:10,615 --> 00:06:14,005
Speaker 5:  Addie And I had a conversation and can only be described as we got into our

82
00:06:14,005 --> 00:06:17,565
Speaker 5:  feelings about copyright law because of these AI decisions. So we gotta talk

83
00:06:17,565 --> 00:06:20,485
Speaker 5:  about that. There's updates on the Trump phone.

84
00:06:21,595 --> 00:06:25,085
Speaker 5:  Brendan's got a new commissioner at the FCC, so he just

85
00:06:25,085 --> 00:06:28,245
Speaker 5:  immediately started doing some of the dumbest stuff in the world. There's

86
00:06:28,245 --> 00:06:32,005
Speaker 5:  a, a side note, like there's some FTC dumbness, it's involving

87
00:06:32,035 --> 00:06:35,965
Speaker 5:  just straightforward Bribery, which is pretty good. A lot, just a

88
00:06:35,965 --> 00:06:39,725
Speaker 5:  lot going on in the world. But we should start with a little update

89
00:06:39,825 --> 00:06:43,725
Speaker 5:  on the Robotaxis, the Tesla Robotaxis in Austin. I think

90
00:06:43,725 --> 00:06:47,405
Speaker 5:  David on the Tuesday show, Andy was on, we were like a day in. Now we're

91
00:06:47,405 --> 00:06:50,205
Speaker 5:  several days in. I would say the obvious has occurred.

92
00:06:50,585 --> 00:06:53,685
Speaker 6:  It was very funny. So we, Andy And I talked about it a bunch and he kept

93
00:06:53,685 --> 00:06:57,605
Speaker 6:  being like, look, we, we should be cautious. Tesla should be cautious,

94
00:06:57,905 --> 00:07:01,325
Speaker 6:  but it's only been a day. We'll, we'll see what happens, you know, hope for

95
00:07:01,325 --> 00:07:04,285
Speaker 6:  the best, expect the worst. And like, boy did it just kind of go sideways

96
00:07:04,835 --> 00:07:06,085
Speaker 6:  from like minute one.

97
00:07:06,465 --> 00:07:10,445
Speaker 5:  So there's a bunch of videos now there's like a media component to

98
00:07:10,445 --> 00:07:13,565
Speaker 5:  this where the only people who seem to have access are Tesla

99
00:07:13,875 --> 00:07:17,485
Speaker 5:  influencers and like hardcore Elon Musk ex

100
00:07:17,595 --> 00:07:18,085
Speaker 5:  posters.

101
00:07:20,475 --> 00:07:23,295
Speaker 5:  And they're all like, we're the media now. And it's like, are are you?

102
00:07:24,425 --> 00:07:27,365
Speaker 5:  But then they're all just doing this like incredible live streaming. Like

103
00:07:27,365 --> 00:07:31,225
Speaker 5:  everything is being posted. And so even through the filter

104
00:07:31,325 --> 00:07:35,225
Speaker 5:  of a bunch of people who's, who hold themselves up as being

105
00:07:35,225 --> 00:07:38,985
Speaker 5:  Tesla boosters, there's just like endless videos of these,

106
00:07:38,985 --> 00:07:42,825
Speaker 5:  these cars just like making mistakes. And the net result of it, just

107
00:07:42,935 --> 00:07:46,625
Speaker 5:  from my perspective as a person watching these videos is that it's just full

108
00:07:46,625 --> 00:07:50,115
Speaker 5:  self-driving, right? There're yes, there's safety

109
00:07:50,115 --> 00:07:53,935
Speaker 5:  monitors in the cars and there's that one

110
00:07:53,935 --> 00:07:57,775
Speaker 5:  picture of one steering wheel in the background of one photo. You know,

111
00:07:57,775 --> 00:08:00,655
Speaker 5:  like maybe they're all being remote controlled, you know, there's all these

112
00:08:00,655 --> 00:08:03,415
Speaker 5:  rumors about this stuff, but you kind of watch it and you're like, oh, this

113
00:08:03,415 --> 00:08:06,415
Speaker 5:  is just like watching any other full self-driving video. Only

114
00:08:07,845 --> 00:08:11,735
Speaker 5:  like the cars are making mistakes without a driver

115
00:08:11,755 --> 00:08:13,735
Speaker 5:  so that it all feels way more dangerous.

116
00:08:13,735 --> 00:08:16,175
Speaker 6:  Right? They're all, only they're sitting in the backseat. Right. Is like

117
00:08:16,695 --> 00:08:20,255
Speaker 6:  a pretty meaningful change in how Right. We're actually applying this

118
00:08:20,255 --> 00:08:20,735
Speaker 6:  technology

119
00:08:20,925 --> 00:08:23,695
Speaker 5:  When FSD messes up and the person's in the driver's seat. They're like, I'm

120
00:08:23,695 --> 00:08:26,295
Speaker 5:  taking control. And they're like, oh, that was scary right Now. It's like,

121
00:08:26,675 --> 00:08:27,095
Speaker 5:  whoops.

122
00:08:28,125 --> 00:08:31,135
Speaker 7:  Yeah. What are these safety monitors actually supposed to do?

123
00:08:32,005 --> 00:08:35,975
Speaker 7:  Just like, like comfort you when when like you rear end somebody. Like it's

124
00:08:35,975 --> 00:08:38,815
Speaker 7:  all right, we, we, we, this, this happens from time to time.

125
00:08:38,905 --> 00:08:42,695
Speaker 6:  There have been a bunch of these And I think when the safety

126
00:08:42,695 --> 00:08:46,495
Speaker 6:  monitor does and doesn't get involved has been sort of fascinating. Like,

127
00:08:46,625 --> 00:08:50,575
Speaker 6:  there, there were, there was the one I think that has been kind of all over

128
00:08:50,575 --> 00:08:54,415
Speaker 6:  the place and Andy And I talked about this on Tuesday where the car

129
00:08:54,415 --> 00:08:58,205
Speaker 6:  just veers into the wrong lane for a second. And the, the safety

130
00:08:58,205 --> 00:09:01,805
Speaker 6:  monitor doesn't seem to flinch that this is just like, it's fine,

131
00:09:01,895 --> 00:09:05,165
Speaker 6:  we're just gonna go in the wrong lane because we thought the turn lane started

132
00:09:05,465 --> 00:09:08,965
Speaker 6:  before it did. But then there are a couple of other ones where like

133
00:09:08,965 --> 00:09:12,525
Speaker 6:  something actually goes where and the car just like

134
00:09:13,175 --> 00:09:16,245
Speaker 6:  stops in the middle of the road and then the safety monitor does get involved.

135
00:09:16,465 --> 00:09:20,285
Speaker 6:  So it seems very much like the safety monitors instructed to

136
00:09:20,285 --> 00:09:23,245
Speaker 6:  basically not do anything no matter what, unless

137
00:09:24,185 --> 00:09:28,005
Speaker 6:  it has like officially broken, which is kind of a

138
00:09:28,075 --> 00:09:31,725
Speaker 6:  wild set of instructions to give to a safety monitor in a car like this

139
00:09:31,945 --> 00:09:35,905
Speaker 6:  in a project as early as this. But I just, I don't know how to

140
00:09:35,905 --> 00:09:38,025
Speaker 6:  read it any other way. Like they, they should have been getting involved

141
00:09:38,025 --> 00:09:40,025
Speaker 6:  more if they were allowed to be getting involved more.

142
00:09:40,055 --> 00:09:43,345
Speaker 5:  There's definitely one where the car gets stuck kinda like between two cars.

143
00:09:43,375 --> 00:09:45,305
Speaker 5:  It's a tight and the safety monitor drives it out.

144
00:09:45,415 --> 00:09:47,225
Speaker 6:  Okay. I didn't see that one. That's something, I

145
00:09:47,225 --> 00:09:50,985
Speaker 5:  Get it right. It's a very limited beta slash pilot of this

146
00:09:50,995 --> 00:09:53,265
Speaker 5:  stuff. There's not a lot of these cars,

147
00:09:55,135 --> 00:09:58,865
Speaker 5:  it's a trial. Sure. They're on public roads with other people and other cars.

148
00:09:59,035 --> 00:10:02,825
Speaker 5:  Right. It does seem to me like the safety monitor should just be in the driver's

149
00:10:02,825 --> 00:10:06,785
Speaker 5:  seat. Especially if it, again, as it appears

150
00:10:07,175 --> 00:10:10,555
Speaker 5:  it's just full self-driving. Like it's not more,

151
00:10:11,015 --> 00:10:14,755
Speaker 5:  if they were faking it and there was a warehouse full of people remote

152
00:10:14,755 --> 00:10:18,375
Speaker 5:  driving cars, I'd be like, yeah, safety monitor in the RC, you know, but

153
00:10:18,375 --> 00:10:21,935
Speaker 5:  like you, you watch these videos and

154
00:10:22,235 --> 00:10:25,695
Speaker 5:  it, it feels anyway from my perspective, like it's just full self-driving

155
00:10:26,395 --> 00:10:30,215
Speaker 5:  and then you get to the fact that most people can't access the cars,

156
00:10:30,225 --> 00:10:34,175
Speaker 5:  right? It is just this limited group at least posting content

157
00:10:34,765 --> 00:10:38,055
Speaker 5:  that is very apt to supporting Tesla.

158
00:10:38,595 --> 00:10:42,015
Speaker 5:  And you have this like war against the media narrative, right? And I'm just

159
00:10:42,015 --> 00:10:45,655
Speaker 5:  gonna say this as many times I can access is poison.

160
00:10:46,395 --> 00:10:48,815
Speaker 5:  And if you hit capture where the companies are covered, you're gonna make

161
00:10:48,815 --> 00:10:51,255
Speaker 5:  bad shit over time and you're gonna lose your audience. And that's just the

162
00:10:51,255 --> 00:10:55,205
Speaker 5:  way it goes. And all of these folks who are like,

163
00:10:55,205 --> 00:10:57,485
Speaker 5:  the media doesn't just gonna have the wrong narrative about Tesla. If Tesla

164
00:10:57,625 --> 00:11:01,245
Speaker 5:  was confident in the product, they would just let reporters

165
00:11:01,355 --> 00:11:04,925
Speaker 5:  into it and let them drive around and say, this is amazing. You could just

166
00:11:04,925 --> 00:11:08,845
Speaker 5:  take Waymo. 'cause Waymo's confident in its product. It think it thinks

167
00:11:08,845 --> 00:11:12,525
Speaker 5:  the products expanding the markets, it operates in this,

168
00:11:13,035 --> 00:11:16,405
Speaker 5:  this thing where you think Tesla has rewarded these influencers

169
00:11:16,825 --> 00:11:20,385
Speaker 5:  is actually a sign of deep insecurity. Like the most

170
00:11:20,385 --> 00:11:24,225
Speaker 5:  insecurity And I, you can see it. I mean we, this is, this is our world,

171
00:11:24,225 --> 00:11:28,065
Speaker 5:  right? You see it with every company when they're not proud of their products,

172
00:11:28,415 --> 00:11:32,265
Speaker 5:  they close the distribution, they limit the access. You know, we play

173
00:11:32,265 --> 00:11:35,865
Speaker 5:  these games, but we, our rules are clear. I if you're listening to this,

174
00:11:35,865 --> 00:11:38,345
Speaker 5:  you know, our rules is clear. You can't buy us, you can't buy our branded

175
00:11:38,345 --> 00:11:41,225
Speaker 5:  content. We're unsponsored for flight, all this stuff. Because that's how

176
00:11:41,225 --> 00:11:45,065
Speaker 5:  we manage the access games. We just draw the

177
00:11:45,065 --> 00:11:48,465
Speaker 5:  lines as clear as we can. We say, look, we won't play the games. If we don't

178
00:11:48,465 --> 00:11:52,415
Speaker 5:  get the access, we'll go and do the next thing. And I like, there's the

179
00:11:52,415 --> 00:11:55,885
Speaker 5:  video, there's the, there's the one video of the woman in the backseat that

180
00:11:55,885 --> 00:11:59,805
Speaker 5:  the car breaks so hard, it throws the phone out of her hand and then she

181
00:11:59,805 --> 00:12:03,585
Speaker 5:  starts apologizing for the car and it's like, oh, this is bad.

182
00:12:03,735 --> 00:12:07,705
Speaker 5:  Like this is a really bad media ecosystem. If you think that the

183
00:12:07,765 --> 00:12:11,745
Speaker 5:  reward is the, the car breaks so hard while you're not wearing a

184
00:12:11,905 --> 00:12:14,705
Speaker 5:  seatbelt, the phone flies. Sorry, we we have it. Do you wanna just play it

185
00:12:14,705 --> 00:12:14,865
Speaker 5:  David?

186
00:12:15,585 --> 00:12:15,865
Speaker 6:  I do.

187
00:12:16,955 --> 00:12:20,785
Speaker 8:  Let's find out. You guys know we're not lying because we have that stopwatch

188
00:12:20,785 --> 00:12:24,545
Speaker 8:  going, whoa. Alright, so we just slammed

189
00:12:24,605 --> 00:12:28,305
Speaker 8:  on the brake. I'm not exactly sure what just happened,

190
00:12:28,885 --> 00:12:32,785
Speaker 8:  but the car thought it saw something and this

191
00:12:32,785 --> 00:12:36,305
Speaker 8:  happens in full self-driving. This is something that does happen. That's

192
00:12:36,305 --> 00:12:39,465
Speaker 8:  something that people have talked about being one of the limitations

193
00:12:40,085 --> 00:12:44,065
Speaker 8:  of full self-driving with Robax is that occasionally it SLAM's

194
00:12:44,065 --> 00:12:46,145
Speaker 8:  on the brakes side of nowhere. You guys kind of saw me react.

195
00:12:48,375 --> 00:12:49,585
Speaker 6:  Yeah, I

196
00:12:49,585 --> 00:12:51,465
Speaker 5:  Mean you get the idea. What are we doing?

197
00:12:52,125 --> 00:12:55,425
Speaker 7:  It happens sometimes guys, sometimes your car just

198
00:12:55,915 --> 00:12:58,145
Speaker 7:  slams the brakes on you and you just sit there

199
00:12:59,015 --> 00:13:02,585
Speaker 5:  With no, the safety, presumably a safety monitor just saying nothing while

200
00:13:02,585 --> 00:13:03,305
Speaker 5:  this video is happening.

201
00:13:03,885 --> 00:13:07,705
Speaker 6:  It is sort of fascinating that this is what Tesla has conditioned

202
00:13:07,725 --> 00:13:09,225
Speaker 6:  its fans to believe is

203
00:13:09,615 --> 00:13:12,905
Speaker 5:  Just how this works. No, I'm saying it's more sinister than that. I'm saying

204
00:13:12,905 --> 00:13:16,825
Speaker 5:  it has conditioned access on that behavior. Oh sure. And

205
00:13:16,825 --> 00:13:20,585
Speaker 5:  that feedback loop is actually bad for the comp. It's bad for people,

206
00:13:21,035 --> 00:13:23,785
Speaker 5:  right? It's bad for people like great products. It's also bad for the company

207
00:13:24,705 --> 00:13:27,305
Speaker 5:  'cause they're not actually getting the negative feedback they need, right?

208
00:13:27,305 --> 00:13:30,025
Speaker 5:  The market is not actually providing any information. They have a bunch of

209
00:13:30,025 --> 00:13:32,585
Speaker 5:  influencers who are afraid of their access being taken away and not getting

210
00:13:32,585 --> 00:13:36,415
Speaker 5:  invited to the next cyber rodeo or whatever. And then they're yelling at

211
00:13:36,415 --> 00:13:40,315
Speaker 5:  reporters who are just trying to do their jobs, but they're

212
00:13:40,315 --> 00:13:44,235
Speaker 5:  like, you can literally hear the lack of objectivity there. And look, e everyone

213
00:13:44,235 --> 00:13:48,035
Speaker 5:  can yell at us. You somehow, in my career, I've gone from being reliably

214
00:13:48,035 --> 00:13:51,515
Speaker 5:  called an Apple shield to being called an Apple hater. Great, I

215
00:13:51,785 --> 00:13:55,315
Speaker 5:  sure whatever you can yell at us about our perceived biases,

216
00:13:55,855 --> 00:13:59,795
Speaker 5:  but we have our rules. Like you can just read the policy. Like those, those

217
00:13:59,795 --> 00:14:03,355
Speaker 5:  rules exist to, to at least like

218
00:14:03,625 --> 00:14:06,915
Speaker 5:  provide a set of guidelines for what you expect from our behavior and how

219
00:14:06,915 --> 00:14:10,355
Speaker 5:  we think about access and all these other things that we, you know,

220
00:14:10,665 --> 00:14:13,475
Speaker 5:  that are part of our job and our careers and the, the media that we make.

221
00:14:14,035 --> 00:14:17,755
Speaker 5:  I but you can hear literally in these videos, the influencers thinking

222
00:14:18,455 --> 00:14:22,445
Speaker 5:  if my access will get taken away And I I

223
00:14:22,445 --> 00:14:25,205
Speaker 5:  need to apologize for this car fucking up. And it's like, no, a real person,

224
00:14:25,405 --> 00:14:29,085
Speaker 5:  a real customer does not stop and say this happens.

225
00:14:29,715 --> 00:14:32,725
Speaker 5:  They just get the fuck out the car and like, that's your problem.

226
00:14:33,435 --> 00:14:36,565
Speaker 6:  Yeah. The video did make me think about like the terrible Uber rides that

227
00:14:36,565 --> 00:14:40,285
Speaker 6:  I've had in the past and like, they happen occasionally and you have them,

228
00:14:41,665 --> 00:14:45,525
Speaker 6:  but like there's a person in the front seat you can tell to like one year

229
00:14:45,525 --> 00:14:49,125
Speaker 6:  at CESI had an Uber driver who drove me at

230
00:14:49,405 --> 00:14:53,245
Speaker 6:  i i 45 miles an hour through parking lots of

231
00:14:53,395 --> 00:14:57,285
Speaker 6:  casinos in Las Vegas to get me to the airport. And we

232
00:14:57,285 --> 00:15:00,325
Speaker 6:  were just like launching ourselves over speed bumps. And for the first time

233
00:15:00,325 --> 00:15:04,245
Speaker 6:  ever I had to be like, guy, you, you gotta stop dude. Like I'm not in a

234
00:15:04,245 --> 00:15:08,045
Speaker 6:  hurry. Please stop driving like this. And he did and it was fine.

235
00:15:08,585 --> 00:15:11,805
Speaker 6:  And I was like, I went from being like, I'm going to die in the back of this

236
00:15:11,835 --> 00:15:12,845
Speaker 6:  like dodge charger.

237
00:15:13,105 --> 00:15:14,445
Speaker 5:  Of course it was a Dodge Charger

238
00:15:14,705 --> 00:15:18,245
Speaker 6:  To, it was okay 'cause there was a person I could tell and it was just like,

239
00:15:18,245 --> 00:15:22,125
Speaker 6:  it's, it's not that these things are

240
00:15:22,725 --> 00:15:25,285
Speaker 6:  uniquely bad drivers. It's it's that like we have a chain of understanding

241
00:15:25,285 --> 00:15:29,165
Speaker 6:  how this stuff works that we just, none of this has made any sense. Again,

242
00:15:29,165 --> 00:15:32,285
Speaker 6:  it goes back to like put the safety monitor in front of the steering wheel.

243
00:15:32,315 --> 00:15:32,965
Speaker 6:  Like Yeah,

244
00:15:33,225 --> 00:15:37,085
Speaker 5:  But he can't. Right. I know why he should And I know why Elon should put

245
00:15:37,085 --> 00:15:40,645
Speaker 5:  the safety monitor behind the wheel. I know why he can't because the future

246
00:15:40,705 --> 00:15:44,605
Speaker 5:  of Tesla's valuation depends on robo taxis working not on

247
00:15:44,605 --> 00:15:45,885
Speaker 5:  selling new or better cars.

248
00:15:46,065 --> 00:15:48,085
Speaker 6:  That's why this whole thing exists. Yeah.

249
00:15:48,835 --> 00:15:52,495
Speaker 5:  Anyhow. Well the more time will pass, presumably more people will get access

250
00:15:52,495 --> 00:15:56,455
Speaker 5:  to this thing. I'm just, I I've been bemused it,

251
00:15:56,455 --> 00:15:59,415
Speaker 5:  it feels like a bunch of people are entering

252
00:16:00,555 --> 00:16:04,515
Speaker 5:  a kind of journalism where the compromises of access will be made

253
00:16:04,515 --> 00:16:06,955
Speaker 5:  clear and some people will learn the lesson and some people won't. And it

254
00:16:06,955 --> 00:16:10,875
Speaker 5:  all kind of feels like speed running to me. Like you're

255
00:16:10,875 --> 00:16:14,395
Speaker 5:  about to have a whole bunch of experiences here. But I'm, I'm watching these

256
00:16:14,395 --> 00:16:17,665
Speaker 5:  videos and I'm like, oh, the thing doesn't work as well as it should. Right?

257
00:16:17,965 --> 00:16:20,345
Speaker 5:  And if you actually wanna compete with Waymo, if you actually wanna build

258
00:16:20,345 --> 00:16:24,105
Speaker 5:  those stuff, it needs to work a lot better than this. A lot faster. And like

259
00:16:24,225 --> 00:16:28,105
Speaker 5:  everything with Tesla, maybe it'll just take a long time. But I don know

260
00:16:28,125 --> 00:16:30,865
Speaker 5:  I'm gonna continue watching this list of Reddit video updates.

261
00:16:31,055 --> 00:16:34,945
Speaker 6:  They're very good. I really, it's Reddit has done an admirable job

262
00:16:34,965 --> 00:16:38,585
Speaker 6:  of putting together timestamped versions of hilariously bad things happening

263
00:16:38,585 --> 00:16:40,705
Speaker 6:  to these people. Yeah. I highly recommend it.

264
00:16:40,845 --> 00:16:44,585
Speaker 5:  By the way, if you wanna be a a auto journalists and you think Tesla is much,

265
00:16:44,805 --> 00:16:48,545
Speaker 5:  by all means, I'm just, I will just say it again. Access is poison.

266
00:16:49,205 --> 00:16:52,995
Speaker 5:  It will it, if you chase access, it will kill you. And

267
00:16:53,475 --> 00:16:56,435
Speaker 5:  I say this to our staff all the time, the less access you need, the more

268
00:16:56,435 --> 00:17:00,165
Speaker 5:  you get. And that, that's a, that's a dynamic that's very hard. Again, I

269
00:17:00,165 --> 00:17:03,805
Speaker 5:  think they're just speed running media in like a very particular way. We'll

270
00:17:03,805 --> 00:17:07,595
Speaker 5:  see how it goes. Alright. Speaking of access, poison

271
00:17:07,735 --> 00:17:10,755
Speaker 5:  and speed runs. There's an update on a Trump phone.

272
00:17:12,185 --> 00:17:14,915
Speaker 6:  Jake, you helped me investigate this. Do you want to take us through what

273
00:17:15,075 --> 00:17:17,755
Speaker 6:  happened with the Trump phone this week? Yeah, well if you guys will recall

274
00:17:18,205 --> 00:17:19,155
Speaker 7:  About a week ago

275
00:17:20,715 --> 00:17:23,755
Speaker 7:  Trump organization announced Trump Mobile. The Trump mobile announced

276
00:17:24,935 --> 00:17:28,075
Speaker 7:  the T one phone 8,002

277
00:17:28,705 --> 00:17:32,355
Speaker 7:  Gold Edition, believe the name is. And I think we came on here and we said,

278
00:17:32,895 --> 00:17:36,235
Speaker 7:  we don't believe that a lot of this is, is accurate.

279
00:17:37,415 --> 00:17:40,435
Speaker 7:  One of the biggest things was that they said it was going to be made in the

280
00:17:40,435 --> 00:17:40,995
Speaker 7:  United States.

281
00:17:42,695 --> 00:17:45,915
Speaker 7:  Eli went on TV and shouted that that was a lie.

282
00:17:46,655 --> 00:17:49,685
Speaker 5:  That's true. I will say I, I was not on the show last week And I listened

283
00:17:49,685 --> 00:17:53,045
Speaker 5:  to it And I had the, I had what I believe is the very common experience of

284
00:17:53,045 --> 00:17:56,925
Speaker 5:  yelling at The Vergecast. So I feel a lot of empathy for the

285
00:17:57,045 --> 00:17:59,045
Speaker 5:  audience. But CC invited me on, I was like, this is fake.

286
00:17:59,825 --> 00:18:01,245
Speaker 7:  And lo and behold,

287
00:18:02,825 --> 00:18:06,415
Speaker 7:  David, earlier this week, scrolling through their website

288
00:18:07,265 --> 00:18:10,725
Speaker 7:  sees that they've changed the website used to say made in the U

289
00:18:10,725 --> 00:18:14,685
Speaker 7:  United States. Now it says what, what's the exact phrase, David? Oh

290
00:18:14,685 --> 00:18:17,805
Speaker 6:  My god, I'm so excited to read this website to you. First of all,

291
00:18:18,425 --> 00:18:21,605
Speaker 6:  shouts to Simon who saw this before I did and sent it to me.

292
00:18:23,185 --> 00:18:26,525
Speaker 6:  Can I just, can I just like read you a bunch of copy please. This is the,

293
00:18:26,625 --> 00:18:30,485
Speaker 6:  the phone page of the T one on the

294
00:18:30,485 --> 00:18:33,765
Speaker 6:  Trump mobile website says at the very top, meet the T one

295
00:18:34,035 --> 00:18:38,005
Speaker 6:  premium performance proudly American. This is the new

296
00:18:38,005 --> 00:18:41,325
Speaker 6:  tagline of this phone, the T one trademark. They put trademark everywhere,

297
00:18:41,325 --> 00:18:44,405
Speaker 6:  which I really respect isn't just another smartphone, it's a bold step towards

298
00:18:44,685 --> 00:18:47,805
Speaker 6:  wireless independence. That doesn't mean anything designed with American

299
00:18:47,805 --> 00:18:51,685
Speaker 6:  values in mind. The T one trademark delivers top tier performance, sleek

300
00:18:51,685 --> 00:18:55,365
Speaker 6:  design and powerful features. All without the inflated price tag designed

301
00:18:55,365 --> 00:18:58,965
Speaker 6:  with American values in mind is what is replacing made in the

302
00:18:58,965 --> 00:19:02,725
Speaker 6:  USA. We also have with American hands behind every

303
00:19:02,725 --> 00:19:06,525
Speaker 6:  device, we bring care precision and trusted quality to every detail. Are

304
00:19:06,525 --> 00:19:06,685
Speaker 6:  those

305
00:19:06,685 --> 00:19:10,605
Speaker 5:  Just Don Jr's hands? Like he's just, he's just collecting his checks.

306
00:19:10,955 --> 00:19:14,725
Speaker 6:  There's just some, some American has put hands

307
00:19:15,115 --> 00:19:15,405
Speaker 6:  near

308
00:19:15,525 --> 00:19:18,525
Speaker 7:  A phone. An American, yeah, that's all we know for an American hand has put

309
00:19:18,525 --> 00:19:21,725
Speaker 7:  their, has has clicked the Ali Express order button.

310
00:19:22,355 --> 00:19:26,125
Speaker 7:  That is what happened to you. I am telling you, you go to Olly

311
00:19:26,125 --> 00:19:29,965
Speaker 7:  Express, you type in big Android phone and you will find what is something

312
00:19:29,965 --> 00:19:33,845
Speaker 7:  that is very close to the T one phone. And I, the

313
00:19:33,845 --> 00:19:36,845
Speaker 7:  other thing that David noticed I is that they've, they've changed the specs

314
00:19:36,905 --> 00:19:40,885
Speaker 7:  as well. The, the phone is not going to ship a as the exact phone

315
00:19:41,045 --> 00:19:44,045
Speaker 7:  that they announced a week ago, which means that like

316
00:19:45,115 --> 00:19:48,765
Speaker 7:  they clearly were not confident in what they announced. I think we all said

317
00:19:48,765 --> 00:19:51,565
Speaker 7:  that would be the case and we were already starting to see it morph in front

318
00:19:51,565 --> 00:19:55,405
Speaker 7:  of our eyes as they realized that it was, it was not correct. David, what

319
00:19:55,405 --> 00:19:57,605
Speaker 7:  is the updated spec? They, I believe they changed the RAM and they changed

320
00:19:57,605 --> 00:19:58,925
Speaker 7:  it screen size. So the

321
00:19:58,985 --> 00:20:02,325
Speaker 6:  Ram is now gone from the website straight. Okay.

322
00:20:02,945 --> 00:20:06,005
Speaker 6:  It normally, there was a, there was, if I remember right, there was a section

323
00:20:06,105 --> 00:20:09,605
Speaker 6:  on the site that said like processor and RAM and it had no processor listed

324
00:20:09,625 --> 00:20:13,365
Speaker 6:  but it said 12 gigs of ram. Now that whole section is gone. So we still don't

325
00:20:13,365 --> 00:20:17,325
Speaker 6:  know the processor and now there is no ram apparently. The other one, which

326
00:20:17,325 --> 00:20:21,245
Speaker 6:  I think is maybe the bigger signal that

327
00:20:21,245 --> 00:20:24,885
Speaker 6:  something big has shifted here is it has gone from a

328
00:20:24,945 --> 00:20:28,765
Speaker 6:  6.78 inch screen to a 6.25 inch screen,

329
00:20:29,015 --> 00:20:32,925
Speaker 6:  which I would just point out is a very different phone. Yeah. If you lop

330
00:20:32,925 --> 00:20:36,165
Speaker 6:  off a half inch of the screen, you've gone from an iPhone plus to an iPhone.

331
00:20:36,355 --> 00:20:39,525
Speaker 6:  Like that's just a different thing. And

332
00:20:40,115 --> 00:20:43,965
Speaker 6:  like the, they fixed a couple of things like now it

333
00:20:43,965 --> 00:20:47,525
Speaker 6:  says battery as battery instead of camera, which is very exciting.

334
00:20:47,635 --> 00:20:51,285
Speaker 5:  It's very good. Although I was always excited for a 5,000 milliamp hour camera.

335
00:20:51,305 --> 00:20:52,765
Speaker 5:  Camera. It was gonna be, it was gonna be

336
00:20:52,765 --> 00:20:53,205
Speaker 6:  A sick camera.

337
00:20:53,915 --> 00:20:54,205
Speaker 5:  That

338
00:20:54,205 --> 00:20:57,805
Speaker 6:  Camera will power your house somehow. We don't know yet. It also,

339
00:20:57,805 --> 00:21:00,485
Speaker 6:  instead of saying it's gonna ship in September, it just says coming later

340
00:21:00,485 --> 00:21:04,205
Speaker 6:  this year, even the coming soon is gone. Which is very funny.

341
00:21:05,425 --> 00:21:09,405
Speaker 6:  So I like, the only thing I can think of is that they got

342
00:21:09,985 --> 00:21:13,845
Speaker 6:  in some kind of legal hot water over the phrase made in the USA,

343
00:21:13,975 --> 00:21:16,885
Speaker 6:  which is a thing that you have to follow certain rules to be able to say

344
00:21:16,885 --> 00:21:20,325
Speaker 6:  about a device or any product. And

345
00:21:20,955 --> 00:21:24,565
Speaker 6:  also that they went from having one

346
00:21:24,805 --> 00:21:28,005
Speaker 6:  supplier to having another supplier. It's the only thing I can think of is

347
00:21:28,005 --> 00:21:30,765
Speaker 6:  that whatever they bought on or thought they were gonna buy on Ali Express,

348
00:21:31,145 --> 00:21:34,245
Speaker 6:  now they're buying something else with a half inch smaller screen. There's

349
00:21:34,245 --> 00:21:36,685
Speaker 6:  just no reason that happens in a week other than that.

350
00:21:36,815 --> 00:21:38,605
Speaker 5:  Especially at 4 99. Right?

351
00:21:38,835 --> 00:21:42,245
Speaker 6:  Yeah. The render is still the same, the price is still the same. I'm still

352
00:21:42,245 --> 00:21:46,165
Speaker 6:  convinced this phone does not exist and will never exist. But even the

353
00:21:46,165 --> 00:21:49,445
Speaker 6:  thing that doesn't exist makes less sense every

354
00:21:49,465 --> 00:21:49,685
Speaker 5:  Day.

355
00:21:51,105 --> 00:21:54,765
Speaker 7:  The made in the US thing was just particularly egregious, right? Like i I

356
00:21:55,035 --> 00:21:58,405
Speaker 7:  from the start, it was wildly obvious that that could not be true.

357
00:22:00,065 --> 00:22:03,965
Speaker 7:  And I guess after enough people asked about it, they realized maybe we

358
00:22:03,965 --> 00:22:07,725
Speaker 7:  shouldn't lie about this one. Which I guess I, I don't wanna say kudos on

359
00:22:07,725 --> 00:22:11,565
Speaker 7:  that, but you know, good, good for stopping lying that that is

360
00:22:11,605 --> 00:22:12,125
Speaker 7:  a positive.

361
00:22:12,435 --> 00:22:13,645
Speaker 6:  It's not, not lying,

362
00:22:14,465 --> 00:22:18,085
Speaker 5:  You know what I mean? The phone doesn't exist. I'll give them credit for

363
00:22:18,305 --> 00:22:22,285
Speaker 5:  for not lying when they ship us a phone. I believe, Jake you

364
00:22:22,285 --> 00:22:23,485
Speaker 5:  ordered one, right? The

365
00:22:23,565 --> 00:22:25,965
Speaker 7:  Verge has ordered two, two of them. Two sh

366
00:22:26,535 --> 00:22:28,805
Speaker 5:  We're a thousand dollars deeper than the Trump phones.

367
00:22:29,075 --> 00:22:32,165
Speaker 7:  Well, yes. Well so Sarah Smithers are

368
00:22:32,865 --> 00:22:36,005
Speaker 5:  Who are, it's take one day off the show, Jake's like blow a grand on Trump

369
00:22:36,005 --> 00:22:36,965
Speaker 5:  phones. We

370
00:22:37,025 --> 00:22:40,365
Speaker 7:  Listen, somebody needs to find out if these phones are real.

371
00:22:41,815 --> 00:22:45,325
Speaker 7:  Sarah Smithers, our wonderful editorial operations director

372
00:22:46,625 --> 00:22:50,365
Speaker 7:  has spent a week trying to order these things. It turns out

373
00:22:50,985 --> 00:22:54,405
Speaker 7:  for the first week they did not have an option to order them if you lived

374
00:22:54,505 --> 00:22:55,845
Speaker 7:  in Washington DC

375
00:22:56,305 --> 00:22:56,605
Speaker 5:  Oh, good.

376
00:22:57,185 --> 00:23:01,165
Speaker 7:  And so we had to keep emailing them saying, Hey, this is part of the

377
00:23:01,165 --> 00:23:03,685
Speaker 7:  United States. Could you please accept our

378
00:23:03,685 --> 00:23:06,205
Speaker 5:  Order? Because our corporate headquarters is in DC Yes,

379
00:23:06,525 --> 00:23:07,085
Speaker 6:  I understand.

380
00:23:07,425 --> 00:23:10,965
Speaker 7:  So we will find out for all of you if this phone exists,

381
00:23:11,365 --> 00:23:14,885
Speaker 7:  if it is any good. And if they, as the website

382
00:23:14,965 --> 00:23:18,845
Speaker 7:  promises randomly start billing you recurring charges at

383
00:23:18,845 --> 00:23:19,205
Speaker 7:  some point.

384
00:23:19,545 --> 00:23:23,045
Speaker 5:  So many, many people have asked us to review the phone

385
00:23:23,855 --> 00:23:27,475
Speaker 5:  and I'm, I'm confident we will. I just, I wanna set some

386
00:23:27,475 --> 00:23:31,445
Speaker 5:  expectations that it's not like the, a review

387
00:23:31,445 --> 00:23:34,805
Speaker 5:  of a mid-range Android phone is often a banger. Do you know what I mean?

388
00:23:34,805 --> 00:23:38,485
Speaker 5:  Like it's a mid-range Andrew. Yeah. You know, you already know what it is.

389
00:23:39,065 --> 00:23:42,765
Speaker 5:  The thing that you want from us is to point out the obvious lies. Yeah.

390
00:23:42,815 --> 00:23:46,725
Speaker 5:  Which is like, you almost don't need the phone to do it, but

391
00:23:46,825 --> 00:23:50,525
Speaker 5:  at best, like the best thing this can be is a

392
00:23:50,525 --> 00:23:52,805
Speaker 5:  competently executed mid-range Android phone.

393
00:23:52,995 --> 00:23:56,885
Speaker 6:  Yeah. That's literally the, the 100th percentile outcome for

394
00:23:56,885 --> 00:24:00,645
Speaker 6:  this thing. Yeah. Is a, it exists and B it's fine. That is the best

395
00:24:01,405 --> 00:24:03,045
Speaker 6:  possible outcome for the Trump phone.

396
00:24:03,565 --> 00:24:07,525
Speaker 5:  I suspect there will be a yawning chasm between the device

397
00:24:08,115 --> 00:24:11,925
Speaker 5:  shipped to us the two devices apparently that will be shipped to us and that

398
00:24:11,925 --> 00:24:15,565
Speaker 5:  outcome and that, you know, that's the point of a review. But I'm just trying

399
00:24:15,565 --> 00:24:18,965
Speaker 5:  to I don't think you're gonna open it. So like light on fire, I think it's

400
00:24:18,965 --> 00:24:22,245
Speaker 5:  gonna be a mid range Android phone with some shit bundled software.

401
00:24:22,785 --> 00:24:26,485
Speaker 6:  All right. Neli Trump mobile project Gen five SISs.

402
00:24:27,065 --> 00:24:31,005
Speaker 6:  You have to bet on one as a grand success in the

403
00:24:31,005 --> 00:24:31,205
Speaker 6:  future.

404
00:24:32,145 --> 00:24:35,805
Speaker 5:  Oh it's, it's Project Gen five sis. Wow. EAs easily big,

405
00:24:36,105 --> 00:24:40,045
Speaker 5:  easily Project Gen five sis. And I'm only saying this

406
00:24:40,045 --> 00:24:43,965
Speaker 5:  because it, to the extent that it exists,

407
00:24:44,265 --> 00:24:47,525
Speaker 6:  You were about to say it exists and they were, there was about to be a party

408
00:24:47,665 --> 00:24:49,565
Speaker 6:  in that office. Yes. You almost said it.

409
00:24:49,865 --> 00:24:52,805
Speaker 5:  To just set this up for the audience again, because I'm often reminded that

410
00:24:52,805 --> 00:24:56,005
Speaker 5:  I, we speak in code and inside jokes like a bunch of high school seniors.

411
00:24:57,555 --> 00:25:00,965
Speaker 5:  Project Genis is the result of T-Mobile

412
00:25:01,255 --> 00:25:04,605
Speaker 5:  being allowed to merge with Sprint in the first Trump administration. And

413
00:25:04,605 --> 00:25:08,445
Speaker 5:  what they did was they told T-Mobile it had to divest a bunch of assets

414
00:25:08,465 --> 00:25:12,205
Speaker 5:  in Spectrum to Dish Network, a satellite television

415
00:25:12,445 --> 00:25:15,365
Speaker 5:  operator, which would then create a new from the ground up 5G network so

416
00:25:15,365 --> 00:25:18,845
Speaker 5:  that we would've five national carriers and there would be competition and

417
00:25:19,715 --> 00:25:23,635
Speaker 5:  look around, does it exist? And then they launched a network,

418
00:25:23,745 --> 00:25:26,355
Speaker 5:  they called it Project on Fivey. They're now rolling it in a Boost mobile

419
00:25:27,075 --> 00:25:30,515
Speaker 5:  whatever. Most of those phones still roam onto at t and T-Mobile 'cause the

420
00:25:30,515 --> 00:25:33,115
Speaker 5:  network doesn't exist. But in some pockets of America the network apparently

421
00:25:33,115 --> 00:25:36,835
Speaker 5:  exists. That's what it got for you. And so just because they built

422
00:25:37,025 --> 00:25:40,955
Speaker 5:  some cell towers and put some radios on, some polls,

423
00:25:41,975 --> 00:25:45,545
Speaker 5:  someone will try to turn that into money because they've spent the money.

424
00:25:46,095 --> 00:25:49,705
Speaker 5:  Yeah, that's true. Here what they've done is they asked Chat GT to make a

425
00:25:49,705 --> 00:25:53,585
Speaker 5:  midrange Android phone, the word Trump on it, and no one has to collect on

426
00:25:53,585 --> 00:25:57,345
Speaker 5:  that investment. And so project Gen five says, just 'cause there's hardware

427
00:25:57,345 --> 00:26:01,185
Speaker 5:  in the world, like a little bit has a, you know, on, on our

428
00:26:01,205 --> 00:26:05,065
Speaker 5:  Go 90 scale where zero is alive and 90

429
00:26:05,165 --> 00:26:09,025
Speaker 5:  is dead is Quibi. I I would put

430
00:26:09,055 --> 00:26:09,865
Speaker 5:  trumpet 90

431
00:26:11,375 --> 00:26:15,185
Speaker 5:  just lives at 90 and, and Gen five sis at like a 75.

432
00:26:15,455 --> 00:26:15,745
Speaker 5:  Yeah,

433
00:26:15,745 --> 00:26:16,505
Speaker 6:  That seems fair.

434
00:26:16,905 --> 00:26:19,665
Speaker 5:  I don't think I've ever applied the, the Go 90 scale gym streaming services

435
00:26:19,805 --> 00:26:22,665
Speaker 5:  to to to telecom providers. But it works.

436
00:26:23,055 --> 00:26:26,305
Speaker 6:  Yeah, there's like three of them that are at 10 and everybody else is hovering

437
00:26:26,305 --> 00:26:28,785
Speaker 6:  in the mid eighties at all times. That's a lot. That's just what we do.

438
00:26:29,465 --> 00:26:32,895
Speaker 5:  There's more phones. There's a new fairphone that's

439
00:26:33,005 --> 00:26:36,695
Speaker 5:  modular. Is this, is this what this is? It's a modular phone. I love a modular

440
00:26:36,695 --> 00:26:36,815
Speaker 5:  phone.

441
00:26:36,975 --> 00:26:40,055
Speaker 6:  I find Fairphone totally fascinating. This company has just been like sort

442
00:26:40,055 --> 00:26:44,045
Speaker 6:  of chugging along in the background doing great work on making like

443
00:26:44,095 --> 00:26:47,885
Speaker 6:  upgradable repairable more well

444
00:26:48,395 --> 00:26:51,525
Speaker 6:  made and thoughtfully designed smartphones than almost anybody.

445
00:26:52,385 --> 00:26:56,245
Speaker 6:  And it's not like a huge hit. And their phones have been not like amazing

446
00:26:56,245 --> 00:26:58,885
Speaker 6:  because they're trying to like do the right thing more than they're trying

447
00:26:58,885 --> 00:27:02,325
Speaker 6:  to build great phones. And those two things are hard to do together. But

448
00:27:02,325 --> 00:27:05,685
Speaker 6:  the zoo one's really interesting. It's, it's the Fairphone six. It is

449
00:27:05,835 --> 00:27:09,725
Speaker 6:  smaller and lighter but has more battery. It's more powerful. They've just

450
00:27:09,725 --> 00:27:13,085
Speaker 6:  like made a bunch of the trade-offs you want them to make for the first time.

451
00:27:13,825 --> 00:27:17,525
Speaker 6:  But they're also doing the, the like modular accessories thing. Like, do

452
00:27:17,525 --> 00:27:20,405
Speaker 6:  you guys remember the, the nothing phone that came out and it had the little

453
00:27:20,795 --> 00:27:23,725
Speaker 6:  loop on the side that you could like stick a lanyard or like change the case

454
00:27:23,785 --> 00:27:27,405
Speaker 6:  and screw that stuff off That's coming to the fairphone. They're starting

455
00:27:27,405 --> 00:27:30,285
Speaker 6:  to do stuff like that. But the thing I think is really cool is they're shipping

456
00:27:30,675 --> 00:27:34,245
Speaker 6:  this phone in basically two different ways.

457
00:27:34,505 --> 00:27:38,485
Speaker 6:  One is with Android, like normal Android and

458
00:27:38,485 --> 00:27:41,365
Speaker 6:  Fairphone has a pretty like refined take on Android, but they're also shipping

459
00:27:41,365 --> 00:27:44,925
Speaker 6:  it with a button that essentially lets you switch to like a minimalist mode

460
00:27:44,945 --> 00:27:48,885
Speaker 6:  of Android. Which is kind of like as we haven't learned a ton about it,

461
00:27:48,885 --> 00:27:52,325
Speaker 6:  but it seems like basically a, a power do not disturb feature, which I think

462
00:27:52,325 --> 00:27:52,845
Speaker 6:  is super clever.

463
00:27:54,425 --> 00:27:58,045
Speaker 6:  And then the other one is it's shipping with a, a

464
00:27:58,225 --> 00:28:02,085
Speaker 6:  de Googled version of Android, which is called EOS made by this company,

465
00:28:02,205 --> 00:28:06,165
Speaker 6:  I believe it's called Morena. And essentially they've

466
00:28:06,165 --> 00:28:09,605
Speaker 6:  spent several years now building out a version of Android

467
00:28:09,915 --> 00:28:13,685
Speaker 6:  that is totally like bereft of Google services. Huh. And

468
00:28:14,025 --> 00:28:17,725
Speaker 6:  that's one you like really have to make a moral choice.

469
00:28:18,275 --> 00:28:21,005
Speaker 6:  It's like, it's like being a vegan, you know what I mean? It's like you have

470
00:28:21,005 --> 00:28:24,925
Speaker 6:  to do it on purpose and live with the consequences, but you're doing it because

471
00:28:24,925 --> 00:28:26,565
Speaker 6:  you believe it's the right thing to do for the world.

472
00:28:26,875 --> 00:28:30,165
Speaker 7:  It's interesting, I I suspect there probably is a lot of overlap between

473
00:28:30,165 --> 00:28:31,965
Speaker 7:  those two markets. Like there're gonna be a bunch

474
00:28:31,965 --> 00:28:33,485
Speaker 6:  Of vegans who hate that. I just made that

475
00:28:33,485 --> 00:28:36,125
Speaker 7:  Comparison. Oh, I was not, I didn't mean vegans and people who are against

476
00:28:36,125 --> 00:28:39,965
Speaker 7:  Google. I I thought I meant people who want to repair their

477
00:28:39,965 --> 00:28:43,525
Speaker 7:  phone endlessly and who want to get away from Google. Oh for sure. But it,

478
00:28:43,545 --> 00:28:47,365
Speaker 7:  it is, I think the bummer about this, you know, I I think there's a, a

479
00:28:47,365 --> 00:28:50,725
Speaker 7:  very exciting device, right? Like this is I think the best looking fair phone

480
00:28:50,785 --> 00:28:54,645
Speaker 7:  yet it looks like a normal phone. It's like an upper mid range Android

481
00:28:54,665 --> 00:28:57,005
Speaker 7:  device. It looks great. The colors are fun.

482
00:28:59,245 --> 00:29:02,285
Speaker 7:  I like that it's opinionated with the, the, you know, the button that does

483
00:29:02,285 --> 00:29:05,845
Speaker 7:  the, the minimal launcher. But if you wanna get it in the US

484
00:29:06,385 --> 00:29:09,045
Speaker 7:  you have to get that D GoogleFi version. Oh,

485
00:29:09,045 --> 00:29:11,765
Speaker 6:  That's the only one in the us Yes. Oh, that

486
00:29:11,765 --> 00:29:15,045
Speaker 7:  Sucks. And that is a problem, right? Yeah. Because

487
00:29:15,725 --> 00:29:19,605
Speaker 7:  choosing to go without Google Services means missing out on a

488
00:29:19,625 --> 00:29:23,445
Speaker 7:  lot of the phone experience. Right. Just like everything's in the play store.

489
00:29:23,795 --> 00:29:27,725
Speaker 7:  Just good, good luck. Right? Like most people that

490
00:29:27,725 --> 00:29:29,325
Speaker 7:  I know use Gmail. Yeah. Where,

491
00:29:29,325 --> 00:29:32,085
Speaker 5:  Where do you get apps for this thing? I'm, I'm like clicking on this website.

492
00:29:32,085 --> 00:29:34,925
Speaker 5:  Where do you get apps for it if you don't have the Play Store

493
00:29:35,435 --> 00:29:36,445
Speaker 7:  Side load? 'em, they

494
00:29:36,445 --> 00:29:39,245
Speaker 6:  Have a proprietary app store and they have some stuff. But it is, I mean

495
00:29:39,245 --> 00:29:42,725
Speaker 6:  it is like vanishingly small number of apps compared to the Play store.

496
00:29:42,985 --> 00:29:46,125
Speaker 5:  Yes. They're side loading and there's, you know, Android is Android, there's

497
00:29:46,125 --> 00:29:50,085
Speaker 5:  other app stores, but the play store and then play services provide

498
00:29:50,125 --> 00:29:53,085
Speaker 5:  a whole lot of capabilities to Android apps.

499
00:29:53,785 --> 00:29:57,685
Speaker 6:  Yes. Well, And I think increasingly so does Gemini.

500
00:29:58,105 --> 00:30:00,685
Speaker 6:  And So I think we're, we're now hitting this place. Like there was also some

501
00:30:00,685 --> 00:30:04,605
Speaker 6:  news this week about Gemini getting more and more

502
00:30:04,605 --> 00:30:08,485
Speaker 6:  integrated with the system and starting to do some like cross app

503
00:30:08,485 --> 00:30:12,085
Speaker 6:  stuff like we've been talking about with like, the future of Siri. And so

504
00:30:13,585 --> 00:30:17,165
Speaker 6:  the, the non Google version of Android and the Google version of Android

505
00:30:17,165 --> 00:30:21,125
Speaker 6:  are getting like much further apart over time and not

506
00:30:21,125 --> 00:30:24,965
Speaker 6:  closer together. So I think the case here for something like Morena

507
00:30:24,965 --> 00:30:28,645
Speaker 6:  is gonna get harder I think because it's like you, you're,

508
00:30:28,645 --> 00:30:31,805
Speaker 6:  you're just gonna have to make a broader set of sacrifices. 'cause I think

509
00:30:31,805 --> 00:30:35,765
Speaker 6:  like if I don't wanna have a Google account, it's not that hard

510
00:30:35,765 --> 00:30:38,885
Speaker 6:  to get like a proton email account and use

511
00:30:39,615 --> 00:30:43,365
Speaker 6:  DuckDuckGo and like you, you can relatively successfully replace most

512
00:30:43,365 --> 00:30:46,365
Speaker 6:  Google services with something else without trying all that hard.

513
00:30:47,145 --> 00:30:50,405
Speaker 6:  But if you believe in all this AI stuff and especially like the way it might

514
00:30:50,405 --> 00:30:53,685
Speaker 6:  change the way that you use your devices, taking that out is gonna make all

515
00:30:53,685 --> 00:30:56,725
Speaker 6:  kinds of other stuff just break. Yeah. It's like, it's like, oh, I don't

516
00:30:56,725 --> 00:31:00,045
Speaker 6:  use HTTP on my web browser. It's like, well then you can't use the internet.

517
00:31:00,405 --> 00:31:03,845
Speaker 6:  Like, and it, it does feel like a lot of the AI stuff is heading that way.

518
00:31:04,185 --> 00:31:07,365
Speaker 5:  Oh, David, you explained this in your piece about Marina it, they just downloading

519
00:31:07,365 --> 00:31:10,125
Speaker 5:  the apps through Google, they're just, they're just piping you to the play

520
00:31:10,125 --> 00:31:13,805
Speaker 5:  store and then they have a clone of play services that runs. Yeah, I mean

521
00:31:13,805 --> 00:31:16,765
Speaker 6:  I knew they had, they had their own clone of play services that doesn't feed

522
00:31:16,785 --> 00:31:20,725
Speaker 6:  any stuff back to Google. I had forgotten that. They just take apps from

523
00:31:20,725 --> 00:31:21,245
Speaker 6:  the play store.

524
00:31:23,195 --> 00:31:27,165
Speaker 5:  Fair enough. Yeah. I mean you wrote this three years ago, so you're,

525
00:31:27,265 --> 00:31:31,165
Speaker 5:  I'm you're forgiven for not remembering about the weird open

526
00:31:31,165 --> 00:31:32,245
Speaker 5:  source Android clone.

527
00:31:32,645 --> 00:31:34,445
Speaker 6:  I have thought this company is interesting for a long time.

528
00:31:35,405 --> 00:31:39,125
Speaker 5:  I mean, I'm fascinated by this and I'm fasted by the, the button that turns

529
00:31:39,125 --> 00:31:42,645
Speaker 5:  it into minimal mode on the phone. Like it's a switch. Love that.

530
00:31:42,835 --> 00:31:43,125
Speaker 6:  Yeah.

531
00:31:44,765 --> 00:31:47,865
Speaker 5:  But yeah, I mean the, this whole the fact that you have to commit to an entirely

532
00:31:47,895 --> 00:31:51,865
Speaker 5:  secondary operating system, that's a, that's

533
00:31:51,865 --> 00:31:52,985
Speaker 5:  a rough, that's a rough ask.

534
00:31:53,295 --> 00:31:57,265
Speaker 6:  Totally. But I think the, the, there's also like something

535
00:31:57,285 --> 00:32:01,145
Speaker 6:  in this combination of like more repairable, you can swap

536
00:32:01,165 --> 00:32:05,065
Speaker 6:  out some parts and this like tiny but

537
00:32:05,065 --> 00:32:08,705
Speaker 6:  growing accessory ecosystem that like, there's something there

538
00:32:08,845 --> 00:32:12,185
Speaker 6:  if you're a person who wants to sort of muck around with your phone from

539
00:32:12,185 --> 00:32:15,305
Speaker 6:  time to time. That I just think is very cool. And to Jake's point, this is

540
00:32:15,305 --> 00:32:18,905
Speaker 6:  the first one that is like on its own, a pretty compelling phone, which is

541
00:32:18,905 --> 00:32:19,105
Speaker 6:  awesome.

542
00:32:19,295 --> 00:32:22,185
Speaker 7:  This is the thing I, I feel like in the past the Fairphone have always looked

543
00:32:22,425 --> 00:32:25,145
Speaker 7:  a little bit lesser than, And I think this one obviously we'll still have

544
00:32:25,145 --> 00:32:28,465
Speaker 7:  to review it, see if it all holds up. This is the first one that you look

545
00:32:28,465 --> 00:32:31,945
Speaker 7:  at And I, it it looks like a modern phone. It does not look like you're going

546
00:32:31,945 --> 00:32:35,545
Speaker 7:  to dramatically miss out on things with the exception of those living in

547
00:32:35,545 --> 00:32:39,285
Speaker 7:  the United States. So that's a bummer. But I'm really

548
00:32:39,285 --> 00:32:40,525
Speaker 7:  excited to see how this pans out.

549
00:32:41,025 --> 00:32:43,925
Speaker 5:  All right. There's one more phone that I'm desperate to talk about this week.

550
00:32:44,195 --> 00:32:47,725
Speaker 5:  It's The Titan two, which the only way you can describe this is that it's

551
00:32:47,725 --> 00:32:51,325
Speaker 5:  a, it's a Blackberry that runs Android. That's what it is. And it looks exactly

552
00:32:51,325 --> 00:32:55,005
Speaker 5:  like an old Blackberry. It has a screen and it physical keyboard

553
00:32:55,385 --> 00:32:56,805
Speaker 5:  And I don't know who this is for.

554
00:32:57,385 --> 00:33:00,685
Speaker 6:  It has already raised a million dollars on Kickstarter.

555
00:33:01,835 --> 00:33:05,605
Speaker 6:  Like, so this is the thing I I I keep and it's, it's done

556
00:33:05,665 --> 00:33:09,445
Speaker 6:  big traffic on the site. Like people are into this thing. And I think my

557
00:33:09,445 --> 00:33:12,565
Speaker 6:  overwhelming theory about devices like this for years has been

558
00:33:13,235 --> 00:33:17,165
Speaker 6:  that people have real nostalgia for this stuff but

559
00:33:17,345 --> 00:33:21,125
Speaker 6:  not actual desire for it. Right? It's the same thing as small iPhones,

560
00:33:21,125 --> 00:33:24,725
Speaker 6:  right? Everyone on earth is like, oh God, remember when

561
00:33:24,995 --> 00:33:28,285
Speaker 6:  like the iPhone five just like fit perfectly in your hand. Wasn't that so

562
00:33:28,285 --> 00:33:30,645
Speaker 6:  great? And then you present them with a phone that small and they're like,

563
00:33:30,645 --> 00:33:34,485
Speaker 6:  why is this so small? I don't want this. The battery sucks. What are we doing

564
00:33:34,485 --> 00:33:38,325
Speaker 6:  here? I've always assumed that that is the same

565
00:33:38,325 --> 00:33:41,445
Speaker 6:  thing with physical keyboards. People remember their Blackberry 'cause it

566
00:33:41,445 --> 00:33:44,365
Speaker 6:  was their first smartphone and it was really exciting and it was like a,

567
00:33:44,485 --> 00:33:48,085
Speaker 6:  a simpler time in the tech world and that was all very cool. But then things

568
00:33:48,115 --> 00:33:51,725
Speaker 6:  like the, the Clicks keyboard come out and they're like sort of a compelling

569
00:33:51,725 --> 00:33:55,005
Speaker 6:  novelty in their, their fun and exciting to have and play with. But it's

570
00:33:55,005 --> 00:33:58,405
Speaker 6:  not a thing that is like a mainstream viable

571
00:33:58,555 --> 00:34:02,285
Speaker 6:  product. But then this thing comes out and raises a million dollars on Kickstarter.

572
00:34:02,285 --> 00:34:05,885
Speaker 6:  And so I'm like, there's clearly some people out there who want a giant

573
00:34:06,225 --> 00:34:09,925
Speaker 6:  ass slab of an Android phone with a big keyboard on it. It looks

574
00:34:09,925 --> 00:34:13,365
Speaker 7:  Delightful. There's two things about this that I think are special that we

575
00:34:13,365 --> 00:34:17,205
Speaker 7:  need to talk about. One is that on the backside of it,

576
00:34:17,315 --> 00:34:21,085
Speaker 7:  there's a second screen that shows notifications and a

577
00:34:21,205 --> 00:34:25,045
Speaker 7:  clock. It, it sort of just looks like an ambient Apple watch. It's it's pretty

578
00:34:25,045 --> 00:34:27,685
Speaker 7:  cool. It's interesting like normally, you know, you're like, I wanna have

579
00:34:27,685 --> 00:34:31,445
Speaker 7:  my phone out but not have it out. So I like flip it over and this way you

580
00:34:31,445 --> 00:34:33,845
Speaker 7:  flip it over but you still have your phone out. So it's actually, you're,

581
00:34:33,845 --> 00:34:37,125
Speaker 7:  you're just not, not being rude. You're still being rude. Which I think is

582
00:34:37,125 --> 00:34:41,005
Speaker 7:  delightful. And then they also advertise multiple times

583
00:34:41,035 --> 00:34:44,965
Speaker 7:  that they have an infrared port and they do not explain what that means or

584
00:34:44,965 --> 00:34:45,565
Speaker 7:  what you do with it.

585
00:34:46,585 --> 00:34:49,765
Speaker 5:  You obviously you turn off motion smoothing at the bar, Jake.

586
00:34:50,245 --> 00:34:53,805
Speaker 7:  I but but does it, does it, is it an outward infrared port or is it an

587
00:34:53,805 --> 00:34:56,925
Speaker 7:  accepting infrared port? Can I control my phone with a remote?

588
00:34:57,385 --> 00:34:59,925
Speaker 5:  Oh I see. Is it an iron blast or an IR window? Know

589
00:34:59,925 --> 00:35:00,085
Speaker 6:  They do

590
00:35:00,085 --> 00:35:01,325
Speaker 7:  Not clarify. They

591
00:35:01,795 --> 00:35:05,365
Speaker 6:  Make phone calls with my TV remote is is what you're talking about. Because

592
00:35:06,595 --> 00:35:10,405
Speaker 7:  That would be pretty innovative. I don't know if it would be useful but

593
00:35:10,745 --> 00:35:10,965
Speaker 7:  it,

594
00:35:11,305 --> 00:35:13,005
Speaker 5:  I'm gonna go ahead and say it's an IR blaster.

595
00:35:14,195 --> 00:35:16,515
Speaker 7:  I mean that's why this thing has raised a million dollars. Yeah

596
00:35:16,515 --> 00:35:19,155
Speaker 5:  People, people just want to go to the bar and like click around

597
00:35:19,155 --> 00:35:22,835
Speaker 6:  Menus. This is officially the most 2013 phone we've seen since

598
00:35:22,835 --> 00:35:24,035
Speaker 6:  2013. It's

599
00:35:24,255 --> 00:35:28,035
Speaker 5:  The, the standard was called erda, IRDA and you could very,

600
00:35:28,105 --> 00:35:31,995
Speaker 5:  very slowly beam contact information between phone like dumb phones

601
00:35:32,105 --> 00:35:34,165
Speaker 5:  back in the day. It's a real thing.

602
00:35:34,355 --> 00:35:34,645
Speaker 6:  Yeah,

603
00:35:35,365 --> 00:35:38,645
Speaker 5:  It wasn't great and no one used it but it was, it was called erda and there

604
00:35:38,645 --> 00:35:41,885
Speaker 5:  was a time in gadget blogging where like if a phone didn't have erda,

605
00:35:42,365 --> 00:35:44,175
Speaker 5:  like the commenters would be like No Erda.

606
00:35:47,345 --> 00:35:50,915
Speaker 6:  I'll say to this thing's credit. I think one very

607
00:35:50,915 --> 00:35:54,515
Speaker 6:  compelling thing about it is that you can buy it right now on Kickstarter

608
00:35:54,515 --> 00:35:58,315
Speaker 6:  for $269 And I think that for like a

609
00:35:58,595 --> 00:36:02,035
Speaker 6:  a half novelty of a phone is a perfect price

610
00:36:02,495 --> 00:36:03,075
Speaker 6:  and we're like starting,

611
00:36:03,075 --> 00:36:05,835
Speaker 5:  Alright two. Two Trump phones or four Titans.

612
00:36:06,175 --> 00:36:10,035
Speaker 6:  Oh four Titans. It's not close. Gimme The Titan and a fairphone over the

613
00:36:10,035 --> 00:36:13,915
Speaker 6:  two Trump phones. I'm a happy guy. There's also

614
00:36:14,215 --> 00:36:17,435
Speaker 7:  Actual motion footage of this phone existing

615
00:36:18,725 --> 00:36:22,435
Speaker 7:  which cannot be said. There's not a single frame of the Trump phone existing.

616
00:36:22,945 --> 00:36:26,275
Speaker 5:  Alright. Some more gadgets to round out our first segment, which is very

617
00:36:26,275 --> 00:36:28,755
Speaker 5:  gadget heavy. I point out we're all over the place. Gadgets

618
00:36:30,595 --> 00:36:34,515
Speaker 5:  Microsoft and Meta announced an Xbox VR headset that is

619
00:36:34,515 --> 00:36:38,385
Speaker 5:  just a Quest three s that's green and comes with a controller.

620
00:36:39,345 --> 00:36:42,805
Speaker 6:  One of my favorite things to do this week has been to troll

621
00:36:43,025 --> 00:36:46,365
Speaker 6:  Reddit. Watching people be confused about what this thing is.

622
00:36:46,875 --> 00:36:50,605
Speaker 6:  It's so funny. So two things are true, right? One,

623
00:36:50,995 --> 00:36:54,965
Speaker 6:  this is the best looking quest there has ever been. The black and green color

624
00:36:55,155 --> 00:36:58,405
Speaker 6:  like works for the quest in a way the like white beige thing kind of doesn't.

625
00:36:58,405 --> 00:37:01,685
Speaker 6:  Yeah. The combo of Xbox stuff and

626
00:37:02,565 --> 00:37:05,325
Speaker 6:  Quest makes sense. Shipping it with a controller that's already paired makes

627
00:37:05,325 --> 00:37:07,325
Speaker 6:  sense. Like this thing is a gaming machine, they're treating it like a gaming

628
00:37:07,325 --> 00:37:11,085
Speaker 6:  machine. It's stream Xbox Games makes perfect sense. There are

629
00:37:11,185 --> 00:37:15,005
Speaker 6:  so many people who just think this is an Xbox that will play Xbox games

630
00:37:15,265 --> 00:37:19,185
Speaker 6:  and it turns out it doesn't. And we, this is

631
00:37:19,185 --> 00:37:22,425
Speaker 6:  what we've talked about before. Like the list of Xboxes that don't play all

632
00:37:22,535 --> 00:37:26,465
Speaker 6:  Xbox games is vast and this is one of them.

633
00:37:26,525 --> 00:37:30,465
Speaker 6:  It will, it will stream some games. If you have

634
00:37:31,255 --> 00:37:34,585
Speaker 6:  Xbox Game Pass Ultimate, which requires jumping through a series of hoops

635
00:37:34,585 --> 00:37:37,905
Speaker 6:  I cannot even begin to describe to you And like

636
00:37:38,455 --> 00:37:42,185
Speaker 6:  this thing is it both is and is not an Xbox in a way that is driving a lot

637
00:37:42,185 --> 00:37:44,745
Speaker 6:  of people on the internet. Absolutely up the wall. Wait,

638
00:37:44,745 --> 00:37:48,025
Speaker 5:  Everything at this point is and is not an Xbox. The state of being an Xbox

639
00:37:48,085 --> 00:37:49,465
Speaker 5:  is probabilistic. This

640
00:37:49,465 --> 00:37:52,985
Speaker 6:  Is what I mean. Yes. But this one's called Xbox.

641
00:37:53,655 --> 00:37:57,345
Speaker 5:  Like every morning I wake up and think to myself, am I an Xbox? And I go

642
00:37:57,345 --> 00:38:00,145
Speaker 5:  through the list of what games I support and yeah. And many days the answer

643
00:38:00,145 --> 00:38:01,105
Speaker 5:  is yes. That's

644
00:38:01,105 --> 00:38:04,305
Speaker 6:  It's beautiful for you. Is is your Samsung frame TV an Xbox?

645
00:38:05,125 --> 00:38:08,945
Speaker 5:  It could be. 'cause you can run Game Pass on it, right? So wait, I have

646
00:38:08,945 --> 00:38:11,785
Speaker 5:  not paid a lot of attention to Game Pass. So this is just a VR headset and

647
00:38:11,785 --> 00:38:15,425
Speaker 5:  it's got the one app in it that runs in 2D. Right? So you're in the VR

648
00:38:15,425 --> 00:38:18,705
Speaker 5:  environment and it puts up a big fake tv, A big virtual tv

649
00:38:19,365 --> 00:38:22,145
Speaker 5:  and then it just runs Game Pass on it. That's right. It's not even running

650
00:38:22,245 --> 00:38:23,465
Speaker 5:  VR games. No,

651
00:38:23,485 --> 00:38:26,425
Speaker 6:  The Quest does that Anyway, this is just

652
00:38:27,455 --> 00:38:30,945
Speaker 6:  Pure like a branding exercise. And again I think the branding exercise makes

653
00:38:30,985 --> 00:38:34,665
Speaker 6:  a lot of sense but it is just, it is just a

654
00:38:34,665 --> 00:38:38,165
Speaker 6:  branding exercise. This is, this is a like, I don't know,

655
00:38:38,395 --> 00:38:42,365
Speaker 6:  imagine some celebrity picked a color scheme for a

656
00:38:42,485 --> 00:38:45,925
Speaker 6:  quest and it did that. This is that it's just called an

657
00:38:46,275 --> 00:38:48,565
Speaker 6:  Xbox. So people assume it's gonna be an Xbox.

658
00:38:48,835 --> 00:38:52,485
Speaker 5:  Well it's an Xbox in the sense that Xbox Game Pass Ultimate is

659
00:38:52,715 --> 00:38:53,005
Speaker 5:  Xbox.

660
00:38:53,225 --> 00:38:55,605
Speaker 6:  If that's an Xbox, we're all Xboxes.

661
00:38:55,885 --> 00:38:58,805
Speaker 5:  That's just what I'm saying. Every morning I wake you're Xbox. Yeah. Alright.

662
00:38:59,315 --> 00:39:02,805
Speaker 5:  It's very confusing. I'm trying to figure out who is making money here.

663
00:39:03,645 --> 00:39:07,405
Speaker 7:  Microsoft seems to have some very long-term plan here to

664
00:39:07,405 --> 00:39:10,605
Speaker 7:  redefine what the Xbox is and they are really

665
00:39:10,645 --> 00:39:14,325
Speaker 7:  overshooting for what they can actually deliver on today that that seems

666
00:39:14,325 --> 00:39:17,765
Speaker 7:  to be like I, I do not see how Xbox is benefiting from this

667
00:39:18,175 --> 00:39:20,325
Speaker 7:  right now when they're just confusing everybody.

668
00:39:20,825 --> 00:39:24,485
Speaker 5:  So my understanding is that Xbox is, it came in second place the last two

669
00:39:24,485 --> 00:39:28,125
Speaker 5:  generations, right? Yes. The PS four and the PS five blew the Xbox away and

670
00:39:28,125 --> 00:39:31,085
Speaker 5:  then the switch and presumably the switch to is they're all gonna be great

671
00:39:31,085 --> 00:39:35,055
Speaker 5:  and that's gonna be a total other thing over there. And I, it

672
00:39:35,055 --> 00:39:38,895
Speaker 5:  feels like Microsoft is reacting to coming in second place as though they

673
00:39:38,895 --> 00:39:42,845
Speaker 5:  have failed so completely they need to burn it all down. Right? And

674
00:39:42,845 --> 00:39:45,445
Speaker 5:  they're like, we will reconceptualize the very concept of Xbox and turn it

675
00:39:45,445 --> 00:39:48,565
Speaker 5:  into like a PC console gaming situation on handheld

676
00:39:49,445 --> 00:39:53,325
Speaker 5:  cloud streaming service on, on VR headsets and then

677
00:39:53,325 --> 00:39:56,165
Speaker 5:  the actual console is like of little value to them.

678
00:39:56,355 --> 00:39:57,325
Speaker 6:  Yeah, I think that's right,

679
00:39:57,415 --> 00:40:00,605
Speaker 5:  Right? That that's kind of what this all feels like. But I'm just saying

680
00:40:01,065 --> 00:40:05,045
Speaker 5:  the Quest three s is not a huge hit for Meta. Right?

681
00:40:05,115 --> 00:40:08,195
Speaker 5:  There's been reporting that they've kind of lost interest in Quest

682
00:40:09,035 --> 00:40:11,315
Speaker 5:  'cause their glasses are the success which we'll talk about in a second.

683
00:40:11,335 --> 00:40:15,155
Speaker 5:  The, the Ray bands and now the Oakley Smart Glasses feel like their future.

684
00:40:15,935 --> 00:40:19,595
Speaker 5:  And so it's like, okay, a an an unsuccessful VR headset

685
00:40:19,735 --> 00:40:23,515
Speaker 5:  is being paired with an unsuccessful gaming brand to

686
00:40:23,515 --> 00:40:24,035
Speaker 5:  accomplish

687
00:40:25,395 --> 00:40:29,245
Speaker 6:  What the thing about it is, I actually think in

688
00:40:29,285 --> 00:40:32,765
Speaker 6:  a certain way calling this thing an Xbox makes it make more sense

689
00:40:33,315 --> 00:40:37,125
Speaker 6:  because it is a gaming machine. Like everybody who works

690
00:40:37,145 --> 00:40:40,485
Speaker 6:  on VR stuff will tell you this is not true. But VR is a gaming platform.

691
00:40:40,625 --> 00:40:41,485
Speaker 6:  It just is. But

692
00:40:41,485 --> 00:40:45,395
Speaker 5:  This isn't play VR games. This is, this is the same as one of those

693
00:40:46,595 --> 00:40:49,835
Speaker 5:  X reel glasses that just picks, puts up a fake tv.

694
00:40:50,145 --> 00:40:53,995
Speaker 6:  Yeah those, that's also an Xbox, apple Vision Pro also an Xbox. Do you know

695
00:40:53,995 --> 00:40:56,675
Speaker 6:  what I mean? Like what isn't an Xbox help?

696
00:40:59,415 --> 00:41:03,315
Speaker 6:  But I think like the, to the extent that this just screams, this is a large

697
00:41:03,335 --> 00:41:07,035
Speaker 6:  screen on which to play games, I'm not bothered by it. Sure. But the thing

698
00:41:07,035 --> 00:41:08,995
Speaker 6:  you're describing is why people are getting tripped up because it's like,

699
00:41:08,995 --> 00:41:12,555
Speaker 6:  oh this should now play my Xbox games and it

700
00:41:12,725 --> 00:41:16,195
Speaker 6:  isn't that simple. And so we're in this position of like

701
00:41:16,655 --> 00:41:20,355
Speaker 6:  nobody, many more people know what to make of Xbox

702
00:41:20,385 --> 00:41:23,555
Speaker 6:  than know what to make of Quest. So I can actually see why

703
00:41:24,755 --> 00:41:28,475
Speaker 6:  bringing Quest and Xbox together makes sense for shipping

704
00:41:28,475 --> 00:41:30,355
Speaker 6:  this headset. Except

705
00:41:31,995 --> 00:41:35,925
Speaker 6:  it's, it's not fully an Xbox, I just, I get turned in circles.

706
00:41:35,925 --> 00:41:38,765
Speaker 6:  But you know, ultimately this thing is like a limited edition and looks very

707
00:41:38,765 --> 00:41:41,765
Speaker 6:  cool and not that many people get it. So it's like

708
00:41:42,925 --> 00:41:45,645
Speaker 6:  whatever. But I do find it sort of fascinating that they're like leaning

709
00:41:45,755 --> 00:41:49,565
Speaker 6:  into this thing is for games and Microsoft is like, games are for everything

710
00:41:49,565 --> 00:41:51,685
Speaker 6:  and they're like, oh I see why you're friends. Like sure,

711
00:41:52,115 --> 00:41:55,645
Speaker 5:  Yeah. Yeah. Just on the one hand, you know, Microsoft spent last week or

712
00:41:55,725 --> 00:41:59,125
Speaker 5:  I guess two weeks ago talking about the, you know, the wrong ally and being

713
00:41:59,125 --> 00:42:03,085
Speaker 5:  like the future of Xbox is Windows games on the go. We also support

714
00:42:03,085 --> 00:42:06,805
Speaker 5:  Steam and one turn later they're like, it's a limited cloud

715
00:42:06,805 --> 00:42:10,605
Speaker 5:  gaming library on the quest it does

716
00:42:10,605 --> 00:42:14,405
Speaker 5:  feel, it just feels like they, they've they've lost sight of what they want

717
00:42:14,475 --> 00:42:18,155
Speaker 5:  Xbox to be. So it's everything and yeah, who knows.

718
00:42:18,295 --> 00:42:21,915
Speaker 5:  Agreed. The Meta Smart glasses, they are a success

719
00:42:22,335 --> 00:42:25,555
Speaker 5:  at least according to the metrics. Meta Keeps saying and they've expanded

720
00:42:25,555 --> 00:42:29,395
Speaker 5:  them this week and now they've got Oakly smart glasses, same basic

721
00:42:29,395 --> 00:42:33,075
Speaker 5:  concept as the Raybans. Only they look like Oakley's

722
00:42:33,575 --> 00:42:36,715
Speaker 5:  and they shoot three K video slightly higher resolution video 'cause they're

723
00:42:36,835 --> 00:42:39,155
Speaker 5:  slightly bigger. Is is basically my understanding of the product.

724
00:42:39,385 --> 00:42:42,635
Speaker 6:  Yeah. Pretty much the, the thing I thought was really interesting, Alex Heath

725
00:42:42,635 --> 00:42:46,475
Speaker 6:  talked to one of the folks on the team building this stuff and they confirmed

726
00:42:46,475 --> 00:42:49,875
Speaker 6:  what we have always suspected, which is that people use these glasses for

727
00:42:49,875 --> 00:42:53,635
Speaker 6:  two things. One is audio and one is camera and they call them

728
00:42:53,815 --> 00:42:57,435
Speaker 6:  AI glasses. Which is very funny because it's like, well we're gonna call

729
00:42:57,435 --> 00:43:00,675
Speaker 6:  this thing a thing no one uses them for but

730
00:43:01,185 --> 00:43:04,955
Speaker 6:  like this, this positioning makes sense. Doing it with

731
00:43:04,955 --> 00:43:08,355
Speaker 6:  Oakley makes sense. I don't think these glasses are particularly good looking,

732
00:43:08,895 --> 00:43:12,675
Speaker 6:  but I also can't pull off Oakley's in general. So I'm not like a great

733
00:43:12,695 --> 00:43:14,235
Speaker 6:  person to judge this. When you

734
00:43:14,235 --> 00:43:17,155
Speaker 5:  Went on the eighth grade trip to dc did everybody buy fake Oakleys?

735
00:43:17,295 --> 00:43:20,435
Speaker 6:  Oh, a hundred percent. Yeah. I've owned so many fake Oakleys joke in my life.

736
00:43:20,615 --> 00:43:20,835
Speaker 6:  We,

737
00:43:20,835 --> 00:43:21,155
Speaker 5:  We called them,

738
00:43:21,615 --> 00:43:25,515
Speaker 6:  What'd you call them? Oakleys. Oh we were, we were Oh very good. Yeah

739
00:43:25,735 --> 00:43:29,595
Speaker 6:  it was, they were good. But yeah, I think like this, this thing Meta is doing

740
00:43:29,915 --> 00:43:33,755
Speaker 6:  continues to make perfect sense to me. It's, it's still

741
00:43:33,815 --> 00:43:37,405
Speaker 6:  not clear how you bridge the gap between this

742
00:43:38,105 --> 00:43:41,565
Speaker 6:  and the Orion glasses that we talked about before because there like the

743
00:43:41,565 --> 00:43:45,285
Speaker 6:  technical breakthroughs required to get from one to the other are vast and

744
00:43:45,405 --> 00:43:45,965
Speaker 6:  unsolved.

745
00:43:47,625 --> 00:43:51,525
Speaker 6:  But just this thing where like Meta has a hit at relative

746
00:43:51,575 --> 00:43:55,565
Speaker 6:  scale doing this basic kind of stuff and they've just decided to lean into

747
00:43:55,565 --> 00:43:58,605
Speaker 6:  this is the thing that everybody wants. We will figure out how to stack capabilities

748
00:43:58,625 --> 00:44:02,045
Speaker 6:  on top of it over time feels like the right move to me

749
00:44:02,425 --> 00:44:06,005
Speaker 7:  It also feels really clear that their partnership with Luxottica has just

750
00:44:06,005 --> 00:44:09,925
Speaker 7:  continued to pay dividends here. Right? Like you need

751
00:44:09,925 --> 00:44:13,845
Speaker 7:  glasses that are naturally chunky enough to fit this hardware.

752
00:44:13,875 --> 00:44:17,725
Speaker 7:  Totally. And they have managed to find multiple styles here

753
00:44:18,115 --> 00:44:18,405
Speaker 7:  that

754
00:44:20,285 --> 00:44:23,315
Speaker 7:  adapt well to this additional hardware that are already

755
00:44:24,335 --> 00:44:27,465
Speaker 7:  formats that people know and love. Right? These are very popular glasses,

756
00:44:27,655 --> 00:44:31,065
Speaker 7:  they're very popular brand names. Expanding to Oakley was a really smart

757
00:44:31,180 --> 00:44:34,925
Speaker 7:  next move after RayBan. And So I Right like this is

758
00:44:35,085 --> 00:44:38,765
Speaker 7:  a, a challenge that every other company getting into the glass space is gonna

759
00:44:38,765 --> 00:44:42,605
Speaker 7:  have. If you put out knockoff pair of Raybans or Oakleys, people will be

760
00:44:42,605 --> 00:44:46,325
Speaker 7:  able to tell that they're a knockoff pair. Right. Meta's biggest

761
00:44:46,325 --> 00:44:49,165
Speaker 7:  advantage here is it is just selling Oakley's with some headphones in them.

762
00:44:50,005 --> 00:44:53,725
Speaker 7:  Right? That's not a bad product at a bare minimum, right? Nobody has to use

763
00:44:53,725 --> 00:44:54,165
Speaker 7:  the ai

764
00:44:55,515 --> 00:44:57,405
Speaker 5:  Well I think Meta is desperate for

765
00:44:57,405 --> 00:44:58,205
Speaker 7:  A platform. Well they would like

766
00:44:58,205 --> 00:45:02,045
Speaker 5:  That where their AI assistant is the first party

767
00:45:02,045 --> 00:45:06,035
Speaker 5:  assistant not an app on the iPhone. And they've settled in these

768
00:45:06,035 --> 00:45:09,995
Speaker 5:  glasses. I will say they, they transition lenses are available, but

769
00:45:09,995 --> 00:45:13,795
Speaker 5:  all the cool photos are, they're reflective orange lenses and Meta has

770
00:45:13,795 --> 00:45:17,595
Speaker 5:  continued attempts to make transitions. Very cool.

771
00:45:18,555 --> 00:45:21,435
Speaker 5:  Continues to amuse. That's all I'm just saying. Especially 'cause if you're

772
00:45:21,435 --> 00:45:25,275
Speaker 5:  wearing transition Oakleys, many decisions have been made along the way.

773
00:45:25,755 --> 00:45:28,555
Speaker 6:  Vindication is coming for my dad who's been wearing transitions for like

774
00:45:28,595 --> 00:45:30,915
Speaker 6:  a decade, the world is coming around to Joey.

775
00:45:31,475 --> 00:45:35,275
Speaker 5:  I don know if you saw Joanna Stern's video with Craig Federighi and Craig

776
00:45:35,385 --> 00:45:38,435
Speaker 5:  Jaws react, but Jaws was wearing transitions in the video

777
00:45:39,325 --> 00:45:42,345
Speaker 5:  in about halfway through. They tinted 'cause they were in the sun. And I

778
00:45:42,345 --> 00:45:45,545
Speaker 5:  was like, well this is, I mean it's fine. Like dude do you whatever you want

779
00:45:45,545 --> 00:45:49,345
Speaker 5:  do, but the fact that they, you can't control them actually sometimes causes

780
00:45:49,345 --> 00:45:49,745
Speaker 5:  problems.

781
00:45:49,835 --> 00:45:52,465
Speaker 6:  Maybe it's a defense mechanism where he's like, these questions are too hard

782
00:45:52,465 --> 00:45:52,825
Speaker 6:  and just,

783
00:45:53,655 --> 00:45:57,465
Speaker 5:  Just black them out. Like I can't see you just like you

784
00:45:57,465 --> 00:45:58,265
Speaker 5:  can't see Siri.

785
00:46:00,545 --> 00:46:03,865
Speaker 5:  I don know I look the fact that it's camera and, and and headphones.

786
00:46:04,255 --> 00:46:08,145
Speaker 5:  Exactly the same problem as Alexa had for years. Sure. It's timers and music

787
00:46:09,055 --> 00:46:12,075
Speaker 5:  and it's great. I think people like having cameras on their face. I got a

788
00:46:12,075 --> 00:46:15,635
Speaker 5:  TikTok ad today for like what can only be described as a necklace with a

789
00:46:15,635 --> 00:46:18,955
Speaker 5:  mag safe mount. Huh? Because people mount their phones here when they like

790
00:46:18,965 --> 00:46:22,475
Speaker 5:  drive their cars and do stuff and they want the like first person view.

791
00:46:23,115 --> 00:46:26,915
Speaker 5:  I get it. I get why people want cameras in their glasses. I

792
00:46:26,915 --> 00:46:30,115
Speaker 5:  don't think Meta has actually trans, I would love to know the rate at which

793
00:46:30,115 --> 00:46:33,515
Speaker 5:  people who buy these glasses also use the AI stuff

794
00:46:34,135 --> 00:46:38,035
Speaker 5:  has is Meta convincing 2% of those people to be like, I love the

795
00:46:38,035 --> 00:46:41,795
Speaker 5:  Meta AI assistant. How many people are looking at stuff and Andre like,

796
00:46:41,795 --> 00:46:45,155
Speaker 5:  what is that? 'cause that's the use case they keep coming up with. Right?

797
00:46:45,325 --> 00:46:48,955
Speaker 5:  Again, I I think the real thing people want to build is Orion the

798
00:46:48,955 --> 00:46:52,315
Speaker 5:  augmented reality glasses. And even Orion doesn't actually do augmented reality

799
00:46:52,375 --> 00:46:56,235
Speaker 5:  yet. Like we're, we're, we're, we're long, long away. But if you

800
00:46:56,235 --> 00:46:59,155
Speaker 5:  love in Oakley and you like glasses, I joined a Stern speaking of Joanna

801
00:46:59,165 --> 00:47:02,035
Speaker 5:  loves her, met Ray Bands, loves him. Can't get enough of them.

802
00:47:02,355 --> 00:47:05,555
Speaker 6:  I wear mine all the time just as sunglasses that happen to do other things.

803
00:47:05,705 --> 00:47:07,075
Speaker 6:  Like that's that's great.

804
00:47:07,455 --> 00:47:08,515
Speaker 5:  Do you have the transition lenses?

805
00:47:09,175 --> 00:47:12,435
Speaker 6:  No. Mine are just sunglasses. So I just get to be the cool guy who like wears

806
00:47:12,435 --> 00:47:14,435
Speaker 6:  sunglasses inside the coffee shop for a while.

807
00:47:14,545 --> 00:47:17,075
Speaker 5:  Does the Meta app support multiple glasses?

808
00:47:17,895 --> 00:47:21,075
Speaker 6:  The last time I tried I could only, I could only be paired to one at a time.

809
00:47:21,595 --> 00:47:24,675
Speaker 5:  I mean these are just like, right, these are, these are problems with the

810
00:47:24,675 --> 00:47:28,155
Speaker 5:  form factor that you're gonna have to overcome. You nobody wears the same

811
00:47:28,155 --> 00:47:31,725
Speaker 5:  pair of glasses all the time, right? Like even people who wear glasses,

812
00:47:31,875 --> 00:47:35,805
Speaker 5:  like prescription glasses. I have two pairs of glasses because when you go

813
00:47:35,805 --> 00:47:38,005
Speaker 5:  to the store with my glasses, they're always like, do you want two pairs

814
00:47:38,005 --> 00:47:38,325
Speaker 5:  of glasses?

815
00:47:40,295 --> 00:47:42,515
Speaker 5:  And then people with sunglasses have lots of different kinds of sunglasses.

816
00:47:42,545 --> 00:47:46,195
Speaker 5:  Like being locked into one pair of one style sunglasses forever is not great.

817
00:47:47,095 --> 00:47:51,015
Speaker 5:  And now they have multiple sauces. I don dunno, I'm just saying the, on

818
00:47:51,015 --> 00:47:54,375
Speaker 5:  the chart of wearable bullshit, you still have to put the thing on your face

819
00:47:54,435 --> 00:47:57,495
Speaker 5:  man. It's the utility has to be much higher than it's right now. But you

820
00:47:57,495 --> 00:47:57,815
Speaker 5:  love them,

821
00:47:58,205 --> 00:48:01,615
Speaker 6:  They're great and they're like, they've solved the face problem because they

822
00:48:01,615 --> 00:48:05,375
Speaker 6:  just look like sunglasses. Like that's the thing that's, that's to, to Jake's

823
00:48:05,375 --> 00:48:09,055
Speaker 6:  point, like the, the hardest thing about this is just

824
00:48:09,335 --> 00:48:13,215
Speaker 6:  provided to Meta by another company that is fabulously good at making glasses.

825
00:48:13,215 --> 00:48:17,175
Speaker 6:  People like and rather than do what

826
00:48:17,525 --> 00:48:21,295
Speaker 6:  Meta tried to do and what Apple is trying to do and what traditionally

827
00:48:21,295 --> 00:48:24,895
Speaker 6:  tech companies have always done, which is like build the hardest, most

828
00:48:24,975 --> 00:48:28,695
Speaker 6:  expensive thing and then try to like trickle the tech down to cheaper

829
00:48:28,695 --> 00:48:32,175
Speaker 6:  prices. Meta was just like, we're gonna do the least and then

830
00:48:32,715 --> 00:48:35,575
Speaker 6:  try to build up from there. And I actually think in this particular case

831
00:48:35,875 --> 00:48:39,855
Speaker 6:  in part because of, you know, the, the problems that come with Eli's chart

832
00:48:39,855 --> 00:48:43,775
Speaker 6:  of wearable bullshit, that's the right move. Like make a thing

833
00:48:43,775 --> 00:48:47,735
Speaker 6:  people like and then figure out how to get it to do more stuff strikes

834
00:48:47,735 --> 00:48:51,255
Speaker 6:  me as way more compelling. Yeah. Than I'm going to build a helmet for your

835
00:48:51,255 --> 00:48:54,015
Speaker 6:  face and then figure out how to make you like wearing it.

836
00:48:55,635 --> 00:48:59,415
Speaker 5:  The Apple Vision Pro everybody. Yeah. Alright, we gotta take a break. We'll

837
00:48:59,415 --> 00:49:03,175
Speaker 5:  be back. Speaking of Meta, we gotta talk about their big AI win Philanthropics

838
00:49:03,175 --> 00:49:06,895
Speaker 5:  big AI win but they're also losses. We'll, we'll be right back. We'll explain.

839
00:51:46,915 --> 00:51:50,695
Speaker 5:  All right, we're back. Addie Robertson's here. Hey Addie. Hey, I'm excited.

840
00:51:50,695 --> 00:51:54,535
Speaker 5:  Usually when you And I talk something very dark has happened

841
00:51:55,335 --> 00:51:58,495
Speaker 5:  like a lot of times you're under coder and we're like, well Addie's here.

842
00:51:58,955 --> 00:52:01,935
Speaker 5:  And you, we both had the same reaction. I'm excited to talk to you about

843
00:52:02,115 --> 00:52:05,975
Speaker 5:  AI and copyright 'cause this is just chaos. Like it is delightful

844
00:52:06,545 --> 00:52:10,415
Speaker 5:  chaos in our nation's courts as the AI copyright cases begin to

845
00:52:10,415 --> 00:52:11,815
Speaker 5:  wind their way to some decisions.

846
00:52:12,115 --> 00:52:13,055
Speaker 11:  Oh yeah, these are fun.

847
00:52:13,355 --> 00:52:17,335
Speaker 6:  Can I back us up here a little bit and just like build from the ground up

848
00:52:17,335 --> 00:52:21,295
Speaker 6:  a little bit because yeah, I confess I have had trouble keeping

849
00:52:21,345 --> 00:52:25,295
Speaker 6:  these two cases this week separate in my head and then also

850
00:52:25,295 --> 00:52:27,295
Speaker 6:  trying to figure out what they have to do with each other because it seems

851
00:52:27,295 --> 00:52:31,095
Speaker 6:  to be a lot. So Addie, can you just like walk us through these

852
00:52:31,095 --> 00:52:34,735
Speaker 6:  trials a little bit? First, let's do anthropic first. 'cause that that decision

853
00:52:34,795 --> 00:52:38,575
Speaker 6:  hit first this week. Just remind us quickly like what that case

854
00:52:38,635 --> 00:52:42,175
Speaker 6:  was about and what was decided this week. This

855
00:52:42,175 --> 00:52:44,655
Speaker 5:  Is David saying Neli can't immediately get in his feelings. That's just what

856
00:52:44,815 --> 00:52:45,575
Speaker 5:  happened. Yeah, NELI

857
00:52:45,965 --> 00:52:49,895
Speaker 6:  Neli was so fast to like, let's talk about the the whatever the

858
00:52:49,895 --> 00:52:53,495
Speaker 6:  fourth factor of copyright law and I'm like, can I remember what books are

859
00:52:53,545 --> 00:52:53,895
Speaker 6:  first?

860
00:52:54,965 --> 00:52:58,895
Speaker 11:  Yeah. So there are two groups of authors here and both of them found out

861
00:52:58,895 --> 00:53:02,735
Speaker 11:  their work is almost certainly ingested into the training data set of

862
00:53:02,895 --> 00:53:06,815
Speaker 11:  a chatbot company. In one case, Meta in one case, anthropic and they both

863
00:53:07,045 --> 00:53:10,575
Speaker 11:  sued. And there are kind of two arguments. One argument is these things

864
00:53:10,575 --> 00:53:14,295
Speaker 11:  were pirated in the first place and then in some ways the bigger

865
00:53:14,575 --> 00:53:18,415
Speaker 11:  argument is well even if you get these things

866
00:53:18,415 --> 00:53:22,335
Speaker 11:  legally, you are putting these into a training set that is then using

867
00:53:22,335 --> 00:53:26,325
Speaker 11:  them to power a very lucrative product. And that's unauthorized,

868
00:53:26,325 --> 00:53:29,765
Speaker 11:  that's copyright infringement. You need to get permission from us and Payos.

869
00:53:29,995 --> 00:53:33,245
Speaker 6:  Okay, so this became Neli a fair use

870
00:53:33,735 --> 00:53:35,405
Speaker 6:  thing. Yep. Kind of all the way down, right?

871
00:53:35,675 --> 00:53:39,245
Speaker 5:  It's very important to separate what specifically is fair use

872
00:53:39,545 --> 00:53:43,445
Speaker 5:  and the courts here don't agree there.

873
00:53:43,445 --> 00:53:47,335
Speaker 5:  There's a lot going on in these, but it's important to just think

874
00:53:47,335 --> 00:53:51,295
Speaker 5:  about AI from the technical perspective. There's training the model

875
00:53:51,715 --> 00:53:55,375
Speaker 5:  and then there's inference where you get outputs from the model. And in these

876
00:53:55,375 --> 00:53:58,425
Speaker 5:  cases it's all about training. The model is

877
00:53:59,295 --> 00:54:02,875
Speaker 5:  in the anthropic case, literally buying a bunch of used books,

878
00:54:03,265 --> 00:54:07,235
Speaker 5:  ripping off the covers, scanning them into a database and then using

879
00:54:07,235 --> 00:54:10,315
Speaker 5:  that database to train a model. Is all of that fair use?

880
00:54:11,135 --> 00:54:15,075
Speaker 5:  Not, if you ask the model to produce some copyrighted

881
00:54:15,075 --> 00:54:18,555
Speaker 5:  book, is that copyright infringement? Right. And that's a big distinction.

882
00:54:19,415 --> 00:54:22,835
Speaker 5:  And the reason people hate lawyers just 'cause lawyers are constantly making

883
00:54:23,225 --> 00:54:27,115
Speaker 5:  extremely pedantic distinctions between things. But

884
00:54:27,115 --> 00:54:30,755
Speaker 5:  in these cases those distinctions are wildly important because the judges

885
00:54:31,205 --> 00:54:34,835
Speaker 5:  react to those distinctions between training and inference between inputs

886
00:54:34,835 --> 00:54:36,515
Speaker 5:  and outputs. Very, very different. So

887
00:54:36,665 --> 00:54:39,315
Speaker 6:  Both of these cases work fundamentally about training, right?

888
00:54:39,905 --> 00:54:43,675
Speaker 11:  Both of these cases are sort of fundamentally about training with a

889
00:54:43,705 --> 00:54:47,275
Speaker 11:  side which is actually also very important of you

890
00:54:47,275 --> 00:54:50,355
Speaker 11:  pirated these books in the first place. Oh, it's kind of the difference

891
00:54:50,355 --> 00:54:54,235
Speaker 11:  between, again, in both cases at first they were

892
00:54:54,235 --> 00:54:57,075
Speaker 11:  like alright, we're going to download all these books and we're gonna train

893
00:54:57,075 --> 00:55:00,835
Speaker 11:  on them. And then Meta kind of kept it there and anthropic went,

894
00:55:00,835 --> 00:55:04,235
Speaker 11:  wait, we need to fix this. We're not comfortable with this. So then as Neela

895
00:55:04,235 --> 00:55:08,155
Speaker 11:  mentioned, they got a bunch of books and they destroyed

896
00:55:08,155 --> 00:55:11,675
Speaker 11:  the books after scanning them. So then there was a copy in the clout

897
00:55:12,895 --> 00:55:16,315
Speaker 11:  and this turned out to be a really hugely important

898
00:55:16,395 --> 00:55:19,075
Speaker 11:  distinction for reasons besides the training.

899
00:55:19,535 --> 00:55:22,475
Speaker 6:  If I told you to just go buy a million books, would you know how to do it?

900
00:55:23,265 --> 00:55:24,435
Speaker 6:  I've been thinking about this all

901
00:55:24,435 --> 00:55:27,875
Speaker 5:  Week. Anthropic hired one of the people who ran Google books. Like they found

902
00:55:28,175 --> 00:55:31,235
Speaker 5:  the one person who knew how to do it. So the philanthropic case is really

903
00:55:31,435 --> 00:55:35,275
Speaker 5:  interesting. For that reason they started by saying we're just going to

904
00:55:35,605 --> 00:55:39,115
Speaker 5:  steal stuff. And they pirated books and they referred to the problem they

905
00:55:39,115 --> 00:55:42,675
Speaker 5:  had as like legal business slog. That's in the opinion.

906
00:55:43,295 --> 00:55:45,955
Speaker 5:  And the legal business slog is like, should we pay money and acquire things

907
00:55:45,955 --> 00:55:48,555
Speaker 5:  legally? And they're like, no, we'll just steal it. And then they came back

908
00:55:48,555 --> 00:55:51,595
Speaker 5:  around and we're like, we're gonna scan a bunch of books. And the judge in

909
00:55:51,595 --> 00:55:55,435
Speaker 5:  that case adding my read was that he cared a lot about

910
00:55:55,435 --> 00:55:56,075
Speaker 5:  acquisition.

911
00:55:56,415 --> 00:55:57,395
Speaker 11:  He was so mad.

912
00:55:57,585 --> 00:56:01,515
Speaker 5:  Well he's very focused on did you acquire these books legally or not? He

913
00:56:01,515 --> 00:56:05,235
Speaker 5:  makes a distinction between the legally acquired books and the the

914
00:56:05,235 --> 00:56:08,835
Speaker 5:  pirated books. And then he spends a lot of time being like, okay, once you

915
00:56:09,145 --> 00:56:12,315
Speaker 5:  have the stuff, what can you do with it that's fair or not?

916
00:56:13,205 --> 00:56:17,105
Speaker 5:  And the first thing, and this is really just again, pedantic steps

917
00:56:17,175 --> 00:56:21,065
Speaker 5:  down the line. The first thing that did was they made a

918
00:56:21,065 --> 00:56:21,985
Speaker 5:  big database of books

919
00:56:22,405 --> 00:56:22,625
Speaker 6:  In.

920
00:56:23,845 --> 00:56:27,585
Speaker 5:  And that is, you don't think about that in the, in the context of

921
00:56:27,585 --> 00:56:31,405
Speaker 5:  you're training a model, but the judge spends a lot of time being like,

922
00:56:31,405 --> 00:56:35,325
Speaker 5:  okay, the first thing you did was make a database of books and then you use

923
00:56:35,325 --> 00:56:37,445
Speaker 5:  that database, you can use that database to do all kinds of stuff. You can

924
00:56:37,445 --> 00:56:41,165
Speaker 5:  just search the database to see what books you have. And he,

925
00:56:42,735 --> 00:56:46,545
Speaker 5:  you know, there's a lot of like consumer rights history

926
00:56:46,575 --> 00:56:50,385
Speaker 5:  here, a lot of Google history here ripping the covers off of books

927
00:56:50,385 --> 00:56:54,225
Speaker 5:  and scanning 'em into a big database. If if you wanted

928
00:56:54,225 --> 00:56:57,425
Speaker 5:  to do that to all the books you owned, you would be like, I can do that.

929
00:56:57,975 --> 00:57:01,725
Speaker 5:  Right? Like if the judge here had found that buying a million books

930
00:57:01,725 --> 00:57:05,205
Speaker 5:  ripping the covers off and scanning 'em into a database was illegal, like

931
00:57:05,605 --> 00:57:08,925
Speaker 5:  a lot of things would break. I think like

932
00:57:09,845 --> 00:57:13,645
Speaker 5:  I I, there's some part of this where copyright has such

933
00:57:13,725 --> 00:57:16,565
Speaker 5:  a maximalist control over what you can do with the things you can buy

934
00:57:17,805 --> 00:57:21,015
Speaker 5:  that the only answer you can get to is you're allowed to buy a million books,

935
00:57:21,135 --> 00:57:23,735
Speaker 5:  strip the covers off of them and scan the in new database as long as you

936
00:57:23,735 --> 00:57:25,535
Speaker 5:  don't let other people access the database. Yeah,

937
00:57:25,535 --> 00:57:28,775
Speaker 11:  It's pretty notable here that they went from, okay, yeah, obviously it's

938
00:57:28,775 --> 00:57:32,695
Speaker 11:  illegal to acquire these digital books but there are many legal ways

939
00:57:32,695 --> 00:57:36,175
Speaker 11:  to acquire digital books and it is notable they didn't do that because you

940
00:57:36,175 --> 00:57:39,935
Speaker 11:  have the most maximalist freedom as a consumer with a

941
00:57:40,055 --> 00:57:40,495
Speaker 11:  physical copy.

942
00:57:40,965 --> 00:57:44,735
Speaker 5:  Yeah, it gets it. Like you, if you wanted to buy a bunch of Kindle books

943
00:57:45,205 --> 00:57:48,775
Speaker 5:  like they come with DRM, they come with license restrictions. You can, as

944
00:57:48,815 --> 00:57:51,935
Speaker 5:  I was saying, you can buy a bunch of eBooks without that but you still end

945
00:57:51,935 --> 00:57:54,895
Speaker 5:  up with like terms of service agreements. But you just go buy a book, you

946
00:57:54,895 --> 00:57:57,455
Speaker 5:  can kind of do a physical booking, gotta do whatever you want. So the judge

947
00:57:57,685 --> 00:58:01,255
Speaker 5:  says, okay, you made this database that was fair use

948
00:58:01,405 --> 00:58:04,575
Speaker 5:  ripping the covers off the physical books and scanning the database. That's

949
00:58:04,575 --> 00:58:05,895
Speaker 5:  fair use. You're allowed to do that.

950
00:58:07,675 --> 00:58:10,795
Speaker 5:  Stealing books and making a database. That's not fair use

951
00:58:11,445 --> 00:58:13,795
Speaker 5:  where stop there the end, you're in trouble for that.

952
00:58:14,295 --> 00:58:17,795
Speaker 6:  Is that even a fair use thing or is it the stealing the books?

953
00:58:18,105 --> 00:58:22,075
Speaker 5:  Well he's made the distinction between is it fair use to buy a bunch

954
00:58:22,075 --> 00:58:24,875
Speaker 5:  of physical books, rip the covers off and scan them? Okay, that's fair use.

955
00:58:25,415 --> 00:58:28,835
Speaker 5:  Is it fair use to steal a bunch of books? No,

956
00:58:29,025 --> 00:58:31,515
Speaker 11:  There's sort of a bunch of separate charges here. There are the charges

957
00:58:31,655 --> 00:58:35,475
Speaker 11:  of, all right, is it copyright infringement to do this or is it also separately

958
00:58:35,675 --> 00:58:37,475
Speaker 11:  copyright infringement to pirate all these books,

959
00:58:37,765 --> 00:58:41,715
Speaker 5:  Right? So and then philanthropic tried to fix it by

960
00:58:41,715 --> 00:58:45,195
Speaker 5:  saying the database is not important, training the model's important.

961
00:58:45,655 --> 00:58:49,635
Speaker 5:  And the judge said, okay, training the model from your legal database. That's

962
00:58:49,635 --> 00:58:53,435
Speaker 5:  fair use, that's very transformative. It especially because

963
00:58:53,495 --> 00:58:57,475
Speaker 5:  the authors one conceded that trading the model was just like reading

964
00:58:57,555 --> 00:59:00,395
Speaker 5:  a book, which is a big thing to concede, right?

965
00:59:00,765 --> 00:59:03,675
Speaker 6:  Which has been an argument of the AI industry for years, right? That it's

966
00:59:03,675 --> 00:59:05,995
Speaker 6:  like, it's the equivalent of reading a webpage and talking about it.

967
00:59:06,175 --> 00:59:08,515
Speaker 5:  And this is gonna come up over and over again. And I think this is why Addie

968
00:59:08,515 --> 00:59:11,955
Speaker 5:  And I both think these are fun. The the lawyers blew it for the authors like

969
00:59:11,955 --> 00:59:14,555
Speaker 5:  left and right and the judges are kind of like, you know, you guys are blowing

970
00:59:14,555 --> 00:59:18,435
Speaker 5:  it like try again but don't blow it. So the lawyers conceded

971
00:59:18,465 --> 00:59:22,445
Speaker 5:  that it was just like reading a book and then they did not argue the

972
00:59:22,475 --> 00:59:25,485
Speaker 5:  outputs of the model would affect the market for their work,

973
00:59:26,585 --> 00:59:29,575
Speaker 5:  right? So they stopped, and this is why it made the big distinction between

974
00:59:29,575 --> 00:59:33,375
Speaker 5:  inputs, snap outputs, they stopped at training, they did not get to,

975
00:59:33,915 --> 00:59:37,215
Speaker 5:  if you ask anthropic, if you ask Claude to make our book, it'll just make

976
00:59:37,215 --> 00:59:40,615
Speaker 5:  our book. Or if you ask Claude to summarize our book, it'll just like spit

977
00:59:40,615 --> 00:59:43,615
Speaker 5:  out the answer to our book and that will ruin the market for a book. They

978
00:59:43,615 --> 00:59:44,295
Speaker 5:  just didn't go there.

979
00:59:44,795 --> 00:59:47,695
Speaker 6:  And the judge is like, like this is where you need to explain the different

980
00:59:47,715 --> 00:59:50,295
Speaker 6:  tenets of of copyright and fair use stuff. Okay,

981
00:59:50,295 --> 00:59:53,615
Speaker 5:  So David, the four actors of fair use, here we are, we it, we got here. So

982
00:59:53,615 --> 00:59:54,615
Speaker 5:  inevitably you've

983
00:59:54,615 --> 00:59:58,255
Speaker 6:  Told me this so many times and it just, it just goes fully. My eyes glaze

984
00:59:58,255 --> 01:00:00,815
Speaker 6:  over every time you say this to me. So I'm gonna try and listen this time

985
01:00:01,045 --> 01:00:04,975
Speaker 5:  It's 17 United States code section 1 0 7 4 factors of

986
01:00:04,975 --> 01:00:08,775
Speaker 5:  phrase. So this is when when someone accuses you of

987
01:00:08,895 --> 01:00:11,735
Speaker 5:  stealing their work, you say no, no it's fair use. And so it's what's called

988
01:00:11,735 --> 01:00:15,415
Speaker 5:  an affirmative defense. You say, yep, I did, I did sample your song.

989
01:00:15,725 --> 01:00:19,665
Speaker 5:  This is fair use. I, I'm I'm, yep, you're

990
01:00:19,665 --> 01:00:22,625
Speaker 5:  right. I made a copy but it's fair use. And then the judges are supposed

991
01:00:22,625 --> 01:00:26,185
Speaker 5:  to evaluate this on a case by case basis. They have four factors, the purpose

992
01:00:26,285 --> 01:00:30,145
Speaker 5:  and character of the use, the nature of the copyrighted work, the amount

993
01:00:30,165 --> 01:00:33,545
Speaker 5:  and substantiality of the portion used and the effect on the use of the market

994
01:00:34,365 --> 01:00:38,305
Speaker 5:  and how important each of these factors are just

995
01:00:38,305 --> 01:00:42,185
Speaker 5:  goes up and down over the years. The Supreme Court issues fair

996
01:00:42,185 --> 01:00:46,105
Speaker 5:  use copyright decisions, you get law review articles being like the third

997
01:00:46,105 --> 01:00:49,865
Speaker 5:  factor is no longer in vogue. Like it's, everyone's always arguing this,

998
01:00:50,085 --> 01:00:54,055
Speaker 5:  But here what everyone should be arguing about is the

999
01:00:54,295 --> 01:00:57,595
Speaker 5:  fourth factor, the effect of the use on the market.

1000
01:00:58,355 --> 01:01:01,055
Speaker 5:  If you train a chatbot on all the books in the world and then you can just

1001
01:01:01,075 --> 01:01:04,175
Speaker 5:  ask the chat bott about those books and it will tell you the answer. The

1002
01:01:04,175 --> 01:01:06,455
Speaker 5:  market for those books will inevitably dry up.

1003
01:01:06,715 --> 01:01:09,375
Speaker 6:  And this, if I'm remembering correctly, is like the New York Times whole

1004
01:01:09,615 --> 01:01:12,815
Speaker 6:  argument in its case against open ai, right? That if you can get open AI

1005
01:01:12,815 --> 01:01:16,695
Speaker 6:  to regurgitate New York Times articles, you can, why would anyone

1006
01:01:16,695 --> 01:01:17,415
Speaker 6:  go to the New York Times?

1007
01:01:17,775 --> 01:01:20,695
Speaker 11:  I mean I would be careful with that because in that case it is specifically

1008
01:01:20,695 --> 01:01:24,095
Speaker 11:  output. They are saying that you could get them to regurgitate exact text

1009
01:01:24,515 --> 01:01:27,975
Speaker 11:  and here it's kind of more a, you can generate a bunch of stuff that imitates

1010
01:01:27,995 --> 01:01:31,535
Speaker 11:  us and that infringes on our ability to sell

1011
01:01:31,815 --> 01:01:35,775
Speaker 11:  things or also that it can summarize and it can give answers and that can

1012
01:01:35,775 --> 01:01:39,495
Speaker 11:  even if it's not really direct clear your copying our stuff

1013
01:01:39,495 --> 01:01:40,775
Speaker 11:  that's still diminishing the market.

1014
01:01:41,195 --> 01:01:44,775
Speaker 5:  And then in the anthropic case, the authors didn't allege the

1015
01:01:44,775 --> 01:01:45,215
Speaker 5:  outputs,

1016
01:01:45,825 --> 01:01:47,735
Speaker 6:  Right? They just didn't argue about it at all. They

1017
01:01:47,735 --> 01:01:50,055
Speaker 5:  Were totally focused on training and they didn't get to outputs. And the

1018
01:01:50,055 --> 01:01:52,575
Speaker 5:  judges even sort of saying throughout the opinion, you didn't talk about

1019
01:01:52,575 --> 01:01:56,365
Speaker 5:  the outputs. So your argument is merely training the

1020
01:01:56,365 --> 01:01:59,485
Speaker 5:  model will have some effect on the market for your work.

1021
01:02:00,365 --> 01:02:04,215
Speaker 5:  But like what is it? You didn't argue it's not here.

1022
01:02:04,285 --> 01:02:07,295
Speaker 5:  Like we don't have a record for it so I'm just not gonna talk about it. So

1023
01:02:07,295 --> 01:02:10,495
Speaker 5:  we're totally focused on training and not on inference, right? We're totally

1024
01:02:10,495 --> 01:02:13,215
Speaker 5:  focused on the inputs to train the model and not on the outputs of the model

1025
01:02:13,215 --> 01:02:17,115
Speaker 5:  itself. And so the authors lose, he says training

1026
01:02:17,115 --> 01:02:19,820
Speaker 5:  is fair use. This is totally transformative. You can take, you can legally

1027
01:02:19,820 --> 01:02:23,205
Speaker 5:  acquire books, you can put them into a database, you can use that database

1028
01:02:23,385 --> 01:02:27,365
Speaker 5:  to train a model. Making the database is fair use, making the

1029
01:02:27,365 --> 01:02:30,805
Speaker 5:  model from what's in the database is fair use and then stop,

1030
01:02:32,795 --> 01:02:36,665
Speaker 5:  right? And like there's a big yawning gap there of well now you have a model

1031
01:02:36,725 --> 01:02:39,825
Speaker 5:  what's going, what's, what do you think is gonna happen? And the answer is

1032
01:02:39,825 --> 01:02:41,985
Speaker 5:  like, oh, you're gonna make a bunch of stuff that competes with the author's

1033
01:02:42,185 --> 01:02:45,945
Speaker 5:  original works. And so that's a real puzzle. Like I, who

1034
01:02:45,945 --> 01:02:49,185
Speaker 5:  knows what happens next. And he even leaves it open like

1035
01:02:49,805 --> 01:02:52,705
Speaker 5:  if you wanna ledge the next thing, you can ledge the next thing. And then

1036
01:02:52,705 --> 01:02:56,465
Speaker 5:  on top of it he says, you what you are not allowed to do is

1037
01:02:56,465 --> 01:03:00,335
Speaker 5:  pirate a bunch of books and make a database he doesn't even get to.

1038
01:03:00,395 --> 01:03:04,175
Speaker 5:  And then you can train the model he just says, doing that first

1039
01:03:04,175 --> 01:03:07,415
Speaker 5:  step where you stole a bunch of books and put them in a database, that's

1040
01:03:07,415 --> 01:03:11,225
Speaker 5:  not fair use. You can't do that. And that is a nightmare. Like

1041
01:03:11,245 --> 01:03:15,105
Speaker 5:  you might read this as training the model as fair use for your legally acquired

1042
01:03:15,425 --> 01:03:17,505
Speaker 5:  physical books that you've ripped the covers off and scanning your database.

1043
01:03:17,815 --> 01:03:21,425
Speaker 5:  What it does not say is it's fair game for anything, right? And so if you,

1044
01:03:21,565 --> 01:03:25,095
Speaker 5:  for example, have opened YouTube

1045
01:03:25,595 --> 01:03:29,535
Speaker 5:  and downloaded a bunch of YouTube videos, I'm looking at you open ai,

1046
01:03:30,635 --> 01:03:33,885
Speaker 5:  well you violated YouTube's terms of service, the copyright and all those

1047
01:03:33,885 --> 01:03:37,725
Speaker 5:  videos is owned by the YouTubers. You have a, you

1048
01:03:37,725 --> 01:03:41,685
Speaker 5:  have a database of content that you're training SOA on that

1049
01:03:41,685 --> 01:03:45,445
Speaker 5:  you don't have the rights to. And so this anthropic decision has

1050
01:03:45,585 --> 01:03:49,525
Speaker 5:  opened the door to being like, we're not even gonna get to training as fair

1051
01:03:49,525 --> 01:03:52,365
Speaker 5:  use, we're gonna get to your illegal database of YouTube videos

1052
01:03:54,105 --> 01:03:57,205
Speaker 5:  and you can't, you're not allowed to clean up that problem by saying training

1053
01:03:57,205 --> 01:04:01,085
Speaker 5:  is fair use. So this first decision, very complicated,

1054
01:04:01,315 --> 01:04:05,085
Speaker 5:  like very technical, very readable. We've profiled judge sup before Sarah

1055
01:04:05,145 --> 01:04:08,725
Speaker 5:  has profiled him. He writes code, he's, he's in the weeds of the the,

1056
01:04:09,305 --> 01:04:13,205
Speaker 5:  the technical details here. But what you have is training

1057
01:04:13,205 --> 01:04:16,805
Speaker 5:  is fair use if you have a legally acquired set of content to train from.

1058
01:04:17,035 --> 01:04:20,085
Speaker 11:  There's also kind of a backwards looking and a forward-looking issue here.

1059
01:04:20,085 --> 01:04:22,925
Speaker 11:  Which is say in the future, if you're kind of just starting from scratch,

1060
01:04:22,955 --> 01:04:26,805
Speaker 11:  then the upshot of this is well okay you gotta buy a million books

1061
01:04:26,915 --> 01:04:30,605
Speaker 11:  that costs a lot. It's a finite number and

1062
01:04:30,605 --> 01:04:34,085
Speaker 11:  backwards looking. If you've already done all this stuff, the potential

1063
01:04:34,085 --> 01:04:37,925
Speaker 11:  damages for each work are just like ruinous if you add them up

1064
01:04:37,925 --> 01:04:41,685
Speaker 11:  to a large enough number. And so anybody who's starting from scratch

1065
01:04:41,685 --> 01:04:45,645
Speaker 11:  with a model that is starting acquiring all this stuff legally, like that's

1066
01:04:45,645 --> 01:04:49,005
Speaker 11:  one economic model. Any of the big players, that's a whole different one.

1067
01:04:49,315 --> 01:04:51,005
Speaker 11:  Yeah. 'cause they've already done all this.

1068
01:04:51,425 --> 01:04:55,175
Speaker 5:  And again, open ai, open AI has a lot of money, but every

1069
01:04:55,175 --> 01:04:57,775
Speaker 5:  YouTuber in the world can read this decision and say, well you, you stole

1070
01:04:57,775 --> 01:05:01,615
Speaker 5:  our videos. And it wasn't, you can't buy physical YouTube

1071
01:05:01,615 --> 01:05:04,975
Speaker 5:  videos and rip the covers off of them. And in the database you access 'em

1072
01:05:04,975 --> 01:05:07,895
Speaker 5:  on YouTube, which has terms of service that says you cannot do this.

1073
01:05:09,015 --> 01:05:12,515
Speaker 5:  So this is a, this is already like puzzle one, this decision.

1074
01:05:13,135 --> 01:05:15,635
Speaker 5:  You read it and you're like, okay, there's a way forward if you pay a bunch

1075
01:05:15,635 --> 01:05:18,725
Speaker 5:  of authors and maybe it doesn't even work, right? Used books don't pay the

1076
01:05:18,725 --> 01:05:22,485
Speaker 5:  authors, but there's, there's parts of copyright law that say you're allowed

1077
01:05:22,485 --> 01:05:25,525
Speaker 5:  to buy used books and do whatever you want with them. And this ruling reaffirms

1078
01:05:25,525 --> 01:05:28,845
Speaker 5:  that. It does not say you can just pirate a bunch of stuff and train it and

1079
01:05:28,845 --> 01:05:31,845
Speaker 5:  call that fair use. Okay, so, okay, so here's a bunch of liability for the

1080
01:05:31,965 --> 01:05:35,845
Speaker 5:  industry. Then literally the next day a decision

1081
01:05:36,065 --> 01:05:39,785
Speaker 5:  in the Meta case comes down, which Addie I'm reading

1082
01:05:39,925 --> 01:05:43,385
Speaker 5:  is basically being like, no, actually Judge Alsup got this wrong. Like this

1083
01:05:43,385 --> 01:05:44,665
Speaker 5:  is all theft left and right.

1084
01:05:45,335 --> 01:05:48,505
Speaker 11:  It's basically Judge Allsup got all the fundamentals of this right? But

1085
01:05:48,505 --> 01:05:52,265
Speaker 11:  he weighted the transformative factor, overarching factor and the fourth

1086
01:05:52,745 --> 01:05:56,345
Speaker 11:  economic market value factor totally wrong. And that makes him

1087
01:05:56,595 --> 01:05:58,105
Speaker 11:  completely wrong on all of it.

1088
01:05:58,295 --> 01:06:01,185
Speaker 5:  Yeah. Go through this case. So this case is slightly different but it's a

1089
01:06:01,185 --> 01:06:04,975
Speaker 5:  bunch of authors suing Meta and if I'm correct,

1090
01:06:05,085 --> 01:06:08,775
Speaker 5:  Meta got caught using Torrented books like Straightforwardly. They just torned

1091
01:06:08,775 --> 01:06:09,015
Speaker 5:  the books

1092
01:06:09,245 --> 01:06:12,895
Speaker 11:  From Library Genesis I believe. Yeah. That collection of

1093
01:06:13,095 --> 01:06:16,735
Speaker 11:  books that was just this very large corpus of pirated books. They

1094
01:06:17,525 --> 01:06:21,215
Speaker 11:  torrented them. There's debate about whether that included them

1095
01:06:21,215 --> 01:06:23,975
Speaker 11:  uploading things, which one typically does when you torrent things.

1096
01:06:24,475 --> 01:06:28,055
Speaker 5:  The classic seaters versus Leers debate hits their nation's copyright law.

1097
01:06:28,165 --> 01:06:32,015
Speaker 11:  Yeah. They have to define seating and leaching in there. There's

1098
01:06:32,295 --> 01:06:35,295
Speaker 11:  a whole section about that. But they're specifically not dealing with the

1099
01:06:35,295 --> 01:06:39,095
Speaker 11:  copyright. Like was this direct copyright infringement

1100
01:06:39,095 --> 01:06:41,415
Speaker 11:  right now? Except to say, yeah look, it basically was.

1101
01:06:43,035 --> 01:06:46,535
Speaker 11:  So there's that. And then After that these authors

1102
01:06:46,765 --> 01:06:50,695
Speaker 11:  sued in a pretty similar way except that they

1103
01:06:51,245 --> 01:06:54,895
Speaker 11:  also specifically said the issue here is

1104
01:06:54,895 --> 01:06:58,615
Speaker 11:  partly that it is recreating our works. So they did focus on output

1105
01:06:59,395 --> 01:07:02,735
Speaker 11:  and then also that there's this market potentially for us

1106
01:07:02,805 --> 01:07:06,575
Speaker 11:  licensing our work to you and that that's a market that you're

1107
01:07:06,875 --> 01:07:10,695
Speaker 11:  taking from us. The judge called both of these clear

1108
01:07:10,755 --> 01:07:14,735
Speaker 11:  losers. He basically thinks everyone here is wrong. So

1109
01:07:15,595 --> 01:07:19,455
Speaker 11:  the Meta won the case, Meta won the case just handily. And it's

1110
01:07:19,455 --> 01:07:22,255
Speaker 11:  because the judge says, look, those are the only things you focused on.

1111
01:07:22,275 --> 01:07:25,975
Speaker 11:  And both of those are bad arguments, partly because they couldn't get

1112
01:07:26,375 --> 01:07:29,775
Speaker 11:  llama to actually spit out more than 50 words of the direct text.

1113
01:07:30,435 --> 01:07:33,935
Speaker 11:  So they were trying to say, look, you're verbatim

1114
01:07:34,205 --> 01:07:38,055
Speaker 11:  producing stuff and it just didn't work out. And then the second

1115
01:07:38,155 --> 01:07:41,535
Speaker 11:  one, because you just can't necessarily say,

1116
01:07:42,125 --> 01:07:46,095
Speaker 11:  yeah, if I were allowed to license this, I would make a lot of money. That

1117
01:07:46,095 --> 01:07:49,855
Speaker 11:  doesn't necessarily mean you should be able to exploit

1118
01:07:49,855 --> 01:07:50,135
Speaker 11:  that.

1119
01:07:51,945 --> 01:07:55,805
Speaker 11:  And so then after saying, look, concluding just

1120
01:07:55,995 --> 01:07:59,525
Speaker 11:  this argument is terrible, these lawyers are bad, please don't do this.

1121
01:08:00,165 --> 01:08:03,885
Speaker 11:  They said there's this other argument that would've been way

1122
01:08:04,125 --> 01:08:07,485
Speaker 11:  stronger. And it's that the fundamental use of these things

1123
01:08:07,905 --> 01:08:11,765
Speaker 11:  is to flood the market in a way that destroys the ability of

1124
01:08:11,765 --> 01:08:15,445
Speaker 11:  authors to make a living and completely diminishes the thing that copyright

1125
01:08:15,445 --> 01:08:18,845
Speaker 11:  is meant to do. Which is encourage human creativity.

1126
01:08:19,155 --> 01:08:23,045
Speaker 11:  Basically ais are creativity killing machines and

1127
01:08:23,075 --> 01:08:26,485
Speaker 11:  that should mean that it outweighs any potential transformative value.

1128
01:08:27,105 --> 01:08:30,695
Speaker 5:  So I wanna just quote from the opinion because it is brutal.

1129
01:08:32,625 --> 01:08:36,075
Speaker 5:  This ruling does not stand for the proposition that meta's use of copy

1130
01:08:36,475 --> 01:08:40,155
Speaker 5:  materials to train its language models is lawful. It stands only for the

1131
01:08:40,155 --> 01:08:43,875
Speaker 5:  proposition that these plaintiffs made the wrong arguments and

1132
01:08:43,875 --> 01:08:46,475
Speaker 5:  failed to develop a record in support of the right one.

1133
01:08:47,865 --> 01:08:49,515
Speaker 5:  He's just saying, you screwed this up. Yeah,

1134
01:08:49,535 --> 01:08:50,515
Speaker 6:  You did it wrong,

1135
01:08:51,095 --> 01:08:51,755
Speaker 5:  You did it wrong.

1136
01:08:51,955 --> 01:08:55,195
Speaker 11:  And then mentions, by the way, these guys only, they only represent 13 artists.

1137
01:08:56,195 --> 01:08:59,275
Speaker 11:  13 authors. So you know, there are a lot of other authors and maybe Meta

1138
01:08:59,585 --> 01:09:02,075
Speaker 11:  scan them. It probably did. Just saying that.

1139
01:09:02,465 --> 01:09:04,995
Speaker 6:  Yeah. Yeah. Wait, okay, so let, let's talk about this here. 'cause the, the

1140
01:09:05,235 --> 01:09:07,875
Speaker 6:  reaction to this this week has been fascinating because I think it's been

1141
01:09:07,945 --> 01:09:11,595
Speaker 6:  sort of a Rorschach test depending on how

1142
01:09:12,095 --> 01:09:15,955
Speaker 6:  who you are promoting here. All of the AI folks are like, this is

1143
01:09:15,995 --> 01:09:19,875
Speaker 6:  a landmark victory for ai. This puts us on the path

1144
01:09:19,895 --> 01:09:23,835
Speaker 6:  to do all the stuff that we needed to do. But then it also sounds like

1145
01:09:23,835 --> 01:09:26,675
Speaker 6:  there's a, there's an easy read of this that this is like a small victory

1146
01:09:27,335 --> 01:09:31,235
Speaker 6:  for these companies who won this case on a technicality

1147
01:09:32,335 --> 01:09:36,075
Speaker 6:  but have just had the door thrown wide open for everyone else on earth to

1148
01:09:36,075 --> 01:09:39,795
Speaker 6:  sue them better and win. Yeah. Which of those things is it?

1149
01:09:40,015 --> 01:09:40,995
Speaker 6:  Or is it both of those things?

1150
01:09:41,185 --> 01:09:44,955
Speaker 5:  It's the second one. Okay. It's, it's, so by far the second one, I think

1151
01:09:45,495 --> 01:09:49,075
Speaker 5:  you know, Addie, and when Sarah's on And I, whenever we're talking about

1152
01:09:49,075 --> 01:09:52,435
Speaker 5:  the legal system on the show, we are constantly reminding people

1153
01:09:52,825 --> 01:09:56,595
Speaker 5:  that the courts are not computers, they're not deterministic systems

1154
01:09:57,585 --> 01:10:01,515
Speaker 5:  that you cannot predict what will happen, right? And So I, I think the

1155
01:10:01,575 --> 01:10:04,995
Speaker 5:  AI companies are like, this is fair use and then they got an outcome that

1156
01:10:05,105 --> 01:10:08,435
Speaker 5:  kind of looks like this is fair use. And they're like, see, we predicted

1157
01:10:08,435 --> 01:10:11,115
Speaker 5:  this correctly. We were right all along. And what you actually have is the

1158
01:10:11,115 --> 01:10:15,035
Speaker 5:  court saying, these lawyers sucked. And if you had better lawyers and

1159
01:10:15,035 --> 01:10:18,635
Speaker 5:  you argued these things differently, we would have reached different outcomes.

1160
01:10:19,415 --> 01:10:23,195
Speaker 5:  But we can't reach those outcomes in the absence of your better lawyers.

1161
01:10:23,815 --> 01:10:27,395
Speaker 5:  And that is a maddening to people. I know it's bating to people because I

1162
01:10:27,395 --> 01:10:30,675
Speaker 5:  have the blue sky app on my phone And I open it from time to time and it's

1163
01:10:30,675 --> 01:10:34,555
Speaker 5:  just people being mad that these lawyers blew it and it, they blew

1164
01:10:34,555 --> 01:10:38,395
Speaker 5:  it. And the judge is saying flatly, they blew it. I don't know

1165
01:10:38,395 --> 01:10:42,155
Speaker 5:  why in the anthropic case, the lawyers happily conceded that

1166
01:10:42,595 --> 01:10:45,755
Speaker 5:  training an AM model is like letting school children read books to learn

1167
01:10:45,755 --> 01:10:49,715
Speaker 5:  things. I wouldn't have conceded that. I do know that it

1168
01:10:49,715 --> 01:10:53,515
Speaker 5:  costs a lot of money to build an expert record about how

1169
01:10:53,655 --> 01:10:57,355
Speaker 5:  AI systems versus work versus the brains of six year olds or whatever.

1170
01:10:57,815 --> 01:11:00,915
Speaker 5:  And they may not have had that money, but you can't know. And that's why

1171
01:11:00,915 --> 01:11:04,555
Speaker 5:  it's not deterministic. Like a lot of decisions got made along the way

1172
01:11:05,925 --> 01:11:09,665
Speaker 5:  and then we have these rulings. But there are more cases to come

1173
01:11:09,725 --> 01:11:13,205
Speaker 5:  as Addie's saying that the court and the Meta case is basically inviting

1174
01:11:13,205 --> 01:11:16,565
Speaker 5:  other authors to try to make the better argument that they have laid out.

1175
01:11:17,685 --> 01:11:20,785
Speaker 5:  You have the New York Times, which is over a million dollars deep into its

1176
01:11:20,785 --> 01:11:24,585
Speaker 5:  open AI litigation that's watching these outcomes and will change its

1177
01:11:24,705 --> 01:11:28,545
Speaker 5:  arguments. Then you have Disney, which is a very late entrant into all these

1178
01:11:28,545 --> 01:11:31,825
Speaker 5:  copyright lawsuits. They're partnered up with Universal and together they're

1179
01:11:31,945 --> 01:11:34,705
Speaker 5:  suing Midjourney about outputs.

1180
01:11:35,775 --> 01:11:38,665
Speaker 5:  They, they like they're, they're watching these cases develop, they're watching

1181
01:11:38,665 --> 01:11:41,305
Speaker 5:  these arguments and they're like, oh, we need to focus on outputs. By the

1182
01:11:41,305 --> 01:11:44,545
Speaker 5:  way, I need to disclose that Universals home by Comcast. NBC Universal is

1183
01:11:44,585 --> 01:11:47,705
Speaker 5:  a part of Comcast is an investor in our parent company. Our parent company

1184
01:11:47,765 --> 01:11:51,225
Speaker 5:  has a deal with OpenAI. One of the reasons these companies have deals with

1185
01:11:51,245 --> 01:11:54,825
Speaker 5:  OpenAI is because they wanna set markets for licensing, they wanna set rates.

1186
01:11:55,165 --> 01:11:58,945
Speaker 5:  So these lawsuits can proceed whatever way they're gonna proceed.

1187
01:11:59,685 --> 01:12:03,675
Speaker 5:  You can see it's not settled. These are just the first little

1188
01:12:03,765 --> 01:12:07,395
Speaker 5:  skirmishes. And a lot of times what's happening is that

1189
01:12:07,575 --> 01:12:11,155
Speaker 5:  the first plaintiffs are authors are

1190
01:12:11,595 --> 01:12:14,675
Speaker 5:  creatives who don't have the money for the most lawyering who need to get

1191
01:12:14,675 --> 01:12:18,635
Speaker 5:  to outcomes quickly. And they're getting outgunned by the lawyers

1192
01:12:18,775 --> 01:12:21,995
Speaker 5:  for the huge technology firms. That's not always gonna be the case.

1193
01:12:22,095 --> 01:12:25,395
Speaker 11:  Record labels are also in on this, oh, the record labels have sued

1194
01:12:25,825 --> 01:12:26,755
Speaker 11:  Suno and, and uio,

1195
01:12:27,435 --> 01:12:29,515
Speaker 5:  Although they're starting to settle some of those on the side.

1196
01:12:31,215 --> 01:12:34,955
Speaker 5:  I'm just saying like the legal system is not istic, it is also reactive to

1197
01:12:34,955 --> 01:12:38,475
Speaker 5:  what happens before. Like necessarily it's precedential, it's reactive to

1198
01:12:38,475 --> 01:12:41,795
Speaker 5:  these cases. And the arguments in the future cases will shift around these

1199
01:12:41,995 --> 01:12:45,715
Speaker 5:  decisions to attack their weak points. Or in the case of the Meta

1200
01:12:45,955 --> 01:12:49,195
Speaker 5:  decision to take the open invitation to get it right.

1201
01:12:50,995 --> 01:12:53,875
Speaker 11:  I would also say we're at the point where basically this is kind of every

1202
01:12:53,925 --> 01:12:57,755
Speaker 11:  judge for themselves at the point where you appeal and then you start getting

1203
01:12:58,065 --> 01:13:01,715
Speaker 11:  appeals court cases and then circuit courts start saying, alright,

1204
01:13:01,735 --> 01:13:04,875
Speaker 11:  here's roughly how we think about it. Then judges have

1205
01:13:05,585 --> 01:13:08,555
Speaker 11:  kind of precedents that they're trying to draw from. And of course if the

1206
01:13:08,555 --> 01:13:11,075
Speaker 11:  Supreme Court says something, then that's something that you have to weight

1207
01:13:11,075 --> 01:13:14,675
Speaker 11:  pretty heavily. But at this point they can kind of just stay stuff to each

1208
01:13:14,675 --> 01:13:18,475
Speaker 11:  other. So it's also not like this one ruling that say

1209
01:13:18,475 --> 01:13:22,235
Speaker 11:  Allsup hands down is a law that everyone else has to follow.

1210
01:13:22,595 --> 01:13:26,035
Speaker 11:  Somebody else can come up with their own theory of fair use, which is pretty

1211
01:13:26,035 --> 01:13:27,115
Speaker 11:  much exactly what we've seen.

1212
01:13:27,445 --> 01:13:31,275
Speaker 5:  Right. And fair use in the law is supposed to be case

1213
01:13:31,295 --> 01:13:35,245
Speaker 5:  by case, right? So one fairies decision is not supposed

1214
01:13:35,265 --> 01:13:39,125
Speaker 5:  to affect all the other ones in reality because of the money and time

1215
01:13:39,125 --> 01:13:42,365
Speaker 5:  and risk involved, they, they get treated as precedent.

1216
01:13:43,355 --> 01:13:47,175
Speaker 5:  But here everybody has incentives to reopen those doors again and again and

1217
01:13:47,175 --> 01:13:51,015
Speaker 5:  again. So I I I I don dunno. Addie what do you think? I think this

1218
01:13:51,115 --> 01:13:55,095
Speaker 5:  is a bunch of small narrow rulings that open the door to much more caste

1219
01:13:55,095 --> 01:13:58,975
Speaker 5:  to come. Not definitive reckoning, but I maybe you feel differently.

1220
01:13:59,415 --> 01:13:59,535
Speaker 5:  I

1221
01:13:59,535 --> 01:14:03,095
Speaker 11:  Don't know in part because I feel like sometimes court cases

1222
01:14:03,395 --> 01:14:07,255
Speaker 11:  are a little bit just they're reflective of a larger vibe

1223
01:14:07,275 --> 01:14:10,815
Speaker 11:  and culture that it, like I think we've seen with section two 30 cases

1224
01:14:11,075 --> 01:14:15,005
Speaker 11:  that it's not necessarily that judges all impact each other, but

1225
01:14:15,005 --> 01:14:18,565
Speaker 11:  that if you start seeing a bunch of rulings around one kind of particular

1226
01:14:18,565 --> 01:14:21,805
Speaker 11:  interpretation of the law or one kind of like weighting of something,

1227
01:14:22,395 --> 01:14:25,845
Speaker 11:  that sometimes means you are seeing a pattern that's gonna continue. It's,

1228
01:14:25,845 --> 01:14:29,765
Speaker 11:  it's correlation, not causation. And also, I don't know,

1229
01:14:29,865 --> 01:14:32,885
Speaker 11:  I'm actually wondering what would happen if all Hepa case that was more

1230
01:14:33,085 --> 01:14:36,805
Speaker 11:  specifically output based. Because on one hand, yeah, it opens up a

1231
01:14:36,805 --> 01:14:40,325
Speaker 11:  bunch of whole new cans of worms on the other, some of his comments

1232
01:14:40,585 --> 01:14:43,805
Speaker 11:  did seem like they kind of indicated it doesn't matter if somebody's say

1233
01:14:43,805 --> 01:14:47,365
Speaker 11:  writing a book with an AI system that

1234
01:14:47,365 --> 01:14:51,005
Speaker 11:  competes with yours, that doesn't inherently mean that

1235
01:14:51,005 --> 01:14:52,245
Speaker 11:  they're, that is unfair.

1236
01:14:52,835 --> 01:14:56,085
Speaker 5:  Yeah. He's got a line that's like, copyright law does not protect you from

1237
01:14:56,085 --> 01:14:59,605
Speaker 5:  competition. It's supposed to inspire more people to create. And there's

1238
01:14:59,645 --> 01:15:02,685
Speaker 5:  a, you know, there's a very deep meaningful tension in there

1239
01:15:03,275 --> 01:15:07,085
Speaker 5:  because the, the mechanism by which copyright law is supposed to inspire

1240
01:15:07,085 --> 01:15:09,765
Speaker 5:  you to create is by letting you sell your stuff,

1241
01:15:10,825 --> 01:15:13,935
Speaker 5:  right? You create stuff, you get rights to it, other people can't take it

1242
01:15:13,995 --> 01:15:17,895
Speaker 5:  for free. They have to pay you money. But at the same time, if

1243
01:15:17,895 --> 01:15:20,095
Speaker 5:  they take your stuff for free and then make stuff that competes with you,

1244
01:15:20,095 --> 01:15:23,855
Speaker 5:  like that's competition. Like there's something really weird in there. But

1245
01:15:23,855 --> 01:15:27,095
Speaker 11:  There are also a lot of pre AI cases where I think people would be a lot

1246
01:15:27,095 --> 01:15:31,055
Speaker 11:  more sympathetic to the idea that you should be able to say copy

1247
01:15:31,055 --> 01:15:34,855
Speaker 11:  things without having to get permission. Like the

1248
01:15:34,855 --> 01:15:38,535
Speaker 11:  fact that you can report on facts that somebody says in a news

1249
01:15:38,535 --> 01:15:42,375
Speaker 11:  story without getting permission from the original writer of that story.

1250
01:15:42,925 --> 01:15:46,855
Speaker 11:  Like the fact that you can do a pastiche of

1251
01:15:46,855 --> 01:15:50,655
Speaker 11:  an artist and that artist's style is not necessarily a thing that's automatically

1252
01:15:50,925 --> 01:15:54,895
Speaker 11:  copyrightable. I think these are all things that when humans are involved,

1253
01:15:55,515 --> 01:15:59,495
Speaker 11:  we actually are pretty okay with the idea that creativity is cumulative

1254
01:15:59,495 --> 01:16:02,855
Speaker 11:  and people build on each other. I think the thing that's really interesting

1255
01:16:02,895 --> 01:16:06,615
Speaker 11:  about the Meta ruling is that it specifically makes the argument that scale

1256
01:16:06,675 --> 01:16:09,575
Speaker 11:  is different, which is something I see a bunch of people make

1257
01:16:10,005 --> 01:16:13,695
Speaker 11:  pragmatically, but I don't think I've seen in the court system as much that

1258
01:16:13,695 --> 01:16:17,535
Speaker 11:  the argument isn't actually, well these things are fundamentally

1259
01:16:17,535 --> 01:16:21,495
Speaker 11:  different and not competitive because the writing is somehow different or

1260
01:16:21,495 --> 01:16:25,215
Speaker 11:  because something's not necessarily learning. It's because

1261
01:16:25,435 --> 01:16:28,655
Speaker 11:  you can create all of them so quickly that they will flood the market and

1262
01:16:28,655 --> 01:16:32,615
Speaker 11:  they will destroy the market for books specifically thinks romance

1263
01:16:32,715 --> 01:16:35,135
Speaker 11:  novels and spy thrillers, romance novels come up a bunch in this,

1264
01:16:37,955 --> 01:16:41,295
Speaker 5:  The whole, the entire like romance novel like Fanfic community right now

1265
01:16:41,315 --> 01:16:44,335
Speaker 5:  is like deeply insulted, right? They're at war seriously with AI

1266
01:16:44,335 --> 01:16:47,175
Speaker 11:  Every day serious. Yeah. Romance authors are starting to use AI and romance

1267
01:16:47,245 --> 01:16:48,855
Speaker 11:  readers are just furious about it.

1268
01:16:49,045 --> 01:16:51,775
Speaker 5:  Well, it's funny 'cause you, you, we've talked about this many times. So

1269
01:16:51,775 --> 01:16:55,455
Speaker 5:  if you go on those forums, like now people just say tropes all in a row and

1270
01:16:55,565 --> 01:16:59,455
Speaker 5:  request stories and like, oh, that's what AI is for. Like, I, I just want

1271
01:16:59,455 --> 01:16:59,815
Speaker 5:  this to happen.

1272
01:17:01,365 --> 01:17:04,695
Speaker 5:  Wild interactions in that community. The scale problem is really interesting

1273
01:17:04,925 --> 01:17:08,175
Speaker 5:  because if, you know, we just play this game right now on the show,

1274
01:17:08,755 --> 01:17:12,445
Speaker 5:  the moral dilemmas of copyright law become plain. So if I say to you,

1275
01:17:12,855 --> 01:17:16,325
Speaker 5:  David, can you rip a CD onto your computer without paying extra money to

1276
01:17:16,325 --> 01:17:19,405
Speaker 5:  a record label, what would you say? Yes. Can you rip 10 CDs?

1277
01:17:20,205 --> 01:17:23,585
Speaker 5:  Yes. Can you David Pierce rip 1000 CDs to your computer without paying record

1278
01:17:23,585 --> 01:17:26,745
Speaker 5:  labels additional money, I guess in theory, yeah. If you had to hard drive

1279
01:17:26,745 --> 01:17:29,305
Speaker 5:  space, could you rip a million CDs to your computer without paying record

1280
01:17:29,365 --> 01:17:33,265
Speaker 5:  labels? You David Pierce? Yeah, I think so. Can Meta do that?

1281
01:17:35,175 --> 01:17:35,395
Speaker 5:  No.

1282
01:17:37,705 --> 01:17:41,435
Speaker 5:  Like why? Like this is the problem, right? Right. You write the rule

1283
01:17:41,825 --> 01:17:45,715
Speaker 5:  that says it's okay to like rip CDs or we,

1284
01:17:45,815 --> 01:17:47,515
Speaker 5:  we got a lot of plexus on the

1285
01:17:47,515 --> 01:17:50,915
Speaker 11:  Show. Wait, fair use can account for this right there. Part of the consideration

1286
01:17:50,915 --> 01:17:53,075
Speaker 11:  of fair use is whether something is commercial or not.

1287
01:17:53,185 --> 01:17:56,955
Speaker 5:  Part of the consideration is whether it's commercial, but in, in the

1288
01:17:56,955 --> 01:18:00,595
Speaker 5:  anthropic case, judge s says, you're allowed to just make a database, right?

1289
01:18:00,595 --> 01:18:03,315
Speaker 5:  If what you wanna do is make it easier to search, make it easier to run different

1290
01:18:03,315 --> 01:18:06,635
Speaker 5:  applications on, it's totally fine for you to buy a million books, rip the

1291
01:18:06,635 --> 01:18:09,875
Speaker 5:  covers off and scan 'em into a database. It's the next use

1292
01:18:10,385 --> 01:18:14,235
Speaker 5:  that gets you into trouble, right? It's okay, is it training

1293
01:18:14,255 --> 01:18:17,835
Speaker 5:  the AI model? Is that fair use if you have the database? He says yes.

1294
01:18:18,335 --> 01:18:22,195
Speaker 5:  If you were to say, I've now made this database publicly available for

1295
01:18:22,215 --> 01:18:26,195
Speaker 5:  anyone to read all the books or listen to all the music in my database, you

1296
01:18:26,195 --> 01:18:30,075
Speaker 5:  would go to jail, right? Like that's, the internet archive

1297
01:18:30,075 --> 01:18:33,315
Speaker 5:  is in trouble for their database of books that they acquired. They scanned

1298
01:18:33,315 --> 01:18:37,035
Speaker 5:  it and they let everybody have access to. So the questions

1299
01:18:37,095 --> 01:18:40,875
Speaker 5:  get really difficult the second you start not just adding scale,

1300
01:18:42,575 --> 01:18:45,995
Speaker 5:  but adding different uses of what they're gonna do. And some of those uses

1301
01:18:45,995 --> 01:18:49,755
Speaker 5:  at commercial change the equation. And this is the problem. This is why

1302
01:18:49,755 --> 01:18:52,835
Speaker 5:  everybody hates copyright law. This is why everybody hates lawyers. You start

1303
01:18:52,835 --> 01:18:56,315
Speaker 5:  to get incredibly pedantic about what happened, when to whom,

1304
01:18:56,695 --> 01:18:59,875
Speaker 5:  who made the copy, when, where does the copy live? And everyone's like, it's

1305
01:18:59,875 --> 01:19:02,435
Speaker 5:  just a computer, right? I can just like open it up and get whatever I want.

1306
01:19:03,505 --> 01:19:06,565
Speaker 5:  And then underneath that, all of those questions determine who gets paid.

1307
01:19:07,485 --> 01:19:11,225
Speaker 5:  And I think here, you know, the big problem with the the,

1308
01:19:11,325 --> 01:19:15,185
Speaker 5:  the anthropic fair use ruling is one, if you buy

1309
01:19:15,265 --> 01:19:18,745
Speaker 5:  a million used books and train a model that generates billions of dollars

1310
01:19:18,745 --> 01:19:22,495
Speaker 5:  in value, the authors who wrote those books get nothing. That,

1311
01:19:22,615 --> 01:19:26,175
Speaker 5:  that's just the way it works in our society. If they pay a one-time fee to

1312
01:19:26,235 --> 01:19:30,215
Speaker 5:  buy a million brand new books and then train a model that

1313
01:19:30,335 --> 01:19:33,815
Speaker 5:  generates billions of dollars in revenue, the authors essentially on that

1314
01:19:33,815 --> 01:19:37,055
Speaker 5:  curve get nothing. They get paid one time. Is that fair?

1315
01:19:37,835 --> 01:19:40,525
Speaker 5:  Like I don't, I I truly don't know the answers to those questions. And these

1316
01:19:40,525 --> 01:19:41,845
Speaker 5:  really kind of don't get you there.

1317
01:19:42,285 --> 01:19:45,125
Speaker 11:  I don't know. I think that the Meta ruling is interesting because it does

1318
01:19:45,405 --> 01:19:48,885
Speaker 11:  actually kind of cut through all this because everything in copyright is

1319
01:19:48,885 --> 01:19:52,525
Speaker 11:  very angels dancing on a head of a pin. Like is putting something in ram

1320
01:19:52,685 --> 01:19:53,045
Speaker 11:  a copy.

1321
01:19:54,945 --> 01:19:58,645
Speaker 11:  But the overall argument is the whole reason copyright

1322
01:19:58,645 --> 01:20:02,445
Speaker 11:  exists is to encourage human creativity. And so the ultimate bar should

1323
01:20:02,445 --> 01:20:06,205
Speaker 11:  not be is this fair or is this hitting this

1324
01:20:06,205 --> 01:20:09,765
Speaker 11:  particular place where it starts being uncomfortable?

1325
01:20:09,985 --> 01:20:13,885
Speaker 11:  Or is the company that's doing this making it commercial? It

1326
01:20:13,885 --> 01:20:17,205
Speaker 11:  is, when you look at this thing holistically, does it serve the purpose

1327
01:20:17,235 --> 01:20:21,125
Speaker 11:  that the law is meant to serve? And that's not mostly how courts

1328
01:20:21,195 --> 01:20:24,605
Speaker 11:  work, but I think if you are considering this from a larger kind of philosophical

1329
01:20:24,695 --> 01:20:27,245
Speaker 11:  sense, that is interesting and worth considering.

1330
01:20:27,615 --> 01:20:31,365
Speaker 5:  Sarah and I have often crashed out because we both started as

1331
01:20:31,435 --> 01:20:35,365
Speaker 5:  like copyright minimalist warriors, you know, like

1332
01:20:35,365 --> 01:20:39,045
Speaker 5:  in our legal careers. And we've now like horseshoe theory around to like

1333
01:20:39,045 --> 01:20:42,725
Speaker 5:  copyright law is good actually, which is crazy. So the argument that you

1334
01:20:42,725 --> 01:20:46,565
Speaker 5:  would make from the other side, and it's a good argument, is that

1335
01:20:46,565 --> 01:20:50,555
Speaker 5:  human beings do not need copyright law to be creative. This, this legal

1336
01:20:50,555 --> 01:20:54,275
Speaker 5:  system that we've invented is there to create scarcity and economic

1337
01:20:54,285 --> 01:20:57,195
Speaker 5:  value for creativity, which certainly incentivizes some people to create.

1338
01:20:57,895 --> 01:21:01,555
Speaker 5:  But if actually, if you look around, our instinct is literal

1339
01:21:01,605 --> 01:21:05,435
Speaker 5:  human beings. Every day is to create, we will do it for free. People

1340
01:21:05,435 --> 01:21:08,395
Speaker 5:  wake up every morning and they have access to creative tools on their computers

1341
01:21:08,615 --> 01:21:12,435
Speaker 5:  and they cannot help themselves. Like it is just the, our

1342
01:21:12,435 --> 01:21:16,235
Speaker 5:  most essential human nature is to make stuff. And that's in real

1343
01:21:16,235 --> 01:21:20,155
Speaker 5:  tension here, right? Like it, it's hard to square the

1344
01:21:20,155 --> 01:21:23,715
Speaker 5:  courts saying we have to preserve the essential nature of copyright law

1345
01:21:23,935 --> 01:21:27,755
Speaker 5:  so that there will be creativity. And then literally those judges go home

1346
01:21:27,755 --> 01:21:30,875
Speaker 5:  and they open the Instagram app on their phone and it's just like the most

1347
01:21:30,875 --> 01:21:33,235
Speaker 5:  free creativity that has ever existed in the history of humanity.

1348
01:21:33,395 --> 01:21:37,075
Speaker 6:  I mean, somebody at Andreesen Horowitz is gonna play that clip that you just

1349
01:21:37,075 --> 01:21:41,035
Speaker 6:  said for everyone who works there. That's that's the crash. That's the, I

1350
01:21:41,035 --> 01:21:43,635
Speaker 6:  mean that is the argument that these companies make in favor of all of this

1351
01:21:43,655 --> 01:21:46,515
Speaker 6:  is that no, we're not diminishing anything because everybody would do it

1352
01:21:46,515 --> 01:21:46,795
Speaker 6:  for free.

1353
01:21:46,965 --> 01:21:49,275
Speaker 5:  Right? And then they, they make that argument from their yachts.

1354
01:21:49,345 --> 01:21:52,635
Speaker 11:  I'll link the counter argument to this, which is that while

1355
01:21:53,175 --> 01:21:56,915
Speaker 11:  people will create a lot of the purpose of creativity is to

1356
01:21:56,915 --> 01:22:00,715
Speaker 11:  share that thing with other people, other people. And if AI

1357
01:22:00,715 --> 01:22:04,355
Speaker 11:  creates a situation in which it is virtually impossible to distribute

1358
01:22:04,505 --> 01:22:08,435
Speaker 11:  your work or to get anyone to see your human made work, I think that you

1359
01:22:08,455 --> 01:22:12,275
Speaker 11:  can say that is really meaningfully chilling creativity. The problem

1360
01:22:12,275 --> 01:22:15,075
Speaker 11:  is that I don't think copyright laws that stands can solve this. There's

1361
01:22:15,075 --> 01:22:18,915
Speaker 11:  this really bizarre tension in the Meta opinion where,

1362
01:22:19,025 --> 01:22:22,755
Speaker 11:  okay, if you take its premises true that this will completely destroy the

1363
01:22:22,755 --> 01:22:26,715
Speaker 11:  market for books, then the only thing copyright law can do is

1364
01:22:26,735 --> 01:22:30,675
Speaker 11:  say, okay, well some people have to get a few dollars a

1365
01:22:30,675 --> 01:22:34,315
Speaker 11:  month as residuals for this to create this entire

1366
01:22:34,445 --> 01:22:38,435
Speaker 11:  creativity killing database. Really the only way

1367
01:22:38,435 --> 01:22:42,075
Speaker 11:  that you can take this argument as true is to assume that also

1368
01:22:42,075 --> 01:22:44,595
Speaker 11:  copyright law is going to be powerless to change it.

1369
01:22:45,055 --> 01:22:48,985
Speaker 5:  It might be, I mean we, oh, one outcome of all these

1370
01:22:48,985 --> 01:22:52,825
Speaker 5:  cases is they hit the Supreme Court and the Supreme Court says fine training

1371
01:22:52,825 --> 01:22:56,225
Speaker 5:  is fair use. Maybe that's an outcome. I think it is much more likely

1372
01:22:56,655 --> 01:23:00,225
Speaker 5:  that as these cases carene towards that risk.

1373
01:23:01,025 --> 01:23:02,225
Speaker 5:  'cause no one knows how that will go.

1374
01:23:04,185 --> 01:23:08,085
Speaker 5:  That actually the incredibly well moneyed interest here

1375
01:23:08,355 --> 01:23:12,345
Speaker 5:  will go get a law, right? That, that Disney and OpenAI and Google,

1376
01:23:12,565 --> 01:23:16,265
Speaker 5:  all of these companies have to deal with each other. They will go that

1377
01:23:16,265 --> 01:23:19,985
Speaker 5:  Nashville is a powerful economic force in copyright law.

1378
01:23:20,195 --> 01:23:22,825
Speaker 11:  Where does that leave the next generation of artists though? Because the

1379
01:23:22,825 --> 01:23:26,585
Speaker 11:  way that I see that gaming out is yeah, all the record labels that currently

1380
01:23:26,585 --> 01:23:30,225
Speaker 11:  have artists sell a bunch of data to companies and then

1381
01:23:30,625 --> 01:23:34,465
Speaker 11:  companies create this database and they train on it and they train

1382
01:23:34,705 --> 01:23:38,425
Speaker 11:  a bunch of, I don't know, music apps and apps for writing romance

1383
01:23:38,565 --> 01:23:42,545
Speaker 11:  novels and all of that. And then they create training

1384
01:23:42,575 --> 01:23:46,385
Speaker 11:  data based on that, that then they sort of cycle into their system over

1385
01:23:46,385 --> 01:23:49,825
Speaker 11:  and over, which is what they're trying to do right now. And then at that

1386
01:23:49,825 --> 01:23:53,705
Speaker 11:  point, where does the next generation get paid? I I don't know that

1387
01:23:53,705 --> 01:23:57,625
Speaker 11:  there's actually a way that happens, especially if there is this situation

1388
01:23:57,625 --> 01:24:00,705
Speaker 11:  where say AI generated content floods everything.

1389
01:24:01,015 --> 01:24:04,705
Speaker 5:  Yeah, I don't know. I that I, I think that will be the heart of the

1390
01:24:04,705 --> 01:24:07,145
Speaker 5:  negotiation fundamentally. Especially because

1391
01:24:08,335 --> 01:24:11,955
Speaker 5:  private equity is in the mix. Like the creative

1392
01:24:12,275 --> 01:24:15,875
Speaker 5:  industry is not clean, it's not pure. It's a lot of essentially banks now.

1393
01:24:16,115 --> 01:24:19,715
Speaker 5:  Yeah. And so, like, you know, private equity companies have bought Bruce

1394
01:24:19,715 --> 01:24:23,675
Speaker 5:  Springsteen's entire catalog for $500 million. They're not

1395
01:24:23,675 --> 01:24:26,035
Speaker 5:  gonna take a one time training payment for that. They bought that because

1396
01:24:26,035 --> 01:24:29,955
Speaker 5:  they think they can exploit that music for years to come in various ways.

1397
01:24:30,435 --> 01:24:33,155
Speaker 11:  I, I don't know, I just think that's not actually satisfying because that

1398
01:24:33,155 --> 01:24:36,035
Speaker 11:  means Bruce Springsteen has a sp a steady string of income, but that there

1399
01:24:36,035 --> 01:24:37,595
Speaker 11:  is no market for the next person.

1400
01:24:37,975 --> 01:24:41,595
Speaker 5:  No, Bruce Springsteen got his one-time payment. I mean this is why it's particularly

1401
01:24:41,595 --> 01:24:44,595
Speaker 5:  gross. Bruce Stein has a half billion. Yeah, okay. Yes, fair. It's the private

1402
01:24:44,595 --> 01:24:48,035
Speaker 5:  equity company that made the investment is gonna wanna return over time

1403
01:24:48,915 --> 01:24:52,615
Speaker 5:  and that return over time is the thing that ideally

1404
01:24:52,745 --> 01:24:56,735
Speaker 5:  makes any new work also valuable. And that's

1405
01:24:56,745 --> 01:25:00,215
Speaker 5:  tough. I don't know how any of that plays out, especially because I don't

1406
01:25:00,215 --> 01:25:02,455
Speaker 5:  think this gets litigated. I don't think it's solved the level of Supreme

1407
01:25:02,455 --> 01:25:06,375
Speaker 5:  Court. I think Congress has to write a law in that you can already

1408
01:25:06,375 --> 01:25:10,015
Speaker 5:  see how messy that will be. That and, and the battle lines of that law is

1409
01:25:10,015 --> 01:25:13,775
Speaker 5:  these court cases wind ever forward are literally

1410
01:25:14,555 --> 01:25:18,055
Speaker 5:  the huge entertainment companies that basically own

1411
01:25:18,565 --> 01:25:21,535
Speaker 5:  this kind of lobbying. This is what they do this and they, they're very,

1412
01:25:21,535 --> 01:25:25,375
Speaker 5:  very good at it. Going up against Disney to walk back. Copyright law is

1413
01:25:25,375 --> 01:25:29,265
Speaker 5:  not historically a successful effort going up against Nashville in the

1414
01:25:29,265 --> 01:25:32,815
Speaker 5:  music industry to walk back copyright law, not historically a successful

1415
01:25:32,875 --> 01:25:36,855
Speaker 5:  effort. And in fact some of the only AI related laws we

1416
01:25:36,855 --> 01:25:40,815
Speaker 5:  have are like the Elvis Act in Tennessee that prevents people from

1417
01:25:40,875 --> 01:25:42,655
Speaker 5:  making ai Elvis impersonators.

1418
01:25:43,045 --> 01:25:46,855
Speaker 6:  Well and things like the Disney one are, are seem more sort of viscerally

1419
01:25:47,095 --> 01:25:49,655
Speaker 6:  straightforward in that you can just go to Midjourney and be like, show me

1420
01:25:49,655 --> 01:25:53,415
Speaker 6:  Simpsons. And it'll just do the Simpsons for you in a way that I think

1421
01:25:54,075 --> 01:25:57,895
Speaker 6:  things like these cases are, are a little, just feel more abstract

1422
01:25:57,895 --> 01:26:01,575
Speaker 6:  because it's like something goes in black box, something else

1423
01:26:01,625 --> 01:26:05,605
Speaker 6:  comes out in that one, it's just something goes in and then that same, something

1424
01:26:05,605 --> 01:26:08,005
Speaker 6:  comes out for free and you can do whatever you want with it. And I feel like

1425
01:26:08,005 --> 01:26:11,845
Speaker 6:  that we're just gonna look at that differently, whether we should

1426
01:26:11,845 --> 01:26:13,125
Speaker 6:  or not. It feels different.

1427
01:26:13,265 --> 01:26:14,805
Speaker 5:  Oh, the pictures always win. Yeah.

1428
01:26:14,835 --> 01:26:18,565
Speaker 11:  Legally different media are also different. Like you can quote

1429
01:26:18,845 --> 01:26:22,525
Speaker 11:  a bunch of a book, but if you want to sample three seconds of a song,

1430
01:26:22,905 --> 01:26:24,805
Speaker 11:  you're on much shakier territory. So

1431
01:26:25,085 --> 01:26:28,285
Speaker 5:  I would actually chalk that up to the fact that the record labels in Hollywood

1432
01:26:28,285 --> 01:26:32,245
Speaker 5:  are better at this than the book publishing industry fair. The the news media

1433
01:26:32,345 --> 01:26:35,685
Speaker 5:  is not great at this historically. And, and those industries are really,

1434
01:26:35,685 --> 01:26:39,645
Speaker 5:  really good at it. And part of it is to David's point, you have

1435
01:26:39,805 --> 01:26:43,605
Speaker 5:  a more visceral emotional reaction to that picture is the same and that song

1436
01:26:43,605 --> 01:26:47,045
Speaker 5:  is the same. Yeah, they are good at fighting these fights. So my

1437
01:26:47,365 --> 01:26:51,205
Speaker 5:  suspicion here, Addie, it might not be satisfying, is

1438
01:26:51,205 --> 01:26:54,925
Speaker 5:  that you get to some solution where there's still value for the individual

1439
01:26:55,235 --> 01:26:59,045
Speaker 5:  song. There's still value for the individual video. It might be

1440
01:26:59,075 --> 01:27:03,045
Speaker 5:  less than before and there might be more supply which will crash that value

1441
01:27:03,045 --> 01:27:06,005
Speaker 5:  because there's only so many people and only so many events to consume songs

1442
01:27:06,005 --> 01:27:09,945
Speaker 5:  and videos in the day. But I don't think you get to a place where this set

1443
01:27:09,945 --> 01:27:12,865
Speaker 5:  of artists gets paid once and no future artists get paid because that will

1444
01:27:12,865 --> 01:27:16,265
Speaker 5:  kill these businesses. And they're not existentially stupid. They can be

1445
01:27:16,265 --> 01:27:18,425
Speaker 5:  stupid. They're not existentially stupid.

1446
01:27:18,965 --> 01:27:22,505
Speaker 11:  My really dark take is that the reason Disney is suing Midjourney is because

1447
01:27:22,505 --> 01:27:25,825
Speaker 11:  Disney is about to use all of its IP to generate infinite Star Wars forever.

1448
01:27:26,645 --> 01:27:29,785
Speaker 11:  So, I don't know, I think we're just shifting the people who are going to

1449
01:27:29,785 --> 01:27:31,505
Speaker 11:  be making these things and flooding the market

1450
01:27:31,805 --> 01:27:35,745
Speaker 5:  Disney learns wrong lesson from and or is a real headline. You can,

1451
01:27:36,655 --> 01:27:38,185
Speaker 5:  it's, it's right there for you. We

1452
01:29:13,805 --> 01:29:17,715
Speaker 5:  We're back the lightning round, which David says has to be two

1453
01:29:17,715 --> 01:29:21,275
Speaker 5:  minutes long, two minute lightning round 'cause we just spent an hour crashing

1454
01:29:21,275 --> 01:29:24,355
Speaker 5:  out about copyright law. So let's talk about Brendan Carr for 45 minutes.

1455
01:29:24,825 --> 01:29:27,835
Speaker 5:  America's favorite podcast for the podcast by the way, lightning round Unsponsored.

1456
01:29:29,245 --> 01:29:32,345
Speaker 5:  If you, if we're in a Tesla and it crashes, we'll point out that it crashes.

1457
01:29:32,605 --> 01:29:36,585
Speaker 5:  I'm just saying you can't buy us. I've noticed a lot of people have started

1458
01:29:36,945 --> 01:29:40,185
Speaker 5:  posting on, on, on various social platforms at me that you can't tell us

1459
01:29:40,185 --> 01:29:43,625
Speaker 5:  what to do. That's all I've ever wanted from the lightning round is Unsponsored

1460
01:29:43,625 --> 01:29:44,065
Speaker 5:  for Flavor,

1461
01:29:45,585 --> 01:29:48,205
Speaker 5:  is people reminding everyone that you can't actually tell us what to do when

1462
01:29:48,205 --> 01:29:50,485
Speaker 5:  you can't buy us. I think when you die, I am gonna write, you can't tell

1463
01:29:50,485 --> 01:29:53,405
Speaker 5:  me what to do on your tombstone and it's gonna feel, it's gonna feel right.

1464
01:29:53,665 --> 01:29:55,885
Speaker 5:  It is the animating thesis of The Verge talking

1465
01:29:57,755 --> 01:30:01,205
Speaker 5:  that and figuring out what's going on with the Frame tv. Okay,

1466
01:30:01,395 --> 01:30:04,725
Speaker 5:  Brandon Carr is a dummy America's favorite podcast. In the podcast. It's

1467
01:30:04,725 --> 01:30:06,125
Speaker 5:  time. This, oh this is a short one

1468
01:30:07,765 --> 01:30:10,815
Speaker 5:  'cause he did something dumb, but it's like predictably dumb.

1469
01:30:11,765 --> 01:30:15,015
Speaker 5:  Like he, he did the dumb thing that Republican commissioners always do at

1470
01:30:15,015 --> 01:30:18,935
Speaker 5:  the FCC. So a couple of things happened. One, everyone

1471
01:30:18,935 --> 01:30:22,815
Speaker 5:  left the FCC presumably. 'cause Brendan is so unpleasant

1472
01:30:22,835 --> 01:30:25,895
Speaker 5:  to work for. Does Anna Gomez still work there? I haven't checked on this

1473
01:30:25,895 --> 01:30:28,775
Speaker 5:  in a minute. She still works there. She is not yet fired by Donald Trump.

1474
01:30:28,925 --> 01:30:32,375
Speaker 5:  Okay. The only Democratic commissioner on the FCC. And she literally will

1475
01:30:32,375 --> 01:30:35,975
Speaker 5:  set tell you she does not know why she, she has not been fired. So, and

1476
01:30:36,055 --> 01:30:39,175
Speaker 5:  she's on tour saying Brandon is a bad guy who's violating the First Amendment

1477
01:30:39,845 --> 01:30:42,895
Speaker 5:  wild stuff over there. So there was just the two of them, which is not a

1478
01:30:42,895 --> 01:30:44,375
Speaker 5:  quorum for the FCC to do its business.

1479
01:30:46,195 --> 01:30:50,135
Speaker 5:  Nathan Simington, who left the FCC, wanted this guy Gavin

1480
01:30:50,195 --> 01:30:53,495
Speaker 5:  Wax to be appointed. He didn't get it. This woman named Olivia Trusty

1481
01:30:54,035 --> 01:30:57,695
Speaker 5:  is the new Republican commissioner of the SEC. She was confirmed by the Senate.

1482
01:30:57,985 --> 01:31:01,895
Speaker 5:  She's a long history with everyone. There were statements about her, like

1483
01:31:02,475 --> 01:31:05,655
Speaker 5:  we would confirm her in a normal time because that's, she's a Republican

1484
01:31:05,655 --> 01:31:08,775
Speaker 5:  commissioner. Republican presidents get Republican commissioners. But Brendan

1485
01:31:08,795 --> 01:31:11,495
Speaker 5:  is such a weird threat to free speech that we're opposing her nomination.

1486
01:31:12,035 --> 01:31:16,005
Speaker 5:  That's, that's where Brendan has taken this organization. This, this

1487
01:31:16,005 --> 01:31:19,645
Speaker 5:  woman who would've sailed through a normal times was opposed by various

1488
01:31:19,885 --> 01:31:22,885
Speaker 5:  Democrats and various free speech troops. And literally they're saying she's

1489
01:31:22,885 --> 01:31:26,445
Speaker 5:  fine, but Brendan sucks. So we're, we can't let Brendan do anything. You

1490
01:31:26,445 --> 01:31:29,405
Speaker 5:  can't, you can't let him have another vote, right? She got through party

1491
01:31:29,425 --> 01:31:31,885
Speaker 5:  on vote. So it goes immediately,

1492
01:31:32,835 --> 01:31:36,725
Speaker 5:  immediately Brendan moves to open comment on letting companies

1493
01:31:36,865 --> 01:31:40,525
Speaker 5:  own more broadcast stations. So we have this concept in the United States

1494
01:31:40,525 --> 01:31:44,285
Speaker 5:  called media Ownership rules that say big companies can't own

1495
01:31:44,305 --> 01:31:48,135
Speaker 5:  all the TV stations for a variety of reasons. First,

1496
01:31:48,555 --> 01:31:52,255
Speaker 5:  you just want your local media to have a relationship to you,

1497
01:31:53,095 --> 01:31:56,925
Speaker 5:  right? That the TV stations and newspapers and radio stations that

1498
01:31:56,925 --> 01:32:00,735
Speaker 5:  serve your area should have some skin in the game where you live. Right?

1499
01:32:00,735 --> 01:32:04,015
Speaker 5:  They should be responsive to their local communities. And this is like deep

1500
01:32:04,035 --> 01:32:07,695
Speaker 5:  in the law and the policy where you, you want the local media to be local.

1501
01:32:08,275 --> 01:32:11,455
Speaker 5:  You don't want national media to ignore your concerns and, and do all this

1502
01:32:11,455 --> 01:32:15,335
Speaker 5:  other stuff. This is waxed and waned over the years. But you

1503
01:32:15,335 --> 01:32:19,055
Speaker 5:  can kind of feel it, right? If you go on social media, they, they're not

1504
01:32:19,055 --> 01:32:22,015
Speaker 5:  local media ownership rules. They're not rules that your algorithm necess

1505
01:32:22,015 --> 01:32:24,615
Speaker 5:  necessarily what's happening in your community. Everything is just national

1506
01:32:24,615 --> 01:32:28,415
Speaker 5:  culture war all the time. 'cause that's what plays in social media. Weird.

1507
01:32:28,445 --> 01:32:31,575
Speaker 5:  It's a weird outcome. So we have these rules eight from ages ago that say,

1508
01:32:31,915 --> 01:32:34,615
Speaker 5:  you know, big companies can't just buy a TV stations and national program

1509
01:32:34,715 --> 01:32:37,935
Speaker 5:  and they actually locally owned. And then we have this other rule that says

1510
01:32:38,435 --> 01:32:42,015
Speaker 5:  you can't, there's a percentage, right? The same company can't own all the

1511
01:32:42,015 --> 01:32:45,535
Speaker 5:  stations in your town. The same company can't own all the stations and all

1512
01:32:45,535 --> 01:32:48,575
Speaker 5:  the radio stations, all these favored. So you have all these media ownership

1513
01:32:48,575 --> 01:32:48,775
Speaker 5:  rules.

1514
01:32:50,615 --> 01:32:54,535
Speaker 5:  Brendan, Brendan thinks they're dumb. He thinks we should open it up

1515
01:32:54,535 --> 01:32:56,455
Speaker 5:  and let everybody on everything. And his argument is

1516
01:32:58,555 --> 01:33:01,965
Speaker 5:  it's not smart that these companies have to compete with the internet.

1517
01:33:02,655 --> 01:33:05,155
Speaker 5:  So we should let them own everything so they can get big enough to compete

1518
01:33:05,155 --> 01:33:07,795
Speaker 5:  with Google and Facebook. And the problem there, of course, is when you open

1519
01:33:07,795 --> 01:33:10,435
Speaker 5:  the social media apps on your phone, you are not subjected to local news.

1520
01:33:10,435 --> 01:33:13,595
Speaker 5:  You're subject to the national culture war, which is the exact reason the

1521
01:33:13,595 --> 01:33:14,195
Speaker 5:  policy exists.

1522
01:33:16,285 --> 01:33:20,005
Speaker 5:  Brendan, everybody, he's just like, just walked into

1523
01:33:20,505 --> 01:33:24,315
Speaker 5:  the literal opposite of the reason for the rule we should make. We should

1524
01:33:24,315 --> 01:33:27,395
Speaker 5:  make everything national culture war so that local news can compete with

1525
01:33:27,395 --> 01:33:29,515
Speaker 5:  Google is not the right answer at all.

1526
01:33:30,185 --> 01:33:34,115
Speaker 6:  Make local news, do national culture wars is what we're after.

1527
01:33:34,135 --> 01:33:34,355
Speaker 6:  Now

1528
01:33:34,455 --> 01:33:38,155
Speaker 5:  That's, sorry, boy Brendan. So that's Brendan,

1529
01:33:38,185 --> 01:33:41,435
Speaker 5:  he's a dummy as always. Brendan, if you can defend any of this stuff, you're

1530
01:33:41,435 --> 01:33:44,315
Speaker 5:  welcome on the show. I'd love to have you. It's been a minute since I've

1531
01:33:44,315 --> 01:33:47,955
Speaker 5:  issued this threatening invitation to you And I amm wishing it again. You're

1532
01:33:47,955 --> 01:33:50,475
Speaker 5:  welcome. On Decoder on The Vergecast. We've only got weeks to go.

1533
01:33:51,915 --> 01:33:53,985
Speaker 5:  David, I believe this is your last show before baby.

1534
01:33:54,335 --> 01:33:58,225
Speaker 6:  This is my last Friday show. I I will be on Tuesday's

1535
01:33:58,225 --> 01:34:01,745
Speaker 6:  show, which due to the magic of technology, we have already

1536
01:34:02,065 --> 01:34:02,225
Speaker 6:  recorded

1537
01:34:02,655 --> 01:34:03,825
Speaker 5:  This cake has already baked.

1538
01:34:04,055 --> 01:34:07,905
Speaker 6:  Exactly. But yeah, this is, this is the, the the last one

1539
01:34:07,905 --> 01:34:09,905
Speaker 6:  I'll be on for a couple of months.

1540
01:34:10,295 --> 01:34:13,985
Speaker 5:  Yeah. And then I, I I'm weeks away here as well, So I miss Brendan and children.

1541
01:34:13,985 --> 01:34:16,785
Speaker 5:  Our children are coming. And Brendan, we will let our children interview

1542
01:34:16,785 --> 01:34:19,585
Speaker 5:  you. I think they can take you, buddy.

1543
01:34:19,685 --> 01:34:20,505
Speaker 6:  That's a good idea.

1544
01:34:21,225 --> 01:34:24,625
Speaker 5:  I I mean Max can take me, so yeah, I think she can,

1545
01:34:25,085 --> 01:34:27,665
Speaker 5:  she can definitely take Brendan. But you're, you're, you're, you're invited

1546
01:34:27,665 --> 01:34:30,425
Speaker 5:  anytime you want. You got a couple weeks to go. I'll come back special just

1547
01:34:30,425 --> 01:34:32,345
Speaker 5:  for you, Brendan. I do have, by the way,

1548
01:34:33,175 --> 01:34:35,905
Speaker 6:  It's gonna be so great when Jake interviews Brendan while we're out.

1549
01:34:37,345 --> 01:34:40,185
Speaker 5:  I think Jake can pick Brendan too. I'm not worried about it in the slightest.

1550
01:34:40,455 --> 01:34:44,025
Speaker 5:  Just a little bonus round. Not the FCC, the Federal Trade Commission did

1551
01:34:44,025 --> 01:34:47,235
Speaker 5:  some dumb shit this week. They are a allowing

1552
01:34:47,745 --> 01:34:51,555
Speaker 5:  Omnicom, which is a big ad agency to buy another big ad agency, but

1553
01:34:51,555 --> 01:34:54,995
Speaker 5:  they're issuing a consent decree that says they're not allowed to,

1554
01:34:55,855 --> 01:34:58,845
Speaker 5:  to like, keep ads off of x basically.

1555
01:35:00,455 --> 01:35:03,995
Speaker 5:  So they, they're literally stepping into the free market and saying you have

1556
01:35:03,995 --> 01:35:07,195
Speaker 5:  to spend money on platforms where you disagree with the political views and

1557
01:35:07,195 --> 01:35:11,165
Speaker 5:  then we'll let your merger go. Which, what kind of free market

1558
01:35:11,305 --> 01:35:14,365
Speaker 5:  is that? Where the government is like, we will let you merge,

1559
01:35:15,375 --> 01:35:18,395
Speaker 5:  but you are not allowed to choose how you spend your money.

1560
01:35:18,895 --> 01:35:22,635
Speaker 6:  You are required to advertise on objectionable content is such a funny

1561
01:35:22,635 --> 01:35:24,155
Speaker 6:  thing for the government to require.

1562
01:35:24,465 --> 01:35:27,635
Speaker 5:  Yeah. The advertisers by the way, the clients are allowed to still make their

1563
01:35:27,795 --> 01:35:31,075
Speaker 5:  decisions. But the ad agencies, which fundamentally what the ad agencies

1564
01:35:31,075 --> 01:35:34,195
Speaker 5:  do is help people spend their ad dollars. They're not allowed to make those

1565
01:35:34,755 --> 01:35:38,515
Speaker 5:  decisions perfect. I can't imagine how that will ever

1566
01:35:38,735 --> 01:35:42,355
Speaker 5:  get, how will anyone get around this, this incredible rule. So

1567
01:35:42,355 --> 01:35:45,555
Speaker 6:  They're like, here's, here's the people, here's the Nazis.

1568
01:35:45,895 --> 01:35:48,755
Speaker 5:  And then just another, another one just to throw it out there.

1569
01:35:50,835 --> 01:35:54,315
Speaker 5:  Paramount is being sued by Trump

1570
01:35:54,975 --> 01:35:58,275
Speaker 5:  for the CBS interview. You, you know about this where he says the Kamala

1571
01:35:58,275 --> 01:36:02,075
Speaker 5:  Harris interview was deceptively edited, blah blah, there's some mediator

1572
01:36:02,075 --> 01:36:05,635
Speaker 5:  that's proposed some settlement, $20 billion and it's held up because

1573
01:36:05,705 --> 01:36:09,695
Speaker 5:  Paramount executives and their lawyers cannot figure out how to

1574
01:36:09,995 --> 01:36:13,295
Speaker 5:  pay this money without being on the hook for Bribery charges

1575
01:36:13,885 --> 01:36:16,055
Speaker 5:  because it's just Bribery.

1576
01:36:17,645 --> 01:36:20,055
Speaker 5:  This is a real problem. If you don't believe me,

1577
01:36:21,695 --> 01:36:25,675
Speaker 5:  you can read the notoriously leftist Wall

1578
01:36:25,675 --> 01:36:29,435
Speaker 5:  Street Journal editorial board saying, don't do this

1579
01:36:29,435 --> 01:36:32,435
Speaker 5:  Paramount, just fight and win this lawsuit. Yeah.

1580
01:36:33,175 --> 01:36:33,465
Speaker 7:  When

1581
01:36:33,485 --> 01:36:37,225
Speaker 5:  Rupert Murdoch is like, don't be an idiot, like go win your first amendment

1582
01:36:37,465 --> 01:36:40,755
Speaker 5:  argument and he agrees with you. You, I'm just letting you know. All right.

1583
01:36:40,755 --> 01:36:44,075
Speaker 5:  That's politics for today. Don't do Bribery. Brendan's an idiot.

1584
01:36:44,895 --> 01:36:47,595
Speaker 5:  Got it all on my system. David, do you have a ballot cleanser?

1585
01:36:47,795 --> 01:36:48,315
Speaker 7:  Jake? Let's

1586
01:36:48,315 --> 01:36:50,835
Speaker 6:  Do, let's do one each before we get outta here. Let's, let's, what

1587
01:36:50,835 --> 01:36:51,075
Speaker 7:  Do you have?

1588
01:36:53,675 --> 01:36:57,515
Speaker 7:  I am super excited about this one. Have you guys ever

1589
01:36:57,515 --> 01:37:00,275
Speaker 7:  looked at your TV and thought not enough pixels here.

1590
01:37:02,035 --> 01:37:04,555
Speaker 6:  I can tell you for knee, like the answer is constantly, every day

1591
01:37:04,675 --> 01:37:04,795
Speaker 5:  The

1592
01:37:04,795 --> 01:37:08,445
Speaker 7:  Answer. The answer is always, every single time you think this isn't enough,

1593
01:37:08,995 --> 01:37:12,805
Speaker 7:  guys, we're going a 16 k, it's happening. The, The

1594
01:37:12,995 --> 01:37:16,885
Speaker 7:  HDMI 2.2 spec is out. Now there are

1595
01:37:16,885 --> 01:37:19,605
Speaker 7:  some drawbacks, And I know this is gonna be a hard one for the gamers out

1596
01:37:19,605 --> 01:37:23,365
Speaker 7:  there. It's only 60 hertz. So I, I know

1597
01:37:23,365 --> 01:37:24,805
Speaker 7:  nobody's gonna want this just yet, right?

1598
01:37:24,865 --> 01:37:27,005
Speaker 6:  How is this gonna look on Bravia core? That's all I wanna know

1599
01:37:27,375 --> 01:37:31,205
Speaker 7:  Until we can get 16 1 20. You

1600
01:37:31,285 --> 01:37:34,925
Speaker 7:  know, it's not for me, but the HDMI forum is on top of it.

1601
01:37:35,155 --> 01:37:39,045
Speaker 7:  They're weirdly ahead of the curve here. I do not know what the current demand

1602
01:37:39,105 --> 01:37:42,805
Speaker 7:  is for support for 16 K 60 Hertz video.

1603
01:37:43,675 --> 01:37:47,015
Speaker 7:  I'm not sure how much of that exists. I don't know where I would get it.

1604
01:37:48,105 --> 01:37:50,645
Speaker 7:  The YouTubers aren't yelling at us to filming it yet.

1605
01:37:53,725 --> 01:37:57,205
Speaker 5:  I love by the way, that they've only revved it to

1606
01:37:57,205 --> 01:38:00,865
Speaker 5:  2.2. Right? Because we're at The HDMI

1607
01:38:00,865 --> 01:38:04,145
Speaker 5:  2.1 right now. I I, right? Yes.

1608
01:38:04,735 --> 01:38:07,585
Speaker 5:  16 1 20 is only worth 0.1.

1609
01:38:09,295 --> 01:38:12,505
Speaker 5:  They've only revved it to HDMI 2.2 to get to

1610
01:38:12,505 --> 01:38:16,305
Speaker 5:  16 1 20 16 K one 20. Look, what is it gonna take you guys?

1611
01:38:19,255 --> 01:38:22,185
Speaker 6:  30 2K 360,

1612
01:38:23,085 --> 01:38:23,905
Speaker 6:  The HDMI three.

1613
01:38:24,055 --> 01:38:27,145
Speaker 5:  This is, this is the goal. We gotta go to the, The HDMI forum headquarters

1614
01:38:27,205 --> 01:38:30,385
Speaker 5:  and just yell numbers at them until they concede that it's time to rev the

1615
01:38:30,385 --> 01:38:30,785
Speaker 5:  main version.

1616
01:38:34,095 --> 01:38:37,735
Speaker 6:  I love this. This is, I'm, I'm all in. I'm really excited to buy a bunch

1617
01:38:37,735 --> 01:38:40,975
Speaker 6:  of expensive HDMI cables and connect them to my $300

1618
01:38:41,265 --> 01:38:43,335
Speaker 6:  42 inch TCL Roku dv.

1619
01:38:43,605 --> 01:38:46,935
Speaker 7:  This is what is most important. As soon as these cables are, are available,

1620
01:38:47,175 --> 01:38:51,095
Speaker 7:  I don't care if you're still running 10 80 p any HDMI cable you buy from

1621
01:38:51,095 --> 01:38:54,375
Speaker 7:  here on out has better be a 16 K cable. 'cause one day

1622
01:38:54,835 --> 01:38:58,375
Speaker 7:  you're gonna wanna change out your Xbox and it's gonna be

1623
01:38:58,635 --> 01:39:01,695
Speaker 7:  too hard to get to. You're gonna need to make sure you have that 16 K support

1624
01:39:01,725 --> 01:39:02,015
Speaker 7:  just

1625
01:39:02,015 --> 01:39:05,295
Speaker 6:  In case my $19 Roku Express is gonna look sick.

1626
01:39:05,355 --> 01:39:06,335
Speaker 7:  Now it's gonna rule.

1627
01:39:06,335 --> 01:39:09,655
Speaker 5:  Yeah. Okay. So Andrew Osky wrote this story for us. Every paragraph for this

1628
01:39:09,655 --> 01:39:13,455
Speaker 5:  story is great because Andrew's a great gadget writer and he wrote it very

1629
01:39:13,465 --> 01:39:17,415
Speaker 5:  dryly. And if you just read it on its face, this is the work of

1630
01:39:17,415 --> 01:39:21,215
Speaker 5:  people who, who have no connection to reality. So those

1631
01:39:21,215 --> 01:39:24,855
Speaker 5:  cables that you're talking about are now officially called HDMI

1632
01:39:24,855 --> 01:39:26,695
Speaker 5:  2.2 Ultra 96 cables

1633
01:39:27,245 --> 01:39:27,535
Speaker 6:  Sick.

1634
01:39:27,805 --> 01:39:31,735
Speaker 5:  Sure. There's a new standard they support called the Latency Indication

1635
01:39:32,095 --> 01:39:32,935
Speaker 5:  Protocol or lip.

1636
01:39:34,085 --> 01:39:37,785
Speaker 6:  That's a sick band name by the way. The latency indication protocol is like

1637
01:39:37,785 --> 01:39:39,265
Speaker 6:  a swing band that needs to exist.

1638
01:39:39,805 --> 01:39:43,705
Speaker 5:  Why is it called lip? Because it matches up the audio in the video, so

1639
01:39:43,705 --> 01:39:45,545
Speaker 5:  the audio matches the lips. Oh, that's

1640
01:39:45,545 --> 01:39:45,785
Speaker 6:  Good.

1641
01:39:46,375 --> 01:39:47,785
Speaker 5:  It's good stuff. That's good stuff.

1642
01:39:48,505 --> 01:39:51,925
Speaker 7:  And some might even say somewhat more important than 16 K

1643
01:39:52,705 --> 01:39:54,685
Speaker 7:  and having your audio synced to your video

1644
01:39:56,285 --> 01:39:57,545
Speaker 7:  htm I 2.2 mostly

1645
01:39:57,545 --> 01:40:01,525
Speaker 6:  Here on The Vergecast. We love a good acronym, So I I will take it. I'm all

1646
01:40:01,525 --> 01:40:02,005
Speaker 6:  in. I'm

1647
01:40:02,005 --> 01:40:05,965
Speaker 5:  Just saying the, it's a new top end. Ultra 96

1648
01:40:05,965 --> 01:40:09,165
Speaker 5:  is the new top end. You get high speed, premium, high speed, and ultra high

1649
01:40:09,165 --> 01:40:10,245
Speaker 5:  speed. And now Ultra 96,

1650
01:40:12,455 --> 01:40:14,275
Speaker 5:  I'm, I'm happy. All right. What do you got, David?

1651
01:40:16,195 --> 01:40:19,875
Speaker 6:  Microsoft is killing the blue screen of death. Finally,

1652
01:40:20,525 --> 01:40:24,475
Speaker 6:  after 40 years when your computer explodes, it will no longer show the blue

1653
01:40:24,475 --> 01:40:26,875
Speaker 6:  screen of death. It will show the black screen of death.

1654
01:40:28,795 --> 01:40:32,355
Speaker 6:  Microsoft is, is redesigning it. A thing that I learned this week, thanks

1655
01:40:32,355 --> 01:40:35,795
Speaker 6:  to a story Tom Warren wrote for The Verge in 2014,

1656
01:40:36,615 --> 01:40:40,115
Speaker 6:  was that Steve Balmer wrote the text of the blue screen of death message.

1657
01:40:40,255 --> 01:40:40,675
Speaker 5:  No way.

1658
01:40:40,675 --> 01:40:44,635
Speaker 6:  Because he didn't like, he, he got the blue screen of death and he thought

1659
01:40:44,655 --> 01:40:47,995
Speaker 6:  the, the wording didn't seem right for when it was going wrong. So he, they,

1660
01:40:48,015 --> 01:40:51,755
Speaker 6:  the Windows team challenged him to go do a better job. And he did. He wrote

1661
01:40:51,755 --> 01:40:55,525
Speaker 6:  the blue screen of death meth message, which is wonderful, but

1662
01:40:55,665 --> 01:40:59,005
Speaker 6:  now they're changing it to something much less

1663
01:40:59,585 --> 01:41:02,765
Speaker 6:  fun and interesting. It just now says your device ran into a problem and

1664
01:41:02,765 --> 01:41:05,965
Speaker 6:  needs to restart, and then at the bottom it actually shows you

1665
01:41:06,545 --> 01:41:10,525
Speaker 6:  the code of what went wrong, which is vastly more useful than what

1666
01:41:10,525 --> 01:41:14,205
Speaker 6:  the blue screen of death did. But it's much less fun and the,

1667
01:41:14,305 --> 01:41:18,065
Speaker 6:  the black screen of death just doesn't, doesn't roll off the tongue the same

1668
01:41:18,065 --> 01:41:21,385
Speaker 6:  way. And I will be very sad to not have my computer just

1669
01:41:21,945 --> 01:41:23,625
Speaker 6:  randomly run up this thing.

1670
01:41:23,845 --> 01:41:26,265
Speaker 5:  You still got to use the acronym, right? It's still the BSOD. It's still

1671
01:41:26,265 --> 01:41:30,025
Speaker 6:  The BSOD, which I assume was intentional, but

1672
01:41:30,735 --> 01:41:34,585
Speaker 6:  like why? Just this is a tragedy. That's, I agree.

1673
01:41:34,585 --> 01:41:38,025
Speaker 6:  There's no other way of looking at this. This is like a canonical thing in

1674
01:41:38,025 --> 01:41:41,305
Speaker 6:  technology. Everybody knew what the blue screen of death was and now it's

1675
01:41:41,305 --> 01:41:41,385
Speaker 6:  gone.

1676
01:41:41,385 --> 01:41:45,225
Speaker 5:  Oh, they're getting rid of the sad face. Yeah. Yeah. Dude, I thought

1677
01:41:45,225 --> 01:41:49,065
Speaker 5:  they were just changing it from blue to black. No, this, the face has

1678
01:41:49,065 --> 01:41:49,225
Speaker 5:  gone,

1679
01:41:49,655 --> 01:41:52,395
Speaker 6:  This is what I'm saying. It's devastating.

1680
01:41:52,705 --> 01:41:54,555
Speaker 5:  It's very bad. I mean, it was supposed to be a power

1681
01:41:54,755 --> 01:41:57,475
Speaker 6:  Computer. Your used to feel bad for you. And now it's like, do either of

1682
01:41:57,475 --> 01:42:00,995
Speaker 6:  you ever watch the office? Yeah. Do you know the scene where, where it's

1683
01:42:00,995 --> 01:42:04,155
Speaker 6:  Kelly's birthday and Dwight just puts up a sign that says it is your birthday.

1684
01:42:04,375 --> 01:42:07,835
Speaker 6:  That's the black screen of death Now it just says your device ran into a

1685
01:42:07,835 --> 01:42:10,995
Speaker 6:  problem and needs to restart your computer used to feel bad for you. It was

1686
01:42:10,995 --> 01:42:14,435
Speaker 6:  like, I'm sorry, I blew, I blew it and I'm gonna fix it now. And now your

1687
01:42:14,555 --> 01:42:16,275
Speaker 6:  computer's just like, well, I'm dead.

1688
01:42:16,775 --> 01:42:20,275
Speaker 5:  In other great moments of sort of dead ahead tech writing that Reid is very

1689
01:42:20,275 --> 01:42:23,435
Speaker 5:  funny. Tom wrote this story and he wrote it. It's very, again, it's very

1690
01:42:23,435 --> 01:42:26,635
Speaker 5:  dead ahead. It's got some good quotes. The whole thing did great reporting

1691
01:42:26,895 --> 01:42:30,835
Speaker 5:  the last line dead ahead. The changes to the blue screen

1692
01:42:30,835 --> 01:42:34,075
Speaker 5:  of death are part of the broader effort by Microsoft to improve the resiliency

1693
01:42:34,135 --> 01:42:37,595
Speaker 5:  of Windows. In the wake of last year's CrowdStrike incident, which left millions

1694
01:42:37,595 --> 01:42:40,755
Speaker 5:  of Windows machines booting to the blue screen of death, and it's like, oh,

1695
01:42:40,755 --> 01:42:42,635
Speaker 5:  part of that effort was making the screens black.

1696
01:42:43,175 --> 01:42:46,755
Speaker 6:  Do you think it's so that if you see a lot of them all at once, it's not

1697
01:42:46,755 --> 01:42:48,715
Speaker 6:  as obvious that they've broken. That's

1698
01:42:48,715 --> 01:42:49,195
Speaker 5:  What I'm saying.

1699
01:42:49,305 --> 01:42:53,035
Speaker 6:  Yeah, right. You can like bunch of black screens a room full of them

1700
01:42:53,375 --> 01:42:57,355
Speaker 6:  and they don't all, it's not, it doesn't immediately scream, right? We're

1701
01:42:57,355 --> 01:42:57,435
Speaker 6:  all

1702
01:42:57,435 --> 01:43:01,315
Speaker 5:  Destroyed. It's like how Tiffany has the trademark on its Pantone blue, you

1703
01:43:01,315 --> 01:43:05,035
Speaker 5:  see the blue screen and Jare, that Window's PC has crashed and now it's just

1704
01:43:05,035 --> 01:43:08,235
Speaker 5:  a black screen. You're like, maybe that's Linux Like it's, it's the wrong

1705
01:43:08,235 --> 01:43:09,595
Speaker 5:  thing to have a trademark on.

1706
01:43:10,615 --> 01:43:12,035
Speaker 6:  Own your shame, Microsoft.

1707
01:43:13,295 --> 01:43:16,635
Speaker 5:  That's the broadcast. Everybody own your shame. We'll see you next week.

1708
01:43:17,585 --> 01:43:18,195
Speaker 5:  Rock and roll.

1709
01:43:24,175 --> 01:43:27,355
Speaker 15:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1710
01:43:27,355 --> 01:43:30,995
Speaker 15:  you. Give us a call at eight six six VERGE one one.

1711
01:43:31,175 --> 01:43:34,635
Speaker 15:  The Vergecast is a production of The Verge and the Vox Media Podcast network.

1712
01:43:34,935 --> 01:43:38,795
Speaker 15:  Our show is produced by Will Por, Eric Gomez and Brandon Keefer. And

1713
01:43:38,795 --> 01:43:40,235
Speaker 15:  that's it. We'll see you next week.

1714
01:43:46,805 --> 01:43:50,775
Speaker 4:  Support for this show comes from Pure Leaf Iced Tea. When you find yourself

1715
01:43:50,775 --> 01:43:54,325
Speaker 4:  in the the afternoon slump, you need the right thing to make you bounce back.

1716
01:43:54,825 --> 01:43:58,765
Speaker 4:  You need Pure Leaf iced tea. It's real brewed tea, made in a variety

1717
01:43:58,825 --> 01:44:02,485
Speaker 4:  of bold flavors. With just the right amount of naturally occurring

1718
01:44:02,765 --> 01:44:06,485
Speaker 4:  caffeine, you're left feeling refreshed and revitalized so you can be ready

1719
01:44:06,485 --> 01:44:10,205
Speaker 4:  to take on what's next. The next time you need to hit the reset button,

1720
01:44:10,475 --> 01:44:14,325
Speaker 4:  grab a Pure Leaf iced tea Time for a tea break. Time for a

1721
01:44:14,325 --> 01:44:14,805
Speaker 4:  Pure Leaf.

