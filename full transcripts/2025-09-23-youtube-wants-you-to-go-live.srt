1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: ec2871f2-5d84-4b2e-8bcc-68cd12bf1044
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-2756368277327774047/-6278290828739312224/s93290-US-5798s-1758622348.mp3
Description: In all the tech news and world news last week, YouTube's Made On event got a little lost. So we circled back: The Verge's Mia Sato explains why YouTube is suddenly all-in on livestreaming, why it seems to be rapidly turning into a shopping mall, and whether all these AI features will improve YouTube or destroy it. After that, it's time for a second round of David's Summer Takes, in which he subjects The Verge's Jake Kastrenakes and Hayden Field to his thoughts on Threads, podcasts, and social media. Finally, Hayden sticks around to answer a question on the Vergecast Hotline (call 866-VERGE11 or emailÂ vergecast@theverge.com!) about the words we use when we talk AI.




2
00:02:09,765 --> 00:02:13,635
Speaker 1:  We're gonna do two things. First, we're gonna talk about what's going on

3
00:02:13,635 --> 00:02:17,035
Speaker 1:  with YouTube. The company had one of its biggest product announcement events

4
00:02:17,035 --> 00:02:20,835
Speaker 1:  of the year last week, and shipped some stuff that I found

5
00:02:21,235 --> 00:02:24,875
Speaker 1:  slightly confusing and slightly surprising. So Mia Sato on our team is gonna

6
00:02:24,875 --> 00:02:28,235
Speaker 1:  come on and help me make sense of it. Then it's gonna be time for round two

7
00:02:28,735 --> 00:02:32,635
Speaker 1:  of David's somewhat deranged Summer Takes, which I'm very excited

8
00:02:32,655 --> 00:02:36,155
Speaker 1:  to shout into a microphone and have Jake and Hayden tell me how insane I

9
00:02:36,155 --> 00:02:39,315
Speaker 1:  am. We also have a question for The Vergecast hotline. Lots of stuff to get

10
00:02:39,315 --> 00:02:42,715
Speaker 1:  to. It's gonna be a really fun show. All of that is coming up in just a second.

11
00:02:43,215 --> 00:02:46,835
Speaker 1:  But first I have to go buy more old gadgets on eBay because thanks to version

12
00:02:46,835 --> 00:02:50,395
Speaker 1:  history, that's a thing I do now. This is The Vergecast, we'll be right back.

13
00:04:46,855 --> 00:04:50,585
Speaker 1:  Alright, we're back. Mia Sato is here. Hi Mia. Hi. Happy

14
00:04:50,585 --> 00:04:51,585
Speaker 1:  Friday. Happy

15
00:04:51,585 --> 00:04:51,905
Speaker 7:  Friday.

16
00:04:52,155 --> 00:04:53,385
Speaker 1:  Let's talk about YouTube ads.

17
00:04:54,715 --> 00:04:56,065
Speaker 9:  Happy, happy Friday.

18
00:04:57,215 --> 00:05:00,705
Speaker 1:  Yeah, we, we should say we're recording Friday afternoon. It's been a week,

19
00:05:01,685 --> 00:05:04,665
Speaker 1:  but one of the things that I think actually got kind of lost in the shuffle

20
00:05:04,815 --> 00:05:08,345
Speaker 1:  this week, which is one of the things I wanna talk about is made on YouTube,

21
00:05:08,345 --> 00:05:11,945
Speaker 1:  YouTube sort of big annual creator event. And I think

22
00:05:12,265 --> 00:05:16,105
Speaker 1:  a thing you And I agree on is that YouTube is culturally

23
00:05:16,165 --> 00:05:19,945
Speaker 1:  way more important than it gets credit for generally

24
00:05:20,105 --> 00:05:23,065
Speaker 1:  speaking. Like do you remember years ago when everybody was like, why isn't

25
00:05:23,095 --> 00:05:26,705
Speaker 1:  YouTube getting dragged in front of Congress to answer for the content moderation

26
00:05:26,705 --> 00:05:29,905
Speaker 1:  stuff? And like I agreed with a lot of that And I still think YouTube is

27
00:05:29,905 --> 00:05:33,145
Speaker 1:  like a very important platform that we don't understand all that well, And

28
00:05:33,145 --> 00:05:36,345
Speaker 1:  I think you understand it better than most. And so I just wanna talk through

29
00:05:36,345 --> 00:05:38,665
Speaker 1:  some of the new stuff and see if you can help me make sense of it. Does that

30
00:05:38,665 --> 00:05:38,945
Speaker 1:  sound good?

31
00:05:38,945 --> 00:05:40,025
Speaker 9:  Yeah, yeah, let's do it.

32
00:05:40,295 --> 00:05:43,285
Speaker 1:  Okay, so let's start with live streaming, which I think

33
00:05:43,715 --> 00:05:47,445
Speaker 1:  surprised me a lot that it was a big focus at

34
00:05:47,475 --> 00:05:50,645
Speaker 1:  Made on YouTube this year. This is basically like, I want everybody to picture

35
00:05:51,395 --> 00:05:54,965
Speaker 1:  like a, a big beautiful room filled with

36
00:05:55,885 --> 00:05:59,765
Speaker 1:  creators who love each other and YouTube. And that is like,

37
00:05:59,795 --> 00:06:02,365
Speaker 1:  this is very much like an event to just sort of bring those people together

38
00:06:02,365 --> 00:06:04,685
Speaker 1:  and tell them about all the lovely stuff that YouTube is doing for them.

39
00:06:05,065 --> 00:06:09,045
Speaker 1:  And so it's a real sort of statement of intent from the platform to be

40
00:06:09,045 --> 00:06:12,045
Speaker 1:  like, here is what matters on YouTube this year. And YouTube, as much as

41
00:06:12,285 --> 00:06:16,165
Speaker 1:  anything picked live streaming just in principle, is

42
00:06:16,165 --> 00:06:17,645
Speaker 1:  that as surprising to you as it was to me?

43
00:06:18,075 --> 00:06:21,765
Speaker 9:  Yeah, it was. It seemed kind of random, honestly. And there was like a

44
00:06:21,765 --> 00:06:25,685
Speaker 9:  boatload of news this year specifically I thought even more than the other

45
00:06:25,685 --> 00:06:29,285
Speaker 9:  years that I've covered. But you know, I think the way that they described

46
00:06:29,285 --> 00:06:33,005
Speaker 9:  it was like, these are the biggest updates to live streaming that we've ever

47
00:06:33,155 --> 00:06:36,775
Speaker 9:  made. And I was like, why? You know? Right.

48
00:06:37,045 --> 00:06:39,175
Speaker 9:  Yeah, it was, it was really random. Well,

49
00:06:39,175 --> 00:06:42,575
Speaker 1:  So try and make sense of it for me. Like why I think there, there's a,

50
00:06:42,845 --> 00:06:46,295
Speaker 1:  there's a cynical version of this that is like YouTube has already won everything

51
00:06:46,295 --> 00:06:49,975
Speaker 1:  else and now it is focusing on live. And I think on the back of like

52
00:06:50,125 --> 00:06:54,015
Speaker 1:  YouTube did an NFL game that went medium it, it is

53
00:06:54,015 --> 00:06:57,855
Speaker 1:  very clearly invested in this idea of live events

54
00:06:57,865 --> 00:07:01,815
Speaker 1:  being a thing on YouTube. And I think my read of this whole

55
00:07:02,095 --> 00:07:06,055
Speaker 1:  industry has been that live never quite became the thing we

56
00:07:06,055 --> 00:07:09,175
Speaker 1:  wanted it to be on the internet. There's lots of ways to live stream twitches

57
00:07:09,175 --> 00:07:13,135
Speaker 1:  working, like there's tons of live stuff out there, but I don't feel like

58
00:07:13,205 --> 00:07:16,855
Speaker 1:  live, at least in my brain, is next to like

59
00:07:17,065 --> 00:07:21,015
Speaker 1:  short form vertical video or like, I don't know, blockbuster

60
00:07:21,015 --> 00:07:24,455
Speaker 1:  movies in the sort of Yeah. Huge pillars of culture.

61
00:07:25,285 --> 00:07:27,655
Speaker 1:  Yeah, YouTube clearly wants it to be. Why do you think that is?

62
00:07:28,215 --> 00:07:32,095
Speaker 9:  I mean, okay, first I have to say I have, I have a quote I've,

63
00:07:32,095 --> 00:07:33,415
Speaker 9:  I've come prepared with a quote.

64
00:07:35,395 --> 00:07:38,855
Speaker 9:  If you look at YouTube as a series of products reverse engineered from

65
00:07:39,215 --> 00:07:41,855
Speaker 9:  advertising budgets, the company's mini offerings start to make a kind of

66
00:07:41,855 --> 00:07:42,935
Speaker 9:  sense. Do you know who wrote that?

67
00:07:43,485 --> 00:07:44,295
Speaker 1:  That sounds like me.

68
00:07:44,845 --> 00:07:48,775
Speaker 9:  It's you. And so like the first, you know, the first lens

69
00:07:48,775 --> 00:07:52,455
Speaker 9:  that I would look at this is like, that's a lot of time for

70
00:07:52,555 --> 00:07:56,295
Speaker 9:  ads, that's a lot of time for putting stuff

71
00:07:56,295 --> 00:08:00,255
Speaker 9:  that YouTube gets a cut of. There are a couple things that YouTube announced

72
00:08:00,255 --> 00:08:04,095
Speaker 9:  that I think like specifically goes go into this. Like one just, I think

73
00:08:04,125 --> 00:08:07,735
Speaker 9:  this was a feature that already existed, but the ability to tag

74
00:08:07,735 --> 00:08:11,655
Speaker 9:  products in streams. Think about like the many, many, I

75
00:08:11,655 --> 00:08:15,535
Speaker 9:  mean at least for like my demographic beauty, YouTube is like a

76
00:08:15,535 --> 00:08:19,055
Speaker 9:  huge place that is where kind of the modern beauty content creator,

77
00:08:19,055 --> 00:08:22,975
Speaker 9:  influencer started is YouTube doing like tutorials. Now

78
00:08:23,035 --> 00:08:26,615
Speaker 9:  all of those people do get ready with me streams on TikTok where they're

79
00:08:26,615 --> 00:08:29,495
Speaker 9:  using the products, telling you what it is and hanging out for like an hour

80
00:08:29,515 --> 00:08:33,135
Speaker 9:  and a half while they do their makeup. YouTube sees that obviously they want

81
00:08:33,135 --> 00:08:36,255
Speaker 9:  a cut of that and even better if they can like put the products into it.

82
00:08:37,195 --> 00:08:40,495
Speaker 9:  The other thing that YouTube announced was this sort of like

83
00:08:41,915 --> 00:08:45,855
Speaker 9:  ad experience where it doesn't interrupt the stream. I was watching a

84
00:08:45,855 --> 00:08:49,215
Speaker 9:  live stream on YouTube the week prior and it was really

85
00:08:49,735 --> 00:08:53,335
Speaker 9:  annoying because I was watching a stream from like a news outlet carrying

86
00:08:53,415 --> 00:08:57,405
Speaker 9:  a press conference and in the middle of them talking, it cut into an

87
00:08:57,465 --> 00:09:00,445
Speaker 9:  ad, like a mid-roll ad. And I was like, wait, I need to know

88
00:09:01,135 --> 00:09:04,765
Speaker 9:  don't do that please. But now YouTube is saying like, we can put

89
00:09:05,115 --> 00:09:08,685
Speaker 9:  your face on one side, the streamer's face on one side, and then the ad next

90
00:09:08,685 --> 00:09:12,325
Speaker 9:  to it and it's an uninterrupted experience. And this really to me,

91
00:09:12,435 --> 00:09:16,125
Speaker 9:  like you see it happening on other media, on other

92
00:09:16,125 --> 00:09:20,045
Speaker 9:  mediums as well. Like if you are a baseball fan, you know that

93
00:09:20,505 --> 00:09:23,685
Speaker 9:  it doesn't matter if Aaron judges at the plate, they're gonna get a word

94
00:09:23,685 --> 00:09:27,285
Speaker 9:  in from Taco Bell and it's gonna be like two seconds long and it's going

95
00:09:27,285 --> 00:09:31,005
Speaker 9:  to be seamless. So that like, it, that's kind of like a

96
00:09:31,165 --> 00:09:34,805
Speaker 9:  straightforward way to think of it is like this is more surface, this is

97
00:09:34,805 --> 00:09:35,445
Speaker 9:  more ad surface.

98
00:09:36,985 --> 00:09:40,605
Speaker 9:  The other thing I was thinking about was like, and this is kind of like a

99
00:09:40,605 --> 00:09:44,325
Speaker 9:  dark example, but this video

100
00:09:44,585 --> 00:09:48,205
Speaker 9:  in video experience where, you know, you can have one

101
00:09:48,245 --> 00:09:52,165
Speaker 9:  streamer reacting to another stream, I feel like the value

102
00:09:52,165 --> 00:09:55,805
Speaker 9:  proposition is just like so obvious because after Charlie

103
00:09:55,875 --> 00:09:59,325
Speaker 9:  Kirk was shot, everyone wanted to watch

104
00:09:59,665 --> 00:10:03,565
Speaker 9:  Hassan Piker react to it. Mm. And indeed they did go, thousands of people

105
00:10:03,565 --> 00:10:07,365
Speaker 9:  watched his Twitch stream where he was just reacting to like CNN

106
00:10:07,365 --> 00:10:10,565
Speaker 9:  or whatever. And you know, that's like,

107
00:10:11,095 --> 00:10:14,405
Speaker 9:  maybe they don't wanna monetize that. Like that's kind of, you know, a, a

108
00:10:15,125 --> 00:10:18,445
Speaker 9:  specific kind of event. But think about like the Met Gala

109
00:10:19,695 --> 00:10:23,205
Speaker 9:  award shows, VMAs just happened, even the election

110
00:10:23,545 --> 00:10:27,125
Speaker 9:  on TikTok on election night. People were just streaming for hours

111
00:10:27,525 --> 00:10:31,445
Speaker 9:  watching returns come in and watching like counties and states being

112
00:10:31,445 --> 00:10:35,405
Speaker 9:  called. So it really makes sense to me. And we

113
00:10:35,405 --> 00:10:39,125
Speaker 9:  can talk about like the actual format of live streaming, but

114
00:10:40,365 --> 00:10:43,365
Speaker 9:  considering YouTube really wants this experience of being on every screen

115
00:10:43,365 --> 00:10:46,565
Speaker 9:  in your home, obviously they want this too.

116
00:10:47,035 --> 00:10:50,805
Speaker 1:  Yeah, I think there's something to the sort of hang

117
00:10:51,015 --> 00:10:54,965
Speaker 1:  being the like thing that knits all of these ideas together in a really

118
00:10:55,085 --> 00:10:58,005
Speaker 1:  interesting way. And like, I'm glad you started by talking about the, the

119
00:10:58,005 --> 00:11:01,925
Speaker 1:  makeup stuff because it's very clear that that is top of mind at

120
00:11:01,925 --> 00:11:05,205
Speaker 1:  YouTube as they build product. And it was all over the place. It made on

121
00:11:05,205 --> 00:11:09,085
Speaker 1:  YouTube. And I think with live in particular, like I confess, I'm not a

122
00:11:09,085 --> 00:11:12,005
Speaker 1:  person who watches get ready with mainstreams, but I suspect you could go

123
00:11:12,005 --> 00:11:14,885
Speaker 1:  like line by line through the live streaming announcements and every one

124
00:11:14,885 --> 00:11:18,405
Speaker 1:  of them would make sense in context of get ready with mainstreams. Like yeah,

125
00:11:18,405 --> 00:11:21,565
Speaker 1:  there's the, the practice before going live thing, which I think I think

126
00:11:21,565 --> 00:11:25,445
Speaker 1:  is very smart and is is a good idea and like a fascinating sort

127
00:11:25,445 --> 00:11:29,245
Speaker 1:  of reflection of culture that we should talk about. But there's the thing

128
00:11:29,245 --> 00:11:33,165
Speaker 1:  where you can do horizontal and vertical lives simultaneously. Again, talking

129
00:11:33,165 --> 00:11:35,565
Speaker 1:  about your your point, we wanna be on every screen possible. That's the kind

130
00:11:35,565 --> 00:11:38,165
Speaker 1:  of thing that like most of these streams are just my face. That's a pretty

131
00:11:38,165 --> 00:11:41,765
Speaker 1:  easy thing to make work. You have the side-by-side ads, like you said that

132
00:11:41,765 --> 00:11:45,525
Speaker 1:  works. You have the AI tagging of products that

133
00:11:45,525 --> 00:11:49,445
Speaker 1:  when a product shows on screen, rather than me having to tell you

134
00:11:49,445 --> 00:11:52,525
Speaker 1:  what it is and how to go get it, it'll just be like, it's this and have a

135
00:11:52,525 --> 00:11:54,165
Speaker 1:  link and send me out to buy it. That's like

136
00:11:54,165 --> 00:11:55,045
Speaker 9:  Crazy. I have to fail

137
00:11:55,045 --> 00:11:58,485
Speaker 1:  Though. All of this is perfectly designed for like, I am a person using products

138
00:11:58,485 --> 00:12:01,685
Speaker 1:  in front of you and we're hanging out and we are going to monetize every

139
00:12:01,685 --> 00:12:05,045
Speaker 1:  single inch of this in a way that like feels

140
00:12:05,875 --> 00:12:09,525
Speaker 1:  grosser the longer you think about it, but also is just where all of these

141
00:12:09,805 --> 00:12:11,245
Speaker 1:  platforms have been headed for a while.

142
00:12:11,595 --> 00:12:15,365
Speaker 9:  Yeah, actually I just got a like push notification on TikTok today.

143
00:12:15,475 --> 00:12:19,085
Speaker 9:  That was like there, by the way, did you know that there's a feature where

144
00:12:19,945 --> 00:12:23,245
Speaker 9:  TikTok will scan your video and look for products using AI

145
00:12:23,745 --> 00:12:27,725
Speaker 9:  and it will create like an outward, like an internal app link

146
00:12:27,825 --> 00:12:31,645
Speaker 9:  to like more so if they see that I have like a record player in the background,

147
00:12:31,995 --> 00:12:35,165
Speaker 9:  they'll create, I don't know if it's like the blue clickable links or what,

148
00:12:35,225 --> 00:12:38,485
Speaker 9:  but it's like it will take you to a page where it's like searching for record

149
00:12:38,485 --> 00:12:42,245
Speaker 9:  players and then if you're a viewer or a creator, you have the ability

150
00:12:42,265 --> 00:12:44,965
Speaker 9:  to like turn that off And I need to still like figure out how to turn that

151
00:12:44,965 --> 00:12:48,685
Speaker 9:  off because I don't really want my content monetized like that, but some

152
00:12:48,685 --> 00:12:52,525
Speaker 9:  people do. Right. And it really feels like we're coalescing

153
00:12:52,525 --> 00:12:56,245
Speaker 9:  around a thing that I have thought for a while, which is like every,

154
00:12:56,415 --> 00:13:00,405
Speaker 9:  every piece of content that we watch online is an invitation to

155
00:13:00,405 --> 00:13:04,285
Speaker 9:  buy something. And that's always been true because of like the ads supported

156
00:13:04,285 --> 00:13:07,885
Speaker 9:  nature of these platforms. But it feels increasingly true

157
00:13:08,255 --> 00:13:12,005
Speaker 9:  right now, especially like post TikTok ascendants

158
00:13:12,895 --> 00:13:16,405
Speaker 9:  where it is really just like, it's a, it's a window into

159
00:13:17,325 --> 00:13:20,925
Speaker 9:  commerce And I think that is, I mean we can argue about whether that's good

160
00:13:20,925 --> 00:13:23,285
Speaker 9:  or bad, but it's, it's kind of unavoidable.

161
00:13:23,745 --> 00:13:27,005
Speaker 1:  Let let's briefly argue about whether it's good or bad because I I I, I think

162
00:13:27,005 --> 00:13:30,885
Speaker 1:  your your point is right that I think that's just where we are. And I think

163
00:13:31,345 --> 00:13:34,925
Speaker 1:  one of the most fascinating things about TikTok was that it did the thing

164
00:13:34,925 --> 00:13:38,525
Speaker 1:  everybody else is doing, which is use videos to sell you stuff. It just did

165
00:13:38,525 --> 00:13:42,205
Speaker 1:  it so much more brazenly, like Instagram spent so long

166
00:13:42,205 --> 00:13:46,005
Speaker 1:  trying to pretend that it was this sort of pure beautiful

167
00:13:46,005 --> 00:13:49,165
Speaker 1:  place for art and they were just kind of doing commerce over there. And then

168
00:13:49,165 --> 00:13:52,165
Speaker 1:  TikTok was like, what if all of the videos were just trying to sell you something?

169
00:13:52,165 --> 00:13:56,085
Speaker 1:  Like what? And, and it turns out people still watched those videos of like,

170
00:13:56,085 --> 00:13:59,645
Speaker 1:  people reviewing candy became a huge thing on like, yeah, it just, these

171
00:13:59,645 --> 00:14:03,565
Speaker 1:  things have just smashed right into each other. And I think one of

172
00:14:03,565 --> 00:14:06,045
Speaker 1:  the things that has been interesting about YouTube is that it has actually

173
00:14:06,045 --> 00:14:09,725
Speaker 1:  been slower to some of that stuff that I think the, the,

174
00:14:10,035 --> 00:14:13,885
Speaker 1:  they hold onto this idea that it is sort of the creator's

175
00:14:13,885 --> 00:14:17,685
Speaker 1:  place to be creative and that that matters. And the idea that they're just

176
00:14:17,705 --> 00:14:20,445
Speaker 1:  now really whole hog into this idea of like, we are going to

177
00:14:21,595 --> 00:14:24,805
Speaker 1:  shop a bull eyes is the hell out of every single thing that you make

178
00:14:25,505 --> 00:14:29,445
Speaker 1:  for you. It does seem like a tone shift on the platform.

179
00:14:29,835 --> 00:14:33,605
Speaker 9:  Yeah. I think also, you know, a lot of creators think of YouTube still

180
00:14:33,825 --> 00:14:37,285
Speaker 9:  as sort of the place where polished work

181
00:14:37,555 --> 00:14:41,205
Speaker 9:  goes. Hmm. And I've heard this come from a lot of like when

182
00:14:41,465 --> 00:14:45,445
Speaker 9:  the TikTok ban seemed like truly imminent, I don't even

183
00:14:45,445 --> 00:14:49,285
Speaker 9:  know, like six months ago, you know, there was a sense of like,

184
00:14:49,285 --> 00:14:53,085
Speaker 9:  where will I go? And a lot of creators were saying, I'm trying to learn

185
00:14:53,475 --> 00:14:57,365
Speaker 9:  YouTube. Right? Learn YouTube. Interesting. And that is because

186
00:14:58,455 --> 00:15:01,125
Speaker 9:  there is more of like, you know, I think people still think of it as like

187
00:15:01,165 --> 00:15:04,805
Speaker 9:  a mid to long form place. There's a little bit more like advanced editing

188
00:15:04,805 --> 00:15:07,525
Speaker 9:  that's happening. You're not just like camera in front of you

189
00:15:08,435 --> 00:15:12,325
Speaker 9:  yapping. You need some sort of like cadence. You need a

190
00:15:12,325 --> 00:15:15,885
Speaker 9:  good title, you need a good thumbnail, you need a good description that's

191
00:15:15,885 --> 00:15:19,605
Speaker 9:  like SEOed to hell. And so I think

192
00:15:19,605 --> 00:15:23,365
Speaker 9:  there's a sense of like, you know, there is a barrier to entry for

193
00:15:23,365 --> 00:15:27,165
Speaker 9:  YouTube And I think YouTube knows this also. That goes into like the

194
00:15:27,445 --> 00:15:30,805
Speaker 9:  practice thing that goes into a lot of the AI driven

195
00:15:31,155 --> 00:15:35,005
Speaker 9:  creator tools in the background that I wrote about earlier this week

196
00:15:35,065 --> 00:15:38,925
Speaker 9:  for for made on. And yeah, I think that YouTube kind

197
00:15:38,925 --> 00:15:42,645
Speaker 9:  of wants a piece of this, like off the cuff

198
00:15:43,035 --> 00:15:46,285
Speaker 9:  easy, literally no production value

199
00:15:47,195 --> 00:15:51,005
Speaker 9:  streams. And you know, the live stream stuff is

200
00:15:51,005 --> 00:15:54,965
Speaker 9:  really interesting because to me that is truly like an

201
00:15:55,075 --> 00:15:58,885
Speaker 9:  endlessly generative content style. What is different from

202
00:15:59,365 --> 00:16:02,485
Speaker 9:  a stream is different from like a traditional video in that literally any

203
00:16:02,485 --> 00:16:06,205
Speaker 9:  moment can be a stream. Like I have, I follow a few TikTok creators who

204
00:16:06,335 --> 00:16:08,925
Speaker 9:  their whole thing is, they're just, they have like a selfie stick. They're

205
00:16:08,925 --> 00:16:12,045
Speaker 9:  walking around New York City and they're just responding to comments for

206
00:16:12,045 --> 00:16:15,725
Speaker 9:  hours and hours and hours and that pays the bills. And that is

207
00:16:15,725 --> 00:16:19,565
Speaker 9:  something I think that YouTube rightfully sees as being like a

208
00:16:19,565 --> 00:16:23,405
Speaker 9:  way that people consume content. Some people like to watch

209
00:16:23,645 --> 00:16:27,085
Speaker 9:  streams because you don't have to swipe, like, you know, people are lazy.

210
00:16:27,275 --> 00:16:30,285
Speaker 9:  Sure, yeah. Some people watch streams because they're really invested in

211
00:16:30,605 --> 00:16:34,285
Speaker 9:  a specific person where if they can watch you do a thing

212
00:16:34,625 --> 00:16:38,525
Speaker 9:  in real time, they will watch it and send you gif obviously that's a

213
00:16:38,525 --> 00:16:42,485
Speaker 9:  big part of it. Yeah. And yeah, YouTube also had like a tool that, that

214
00:16:42,685 --> 00:16:45,925
Speaker 9:  they announced where it will cut down pieces of your stream

215
00:16:46,465 --> 00:16:50,245
Speaker 9:  to become other types of content like shorts or mid form or

216
00:16:50,405 --> 00:16:54,335
Speaker 9:  whatever. I mean, it's kind of a no brainer, right?

217
00:16:54,335 --> 00:16:58,295
Speaker 9:  They need more content, they want people to make more stuff. And so of course

218
00:16:58,295 --> 00:17:01,775
Speaker 9:  they're going to one, tell people to stream for hours.

219
00:17:02,235 --> 00:17:06,175
Speaker 9:  And two, give you an easy way to just like pull that out into more like

220
00:17:07,335 --> 00:17:08,175
Speaker 9:  permanent content.

221
00:17:08,795 --> 00:17:12,655
Speaker 1:  That's really interesting because that makes live streaming sound simultaneously

222
00:17:12,805 --> 00:17:16,615
Speaker 1:  both really high stakes and really low stakes, right? Like

223
00:17:16,615 --> 00:17:20,495
Speaker 1:  it's designed to be the most human off the

224
00:17:20,495 --> 00:17:24,175
Speaker 1:  cuff version of this, but it's also like, it, it becomes the sort of

225
00:17:24,175 --> 00:17:27,735
Speaker 1:  entry point for both a lot of your audience and a lot of your content, which

226
00:17:27,735 --> 00:17:31,335
Speaker 1:  makes this practice thing even more interesting. Tell me why you think the

227
00:17:31,615 --> 00:17:34,695
Speaker 1:  practice before you go live feature is, is fascinating. Okay.

228
00:17:34,975 --> 00:17:38,095
Speaker 9:  I, this one truly perplexed me

229
00:17:38,565 --> 00:17:42,295
Speaker 9:  because I think, like I understand the utility, right? If you are

230
00:17:42,345 --> 00:17:46,335
Speaker 9:  maybe like a dancer and you're going to stream practice that day, right?

231
00:17:46,355 --> 00:17:50,165
Speaker 9:  You work, you're, you are a dancer at one of these like viral, also like

232
00:17:50,355 --> 00:17:53,365
Speaker 9:  popularized by YouTube dance studios and you're gonna just like stream your

233
00:17:53,525 --> 00:17:57,245
Speaker 9:  practice. Sure. Maybe I like, I'm just making up a thing, but I understand

234
00:17:57,245 --> 00:18:00,595
Speaker 9:  that. But also, it's so funny to me,

235
00:18:01,095 --> 00:18:04,755
Speaker 9:  you're gonna practice to be off the cuff, you're gonna practice to be live

236
00:18:04,755 --> 00:18:08,635
Speaker 9:  with your fans. You know, it's just like, it's so weird

237
00:18:09,055 --> 00:18:10,035
Speaker 9:  And I kind of, I mean,

238
00:18:10,075 --> 00:18:12,915
Speaker 1:  I hate to be the one to break this mirage for you Mia, But that is kind of

239
00:18:12,915 --> 00:18:13,555
Speaker 1:  how it works.

240
00:18:14,025 --> 00:18:17,635
Speaker 9:  Totally. Totally. But the idea that there needs to be like a

241
00:18:17,635 --> 00:18:21,555
Speaker 9:  dedicated feature and kind of like a pulling back the

242
00:18:21,555 --> 00:18:24,355
Speaker 9:  curtain moment where it's just like we're just gonna straight up say like,

243
00:18:25,625 --> 00:18:29,195
Speaker 9:  none of this stuff is organic. None of this stuff is like authentic

244
00:18:29,335 --> 00:18:32,635
Speaker 9:  really in the way that viewers might be led to believe. Like,

245
00:18:33,285 --> 00:18:36,835
Speaker 9:  we're gonna just put a practice screen, you know, like a green room into

246
00:18:36,835 --> 00:18:40,235
Speaker 9:  this. It's just, it's so funny to me.

247
00:18:40,695 --> 00:18:44,595
Speaker 9:  And we can, I think this kind of feeds into another thing

248
00:18:44,595 --> 00:18:48,555
Speaker 9:  I've been thinking about, which is just like, we can talk about it later.

249
00:18:48,875 --> 00:18:52,555
Speaker 9:  I wanna hear what you think about practice, but this idea that tech

250
00:18:52,835 --> 00:18:56,595
Speaker 9:  platforms and social media platforms have never been more open

251
00:18:56,745 --> 00:18:59,515
Speaker 9:  with what they want. You know, it is just in a weird way.

252
00:19:00,855 --> 00:19:04,595
Speaker 9:  And yeah, let's, we'll get into that later, but what do you think about practice?

253
00:19:04,735 --> 00:19:07,475
Speaker 1:  We are very much in the like saying the quiet part Loud. Yes.

254
00:19:08,425 --> 00:19:09,235
Speaker 9:  Loud as hell.

255
00:19:09,235 --> 00:19:12,435
Speaker 1:  Yeah. The, the practice feature made me think of like, did you ever have

256
00:19:12,435 --> 00:19:16,355
Speaker 1:  that moment when you learned how reality TV

257
00:19:16,355 --> 00:19:19,115
Speaker 1:  shows actually work and that there are like producers in the background being

258
00:19:19,115 --> 00:19:22,915
Speaker 1:  like, you guys should fight about this. Or like, you see the, my wife And

259
00:19:22,915 --> 00:19:25,635
Speaker 1:  I watch house shows all the time and there's always that moment where it's

260
00:19:25,635 --> 00:19:29,475
Speaker 1:  like the couple sitting at a table drinking smoothies, like talking about

261
00:19:29,495 --> 00:19:32,795
Speaker 1:  how much money they're gonna make from this house. And it's like you,

262
00:19:33,365 --> 00:19:36,515
Speaker 1:  until you start thinking about it, it all feels sort of natural and normal.

263
00:19:36,535 --> 00:19:39,755
Speaker 1:  And then it's like, oh no, this is like days later. They've already had this

264
00:19:39,755 --> 00:19:43,395
Speaker 1:  conversation six times and a producer just sat them down and said, and now

265
00:19:43,985 --> 00:19:46,435
Speaker 1:  talk about how much money you're gonna make from selling this house. And

266
00:19:46,435 --> 00:19:50,395
Speaker 1:  it's, as soon as it like the, the veil is lifted,

267
00:19:51,255 --> 00:19:54,035
Speaker 1:  it starts to feel weird. And now I watch these shows and my wife And I like

268
00:19:54,035 --> 00:19:57,035
Speaker 1:  have made a game out of trying to figure out what's, what's off the cuff

269
00:19:57,035 --> 00:20:00,635
Speaker 1:  and what isn't. And it just changes the way that you look at all of these

270
00:20:00,635 --> 00:20:04,035
Speaker 1:  things. And it's like some of this stuff like the, the practice before going

271
00:20:04,035 --> 00:20:07,995
Speaker 1:  live and even the way that we're talking about like AI product tag, it's

272
00:20:07,995 --> 00:20:10,795
Speaker 1:  like this stuff has always been more

273
00:20:11,345 --> 00:20:15,035
Speaker 1:  intentional and more thought out and more rehearsed and more

274
00:20:15,035 --> 00:20:18,915
Speaker 1:  practiced than you want it to be. It's just that the, the

275
00:20:18,915 --> 00:20:21,595
Speaker 1:  shame of that is completely gone. And I, in a lot of ways I think that's

276
00:20:21,595 --> 00:20:24,635
Speaker 1:  fine. Right? Like we, we should stop pretending that this stuff is real life

277
00:20:24,635 --> 00:20:28,315
Speaker 1:  Yeah. When it's not. Yeah. And I don't know that it actually changes the

278
00:20:28,315 --> 00:20:31,155
Speaker 1:  appeal in a lot of ways that it's not real life, but it isn't, it never has

279
00:20:31,155 --> 00:20:34,435
Speaker 1:  been. And we should, we should stop talking about it like it is. Yeah. But

280
00:20:34,435 --> 00:20:37,835
Speaker 1:  it is interesting to see every platform. And I think YouTube

281
00:20:38,455 --> 00:20:42,395
Speaker 1:  to your point is like the, the last one to really get on board with the

282
00:20:42,395 --> 00:20:45,755
Speaker 1:  idea of like, this is a ruthless moneymaking machine and we're gonna start

283
00:20:45,755 --> 00:20:46,635
Speaker 1:  showing it to everybody

284
00:20:47,065 --> 00:20:50,165
Speaker 9:  And that's okay. And we're not sorry. No. You know what I mean? It's, it's

285
00:20:50,165 --> 00:20:53,605
Speaker 9:  just this is what we're doing here. Yeah. Yeah. For sure. I wrote about this

286
00:20:53,605 --> 00:20:57,325
Speaker 9:  in my piece that I published around Madeon when I was writing about

287
00:20:57,355 --> 00:21:01,245
Speaker 9:  like behind the scenes creator tools. Mm. But it really feels

288
00:21:01,275 --> 00:21:05,005
Speaker 9:  like it's a new era both for the platforms and also

289
00:21:05,165 --> 00:21:08,885
Speaker 9:  for creators online. Like in that story I wrote

290
00:21:08,885 --> 00:21:12,605
Speaker 9:  about how, I talked a little bit about how like, you know, we

291
00:21:13,845 --> 00:21:16,805
Speaker 9:  historically we've really talked about platforms being black boxes

292
00:21:18,225 --> 00:21:21,125
Speaker 9:  and to some extent that's still true. Like there's a lot in the algorithm

293
00:21:21,395 --> 00:21:24,725
Speaker 9:  that we don't know. There's a lot in, you know, internal

294
00:21:24,875 --> 00:21:28,405
Speaker 9:  conversations and discussions at these companies that creators aren't privy

295
00:21:28,405 --> 00:21:32,045
Speaker 9:  to. We, there are things that we don't know about how they operate, but

296
00:21:32,115 --> 00:21:36,085
Speaker 9:  also it feels like it's never been clearer. Right? And

297
00:21:36,195 --> 00:21:39,405
Speaker 9:  it's because the platforms will just come out and say it what they want from

298
00:21:39,685 --> 00:21:43,005
Speaker 9:  creators. And I think of it almost as like,

299
00:21:43,845 --> 00:21:46,845
Speaker 9:  I, I see it as really mirroring what happened in SEO

300
00:21:48,185 --> 00:21:52,005
Speaker 9:  and it's just like we're just gonna tell you what to make

301
00:21:52,185 --> 00:21:55,845
Speaker 9:  and how to make it right. When TikTok changes its creator fund

302
00:21:55,895 --> 00:21:59,445
Speaker 9:  where only people who make videos over one minute are eligible.

303
00:22:00,075 --> 00:22:03,845
Speaker 9:  It's just saying make longer videos. We, we wanna compete with

304
00:22:03,845 --> 00:22:07,565
Speaker 9:  YouTube. Stop making your little 15 second things. We want long videos,

305
00:22:07,665 --> 00:22:11,565
Speaker 9:  we want long form, we want you to make videos in horizontal,

306
00:22:11,565 --> 00:22:15,405
Speaker 9:  which is like a thing that they did at one point. YouTube is giving creators

307
00:22:16,575 --> 00:22:20,205
Speaker 9:  tools to just like, use this and figure out which

308
00:22:20,595 --> 00:22:23,925
Speaker 9:  thumbnail and title combination your viewers like best that lead to the highest

309
00:22:23,925 --> 00:22:27,765
Speaker 9:  watch time. Just do it. Like, we'll tell you previously, that was all

310
00:22:27,765 --> 00:22:31,005
Speaker 9:  stuff that creators had to kind of like figure out themselves

311
00:22:31,945 --> 00:22:35,925
Speaker 9:  and you know, these, these optimizing this, this kind of like

312
00:22:36,165 --> 00:22:38,845
Speaker 9:  ruthless optimizing is

313
00:22:39,655 --> 00:22:43,605
Speaker 9:  beneficial for both parties, right? YouTube wants people watching

314
00:22:43,605 --> 00:22:46,285
Speaker 9:  videos longer. They don't, I don't think they really care what videos they

315
00:22:46,285 --> 00:22:50,045
Speaker 9:  are, they just want more watch time. Yeah. Creators specifically want people

316
00:22:50,045 --> 00:22:53,805
Speaker 9:  to be watching their videos. And so, I mean, it makes sense. I can see why

317
00:22:54,085 --> 00:22:57,605
Speaker 9:  creators wanted a tool like the thumbnail and title thing for so long, but

318
00:22:57,605 --> 00:23:01,405
Speaker 9:  it's really fascinating to me, you know, YouTube is giving you an AI

319
00:23:01,405 --> 00:23:04,805
Speaker 9:  chat bot that just tells you what's wrong with your content and is like,

320
00:23:05,025 --> 00:23:08,765
Speaker 9:  change this, right? Like, you didn't do the intro, right?

321
00:23:08,795 --> 00:23:12,485
Speaker 9:  Like, do change this next time. And yeah, like on

322
00:23:12,545 --> 00:23:16,285
Speaker 9:  TikTok there will be like, sometimes when you search for things on

323
00:23:16,285 --> 00:23:20,205
Speaker 9:  TikTok, there will be like a weird little box that

324
00:23:20,425 --> 00:23:23,645
Speaker 9:  TikTok shows you that says like, Hey, there's not that much content about

325
00:23:23,645 --> 00:23:27,245
Speaker 9:  this search term. Why don't you just make it like, interesting.

326
00:23:27,475 --> 00:23:30,725
Speaker 9:  It's so funny to me. It's really, really interesting. And I think like

327
00:23:31,435 --> 00:23:35,325
Speaker 9:  this is kind of what is happening with live on YouTube right

328
00:23:35,325 --> 00:23:38,925
Speaker 9:  now. They're like, we don't think enough people are streaming here. We don't

329
00:23:38,925 --> 00:23:42,565
Speaker 9:  like, maybe they don't like dislike, but you know, we want a different type

330
00:23:42,565 --> 00:23:46,325
Speaker 9:  of streamer on here. They're trying to court those people. So now

331
00:23:46,325 --> 00:23:50,285
Speaker 9:  they're like, just, why don't you just stream and just like make your videos

332
00:23:50,515 --> 00:23:52,605
Speaker 9:  from your streams, right? That's what we want.

333
00:23:52,905 --> 00:23:56,485
Speaker 1:  And what we've learned over the years is that these platforms have

334
00:23:57,265 --> 00:24:00,805
Speaker 1:  all of the tools and all of the leverage in that debate, right? Like,

335
00:24:01,165 --> 00:24:03,525
Speaker 1:  'cause you get to a point where it's like, okay, if I'm gonna keep making

336
00:24:03,555 --> 00:24:07,485
Speaker 1:  what I want, even if it's against what quote unquote the platform wants

337
00:24:08,065 --> 00:24:10,685
Speaker 1:  and what the algorithm wants, and I'm eventually just going to be making

338
00:24:10,685 --> 00:24:14,445
Speaker 1:  videos into a void Because, because what, what the algorithm

339
00:24:14,475 --> 00:24:18,245
Speaker 1:  will show people and what the platform wants has moved away from me. And

340
00:24:18,365 --> 00:24:20,885
Speaker 1:  I think we, we've seen, and you And I both talked to a lot of people who

341
00:24:20,885 --> 00:24:24,245
Speaker 1:  are stuck in that place of like, I know what works And I know what I wanna

342
00:24:24,245 --> 00:24:28,205
Speaker 1:  do. And putting those things together is an

343
00:24:28,205 --> 00:24:31,325
Speaker 1:  increasingly big challenge. And I think especially right now,

344
00:24:32,115 --> 00:24:35,805
Speaker 1:  YouTube is being increasingly clear about what it wants and it has all of

345
00:24:35,805 --> 00:24:39,605
Speaker 1:  the levers to pull to make people who do more

346
00:24:39,605 --> 00:24:43,325
Speaker 1:  live streaming be more successful, right? Yeah. Like, it just, it just can

347
00:24:43,325 --> 00:24:46,805
Speaker 1:  do that. Your, your lives don't have to be good. YouTube can still put them

348
00:24:46,805 --> 00:24:49,685
Speaker 1:  in front of people and they can be great. And if YouTube doesn't put them

349
00:24:49,685 --> 00:24:53,325
Speaker 1:  in front of people, you're, you're host. And so I think it's just

350
00:24:53,675 --> 00:24:57,445
Speaker 1:  like we, we've seen YouTube going from sort of pretending that

351
00:24:57,725 --> 00:25:01,125
Speaker 1:  creators lead the dynamic to, like you said, telling creators what to do.

352
00:25:01,195 --> 00:25:04,325
Speaker 1:  Yeah. And TikTok sort of to its credit

353
00:25:05,145 --> 00:25:08,885
Speaker 1:  was the one that has enabled a lot of this because it never pretended otherwise.

354
00:25:09,285 --> 00:25:13,125
Speaker 1:  TikTok always a place to ruthlessly optimize your content to be viewed by

355
00:25:13,125 --> 00:25:16,405
Speaker 1:  as many people as possible. It was a marketing platform and it, it has been

356
00:25:16,425 --> 00:25:19,405
Speaker 1:  an enormously successful one. And YouTube was always the one that was like,

357
00:25:19,405 --> 00:25:23,325
Speaker 1:  okay, go get big audience discovery gaming the TikTok

358
00:25:23,325 --> 00:25:27,285
Speaker 1:  algorithm and then come here and make your art. And that totally, it is completely

359
00:25:28,025 --> 00:25:30,285
Speaker 1:  losing that pretense kind of every day.

360
00:25:30,635 --> 00:25:33,925
Speaker 9:  Yeah. Yeah. It's interesting because it's like, you kind of see

361
00:25:34,435 --> 00:25:38,285
Speaker 9:  this dynamic play out in different ways on different platforms. Like

362
00:25:38,755 --> 00:25:42,685
Speaker 9:  there's a reason why every Instagram photographer is now a reels like

363
00:25:42,685 --> 00:25:46,125
Speaker 9:  an Instagram reels photographer or whatever, you know, it's just like, that

364
00:25:46,125 --> 00:25:49,085
Speaker 9:  is what it is. And I think YouTube still

365
00:25:50,025 --> 00:25:53,885
Speaker 9:  is trying to hold on to this idea that they

366
00:25:53,885 --> 00:25:57,605
Speaker 9:  are the creator platform, like you said, and

367
00:25:58,115 --> 00:26:01,765
Speaker 9:  some of the stuff, when you read it, it just like really punctures that veil.

368
00:26:03,185 --> 00:26:07,005
Speaker 9:  And I think I'm, I'm just, I'm curious how

369
00:26:08,075 --> 00:26:12,005
Speaker 9:  YouTubers react to it because I think it's true that like,

370
00:26:12,025 --> 00:26:15,525
Speaker 9:  not everyone wants to stream, not everyone wants to be on live

371
00:26:16,585 --> 00:26:20,365
Speaker 9:  and you know, there's like, I I just am, I'm curious like what level of

372
00:26:20,645 --> 00:26:21,365
Speaker 9:  resistance really

373
00:26:23,215 --> 00:26:25,605
Speaker 9:  is tenable, right?

374
00:26:26,355 --> 00:26:29,645
Speaker 1:  Yeah. Like how much can you fight this and and still be viable on YouTube

375
00:26:29,705 --> 00:26:32,965
Speaker 1:  is is gonna be an interesting question. Yeah. I have to let you go here in

376
00:26:32,965 --> 00:26:35,605
Speaker 1:  a minute, but let's talk about AI just before we do. 'cause we've, we've,

377
00:26:35,605 --> 00:26:39,165
Speaker 1:  we've talked a little bit about it and this is like the second or third year

378
00:26:39,165 --> 00:26:42,685
Speaker 1:  in a row that AI was a big story of made on, and not just like

379
00:26:43,545 --> 00:26:46,325
Speaker 1:  AI as a sort of behind the scenes tool to help you answer comments and manage

380
00:26:46,605 --> 00:26:50,485
Speaker 1:  whatever AI as like a, a creative tool for shorts in particular. Yeah.

381
00:26:50,485 --> 00:26:53,445
Speaker 1:  And this year they're just like, just do a prompt and it'll make a short

382
00:26:53,445 --> 00:26:57,405
Speaker 1:  for you and publish your shorts. Yeah. And it's like, what, what's your

383
00:26:58,045 --> 00:27:01,325
Speaker 1:  I have not seen a ton of that. I, I was saying this on, on Friday show, but

384
00:27:01,405 --> 00:27:05,365
Speaker 1:  I deleted Instagram and TikTok from my phone for the summer as a way to be

385
00:27:05,365 --> 00:27:08,445
Speaker 1:  like, I'm on parental leave, I'm gonna spend my time with my child. And it's

386
00:27:08,445 --> 00:27:11,445
Speaker 1:  very not on YouTube. Very. And what actually happened is I just watched a

387
00:27:11,445 --> 00:27:15,325
Speaker 1:  lot of YouTube shorts. I encountered almost no

388
00:27:15,475 --> 00:27:19,285
Speaker 1:  like generative AI stuff on shorts. But I, I understand that that's like

389
00:27:19,385 --> 00:27:23,005
Speaker 1:  my own algorithm, but I'm curious like, are you seeing this stuff

390
00:27:23,195 --> 00:27:26,965
Speaker 1:  take off this idea that like, upload a photo to YouTube and tell it what

391
00:27:26,965 --> 00:27:30,525
Speaker 1:  to do and it'll use AI to add motion and turn it into a Jan challenge and

392
00:27:30,525 --> 00:27:31,565
Speaker 1:  you can make that a short, like,

393
00:27:32,165 --> 00:27:35,605
Speaker 9:  I don't know if it's like that level. I think there is a level of

394
00:27:36,005 --> 00:27:39,885
Speaker 9:  slop and like, you know, I think a couple weeks ago or

395
00:27:40,005 --> 00:27:43,685
Speaker 9:  a few, like a month ago, that was kind of the fear, right? That YouTube was

396
00:27:43,685 --> 00:27:47,525
Speaker 9:  like tweaking something about its policies for monetization and people

397
00:27:47,525 --> 00:27:50,605
Speaker 9:  freaked out because they thought that it meant anyone who's using AI and

398
00:27:50,605 --> 00:27:53,565
Speaker 9:  their videos would be affected. And then they went back and clarified and

399
00:27:53,565 --> 00:27:57,085
Speaker 9:  it's like, no, we're just updating what we mean by like

400
00:27:57,265 --> 00:28:01,045
Speaker 9:  spammy content. Right? I think they changed the name to Inauthentic, but

401
00:28:01,275 --> 00:28:05,165
Speaker 9:  yeah, I think they probably recognize that this is a way that people

402
00:28:05,395 --> 00:28:08,965
Speaker 9:  have been filling the platform with garbage. I think that's true

403
00:28:10,045 --> 00:28:13,865
Speaker 9:  across platforms, whether they're using the in-app

404
00:28:13,865 --> 00:28:16,945
Speaker 9:  tools or not. Like TikTok is filled with

405
00:28:17,805 --> 00:28:21,705
Speaker 9:  AI sludge one time actually, like I wrote a story and then I

406
00:28:21,715 --> 00:28:25,145
Speaker 9:  found a video of an AI generated avatar

407
00:28:25,465 --> 00:28:29,065
Speaker 9:  reciting my story back to me And I was like, whoa, what is happening?

408
00:28:29,175 --> 00:28:31,745
Speaker 9:  It's interesting because I think that YouTube is kind of like talking out

409
00:28:31,745 --> 00:28:34,825
Speaker 9:  of its mouth on one side, and then there's like everything else that's happening

410
00:28:34,825 --> 00:28:38,425
Speaker 9:  in the background, which is that millions of YouTube videos

411
00:28:38,935 --> 00:28:42,505
Speaker 9:  have been trained on without the creator's permission. Yep. And

412
00:28:42,655 --> 00:28:45,905
Speaker 9:  YouTube has been sort of like staying out of it

413
00:28:46,445 --> 00:28:50,425
Speaker 9:  for the most part. There is actually a great piece in The Atlantic

414
00:28:50,425 --> 00:28:53,905
Speaker 9:  where you can search for creators and see, you know, what, what videos we're

415
00:28:53,905 --> 00:28:57,385
Speaker 9:  trained on. And it's a lot of like how to content,

416
00:28:58,965 --> 00:29:02,625
Speaker 9:  but at the same time it's also like encouraging content creators

417
00:29:03,125 --> 00:29:07,025
Speaker 9:  to use these tools. And I have to, I really wonder how

418
00:29:07,265 --> 00:29:10,985
Speaker 9:  creators feel about that. If you're, if this

419
00:29:11,085 --> 00:29:14,465
Speaker 9:  bot can also like come up with video ideas, write the script

420
00:29:15,685 --> 00:29:19,625
Speaker 9:  dub your lips, right? To like do the lip matching for voice, for like lip

421
00:29:19,785 --> 00:29:23,585
Speaker 9:  syncing with dubs. What is your job? It can also

422
00:29:23,585 --> 00:29:26,345
Speaker 9:  edit your shorts by the way. Right, right. Like, I'm, I'm just that's such,

423
00:29:26,365 --> 00:29:29,345
Speaker 1:  That's such a good version of the question because I think like the, the,

424
00:29:29,845 --> 00:29:33,305
Speaker 1:  the pro case for all of this stuff, which I'm it's actually

425
00:29:33,305 --> 00:29:36,185
Speaker 1:  sympathetic to, is that it makes it easier to make stuff. And I think

426
00:29:37,145 --> 00:29:39,945
Speaker 1:  what we have seen generally speaking is that when technology makes it easier

427
00:29:39,965 --> 00:29:43,565
Speaker 1:  to make stuff, people make more stuff, some of it is bad, some of it is good.

428
00:29:43,665 --> 00:29:47,085
Speaker 1:  And like the, the idea of these tools is like democratizing forces to help

429
00:29:47,405 --> 00:29:51,085
Speaker 1:  creative people make more stuff, I think is awesome. But this question of

430
00:29:51,085 --> 00:29:54,725
Speaker 1:  what is your job I think is, is like the question in front of

431
00:29:54,795 --> 00:29:58,005
Speaker 1:  both the people making these tools and the people using them, because you're

432
00:29:58,005 --> 00:30:01,645
Speaker 1:  right, like I could sit here, I could type one line into

433
00:30:01,645 --> 00:30:04,685
Speaker 1:  YouTube's backend and it could, it could make the short, it could edit the

434
00:30:04,685 --> 00:30:06,685
Speaker 1:  short and it could publish the short and it could optimize the short and

435
00:30:06,685 --> 00:30:08,325
Speaker 1:  it could put it out. And that might get a lot of views.

436
00:30:10,285 --> 00:30:14,185
Speaker 1:  It feels like a bad outcome for like big human ways that I

437
00:30:14,185 --> 00:30:17,185
Speaker 1:  can't even necessarily describe That feels like a bad outcome. And I think

438
00:30:18,255 --> 00:30:21,905
Speaker 1:  what seems to me is that a lot of creators are gonna have to spend the next

439
00:30:22,775 --> 00:30:26,545
Speaker 1:  year or years trying to figure out what they do here

440
00:30:26,725 --> 00:30:30,705
Speaker 1:  and what the thing is that makes them different. Yeah. 'cause

441
00:30:30,705 --> 00:30:34,505
Speaker 1:  it's like the, everyone is gonna have access to these same tools that do

442
00:30:34,505 --> 00:30:38,305
Speaker 1:  the same mediocre job for you. Yeah. How, what do

443
00:30:38,305 --> 00:30:42,025
Speaker 1:  you do here and how do you, how do you elevate this stuff? And I feel like

444
00:30:42,025 --> 00:30:45,905
Speaker 1:  that just gets murkier and murkier with every one of these new tools

445
00:30:45,905 --> 00:30:48,745
Speaker 1:  that gets added That is just a new thing you don't even have to do anymore.

446
00:30:49,015 --> 00:30:51,745
Speaker 9:  Yeah. And if you're just the person on the screen, well guess what? They

447
00:30:51,745 --> 00:30:55,225
Speaker 9:  have something for that too. Like, it's just like, I

448
00:30:55,365 --> 00:30:58,905
Speaker 9:  really, really wonder, we should have, we should have like a bunch of content

449
00:30:59,145 --> 00:31:02,385
Speaker 9:  creators on here And I would love to ask like, do you feel the fear of like

450
00:31:02,565 --> 00:31:06,225
Speaker 9:  job replacement, you know what I mean, that other workers do because you

451
00:31:06,225 --> 00:31:06,705
Speaker 9:  are a worker

452
00:31:08,525 --> 00:31:11,425
Speaker 9:  and Yeah, I don't know. It's really weird. It's really weird. And also do

453
00:31:11,665 --> 00:31:15,585
Speaker 9:  audiences care, right? Do they care if your scripts are

454
00:31:15,605 --> 00:31:19,105
Speaker 9:  all written by AI now that you're getting all your ideas from YouTube's like

455
00:31:19,165 --> 00:31:19,705
Speaker 9:  in app,

456
00:31:21,495 --> 00:31:23,105
Speaker 9:  like AI bot,

457
00:31:23,355 --> 00:31:26,025
Speaker 1:  Right? I don't know. It's a great question. And how do they, how do those

458
00:31:26,055 --> 00:31:29,345
Speaker 1:  viewers show you that they care too, right? Because it's like, what we keep

459
00:31:29,345 --> 00:31:33,105
Speaker 1:  hearing is basically the numbers keep going up And I, I just

460
00:31:33,225 --> 00:31:36,225
Speaker 1:  keep thinking about this company that, that, I forget the exact name of it

461
00:31:36,245 --> 00:31:39,665
Speaker 1:  and I'm glad to not be able to say it out loud, but basically their thing

462
00:31:39,665 --> 00:31:43,625
Speaker 1:  was, we're gonna make Infinity podcasts and every one of those podcasts

463
00:31:43,625 --> 00:31:46,985
Speaker 1:  is going to get 10 listeners, but it's so cheap for us to make and we're

464
00:31:46,985 --> 00:31:49,665
Speaker 1:  gonna do, it's such incredible volume that it's gonna make us money. Yeah.

465
00:31:49,965 --> 00:31:53,085
Speaker 1:  And on the one hand, that's probably gonna work,

466
00:31:53,905 --> 00:31:57,805
Speaker 1:  on the other hand, that is a, that that sucks And I hate it And I

467
00:31:57,805 --> 00:32:01,685
Speaker 1:  want that thing to not exist because what you're saying is we are going to

468
00:32:01,695 --> 00:32:05,685
Speaker 1:  flood this thing with so much crap that it's gonna work

469
00:32:05,685 --> 00:32:08,965
Speaker 1:  because it's gonna work because the, the volume will outplay the quality

470
00:32:09,385 --> 00:32:12,525
Speaker 1:  and we're gonna keep making money off of it. And like, again, this is the,

471
00:32:12,525 --> 00:32:16,125
Speaker 1:  that was the SEO play was we're just, we are going to ruthlessly optimize

472
00:32:16,145 --> 00:32:19,725
Speaker 1:  our junk so that even if you click it by accident, it doesn't matter, we're

473
00:32:19,725 --> 00:32:22,885
Speaker 1:  still gonna get paid. And you worry that, like

474
00:32:23,865 --> 00:32:26,685
Speaker 1:  we as viewers who our main thing to do is scroll.

475
00:32:28,145 --> 00:32:30,805
Speaker 1:  If if you hit the video, they're gonna make the money. Yeah. And so it's

476
00:32:30,805 --> 00:32:34,685
Speaker 1:  like my incentive is actually to make more of this stuff if all I care

477
00:32:34,685 --> 00:32:38,485
Speaker 1:  about is just getting views even even five at a time.

478
00:32:38,595 --> 00:32:42,285
Speaker 1:  Yeah. And so I, And I wonder, And I think, I do think these platforms know,

479
00:32:42,285 --> 00:32:46,125
Speaker 1:  And I think YouTube knows better than most that if it lets itself be completely

480
00:32:46,195 --> 00:32:49,925
Speaker 1:  overrun by this lop it, it will be a long-term bad decision.

481
00:32:49,925 --> 00:32:53,605
Speaker 1:  Totally. And like if your platform's offerings are the NFL and

482
00:32:53,745 --> 00:32:57,565
Speaker 1:  AI slop, you're in trouble. And, but I, I think every

483
00:32:57,725 --> 00:33:00,205
Speaker 1:  creator is gonna have to figure this out for themselves. And these platforms

484
00:33:00,205 --> 00:33:03,605
Speaker 1:  are gonna have to figure out like what is the line between useful thing we

485
00:33:03,605 --> 00:33:06,925
Speaker 1:  can do to help creator and thing that in the long run will actually destroy

486
00:33:06,985 --> 00:33:08,525
Speaker 1:  all of the quality stuff on our platform.

487
00:33:08,715 --> 00:33:12,005
Speaker 9:  Yeah, it's a great question. It's also just like we,

488
00:33:13,145 --> 00:33:17,125
Speaker 9:  we can't go operating at like where the goal is

489
00:33:17,705 --> 00:33:21,605
Speaker 9:  as much content as possible because it is Right. That's not

490
00:33:21,605 --> 00:33:24,885
Speaker 9:  good. There aren't enough people to watch it,

491
00:33:25,245 --> 00:33:26,045
Speaker 9:  frankly. Yeah.

492
00:33:26,045 --> 00:33:29,285
Speaker 1:  Like YouTube's, YouTube's good. Like at some point YouTube should just be

493
00:33:29,285 --> 00:33:32,445
Speaker 1:  like, actually guys, everybody just like take a month off. We actually don't

494
00:33:32,445 --> 00:33:35,605
Speaker 1:  need any more stuff. We're just gonna keep researching all the stuff people

495
00:33:35,605 --> 00:33:39,565
Speaker 1:  haven't watched. There's 50 lifetimes of things everybody likes already available

496
00:33:39,585 --> 00:33:42,045
Speaker 1:  to them. Y'all go take a vacation.

497
00:33:42,755 --> 00:33:44,405
Speaker 9:  Yeah. Yeah. I think nothing

498
00:33:44,405 --> 00:33:44,925
Speaker 1:  Would change.

499
00:33:45,565 --> 00:33:49,485
Speaker 9:  I don't think I I completely agree. I completely agree. Especially now

500
00:33:49,485 --> 00:33:52,765
Speaker 9:  with this livestream stuff. I'm like, oh my God, who's watching this? I know

501
00:33:52,785 --> 00:33:53,885
Speaker 9:  who can possibly watch this?

502
00:33:54,595 --> 00:33:58,525
Speaker 1:  Seriously? But the answer is both of us watching Get Ready With Me

503
00:33:58,525 --> 00:34:01,245
Speaker 1:  Streams, which you've now talked me into and is going to be this is, this

504
00:34:01,245 --> 00:34:02,005
Speaker 1:  is gonna be my new thing.

505
00:34:02,005 --> 00:34:04,165
Speaker 9:  They're oddly satisfying. I'll say,

506
00:34:04,765 --> 00:34:07,205
Speaker 1:  I would like to get ready with somebody. Yeah. This is, this would be great.

507
00:34:07,235 --> 00:34:11,045
Speaker 1:  Yeah. My process takes like four minutes and involves a t-shirt, so

508
00:34:11,045 --> 00:34:12,805
Speaker 1:  I'm gonna need to improve my Get Ready with

509
00:34:12,805 --> 00:34:14,325
Speaker 9:  Me. That would hit, it would hit on YouTube. I'm sure.

510
00:34:14,955 --> 00:34:16,125
Speaker 1:  Yeah. I'm gonna have the shortest put

511
00:34:16,125 --> 00:34:17,885
Speaker 9:  It on shorts in history on

512
00:34:18,755 --> 00:34:22,085
Speaker 1:  Love this for us. All right, Mia, thank you as always. It's good to have

513
00:34:22,085 --> 00:34:24,885
Speaker 1:  you here. Let's, anytime we should, let's drag some creators on here over

514
00:34:24,885 --> 00:34:26,765
Speaker 1:  the next few months and just talk through all this stuff. This will be fun.

515
00:34:26,765 --> 00:34:27,285
Speaker 1:  Love it. Yeah.

516
00:34:27,375 --> 00:34:27,725
Speaker 9:  All's

517
00:34:27,725 --> 00:34:30,165
Speaker 1:  Do it. Until then, we're gonna take a break and then we're gonna come back

518
00:34:30,385 --> 00:34:33,005
Speaker 1:  And I have some hot takes that are gonna get me fired. We'll be right back.

519
00:37:40,115 --> 00:37:43,925
Speaker 1:  into a microphone and subjects other people to them live on The Vergecast.

520
00:37:44,395 --> 00:37:48,285
Speaker 1:  Back with me to do this. Jake Cast. Hi Jake. Hey, good

521
00:37:48,285 --> 00:37:51,045
Speaker 1:  to be here. Hayden Field, welcome back. So

522
00:37:51,075 --> 00:37:52,005
Speaker 11:  Psyched. Thanks.

523
00:37:52,345 --> 00:37:56,285
Speaker 1:  All right, so last week I, I threw a bunch of AI related takes

524
00:37:56,305 --> 00:38:00,285
Speaker 1:  at both of you and in, in the spirit of our favorite show,

525
00:38:00,285 --> 00:38:03,645
Speaker 1:  subway Takes your, your allowances were to either

526
00:38:03,745 --> 00:38:07,645
Speaker 1:  100% agree or 100% disagree, or I

527
00:38:07,645 --> 00:38:11,485
Speaker 1:  gave you the option to just bail because David is lighting his career on

528
00:38:11,485 --> 00:38:15,405
Speaker 1:  fire And I want no part of this. Neither of you picked that either for,

529
00:38:15,425 --> 00:38:19,125
Speaker 1:  for any of them last week. I would say a lot of people have had a lot of

530
00:38:19,125 --> 00:38:22,885
Speaker 1:  feelings about my feelings And I'm okay with that And I have more of them

531
00:38:23,235 --> 00:38:26,085
Speaker 1:  this week. This is the last one of these we're gonna do, and this is just,

532
00:38:27,165 --> 00:38:30,725
Speaker 1:  I would say sort of unordered thoughts. I had strong opinions about over

533
00:38:30,725 --> 00:38:34,125
Speaker 1:  the course of a summer, spent on my couch reading news,

534
00:38:34,555 --> 00:38:37,845
Speaker 1:  talking to newborns and hanging out. So I have, I have seven

535
00:38:38,255 --> 00:38:41,565
Speaker 1:  completely unrelated and Unordered

536
00:38:41,945 --> 00:38:44,805
Speaker 1:  summer takes for the two of you. Are you ready? If,

537
00:38:44,985 --> 00:38:48,165
Speaker 12:  If we're wrapping up, I have to assume it's because you get canceled at

538
00:38:48,165 --> 00:38:48,445
Speaker 12:  the end of

539
00:38:48,445 --> 00:38:51,925
Speaker 1:  This. Yeah, if we, if suddenly we only get through five of them,

540
00:38:52,325 --> 00:38:56,245
Speaker 1:  you'll know that two of them were, I just immediately David

541
00:38:56,305 --> 00:38:59,045
Speaker 1:  is removed from The Verge and that's the end of everything. Yeah.

542
00:38:59,265 --> 00:39:03,245
Speaker 12:  You spent a long time talking to, to Annie on, on GR this summer, didn't

543
00:39:03,245 --> 00:39:03,365
Speaker 12:  you?

544
00:39:04,545 --> 00:39:07,485
Speaker 1:  Listen, you spend a lot of time on YouTube shorts and things just start to

545
00:39:07,485 --> 00:39:11,165
Speaker 1:  get weird. Oh yeah. Okay. Summer take

546
00:39:11,185 --> 00:39:15,125
Speaker 1:  number one is that podcasts should be listened to at one

547
00:39:15,245 --> 00:39:18,875
Speaker 1:  x and all other fast speeds are wrong.

548
00:39:19,295 --> 00:39:23,155
Speaker 1:  If you don't have time to listen to the podcast, listen to fewer podcasts,

549
00:39:23,225 --> 00:39:26,845
Speaker 1:  it's fine. One x is the correct speed at which you should listen to a podcast.

550
00:39:27,885 --> 00:39:29,145
Speaker 1:  All other speeds are insane.

551
00:39:29,775 --> 00:39:33,385
Speaker 13:  I've gotta say 100% disagree for that one.

552
00:39:34,345 --> 00:39:38,225
Speaker 13:  I have a life, I have a brain that goes

553
00:39:38,375 --> 00:39:41,705
Speaker 13:  fast actually, I feel like, I don't know if my brain goes fast all the time,

554
00:39:41,705 --> 00:39:44,385
Speaker 13:  but you know what, when I'm listening to people talk, it does, I'm a visual

555
00:39:44,385 --> 00:39:48,065
Speaker 13:  learner, I can read really fast, but when I'm listening to people talk in

556
00:39:48,065 --> 00:39:51,825
Speaker 13:  my ear sometimes they're getting, taking a while to get to the point. You

557
00:39:51,825 --> 00:39:55,265
Speaker 13:  know, so I gotta listen to 1.5 x two X,

558
00:39:55,895 --> 00:39:59,545
Speaker 13:  same with tiktoks. You know, I feel that I,

559
00:39:59,695 --> 00:40:02,465
Speaker 13:  many people will agree with me about this and that,

560
00:40:03,355 --> 00:40:06,145
Speaker 13:  David, you could have such like a

561
00:40:07,455 --> 00:40:11,105
Speaker 13:  awakening if you could just, you know, be done with the podcast

562
00:40:11,405 --> 00:40:15,345
Speaker 13:  in two ti like 0.5, the amount of time as you would

563
00:40:15,345 --> 00:40:15,785
Speaker 13:  have before,

564
00:40:16,165 --> 00:40:19,705
Speaker 1:  But just don't listen to the pod. It's so easy to not listen to a podcast

565
00:40:19,765 --> 00:40:23,385
Speaker 1:  you're not interested in. And I realize I'm saying this to a large audience

566
00:40:23,385 --> 00:40:27,025
Speaker 1:  of people who I assume listen to this podcast at faster than one x and me

567
00:40:27,025 --> 00:40:29,385
Speaker 1:  being like, well, if you have to listen to it too fast, just don't listen

568
00:40:29,385 --> 00:40:33,145
Speaker 1:  to it is the most like self owning thing I could possibly do. So

569
00:40:33,145 --> 00:40:36,945
Speaker 1:  please know that I understand what I'm doing here. Just watch

570
00:40:36,965 --> 00:40:40,665
Speaker 1:  and listen to less stuff. Like I, I I have come to a point where it's like,

571
00:40:40,665 --> 00:40:44,545
Speaker 1:  to me listening to a podcast of two x is like looking at the

572
00:40:44,645 --> 00:40:48,175
Speaker 1:  AI generated summary of a book and saying, you've read the book, you haven't,

573
00:40:48,735 --> 00:40:52,415
Speaker 13:  I do have a a take here that is a follow up, okay for me,

574
00:40:52,965 --> 00:40:56,495
Speaker 13:  it's not about that I wanna listen to more content. I'm not like, oh, let

575
00:40:56,495 --> 00:41:00,255
Speaker 13:  me finish this podcast or this video so I can like see the next thing for

576
00:41:00,255 --> 00:41:03,935
Speaker 13:  me. It's really, I think just the fact that I was raised in the Gilmore

577
00:41:03,935 --> 00:41:07,215
Speaker 13:  Girls era of people fast talking on tv.

578
00:41:07,795 --> 00:41:11,215
Speaker 13:  So I'm used to people talking quicker than they do on most

579
00:41:11,335 --> 00:41:15,215
Speaker 13:  podcasts. And I'm like, I gotta amp this up to the Gilmore Girls speed.

580
00:41:15,215 --> 00:41:17,775
Speaker 13:  That's what I was raised on. That's what I'm used to. You know,

581
00:41:18,115 --> 00:41:21,295
Speaker 1:  That's a good take. My my wife is currently watching Gilmore Girls for like

582
00:41:21,295 --> 00:41:23,975
Speaker 1:  the 55th time, which means I'm, because it's fun, of course currently watching

583
00:41:23,975 --> 00:41:27,895
Speaker 1:  Gilmore Girls flex the 55th time that show rips and they do, they

584
00:41:27,895 --> 00:41:31,875
Speaker 1:  talk So fast. That's okay. That's a decent take. Jake, you've been

585
00:41:31,875 --> 00:41:32,955
Speaker 1:  suspiciously quiet here.

586
00:41:33,555 --> 00:41:37,325
Speaker 12:  Yeah, you know, the, The Vergecast is a work of art And I think

587
00:41:37,555 --> 00:41:41,325
Speaker 12:  like you wouldn't watch Oppenheimer on two x, you should not

588
00:41:41,385 --> 00:41:45,045
Speaker 12:  listen to The Vergecast on two x every other podcast

589
00:41:45,105 --> 00:41:48,605
Speaker 12:  though. Do it. Do what you please. I, I I strong disagree with you, David.

590
00:41:50,345 --> 00:41:54,285
Speaker 12:  Listen, sometimes I just, I just wanna get that info. I

591
00:41:54,285 --> 00:41:57,805
Speaker 12:  wanna get in and out. I'm not, I'm not here for the, the artistry for the

592
00:41:57,805 --> 00:42:01,165
Speaker 12:  subtle p with, with deep, deep apologies

593
00:42:01,905 --> 00:42:05,565
Speaker 12:  to our audio editing team who, who are are, are truly

594
00:42:05,565 --> 00:42:06,525
Speaker 12:  masterful. Okay, wait,

595
00:42:06,525 --> 00:42:10,205
Speaker 1:  So here, here's my follow up question for both of you. Is, are

596
00:42:10,365 --> 00:42:14,205
Speaker 1:  you, do you have a setting like universally

597
00:42:14,405 --> 00:42:18,325
Speaker 1:  I listen to all of my shows at 1.5 X, do you have like a setting per

598
00:42:18,325 --> 00:42:21,405
Speaker 1:  show or is it a thing you like dial up and down as you're listening?

599
00:42:21,585 --> 00:42:23,005
Speaker 13:  For me as I'm listening.

600
00:42:23,435 --> 00:42:23,725
Speaker 1:  Okay.

601
00:42:23,825 --> 00:42:24,765
Speaker 12:  Oh, interesting. So

602
00:42:24,765 --> 00:42:27,565
Speaker 1:  You'll start an, an episode of a show and be like, we gotta, we gotta keep

603
00:42:27,565 --> 00:42:29,245
Speaker 1:  moving here and just crank it to two x.

604
00:42:29,865 --> 00:42:32,885
Speaker 13:  It really depends on how slow the person is talking. You'd be so surprised

605
00:42:32,885 --> 00:42:36,805
Speaker 13:  at how slow some people talk, you know, or it's, it's slower than

606
00:42:36,825 --> 00:42:40,725
Speaker 13:  the my brain. Like I feel like I have to calibrate it with the

607
00:42:40,735 --> 00:42:43,125
Speaker 13:  speed at which I think I'm like, if you're talking at the speed at which

608
00:42:43,125 --> 00:42:46,845
Speaker 13:  I think great if you're talking like three times slower than I think I've

609
00:42:46,845 --> 00:42:47,645
Speaker 13:  gotta dial you up.

610
00:42:48,355 --> 00:42:49,965
Speaker 1:  Okay. Jake, what about you?

611
00:42:50,525 --> 00:42:54,205
Speaker 12:  I would say like most YouTube videos would be improved by a default.

612
00:42:54,305 --> 00:42:58,205
Speaker 12:  1.25 x like just a lit little bit more, little

613
00:42:58,205 --> 00:43:02,115
Speaker 12:  bit more TikTok. Oh yeah, two x on everything. Just like

614
00:43:02,205 --> 00:43:05,765
Speaker 12:  those things are, are going seriously, they made 'em, I, so I

615
00:43:06,045 --> 00:43:09,525
Speaker 12:  set a 15 minute timer for myself on TikTok, so I can't watch too much every

616
00:43:09,525 --> 00:43:13,045
Speaker 12:  day. And this way I can squeeze 30 minutes of TikTok in 15 minutes. Not

617
00:43:13,065 --> 00:43:13,285
Speaker 12:  bad.

618
00:43:13,515 --> 00:43:13,805
Speaker 1:  Okay,

619
00:43:14,185 --> 00:43:15,365
Speaker 12:  That's ter it's terrible.

620
00:43:15,985 --> 00:43:19,125
Speaker 1:  So what you've just said is, I don't wanna watch TikTok. So I've invented

621
00:43:19,125 --> 00:43:22,325
Speaker 1:  ways so I can watch more TikTok in the time. I shouldn't be watching TikTok

622
00:43:22,465 --> 00:43:22,685
Speaker 1:  and

623
00:43:22,685 --> 00:43:24,925
Speaker 12:  Have a, have a worst time watching it. Just

624
00:43:24,975 --> 00:43:26,525
Speaker 1:  Don't look at TikTok, Jake.

625
00:43:26,525 --> 00:43:30,365
Speaker 12:  Yeah, yeah. This is, ugh, I gotta know what's happening on TikTok. How

626
00:43:30,365 --> 00:43:32,485
Speaker 12:  will I know what Stanley water bottle to buy?

627
00:43:32,745 --> 00:43:36,085
Speaker 1:  But like the, the world would be a better place if everyone only listened

628
00:43:36,085 --> 00:43:39,325
Speaker 1:  to their podcast on one x is really kind of where I'm landing on this that

629
00:43:39,525 --> 00:43:40,365
Speaker 12:  I a hundred percent agree with.

630
00:43:41,485 --> 00:43:45,405
Speaker 13:  I mean, I will say that it is Zen to just

631
00:43:46,465 --> 00:43:49,285
Speaker 13:  not care how fast this person is speaking and really live in the moment.

632
00:43:49,295 --> 00:43:52,605
Speaker 13:  Maybe I need to meditate more and maybe my form of meditation will be

633
00:43:52,795 --> 00:43:56,645
Speaker 13:  listening to podcasts on one x and just really living in the moment and

634
00:43:56,645 --> 00:43:58,005
Speaker 13:  hanging on their every word. You know?

635
00:43:58,545 --> 00:44:02,525
Speaker 1:  Do you guys have podcasts you listen to? Not because you, you know,

636
00:44:02,525 --> 00:44:05,765
Speaker 1:  like the host or like the storytelling style or whatever, but for like Pure

637
00:44:05,765 --> 00:44:07,045
Speaker 1:  informational reasons.

638
00:44:07,785 --> 00:44:11,045
Speaker 13:  I'm a visual learner, so I feel like if I only want it for informational

639
00:44:11,045 --> 00:44:13,805
Speaker 13:  reasons, I've gotta read it or it kind of doesn't stick with me.

640
00:44:14,115 --> 00:44:17,085
Speaker 12:  Yeah. I'm fully with you Aidan. Yeah, audio is like,

641
00:44:18,015 --> 00:44:21,625
Speaker 12:  it's more for a, I don't know, there's like a hanging out vibe to it,

642
00:44:21,875 --> 00:44:24,865
Speaker 12:  right? But yeah, trying, trying to get like news that way

643
00:44:26,115 --> 00:44:28,495
Speaker 12:  it is not, not quite working for me. Okay.

644
00:44:28,955 --> 00:44:31,870
Speaker 1:  So that's the one kind of podcast that I'm like, yeah, listen to that as

645
00:44:31,870 --> 00:44:34,645
Speaker 1:  fast as you want. Right? It's, it's like, it's the audio version of like

646
00:44:34,645 --> 00:44:38,405
Speaker 1:  skimming the headlines just like three x your way through whatever

647
00:44:38,605 --> 00:44:42,165
Speaker 1:  headlines podcast you're listening to. But I think I've just only selected

648
00:44:42,185 --> 00:44:46,045
Speaker 1:  for podcasts that I sort of like hanging out with and putting

649
00:44:46,045 --> 00:44:49,925
Speaker 1:  those at two x feels weird. The problem is once you go up you

650
00:44:49,925 --> 00:44:52,325
Speaker 1:  can never go back down because then you're like, why are you all talking

651
00:44:52,345 --> 00:44:55,165
Speaker 1:  so slowly? What is like, why did you all do drugs right before you started

652
00:44:55,165 --> 00:44:58,245
Speaker 1:  this episode? And you're just like, no, this is how fast they normally talk.

653
00:44:58,245 --> 00:45:01,405
Speaker 1:  Like I've had people meet me in person and be like, you don't sound like

654
00:45:01,405 --> 00:45:04,245
Speaker 1:  I'm used to it. And then they realize in the course of our conversation that

655
00:45:04,515 --> 00:45:08,445
Speaker 1:  they only ever listen to the podcast at 1.5 or two x. So there used to

656
00:45:08,445 --> 00:45:09,965
Speaker 1:  me just talking like this all the time and they're like, why don't you talk

657
00:45:09,965 --> 00:45:12,565
Speaker 1:  like this all the time? It's insane. And I'm like, I can talk that fast.

658
00:45:12,595 --> 00:45:14,485
Speaker 1:  Like we, we can do this. I can be this person.

659
00:45:14,575 --> 00:45:16,445
Speaker 13:  Especially now that you're watching Gilmore Girls.

660
00:45:17,065 --> 00:45:20,445
Speaker 1:  Oh my god. I mean it's, it's outta control. Also a very strange thing has

661
00:45:20,605 --> 00:45:23,445
Speaker 1:  happened to me now that I have children as I identify with very different

662
00:45:23,445 --> 00:45:27,405
Speaker 1:  people in those shows and now I'm like les's parents. Were basically right

663
00:45:27,405 --> 00:45:31,165
Speaker 1:  about everything and, and Lorelei is kind of a menace, but that's

664
00:45:31,425 --> 00:45:34,805
Speaker 1:  for Hayden when we record our Gilmore Girls podcast. We'll we'll come back

665
00:45:34,805 --> 00:45:35,045
Speaker 1:  to that.

666
00:45:35,325 --> 00:45:36,365
Speaker 13:  Perfect. Can't wait.

667
00:45:36,625 --> 00:45:39,765
Speaker 1:  My second take is that Threads is good actually

668
00:45:40,345 --> 00:45:41,245
Speaker 1:  that's it. That's the whole

669
00:45:41,245 --> 00:45:44,765
Speaker 12:  Take. I'm gonna agree with you. I think Threads gets way too much hate.

670
00:45:46,025 --> 00:45:50,005
Speaker 12:  And you know what, I'm gonna go even further. The algorithmic feed. It's

671
00:45:50,005 --> 00:45:50,325
Speaker 12:  all right.

672
00:45:51,285 --> 00:45:51,685
Speaker 1:  I agree.

673
00:45:52,555 --> 00:45:56,365
Speaker 12:  Like I, I feel like I enjoy threads more

674
00:45:56,395 --> 00:46:00,005
Speaker 12:  than X or Blue Sky, sorry,

675
00:46:00,005 --> 00:46:01,405
Speaker 12:  Mastodon, that's not even on my radar.

676
00:46:02,805 --> 00:46:06,725
Speaker 1:  Mastodon is, I'm sorry, David, full of good ideas and not full of interesting

677
00:46:06,725 --> 00:46:07,965
Speaker 1:  people doing interesting things. Thread

678
00:46:07,985 --> 00:46:11,325
Speaker 12:  Is is pleasant. I feel like people on threads, or at least the algorithm

679
00:46:11,385 --> 00:46:15,045
Speaker 12:  on threads keeps things pleasant and

680
00:46:15,115 --> 00:46:18,885
Speaker 12:  down the line. And I feel like on other social networks, people are

681
00:46:18,885 --> 00:46:22,805
Speaker 12:  very raw and passionate and intense And I

682
00:46:22,805 --> 00:46:26,125
Speaker 12:  don't need that all the time. Sometimes I just want a laid back take about

683
00:46:26,225 --> 00:46:28,845
Speaker 12:  the new iPhone and like that's what I get on Threads.

684
00:46:29,265 --> 00:46:29,685
Speaker 1:  Hayden,

685
00:46:30,205 --> 00:46:33,965
Speaker 13:  I feel like I can't speak to this with actual knowledge

686
00:46:33,965 --> 00:46:37,605
Speaker 13:  because for threads, like I get on and then I leave. Like I post my story

687
00:46:37,825 --> 00:46:41,645
Speaker 13:  and then I log off. So I honestly haven't spent that much time

688
00:46:41,795 --> 00:46:45,605
Speaker 13:  like scrolling on threads because I have so many social media platforms

689
00:46:45,605 --> 00:46:49,445
Speaker 13:  that I have to be on now that I just Threads is the one that I like.

690
00:46:49,505 --> 00:46:53,205
Speaker 13:  One had to fall and it's threads for me. It's like I go on, I post my stories,

691
00:46:53,365 --> 00:46:54,325
Speaker 13:  I leave, I don't scroll.

692
00:46:54,335 --> 00:46:55,045
Speaker 1:  Where do you scroll?

693
00:46:55,475 --> 00:46:59,125
Speaker 13:  Blue Sky and Twitter. But yeah, threads

694
00:46:59,325 --> 00:47:03,245
Speaker 13:  and LinkedIn for me both. I'm like, I go on, I post my thing And

695
00:47:03,285 --> 00:47:06,845
Speaker 13:  I leave. I don't have any understanding of like

696
00:47:06,985 --> 00:47:10,045
Speaker 13:  what's going on on there. Instagram, I scroll like personally

697
00:47:10,915 --> 00:47:14,405
Speaker 13:  Blue Sky and Twitter XI scroll like for work.

698
00:47:14,905 --> 00:47:18,805
Speaker 13:  And then the other two I just kind of let fall by the wayside and I'm there

699
00:47:18,805 --> 00:47:20,405
Speaker 13:  technically, but not in actuality.

700
00:47:20,905 --> 00:47:24,685
Speaker 1:  So to me, my experience this summer over and over was that

701
00:47:25,595 --> 00:47:29,485
Speaker 1:  Twitter has essentially like lost its mind. It's it's, it's a 10 out of 10

702
00:47:29,485 --> 00:47:32,485
Speaker 1:  like right wing cesspool all the time.

703
00:47:34,595 --> 00:47:37,685
Speaker 1:  Blue Sky is like the same, but on the opposite end of the political spectrum.

704
00:47:37,715 --> 00:47:40,845
Speaker 1:  Like I think there, there's a lot of fun to be had on Blue Sky, but I think

705
00:47:40,945 --> 00:47:43,925
Speaker 1:  the thing where the culture has turned into a lot of people scolding each

706
00:47:43,925 --> 00:47:47,845
Speaker 1:  other is real and true. It's

707
00:47:47,925 --> 00:47:51,845
Speaker 1:  at least more full of people saying things that are real than Twitter is

708
00:47:51,845 --> 00:47:55,325
Speaker 1:  at least on my feed. But like, I just don't find either one to be a particularly

709
00:47:55,355 --> 00:47:59,045
Speaker 1:  good time and threads. I think Jake, I have the exact same experience that

710
00:47:59,045 --> 00:48:02,565
Speaker 1:  you do too, which is like the, the temperature on threads is not as high.

711
00:48:03,065 --> 00:48:06,845
Speaker 1:  And I think a lot of people in our business take that to mean, oh,

712
00:48:06,845 --> 00:48:10,125
Speaker 1:  it's not real time, it's not lively, it's not exciting, it's not what's happening,

713
00:48:10,125 --> 00:48:13,405
Speaker 1:  it's not the news. And to me I'm like, oh, I actually, when I come to this,

714
00:48:13,485 --> 00:48:17,205
Speaker 1:  I don't want the news all the time. Yeah. It's just nice to have some people

715
00:48:17,205 --> 00:48:20,485
Speaker 1:  posting about some things. And I think Threads has gotten better at being

716
00:48:20,665 --> 00:48:24,325
Speaker 1:  timely. Like I'm seeing many fewer yesterday and four days ago

717
00:48:24,655 --> 00:48:27,645
Speaker 1:  posts on my threads timeline every time I open the app. So they're like,

718
00:48:27,645 --> 00:48:31,325
Speaker 1:  they're getting better with that algorithm. And it just like I, I have a

719
00:48:31,325 --> 00:48:35,165
Speaker 1:  nice time on threads in a way that I no longer have a nice time on Blue Sky

720
00:48:35,225 --> 00:48:38,685
Speaker 1:  or on Twitter. And I feel like that is, I just, I hit this point where I

721
00:48:38,685 --> 00:48:41,365
Speaker 1:  was like, why am I subjecting myself to opening Twitter five times a day?

722
00:48:41,365 --> 00:48:45,165
Speaker 1:  Like this is an insane waste of time because I feel some like

723
00:48:45,165 --> 00:48:48,765
Speaker 1:  journalistic obligation to know what these Luna Hicks are saying to each

724
00:48:48,765 --> 00:48:51,805
Speaker 1:  other. And it's just, I don't know, like I think

725
00:48:52,885 --> 00:48:56,605
Speaker 1:  ultimately maybe the sort of boring middle is actually the right place

726
00:48:56,945 --> 00:48:58,565
Speaker 1:  for social media to land. Can

727
00:48:58,565 --> 00:49:02,485
Speaker 12:  I tell you, threads is actually the only other app on my phone that I

728
00:49:02,485 --> 00:49:05,965
Speaker 12:  have a timer on and it's because wow, it is like, it is pleasant

729
00:49:06,385 --> 00:49:09,805
Speaker 12:  and because it's algorithmic by default and it's like a decent algorithm,

730
00:49:10,225 --> 00:49:13,405
Speaker 12:  you can keep going forever. And I do not wanna keep going forever. So yeah,

731
00:49:13,485 --> 00:49:17,445
Speaker 12:  I, I think thread's like totally pleasant. It is not, it's true. It's not

732
00:49:17,445 --> 00:49:19,365
Speaker 12:  as spicy and that's kind of nice.

733
00:49:19,995 --> 00:49:23,085
Speaker 13:  Okay. Maybe I'll spend some more time on threads. You've convinced me,

734
00:49:23,745 --> 00:49:27,325
Speaker 1:  It took me a while because it's actually like, you have to realize that being

735
00:49:27,565 --> 00:49:30,365
Speaker 1:  slightly bored by it is actually a good thing and not a bad thing. Where

736
00:49:30,365 --> 00:49:33,885
Speaker 1:  it's like, I don't open the app And immediately have like a fight or flight

737
00:49:34,165 --> 00:49:38,005
Speaker 1:  response is, is probably a healthy thing for how we deal with our

738
00:49:38,005 --> 00:49:40,925
Speaker 1:  lives on the internet, but feels very different than I think what we're used

739
00:49:40,925 --> 00:49:44,005
Speaker 1:  to. Which brings me to my third take,

740
00:49:44,735 --> 00:49:48,245
Speaker 1:  which is that it's actually pretty easy to quit social media And I think

741
00:49:48,245 --> 00:49:49,565
Speaker 1:  many more people should do it.

742
00:49:51,985 --> 00:49:55,865
Speaker 1:  I, I deleted Instagram and TikTok from my phone

743
00:49:56,055 --> 00:49:59,825
Speaker 1:  basically right as I had a kid And I have actually had zero impulse to put

744
00:49:59,825 --> 00:50:03,745
Speaker 1:  either one of them back. The thing Instagram does where if you click a link,

745
00:50:03,765 --> 00:50:07,505
Speaker 1:  it won't show it to you until you open the app is really annoying. Oh yeah.

746
00:50:07,505 --> 00:50:11,425
Speaker 1:  But has not actually caused me all that much trouble. TikTok

747
00:50:11,425 --> 00:50:13,505
Speaker 1:  is fine. Somebody sends you a link, it opens in the webpage, you, you watch

748
00:50:13,505 --> 00:50:14,865
Speaker 1:  the TikTok, you move on through day.

749
00:50:16,505 --> 00:50:20,305
Speaker 1:  I, I have essentially stopped spending

750
00:50:20,305 --> 00:50:24,265
Speaker 1:  time on Blue Sky and Twitter. And even Threads is like, it's there because

751
00:50:24,345 --> 00:50:28,325
Speaker 1:  I feel a sort of obligation to be around. But like if, if you're,

752
00:50:29,395 --> 00:50:33,365
Speaker 1:  most people are like in a place where they should spend less time on

753
00:50:33,365 --> 00:50:36,405
Speaker 1:  social media and it's actually not that hard to do, and maybe we should just

754
00:50:36,405 --> 00:50:40,165
Speaker 1:  all quit social forever right now. Let's do it. Agree or disagree.

755
00:50:40,645 --> 00:50:44,045
Speaker 13:  I feel like you're right in that, of course we'd all be so much happier

756
00:50:44,305 --> 00:50:47,525
Speaker 13:  if we didn't have social media. But

757
00:50:48,685 --> 00:50:52,565
Speaker 13:  I will say, I think there are ways to use it

758
00:50:52,565 --> 00:50:56,285
Speaker 13:  in ways that are a little less harmful. So one Jake's timer

759
00:50:56,315 --> 00:50:59,525
Speaker 13:  idea. Amazing. Yep. Two, something that I do

760
00:51:00,475 --> 00:51:04,325
Speaker 13:  that I has always like given me a lot of peace is

761
00:51:04,325 --> 00:51:08,005
Speaker 13:  that I don't, I mean, let's use Instagram as an example. I don't look at

762
00:51:08,005 --> 00:51:11,565
Speaker 13:  who likes my stuff And I don't look at who watches

763
00:51:12,025 --> 00:51:15,445
Speaker 13:  my stuff so that I feel like I'm always just shouting it to the void in

764
00:51:15,445 --> 00:51:18,725
Speaker 13:  a good way. Like, I am not aware of myself being perceived.

765
00:51:19,345 --> 00:51:23,205
Speaker 13:  I'm not like, oh my God, this person from high school saw my

766
00:51:23,895 --> 00:51:26,685
Speaker 13:  story of this. I wonder what they thought. Let's think about this through

767
00:51:26,685 --> 00:51:30,445
Speaker 13:  their eyes. It's like, I don't know. So I don't care. I'm like

768
00:51:30,445 --> 00:51:34,045
Speaker 13:  blissfully ignorant. I just feel like I'm posting, I, this is how I feel.

769
00:51:34,205 --> 00:51:37,565
Speaker 13:  I think I would use Instagram the exact same way. If I had no followers,

770
00:51:37,755 --> 00:51:40,805
Speaker 13:  like if I was just posting for myself, I think I would do the exact same

771
00:51:40,805 --> 00:51:44,605
Speaker 13:  thing. And it's because I don't, since I don't see who's looking at

772
00:51:44,785 --> 00:51:48,445
Speaker 13:  my stuff, I don't like, you know, really I'm not,

773
00:51:48,625 --> 00:51:52,485
Speaker 13:  I'm blissfully ignorant of anyone looking at my stuff. However, I do like

774
00:51:52,485 --> 00:51:56,205
Speaker 13:  to use it a lot as a messaging platform. Like a lot of times maybe it's

775
00:51:56,205 --> 00:51:59,685
Speaker 13:  just like, you know, something about

776
00:52:00,145 --> 00:52:03,765
Speaker 13:  people today, but it feels a little too intimate sometimes to ask for

777
00:52:03,765 --> 00:52:06,405
Speaker 13:  someone's like number. So you ask for their Instagram, then you message

778
00:52:06,425 --> 00:52:09,445
Speaker 13:  on Instagram. So I love using it as like a messaging app. Same with Twitter.

779
00:52:09,475 --> 00:52:13,125
Speaker 13:  Like I can message someone on there and then we can move it to another platform

780
00:52:13,125 --> 00:52:16,965
Speaker 13:  after. But I think all of these tools, their biggest values in their like

781
00:52:16,965 --> 00:52:20,765
Speaker 13:  direct messaging features and then scrolling and infinite scroll

782
00:52:20,905 --> 00:52:24,885
Speaker 13:  is like the thing that basically causes you internal turmoil. And

783
00:52:24,885 --> 00:52:28,605
Speaker 13:  you have to find a way to, you know, address that. But I don't know, I mean,

784
00:52:28,625 --> 00:52:31,685
Speaker 13:  I'm like, of course we'd all be happier if they, if we didn't use them as

785
00:52:31,685 --> 00:52:35,005
Speaker 13:  much. But I also think there's some value in using it in a way that's like

786
00:52:35,485 --> 00:52:37,485
Speaker 13:  measured if you can do that, which is hard.

787
00:52:38,145 --> 00:52:41,565
Speaker 1:  So I, I agree. But I think even, even the premise of all of this is just

788
00:52:41,565 --> 00:52:45,125
Speaker 1:  like, this is an unavoidable thing that we have to

789
00:52:45,505 --> 00:52:48,805
Speaker 1:  manage the best way that we can. And I'm, I'm increasingly of the mind that

790
00:52:49,045 --> 00:52:52,935
Speaker 1:  actually it's really easy to quit. Like I was shocked at how simple it was

791
00:52:53,275 --> 00:52:57,175
Speaker 1:  to just not be on Instagram anymore and like, do

792
00:52:57,175 --> 00:53:00,495
Speaker 1:  I occasionally feel like I'm missing something? Like honestly, no.

793
00:53:00,885 --> 00:53:04,415
Speaker 1:  Like, I've probably missed some dms, but

794
00:53:04,835 --> 00:53:07,855
Speaker 1:  that's fine. I think we do have a real like

795
00:53:08,645 --> 00:53:12,255
Speaker 1:  messaging problem across all of these different

796
00:53:12,575 --> 00:53:16,095
Speaker 1:  platforms. I've been using this at Beeper that we've talked about a lot on

797
00:53:16,095 --> 00:53:19,535
Speaker 1:  this show to try and like manage all of the dms in one place and that like

798
00:53:19,875 --> 00:53:23,815
Speaker 1:  almost kind of works. But in general it's just like, I don't know.

799
00:53:23,895 --> 00:53:27,205
Speaker 1:  I, the thing that really shocked me was how, how easy it was to get rid of

800
00:53:27,225 --> 00:53:31,045
Speaker 1:  and how few times I have missed TikTok and Instagram and

801
00:53:31,345 --> 00:53:34,725
Speaker 1:  how rare it has been to like, want to be on Blue Sky this summer.

802
00:53:35,245 --> 00:53:38,805
Speaker 13:  I did once quit social media for like a month and it was very easy. Like

803
00:53:38,805 --> 00:53:42,165
Speaker 13:  it was weirdly easy. And I think it's just because yeah, I reverted back

804
00:53:42,165 --> 00:53:45,245
Speaker 13:  to like my old life of just living in the moment.

805
00:53:45,805 --> 00:53:49,445
Speaker 1:  Yeah. What a crazy idea. Jake, what about you? You've just been scrolling

806
00:53:49,585 --> 00:53:51,285
Speaker 1:  TikTok this whole time. You're not even, you're not even

807
00:53:51,285 --> 00:53:53,405
Speaker 12:  Listening to TikTok. Oh, sorry. Were you guys still still going?

808
00:53:55,145 --> 00:53:58,885
Speaker 12:  You, you weren't going fast enough. This is the problem. I agree with you

809
00:53:59,135 --> 00:54:01,205
Speaker 12:  David, but I kind of feel bad about it.

810
00:54:03,265 --> 00:54:06,845
Speaker 12:  You know, I have never been very good about posting

811
00:54:07,185 --> 00:54:10,285
Speaker 12:  to social media and there's a lot of,

812
00:54:11,885 --> 00:54:15,405
Speaker 12:  I would say like acquaintances I follow, right? Like old friends from college,

813
00:54:15,545 --> 00:54:19,325
Speaker 12:  people who, you know, we used to work with who

814
00:54:19,805 --> 00:54:23,605
Speaker 12:  I, you know, I'm not gonna stay up to date with them day to day, week

815
00:54:23,605 --> 00:54:27,365
Speaker 12:  to week. But by following along with their posts, I'm able to get some sense

816
00:54:27,365 --> 00:54:29,645
Speaker 12:  of what's going on in their life. And there's this really weird thing that

817
00:54:29,645 --> 00:54:32,685
Speaker 12:  happens where I'm like, oh, I know what's going on with you. I actually

818
00:54:32,685 --> 00:54:36,365
Speaker 12:  still feel like somewhat close to you. You have no idea what's happening

819
00:54:36,365 --> 00:54:40,125
Speaker 12:  with me. You like, and you will not because I'm not putting

820
00:54:40,365 --> 00:54:43,925
Speaker 12:  anything back out there for you. And so there is this sort of like interesting

821
00:54:45,755 --> 00:54:49,695
Speaker 12:  way that you can use this to maintain a friendship. Even if it's like,

822
00:54:49,725 --> 00:54:53,615
Speaker 12:  even if I'm not really doing more than like liking a post, I think

823
00:54:53,615 --> 00:54:57,335
Speaker 12:  that's nice. Like it lets you keep up with people. I just, I think all the

824
00:54:57,335 --> 00:54:59,855
Speaker 12:  things that hate Hayden talked about where it's like, it shows you, everyone

825
00:54:59,855 --> 00:55:03,255
Speaker 12:  who views it shows you everyone who liked it. It's like, I can't not look

826
00:55:03,255 --> 00:55:06,575
Speaker 12:  at those things. And whenever I post an Instagram story, I'm like,

827
00:55:07,165 --> 00:55:10,415
Speaker 12:  like, did I post this at the wrong time? Is this, is this inconvenient for

828
00:55:10,415 --> 00:55:14,375
Speaker 12:  everybody? Nobody's seeing it. And I am like, I, I don't feel good about,

829
00:55:14,445 --> 00:55:17,855
Speaker 12:  like, I don't, I don't feel good. Right? Like when I, when I watch other

830
00:55:17,855 --> 00:55:20,895
Speaker 12:  people's stories, I'm like, oh, this is so nice. They're doing things, I'm

831
00:55:20,895 --> 00:55:24,585
Speaker 12:  so happy for them. And I'm just like, I feel stress

832
00:55:24,695 --> 00:55:28,545
Speaker 12:  when I'm posting. Yeah. So I would love for there to be a version of social

833
00:55:28,545 --> 00:55:31,945
Speaker 12:  media that, that I enjoy. I feel bad And I think like

834
00:55:32,365 --> 00:55:36,225
Speaker 12:  not engaging on social media is maybe sort of like a ne negative

835
00:55:36,725 --> 00:55:39,625
Speaker 12:  for like, I don't know, friends society.

836
00:55:40,455 --> 00:55:40,745
Speaker 1:  Yeah.

837
00:55:40,965 --> 00:55:44,905
Speaker 12:  But but also like, is it easy to tap out and is it like maybe healthy

838
00:55:45,005 --> 00:55:48,705
Speaker 12:  for a lot of people to tap out? Like yeah, you like the people

839
00:55:48,845 --> 00:55:52,145
Speaker 12:  who I'm gonna hang out with. I'm texting with them anyway. It's fine.

840
00:55:52,885 --> 00:55:56,785
Speaker 1:  You know what's so funny is what both of you just described as the ideal

841
00:55:56,785 --> 00:56:00,585
Speaker 1:  outcome of this is original Facebook where I have a

842
00:56:00,585 --> 00:56:03,505
Speaker 1:  wall, I post stuff on the wall and if you wanna know what's going on with

843
00:56:03,505 --> 00:56:06,305
Speaker 1:  me, you go to my wall and you look and if you want to interact, you can.

844
00:56:06,305 --> 00:56:09,385
Speaker 1:  And if not, no hard feelings. I'm not gonna know either way. That's how it

845
00:56:09,385 --> 00:56:12,985
Speaker 1:  worked. That was, that was good. Like that was the right

846
00:56:13,075 --> 00:56:16,825
Speaker 1:  thing. And then, and then they realized that people would use it more if

847
00:56:16,825 --> 00:56:20,145
Speaker 1:  they made it feel worse to use. And now here we are 20 years later and everything's

848
00:56:20,145 --> 00:56:24,005
Speaker 1:  a disaster. Like you both just pitched me on what if we had Facebook 1.0

849
00:56:24,185 --> 00:56:24,925
Speaker 1:  And I think it's right.

850
00:56:26,045 --> 00:56:29,805
Speaker 13:  I agree. But I will say I do remember in high school being

851
00:56:29,805 --> 00:56:32,845
Speaker 13:  worried that my wall posts were like too old And I would call my friends

852
00:56:32,845 --> 00:56:35,285
Speaker 13:  and be like, oh, can you post something on my wall? It looks like no one

853
00:56:35,285 --> 00:56:38,445
Speaker 13:  cares about me this week. So that's fair. Even then we had some insecurities,

854
00:56:38,465 --> 00:56:40,845
Speaker 13:  but I agree it was a better time. It was a better time.

855
00:56:41,915 --> 00:56:45,685
Speaker 1:  That is very fair. Alright, my next take, and this is kind of

856
00:56:45,685 --> 00:56:49,285
Speaker 1:  related to all of this, and I'm actually, this is the one I am least convicted

857
00:56:49,285 --> 00:56:53,125
Speaker 1:  on and kind of wanna talk through, but my take here is that the, the

858
00:56:53,235 --> 00:56:57,165
Speaker 1:  anti phone movement is real. And we,

859
00:56:57,265 --> 00:57:01,045
Speaker 1:  and the tech industry should take it seriously. The, the people who are

860
00:57:01,365 --> 00:57:05,165
Speaker 1:  actively trying to get away from technology and not just talking

861
00:57:05,165 --> 00:57:08,885
Speaker 1:  about how terrific it would be to use technology less is it's a real

862
00:57:08,945 --> 00:57:11,485
Speaker 1:  and growing and powerful movement and it's going to change things.

863
00:57:13,025 --> 00:57:16,965
Speaker 12:  I I disagree with the first half of that. Okay. But I think

864
00:57:16,965 --> 00:57:20,765
Speaker 12:  that you said two separate things actually. Okay. I think if

865
00:57:20,765 --> 00:57:23,405
Speaker 12:  we're talking about the people who are like, I'm gonna get a dumb phone,

866
00:57:23,545 --> 00:57:27,445
Speaker 12:  I'm gonna use the, oh God, what's it called? The light phone. The, the light

867
00:57:27,445 --> 00:57:30,895
Speaker 12:  phone. Like, okay, there's a, there's a small,

868
00:57:31,345 --> 00:57:35,135
Speaker 12:  small number of people who are gonna use these things. Smartphone is, is

869
00:57:35,135 --> 00:57:38,815
Speaker 12:  fine. How you use the smartphone is not fine as far as like the

870
00:57:39,195 --> 00:57:43,135
Speaker 12:  anti-technology. I'm worried about screen stuff more broadly

871
00:57:43,645 --> 00:57:45,735
Speaker 12:  that I agree with you and that I think is real

872
00:57:46,095 --> 00:57:47,815
Speaker 1:  Interesting. And you think those are separate groups of people?

873
00:57:48,355 --> 00:57:51,935
Speaker 12:  Yes, because I think going for, you know, a dumb phone

874
00:57:52,555 --> 00:57:56,415
Speaker 12:  or being, not having a phone and being disconnected, that's just

875
00:57:56,695 --> 00:58:00,455
Speaker 12:  like arbitrarily inconvenient. And I think

876
00:58:00,455 --> 00:58:03,895
Speaker 12:  like, I would like to be less connected at home,

877
00:58:04,275 --> 00:58:07,975
Speaker 12:  so I read more books. Mm. That's different, right? Like,

878
00:58:08,355 --> 00:58:12,335
Speaker 12:  you can still do that and be capable of sending, you

879
00:58:12,335 --> 00:58:14,455
Speaker 12:  know, of making a FaceTime call to your mom.

880
00:58:14,615 --> 00:58:15,455
Speaker 1:  I see. Okay. Yeah,

881
00:58:16,015 --> 00:58:19,975
Speaker 13:  I agree with what Jake said. I mean, because everyone I

882
00:58:19,975 --> 00:58:23,935
Speaker 13:  know that has a dumb phone or like a flip phone, it's one

883
00:58:23,935 --> 00:58:26,895
Speaker 13:  of their two phones. So it's like they bring it when they go out or they

884
00:58:26,895 --> 00:58:30,215
Speaker 13:  bring it when they go hiking or they have it in certain situations,

885
00:58:31,595 --> 00:58:35,215
Speaker 13:  you know. But I don't know anyone that has switched it out completely

886
00:58:35,805 --> 00:58:39,215
Speaker 13:  just because sometimes they need like Google maps, sometimes they, like,

887
00:58:39,215 --> 00:58:42,295
Speaker 13:  even from a safety standpoint, sometimes they need some of those features

888
00:58:42,295 --> 00:58:45,015
Speaker 13:  even if they don't really wanna use them. I know people that have kind of

889
00:58:45,275 --> 00:58:48,735
Speaker 13:  turned their smartphone into a dumb phone in a way by like, you know,

890
00:58:48,815 --> 00:58:52,255
Speaker 13:  bricking it during certain times or, you know, setting

891
00:58:52,435 --> 00:58:56,415
Speaker 13:  timers on apps or deleting all apps except

892
00:58:56,415 --> 00:58:59,935
Speaker 13:  like three, you know, or doing the black and white screen thing. So

893
00:59:00,305 --> 00:59:02,815
Speaker 13:  there are ways, I think there's a lot of ways that people are trying to

894
00:59:03,035 --> 00:59:06,855
Speaker 13:  be less connected and less reachable all the time. But I don't know anyone

895
00:59:06,855 --> 00:59:10,655
Speaker 13:  that's just tossing their, you know, smartphone full-time for a

896
00:59:10,815 --> 00:59:12,935
Speaker 13:  dumb phone. Yeah, that's

897
00:59:12,935 --> 00:59:16,815
Speaker 12:  Fair. And I think if you look at like, you know, maybe kids shouldn't

898
00:59:16,815 --> 00:59:20,255
Speaker 12:  have their phones in schools, people are like, oh, you know what? We went

899
00:59:20,255 --> 00:59:23,165
Speaker 12:  a little far in that. And that's like, there's starting to be a shift there

900
00:59:23,745 --> 00:59:27,525
Speaker 12:  And I feel like we're gonna see more of that where it's like these specific

901
00:59:27,585 --> 00:59:31,365
Speaker 12:  places and instances where we have overstepped with technology

902
00:59:31,695 --> 00:59:35,005
Speaker 12:  where we're like, we're maybe a little too connected, but I, I don't think

903
00:59:35,005 --> 00:59:38,205
Speaker 12:  we're gonna like fully throw it out the door that that feels like

904
00:59:38,925 --> 00:59:41,885
Speaker 12:  a little bit of an overreaction or Okay. Maybe like a niche

905
00:59:42,695 --> 00:59:45,685
Speaker 12:  trend than, than an actual like, societal movement.

906
00:59:45,785 --> 00:59:49,445
Speaker 1:  So you're, you're on the side of like sensible regulation, not like

907
00:59:49,705 --> 00:59:51,245
Speaker 1:  let the Luddites take over the world.

908
00:59:51,795 --> 00:59:52,085
Speaker 12:  Yeah.

909
00:59:52,395 --> 00:59:55,925
Speaker 1:  Okay. I think that's probably right. I think I just keep being

910
00:59:56,145 --> 00:59:59,885
Speaker 1:  struck by a, talking to a lot of people who are in the

911
01:00:00,085 --> 01:00:03,565
Speaker 1:  situation that I'm in of like, you're, you're being busy young parents with

912
01:00:03,565 --> 01:00:05,765
Speaker 1:  young kids and it's like trying to figure out

913
01:00:05,865 --> 01:00:07,405
Speaker 12:  How many iPads you give them. Yeah.

914
01:00:07,405 --> 01:00:11,165
Speaker 1:  Like, is is it okay to be an iPad kid is like a weird and complicated question

915
01:00:11,785 --> 01:00:15,405
Speaker 1:  in, in life in front of a lot of people right now. And then I'm sitting there

916
01:00:15,675 --> 01:00:19,565
Speaker 1:  next to my kid like scrolling Reddit and I'm like, what am I doing?

917
01:00:19,755 --> 01:00:22,925
Speaker 1:  This sucks. And so I've hit this point where, and you read all these studies

918
01:00:22,925 --> 01:00:26,645
Speaker 1:  about like every time they do a study that's like, we took the

919
01:00:26,755 --> 01:00:29,565
Speaker 1:  cell phones away from a bunch of kids and their mental health scores went

920
01:00:29,565 --> 01:00:32,285
Speaker 1:  through the roof. And it's like, there's just, there is something

921
01:00:33,485 --> 01:00:37,245
Speaker 1:  bubbling here. And I think we've spent a long time listening to

922
01:00:37,605 --> 01:00:41,445
Speaker 1:  companies essentially say, you can say whatever you want. We know how much

923
01:00:41,465 --> 01:00:45,085
Speaker 1:  you use our product, and the more we give it to you, the more you will use

924
01:00:45,085 --> 01:00:48,885
Speaker 1:  it. And as far as I understand, there's not a lot of

925
01:00:49,045 --> 01:00:52,725
Speaker 1:  evidence that that's changing yet. But like, anecdotally, the

926
01:00:52,865 --> 01:00:56,805
Speaker 1:  the smoke is real that like people in some meaningful way

927
01:00:56,805 --> 01:00:59,245
Speaker 1:  are starting to pull out and fight back.

928
01:01:00,785 --> 01:01:03,885
Speaker 1:  And I just think, I, I think we're, we're gonna start to see that show up

929
01:01:03,885 --> 01:01:07,285
Speaker 1:  in meaningful ways. Because every time a company has even tried to like address

930
01:01:07,285 --> 01:01:09,565
Speaker 1:  that, like Instagram had the thing where it was like, you're all caught up

931
01:01:09,665 --> 01:01:11,645
Speaker 1:  and then do you know what happened? They got rid of it because they were

932
01:01:11,645 --> 01:01:15,565
Speaker 1:  like, oh, actually we want people to keep scrolling and they will. And, and

933
01:01:15,605 --> 01:01:19,565
Speaker 1:  I think, I think, And I hope we're about to see that start to

934
01:01:19,565 --> 01:01:22,165
Speaker 1:  shift against some of that trend.

935
01:01:22,295 --> 01:01:23,525
Speaker 13:  Definitely. I

936
01:01:23,525 --> 01:01:27,005
Speaker 1:  Hope maybe I'm, maybe I'm wrong. Maybe, maybe we're all gonna like bitch

937
01:01:27,005 --> 01:01:30,925
Speaker 1:  and moan about screen time as we ever increase our screen time until

938
01:01:31,165 --> 01:01:31,565
Speaker 1:  we're dead.

939
01:01:31,865 --> 01:01:34,845
Speaker 13:  You know? I think you're right though. I mean, I have a friend that's starting

940
01:01:34,845 --> 01:01:38,565
Speaker 13:  like a movement for connecting in real life

941
01:01:38,745 --> 01:01:42,525
Speaker 13:  and I'm going to one of her events this we this weekend or next weekend,

942
01:01:42,545 --> 01:01:46,245
Speaker 13:  and I'm excited. It's like, you know, they're, we're seeing more like community

943
01:01:46,435 --> 01:01:50,185
Speaker 13:  kind of almost activism I guess around

944
01:01:50,215 --> 01:01:54,105
Speaker 13:  like connecting in real life and how to, how to just like hang out with

945
01:01:54,105 --> 01:01:56,745
Speaker 13:  people, which is sad that we have to do it. Yeah. But it's cool that it's

946
01:01:56,745 --> 01:01:56,985
Speaker 13:  happening.

947
01:01:57,465 --> 01:02:01,185
Speaker 1:  A movement to hang out in real life is like a, just a terrifying concept

948
01:02:01,185 --> 01:02:03,305
Speaker 1:  that that needs to exist at all. But I'm glad it exists.

949
01:02:04,135 --> 01:02:05,385
Speaker 13:  Exactly. I

950
01:02:05,385 --> 01:02:08,745
Speaker 1:  Don't know, I say this as I'm staring into my screen for the 15th hour today.

951
01:02:08,745 --> 01:02:09,465
Speaker 1:  Like, I don't know.

952
01:02:09,535 --> 01:02:13,465
Speaker 12:  Yeah, on one hand I feel like it has just been like enough years with social

953
01:02:13,465 --> 01:02:17,305
Speaker 12:  media, with electronics that like, naturally it we're like kind of due for

954
01:02:17,905 --> 01:02:21,885
Speaker 12:  a little bit of that pushback. On the other hand, I was at

955
01:02:21,885 --> 01:02:25,845
Speaker 12:  the grocery store when TikTok got Unbound back in, what was

956
01:02:25,845 --> 01:02:27,765
Speaker 12:  it? February and

957
01:02:29,305 --> 01:02:32,885
Speaker 12:  the, the, the cashier at the at register was like,

958
01:02:33,345 --> 01:02:37,245
Speaker 12:  Hey, she like said to her manager, she's like, Hey, can I leave like tiktoks

959
01:02:37,245 --> 01:02:40,685
Speaker 12:  back online? And I'm like, yo, it was, it was banned for like eight hours.

960
01:02:40,995 --> 01:02:42,565
Speaker 12:  Like, yeah, you're fine.

961
01:02:43,515 --> 01:02:47,405
Speaker 1:  It's gonna be all right. Yeah. Yeah. I think, I don't know,

962
01:02:47,545 --> 01:02:51,525
Speaker 1:  I'm, I'm hopeful, But that is like, as a reporter, very much a thing

963
01:02:51,545 --> 01:02:55,205
Speaker 1:  I'm paying attention to now. It's like the, the, the smoke feels like it's

964
01:02:55,205 --> 01:02:58,885
Speaker 1:  there in a way that it has not been before. All right, my next take,

965
01:02:58,955 --> 01:03:02,845
Speaker 1:  this is the one I feel the most strongly about. All the streaming

966
01:03:03,005 --> 01:03:06,005
Speaker 1:  services are bad and they should feel bad and they have made the experience

967
01:03:06,005 --> 01:03:07,405
Speaker 1:  of trying to watch shows and movies. Awful.

968
01:03:07,795 --> 01:03:09,125
Speaker 13:  I'll let Jake take this one first.

969
01:03:10,565 --> 01:03:14,465
Speaker 1:  I will elaborate if you would like, but I feel this very strongly.

970
01:03:16,205 --> 01:03:19,065
Speaker 12:  Oh man, there's just a whole lot bundled up in that.

971
01:03:19,355 --> 01:03:21,385
Speaker 1:  Would you like me to make it simpler for you? Yes,

972
01:03:21,445 --> 01:03:23,865
Speaker 12:  Yes, yes. I'm gonna take that lifeline. It's

973
01:03:23,865 --> 01:03:27,705
Speaker 1:  Just the last thing at, at this moment in time, the experience

974
01:03:27,705 --> 01:03:29,985
Speaker 1:  of trying to watch TV shows and movies is awful.

975
01:03:30,625 --> 01:03:34,545
Speaker 12:  I mean, I disagree with that. Agree. Like, agree, it's fine, it's

976
01:03:34,545 --> 01:03:38,425
Speaker 12:  fine. I, I have like a nuanced disagreement,

977
01:03:38,525 --> 01:03:42,305
Speaker 12:  but like if there's a show that I want to watch on HBO or

978
01:03:42,415 --> 01:03:45,905
Speaker 12:  Hulu or whatever, that's fine. I sign up for the service, I go to the service,

979
01:03:46,025 --> 01:03:49,545
Speaker 12:  I click the button, it plays, it's fine. May maybe if my, if my,

980
01:03:49,845 --> 01:03:53,585
Speaker 12:  you know, Google tv, whatever streamer is, is feeling good about it,

981
01:03:53,805 --> 01:03:56,225
Speaker 12:  it might even remember that I was watching that series and just put the

982
01:03:56,225 --> 01:04:00,075
Speaker 12:  next episode on my home screen. Might not, I don't know, but it's fine.

983
01:04:00,215 --> 01:04:00,435
Speaker 12:  And

984
01:04:00,435 --> 01:04:02,955
Speaker 1:  This is a good experience that you're describing this, this you're happy

985
01:04:02,955 --> 01:04:03,595
Speaker 1:  about this. It's,

986
01:04:03,635 --> 01:04:06,715
Speaker 12:  I I can get there. I can even, I can like use my voice to, I have to go

987
01:04:06,715 --> 01:04:10,435
Speaker 1:  To some unknowable app that I pay an unknowable amount of money for. No,

988
01:04:10,435 --> 01:04:10,515
Speaker 1:  no,

989
01:04:10,675 --> 01:04:13,155
Speaker 12:  No. It's not un I it's easy enough to remember what,

990
01:04:13,295 --> 01:04:15,955
Speaker 1:  How much do you pay for HBO max right now? Tell me off the top of your head.

991
01:04:15,975 --> 01:04:19,555
Speaker 12:  Oh, it's too much money. It's like HBO O max is like $90 a month. I don't

992
01:04:19,555 --> 01:04:19,635
Speaker 12:  know.

993
01:04:19,775 --> 01:04:22,395
Speaker 1:  Do you know, I'm serious. Do you know how much you pay for HBO Max right

994
01:04:22,395 --> 01:04:22,435
Speaker 1:  now

995
01:04:22,695 --> 01:04:26,195
Speaker 12:  Off the top of your head? 20. I wanna say it's 20 for the 4K. It's too much

996
01:04:26,195 --> 01:04:29,675
Speaker 12:  money. It's too much money for four. So, well this is, but I would, I would

997
01:04:29,865 --> 01:04:33,195
Speaker 12:  make a distinction, right? Like is the experience good? Like yeah, I can

998
01:04:33,195 --> 01:04:36,955
Speaker 12:  sign up easy, I can sign, I can like turn off my subscription easy

999
01:04:36,975 --> 01:04:40,755
Speaker 12:  enough. The thing that I am like I wanna tell

1000
01:04:41,075 --> 01:04:44,955
Speaker 12:  everyone you should actually do, if you want to watch movies, do not just

1001
01:04:44,955 --> 01:04:48,115
Speaker 12:  sign up for Netflix. Do not just sign up for Hulu, whatever,

1002
01:04:48,745 --> 01:04:52,475
Speaker 12:  just pay $3 and rent them for the

1003
01:04:52,475 --> 01:04:55,475
Speaker 12:  weekend. Y you'll spend less than you would

1004
01:04:56,625 --> 01:04:59,725
Speaker 12:  if you were subscribing to a service for an entire month and you get a bigger

1005
01:05:00,005 --> 01:05:02,005
Speaker 12:  selection. You don't have to worry about which service you're on, which

1006
01:05:02,005 --> 01:05:05,725
Speaker 12:  service you're not on. But if we're talking about TV shows, it's a little

1007
01:05:05,725 --> 01:05:05,965
Speaker 12:  different.

1008
01:05:06,475 --> 01:05:09,645
Speaker 13:  Yeah, I agree with that. I mean for me

1009
01:05:10,235 --> 01:05:14,165
Speaker 13:  it's not a bad experience, you know, but

1010
01:05:14,845 --> 01:05:17,805
Speaker 13:  I will say I've been really into paying for physical media

1011
01:05:18,825 --> 01:05:22,565
Speaker 13:  in Brooklyn now. There is a DVD store

1012
01:05:23,145 --> 01:05:26,965
Speaker 13:  and it's so exciting to be able to go in there And I hope it survives.

1013
01:05:26,995 --> 01:05:30,645
Speaker 13:  It's only been open for like six months but I hope it survives and right

1014
01:05:30,645 --> 01:05:34,605
Speaker 13:  now you can go in and just buy your favorite TBDs and now you

1015
01:05:34,605 --> 01:05:37,485
Speaker 13:  know, I have a collection of my favorite ones that I watch over and over

1016
01:05:37,485 --> 01:05:41,365
Speaker 13:  because I am a chronic re watcher. I rewatch everything so many times.

1017
01:05:41,645 --> 01:05:44,525
Speaker 13:  I think it's like an anxiety thing. It's very comforting to me to rewatch

1018
01:05:44,525 --> 01:05:48,325
Speaker 13:  my favorite movies. So I have all my favorites And I can just do it at any

1019
01:05:48,325 --> 01:05:51,645
Speaker 13:  time. Even old ones that are no longer on streaming services like

1020
01:05:51,875 --> 01:05:55,445
Speaker 13:  childhood Mary Kay Nally movies, I can now watch them, you know?

1021
01:05:55,665 --> 01:05:59,445
Speaker 13:  But yeah, I don't think it's a bad experience to you know,

1022
01:05:59,445 --> 01:06:02,765
Speaker 13:  watch. I mean it's annoying to have to Google which streaming service I

1023
01:06:02,765 --> 01:06:06,205
Speaker 13:  watch whatever thing on. But you know, I think it's all about

1024
01:06:06,205 --> 01:06:09,925
Speaker 13:  diversification and everyone should buy some DVDs from their local

1025
01:06:09,975 --> 01:06:11,685
Speaker 13:  store to keep it alive. That's what I think

1026
01:06:11,855 --> 01:06:15,405
Speaker 1:  Hated. Let me tell you what I just heard you say. What I just heard you say

1027
01:06:15,405 --> 01:06:18,965
Speaker 1:  is actually streaming is fine but it doesn't have anything that I wanna watch

1028
01:06:18,995 --> 01:06:22,965
Speaker 1:  such that I have to leave my house and go buy a DVD so that I'm

1029
01:06:22,965 --> 01:06:25,565
Speaker 1:  able to actually watch the things that I wanna watch when I wanna watch them.

1030
01:06:25,745 --> 01:06:26,645
Speaker 1:  That's what you just said.

1031
01:06:26,865 --> 01:06:30,445
Speaker 13:  Listen, that's not, that's not the streaming services fault,

1032
01:06:31,005 --> 01:06:33,965
Speaker 13:  honestly. Yes, it's, there's a lot of weird, there's a lot of weird rights

1033
01:06:33,965 --> 01:06:37,845
Speaker 13:  issues with my childhood faves. Like there's a Tyra

1034
01:06:37,895 --> 01:06:41,405
Speaker 13:  Banks movie called Lifesize that no one could find on any

1035
01:06:41,715 --> 01:06:45,645
Speaker 13:  streaming service anywhere. Not even Prime video. So I had to order it

1036
01:06:45,645 --> 01:06:49,605
Speaker 13:  on eBay like a Luddite but or minus the eBay park. But yeah,

1037
01:06:50,085 --> 01:06:54,005
Speaker 1:  This is the problem. This is the problem. I

1038
01:06:54,415 --> 01:06:58,205
Speaker 1:  can't fi like imagine, imagine if you went on Spotify and just like four

1039
01:06:58,225 --> 01:07:01,205
Speaker 1:  out of 10 songs you wanted to listen to just weren't there and you were like

1040
01:07:01,205 --> 01:07:04,525
Speaker 1:  cool, I'll still listen to Spotify every day. Like no that might be the case

1041
01:07:04,525 --> 01:07:08,485
Speaker 1:  soon. This is, it's outrageous that this is where we have landed that that

1042
01:07:08,505 --> 01:07:12,485
Speaker 1:  you actually pay as however many services you wanna pay for And I pay

1043
01:07:12,485 --> 01:07:16,285
Speaker 1:  for a lot of them. You still can't reasonably expect to

1044
01:07:16,285 --> 01:07:18,925
Speaker 1:  watch the thing that you wanna watch and that it will be in the same place

1045
01:07:18,925 --> 01:07:21,125
Speaker 1:  when you go back to find it and that it will have all the stuff that you

1046
01:07:21,125 --> 01:07:24,925
Speaker 1:  want. I watch always Sunny on Hulu. There are just a bunch of episodes of

1047
01:07:24,925 --> 01:07:28,005
Speaker 1:  Always Sunny that Hulu doesn't have because it finds them objectionable.

1048
01:07:28,115 --> 01:07:31,085
Speaker 1:  Just is that this just don't exist. Is that the show that like that put you

1049
01:07:31,085 --> 01:07:34,125
Speaker 1:  over the edge? What was it? No, I've been over, I was over the edge all summer

1050
01:07:34,925 --> 01:07:38,885
Speaker 1:  because what I have is a toddler who wants to watch

1051
01:07:38,945 --> 01:07:42,645
Speaker 1:  the same movie 650 times in a row And I can

1052
01:07:42,815 --> 01:07:46,405
Speaker 1:  never remember where it is and the search never works and it drives me

1053
01:07:47,305 --> 01:07:51,285
Speaker 1:  insane and I'm paying a billion dollars a month to have

1054
01:07:51,285 --> 01:07:55,205
Speaker 1:  70 different apps that are all really slow on my Roku TV and none of

1055
01:07:55,205 --> 01:07:58,885
Speaker 1:  them put the continue watching ro in the same place. And the search sucks

1056
01:07:58,885 --> 01:08:02,685
Speaker 1:  across all. Like it's just, there is more content available to all of us

1057
01:08:02,685 --> 01:08:06,325
Speaker 1:  than ever and it is being put together in the stupidest, most hostile

1058
01:08:06,585 --> 01:08:08,845
Speaker 1:  way and it drives me nuts.

1059
01:08:09,505 --> 01:08:13,405
Speaker 13:  The solution is putting K-pop demon hunters on a loop forever and

1060
01:08:13,405 --> 01:08:16,645
Speaker 13:  never switching to any other movie. And then your toddler, you will be thrilled.

1061
01:08:17,165 --> 01:08:21,005
Speaker 1:  I also blame Netflix because it took Netflix like 10 days to put

1062
01:08:21,055 --> 01:08:24,965
Speaker 1:  K-pop demon hunters on my home screen And I was just sitting at home unaware

1063
01:08:24,965 --> 01:08:27,565
Speaker 1:  of K-pop demon hunters for far too long.

1064
01:08:27,955 --> 01:08:31,125
Speaker 13:  Unfortunately my entire Spotify rap this year is going to be

1065
01:08:31,975 --> 01:08:35,805
Speaker 13:  K-pop demon hunters the soundtrack. And I, I'm, I accept that but I'm not

1066
01:08:35,805 --> 01:08:38,125
Speaker 13:  gonna be sharing my Spotify raft this year. That's,

1067
01:08:38,285 --> 01:08:40,405
Speaker 1:  I don't think there's anything wrong with that at all. Also, Jake, by the

1068
01:08:40,405 --> 01:08:43,725
Speaker 1:  way, I just checked and you're paying $21 a month for HBO max.

1069
01:08:43,875 --> 01:08:46,605
Speaker 12:  It's too much. I don't even know what I'm watching on HBO right now.

1070
01:08:46,705 --> 01:08:49,765
Speaker 1:  And David Zoff recently said it's way underpriced so that number's probably

1071
01:08:49,765 --> 01:08:50,245
Speaker 1:  gonna go up.

1072
01:08:50,475 --> 01:08:53,845
Speaker 12:  Yeah, that's not exactly, sorry. That's why I listen, I subscribed to this

1073
01:08:53,845 --> 01:08:56,365
Speaker 12:  thing for like a month at a time and then I'm turning it off. 'cause that

1074
01:08:56,365 --> 01:08:57,485
Speaker 12:  is way too much money.

1075
01:08:57,875 --> 01:09:01,045
Speaker 1:  Yeah, because what's a better user experience than having to like balance

1076
01:09:01,045 --> 01:09:03,885
Speaker 1:  your checkbook against streaming 16 times a year?

1077
01:09:04,065 --> 01:09:07,965
Speaker 12:  It is rough. Yeah. I would pay a subscription fee for somebody to just manage

1078
01:09:07,965 --> 01:09:11,805
Speaker 12:  these for me And I would probably come out ahead. I'm like, I'm coming

1079
01:09:11,805 --> 01:09:12,845
Speaker 12:  around I, I'm

1080
01:09:13,295 --> 01:09:13,725
Speaker 1:  About this

1081
01:09:13,725 --> 01:09:16,925
Speaker 12:  One. I'm a hundred percent agree with your anger. Even if I don't necessarily

1082
01:09:17,045 --> 01:09:19,205
Speaker 12:  a hundred percent agree with this in practice,

1083
01:09:19,645 --> 01:09:23,285
Speaker 1:  I don't care whose fault it's the streaming to be so clear whether or not

1084
01:09:23,425 --> 01:09:27,245
Speaker 1:  all of this is Netflix's fault, the fact that there literally is not a

1085
01:09:27,305 --> 01:09:30,685
Speaker 1:  way for me to watch the movie The Nice Guys, which I tried to watch the other

1086
01:09:30,685 --> 01:09:34,165
Speaker 1:  day. Yeah. It literally does not exist in a digital

1087
01:09:34,475 --> 01:09:38,205
Speaker 1:  streaming service. And I pay for all of them that is dumb and

1088
01:09:38,345 --> 01:09:40,005
Speaker 1:  bad and everyone should feel bad.

1089
01:09:40,795 --> 01:09:41,325
Speaker 12:  This is what

1090
01:09:41,325 --> 01:09:44,285
Speaker 13:  Kills me. I'm gonna buy it for you at my DVD store instead of tv. Yes please.

1091
01:09:44,585 --> 01:09:48,525
Speaker 1:  And then I will buy a DVD player because one of those in forever.

1092
01:09:49,065 --> 01:09:52,805
Speaker 1:  And then my life will be good again. I have come back around to like, especially

1093
01:09:52,825 --> 01:09:55,485
Speaker 1:  for shows that I wanna watch again, there are a couple of streaming services

1094
01:09:55,605 --> 01:09:59,165
Speaker 1:  I basically subscribe to for like one or two shows and

1095
01:09:59,745 --> 01:10:02,925
Speaker 1:  I'm getting to like, I'm just gonna buy those shows on

1096
01:10:03,565 --> 01:10:07,165
Speaker 1:  DVD or digital or whatever and then just watch them and like,

1097
01:10:07,785 --> 01:10:11,775
Speaker 1:  who needs the streaming service anymore? Okay, I need to calm down.

1098
01:10:11,925 --> 01:10:14,895
Speaker 1:  I've, I should have done that one last and then I could have like, I could

1099
01:10:14,895 --> 01:10:18,335
Speaker 1:  have like relaxed into the break. I'm gonna give you a calmer take. My calmer

1100
01:10:18,335 --> 01:10:22,295
Speaker 1:  take, and this is the second to last one is I think we will look back

1101
01:10:22,435 --> 01:10:26,335
Speaker 1:  on 2025 as the year self-driving cars actually

1102
01:10:26,615 --> 01:10:30,585
Speaker 1:  happened. And I, I'm not sure we're talking about it like we're, we're in

1103
01:10:30,585 --> 01:10:34,465
Speaker 1:  sort of the like boiling frog state of like city by

1104
01:10:34,465 --> 01:10:38,385
Speaker 1:  city, company by company. Waymo is starting to pull this off. Like the

1105
01:10:38,445 --> 01:10:41,945
Speaker 1:  the we've, we've been lied to about self-driving cars for a full decade,

1106
01:10:43,385 --> 01:10:46,845
Speaker 1:  but I think they did it now And I think, I think this is the year they're

1107
01:10:46,845 --> 01:10:49,035
Speaker 1:  doing it. What do you guys think?

1108
01:10:49,835 --> 01:10:52,555
Speaker 13:  I mean it's hard when you have a nuanced take on these things. You know,

1109
01:10:52,835 --> 01:10:54,795
Speaker 13:  I, I've been covering it not allowed. I remember.

1110
01:10:56,475 --> 01:11:00,075
Speaker 13:  I mean it's like, I guess I disagree because

1111
01:11:00,435 --> 01:11:04,075
Speaker 13:  I don't believe that they've done it now. I think they've made strides.

1112
01:11:04,685 --> 01:11:08,315
Speaker 13:  Waymo is testing vehicles in New York City, which they never did before

1113
01:11:08,315 --> 01:11:12,155
Speaker 13:  because they said there were too many crazy

1114
01:11:12,225 --> 01:11:16,165
Speaker 13:  factors that they couldn't really control for and they always want some

1115
01:11:16,385 --> 01:11:20,245
Speaker 13:  uncontrollable factors, but there were just too many in New York City. Now

1116
01:11:20,245 --> 01:11:24,125
Speaker 13:  there aren't. So I do think they made a lot of strides, but I won't really

1117
01:11:24,185 --> 01:11:27,085
Speaker 13:  say they've done it until it's something that

1118
01:11:28,545 --> 01:11:32,275
Speaker 13:  most people I know do to get to work like city

1119
01:11:32,305 --> 01:11:35,915
Speaker 13:  bike or like until it's as common as biking in general.

1120
01:11:36,595 --> 01:11:40,275
Speaker 13:  I don't think I will say that they have done it, but I know in SF everyone

1121
01:11:40,295 --> 01:11:43,995
Speaker 13:  is loving Waymo's. I've heard of some parents like putting their kid in

1122
01:11:43,995 --> 01:11:47,915
Speaker 13:  a Waymo to take them somewhere. That certainly is, you know, it

1123
01:11:47,915 --> 01:11:51,715
Speaker 13:  does mean that it's come a long way, but I think for me until it's as

1124
01:11:51,715 --> 01:11:55,155
Speaker 13:  common as say like biking, which, you know, I have a handful of friends

1125
01:11:55,155 --> 01:11:58,795
Speaker 13:  that bike everywhere. I feel like when self-driving cars

1126
01:11:58,845 --> 01:12:01,955
Speaker 13:  reach that status, I will say they've done it. Okay.

1127
01:12:02,095 --> 01:12:04,075
Speaker 1:  That's a good take. Jake, what do you think? I think

1128
01:12:04,075 --> 01:12:07,675
Speaker 12:  You're right at least that the, that 2025 might be a turning point.

1129
01:12:08,605 --> 01:12:12,385
Speaker 12:  You know, it's not, it's not clear to me what has improved, right?

1130
01:12:12,385 --> 01:12:15,865
Speaker 12:  Like, I don't know if the technology got better or if the regulations just

1131
01:12:15,865 --> 01:12:19,025
Speaker 12:  got a little fuzzier and they're like, oh, let's see what happens. But

1132
01:12:19,745 --> 01:12:23,625
Speaker 12:  I I it's, it's Waymo in particular, I I do not

1133
01:12:23,625 --> 01:12:27,185
Speaker 12:  count the ro robax. I, I'm sorry. No, but

1134
01:12:28,075 --> 01:12:32,025
Speaker 12:  right like Waymo, which I assume cares more about

1135
01:12:32,025 --> 01:12:33,625
Speaker 12:  their reputation and

1136
01:12:33,685 --> 01:12:37,545
Speaker 1:  Safety of people. Yes. Yeah. Right.

1137
01:12:37,545 --> 01:12:41,385
Speaker 12:  Like I've been hearing a ton about their rollout in San Francisco

1138
01:12:42,045 --> 01:12:45,945
Speaker 12:  and yeah, now they're in New York. So I've, I've been hurt hearing about

1139
01:12:45,945 --> 01:12:49,505
Speaker 12:  at least some pushback to them. I believe it was Waymo rolling out in Boston,

1140
01:12:49,995 --> 01:12:53,785
Speaker 12:  right? Like once, once stuff starts happening, like that means that the

1141
01:12:53,785 --> 01:12:56,825
Speaker 12:  technology is improving, right? If they're putting it in a place that the

1142
01:12:56,825 --> 01:12:59,625
Speaker 12:  average person can experience 'em, it doesn't necessarily mean everybody's

1143
01:12:59,625 --> 01:13:03,545
Speaker 12:  gonna use them, But that feels like a turning point. I

1144
01:13:03,545 --> 01:13:07,385
Speaker 12:  have not gotten a chance to ride one yet. I don't know how the experience

1145
01:13:07,385 --> 01:13:11,345
Speaker 12:  is. I don't know if they really are all there yet. And it does seem like

1146
01:13:11,345 --> 01:13:15,265
Speaker 12:  there's still quite a few limitations to where they can go and what

1147
01:13:15,265 --> 01:13:19,145
Speaker 12:  environments they're good in. But I I am like changing your take

1148
01:13:19,225 --> 01:13:23,025
Speaker 12:  a little bit, David, but it it, I I think you, you may

1149
01:13:23,105 --> 01:13:26,345
Speaker 12:  well be onto something that like 2025 something happened this year.

1150
01:13:26,745 --> 01:13:28,145
Speaker 13:  I agree that it was a turning point.

1151
01:13:28,295 --> 01:13:28,585
Speaker 12:  Yeah,

1152
01:13:28,625 --> 01:13:31,185
Speaker 1:  I think there will be like a chapter in the book about

1153
01:13:32,435 --> 01:13:35,695
Speaker 1:  how self-driving cars happened. That is 2025 and it'll be like

1154
01:13:36,675 --> 01:13:40,015
Speaker 1:  the, the year real people started taking it seriously.

1155
01:13:40,185 --> 01:13:43,575
Speaker 12:  There is nothing for a dec, right? We got, we got, we went from zero

1156
01:13:43,575 --> 01:13:47,455
Speaker 12:  self-driving to like, we can basically do self-driving but actually they're

1157
01:13:47,455 --> 01:13:51,415
Speaker 12:  not safe enough to put on real roads and that was 10 years and now this

1158
01:13:51,415 --> 01:13:53,535
Speaker 12:  year they're like, we're just gonna roll it out to a bunch of cities. And

1159
01:13:53,535 --> 01:13:55,495
Speaker 12:  like, I don't know what changed here. Uber and

1160
01:13:55,495 --> 01:13:59,175
Speaker 1:  Lyft were promising that it was gonna be 2020. I would, I would remind everyone

1161
01:13:59,365 --> 01:14:03,295
Speaker 1:  that those companies made a huge amount of noise and a huge

1162
01:14:03,295 --> 01:14:07,255
Speaker 1:  amount of money lying to everybody about how close we were to

1163
01:14:07,255 --> 01:14:09,775
Speaker 1:  self-driving cars. Tesla has been doing the same thing for a very long time.

1164
01:14:10,195 --> 01:14:14,165
Speaker 1:  And I think personally I got so jaded by all of

1165
01:14:14,165 --> 01:14:16,805
Speaker 1:  that that I got to the point where anytime somebody would like be bullish

1166
01:14:16,805 --> 01:14:19,205
Speaker 1:  on self-driving cars, they'd be like, oh congratulations. Like, whatever,

1167
01:14:19,445 --> 01:14:23,365
Speaker 1:  I don't care. Now I kind of buy it. Like IIII agree with

1168
01:14:23,365 --> 01:14:26,965
Speaker 1:  you Hayden, that I think we're a ways away from it being like ubiquitous

1169
01:14:26,985 --> 01:14:30,805
Speaker 1:  in a really important way And I think the, the sort of world tilt

1170
01:14:30,805 --> 01:14:33,605
Speaker 1:  towards self-driving cars is gonna take a really long time.

1171
01:14:35,065 --> 01:14:38,925
Speaker 1:  But at this point it's the, like do I think a person who doesn't pay

1172
01:14:38,925 --> 01:14:42,685
Speaker 1:  any attention to this might get in a Waymo in 2025? I think the answer is

1173
01:14:42,685 --> 01:14:44,765
Speaker 1:  yeah, And I think that's already happening And I think that's a big deal

1174
01:14:45,435 --> 01:14:49,235
Speaker 1:  that like, it just becomes a way to get somewhere, not a

1175
01:14:49,365 --> 01:14:50,635
Speaker 1:  technology story.

1176
01:14:51,075 --> 01:14:54,595
Speaker 13:  I think they had, like you said, we did have 10 years without,

1177
01:14:54,975 --> 01:14:58,875
Speaker 13:  not without a lot of progress. But I do think for me it's

1178
01:14:58,875 --> 01:15:02,795
Speaker 13:  like I was getting all the press releases of the tiny incremental changes

1179
01:15:02,795 --> 01:15:06,115
Speaker 13:  that they were making every year. Like they were testing in a new city,

1180
01:15:06,115 --> 01:15:09,995
Speaker 13:  they were suddenly starting to test in snow for the first time, right? They

1181
01:15:09,995 --> 01:15:13,555
Speaker 13:  were suddenly starting to test somewhere with a lot of pedestrians and crowds

1182
01:15:13,555 --> 01:15:16,395
Speaker 13:  that were a little more unpredictable. So I think for me it's like I've

1183
01:15:16,395 --> 01:15:19,995
Speaker 13:  seen incremental progress year after year, but it was nothing to write home

1184
01:15:19,995 --> 01:15:23,955
Speaker 13:  about. And 2025 I think is when kind of the snowball

1185
01:15:23,975 --> 01:15:27,035
Speaker 13:  effect happened. Like all that incremental progress that was like kind of

1186
01:15:27,035 --> 01:15:30,955
Speaker 13:  not worth writing about sometimes, but came worth

1187
01:15:30,955 --> 01:15:34,595
Speaker 13:  it. And you know, I did drive in a couple waymo's last year And

1188
01:15:35,035 --> 01:15:38,955
Speaker 13:  I have to say my favorite thing about it was honestly that I could just

1189
01:15:38,955 --> 01:15:42,035
Speaker 13:  be completely alone. I didn't have to talk to anyone. I didn't have to worry

1190
01:15:42,035 --> 01:15:45,875
Speaker 13:  about playing my music aloud. I was just like in my own little room,

1191
01:15:45,975 --> 01:15:49,195
Speaker 13:  you know? So that's my favorite part, just being solo and being able to

1192
01:15:49,195 --> 01:15:52,995
Speaker 13:  play, you know, k-pop demon hunters that it wasn't out yet,

1193
01:15:53,255 --> 01:15:54,355
Speaker 13:  but that's what I'd be playing. Now

1194
01:15:54,895 --> 01:15:57,915
Speaker 1:  Are you guys nice to your Uber drivers? Like do you, are you, are you people

1195
01:15:57,915 --> 01:16:00,875
Speaker 1:  who like get in the back and like hang with the driver the whole time? Or

1196
01:16:00,875 --> 01:16:04,035
Speaker 1:  do you just like wear headphones? No talking wave and get out?

1197
01:16:05,235 --> 01:16:08,435
Speaker 12:  I say, I say like a very, I'm always like, Hey, thanks for coming by, I

1198
01:16:08,435 --> 01:16:12,075
Speaker 12:  appreciate it. And then, then we're done. The, the thing is they're in New York,

1199
01:16:12,075 --> 01:16:14,275
Speaker 12:  they're on the phone with somebody, they don't wanna talk to me

1200
01:16:14,295 --> 01:16:17,915
Speaker 1:  Always. Yeah, yeah, that's very true. I, I used to like try and talk to people

1201
01:16:18,015 --> 01:16:21,755
Speaker 1:  and then I saw a thing that was like a meme of somebody being like, so how

1202
01:16:21,835 --> 01:16:24,875
Speaker 1:  long have you been doing this for? And the Uber driver like leaping out of

1203
01:16:24,875 --> 01:16:27,475
Speaker 1:  a window And I was like, oh no, I asked that question to all of my Uber drivers.

1204
01:16:27,815 --> 01:16:29,075
Speaker 1:  So now I don't talk very much.

1205
01:16:30,155 --> 01:16:33,195
Speaker 13:  I just get in And I say, Hey thanks. And then I put in my headphones and

1206
01:16:33,195 --> 01:16:36,035
Speaker 13:  don't talk. 'cause yeah, New York, I mean they don't want you to talk, you

1207
01:16:36,035 --> 01:16:38,635
Speaker 13:  don't wanna talk. There's no false pretenses. But you know what, whenever

1208
01:16:38,675 --> 01:16:42,355
Speaker 13:  I take an Uber in Atlanta, I am in the longest

1209
01:16:42,355 --> 01:16:45,475
Speaker 13:  conversation of my life every single time. I don't know what it is, but

1210
01:16:46,155 --> 01:16:49,235
Speaker 1:  I have invented phone calls to get out of those conversations before. That's,

1211
01:16:49,415 --> 01:16:50,115
Speaker 1:  I'm not proud of that, but

1212
01:16:50,535 --> 01:16:51,435
Speaker 13:  I'm gonna call you next time.

1213
01:16:51,975 --> 01:16:54,355
Speaker 1:  Oh, that's a perfect segue. That was not intentional. But this is a perfect

1214
01:16:54,355 --> 01:16:58,315
Speaker 1:  segue to my last take, my final summer take of 2025

1215
01:16:58,855 --> 01:17:02,275
Speaker 1:  is that we should bring back phone calls as like a normal thing that people

1216
01:17:02,295 --> 01:17:06,195
Speaker 1:  do to each other all day every day. But I wanna bring back phone calls in

1217
01:17:06,195 --> 01:17:10,005
Speaker 1:  one very specific way And I wanna bring back phone calls like cops do

1218
01:17:10,025 --> 01:17:13,925
Speaker 1:  in movies where there's no, hello, how

1219
01:17:13,945 --> 01:17:17,605
Speaker 1:  are you? Let's small talk for 60 seconds before we get into it. We don't

1220
01:17:17,605 --> 01:17:21,525
Speaker 1:  even say goodbye at the end. You, you call me Hayden, you call me,

1221
01:17:21,605 --> 01:17:25,365
Speaker 1:  I pick up the phone And I say, hey, you say, hey, I have a, was it in Brooklyn

1222
01:17:25,365 --> 01:17:27,885
Speaker 1:  or Manhattan? I say Brooklyn and you say cool and you hang up the phone,

1223
01:17:28,115 --> 01:17:31,715
Speaker 1:  done the end. We, we should call like we text

1224
01:17:31,935 --> 01:17:33,235
Speaker 1:  and we should do it all the time.

1225
01:17:33,875 --> 01:17:37,795
Speaker 13:  I could not agree more. I 100% agree. I

1226
01:17:37,795 --> 01:17:41,115
Speaker 13:  think it's because my friends And I don't like phone calls that much, but

1227
01:17:41,115 --> 01:17:44,835
Speaker 13:  we call each other a lot. So we have defaulted to that. Literally. Yeah,

1228
01:17:44,905 --> 01:17:47,955
Speaker 13:  I'll call and be like, hi, you know,

1229
01:17:48,785 --> 01:17:51,875
Speaker 13:  literally what you just said. I'll be like, hi, blah, blah blah. There's

1230
01:17:51,875 --> 01:17:55,715
Speaker 13:  no pleasantries, there's nothing. And we know the love is there, it's, we

1231
01:17:55,715 --> 01:17:59,395
Speaker 13:  know what's going on. Yes. And then if I'm calling for like, you know, just

1232
01:17:59,415 --> 01:18:03,355
Speaker 13:  to catch up, I'll be like, hi, I have no reason to call.

1233
01:18:03,535 --> 01:18:07,075
Speaker 13:  I'm just like calling on a whim and then I set the expectations then we

1234
01:18:07,075 --> 01:18:10,435
Speaker 13:  can small talk and catch up about our lives. But like most of our calls

1235
01:18:10,545 --> 01:18:13,275
Speaker 13:  it's like if I pick up a call from my friend, I know it's gonna be within

1236
01:18:13,385 --> 01:18:17,275
Speaker 13:  like under one minute or five minutes and that's amazing. And then if

1237
01:18:17,275 --> 01:18:18,315
Speaker 13:  it's a whim call, great.

1238
01:18:18,575 --> 01:18:22,555
Speaker 1:  If you could assume that every time you got a phone call, the average duration

1239
01:18:22,555 --> 01:18:26,395
Speaker 1:  of it was gonna be like 45 seconds. I think everything about making and

1240
01:18:26,395 --> 01:18:29,475
Speaker 1:  receiving phone calls would get better. That sounds wonder like, Jake, I

1241
01:18:29,475 --> 01:18:32,115
Speaker 1:  just wanna be able to call you and be like, Jake, you'd be, you'd be like

1242
01:18:32,115 --> 01:18:34,915
Speaker 1:  hey. And I'd be like, hey two 30 or three and you'd be like three And I'd

1243
01:18:34,915 --> 01:18:38,275
Speaker 1:  be like cool. The end done, done. We're moving on with our lives. The

1244
01:18:38,275 --> 01:18:41,915
Speaker 13:  No goodbye is a key part. I love that. Just saying cool. Nobody

1245
01:18:41,995 --> 01:18:45,355
Speaker 1:  Ever says goodbye in movies and it drives my wife crazy because every time

1246
01:18:45,355 --> 01:18:47,195
Speaker 1:  we're watching a movie she's like, why didn't they even, they didn't even

1247
01:18:47,195 --> 01:18:49,475
Speaker 1:  say goodbye. And I'm like, well that's not like an interesting part of the

1248
01:18:49,475 --> 01:18:53,315
Speaker 1:  movie. People like small talking on the phone, but I think we should do it.

1249
01:18:53,335 --> 01:18:55,555
Speaker 1:  And you're just like, cool. Hang up, get done

1250
01:18:56,215 --> 01:18:59,165
Speaker 12:  David, you should absolutely start communicating with me this way because

1251
01:18:59,205 --> 01:19:02,765
Speaker 12:  I a hundred percent agree with you. Yes dude. I hate text

1252
01:19:02,765 --> 01:19:06,525
Speaker 12:  messaging. Text messaging is so tedious. The amount of time

1253
01:19:06,545 --> 01:19:10,285
Speaker 12:  and energy that goes into like deciding on a restaurant over text.

1254
01:19:10,545 --> 01:19:14,085
Speaker 12:  No, no. Get on the phone. You'll be done in 30 seconds. Yes.

1255
01:19:14,185 --> 01:19:18,045
Speaker 12:  People are so afraid of phone calls. Yeah. And

1256
01:19:18,045 --> 01:19:20,965
Speaker 12:  it is so much easier to get anything done.

1257
01:19:21,955 --> 01:19:24,375
Speaker 12:  Do it. Yes. Call your friends, it's great

1258
01:19:24,755 --> 01:19:28,615
Speaker 1:  And stop saying how are you? Stop saying hope all

1259
01:19:28,615 --> 01:19:32,495
Speaker 1:  is well. Stop saying what's new? No, if you have a question for

1260
01:19:32,495 --> 01:19:34,895
Speaker 1:  me, ask me the question and then when I've answered your question, hang up

1261
01:19:34,895 --> 01:19:38,815
Speaker 1:  the phone. Which means we need all these companies to make it a louder

1262
01:19:38,825 --> 01:19:41,575
Speaker 1:  noise when you hang up the phone again because I need to know when you've

1263
01:19:41,575 --> 01:19:44,295
Speaker 1:  hung up on me. So that's a, that's a product change I need these companies

1264
01:19:44,295 --> 01:19:47,775
Speaker 1:  to make is like, I want a loud click now that the phone is over

1265
01:19:48,195 --> 01:19:50,255
Speaker 1:  to let me know that you're gone and

1266
01:19:50,255 --> 01:19:54,015
Speaker 13:  That's it. I really miss my Motorola razor where I could, no, sorry,

1267
01:19:54,475 --> 01:19:57,335
Speaker 13:  not the, well I missed the razor. Yes. But what I'm actually talking about

1268
01:19:57,335 --> 01:20:01,215
Speaker 13:  is that I miss my juke because it was even cooler. You remember, you like

1269
01:20:01,215 --> 01:20:04,175
Speaker 13:  flip it up like that and then you'd flip it down like that. It was so cool.

1270
01:20:04,175 --> 01:20:04,455
Speaker 13:  It was like

1271
01:20:04,455 --> 01:20:07,210
Speaker 1:  A cool, you were like, it was like a comb from Grease that you were like

1272
01:20:07,365 --> 01:20:08,085
Speaker 1:  flipping open. Yeah,

1273
01:20:08,705 --> 01:20:12,445
Speaker 13:  It was so cool. And the funniest part is I never even had any music on my

1274
01:20:12,445 --> 01:20:16,125
Speaker 13:  Duke phone. I only bought it because I loved the way you could like, oh

1275
01:20:16,125 --> 01:20:19,285
Speaker 13:  answer a phone call and hang up a phone call. Incredible

1276
01:20:20,105 --> 01:20:23,365
Speaker 1:  The rules. This is good. I'm glad we agree this. I think, I think we have

1277
01:20:23,365 --> 01:20:27,285
Speaker 1:  the power to start a nationwide movement to call like we

1278
01:20:27,285 --> 01:20:28,965
Speaker 1:  text and it will just make everything better.

1279
01:20:29,945 --> 01:20:30,845
Speaker 13:  I'm in. Love

1280
01:20:30,845 --> 01:20:34,565
Speaker 1:  It. Alright, we need to take a break. Thank you both. I, I have no more summer

1281
01:20:34,565 --> 01:20:37,405
Speaker 1:  takes. I've gotten all my feelings out And I appreciate you both being here

1282
01:20:37,705 --> 01:20:40,085
Speaker 1:  to absorb and reflect my feelings back at me.

1283
01:20:40,605 --> 01:20:41,245
Speaker 13:  That's an honor

1284
01:20:41,745 --> 01:20:44,925
Speaker 1:  If, if I'm fired it. It's been a pleasure to do this with both of you.

1285
01:20:45,315 --> 01:20:47,685
Speaker 1:  Alright Hayden, you're gonna stick around for a minute 'cause we have a question

1286
01:20:47,685 --> 01:20:50,925
Speaker 1:  from The Vergecast hotline that you are the perfect person to answer. Jake,

1287
01:20:51,225 --> 01:20:54,005
Speaker 1:  you can go. We're gonna take a break. Thank you Beth. We'll be right back.

1288
01:24:42,805 --> 01:24:45,885
Speaker 1:  lot about ai. We got a bunch of good questions about AI and we're gonna talk

1289
01:24:45,885 --> 01:24:49,325
Speaker 1:  about one in particular. But first I just want to give a shout out to Remy

1290
01:24:49,585 --> 01:24:53,285
Speaker 1:  who sent us a very good question that included this paragraph. I wanted to

1291
01:24:53,285 --> 01:24:56,605
Speaker 1:  say, I love your AI coverage, especially since Hayden joined, who is a star.

1292
01:24:57,275 --> 01:25:00,405
Speaker 1:  Just wanted you, just wanted you to know that's so sweet. Shouts to Remy

1293
01:25:01,025 --> 01:25:04,005
Speaker 1:  who may or may not be Hayden's burner Gmail, but we'll, we'll talk about

1294
01:25:04,005 --> 01:25:07,285
Speaker 1:  that later. So the, let's do a question from the hotline as always. It's

1295
01:25:07,485 --> 01:25:10,965
Speaker 1:  Vergecast The Verge dot com is the email 8 6, 6 version one. One is the

1296
01:25:11,355 --> 01:25:15,285
Speaker 1:  hotline. You can call, you can text me, you can stop Hayden on the

1297
01:25:15,285 --> 01:25:18,205
Speaker 1:  street and yell questions at her, whatever you want. We love hearing all

1298
01:25:18,205 --> 01:25:21,965
Speaker 1:  of your questions. This one comes from, I believe it's pronounced

1299
01:25:22,005 --> 01:25:25,925
Speaker 1:  Philippo, and it says, I listened to David's answer to a listener's question

1300
01:25:25,925 --> 01:25:29,885
Speaker 1:  who asked if Apple might not even believe in ai? The answer was measured

1301
01:25:29,885 --> 01:25:33,605
Speaker 1:  and insightful as usual. Thank you. Philippo, this is not my burner Gmail

1302
01:25:33,845 --> 01:25:37,365
Speaker 1:  I should say, but it inspired one intuition. Shouldn't we differentiate much

1303
01:25:37,435 --> 01:25:41,125
Speaker 1:  more the term ai? Like literally use more words to describe

1304
01:25:41,125 --> 01:25:44,685
Speaker 1:  clearly the different applications and products it fuels slash enables Using

1305
01:25:44,715 --> 01:25:48,045
Speaker 1:  only the general AI term is, I feel doing us a disservice in terms of our

1306
01:25:48,045 --> 01:25:51,245
Speaker 1:  understanding of the technology it's value and use and separating buzz from

1307
01:25:51,245 --> 01:25:55,045
Speaker 1:  reality. This is the thing, Hayden, you, you And I have talked in bits and

1308
01:25:55,045 --> 01:25:58,765
Speaker 1:  pieces about for a long time, but I'm, I'm curious, you're the person on

1309
01:25:58,765 --> 01:26:02,165
Speaker 1:  our team who has to live in this space all day every day.

1310
01:26:02,865 --> 01:26:06,805
Speaker 1:  It really feels to me like we desperately need a new vocabulary to

1311
01:26:06,805 --> 01:26:08,125
Speaker 1:  talk about ai. Do you agree?

1312
01:26:08,665 --> 01:26:12,525
Speaker 13:  Yes, I really do because it is something that it, it's

1313
01:26:12,525 --> 01:26:16,445
Speaker 13:  tough because for me, like colloquially, you know, the public

1314
01:26:16,445 --> 01:26:19,845
Speaker 13:  understands things is just being ai. So I often have to write in

1315
01:26:20,195 --> 01:26:23,725
Speaker 13:  that context, you know, just AI and keeping it simple.

1316
01:26:24,625 --> 01:26:28,565
Speaker 13:  But I remember even when I started on this beat almost six years ago

1317
01:26:28,715 --> 01:26:32,565
Speaker 13:  when I would write articles about ai, and this was before, you know, generative

1318
01:26:32,785 --> 01:26:36,685
Speaker 13:  AI was as much of a thing, chat g PT wasn't out, et cetera. But when

1319
01:26:36,725 --> 01:26:40,485
Speaker 13:  I would write articles about ai, I remember when I would do my fact check

1320
01:26:40,485 --> 01:26:44,125
Speaker 13:  calls, researchers would be like, Hey, you know, you should really

1321
01:26:44,155 --> 01:26:47,725
Speaker 13:  specify the type of AI we're talking about here. AI is a catchall term.

1322
01:26:47,725 --> 01:26:50,405
Speaker 13:  It doesn't really mean anything anymore. It means everything, which means

1323
01:26:50,405 --> 01:26:54,325
Speaker 13:  it means nothing. I of course kind of did just have to say AI because

1324
01:26:54,665 --> 01:26:57,325
Speaker 13:  you know, if I get really jargony right away, no one's gonna know what I'm

1325
01:26:57,325 --> 01:26:59,765
Speaker 13:  talking about. But I do remember I had to write a lot of like

1326
01:27:00,925 --> 01:27:04,845
Speaker 13:  encyclopedia type articles defining a lot of different terms. And I think

1327
01:27:04,845 --> 01:27:08,605
Speaker 13:  that we need more of that. I think we need more mentions of

1328
01:27:08,755 --> 01:27:12,325
Speaker 13:  what the sub categories of AI really are.

1329
01:27:12,825 --> 01:27:16,285
Speaker 13:  You know, people are confused about a GI, they're confused about

1330
01:27:16,615 --> 01:27:20,565
Speaker 13:  super intelligence, they're confused about ai, they're confused about

1331
01:27:21,075 --> 01:27:24,605
Speaker 13:  deep learning, reinforcement learning, recursive

1332
01:27:24,605 --> 01:27:28,525
Speaker 13:  self-improvement, all these terms that really will help people have a

1333
01:27:28,525 --> 01:27:32,285
Speaker 13:  better understanding of how this technology works and

1334
01:27:32,475 --> 01:27:36,245
Speaker 13:  exactly how it responds to you and why it

1335
01:27:36,245 --> 01:27:40,125
Speaker 13:  responds the way it does. I think if we had a better vocabulary for

1336
01:27:40,125 --> 01:27:43,885
Speaker 13:  talking about ai, it would actually be really, really helpful for

1337
01:27:44,275 --> 01:27:48,245
Speaker 13:  combating, you know, some of the misconceptions about

1338
01:27:48,425 --> 01:27:51,485
Speaker 13:  AI that are out there right now with people kind of thinking sometimes their

1339
01:27:51,685 --> 01:27:55,645
Speaker 13:  emergent beings inside it or thinking that it knows more

1340
01:27:55,645 --> 01:27:59,365
Speaker 13:  than it does, or thinking that it's almost sometimes an oracle that can

1341
01:27:59,365 --> 01:28:03,205
Speaker 13:  predict certain things. Like is it a good pattern matcher? Yes.

1342
01:28:03,265 --> 01:28:07,245
Speaker 13:  And can it find amazing insights and trends within a large

1343
01:28:07,245 --> 01:28:10,685
Speaker 13:  amount of data and you know, give great answers. Yes. But

1344
01:28:11,145 --> 01:28:14,205
Speaker 13:  you know, with the rise of, you know, some of the mental health concerns

1345
01:28:14,205 --> 01:28:16,845
Speaker 13:  we've seen recently, I think that would be helped a lot if we could just

1346
01:28:17,065 --> 01:28:20,965
Speaker 13:  use the actual terms and define them on like, you know, how different models

1347
01:28:20,985 --> 01:28:24,685
Speaker 13:  are trained and why, and also these methods are changing

1348
01:28:24,825 --> 01:28:28,165
Speaker 13:  all the time. So it'd be great to just have kind of like a, you know, constantly

1349
01:28:28,565 --> 01:28:32,165
Speaker 13:  updating vocabulary, showing people how AI is

1350
01:28:32,165 --> 01:28:36,045
Speaker 13:  advancing and how fast it's moving and then exactly

1351
01:28:36,225 --> 01:28:39,605
Speaker 13:  how it works. Even though we don't know everything about how it works, we

1352
01:28:39,605 --> 01:28:41,965
Speaker 13:  can explain a lot more than is currently being explained.

1353
01:28:42,595 --> 01:28:45,645
Speaker 1:  Yeah, the, the struggle for me with all of this has been that the

1354
01:28:46,925 --> 01:28:50,725
Speaker 1:  solution just requires so much learning on the

1355
01:28:50,725 --> 01:28:54,125
Speaker 1:  part of the, the consumer, right? That like to

1356
01:28:54,425 --> 01:28:57,685
Speaker 1:  reckon with the differences in how these systems are trained,

1357
01:28:58,435 --> 01:29:02,165
Speaker 1:  it's just a lot to ask of people, right? And it's like, I remember we were

1358
01:29:02,165 --> 01:29:05,845
Speaker 1:  even going through this in like the early days of talking about virtual assistants

1359
01:29:05,845 --> 01:29:08,525
Speaker 1:  and that kind of thing where it's like, okay, I have to teach you the difference

1360
01:29:08,525 --> 01:29:12,125
Speaker 1:  between reinforcement learning and neural networks and

1361
01:29:12,235 --> 01:29:15,685
Speaker 1:  when those things overlap and when they don't. And it's like, I'm, I'm having

1362
01:29:15,685 --> 01:29:18,445
Speaker 1:  to sort of give myself and everybody else like a computer science degree

1363
01:29:18,505 --> 01:29:21,925
Speaker 1:  in order to understand how Google assistant works. And that is,

1364
01:29:22,145 --> 01:29:24,925
Speaker 1:  that's just a lot to ask. And I feel like we're still in that place where

1365
01:29:24,925 --> 01:29:28,845
Speaker 1:  it's like the only way to be more specific than AI is with

1366
01:29:28,925 --> 01:29:31,885
Speaker 1:  a sort of increasingly complex scientific terms. And

1367
01:29:32,885 --> 01:29:35,205
Speaker 1:  I just don't know that that's ever gonna work. And I think this is the thing

1368
01:29:35,205 --> 01:29:38,525
Speaker 1:  that we've struggled with is how do we talk about this stuff in a way that

1369
01:29:38,525 --> 01:29:42,285
Speaker 1:  is like you're saying, accessible and understandable that

1370
01:29:42,285 --> 01:29:45,765
Speaker 1:  doesn't feel like school, but is still accurate.

1371
01:29:46,245 --> 01:29:48,445
Speaker 1:  And I think, like, I know one thing we've talked a lot about here at The

1372
01:29:48,525 --> 01:29:52,365
Speaker 1:  Verge is, is when do we call something AI versus when do we

1373
01:29:52,365 --> 01:29:56,085
Speaker 1:  call it like a large language model versus when do we call it a chat bot,

1374
01:29:56,285 --> 01:30:00,005
Speaker 1:  right? Because like a thing I hate my, my least favorite thing

1375
01:30:00,035 --> 01:30:03,805
Speaker 1:  that everybody does in AI is refer to the AI as if it's like a

1376
01:30:03,805 --> 01:30:07,525
Speaker 1:  character. Oh yeah. That is not a thing. The AI does not

1377
01:30:07,525 --> 01:30:11,405
Speaker 1:  exist. It it, it is either some like interface on top of

1378
01:30:11,425 --> 01:30:15,005
Speaker 1:  ai, maybe it's a chat bot, maybe it's whatever. But the, the AI as

1379
01:30:15,365 --> 01:30:19,285
Speaker 1:  a, as a being is such a bad mental image for people to

1380
01:30:19,285 --> 01:30:21,085
Speaker 1:  have for all the reasons that you're talking about. Or

1381
01:30:21,205 --> 01:30:21,325
Speaker 13:  N

1382
01:30:21,505 --> 01:30:25,485
Speaker 1:  Ai Yeah. Or n ai, like any of these things, it is not, AI is

1383
01:30:25,485 --> 01:30:28,805
Speaker 1:  not an object and we should stop treating it as such, but it is really hard

1384
01:30:28,805 --> 01:30:32,725
Speaker 1:  to know what else to call it. Especially now it feels like in so many

1385
01:30:32,725 --> 01:30:36,485
Speaker 1:  ways that the toothpaste is outta the tube on this stuff and like

1386
01:30:36,815 --> 01:30:40,525
Speaker 1:  maybe that just is gonna be what it's called even though it's wrong, but

1387
01:30:40,525 --> 01:30:41,205
Speaker 1:  it feels wrong

1388
01:30:41,505 --> 01:30:45,365
Speaker 13:  For me it's about kind of peppering And I don't know if this is right, but

1389
01:30:45,365 --> 01:30:49,245
Speaker 13:  what I try to do is pepper in these types of definitions in pieces

1390
01:30:49,245 --> 01:30:53,045
Speaker 13:  that I write whenever I can as like a quick

1391
01:30:53,045 --> 01:30:56,925
Speaker 13:  parenthetical, you know, like when I define a GI in a

1392
01:30:56,925 --> 01:31:00,325
Speaker 13:  piece, like I can't go without, you know, doing a quick like parenthetical

1393
01:31:00,325 --> 01:31:03,805
Speaker 13:  phrase saying what that is. And you know, with reinforcement learning, deep

1394
01:31:03,965 --> 01:31:07,805
Speaker 13:  learning all these methods, it, it's helpful to just have a

1395
01:31:07,805 --> 01:31:11,485
Speaker 13:  quick like, you know, one sentence or like seven

1396
01:31:11,595 --> 01:31:15,525
Speaker 13:  word way of describing it, even if it's oversimplified that can kind

1397
01:31:15,525 --> 01:31:19,005
Speaker 13:  of help people understand what's really going on here and the different

1398
01:31:19,035 --> 01:31:22,405
Speaker 13:  ways in which the tech is advancing and how it changes and how it's trained

1399
01:31:22,405 --> 01:31:25,685
Speaker 13:  and all these things. I think it's hard because sometimes you do oversimplify

1400
01:31:25,685 --> 01:31:29,485
Speaker 13:  it and then people are like, well that's not 100% right. But also, okay,

1401
01:31:29,645 --> 01:31:32,845
Speaker 13:  you're trying to do it in like seven words, so you have to be accurate and

1402
01:31:33,125 --> 01:31:36,845
Speaker 13:  simplify without oversimplifying it. It's, it's tough but I think it's something

1403
01:31:36,845 --> 01:31:40,125
Speaker 13:  we can do if we just like start bringing these terms

1404
01:31:40,835 --> 01:31:44,405
Speaker 13:  into the public eye more. One example is large language models, right? I

1405
01:31:44,605 --> 01:31:48,045
Speaker 13:  remember back when I couldn't say that term without it being too jargony.

1406
01:31:48,115 --> 01:31:51,965
Speaker 13:  Totally. So I just have to say AI now large language models is

1407
01:31:52,365 --> 01:31:56,285
Speaker 13:  a household term. I think maybe I'm just in my own bubble, but I think it

1408
01:31:56,285 --> 01:32:00,245
Speaker 13:  is. And so I can say that without defining what that is now usually in an

1409
01:32:00,245 --> 01:32:03,925
Speaker 13:  article. And so I think we can get there with other terms. So

1410
01:32:04,065 --> 01:32:07,485
Speaker 13:  you know, hopefully we just keep building our vocabulary, do it a little

1411
01:32:07,485 --> 01:32:11,445
Speaker 13:  bit at a time and we get, you know, to where we are with large language

1412
01:32:11,445 --> 01:32:14,005
Speaker 13:  models, with other terms and we just kind of like increase public understanding

1413
01:32:14,005 --> 01:32:17,325
Speaker 13:  of this stuff. Because the cool thing is, like you said, I mean

1414
01:32:17,925 --> 01:32:21,565
Speaker 13:  consumers, we don't wanna put the burden on the consumer to, you know, go

1415
01:32:21,565 --> 01:32:25,005
Speaker 13:  to school basically and learn all this stuff. But the other cool thing is

1416
01:32:25,005 --> 01:32:28,645
Speaker 13:  that consumers do wanna know, you know, people are

1417
01:32:28,655 --> 01:32:32,045
Speaker 13:  super curious about this. And so I think with the combination of

1418
01:32:32,465 --> 01:32:36,405
Speaker 13:  people wanting to know more about what is and isn't

1419
01:32:36,465 --> 01:32:40,085
Speaker 13:  AI and what falls into every category and what's just data science and what's

1420
01:32:40,085 --> 01:32:43,965
Speaker 13:  just, if this then that rules and all these things and us being

1421
01:32:43,965 --> 01:32:46,885
Speaker 13:  able to, you know, give them like kind of simple explanations and then if

1422
01:32:46,885 --> 01:32:50,405
Speaker 13:  you wanna dive deeper, here's more I think we can get there. Yeah.

1423
01:32:50,425 --> 01:32:54,005
Speaker 1:  Is there another term like LLMs? Like if you could just wish another one

1424
01:32:54,005 --> 01:32:57,365
Speaker 1:  into like mainstream acceptability that would make your life as a reporter

1425
01:32:57,365 --> 01:32:59,885
Speaker 1:  and writer easier, what would the next one be?

1426
01:33:00,725 --> 01:33:04,095
Speaker 13:  It's hard because it changes like by the month and by the year, you know,

1427
01:33:04,155 --> 01:33:07,975
Speaker 13:  it really does. Like I remember there was a time when

1428
01:33:07,975 --> 01:33:10,535
Speaker 13:  like deep learning would've been my choice and then I remember there was

1429
01:33:10,535 --> 01:33:14,175
Speaker 13:  a time when reinforcement learning would've been my choice. Mm. But

1430
01:33:14,395 --> 01:33:16,655
Speaker 13:  now I think it would be

1431
01:33:18,295 --> 01:33:22,255
Speaker 13:  recursive self-improvement. Ooh. So yeah, I think

1432
01:33:22,355 --> 01:33:26,015
Speaker 13:  it would be cool if people could know how these models are

1433
01:33:26,155 --> 01:33:29,495
Speaker 13:  making themselves better and also the societal

1434
01:33:29,495 --> 01:33:33,295
Speaker 13:  implications of that and the economic implications of that

1435
01:33:33,295 --> 01:33:36,575
Speaker 13:  and what that might mean for jobs and things like that. I'm hearing that

1436
01:33:36,575 --> 01:33:40,535
Speaker 13:  phrase a lot and it's something that I think it would be great if the

1437
01:33:40,535 --> 01:33:43,135
Speaker 13:  public could know more about. So I would say, yeah, for me, the phrase that

1438
01:33:43,135 --> 01:33:46,935
Speaker 13:  I would want to make a household name changes by the like three month period,

1439
01:33:47,035 --> 01:33:47,895
Speaker 13:  but right now it's that

1440
01:33:48,775 --> 01:33:51,615
Speaker 1:  I think recursive self-improvement is also like a very good memoir title

1441
01:33:51,685 --> 01:33:54,815
Speaker 1:  that you should maybe just lay claim to right now. Perfect.

1442
01:33:54,875 --> 01:33:56,775
Speaker 13:  If Hayden feels copyright it. Yeah.

1443
01:33:57,295 --> 01:34:00,695
Speaker 1:  I love, love it, love it. Alright Hayden, thank you as always, I appreciate

1444
01:34:00,695 --> 01:34:00,815
Speaker 1:  it.

1445
01:34:01,275 --> 01:34:01,695
Speaker 13:  Thanks.

1446
01:34:02,405 --> 01:34:05,655
Speaker 1:  Alright, that is it for The Vergecast. Thank you to everyone who is on the

1447
01:34:05,655 --> 01:34:09,335
Speaker 1:  show today and thank you as always for watching and listening. If you have

1448
01:34:09,335 --> 01:34:13,095
Speaker 1:  thoughts or questions or feelings or want to just like call me

1449
01:34:13,155 --> 01:34:15,735
Speaker 1:  and say something really short and then hang up immediately just to prove

1450
01:34:15,815 --> 01:34:19,215
Speaker 1:  a point, you can always call the hotline. Eight six six VERGE one one. Send

1451
01:34:19,215 --> 01:34:22,655
Speaker 1:  us an email Vergecast at The Verge dot com. I wanna hear all of your summer

1452
01:34:22,655 --> 01:34:26,415
Speaker 1:  takes. Also, if you just have a feeling that is just burning up inside of

1453
01:34:26,415 --> 01:34:30,175
Speaker 1:  you, call me, hit us up And I will either

1454
01:34:30,335 --> 01:34:33,375
Speaker 1:  a hundred percent agree or a hundred percent disagree or just cancel you

1455
01:34:33,535 --> 01:34:37,335
Speaker 1:  entirely. We'll see what happens. Get at us. Like I said, please subscribe

1456
01:34:37,355 --> 01:34:40,415
Speaker 1:  to version history, the first eight episodes. The whole first season is gonna

1457
01:34:40,435 --> 01:34:44,375
Speaker 1:  be on The Verge feed. So if you're hearing or seeing this, odds are

1458
01:34:44,375 --> 01:34:47,655
Speaker 1:  you will encounter version history when it launches on October 5th. But also

1459
01:34:47,655 --> 01:34:51,415
Speaker 1:  please subscribe to the feed. That's an important thing that we look at in

1460
01:34:51,415 --> 01:34:54,175
Speaker 1:  terms of like whether we get to keep making this thing. So please subscribe.

1461
01:34:54,815 --> 01:34:58,125
Speaker 1:  I think you're gonna really like this show. Also, The Vergecast is The Verge

1462
01:34:58,125 --> 01:35:00,845
Speaker 1:  production and part of the Vox Media podcast network. The show is produced

1463
01:35:00,845 --> 01:35:04,725
Speaker 1:  by Eric Gomez, Brandon Kiefer, and Travis Uck. We will all be back on

1464
01:35:04,725 --> 01:35:08,165
Speaker 1:  Friday to talk about whatever's going on in the A OI world because there's

1465
01:35:08,165 --> 01:35:11,205
Speaker 1:  always stuff going on in the AI world. We have some Google News coming, we

1466
01:35:11,205 --> 01:35:15,125
Speaker 1:  have some Amazon news coming next week. Pure chaos in the

1467
01:35:15,125 --> 01:35:18,605
Speaker 1:  tech industry right now. And we have an awful lot to talk about until then,

1468
01:35:19,035 --> 01:35:19,605
Speaker 1:  rock and Roll

1469
01:35:24,425 --> 01:35:24,925
Speaker 21:  And Doug

1470
01:35:25,115 --> 01:35:25,405
Speaker 22:  Limu

