1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: c10580b7-1638-4e5e-b8d4-e753b502485d
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/3931055994144455590/7960551584400941355/s93290-US-5753s-1734087880.mp3
Description: Nilay, David, and The Verge's Richard Lawler talk about a big week in AI news. First, they go over all the latest on Google's Gemini 2.0 launch, and try to figure out whether Project Astra and Project Mariner will ever turn into products people use. They also discuss OpenAI's release (and un-release) of Sora, the new Reddit Answers tool, and what's new in iOS 18.2. Finally, in the lightning round, there's talk of YouTube, Instagram, TikTok, Sonos, and Cruise. There also is and isn't talk of quantum computing. Because that's possible now.


2
00:01:58,655 --> 00:02:02,365
Speaker 3:  Hello and welcome to Our Chest, the flagship podcast of the Agentic era.

3
00:02:03,345 --> 00:02:05,925
Speaker 3:  That's where you just sent us requests and David has to do that,

4
00:02:07,305 --> 00:02:08,245
Speaker 4:  But we say it's AI

5
00:02:08,755 --> 00:02:12,405
Speaker 3:  Like every other AI company. I want to be clear that is how they all work.

6
00:02:13,075 --> 00:02:16,405
Speaker 3:  Tesla's robotaxis, fleet of human operators waiting in just a guy, the background

7
00:02:16,405 --> 00:02:19,525
Speaker 3:  guy. Yeah, to pounce. It's just how it works. There's a lot going on this

8
00:02:19,525 --> 00:02:23,045
Speaker 3:  week. I'm your friend Neli. David Pierce is here. Hello. Waiting to take

9
00:02:23,045 --> 00:02:25,965
Speaker 3:  your commands. David, memorize this list of names

10
00:02:26,985 --> 00:02:27,965
Speaker 3:  and then make new names.

11
00:02:28,075 --> 00:02:31,645
Speaker 4:  What I want to talk about, which is just your bad websites made

12
00:02:31,865 --> 00:02:34,445
Speaker 4:  AI necessary. Yeah. This is what I've come to.

13
00:02:34,715 --> 00:02:36,485
Speaker 3:  Richard Lawler is here.

14
00:02:37,245 --> 00:02:41,005
Speaker 5:  I love ai. I'm a big AI guy now I've moved on from crypto.

15
00:02:41,755 --> 00:02:42,845
Speaker 5:  It's all about AI now.

16
00:02:42,995 --> 00:02:43,485
Speaker 3:  Richard

17
00:02:43,625 --> 00:02:45,605
Speaker 4:  Big AI guy. That's what everybody says in ai.

18
00:02:45,605 --> 00:02:49,405
Speaker 3:  I'm saying Richard, you have not had AI change that car behind you

19
00:02:49,405 --> 00:02:50,885
Speaker 3:  from Mercedes to Ferrari, yet

20
00:02:52,165 --> 00:02:55,995
Speaker 5:  There is no Mercedes, there is only Ferrari and but therefore

21
00:02:56,055 --> 00:02:57,235
Speaker 5:  it is a Ferrari already.

22
00:02:58,015 --> 00:03:01,285
Speaker 3:  That's a be wherever Hamilton goes, that's a Ferrari.

23
00:03:02,075 --> 00:03:05,685
Speaker 3:  That is also in many ways how AI is being defined. Whatever this computer

24
00:03:05,685 --> 00:03:09,235
Speaker 3:  does is ai. They should get, they should get Lewis

25
00:03:09,475 --> 00:03:10,955
Speaker 3:  Hamilton to be this. You know what he is.

26
00:03:11,115 --> 00:03:14,635
Speaker 5:  I love just remembering three commands to be able to tell my computer to

27
00:03:14,635 --> 00:03:17,355
Speaker 5:  do one simple thing. It's just the best way to use it.

28
00:03:17,735 --> 00:03:20,955
Speaker 3:  You know? Wait, there is a lot of like dumb crypto money in F1. Is there

29
00:03:20,955 --> 00:03:22,795
Speaker 3:  a lot of dumb AI money in F1 yet?

30
00:03:23,775 --> 00:03:26,235
Speaker 5:  Not yet. They've been kind of late on that. I don't know what's going on.

31
00:03:26,295 --> 00:03:26,715
Speaker 5:  That's

32
00:03:26,715 --> 00:03:26,795
Speaker 4:  True.

33
00:03:27,265 --> 00:03:30,515
Speaker 3:  Well, the new season, it, A new season is coming, a new sponsorship opportunity

34
00:03:30,515 --> 00:03:34,275
Speaker 3:  is await. I will say that Chrome sponsoring the McLaren and then

35
00:03:34,415 --> 00:03:37,395
Speaker 3:  making the cart literally Chrome genius.

36
00:03:38,305 --> 00:03:41,635
Speaker 4:  It's very good in terms of like just pure

37
00:03:42,655 --> 00:03:46,635
Speaker 4:  actual like branding of the thing inside of the actual product, like

38
00:03:46,635 --> 00:03:48,115
Speaker 4:  crushed it and McLaren, yes,

39
00:03:48,115 --> 00:03:51,195
Speaker 5:  Chrome wheels are a great idea, especially when they're not Chrome, but they

40
00:03:51,225 --> 00:03:52,555
Speaker 5:  look like Google Chrome. Yeah,

41
00:03:52,975 --> 00:03:56,955
Speaker 3:  No, but they added actual Chrome to the car that was this year's vague innovation.

42
00:03:57,345 --> 00:04:00,995
Speaker 3:  They, they put literal chrome on the car, which is not what you think about

43
00:04:00,995 --> 00:04:04,555
Speaker 3:  when you think of the F1 car. Like what if this looked like a 1950s cab?

44
00:04:04,625 --> 00:04:08,075
Speaker 3:  It's very good. That's all I'm saying. Alright. There's a lot of news, actual

45
00:04:08,095 --> 00:04:12,035
Speaker 3:  AI news this week, not just Chrome in all senses of

46
00:04:12,035 --> 00:04:15,635
Speaker 3:  the word. And a lot of it is Google and David, you wrote a,

47
00:04:15,915 --> 00:04:19,555
Speaker 3:  a bunch of these little bits and Bobs Gemini 2.0 is out.

48
00:04:19,975 --> 00:04:23,555
Speaker 3:  Google is making some mixed reality noises.

49
00:04:23,945 --> 00:04:27,115
Speaker 3:  There's an Android mixed reality thing yet again. What?

50
00:04:28,215 --> 00:04:30,515
Speaker 3:  Be honest about that. They've, they've tried this before. They're back again.

51
00:04:31,075 --> 00:04:34,435
Speaker 3:  There's a new jus AI for code. What's, what's all going on here, David?

52
00:04:34,745 --> 00:04:38,395
Speaker 4:  Okay, so let's, let's break all that into three pieces

53
00:04:39,255 --> 00:04:41,875
Speaker 4:  and we, we should talk about all of them. And I, I think the three pieces

54
00:04:42,095 --> 00:04:46,075
Speaker 4:  are Gemini 2.0, there's Astra and

55
00:04:46,075 --> 00:04:49,755
Speaker 4:  Mariner, which are kind of like overlapping with Gemini 2.0 but are, are

56
00:04:50,545 --> 00:04:53,315
Speaker 4:  like products and they're interesting and we should talk about them and then

57
00:04:53,855 --> 00:04:57,515
Speaker 4:  we should talk about the XR stuff at the end. It's called Android XR.

58
00:04:57,875 --> 00:05:01,795
Speaker 4:  I have many thoughts about the decision to use xr, but we, we should

59
00:05:01,795 --> 00:05:04,555
Speaker 4:  talk about this, but let's, let's start with Gemini. So the, the big news

60
00:05:04,555 --> 00:05:08,515
Speaker 4:  of this week was Google is starting to roll out Gemini 2.0, which

61
00:05:08,535 --> 00:05:12,355
Speaker 4:  is the successor to 1.5, which I think it first launched

62
00:05:12,455 --> 00:05:15,835
Speaker 4:  in February. So like nine months of

63
00:05:15,835 --> 00:05:19,715
Speaker 4:  development later. This is the new thing. And what

64
00:05:19,715 --> 00:05:23,555
Speaker 4:  de ve told me, who he runs Deep Mind at Google

65
00:05:23,575 --> 00:05:26,635
Speaker 4:  and is in charge of all their AI stuff. And by the way, I talked to him earlier

66
00:05:26,665 --> 00:05:30,535
Speaker 4:  this week while he was in Sweden accepting a Nobel Prize.

67
00:05:30,715 --> 00:05:34,375
Speaker 4:  He like called me from his hotel room before the Nobel Gala,

68
00:05:34,545 --> 00:05:38,535
Speaker 4:  which is like the worst power dynamic of an interview I've ever had

69
00:05:38,535 --> 00:05:39,295
Speaker 4:  in my entire life.

70
00:05:40,995 --> 00:05:43,215
Speaker 4:  It was really something, it's like, I don't know how to tell you you're wrong

71
00:05:43,215 --> 00:05:46,295
Speaker 4:  on any of this because you're literally, you're going to get a Nobel Prize

72
00:05:46,295 --> 00:05:49,655
Speaker 4:  tonight. But anyway, what he told me is that basically

73
00:05:50,365 --> 00:05:54,055
Speaker 4:  what Gemini 2.0 flash is, is

74
00:05:54,055 --> 00:05:57,965
Speaker 4:  roughly performance equivalent to what Gemini 1.5 Pro

75
00:05:58,505 --> 00:06:01,725
Speaker 4:  is. Sure. So the way to think about it is he it's, it's like a full step

76
00:06:01,865 --> 00:06:05,725
Speaker 4:  up in terms of like efficiency and speed and latency, which is

77
00:06:05,725 --> 00:06:09,325
Speaker 4:  stuff that really, really matters with still the same amount of

78
00:06:09,325 --> 00:06:11,565
Speaker 4:  performance as the pro level tier from last time.

79
00:06:11,595 --> 00:06:15,365
Speaker 3:  Well, I mean just to unpack some Google product names here. Flashes the

80
00:06:15,485 --> 00:06:18,765
Speaker 3:  little model that's supposed to run on phones. That's

81
00:06:18,765 --> 00:06:22,125
Speaker 4:  Right. So they they call it the workhorse model. Okay. It's the one most

82
00:06:22,125 --> 00:06:24,285
Speaker 4:  people encounter most of the time basically.

83
00:06:24,345 --> 00:06:27,085
Speaker 3:  Got it. But so that's not the phones one. That's just the sort of like, here's

84
00:06:27,085 --> 00:06:27,925
Speaker 3:  the one you get Flash.

85
00:06:28,115 --> 00:06:30,965
Speaker 4:  There's one below Flash that I can't remember the name of, but it is, it

86
00:06:30,965 --> 00:06:34,125
Speaker 4:  is like, it's the, it's the main one. Like when you use Gemini

87
00:06:34,905 --> 00:06:37,965
Speaker 4:  now you're using 1.5 flash almost certainly. Okay.

88
00:06:38,245 --> 00:06:40,925
Speaker 3:  But Nano is the one on the phone. Nano is on N is the one on the phone. Okay.

89
00:06:40,925 --> 00:06:43,765
Speaker 3:  I just wanna be Google product names also. I just wanna point out they named

90
00:06:43,765 --> 00:06:47,725
Speaker 3:  it Flash. Yeah. Like in the history of technology names to Reanimate,

91
00:06:49,335 --> 00:06:50,785
Speaker 3:  they picked Flash. Yeah.

92
00:06:51,055 --> 00:06:52,225
Speaker 4:  It's always cool. You didn't like

93
00:06:52,225 --> 00:06:52,705
Speaker 3:  New grounds.

94
00:06:55,055 --> 00:06:58,425
Speaker 3:  Look, Homestar runner was great, right? And if these, if these AI systems

95
00:06:58,525 --> 00:07:00,265
Speaker 3:  can deliver me Homestar runner, that would be amazing

96
00:07:00,485 --> 00:07:04,465
Speaker 4:  If Google made Homestar runner the official mascot of all of its AI

97
00:07:04,485 --> 00:07:07,265
Speaker 4:  and like that was the voice and personality I was talking to, I would be

98
00:07:07,265 --> 00:07:10,265
Speaker 4:  all in on ai honestly. Like done lock it down, get these

99
00:07:10,265 --> 00:07:13,545
Speaker 3:  Woke eyes outta here, put strong that in. Let's make this happen. All right,

100
00:07:13,545 --> 00:07:14,225
Speaker 3:  keep going. Exactly.

101
00:07:15,165 --> 00:07:19,105
Speaker 4:  But so, so yeah, so it's, it's kind of a, a magnitude leap in power for

102
00:07:19,105 --> 00:07:22,905
Speaker 4:  them. It's also like natively doing a bunch of things that

103
00:07:22,905 --> 00:07:26,665
Speaker 4:  were separate in products before. So Gemini 2.0 can now do

104
00:07:26,685 --> 00:07:29,905
Speaker 4:  images natively, which used to be a thing Google did with a separate model.

105
00:07:30,325 --> 00:07:33,345
Speaker 4:  It can do audio natively, which is a thing it was doing in a separate model.

106
00:07:33,605 --> 00:07:37,105
Speaker 4:  So there's just a lot of this like multimodal stuff coming

107
00:07:37,335 --> 00:07:41,265
Speaker 4:  into Gemini, which is a big deal for Google in a bunch of ways.

108
00:07:41,445 --> 00:07:45,025
Speaker 4:  But the main thing is like this is going to be the model that Google

109
00:07:45,115 --> 00:07:48,905
Speaker 4:  tries to put everywhere. It's the one that they're gonna try to sell to cloud

110
00:07:49,385 --> 00:07:51,505
Speaker 4:  customers who wanna do all this kind of stuff. It's the one they're gonna

111
00:07:51,505 --> 00:07:54,305
Speaker 4:  put into AI overviews and search. It's the one they're gonna put into Gmail.

112
00:07:54,375 --> 00:07:57,785
Speaker 4:  It's the one that's gonna be in Gemini. Like increasingly Google

113
00:07:58,365 --> 00:08:01,585
Speaker 4:  is trying to do kind of the opposite of what open AI is doing. Open AI is

114
00:08:01,585 --> 00:08:03,465
Speaker 4:  like we have a bunch of models for a bunch of different things and you can

115
00:08:03,465 --> 00:08:06,945
Speaker 4:  use them all and they all kind of interoperate, but it is like different

116
00:08:06,945 --> 00:08:09,705
Speaker 4:  models for different needs and Google is just like Gemini

117
00:08:10,725 --> 00:08:14,145
Speaker 4:  and that is very much like how this company wants to approach ai.

118
00:08:14,605 --> 00:08:18,425
Speaker 4:  And I get the real sense they see this as a big turn

119
00:08:18,565 --> 00:08:19,545
Speaker 4:  in that direction.

120
00:08:19,575 --> 00:08:23,505
Speaker 3:  Wait, lemme ask just a foundational question here. Yeah. There's a

121
00:08:23,505 --> 00:08:26,785
Speaker 3:  big conversation in the AI industry about scaling laws and whether training

122
00:08:26,845 --> 00:08:30,665
Speaker 3:  the next model will result in more capability. You

123
00:08:30,675 --> 00:08:34,465
Speaker 3:  wrote very disparagingly about OpenAI trying to redefine a GI down.

124
00:08:34,625 --> 00:08:36,905
Speaker 3:  I think you actually said we should just make fun of Sam Alman on this show

125
00:08:37,025 --> 00:08:37,625
Speaker 3:  a couple weeks ago.

126
00:08:37,705 --> 00:08:38,905
Speaker 4:  I did. I still believe that. Right.

127
00:08:38,905 --> 00:08:42,785
Speaker 3:  So we're trying to drag the word a GI down to the ground and just be

128
00:08:42,785 --> 00:08:45,535
Speaker 3:  like, anything this F1 car is a GI.

129
00:08:46,805 --> 00:08:50,375
Speaker 3:  Sure. Then there's Google and it's their newest

130
00:08:50,545 --> 00:08:54,295
Speaker 3:  model is the same capability but more efficient,

131
00:08:55,295 --> 00:08:59,085
Speaker 3:  which seems important, but is kind of lost in the a GI

132
00:08:59,085 --> 00:09:02,765
Speaker 3:  conversation. 'cause all of that is like, can we make a GI

133
00:09:02,865 --> 00:09:06,085
Speaker 3:  out of a bunch of Nvidia chips that we have today?

134
00:09:06,705 --> 00:09:09,085
Speaker 3:  If we throw more data at them, will they get smarter? And Google's like,

135
00:09:09,085 --> 00:09:10,405
Speaker 3:  it's the same smart but more efficient.

136
00:09:10,675 --> 00:09:14,085
Speaker 4:  Yeah. So I, I actually talked a bunch about this with Des and I'm very glad

137
00:09:14,085 --> 00:09:17,085
Speaker 4:  you brought this up because I've been thinking a lot about this in the context

138
00:09:17,145 --> 00:09:20,525
Speaker 4:  of these announcements. And I think Google's stance seems to be

139
00:09:21,675 --> 00:09:25,525
Speaker 4:  that there is more headroom to be had he, he kind of

140
00:09:25,525 --> 00:09:29,445
Speaker 4:  allowed that it's slowing down that the idea of this stuff, just

141
00:09:29,445 --> 00:09:33,325
Speaker 4:  like linearly getting better with every new model is probably not

142
00:09:33,325 --> 00:09:36,405
Speaker 4:  true. But he did say he's like, there are still performance gains to be had.

143
00:09:36,405 --> 00:09:39,485
Speaker 4:  There's new stuff we can learn how to do. And he also said that actually

144
00:09:39,915 --> 00:09:43,645
Speaker 4:  what we need is, is the new technique. We don't need

145
00:09:43,675 --> 00:09:47,365
Speaker 4:  more, we need new is kind of what he kept saying that like the, the way that

146
00:09:47,725 --> 00:09:50,605
Speaker 4:  Transformers change the way we think about this stuff, like we need another

147
00:09:50,605 --> 00:09:54,405
Speaker 4:  one of those if we're gonna unlock like the real next step change in ai.

148
00:09:54,705 --> 00:09:57,525
Speaker 4:  And he seems to think those are out there and exist and they're working on

149
00:09:57,525 --> 00:10:00,125
Speaker 4:  them and he has some theories he didn't want to tell me about them, but like

150
00:10:00,695 --> 00:10:04,245
Speaker 4:  there is room upwards, but it is not just bigger

151
00:10:04,305 --> 00:10:08,285
Speaker 4:  models. What's next? And I think what we're seeing right now, both with

152
00:10:08,355 --> 00:10:12,005
Speaker 4:  what Google is doing and with what OpenAI is doing with this 12 days of

153
00:10:12,125 --> 00:10:15,245
Speaker 4:  shipments thing, is everybody is trying to figure out now how to make these

154
00:10:15,245 --> 00:10:19,205
Speaker 4:  things products. Like how do I get these things in front of

155
00:10:19,205 --> 00:10:23,125
Speaker 4:  you in a way that is useful and valuable and worth paying

156
00:10:23,185 --> 00:10:27,085
Speaker 4:  for, or at least using and in some like very meaningful way

157
00:10:27,235 --> 00:10:31,205
Speaker 4:  pays off as a business for these companies that are just pouring money

158
00:10:31,205 --> 00:10:34,125
Speaker 4:  down the toilet making these things work. So

159
00:10:35,185 --> 00:10:39,125
Speaker 4:  for Google, one thing I liked about the way demos is framing it is

160
00:10:39,125 --> 00:10:42,125
Speaker 4:  like Google works on all this stuff at sort of a technological

161
00:10:42,385 --> 00:10:45,645
Speaker 4:  infrastructural level and then Google

162
00:10:46,545 --> 00:10:50,165
Speaker 4:  is Google's biggest customer, right? So there's like, he was like, we we

163
00:10:50,165 --> 00:10:52,765
Speaker 4:  think of this as, and this is like you're, you're making a face. This is

164
00:10:52,765 --> 00:10:54,165
Speaker 4:  what everybody says about their stuff. Yeah. If

165
00:10:54,165 --> 00:10:56,925
Speaker 3:  You're listening to this in your car just to match me rolling my eyes so

166
00:10:56,925 --> 00:10:57,965
Speaker 3:  hard, your car crashed.

167
00:10:58,365 --> 00:11:01,725
Speaker 4:  Everyone ever on decoder has said that about their companies. But like it's

168
00:11:01,725 --> 00:11:04,045
Speaker 4:  true in a lot of cases, and I think it's true in this case too, that like

169
00:11:04,555 --> 00:11:07,445
Speaker 4:  what Google needs is for

170
00:11:08,265 --> 00:11:12,165
Speaker 4:  Gemini and Gmail to not run Google out of business because it's so

171
00:11:12,165 --> 00:11:15,685
Speaker 4:  expensive to operate. Right? Right. And so, so the idea of making this stuff

172
00:11:15,785 --> 00:11:19,605
Speaker 4:  faster and more efficient and lower latency becomes

173
00:11:19,605 --> 00:11:23,085
Speaker 4:  really meaningful. Not only because Google is in this like incredible arms

174
00:11:23,085 --> 00:11:26,965
Speaker 4:  race with every other cloud company to offer AI services and

175
00:11:26,995 --> 00:11:29,885
Speaker 4:  this stuff is being really quickly commoditized. So being able to say we

176
00:11:29,885 --> 00:11:33,045
Speaker 4:  are the cheapest and the fastest is becoming very powerful very quickly,

177
00:11:33,985 --> 00:11:37,685
Speaker 4:  but also like it matters to Google's own

178
00:11:37,685 --> 00:11:41,485
Speaker 4:  products in a pretty big way. So like he kept getting really excited about

179
00:11:41,485 --> 00:11:44,885
Speaker 4:  efficiency and I realized he is like, oh no, like you're, you're serious.

180
00:11:44,995 --> 00:11:48,925
Speaker 4:  Like this stuff, this means you can build this into products in a

181
00:11:48,925 --> 00:11:52,445
Speaker 4:  way that is like, that works at scale without just

182
00:11:52,965 --> 00:11:54,365
Speaker 4:  absolutely hemorrhaging cash the whole way.

183
00:11:54,475 --> 00:11:56,965
Speaker 3:  Yeah. But I mean we should talk about those products and and we will, but

184
00:11:57,005 --> 00:12:00,155
Speaker 3:  I just, I just wanna say this very clearly and I'm the person who interviews

185
00:12:00,155 --> 00:12:03,715
Speaker 3:  all the CEOs about their costs. I don't care. Right? Like

186
00:12:03,975 --> 00:12:07,945
Speaker 3:  we made it cheaper to hallucinate is like a, I mean, honestly

187
00:12:07,945 --> 00:12:10,985
Speaker 3:  there was a time in my life where that would've been a very compelling pitch,

188
00:12:12,245 --> 00:12:16,165
Speaker 3:  but like, they're not better. Right? They're just cheaper to

189
00:12:16,165 --> 00:12:19,005
Speaker 3:  run. And I, I I feel like I'm just stuck there. Like that's a big deal. I

190
00:12:19,065 --> 00:12:22,245
Speaker 3:  I'm not discounting, it's a, it's not a big deal to put the thing more places

191
00:12:22,265 --> 00:12:25,005
Speaker 3:  and see what kind of product you can build. And maybe all anybody wants is

192
00:12:25,005 --> 00:12:28,645
Speaker 3:  to hallucinate some emails like great, but they're not more capable

193
00:12:28,865 --> 00:12:32,375
Speaker 3:  yet. Like the big innovation here is that it, it got cheaper.

194
00:12:32,635 --> 00:12:36,175
Speaker 5:  If Google wants to save some money on ai, they could just turn off the AI

195
00:12:36,375 --> 00:12:40,095
Speaker 5:  overviews on my search results. Like I don't know how many pennies that would,

196
00:12:40,095 --> 00:12:43,495
Speaker 5:  that would save them, but it's gotta be at least 5 cents considering the

197
00:12:43,495 --> 00:12:44,295
Speaker 5:  amount of services I do.

198
00:12:44,435 --> 00:12:47,975
Speaker 4:  It was really funny, Google, I think it was soon, wrote a blog post with

199
00:12:47,975 --> 00:12:51,935
Speaker 4:  all these announcements that was like, you know, there are now more than

200
00:12:51,975 --> 00:12:55,535
Speaker 4:  a billion people are experiencing AI overviews. And I was like, that's such

201
00:12:55,615 --> 00:12:59,135
Speaker 4:  a different way of putting it than more than a billion people really are

202
00:12:59,135 --> 00:13:00,335
Speaker 4:  excited to use AI

203
00:13:00,655 --> 00:13:03,335
Speaker 3:  Overviews. We have foisted this upon a billion. Yeah,

204
00:13:03,335 --> 00:13:07,055
Speaker 4:  Exactly. It's like this is, this is one of the fastest growing search enhancements

205
00:13:07,085 --> 00:13:10,135
Speaker 4:  ever. And I just kept thinking about U YouTube being like, this is our most

206
00:13:10,135 --> 00:13:13,695
Speaker 4:  successful album ever because we shoved it onto everyone's iPhones. It's

207
00:13:13,695 --> 00:13:15,975
Speaker 4:  like, I don't know that that's the flex you think it is guys.

208
00:13:16,125 --> 00:13:18,855
Speaker 3:  Well so we should talk about the product. I just, that's the thing that really

209
00:13:18,905 --> 00:13:22,855
Speaker 3:  leapt out to me is we have very quickly moved on from

210
00:13:23,285 --> 00:13:26,935
Speaker 3:  talking about the models getting more capable and like we're worried that

211
00:13:26,935 --> 00:13:30,695
Speaker 3:  these are gonna be super intelligence level extinction events for us

212
00:13:31,115 --> 00:13:34,015
Speaker 3:  to maybe Sam Altman will just say it's a GI 'cause he wants to,

213
00:13:36,135 --> 00:13:38,695
Speaker 3:  'cause that'll get him out of a Microsoft contract or something will happen

214
00:13:38,715 --> 00:13:42,695
Speaker 3:  and it'll boost his stock price. And when they go public to we've made

215
00:13:42,695 --> 00:13:45,335
Speaker 3:  it cheaper, we can put it more products and hopefully we can all start making

216
00:13:45,335 --> 00:13:48,795
Speaker 3:  some money and that that's fine. Like

217
00:13:49,165 --> 00:13:52,595
Speaker 3:  great, like very cool. The products still aren't there, they're not very

218
00:13:52,595 --> 00:13:56,235
Speaker 3:  compelling yet. Maybe some, someone just has to figure it out

219
00:13:56,495 --> 00:14:00,475
Speaker 3:  and then it, it just feels like I keep making the comparison to

220
00:14:00,475 --> 00:14:03,475
Speaker 3:  Bluetooth. It's like they keep telling me Bluetooth is awesome. And I'm like,

221
00:14:03,635 --> 00:14:07,505
Speaker 3:  but these headphones are bad. And we we're just stuck in that loop. So tell

222
00:14:07,505 --> 00:14:09,505
Speaker 3:  me, I guess, tell me about the headphones. Are the headphones any good?

223
00:14:10,935 --> 00:14:13,905
Speaker 4:  Well, okay, let me describe the headphones to you. Okay. Google launched

224
00:14:13,905 --> 00:14:14,945
Speaker 4:  four new pairs of headphones.

225
00:14:16,885 --> 00:14:20,825
Speaker 4:  One is Project Astra, which we've seen before. We've talked about

226
00:14:20,825 --> 00:14:24,785
Speaker 4:  it a bunch on the show. It's, it's basically like the most sort of

227
00:14:24,805 --> 00:14:28,465
Speaker 4:  all encompassing, ambitious version of Google's

228
00:14:28,605 --> 00:14:32,425
Speaker 4:  AI idea. It's visual, it's listening, it has memory. It's,

229
00:14:32,425 --> 00:14:35,945
Speaker 4:  it's the thing you like walk around your house with and then you're like,

230
00:14:35,945 --> 00:14:38,225
Speaker 4:  where did I leave my glasses? And it tells you because the camera saw your

231
00:14:38,225 --> 00:14:42,105
Speaker 4:  glasses. Like that's the idea. They're able to do some new stuff

232
00:14:42,105 --> 00:14:45,625
Speaker 4:  on there. It's also now connected to things like Google Maps, which is cool.

233
00:14:45,815 --> 00:14:48,905
Speaker 4:  Yeah. So there's like, they're starting to be able to plug some of these

234
00:14:48,905 --> 00:14:52,110
Speaker 4:  things in and that's where like again, having Gemini underneath all of this

235
00:14:52,110 --> 00:14:56,085
Speaker 4:  stuff makes that doable in a way that's been much harder in the past.

236
00:14:57,145 --> 00:15:00,845
Speaker 4:  So that's one thing. Number two is what's called Project Mariner.

237
00:15:00,945 --> 00:15:04,645
Speaker 4:  And this is, it's a Chrome extension. Google calls it an experiment. They

238
00:15:04,645 --> 00:15:08,485
Speaker 4:  have a million different terms that all mean some version of prototype.

239
00:15:08,665 --> 00:15:09,005
Speaker 4:  We can

240
00:15:09,005 --> 00:15:10,405
Speaker 3:  Kill this without you yelling at us,

241
00:15:10,725 --> 00:15:14,325
Speaker 4:  Right? This is like the most prototype you prototype, but it is like

242
00:15:14,625 --> 00:15:18,245
Speaker 4:  an agent in the way that we've been talking about agents for a while in that

243
00:15:18,265 --> 00:15:22,005
Speaker 4:  it can like go and browse the web and do things for you.

244
00:15:23,255 --> 00:15:27,125
Speaker 4:  Kylie Robinson and our team got some demos of this and her take

245
00:15:27,125 --> 00:15:30,885
Speaker 4:  was basically, it works, it's really slow,

246
00:15:31,235 --> 00:15:35,125
Speaker 4:  it's kind of wonky. I'm not sure this is meaningfully better than me just

247
00:15:35,125 --> 00:15:39,045
Speaker 4:  like looking at some web pages myself. But one of the demos they give is

248
00:15:39,045 --> 00:15:42,965
Speaker 4:  like, look at these webpage and find the contact emails for me and it will

249
00:15:43,165 --> 00:15:46,965
Speaker 4:  actually go and literally click around the pages and try to find the

250
00:15:46,965 --> 00:15:47,805
Speaker 4:  email for you. Can,

251
00:15:47,825 --> 00:15:50,045
Speaker 5:  Can we talk about that one for a second? That was the one I was talking about

252
00:15:50,175 --> 00:15:53,285
Speaker 5:  right at the beginning. Yeah, the request and, and you've gotta see the,

253
00:15:53,285 --> 00:15:56,925
Speaker 5:  the video and we put a screenshot of it on the site, but the request is,

254
00:15:57,105 --> 00:16:01,045
Speaker 5:  is absurd to me. It just starts off with memorize this list. What does

255
00:16:01,365 --> 00:16:05,165
Speaker 5:  memorize mean to a bot? Like if I gave you a list, I assume that

256
00:16:05,285 --> 00:16:08,565
Speaker 5:  you remember it now If you are a computer because that's what you do.

257
00:16:08,745 --> 00:16:10,605
Speaker 3:  No, no, no, these computers are on drugs, Richard.

258
00:16:11,985 --> 00:16:15,685
Speaker 5:  And now I have to tell the computer to memorize and then somewhere else in,

259
00:16:15,685 --> 00:16:19,565
Speaker 5:  in the command it also said, remember again, so now I have to remind you

260
00:16:19,905 --> 00:16:23,445
Speaker 5:  to remember this thing to go do a thing. And then in the context box it

261
00:16:23,725 --> 00:16:27,165
Speaker 5:  tells you the results are unreliable, like right below it. So I'm telling

262
00:16:27,225 --> 00:16:31,165
Speaker 5:  an unreliable party instructions that it can't follow. If I was

263
00:16:31,165 --> 00:16:34,845
Speaker 5:  gonna have someone screw up a task, I would just do it myself. Like, like

264
00:16:34,845 --> 00:16:36,205
Speaker 5:  what is AI doing from you? But

265
00:16:36,305 --> 00:16:38,405
Speaker 3:  No, this lets you screw up higher value tasks,

266
00:16:39,055 --> 00:16:39,765
Speaker 5:  Right? Oh, it

267
00:16:39,875 --> 00:16:43,605
Speaker 3:  This fails to get a bunch of email addresses while you like, you know, crash

268
00:16:43,685 --> 00:16:45,245
Speaker 3:  a bike like you're doing something

269
00:16:45,245 --> 00:16:49,085
Speaker 4:  Else. There was a good one on there too in the, in the Project Astra

270
00:16:49,395 --> 00:16:53,365
Speaker 4:  demo video. It, it starts and ends with a guy

271
00:16:53,365 --> 00:16:56,645
Speaker 4:  trying to get into an apartment building and the first thing he does is he's

272
00:16:56,645 --> 00:17:00,325
Speaker 4:  like, look in my email and find the, the code to this

273
00:17:00,565 --> 00:17:04,445
Speaker 4:  front door and then remember it. And it's like I had the same reaction. It's

274
00:17:04,445 --> 00:17:07,645
Speaker 4:  like, first of all, it's just in my email. Like it can just look in my email

275
00:17:07,645 --> 00:17:11,485
Speaker 4:  every time who is remembering anything. And then at the end he's like, what

276
00:17:11,485 --> 00:17:13,925
Speaker 4:  was that code? And it's like the code you asked me to remember and it's like,

277
00:17:13,925 --> 00:17:17,445
Speaker 4:  no, it's still just the code in my email. Like what are we accomplishing?

278
00:17:17,545 --> 00:17:21,085
Speaker 4:  You just, just look at the email again If you want to. It's fine. Like,

279
00:17:21,085 --> 00:17:23,925
Speaker 5:  Now I have to remember the conversation I'm having with the computer about

280
00:17:23,925 --> 00:17:24,845
Speaker 5:  the thing I'm trying to do.

281
00:17:25,135 --> 00:17:29,045
Speaker 4:  Right? I have to tell you what I've told you so that you

282
00:17:29,045 --> 00:17:32,445
Speaker 4:  can tell it to me again. And it's just like, what is, what is just, just

283
00:17:32,445 --> 00:17:33,765
Speaker 4:  tell me the code. Just what is the code?

284
00:17:33,905 --> 00:17:37,045
Speaker 5:  We, we fixed this, you let me pin notes on my phone screen and

285
00:17:37,635 --> 00:17:38,525
Speaker 5:  then I had the code.

286
00:17:39,215 --> 00:17:43,065
Speaker 4:  Yeah, there's, there's a lot of weird UI around this

287
00:17:43,065 --> 00:17:46,585
Speaker 4:  stuff. And the question of like, what is this actually accomplishing that

288
00:17:46,585 --> 00:17:47,825
Speaker 4:  is AI and is new,

289
00:17:49,595 --> 00:17:53,365
Speaker 4:  hard to say. Again, I I like, I can't stop talking

290
00:17:53,365 --> 00:17:57,285
Speaker 4:  about the fact that hacking your way into like pretending to use

291
00:17:57,285 --> 00:18:00,605
Speaker 4:  a web browser on my behalf is not impressive and not interesting.

292
00:18:01,045 --> 00:18:03,325
Speaker 4:  Like do other things. Well, so

293
00:18:03,325 --> 00:18:07,045
Speaker 3:  Let's split these two up. Astro's interesting, right? Because we're

294
00:18:07,425 --> 00:18:11,405
Speaker 3:  the same day that Google announced all this stuff. Apple announced iOS 18.2

295
00:18:11,605 --> 00:18:15,525
Speaker 3:  released iOS 18.2 with Visual Intelligence. Yeah. And the, Hey look

296
00:18:15,525 --> 00:18:18,925
Speaker 3:  at something and tell me about it. We, that's the world we live in now. This

297
00:18:18,925 --> 00:18:22,085
Speaker 3:  is the new great AI feature which, you know, other companies have had,

298
00:18:23,025 --> 00:18:26,165
Speaker 3:  but now it's built right into the iPhone and then Google is talking about

299
00:18:26,255 --> 00:18:29,365
Speaker 3:  Astro with it. It seems like Astro is very much the future of Google lens,

300
00:18:29,365 --> 00:18:32,005
Speaker 3:  right? You're just like, yes, looking at stuff, it's talking to you about

301
00:18:32,005 --> 00:18:35,885
Speaker 3:  it, you're having a great time. But it is very much like I looked

302
00:18:35,885 --> 00:18:39,045
Speaker 3:  at a picture, I scanned all the texts in the picture or I did some visual

303
00:18:39,045 --> 00:18:41,685
Speaker 3:  recognition of what I'm looking at and here's some information you can have

304
00:18:41,685 --> 00:18:45,285
Speaker 3:  a conversation with me about it that to whatever extent that is

305
00:18:45,655 --> 00:18:48,725
Speaker 3:  super useful for people. I get it. I think there's

306
00:18:49,415 --> 00:18:53,165
Speaker 3:  still just what I keep calling the US capital problem. Like

307
00:18:53,665 --> 00:18:56,925
Speaker 3:  you ask Apple Visual Intelligence what happened at the Capitol building on

308
00:18:56,925 --> 00:19:00,685
Speaker 3:  January 6th, like that answer is pretty dicey. Yeah. Right. And like there's

309
00:19:00,685 --> 00:19:04,525
Speaker 3:  a, a universe of actors who would love to change what that answer is across

310
00:19:04,525 --> 00:19:07,765
Speaker 3:  the political spectrum. And none of these companies have contended with it.

311
00:19:08,515 --> 00:19:11,005
Speaker 3:  Astro's got the same problem, you know, everyone else has the same problem,

312
00:19:11,265 --> 00:19:14,405
Speaker 3:  but they're headed that way. Well, and Google Lens has been headed that way

313
00:19:14,405 --> 00:19:14,605
Speaker 3:  for a

314
00:19:14,605 --> 00:19:17,445
Speaker 4:  Long time. Wait, do you think it's like a real mainstream use case that a

315
00:19:17,445 --> 00:19:19,885
Speaker 4:  lot of people are gonna walk by the capitol and be like, what happened here

316
00:19:19,885 --> 00:19:20,325
Speaker 4:  on January?

317
00:19:20,425 --> 00:19:22,925
Speaker 3:  So I think it's a real mainstream use case that you're gonna be wearing a

318
00:19:22,925 --> 00:19:26,565
Speaker 3:  pair of glasses in the future that purports to augment reality and everyone

319
00:19:26,565 --> 00:19:29,965
Speaker 3:  lives in a weird custom political reality. Yeah, sure. Whether or not you're

320
00:19:29,965 --> 00:19:30,765
Speaker 3:  looking at the Capitol or not,

321
00:19:30,915 --> 00:19:31,405
Speaker 4:  Granted,

322
00:19:32,295 --> 00:19:36,245
Speaker 3:  Right. I mean, and I'm not gonna go to it 'cause it's super dicey and

323
00:19:36,245 --> 00:19:39,045
Speaker 3:  loaded and we should probably just do an entire episode on someday. But you

324
00:19:39,045 --> 00:19:42,645
Speaker 3:  can just think of the infinite questions that come about when you look at

325
00:19:42,645 --> 00:19:46,445
Speaker 3:  something that are ultra loaded and that someone will start a culture war

326
00:19:46,445 --> 00:19:50,045
Speaker 3:  over, right? Yeah. Just e every in your house you can just look at stuff

327
00:19:50,145 --> 00:19:53,955
Speaker 3:  and be like, you know, like, are the cowboys good? And

328
00:19:53,955 --> 00:19:57,275
Speaker 3:  like there's just like a lot of questions underneath that that are like really

329
00:19:57,275 --> 00:20:00,875
Speaker 3:  hard. Like, Hey is that Aaron Rogers? Tell me about his ideas. Like

330
00:20:01,975 --> 00:20:05,715
Speaker 3:  If you just go for it, like just normal everyday things.

331
00:20:06,895 --> 00:20:10,675
Speaker 3:  And that to me is, it's great that we're building technology for it and Astros

332
00:20:10,675 --> 00:20:14,195
Speaker 3:  is very much the future of Google Lens, but there's this whole universe of

333
00:20:14,195 --> 00:20:17,555
Speaker 3:  those problems that come with, I'm looking at stuff. Tell me about it.

334
00:20:18,455 --> 00:20:20,475
Speaker 3:  Who will augment reality is like a great question.

335
00:20:20,945 --> 00:20:24,395
Speaker 4:  Yeah, I think it's, I think there, there are sort of two separate

336
00:20:24,935 --> 00:20:28,835
Speaker 4:  pieces of this kind of thing and I'm, I'm really bullish on one

337
00:20:28,835 --> 00:20:32,595
Speaker 4:  and kind of bearish on the other. And I think the like how to

338
00:20:32,995 --> 00:20:36,155
Speaker 4:  logistically get through your day thing. I think things like Astro are going

339
00:20:36,155 --> 00:20:39,715
Speaker 4:  to be really useful for, right? Like there's a, there's a moment

340
00:20:39,935 --> 00:20:43,115
Speaker 4:  in the demo video they just released where the guy holds up his phone and

341
00:20:43,115 --> 00:20:46,155
Speaker 4:  he points it as at a bus that's just going by and he's like, well this bus

342
00:20:46,155 --> 00:20:49,805
Speaker 4:  kept me to Chinatown. There are a lot of like complicated problems to solve

343
00:20:49,945 --> 00:20:53,765
Speaker 4:  technologically there. But that's a really interesting UI problem

344
00:20:53,765 --> 00:20:57,725
Speaker 4:  because that's a hard question to answer and if the AI assistant

345
00:20:57,785 --> 00:21:01,605
Speaker 4:  can answer it, that's very cool. And the like stuff around

346
00:21:01,605 --> 00:21:04,845
Speaker 4:  me and how I get places and like just like how to do your day,

347
00:21:05,475 --> 00:21:07,685
Speaker 4:  this kind of thing can be really useful for it if it works.

348
00:21:07,755 --> 00:21:10,965
Speaker 3:  Wait, but but I mean, can I just hold on the bus example? Sure. For one second.

349
00:21:11,585 --> 00:21:15,485
Speaker 3:  One, ideally the will this bus get to me to where I'm going is not a culture

350
00:21:15,585 --> 00:21:19,245
Speaker 3:  war problem, right? It's a question with an answer like, will this go there?

351
00:21:19,505 --> 00:21:23,045
Speaker 3:  And it's an answer that Google already has in a million structured

352
00:21:23,285 --> 00:21:27,165
Speaker 3:  databases with deterministic systems running them. Sure. So you can

353
00:21:27,165 --> 00:21:30,845
Speaker 3:  just ask Google Maps this, Google has all kinds of public

354
00:21:30,845 --> 00:21:34,365
Speaker 3:  transit infrastructure projects that have put sensors on buses and there's

355
00:21:34,365 --> 00:21:37,205
Speaker 3:  open systems and you can build products and all that. And they've just built

356
00:21:37,205 --> 00:21:41,005
Speaker 3:  that stuff to enable maps a decade ago. And so now you've got a

357
00:21:41,005 --> 00:21:44,725
Speaker 3:  conversational layer that is a new interface for that where you're just like,

358
00:21:44,725 --> 00:21:47,885
Speaker 3:  look at this, I'm asking about it, figure out what that is. And then it

359
00:21:48,425 --> 00:21:51,885
Speaker 3:  parses that into a structured query for its existing Google Map systems

360
00:21:52,225 --> 00:21:54,925
Speaker 3:  and delivers you the answer international language way. And so really what

361
00:21:54,925 --> 00:21:58,605
Speaker 3:  you've just built a new interface for the existing good system that works.

362
00:21:59,815 --> 00:22:01,395
Speaker 3:  And I think that's very power. I I

363
00:22:01,395 --> 00:22:03,555
Speaker 4:  I all the good AI stuff is exactly that,

364
00:22:03,605 --> 00:22:07,515
Speaker 3:  Right? And I just, that's like one, and when people talk about it, it's being

365
00:22:07,515 --> 00:22:10,435
Speaker 3:  a platform shift. Like that's the one I see. Like we made a click wheel and

366
00:22:10,435 --> 00:22:13,715
Speaker 3:  now we have iPods, we made Multitouch and now we have smartphones, we made

367
00:22:13,905 --> 00:22:16,995
Speaker 3:  natural language UI that is pretty good and now we're gonna get a bunch of

368
00:22:16,995 --> 00:22:20,795
Speaker 3:  other stuff and we made a UI that can listen

369
00:22:20,795 --> 00:22:23,545
Speaker 3:  to a song and tell you about it. Yeah. Right. There's, there's like a lot

370
00:22:23,545 --> 00:22:27,345
Speaker 3:  of that stuff that I think is cool. It's the next turn where

371
00:22:27,345 --> 00:22:31,245
Speaker 3:  it's like, because it's a conversational natural language ui,

372
00:22:31,625 --> 00:22:34,765
Speaker 3:  you expect to have conversations with you and like be a useful companion.

373
00:22:34,905 --> 00:22:38,885
Speaker 3:  And that's where it just always seems to fall down. Yes. Because the systems

374
00:22:38,885 --> 00:22:42,805
Speaker 3:  aren't actually intelligent and that, like that's to me is like, it,

375
00:22:42,905 --> 00:22:46,805
Speaker 3:  it, we keep sliding from one to the other. Like, here's a

376
00:22:46,835 --> 00:22:50,325
Speaker 3:  kick ass way to use Google Maps into this is your best friend that will,

377
00:22:50,625 --> 00:22:54,525
Speaker 3:  you know, If you ask it nicely we'll try to have sex with you. You're just

378
00:22:54,525 --> 00:22:57,205
Speaker 3:  like, well that's a pretty, that's a pretty long road actually.

379
00:22:57,415 --> 00:23:01,365
Speaker 4:  Right? We assume because one thus inevitably the

380
00:23:01,365 --> 00:23:05,205
Speaker 4:  other. Yeah. And I think, I think everyone needs to stop doing that. And

381
00:23:05,245 --> 00:23:09,085
Speaker 4:  I think one thing that Apple is doing right to its credit is like,

382
00:23:09,185 --> 00:23:13,045
Speaker 4:  it is, it is stuck on the idea that these are like features that

383
00:23:13,045 --> 00:23:16,805
Speaker 4:  should be useful and not best friend companions

384
00:23:16,805 --> 00:23:20,325
Speaker 4:  that you have. And I, I think like for the

385
00:23:20,355 --> 00:23:24,245
Speaker 4:  foreseeable future that is, you should a pick one of the

386
00:23:24,245 --> 00:23:26,805
Speaker 4:  two and not try to do both. And I think the one you should pick is like,

387
00:23:26,805 --> 00:23:30,765
Speaker 4:  try to be helpful because we just keep seeing stories about

388
00:23:30,765 --> 00:23:33,045
Speaker 4:  the friends being weird and problematic.

389
00:23:33,185 --> 00:23:35,405
Speaker 3:  You just ran one of those stories from Josh like a long feature.

390
00:23:35,515 --> 00:23:39,405
Speaker 4:  Yeah. Like it's, it's a mess and it's, that stuff is real. But

391
00:23:39,435 --> 00:23:43,115
Speaker 4:  like, but in this case, like again, I think like

392
00:23:43,305 --> 00:23:47,245
Speaker 4:  down every road lies a culture war. I I agree. Right? Like If you, If you,

393
00:23:47,245 --> 00:23:49,565
Speaker 4:  If you follow it long enough, there's a culture war there somewhere. But

394
00:23:49,565 --> 00:23:53,435
Speaker 4:  like there are things along the way that work and are

395
00:23:53,435 --> 00:23:56,075
Speaker 4:  useful. And I think Google is starting to poke at some of that stuff in pretty

396
00:23:56,275 --> 00:23:58,675
Speaker 3:  Interesting. But I, but I'm like, the culture war is like, not even, I'm

397
00:23:58,675 --> 00:24:01,275
Speaker 3:  not even saying it's like politically dicey. I'm saying you're somewhere

398
00:24:01,295 --> 00:24:05,075
Speaker 3:  and you're listening to that new choral version of like a prayer by Madonna

399
00:24:05,255 --> 00:24:08,715
Speaker 3:  and you're like, is this song appropriate for church? And some people think

400
00:24:08,715 --> 00:24:12,395
Speaker 3:  the answer is yes because they're performing it in churches in

401
00:24:12,395 --> 00:24:15,875
Speaker 3:  America today. And it's like, well you just gotta, you gotta just,

402
00:24:16,375 --> 00:24:19,835
Speaker 3:  the robots gotta unpack that box for you, right? Like, and I, I don't think

403
00:24:19,835 --> 00:24:23,195
Speaker 3:  these companies are ready for that, which is what will truly make them useful

404
00:24:23,735 --> 00:24:27,075
Speaker 3:  is by giving you an answer you might not like. And I, they're super not ready

405
00:24:27,075 --> 00:24:30,515
Speaker 3:  for that in the, in the current political climate. And that's like, that's

406
00:24:30,515 --> 00:24:33,155
Speaker 3:  when you make the turn. Like that's when it actually becomes a useful assistant

407
00:24:33,155 --> 00:24:35,835
Speaker 3:  to you go find me misinformation. Even if I don't like it as

408
00:24:35,835 --> 00:24:38,875
Speaker 5:  A slightly lower stakes version of that question. One of the demos I'm saying

409
00:24:38,895 --> 00:24:42,755
Speaker 3:  Is like a prayer song about doing it is like not a high stakes problem.

410
00:24:44,795 --> 00:24:45,855
Speaker 5:  It depends on where you are.

411
00:24:46,295 --> 00:24:46,695
Speaker 3:  I suppose

412
00:24:47,195 --> 00:24:50,215
Speaker 5:  One of the examples that they showed with Project Mariner was someone pulling

413
00:24:50,215 --> 00:24:53,575
Speaker 5:  up a recipe in Google Docs and saying, add the veggies from this res recipe

414
00:24:53,675 --> 00:24:57,615
Speaker 5:  to my Safeway cart. What if one of those things is a fruit and I

415
00:24:57,795 --> 00:25:01,455
Speaker 5:  forgot that it was a fruit, not a vegetable, or I just disagree. Yeah.

416
00:25:02,395 --> 00:25:06,255
Speaker 3:  Now that's a high stakes. Yeah. Add these sandwiches to my cart. One of

417
00:25:06,335 --> 00:25:09,415
Speaker 3:  'em is a hotdog and the thing just like a data center explodes somewhere

418
00:25:09,415 --> 00:25:09,695
Speaker 3:  in America

419
00:25:09,845 --> 00:25:12,655
Speaker 5:  This is is something that, that will happen to someone. Yeah.

420
00:25:12,955 --> 00:25:15,935
Speaker 3:  All right. So that's, I just wanna like call that out like the, there's the

421
00:25:15,935 --> 00:25:19,695
Speaker 3:  thing we want them to do, which is not just a

422
00:25:19,695 --> 00:25:23,415
Speaker 3:  technology problem, like a straight up culture problem, like a society level

423
00:25:23,415 --> 00:25:27,295
Speaker 3:  problem. And then there's like the really interesting part, which is build

424
00:25:27,295 --> 00:25:30,615
Speaker 3:  a better interface for systems that already work. And then there's this weird

425
00:25:30,615 --> 00:25:34,335
Speaker 3:  middle ground, which I think Mariner is, which you know, copilot is at Microsoft,

426
00:25:34,335 --> 00:25:38,175
Speaker 3:  all these age agentic systems, which is what if we just use

427
00:25:38,175 --> 00:25:42,055
Speaker 3:  the internet for you? And that's how we solve the problem of we

428
00:25:42,055 --> 00:25:44,775
Speaker 3:  need something in the middle that works. Right. And David, I think this is

429
00:25:44,775 --> 00:25:48,325
Speaker 3:  where you're getting to like what a stunning indictment of web design because

430
00:25:48,325 --> 00:25:51,405
Speaker 3:  you're like, well the internet's pretty broken, so it would be better if

431
00:25:51,405 --> 00:25:54,525
Speaker 3:  some unreliable ho robot used it for me. Right.

432
00:25:54,525 --> 00:25:58,445
Speaker 4:  The the assumption is that you can't find anything, which is a very

433
00:25:58,445 --> 00:26:01,685
Speaker 4:  funny thing for the company that made Google search to say. But yeah, we're,

434
00:26:01,685 --> 00:26:05,245
Speaker 4:  we're at a moment where it is perceived to be easier

435
00:26:06,105 --> 00:26:10,085
Speaker 4:  to ask this thing to go find a contact

436
00:26:10,135 --> 00:26:14,085
Speaker 4:  email address than it is to just go find the contact email address. And

437
00:26:14,085 --> 00:26:17,725
Speaker 4:  again, like I think a, a place we are landing

438
00:26:17,915 --> 00:26:21,205
Speaker 4:  that is very funny is that all these companies are slowly

439
00:26:21,565 --> 00:26:24,925
Speaker 4:  discovering that the most useful thing their tools do

440
00:26:25,465 --> 00:26:29,405
Speaker 4:  are really boring and really like back of house accounting firm kind of

441
00:26:29,405 --> 00:26:33,085
Speaker 4:  stuff. And that turns out to not be very

442
00:26:33,365 --> 00:26:37,245
Speaker 4:  interesting to like a mainstream public that you need to get excited

443
00:26:37,245 --> 00:26:40,245
Speaker 4:  about using your products. So they're stuck in this place of all these companies

444
00:26:40,245 --> 00:26:43,845
Speaker 4:  are actually playing like a thoroughly B2B game and just

445
00:26:43,845 --> 00:26:47,205
Speaker 4:  pretending that they're making like huge

446
00:26:47,395 --> 00:26:50,685
Speaker 4:  strides toward mainstream consumer products.

447
00:26:52,135 --> 00:26:55,635
Speaker 4:  That's all fine and good. And I think like Neil, I think you're really like,

448
00:26:55,815 --> 00:26:58,355
Speaker 4:  you're right to be hung up on the fact that these things get some things

449
00:26:58,355 --> 00:27:02,175
Speaker 4:  wrong, but also like people use 'em anyway. Yeah.

450
00:27:02,205 --> 00:27:06,055
Speaker 4:  Like it, it like the, this thing will write my email for me

451
00:27:06,075 --> 00:27:08,815
Speaker 4:  and it will make two weird mistakes, but then I can fix them and send it

452
00:27:08,815 --> 00:27:10,655
Speaker 4:  and that's better than having to write the email from scratch

453
00:27:10,835 --> 00:27:13,815
Speaker 3:  Or not fix them. Yeah. As we are learning every single day, the answer is

454
00:27:13,815 --> 00:27:16,575
Speaker 3:  not fix them. Yeah. The answer to some extent is read them out loud on cable

455
00:27:16,575 --> 00:27:16,855
Speaker 3:  news.

456
00:27:17,275 --> 00:27:21,215
Speaker 4:  The answer is Hunter debuts will, will, will save us.

457
00:27:22,315 --> 00:27:23,015
Speaker 4:  But yeah. And like

458
00:27:25,195 --> 00:27:29,015
Speaker 4:  the Hunter deButts will like go down in history as like

459
00:27:29,255 --> 00:27:29,375
Speaker 4:  I

460
00:27:29,375 --> 00:27:32,335
Speaker 3:  Refuse to explain and it's there you can look it up.

461
00:27:32,575 --> 00:27:36,135
Speaker 4:  Yeah. Ironically, If you Google Hunter deButts you'll get correct information

462
00:27:36,135 --> 00:27:39,535
Speaker 4:  about what we're talking about. But, but anyway,

463
00:27:41,155 --> 00:27:44,795
Speaker 4:  I just think we're, we're at a moment now where the question for all of this

464
00:27:44,855 --> 00:27:48,315
Speaker 4:  AI stuff, just to bring it all the way back around is what is any of this

465
00:27:48,795 --> 00:27:52,755
Speaker 4:  actually useful for? And I think the truth is the

466
00:27:52,755 --> 00:27:55,615
Speaker 4:  models that we have now are better

467
00:27:56,605 --> 00:28:00,215
Speaker 4:  than we need for a lot of things and vastly

468
00:28:01,205 --> 00:28:04,455
Speaker 4:  like a million miles away from being good enough for what everybody keeps

469
00:28:04,455 --> 00:28:08,445
Speaker 4:  talking about. Right? So now the question is, okay, we're not

470
00:28:08,445 --> 00:28:12,165
Speaker 4:  gonna build the perfect thing for a long time, if ever, what can we do with

471
00:28:12,165 --> 00:28:15,485
Speaker 4:  what we have now? And that's where things like efficiency come in because

472
00:28:15,485 --> 00:28:17,765
Speaker 4:  at some point, like running a business costs money and you have to be able

473
00:28:17,765 --> 00:28:20,125
Speaker 4:  to do these things and If you can make them cheaper, more people can use

474
00:28:20,125 --> 00:28:24,085
Speaker 4:  them and find new things to do and like on and on and on. But the, the

475
00:28:24,285 --> 00:28:27,245
Speaker 4:  question is like, okay, If you just take the technology we have right now

476
00:28:27,545 --> 00:28:31,405
Speaker 4:  and you don't pretend it's God, what is it good for? And that is

477
00:28:31,405 --> 00:28:34,165
Speaker 4:  like, that is the question Google is starting to try to answer. It's the

478
00:28:34,245 --> 00:28:37,445
Speaker 4:  question OpenAI is starting to try to answer that is like the question of

479
00:28:37,445 --> 00:28:40,965
Speaker 4:  2025 is like, what is any of this actually useful for? Because there are

480
00:28:40,965 --> 00:28:44,725
Speaker 4:  answers, they're just not as, like, they're not God and

481
00:28:44,725 --> 00:28:47,685
Speaker 4:  they're not fire and they're not the industrial revolution. So what are they?

482
00:28:48,035 --> 00:28:51,365
Speaker 3:  It's like a tour guide and an extremely unreliable personal assistant,

483
00:28:52,775 --> 00:28:56,125
Speaker 4:  Which to be fair is not nothing. Yeah. It's not nothing.

484
00:28:56,625 --> 00:29:00,005
Speaker 3:  And I, I can see why they're compelling and I can see why. I mean, just the

485
00:29:00,005 --> 00:29:03,405
Speaker 3:  demo, I mean Richard, you already brought it up like the Google demo of Mariner

486
00:29:03,535 --> 00:29:07,525
Speaker 3:  where you're just like looking at a recipe in Google Docs and you just

487
00:29:07,625 --> 00:29:10,965
Speaker 3:  ask it to go build you a shopping cart on a grocery site and it's just like,

488
00:29:10,965 --> 00:29:14,565
Speaker 3:  does it for you neat. Like legitimately neat,

489
00:29:14,795 --> 00:29:18,125
Speaker 3:  very slow, so slow that the Google PM

490
00:29:18,435 --> 00:29:22,165
Speaker 3:  literally saw Kylie noticing how slow it was and said that's the elephant

491
00:29:22,165 --> 00:29:26,155
Speaker 3:  in the room, that it's ponderously slow. Right?

492
00:29:26,255 --> 00:29:29,235
Speaker 3:  But like, they know, they, they know it, they're not not hiding from it.

493
00:29:29,265 --> 00:29:33,195
Speaker 3:  It's a research demo, but like I am, I'm stuck at

494
00:29:34,335 --> 00:29:38,275
Speaker 3:  the systems are not themselves capable. So they are free riding on

495
00:29:38,275 --> 00:29:42,085
Speaker 3:  other systems that can do the thing. The grocery site has

496
00:29:42,085 --> 00:29:45,965
Speaker 3:  to exist and work and it needs to have a database of all the food

497
00:29:45,965 --> 00:29:48,885
Speaker 3:  in the grocery store. And then you need to be able to put it in a cart and

498
00:29:48,885 --> 00:29:52,485
Speaker 3:  like it, like all that has to work and be profitable

499
00:29:52,665 --> 00:29:56,445
Speaker 3:  and sustainable. And then on top of it you're just like, and then this robot

500
00:29:56,445 --> 00:29:59,405
Speaker 3:  will cost $20 a month and like do it for you. And eventually you're just

501
00:29:59,405 --> 00:30:03,285
Speaker 3:  like, what if, what if this undercuts all of that? I keep

502
00:30:03,285 --> 00:30:07,185
Speaker 3:  calling this a DoorDash problem where DoorDash has to

503
00:30:07,195 --> 00:30:11,105
Speaker 3:  exist for a bunch of these services to get you a sandwich. And

504
00:30:11,105 --> 00:30:14,105
Speaker 3:  If you just sort of delete door dash's business by cutting them out of the

505
00:30:14,265 --> 00:30:18,185
Speaker 3:  equation and taking you the customer away from DoorDash, like

506
00:30:18,365 --> 00:30:21,625
Speaker 3:  how is anyone gonna get a sandwich? Like you need this big

507
00:30:22,135 --> 00:30:26,065
Speaker 3:  traditional deterministic logical computer

508
00:30:26,935 --> 00:30:30,865
Speaker 3:  with a database that is reliable doing a bunch of work. So

509
00:30:30,865 --> 00:30:33,625
Speaker 3:  your natural language interface can be useful. And I don't think anybody

510
00:30:33,645 --> 00:30:36,785
Speaker 3:  has solved the problem of like, how do you keep DoorDash in business, including

511
00:30:36,785 --> 00:30:37,265
Speaker 4:  DoorDash

512
00:30:37,265 --> 00:30:40,785
Speaker 3:  And I'm not just speaking in DoorDash, whatever Uber, right? Like rabbits,

513
00:30:40,785 --> 00:30:44,185
Speaker 3:  just like we're gonna click around Uber's website. All of these middle companies,

514
00:30:44,185 --> 00:30:48,145
Speaker 3:  these like web two oh companies that digitize the interfaces for very

515
00:30:48,425 --> 00:30:51,955
Speaker 3:  physical things. Like they're pretty, they were pretty

516
00:30:52,025 --> 00:30:55,195
Speaker 3:  vulture in their times, right? Like Uber is the biggest taxi company in the

517
00:30:55,195 --> 00:30:58,875
Speaker 3:  world, doesn't own a single taxi, Airbnb doesn't own a single hotel. Like

518
00:30:58,975 --> 00:31:02,275
Speaker 3:  that's the joke about those businesses. And now they're under pressure

519
00:31:02,745 --> 00:31:06,515
Speaker 3:  from AI systems that might use their interfaces

520
00:31:06,615 --> 00:31:10,475
Speaker 3:  and never show them to a user. Right? And it's like where this has to

521
00:31:10,535 --> 00:31:13,075
Speaker 3:  go somewhere that is actually useful for people. Well

522
00:31:13,075 --> 00:31:17,035
Speaker 4:  It, it's a weird thing because you have all of the AI companies

523
00:31:17,125 --> 00:31:20,795
Speaker 4:  would agree with you, but they see that as a strength and not a weakness.

524
00:31:20,795 --> 00:31:24,755
Speaker 4:  Right? Where what they're saying is, okay, we are going to have an army of

525
00:31:24,755 --> 00:31:28,635
Speaker 4:  service providers who just invisibly do the job on your behalf and you

526
00:31:28,635 --> 00:31:32,355
Speaker 4:  don't have to worry about who they are. You don't have to go to the DoorDash

527
00:31:32,555 --> 00:31:35,955
Speaker 4:  website every time you wanna order McDonald's, you just order

528
00:31:36,275 --> 00:31:39,235
Speaker 4:  McDonald's and we'll get it done for you. And DoorDash or whoever will be

529
00:31:39,235 --> 00:31:42,475
Speaker 4:  our provider of choice to do that. They would call that a victory, right?

530
00:31:42,475 --> 00:31:45,675
Speaker 4:  Because I, as a consumer, I don't, I shouldn't have to interact at DoorDash

531
00:31:45,675 --> 00:31:48,595
Speaker 4:  to get McDonald's. I shouldn't interact with McDonald's. But what what they've

532
00:31:48,755 --> 00:31:52,675
Speaker 4:  actually done is a insert another step and B completely commoditize all of

533
00:31:52,675 --> 00:31:55,555
Speaker 4:  these providers in such a way that they will eventually just

534
00:31:56,405 --> 00:32:00,395
Speaker 4:  crash them all in an incredible race to the bottom to be one of these providers.

535
00:32:00,615 --> 00:32:04,595
Speaker 4:  And so it's, it's like when all the carriers got really nervous

536
00:32:04,595 --> 00:32:08,475
Speaker 4:  about being dumb pipes and so they bought content businesses, it's like

537
00:32:09,005 --> 00:32:12,115
Speaker 4:  there there is a, there is another turn of that coming where

538
00:32:12,865 --> 00:32:16,435
Speaker 4:  Uber is going to be terrified of being abstracted away by all of these AI

539
00:32:16,715 --> 00:32:20,475
Speaker 4:  companies. And so it's going to do more to try and get you to use Uber and

540
00:32:20,475 --> 00:32:22,395
Speaker 4:  it's just gonna get really weird as a result.

541
00:32:22,655 --> 00:32:25,155
Speaker 3:  Or it's gonna just raise the price or

542
00:32:25,155 --> 00:32:26,275
Speaker 4:  It's gonna raise the price. Yeah. Or

543
00:32:26,275 --> 00:32:30,035
Speaker 3:  It'll raise the price for the AI systems. So an Uber

544
00:32:30,035 --> 00:32:33,595
Speaker 3:  costs more If you book it through AI than If you book it yourself in the

545
00:32:33,595 --> 00:32:34,755
Speaker 3:  app and look at an ad for Uber.

546
00:32:34,865 --> 00:32:37,235
Speaker 4:  Yeah. It's just the next generation version of the app store tax

547
00:32:37,455 --> 00:32:39,925
Speaker 3:  And you're we're, I think we're just gonna see a bunch of this stuff play

548
00:32:39,925 --> 00:32:43,165
Speaker 3:  out. And every time everyone talks to me about these agents, and Google in

549
00:32:43,165 --> 00:32:45,885
Speaker 3:  particular isn't a very privileged position because they run Chrome

550
00:32:47,025 --> 00:32:50,285
Speaker 3:  and they run search and like one of their, their deep research preview that

551
00:32:50,285 --> 00:32:54,045
Speaker 3:  they released in the same set of releases where it's like,

552
00:32:54,385 --> 00:32:57,045
Speaker 3:  go research a topic for me and it comes with a research plan and then it

553
00:32:57,045 --> 00:33:00,605
Speaker 3:  goes, searches the web for interesting results and then

554
00:33:00,605 --> 00:33:04,435
Speaker 3:  delivers that research back to you. That's just perplexity, right?

555
00:33:04,505 --> 00:33:07,475
Speaker 3:  It's just perplexity by another name and perplexity is in a bunch of trouble

556
00:33:07,875 --> 00:33:10,795
Speaker 3:  'cause it's not paying any of the sources it's using Yep. To scour the web

557
00:33:10,795 --> 00:33:14,235
Speaker 3:  in that way. And it's like at some point all of the people who make websites,

558
00:33:14,235 --> 00:33:17,915
Speaker 3:  us included. And I feel like disclosure, the company has some sort of

559
00:33:17,915 --> 00:33:21,035
Speaker 3:  meandering deal with open AI that hasn't really resulted in it as far as

560
00:33:21,035 --> 00:33:21,995
Speaker 3:  I can tell. But like there it is.

561
00:33:24,145 --> 00:33:27,755
Speaker 3:  Like all of these companies are gonna say, well we make the information,

562
00:33:28,745 --> 00:33:32,325
Speaker 3:  we make the interface to book a taxi or whatever it is.

563
00:33:32,625 --> 00:33:36,605
Speaker 3:  You, you have to pay us. 'cause If you just take the customers away

564
00:33:36,605 --> 00:33:40,005
Speaker 3:  from us, we will go away and the bots will useless. And I think that just

565
00:33:40,035 --> 00:33:43,965
Speaker 3:  that economic reality will result in some payments whether or not the,

566
00:33:43,965 --> 00:33:46,725
Speaker 3:  you know, the lawsuits or whatever results in payments are blocking robots

567
00:33:46,725 --> 00:33:49,965
Speaker 3:  at TXT. But it's come, it's, it has to be coming.

568
00:33:50,985 --> 00:33:54,925
Speaker 3:  And I think the, I think it's gonna like follow the same pattern

569
00:33:54,925 --> 00:33:58,565
Speaker 3:  everything else follows. Like Uber will be exclusive to copilot and DoorDash

570
00:33:58,565 --> 00:34:02,205
Speaker 3:  will be exclusive to Gemini and like it's, that part's gonna suck for a long

571
00:34:02,205 --> 00:34:02,365
Speaker 3:  time.

572
00:34:03,065 --> 00:34:07,045
Speaker 4:  Yes. Or we're gonna, somebody is gonna figure out how to like

573
00:34:07,065 --> 00:34:11,045
Speaker 4:  do the hack to make it work whether the companies want it to or not. And

574
00:34:11,045 --> 00:34:13,325
Speaker 4:  then we're gonna have an entirely different kind of mess. And

575
00:34:13,325 --> 00:34:16,365
Speaker 3:  Then a new generation of college students will we radicalize by Napster and

576
00:34:16,365 --> 00:34:19,765
Speaker 3:  then that's how we will v for Vendetta my replacement. It'll be great. We

577
00:34:19,765 --> 00:34:22,125
Speaker 3:  gotta take a quick break, but talk about this XR thing really quickly.

578
00:34:22,315 --> 00:34:25,325
Speaker 4:  Yeah. Okay. So you know, Android, how is Android?

579
00:34:25,995 --> 00:34:29,965
Speaker 4:  Imagine if smart glasses were a thing and if if

580
00:34:30,265 --> 00:34:33,965
Speaker 4:  Google just did Android again for those, that's what Android XR is. And

581
00:34:33,965 --> 00:34:37,645
Speaker 4:  basically Google has been at this Smart glasses thing for like

582
00:34:38,355 --> 00:34:42,285
Speaker 4:  15 years now, maybe has tried a bunch of

583
00:34:42,285 --> 00:34:45,885
Speaker 4:  things. Do you remember Daydream? Daydream is one I had forgotten about that

584
00:34:45,885 --> 00:34:49,445
Speaker 4:  I just randomly rediscovered today. Daydream existed. I wrote a whole thing

585
00:34:49,445 --> 00:34:52,565
Speaker 4:  about it for Wired a million years ago. Completely forgot about that. But

586
00:34:52,565 --> 00:34:56,365
Speaker 4:  anyway, so Google announced like a developer preview

587
00:34:56,385 --> 00:35:00,205
Speaker 4:  of Android XR. It's basically what you think it's a, it's immersive versions

588
00:35:00,205 --> 00:35:04,005
Speaker 4:  of some Google apps and some Gemini stuff. Basically the,

589
00:35:04,025 --> 00:35:07,925
Speaker 4:  the underpinning of all of this is the assistant Gemini

590
00:35:07,925 --> 00:35:10,765
Speaker 4:  stuff that you might wanna wear on your face. That's sort of the big theory.

591
00:35:12,275 --> 00:35:15,645
Speaker 4:  They have a bunch of hardware partners. I've been hearing from like every

592
00:35:15,915 --> 00:35:18,885
Speaker 4:  company that wants to sell you glasses over the last few weeks being like,

593
00:35:18,895 --> 00:35:22,125
Speaker 4:  we're doing a thing with Google that we can't quite tell you about yet. That's

594
00:35:22,125 --> 00:35:25,765
Speaker 4:  Android XR. So starting next year, I think we're gonna start to see

595
00:35:25,875 --> 00:35:29,485
Speaker 4:  real development and hardware products. It sounds like Samsung

596
00:35:29,745 --> 00:35:33,525
Speaker 4:  is gonna be the first outta the gate with like a real honest to god Android

597
00:35:33,525 --> 00:35:37,445
Speaker 4:  pair of glasses. But it seems very clear that like Apple is trying

598
00:35:37,445 --> 00:35:41,085
Speaker 4:  to be the apple of your face with the Vision Pro

599
00:35:41,505 --> 00:35:45,405
Speaker 4:  and Google is very happy being the Android of your face. I think the question

600
00:35:45,405 --> 00:35:47,885
Speaker 4:  of whether Google is going to make hardware is really interesting. Google's

601
00:35:47,885 --> 00:35:51,125
Speaker 4:  made a bunch of prototypes and gave us some indication this week that it

602
00:35:51,125 --> 00:35:55,075
Speaker 4:  is interested in them being more than prototypes. And If you want to do

603
00:35:55,095 --> 00:35:58,635
Speaker 4:  all this Gemini stuff and Project Astral, like you gotta have glasses. Like

604
00:35:59,215 --> 00:36:03,095
Speaker 4:  if, if this AI future that everybody's imagining

605
00:36:03,095 --> 00:36:06,015
Speaker 4:  and trying to sell you on is going to come true, it's going to be glasses.

606
00:36:06,355 --> 00:36:09,215
Speaker 4:  It just is. I I'm a hundred percent convinced.

607
00:36:09,515 --> 00:36:12,495
Speaker 3:  I'm just saying it's a decade from now. There's an eighth grader,

608
00:36:13,405 --> 00:36:16,865
Speaker 3:  one of our children will be in eighth grade at some point. They're on their

609
00:36:16,865 --> 00:36:20,145
Speaker 3:  eighth grade ship to dc they're wearing their glasses. They look at the capitol

610
00:36:20,165 --> 00:36:22,985
Speaker 3:  and say, what happened here? What's, what are some notable events in American

611
00:36:22,985 --> 00:36:26,305
Speaker 3:  history? Does it say an insurrection or does it say it was a day of love?

612
00:36:27,365 --> 00:36:30,845
Speaker 3:  And are the answers on two different eighth graders different? And these

613
00:36:31,125 --> 00:36:35,005
Speaker 3:  companies are racing towards a future where they're going to have to answer

614
00:36:35,005 --> 00:36:37,965
Speaker 3:  that question regardless of whether they make the hardware or not. Aren't

615
00:36:37,965 --> 00:36:38,245
Speaker 3:  you just

616
00:36:38,245 --> 00:36:39,925
Speaker 4:  Describing the internet though? Like

617
00:41:42,145 --> 00:41:45,405
Speaker 3:  All right, we're back. My feeling is that Bixby would say it's a day of love.

618
00:41:48,395 --> 00:41:51,375
Speaker 3:  No, no further commentary. We should really

619
00:41:51,635 --> 00:41:55,495
Speaker 4:  Do like a political alignment matrix of all of the different virtual

620
00:41:55,545 --> 00:41:56,215
Speaker 4:  assistants.

621
00:41:56,795 --> 00:41:57,215
Speaker 3:  Oh my God,

622
00:41:57,295 --> 00:41:59,415
Speaker 4:  I feel like if I sat down and thought about it for five minutes, I would

623
00:41:59,415 --> 00:42:01,095
Speaker 4:  end up with really strong opinions. Who did

624
00:42:01,175 --> 00:42:04,815
Speaker 3:  T vote for? Yep. So my question is, do you wanna see the thing, know the

625
00:42:04,815 --> 00:42:08,135
Speaker 3:  answer to that? Do you wanna go viral on X in this specific way? You're like,

626
00:42:08,135 --> 00:42:11,975
Speaker 3:  we're gonna milk the last drop of traffic from this one dead platform

627
00:42:12,715 --> 00:42:15,575
Speaker 3:  and it's up the political alignment chart of different ai.

628
00:42:17,235 --> 00:42:21,095
Speaker 3:  No thank you. Alright. There's more AI

629
00:42:21,095 --> 00:42:25,055
Speaker 3:  news, it's it, it's just a lot of things got released this week. Yeah, open.

630
00:42:25,215 --> 00:42:28,815
Speaker 3:  I released SOA iOS 18.2 is out with Chad. GBD

631
00:42:28,815 --> 00:42:31,735
Speaker 3:  integration, visual intelligence, which what, what, what, what do you wanna

632
00:42:31,735 --> 00:42:32,015
Speaker 3:  start with?

633
00:42:32,345 --> 00:42:33,895
Speaker 4:  Let's do iOS 18.2 first.

634
00:42:34,245 --> 00:42:38,175
Speaker 3:  Okay. I mean we've, it, it's very familiar. I think we had a moment this

635
00:42:38,175 --> 00:42:40,455
Speaker 3:  morning we're like, how much should we cover this? 'cause people have seen

636
00:42:40,455 --> 00:42:44,295
Speaker 3:  it, they've used it. It's been in beta. The release candidate was last week,

637
00:42:44,715 --> 00:42:48,415
Speaker 3:  but it's out now. If you have an iPhone 16 or up or even iPhone 15 Pro,

638
00:42:48,835 --> 00:42:52,495
Speaker 3:  you get visual intelligence, you get Gen Moji chat. BT

639
00:42:52,495 --> 00:42:56,215
Speaker 3:  integration is there. Joanna Stern wrote a very funny column where she's

640
00:42:56,215 --> 00:43:00,055
Speaker 3:  like, you invoke chat GBT by saying like secret words. Like If you

641
00:43:00,055 --> 00:43:02,735
Speaker 3:  ask Siri one way, you get a regular Siri answer. But If you ask Siri, like

642
00:43:02,735 --> 00:43:06,615
Speaker 3:  write me a list, a ChatGPT is the list for you. Which is

643
00:43:06,615 --> 00:43:07,175
Speaker 3:  very funny.

644
00:43:08,925 --> 00:43:11,575
Speaker 3:  I've been, look, I've been using this stuff, David, I think you've been using

645
00:43:11,575 --> 00:43:13,535
Speaker 3:  this stuff. What? What do you think

646
00:43:13,535 --> 00:43:16,415
Speaker 4:  Richard famously uses Windows phone. So he, God only

647
00:43:16,415 --> 00:43:18,135
Speaker 3:  Knows it's the only way to live.

648
00:43:19,205 --> 00:43:23,015
Speaker 4:  I'll say two things. I think one, the single coolest thing about iOS

649
00:43:23,135 --> 00:43:26,975
Speaker 4:  18.2 is the ability to change default apps for various things.

650
00:43:26,975 --> 00:43:29,735
Speaker 4:  Like when you tap on a phone number, you can change which app it goes to.

651
00:43:30,575 --> 00:43:33,895
Speaker 4:  Everyone should go examine all of those defaults because If you get that

652
00:43:33,895 --> 00:43:36,895
Speaker 4:  kind of thing right, it makes your phone a lot better. This is the sort of

653
00:43:36,895 --> 00:43:40,655
Speaker 4:  thing Android has been better at than iOS for forever and thank you

654
00:43:40,675 --> 00:43:44,575
Speaker 4:  to occasionally confused EU regulators for making this

655
00:43:44,575 --> 00:43:48,255
Speaker 4:  happen. The the chat GPT thing I think is

656
00:43:49,725 --> 00:43:53,605
Speaker 4:  interesting. What I've found is that when I ask compound

657
00:43:53,965 --> 00:43:57,845
Speaker 4:  questions, it punts to chat GPT more often than not,

658
00:43:57,845 --> 00:44:01,685
Speaker 4:  right? Like if my question kind of has two parts or has

659
00:44:01,765 --> 00:44:04,445
Speaker 4:  a a thing and then a modifier, like one I just did the other day was like

660
00:44:04,445 --> 00:44:08,375
Speaker 4:  if I ask for a chocolate chip cookie recipe, Siri will just kind of

661
00:44:08,445 --> 00:44:12,055
Speaker 4:  pull it and I'm gonna not say that word again because it's just

662
00:44:12,055 --> 00:44:13,215
Speaker 4:  activated three devices.

663
00:44:14,955 --> 00:44:18,775
Speaker 4:  But if I ask for a chocolate chip cookie recipe that

664
00:44:18,775 --> 00:44:22,695
Speaker 4:  maybe will taste like I haven't tried before, then it punts to chat GBT

665
00:44:24,035 --> 00:44:27,655
Speaker 4:  and then it's, it's a relatively consistent experience. Actually, one thing

666
00:44:27,655 --> 00:44:31,205
Speaker 4:  that I've liked is that when it goes to chat GBT, the thing that it brings

667
00:44:31,205 --> 00:44:34,045
Speaker 4:  down still looks like the normal

668
00:44:35,235 --> 00:44:39,005
Speaker 4:  Siri response. So I think in that like the, my mental

669
00:44:39,405 --> 00:44:42,525
Speaker 4:  construct of what's going on is very confused in a way that I don't like,

670
00:44:42,905 --> 00:44:46,525
Speaker 4:  but the actual user experience of it I think is actually pretty good.

671
00:44:46,995 --> 00:44:50,645
Speaker 3:  Yeah, I mean in terms of like, this is a good way to get better

672
00:44:50,645 --> 00:44:54,525
Speaker 3:  answers from a Siri and so you just get, I I

673
00:44:54,525 --> 00:44:56,125
Speaker 3:  think that works to some extent.

674
00:44:56,275 --> 00:45:00,045
Speaker 4:  Yeah. Like do I have feelings about the idea that my stuff is

675
00:45:00,045 --> 00:45:03,725
Speaker 4:  going to chat GPT over and over and over again? Sure. But If you

676
00:45:03,775 --> 00:45:07,525
Speaker 4:  don't, that friction should go away and there isn't actually as much

677
00:45:07,765 --> 00:45:11,575
Speaker 4:  friction as I expected. So I think that's a probably on app,

678
00:45:11,595 --> 00:45:12,615
Speaker 4:  on balance a good thing.

679
00:45:13,005 --> 00:45:16,655
Speaker 3:  Yeah. I mean, again, I've had the action button on my phone mapped to

680
00:45:16,835 --> 00:45:20,695
Speaker 3:  the ChatGPT voice assistant forever. That's right. So this is just like

681
00:45:20,735 --> 00:45:24,455
Speaker 3:  a slightly faster version of that for me with like more annoying

682
00:45:24,455 --> 00:45:28,365
Speaker 3:  animation. Like I, I actually think the new sir animation

683
00:45:28,365 --> 00:45:29,885
Speaker 3:  is, is so overwrought.

684
00:45:30,485 --> 00:45:34,325
Speaker 4:  I agree. I missed the little swirling ball. The thing where it

685
00:45:34,325 --> 00:45:36,845
Speaker 4:  like it now takes over my whole phone screen. I don't like it.

686
00:45:36,995 --> 00:45:40,245
Speaker 3:  Yeah, it's, it's very much like jazz hands. It's like get ready

687
00:45:41,145 --> 00:45:44,805
Speaker 3:  for an experience. It's like, I don't need this tap. No, it's cool. But it

688
00:45:44,805 --> 00:45:47,885
Speaker 3:  just means that it's gonna take a little bit longer to load. Right. And then

689
00:45:47,885 --> 00:45:49,765
Speaker 3:  you talk, add a couple front, then talk, and then it does a thing where it

690
00:45:49,765 --> 00:45:53,715
Speaker 3:  like fills out the words for you. There's, there's some amount of

691
00:45:53,715 --> 00:45:57,635
Speaker 3:  like obfuscating how slow it was with the swirly ball. Yes.

692
00:45:57,705 --> 00:46:00,915
Speaker 3:  Made it appear fast. You know, like how progress bars are.

693
00:46:01,225 --> 00:46:04,955
Speaker 3:  They're, they're mostly designed to go really fast for the first part and

694
00:46:05,235 --> 00:46:07,675
Speaker 3:  then they can be slow at the end. But you've already perceived it being fast.

695
00:46:08,805 --> 00:46:12,425
Speaker 3:  That's very much what the old Siri was doing. And now it's just like

696
00:46:12,425 --> 00:46:15,425
Speaker 3:  everything's glowing and you can see the world's and it's like, I don't need,

697
00:46:15,905 --> 00:46:18,865
Speaker 3:  I don't need to see the mouse running the wheel guys just like let me know

698
00:46:18,865 --> 00:46:19,625
Speaker 3:  when there's an answer.

699
00:46:20,185 --> 00:46:24,105
Speaker 4:  I, I think that's right. I had actually not really ever thought

700
00:46:24,105 --> 00:46:28,025
Speaker 4:  about Siri as a, as a spinning like max style

701
00:46:28,025 --> 00:46:31,465
Speaker 4:  beach ball of loading things until it switched to this. And then it is like,

702
00:46:31,465 --> 00:46:34,905
Speaker 4:  it is so desperate to make it seem like something is happening at all times

703
00:46:35,255 --> 00:46:38,945
Speaker 4:  because it's slow. So it's just like, look at these beautiful pink and blue

704
00:46:39,465 --> 00:46:43,265
Speaker 4:  swirlies until it finishes. Which it will do

705
00:46:43,595 --> 00:46:45,345
Speaker 4:  eventually. It's

706
00:46:45,345 --> 00:46:45,505
Speaker 3:  Like,

707
00:46:46,095 --> 00:46:49,505
Speaker 4:  It's like alright, this is actually like pretty good design on top of a pretty

708
00:46:49,725 --> 00:46:50,905
Speaker 4:  bad system. Yeah,

709
00:46:51,305 --> 00:46:55,265
Speaker 3:  I will say, I mean it's, it is again the natural language capabilities that

710
00:46:55,265 --> 00:46:59,225
Speaker 3:  is what AI is best at. So it, I, it understands me a lot

711
00:46:59,225 --> 00:47:01,945
Speaker 3:  better. And then chat G PD is familiar.

712
00:47:03,155 --> 00:47:05,555
Speaker 3:  I don't know if it's hallucinating ny, but like it's, it's all happening

713
00:47:05,555 --> 00:47:08,715
Speaker 3:  and there's some guardrails in there. It to me, the ones that I,

714
00:47:09,525 --> 00:47:13,445
Speaker 3:  I don't think those are, the Apple intelligence is here features the, now

715
00:47:13,475 --> 00:47:17,085
Speaker 3:  Siri can talk to Cha G pd. I think that is table stakes and it's kind of

716
00:47:17,085 --> 00:47:20,805
Speaker 3:  weird that it wasn't there from the jump. The ones that I think are, are

717
00:47:20,825 --> 00:47:24,765
Speaker 3:  big are image playground, gen, moji and visual intelligence,

718
00:47:24,765 --> 00:47:28,165
Speaker 3:  right? Where the phone gets multimodal and it can now

719
00:47:28,595 --> 00:47:32,485
Speaker 3:  talk in like both in and out. But I

720
00:47:32,955 --> 00:47:36,405
Speaker 3:  like Gen Moji is just purely silly to me. Image playground is like

721
00:47:36,655 --> 00:47:40,485
Speaker 3:  50 steps behind, although that's probably the safest way for it to be.

722
00:47:40,685 --> 00:47:41,845
Speaker 4:  Yeah, that seems to be deliberate

723
00:47:41,945 --> 00:47:44,325
Speaker 3:  And visual intelligence is, you know, it's just the same thing we're talking

724
00:47:44,325 --> 00:47:46,725
Speaker 3:  about. We're like, I look at is this a flower? And it's like, it sure is.

725
00:47:47,105 --> 00:47:51,085
Speaker 3:  And like the next turn isn't quite there yet. And but like maybe

726
00:47:51,105 --> 00:47:51,845
Speaker 3:  I'm just being a hater.

727
00:47:51,955 --> 00:47:54,525
Speaker 4:  Well a, you're definitely just being a hater. That's certainly true.

728
00:47:54,625 --> 00:47:56,045
Speaker 3:  I'm in a real little salty moods. I don't know why

729
00:47:56,105 --> 00:47:59,125
Speaker 4:  You are. I can tell I'm enjoying this very much. You're like, what if everything

730
00:47:59,145 --> 00:48:01,685
Speaker 4:  was awful and woke and I hated it?

731
00:48:02,915 --> 00:48:04,165
Speaker 3:  Look, I know where the views are.

732
00:48:04,725 --> 00:48:08,405
Speaker 4:  I don't know what, like the gen moji thing is one that I'm, I'm really

733
00:48:08,475 --> 00:48:12,445
Speaker 4:  torn on because I feel like we've been covering apple's weird

734
00:48:12,445 --> 00:48:16,005
Speaker 4:  ideas about emoji for like the whole life of The Verge.

735
00:48:17,405 --> 00:48:21,265
Speaker 4:  And I don't think I have ever once earnestly sent

736
00:48:21,265 --> 00:48:25,105
Speaker 4:  or received one of apple's weird emoji other than

737
00:48:25,415 --> 00:48:29,265
Speaker 4:  like to goof on another tech reporter. I think if, like, if I just

738
00:48:29,265 --> 00:48:33,065
Speaker 4:  made a Jen Moji and sent it to my wife, she would like call the police.

739
00:48:33,665 --> 00:48:37,505
Speaker 4:  Like it would be so weird as, as a thing to just do

740
00:48:37,525 --> 00:48:40,945
Speaker 3:  Has, wait, has your kid discovered the, you can hold on a face in a photo

741
00:48:40,965 --> 00:48:42,065
Speaker 3:  and make a sticker of a face?

742
00:48:42,565 --> 00:48:43,585
Speaker 4:  No, thank God, because

743
00:48:43,825 --> 00:48:47,345
Speaker 3:  I have a camera roll full of those or wherever they are in the file system

744
00:48:47,525 --> 00:48:51,225
Speaker 3:  on, I like just billions of stickers like left and right

745
00:48:51,765 --> 00:48:55,065
Speaker 3:  and those are very funny to send around because they're horrifying.

746
00:48:57,175 --> 00:48:59,995
Speaker 3:  But that's very much like we use the walkie-talkie feature on our Apple watches

747
00:48:59,995 --> 00:49:02,355
Speaker 3:  and we're the only people use it with the walkie-talkie feature on Apple

748
00:49:02,355 --> 00:49:02,555
Speaker 3:  watches.

749
00:49:02,555 --> 00:49:06,475
Speaker 4:  Yeah, and I just feel like all of this stuff, image, playground, gen, Moji,

750
00:49:06,475 --> 00:49:09,995
Speaker 4:  like, and this is true o of lots of other people's products too, is like,

751
00:49:10,135 --> 00:49:13,915
Speaker 4:  I'm, I'm convinced that the, all of the AI

752
00:49:13,915 --> 00:49:17,835
Speaker 4:  image generation stuff is more of a meme than a product at this

753
00:49:17,835 --> 00:49:21,355
Speaker 4:  point. And I just don't, I don't know that there are I actual reasons

754
00:49:21,625 --> 00:49:24,715
Speaker 4:  that they exist other than somebody made them like,

755
00:49:25,755 --> 00:49:28,755
Speaker 4:  I don't know, Richard, am I crazy? Are you, are you just like sending Jen

756
00:49:28,975 --> 00:49:30,275
Speaker 4:  Mos to all your pals every day?

757
00:49:31,195 --> 00:49:35,155
Speaker 5:  I am not. I unfortunately do not have a device ready to run Apple

758
00:49:35,155 --> 00:49:38,995
Speaker 5:  Intelligence in in my home. I probably never will because I

759
00:49:38,995 --> 00:49:41,755
Speaker 5:  do not wanna send Jen Moji. I I don't wanna do this, but like

760
00:49:41,955 --> 00:49:44,635
Speaker 4:  With Gemini 2.0, you're gonna be able to generate some weird stuff too. So

761
00:49:44,635 --> 00:49:45,155
Speaker 4:  get ready when

762
00:49:45,155 --> 00:49:48,995
Speaker 5:  You open up Instagram. I just wanna like search for an account and oh, now

763
00:49:48,995 --> 00:49:51,115
Speaker 5:  I'm in the AI window. Thanks.

764
00:49:52,715 --> 00:49:53,395
Speaker 4:  I really do hate that.

765
00:49:54,345 --> 00:49:58,285
Speaker 5:  But I've, I I I, it's bizarre because I I I've never wanted

766
00:49:58,285 --> 00:50:02,195
Speaker 5:  to generate an emoji. Not that I don't know if all of

767
00:50:02,195 --> 00:50:06,035
Speaker 5:  the emojis that are in the phone exist and cover every feeling I've ever

768
00:50:06,055 --> 00:50:09,235
Speaker 5:  had, but whatever feelings I have that aren't covered by those emoji,

769
00:50:09,975 --> 00:50:11,675
Speaker 5:  I'm not in touch with you yet. I just haven't

770
00:50:11,875 --> 00:50:12,595
Speaker 3:  Considered it. It's not,

771
00:50:12,785 --> 00:50:15,915
Speaker 5:  It's not something that's, that's just dying to get out of me. That, that

772
00:50:15,915 --> 00:50:19,795
Speaker 5:  I cannot, I I'm, I must, I must find the

773
00:50:19,915 --> 00:50:23,195
Speaker 5:  zebra of many colors with, with, you know, anatomical

774
00:50:23,345 --> 00:50:26,835
Speaker 5:  incorrectness that represents how I'm feeling right now. I just, I, I haven't

775
00:50:26,835 --> 00:50:27,445
Speaker 5:  gotten it yet.

776
00:50:27,965 --> 00:50:31,165
Speaker 3:  I think it's very important for everyone listening to this right now who

777
00:50:31,165 --> 00:50:35,045
Speaker 3:  has access to Apple intelligence to pull over in their cars, send

778
00:50:35,045 --> 00:50:37,605
Speaker 3:  in that prompt and then send us whatever it

779
00:50:37,765 --> 00:50:38,045
Speaker 4:  Generates.

780
00:50:39,475 --> 00:50:43,325
Speaker 3:  Like, I, I, I issue a lot of instructions on this show. Ask

781
00:50:44,155 --> 00:50:47,565
Speaker 3:  your phone to make a zebra of many colors that's

782
00:50:47,565 --> 00:50:51,405
Speaker 3:  anatomically incorrect. That is expresses a feeling that you don't know

783
00:50:51,405 --> 00:50:55,285
Speaker 3:  you're having or whatever it is that Richard just said. And just let us know

784
00:50:55,285 --> 00:50:57,635
Speaker 3:  what it gives you. I'm dying to know.

785
00:50:57,755 --> 00:50:58,395
Speaker 4:  I feel good about that.

786
00:50:58,985 --> 00:51:02,835
Speaker 3:  Look, I, I think ultimately Apple wants it to seem like the phone is very

787
00:51:02,835 --> 00:51:06,755
Speaker 3:  capable and AI makes, whether or not it actually is capable

788
00:51:07,015 --> 00:51:09,395
Speaker 3:  AI makes it seem like the phone can do more stuff than it could do yesterday.

789
00:51:09,445 --> 00:51:13,395
Speaker 3:  Right now you can press camera control on your iPhone 16 and look at something

790
00:51:13,395 --> 00:51:16,875
Speaker 3:  and it'll be like, here's a flower you, it can write for you in a way that

791
00:51:16,955 --> 00:51:20,755
Speaker 3:  I think those ads are doing it a massive disservice to, 'cause it just seems

792
00:51:20,755 --> 00:51:21,835
Speaker 3:  like everyone's a jerk at work.

793
00:51:22,305 --> 00:51:25,675
Speaker 4:  Yeah. Every one of those ads is like, Hey, dumb ass. Yeah. One seem like

794
00:51:25,675 --> 00:51:27,235
Speaker 4:  not such a dumb ass happily

795
00:51:27,625 --> 00:51:30,195
Speaker 5:  They have, they have a real problem with those ads because they have to find

796
00:51:30,195 --> 00:51:33,795
Speaker 5:  something that is so important that you have to send the message, but also

797
00:51:33,975 --> 00:51:37,675
Speaker 5:  not so important that you need to write it yourself. And what is that?

798
00:51:38,025 --> 00:51:38,315
Speaker 5:  Like

799
00:51:38,415 --> 00:51:39,675
Speaker 3:  How do you, how do you find that

800
00:51:39,675 --> 00:51:42,395
Speaker 4:  You could conceivably be hired for, even though you're dumb as a rock,

801
00:51:43,415 --> 00:51:47,115
Speaker 3:  The one where the guy rolls his chair out of the meeting to summarize whatever

802
00:51:47,215 --> 00:51:51,075
Speaker 3:  and rolls back in the meeting. I get it. It's a very funny It is, it is,

803
00:51:51,855 --> 00:51:54,595
Speaker 3:  it is attention getting, which is the purpose of advertising, but it's like,

804
00:51:54,695 --> 00:51:58,315
Speaker 3:  oh that sucks. Like I would be like, get out of this meeting. Like I don't

805
00:51:58,315 --> 00:52:01,555
Speaker 3:  need to waste my time. If you haven't done the reading, you can tell what

806
00:52:01,555 --> 00:52:05,475
Speaker 3:  our reading's. Like, I I just think like they've made the phone slightly

807
00:52:05,475 --> 00:52:08,635
Speaker 3:  more capable, but this big apple intelligence is here moment.

808
00:52:09,575 --> 00:52:13,375
Speaker 3:  I don't know. It it, it's not the same as the chat GBT moment that

809
00:52:13,535 --> 00:52:16,815
Speaker 3:  everyone talks about. Like no, your mind is not expanded because

810
00:52:17,395 --> 00:52:21,135
Speaker 3:  the, the messages notifications are 8% better than they were in

811
00:52:21,195 --> 00:52:22,015
Speaker 3:  iOS 18.1.

812
00:52:22,675 --> 00:52:26,655
Speaker 4:  If if there's going to be that thing anytime soon, it's gonna

813
00:52:26,655 --> 00:52:29,855
Speaker 4:  be agents. And even even Apple agrees with that, right? Like you talk about

814
00:52:29,875 --> 00:52:33,375
Speaker 4:  the, the intense stuff that they're starting to build into iOS that's gonna

815
00:52:33,375 --> 00:52:37,335
Speaker 4:  get Siri more access to apps so it can actually go and like

816
00:52:38,115 --> 00:52:42,055
Speaker 4:  get information and do things inside of those apps. Like that's what everyone

817
00:52:42,055 --> 00:52:45,575
Speaker 4:  is trying to build. And if somebody gets it right in a way that is useful

818
00:52:45,595 --> 00:52:49,535
Speaker 4:  and doesn't cause a culture war Patel, that's gonna be

819
00:52:49,535 --> 00:52:51,415
Speaker 4:  the next thing. Because those will be the moments where people are like,

820
00:52:51,415 --> 00:52:54,975
Speaker 4:  oh, I, I, I now understand a thing I can ask this to do

821
00:52:55,765 --> 00:52:57,655
Speaker 4:  that is useful and new.

822
00:52:58,315 --> 00:53:01,815
Speaker 3:  The culture where I just wanna point out is in AR is in visual intelligence.

823
00:53:02,095 --> 00:53:02,535
Speaker 4:  I see, okay.

824
00:53:02,675 --> 00:53:06,455
Speaker 3:  App intense is money, right? Because Apple is

825
00:53:06,655 --> 00:53:10,135
Speaker 3:  counting on a bunch of app developers to do what they say, which they usually

826
00:53:10,155 --> 00:53:12,695
Speaker 3:  do. Yep. 'cause their customers are on the iPhone and they're in the app

827
00:53:12,695 --> 00:53:16,575
Speaker 3:  store and to say you have to do app intent, just

828
00:53:16,575 --> 00:53:19,735
Speaker 3:  like they said to a bunch of app developers, you have to do in-app purchases.

829
00:53:19,735 --> 00:53:23,295
Speaker 3:  So we can take our 30% cut. That's how you get a Siri that can actually use

830
00:53:23,295 --> 00:53:26,605
Speaker 3:  the apps. You have to build the features in a bunch of developers are gonna

831
00:53:26,625 --> 00:53:30,555
Speaker 3:  say no, I don't know why DoorDash would say yes to that. The

832
00:53:30,555 --> 00:53:34,195
Speaker 3:  answer has to be money and like no one has figured out that money yet. And

833
00:53:34,195 --> 00:53:36,995
Speaker 3:  I I, I do just keep picking on DoorDash. If anyone from DoorDash wants to

834
00:53:36,995 --> 00:53:39,595
Speaker 3:  come on and tell me that they're gonna happily let everyone disintermediate

835
00:53:39,595 --> 00:53:43,395
Speaker 3:  their service, like that's great. But I just look at that per put

836
00:53:43,515 --> 00:53:44,435
Speaker 3:  any company in that mix.

837
00:53:44,785 --> 00:53:46,275
Speaker 5:  Well look at Netflix. Yeah,

838
00:53:46,295 --> 00:53:48,555
Speaker 3:  You don't have to participate If you have your own audience

839
00:53:48,775 --> 00:53:51,515
Speaker 5:  And what they've done with the Apple TV and Vision Pro. If you're big enough

840
00:53:51,515 --> 00:53:52,835
Speaker 5:  you can just say no. Yeah.

841
00:53:52,855 --> 00:53:56,155
Speaker 3:  So I that I, I just see that coming but everyone wants to build these agents

842
00:53:56,455 --> 00:53:59,635
Speaker 3:  and the the systems they want those agents to use do not have to play ball

843
00:54:00,135 --> 00:54:04,035
Speaker 3:  and we just haven't sorted that out. Alright, so that's 18.2 people get it.

844
00:54:04,035 --> 00:54:07,715
Speaker 3:  Let us know you think, send us your multicolor emotional zebra please.

845
00:54:08,445 --> 00:54:12,435
Speaker 3:  Let's talk about soa. SOA is fascinating to me in that it doesn't

846
00:54:12,435 --> 00:54:16,235
Speaker 3:  appear to be great but it's super good enough to be like

847
00:54:16,445 --> 00:54:17,195
Speaker 3:  ultra interesting.

848
00:54:17,705 --> 00:54:21,075
Speaker 4:  Yeah, that's about my read of it. I mean I think the

849
00:54:21,665 --> 00:54:24,955
Speaker 4:  Sora is so bizarre because it

850
00:54:25,995 --> 00:54:29,935
Speaker 4:  is another one on the list of things that OpenAI is like, this is too

851
00:54:29,935 --> 00:54:33,015
Speaker 4:  good. We should barely even be allowed to launch this.

852
00:54:33,665 --> 00:54:37,535
Speaker 4:  We're desperately afraid of all the things that this will visit upon the

853
00:54:37,535 --> 00:54:41,135
Speaker 4:  world if we release it. And then there's just like here, it's a person

854
00:54:41,225 --> 00:54:44,815
Speaker 4:  doing gymnastics whose body explodes in a thousand directions every time

855
00:54:45,045 --> 00:54:48,735
Speaker 4:  they do a flip. And it's like, this is another one where I'm like, does this

856
00:54:48,735 --> 00:54:52,455
Speaker 4:  exist for any reason other than so that people will post

857
00:54:52,545 --> 00:54:55,335
Speaker 4:  about it on the internet? I, I sincerely don't know.

858
00:54:55,685 --> 00:54:59,335
Speaker 5:  Also it's not exactly available. They very quickly

859
00:54:59,675 --> 00:55:02,015
Speaker 5:  ran out of capacity and stopped accepting signups. Yeah.

860
00:55:02,015 --> 00:55:04,495
Speaker 4:  Wait, can you explain to me what happened here? I was away from the internet

861
00:55:04,495 --> 00:55:08,295
Speaker 4:  for several hours and it went from SOA is available to SOA is not

862
00:55:08,295 --> 00:55:09,695
Speaker 4:  available while I was gone. What

863
00:55:09,855 --> 00:55:12,415
Speaker 5:  Happened? Yeah. That's pretty much what happened in in like it was like a

864
00:55:12,415 --> 00:55:16,015
Speaker 5:  shoe release. Like oh it's out now it's not. And then Sam

865
00:55:16,315 --> 00:55:19,655
Speaker 5:  was like ah, you know, we just underestimated the capacity and we'll be opening

866
00:55:19,655 --> 00:55:23,215
Speaker 5:  it up at some point. So how much capacity do they actually have? How much

867
00:55:23,415 --> 00:55:26,575
Speaker 5:  capacity can they support to run this thing? We don't know. You do have to

868
00:55:26,575 --> 00:55:30,335
Speaker 5:  pay to use it. You have to have either the $20 or $200 plan. If you have

869
00:55:30,335 --> 00:55:34,215
Speaker 5:  the $200 per month chat GPT Pro plan and you got access, then you

870
00:55:34,215 --> 00:55:37,735
Speaker 5:  can make 10 80 p videos that are up to 20 seconds long. So I mean pretty

871
00:55:37,735 --> 00:55:41,455
Speaker 5:  good deal. I think like 20 seconds of 10 80 p video

872
00:55:41,755 --> 00:55:45,335
Speaker 5:  for just $200 a month. Wait two a month. Don't say everyone doesn't have

873
00:55:45,335 --> 00:55:49,295
Speaker 5:  two accounts. $200 a month on the chat GPT Pro plan. I mean you can use

874
00:55:49,315 --> 00:55:49,695
Speaker 5:  on the

875
00:55:49,915 --> 00:55:51,495
Speaker 3:  That's one they just announced. Yeah. Yeah.

876
00:55:51,495 --> 00:55:55,175
Speaker 5:  You can use the ChatGPT plus plan, but like that's a seven 20 p five seconds.

877
00:55:55,335 --> 00:55:56,455
Speaker 5:  I mean, who's gonna do that? Like

878
00:55:56,715 --> 00:56:00,575
Speaker 3:  That's nothing. You can't destabilize one country with five

879
00:56:00,575 --> 00:56:01,895
Speaker 3:  seconds of seven 20 p video.

880
00:56:04,285 --> 00:56:07,765
Speaker 3:  I mean look, I I watched Marquez Brown Lee's video about soa.

881
00:56:08,105 --> 00:56:10,845
Speaker 3:  The most fascinating piece about it is when he asked it to make a, a tech

882
00:56:10,845 --> 00:56:14,685
Speaker 3:  review video and then it had his plant in it. Yep. Because

883
00:56:14,705 --> 00:56:18,605
Speaker 3:  it has clearly trained on his videos without any permission.

884
00:56:20,455 --> 00:56:24,365
Speaker 3:  What are we doing guys like that? I I can't tell if

885
00:56:24,365 --> 00:56:26,445
Speaker 3:  Google's gonna file that lawsuit or the YouTubers will file that lawsuit

886
00:56:26,465 --> 00:56:28,965
Speaker 3:  but that lawsuit's coming just from that alone. That lawsuit's coming.

887
00:56:28,965 --> 00:56:32,405
Speaker 4:  Oh yeah. I mean yeah, there, there, there, there is so much

888
00:56:33,415 --> 00:56:37,185
Speaker 4:  obvious clear clean evidence that a lot of

889
00:56:37,185 --> 00:56:41,105
Speaker 4:  open AI is just built on YouTube. It's pretty horrifying.

890
00:56:41,135 --> 00:56:44,505
Speaker 4:  Also, Richard, I would point out you forgot one key difference between

891
00:56:44,925 --> 00:56:48,905
Speaker 4:  the $20 and the $200 chat GPT subscription, which is that

892
00:56:48,905 --> 00:56:52,705
Speaker 4:  If you pay more you can download the videos without any watermarks. Oh

893
00:56:52,705 --> 00:56:52,945
Speaker 3:  Good.

894
00:56:53,325 --> 00:56:53,545
Speaker 4:  So

895
00:56:53,545 --> 00:56:56,145
Speaker 3:  That's, you can definitely destabilize the country If you pay more than that's,

896
00:56:56,725 --> 00:56:57,145
Speaker 3:  that's,

897
00:56:57,165 --> 00:56:58,145
Speaker 5:  That's worth $200.

898
00:56:59,365 --> 00:57:02,745
Speaker 3:  Marque also said something really interesting in his video that I have just

899
00:57:02,745 --> 00:57:05,905
Speaker 3:  been ruminating about ever since he said I had it make a bunch of

900
00:57:06,025 --> 00:57:09,985
Speaker 3:  CCTV footage because it's so bad that people would

901
00:57:10,005 --> 00:57:13,945
Speaker 3:  assume it's real. Like he intentionally generated footage that was

902
00:57:13,945 --> 00:57:17,665
Speaker 3:  already low quality of like cars driving down the street but in like black

903
00:57:17,665 --> 00:57:21,225
Speaker 3:  and white security camera footage. And that's like super interesting right?

904
00:57:21,225 --> 00:57:24,825
Speaker 3:  Because they are 22nd clips with no sound

905
00:57:25,525 --> 00:57:29,465
Speaker 3:  and people experience a lot of grainy low quality clips

906
00:57:29,465 --> 00:57:33,305
Speaker 3:  with no sound that they take to be reality in a lot of contexts. Particularly,

907
00:57:33,445 --> 00:57:36,265
Speaker 3:  Hey we found some security camera footage or we found some ring footage or

908
00:57:36,385 --> 00:57:40,305
Speaker 3:  whatever it is. So you can pay to get rid of the visible watermark.

909
00:57:40,305 --> 00:57:44,185
Speaker 3:  You pay $2 a month or you just crop it whatever. Are they doing C

910
00:57:44,185 --> 00:57:46,865
Speaker 3:  two PA the the like the fancy embedded one?

911
00:57:47,365 --> 00:57:50,585
Speaker 5:  Yes. OpenAI says that videos generated with, so we will have both visible

912
00:57:50,585 --> 00:57:54,385
Speaker 5:  watermarks If you have the cheap version and C two P metadata to

913
00:57:54,665 --> 00:57:58,545
Speaker 5:  indicate that they're made with ai. We, we hope that that works in in safe

914
00:57:58,545 --> 00:57:59,185
Speaker 5:  civilization.

915
00:57:59,455 --> 00:58:03,305
Speaker 3:  I'll say we've written a lot about C two PA, the content authenticity initiative.

916
00:58:03,615 --> 00:58:07,345
Speaker 3:  Jess Weathered has done incredible work just trying to understand what it

917
00:58:07,345 --> 00:58:10,705
Speaker 3:  is and how it will play out and whether it will actually mean anything to

918
00:58:10,705 --> 00:58:14,505
Speaker 3:  anyone. And every time we read about it we get a bunch of angry notes from

919
00:58:14,505 --> 00:58:18,265
Speaker 3:  readers who are, who have been deep in it, who are like, this is a

920
00:58:18,265 --> 00:58:22,145
Speaker 3:  total fraud. Like this will never work the way you want it to for infinity

921
00:58:22,145 --> 00:58:26,065
Speaker 3:  reasons. But it's the thing that we have and that's why

922
00:58:26,065 --> 00:58:28,105
Speaker 3:  I keep writing about it. 'cause there's nothing else. There's, there's no

923
00:58:28,105 --> 00:58:32,065
Speaker 3:  other choice. There's isn't like market competition to this metadata standard.

924
00:58:32,135 --> 00:58:35,945
Speaker 3:  Like there's this one and Google has chosen to use it and it sounds like

925
00:58:35,945 --> 00:58:39,865
Speaker 3:  open eyes using it, Adobe's using it. The platforms have to

926
00:58:39,865 --> 00:58:43,665
Speaker 3:  start using it, right? The YouTubes and the Facebooks and the

927
00:58:43,735 --> 00:58:47,705
Speaker 3:  tiktoks have to agree to display this metadata and they're

928
00:58:47,705 --> 00:58:51,545
Speaker 3:  not uniformly agreeing to it and Apple hasn't chosen to

929
00:58:51,665 --> 00:58:53,545
Speaker 3:  use it on the creation side. So we're just

930
00:58:53,545 --> 00:58:57,465
Speaker 4:  Still, meanwhile all those platforms are furiously building AI tools

931
00:58:57,465 --> 00:59:01,385
Speaker 4:  of their own and have exactly no incentive to point out when things

932
00:59:01,385 --> 00:59:05,265
Speaker 4:  are made with ai. Like I just, I who benefits from doing

933
00:59:05,265 --> 00:59:08,705
Speaker 4:  that other than regular people and thus why would they do it?

934
00:59:09,375 --> 00:59:13,185
Speaker 3:  Yeah, we're just headed towards, I think what where we're gonna end up

935
00:59:13,185 --> 00:59:16,865
Speaker 3:  with is a series of more closed platforms

936
00:59:17,655 --> 00:59:21,605
Speaker 3:  that basically guarantee you stuff from like real people

937
00:59:22,545 --> 00:59:25,765
Speaker 3:  and then open platforms where it's a free for all and maybe that's gonna

938
00:59:25,765 --> 00:59:29,445
Speaker 3:  be good. Like honestly maybe it's good to be like on, if I open this app,

939
00:59:30,125 --> 00:59:33,605
Speaker 3:  I know it's a bunch of like Hollywood movie directors who have actually made

940
00:59:33,605 --> 00:59:37,565
Speaker 3:  the thing and if I open this app, who knows what random

941
00:59:37,785 --> 00:59:41,405
Speaker 3:  AI generated slop I will get And like maybe that's fine. Maybe that's the

942
00:59:41,405 --> 00:59:44,765
Speaker 3:  thing that breaks apart the internet in that way or like causes that fragmentation.

943
00:59:44,845 --> 00:59:48,125
Speaker 3:  I just, I don't know. But it's, we're just at a point now where

944
00:59:48,865 --> 00:59:52,525
Speaker 3:  all of the channels we're used to are gonna get completely

945
00:59:52,755 --> 00:59:54,765
Speaker 3:  infused with AI generat content. I

946
00:59:54,765 --> 00:59:58,645
Speaker 4:  Think the only problem with that outcome is I think the audience split between

947
00:59:58,645 --> 01:00:02,605
Speaker 4:  those two different kinds of platforms would not be what you

948
01:00:02,605 --> 01:00:03,245
Speaker 4:  hope it would be.

949
01:00:03,465 --> 01:00:06,925
Speaker 3:  Oh I think it, yeah. One will be expensive. Yeah. One will have a small audience

950
01:00:06,925 --> 01:00:10,765
Speaker 3:  and cost money like that that that seems correct. But I even now in TikTok

951
01:00:10,805 --> 01:00:14,285
Speaker 3:  I see videos and it it just like silly stuff. It's like

952
01:00:14,665 --> 01:00:17,565
Speaker 3:  here's a big wave crashing over a building and it's like pretty obviously

953
01:00:17,595 --> 01:00:19,805
Speaker 3:  real and then all the comments are like this is ai.

954
01:00:20,065 --> 01:00:20,445
Speaker 4:  Oh yeah.

955
01:00:20,745 --> 01:00:24,725
Speaker 3:  And we've just destabilized that entire information economy. Yeah. LeBron

956
01:00:24,745 --> 01:00:26,645
Speaker 3:  was never dunked on that. Was AI

957
01:00:29,445 --> 01:00:33,125
Speaker 3:  a little more, there's more AI stuff, tragedy, we launched

958
01:00:33,185 --> 01:00:35,005
Speaker 3:  Canvas view. What's going on there?

959
01:00:35,725 --> 01:00:39,705
Speaker 4:  Canvas is basically the, the slightly more sort of interactive

960
01:00:39,705 --> 01:00:43,025
Speaker 4:  thing that you can do where like If you have it write something for you,

961
01:00:43,045 --> 01:00:46,665
Speaker 4:  it can display the thing that it's making next to the conversation you're

962
01:00:46,665 --> 01:00:49,745
Speaker 4:  having about it and you can sort of edit the thing in real time with the

963
01:00:49,765 --> 01:00:53,705
Speaker 4:  bot. This is the thing that Claude has been doing philanthropic spot

964
01:00:53,805 --> 01:00:57,225
Speaker 4:  for a while and I think it's actually very clever, right? Like if, If you

965
01:00:57,995 --> 01:01:01,145
Speaker 4:  allow for the possibility that maybe

966
01:01:02,235 --> 01:01:06,145
Speaker 4:  generating several paragraphs of nah text is the best thing a chat

967
01:01:06,145 --> 01:01:08,625
Speaker 4:  bot can do, this is actually a pretty good version of that

968
01:01:09,795 --> 01:01:13,205
Speaker 4:  interaction where like you can have it to a thing and it, it generates it

969
01:01:13,205 --> 01:01:15,805
Speaker 4:  in the right column and then you can, you can go back and forth with the

970
01:01:15,805 --> 01:01:18,765
Speaker 4:  bot tuning it and changing it and you can make changes to it and it becomes

971
01:01:18,765 --> 01:01:21,885
Speaker 4:  this sort of interactive thing rather than just

972
01:01:22,465 --> 01:01:25,685
Speaker 4:  blocks of messages, which is what we've had before. This is like the big

973
01:01:25,685 --> 01:01:28,845
Speaker 4:  trend here with all these chat bots is everyone has run up against

974
01:01:29,585 --> 01:01:33,565
Speaker 4:  how much you can do inside of a back and forth messaging system. So they're

975
01:01:33,565 --> 01:01:37,165
Speaker 4:  all trying to tack on little bits of UI and little bits of features that

976
01:01:37,165 --> 01:01:40,885
Speaker 4:  don't feel like that but still work inside that kind of

977
01:01:40,895 --> 01:01:44,845
Speaker 4:  basic construct. So I think in, in that case like Canvas is, is a

978
01:01:44,845 --> 01:01:47,925
Speaker 4:  good idea whether you're, you know, writing stuff or writing code or whatever

979
01:01:49,435 --> 01:01:52,885
Speaker 4:  it's been, it's been out, it's been tested for a while. I, I haven't heard

980
01:01:52,885 --> 01:01:56,085
Speaker 4:  a ton about it but the people I've heard from seem to like it. So it seems

981
01:01:56,085 --> 01:01:56,805
Speaker 4:  good. This

982
01:01:56,805 --> 01:01:59,885
Speaker 3:  Thing you're saying about UI across all of these products is super interesting

983
01:01:59,885 --> 01:02:03,775
Speaker 3:  to me. Like Chad CBD was the moment, right? Everyone

984
01:02:03,775 --> 01:02:07,375
Speaker 3:  talks about it like my eyes were opened, I could see the world in color

985
01:02:07,675 --> 01:02:11,415
Speaker 3:  and then like two years later we're like actually that interface

986
01:02:11,415 --> 01:02:14,495
Speaker 3:  isn't like we have to build all these other kinds of products

987
01:02:15,275 --> 01:02:19,055
Speaker 3:  to make this useful and the arms race of like what will

988
01:02:19,170 --> 01:02:20,620
Speaker 3:  the chatbots themselves look like

989
01:02:22,295 --> 01:02:25,095
Speaker 3:  Fundamentally, I think they're gonna get subsumed into operating systems

990
01:02:25,155 --> 01:02:28,495
Speaker 3:  the way that Apple is subsuming them into Siri and then that they'll be more

991
01:02:28,495 --> 01:02:31,895
Speaker 3:  specialized for other tasks. That's my guess. But it's interesting to see

992
01:02:31,895 --> 01:02:32,375
Speaker 3:  them rev it.

993
01:02:32,655 --> 01:02:35,615
Speaker 4:  I think that's probably right. I mean, and I think like the, the way that

994
01:02:35,805 --> 01:02:39,615
Speaker 4:  I've come to frame, the way that I'm thinking about AI for next year

995
01:02:39,795 --> 01:02:43,575
Speaker 4:  is the last two years have been like running towards

996
01:02:43,595 --> 01:02:47,415
Speaker 4:  the point of diminishing returns for the actual underlying technology

997
01:02:48,075 --> 01:02:51,415
Speaker 4:  and we're there, right? This stuff will keep getting better. There might

998
01:02:51,435 --> 01:02:55,335
Speaker 4:  be some incredible physics miracle that changes

999
01:02:55,555 --> 01:02:59,415
Speaker 4:  how all of this stuff works, but on the path that we're on this stuff

1000
01:02:59,415 --> 01:03:02,975
Speaker 4:  is going to keep getting better but much slower than it has before. And so

1001
01:03:03,035 --> 01:03:06,795
Speaker 4:  now the question is, is any of this useful for

1002
01:03:07,075 --> 01:03:09,835
Speaker 4:  anything? And we have not had to ask that question because we've been on

1003
01:03:09,835 --> 01:03:13,595
Speaker 4:  basically a two year run for all these companies of novelty

1004
01:03:14,055 --> 01:03:17,835
Speaker 4:  and they've made it a really long way just by finding

1005
01:03:18,155 --> 01:03:22,115
Speaker 4:  slightly new seeming things to do and just the fact that I could

1006
01:03:22,115 --> 01:03:24,715
Speaker 4:  take a picture and say, is this a flower? And I would say yes, it's a flower

1007
01:03:24,815 --> 01:03:28,715
Speaker 4:  was like novel and cool and exciting and we are at the end of

1008
01:03:28,715 --> 01:03:31,915
Speaker 4:  that moment and, and I think you're starting to see it like the disillusionment

1009
01:03:31,915 --> 01:03:35,795
Speaker 4:  with what all these things are attempting to do and whether they're

1010
01:03:35,995 --> 01:03:39,395
Speaker 4:  actually good is coming and it's here. And so the question now is like

1011
01:03:39,965 --> 01:03:43,955
Speaker 4:  let's assume the technology is not going to get several orders of

1012
01:03:44,195 --> 01:03:47,395
Speaker 4:  magnitude better next year. What else is there to do with the technology

1013
01:03:47,395 --> 01:03:49,075
Speaker 4:  that we have? Is the question,

1014
01:03:49,625 --> 01:03:52,995
Speaker 3:  Okay, so here's my answer. It's the last one of this section. The most useful

1015
01:03:52,995 --> 01:03:55,915
Speaker 3:  thing I've seen with AI yet Reddit built Reddit answers

1016
01:03:56,685 --> 01:04:00,235
Speaker 3:  which lets you just search Reddit without having to go through Google and

1017
01:04:00,235 --> 01:04:03,475
Speaker 3:  it has AI in it so it'll summarize a bunch of Reddit threads. So when you

1018
01:04:03,475 --> 01:04:07,435
Speaker 3:  search for whatever, it'll show you a bunch of Reddit answers but summarize

1019
01:04:07,515 --> 01:04:09,635
Speaker 3:  'em with ai. That might be the best thing yet.

1020
01:04:10,105 --> 01:04:13,875
Speaker 4:  This might be the best AI feature of all time. I

1021
01:04:13,875 --> 01:04:17,035
Speaker 4:  haven't used it. J Peter's used it a little bit and had I would say sort

1022
01:04:17,035 --> 01:04:20,955
Speaker 4:  of mixed experiences with it. But the

1023
01:04:20,955 --> 01:04:24,755
Speaker 4:  idea of being able to kind of poll all of Reddit with a question

1024
01:04:24,755 --> 01:04:28,655
Speaker 4:  that you have sounds amazing and it doesn't

1025
01:04:28,885 --> 01:04:32,495
Speaker 4:  work quite as well even in Google search, which

1026
01:04:32,645 --> 01:04:36,175
Speaker 4:  made this very expensive deal with Reddit to get the data because it just,

1027
01:04:36,175 --> 01:04:40,055
Speaker 4:  it just isn't compiling them the same way. But like if I

1028
01:04:40,055 --> 01:04:43,775
Speaker 4:  could just quickly be like, okay, what, what baby

1029
01:04:43,885 --> 01:04:47,215
Speaker 4:  gear does everybody on Reddit think you should buy? Like that that information

1030
01:04:47,215 --> 01:04:51,055
Speaker 4:  exists inside of Reddit and if it can go and

1031
01:04:51,295 --> 01:04:53,895
Speaker 4:  actually like compile all of that stuff in a way that works and makes sense,

1032
01:04:54,685 --> 01:04:56,695
Speaker 4:  kick ass dude I will use that all the time.

1033
01:04:56,845 --> 01:05:00,655
Speaker 3:  Okay, now that I've said that, I do agree this is very good there. Of course

1034
01:05:01,555 --> 01:05:03,735
Speaker 3:  the supplied examples are

1035
01:05:03,845 --> 01:05:05,535
Speaker 4:  What does Reddit think happened on January 6th?

1036
01:05:07,995 --> 01:05:09,495
Speaker 3:  The the, the one

1037
01:05:11,275 --> 01:05:15,135
Speaker 3:  Jay put in was tips for flying with the baby for the first time. Pretty

1038
01:05:15,135 --> 01:05:19,005
Speaker 3:  good like a perfect Reddit query, right? Like and

1039
01:05:19,025 --> 01:05:22,965
Speaker 3:  and honestly I what I want outta that Reddit experience is like give me 5,000

1040
01:05:22,965 --> 01:05:26,705
Speaker 3:  stories about this, right? Yeah. And so it summarized everyone's stories

1041
01:05:27,005 --> 01:05:27,585
Speaker 3:  and the answers

1042
01:05:34,165 --> 01:05:34,925
Speaker 3:  in first class helpful,

1043
01:05:35,415 --> 01:05:35,765
Speaker 4:  Solid.

1044
01:05:36,195 --> 01:05:39,885
Speaker 3:  Cool. I personally find my private shot very helpful with our baby

1045
01:05:40,075 --> 01:05:43,725
Speaker 3:  feed them during takeoff or landing classic advice by the way. And then bring

1046
01:05:43,725 --> 01:05:47,565
Speaker 3:  plenty of snacks and drinks to keep your baby occupied. And it's like, well

1047
01:05:47,565 --> 01:05:48,805
Speaker 3:  yeah, that's just stuff. Okay

1048
01:05:48,805 --> 01:05:51,845
Speaker 4:  Wait, this is very fun. I might just immediately take back everything I just

1049
01:05:51,845 --> 01:05:55,085
Speaker 4:  said because If you go down to the next screenshot, which is the Google searches,

1050
01:05:55,585 --> 01:05:56,005
Speaker 4:  the stuff

1051
01:06:03,055 --> 01:06:06,925
Speaker 4:  Don't board first. Good tips. Being strapped into the car seat is the safest

1052
01:06:06,925 --> 01:06:10,685
Speaker 4:  way for baby to fly. Good tip baby will need to be in a car seat. Baby will

1053
01:06:10,685 --> 01:06:13,325
Speaker 4:  also need to be held by you the entire flight. That seems confusing.

1054
01:06:14,745 --> 01:06:16,325
Speaker 3:  You're holding the car seat, holding the

1055
01:06:16,325 --> 01:06:16,725
Speaker 4:  Car seat.

1056
01:06:18,875 --> 01:06:21,485
Speaker 4:  Take some sanitizing wipes to clean the areas and plan your baby will be

1057
01:06:21,605 --> 01:06:24,165
Speaker 4:  touching. Have a light pad you can use for changing a plane. All of that

1058
01:06:24,165 --> 01:06:25,885
Speaker 4:  is more useful than this stuff that by the

1059
01:06:25,885 --> 01:06:28,925
Speaker 3:  Way, can I say none of that is the most useful tip that we found with with

1060
01:06:29,025 --> 01:06:32,845
Speaker 3:  Max, which is just bring five packs of differently colored post-it

1061
01:06:32,845 --> 01:06:33,925
Speaker 3:  notes and let her go crazy.

1062
01:06:34,185 --> 01:06:38,075
Speaker 4:  Ooh, that's good. Mine was just snacks. We bought I would say

1063
01:06:38,205 --> 01:06:39,995
Speaker 4:  40 different snacks on the channel. It's not

1064
01:06:39,995 --> 01:06:43,955
Speaker 3:  A parenting podcast, I'm just saying it's just funny, right? 'cause If you

1065
01:06:43,955 --> 01:06:47,475
Speaker 3:  take the world's body of information and Reddit, which is a lot of

1066
01:06:47,875 --> 01:06:51,755
Speaker 3:  personal experiences which are kind of useful and like for this kind of

1067
01:06:51,755 --> 01:06:55,355
Speaker 3:  thing where there's not an answer and then you try to shove it into the form

1068
01:06:55,355 --> 01:06:58,875
Speaker 3:  of this is the right answer you end up with put baby in chair.

1069
01:06:59,955 --> 01:07:00,245
Speaker 3:  Yeah.

1070
01:07:00,465 --> 01:07:03,365
Speaker 5:  That's the problem that we've run into with Apple Intelligence. Like notification

1071
01:07:03,365 --> 01:07:06,885
Speaker 5:  summaries. Like notifications were already summarized. Like the people who

1072
01:07:06,885 --> 01:07:10,845
Speaker 5:  wrote the post already wrote them to be read by someone who wasn't really

1073
01:07:10,845 --> 01:07:14,325
Speaker 5:  paying attention and didn't have a lot of time and they, they laid it out

1074
01:07:14,325 --> 01:07:17,485
Speaker 5:  in a way they were upvoted because they were written well and they got you

1075
01:07:17,485 --> 01:07:20,765
Speaker 5:  the information they needed. You didn't need to summarize them and and extract

1076
01:07:20,765 --> 01:07:24,445
Speaker 5:  further value out of one particular line. You needed the whole thing

1077
01:07:25,825 --> 01:07:25,945
Speaker 4:  A

1078
01:07:25,945 --> 01:07:28,385
Speaker 5:  Hundred percent. We did it the wrong way. I

1079
01:07:28,865 --> 01:07:32,825
Speaker 4:  Mean this is, this is the the like ongoing, it's like,

1080
01:07:32,825 --> 01:07:36,705
Speaker 4:  it's not quite context collapse but it is, it is just

1081
01:07:36,725 --> 01:07:40,585
Speaker 4:  the pure like lowest common denominator of ai

1082
01:07:40,835 --> 01:07:44,385
Speaker 4:  which is that If you take everything and you try to shove it into two sentences,

1083
01:07:44,565 --> 01:07:48,545
Speaker 4:  you're going to get two incredibly uninteresting sentences. And

1084
01:07:48,545 --> 01:07:51,745
Speaker 4:  I think what we've seen from some others, like I think Notebook LM from Google

1085
01:07:51,775 --> 01:07:55,505
Speaker 4:  does a pretty good job of this, of basically it, it goes and

1086
01:07:55,515 --> 01:07:59,065
Speaker 4:  finds stuff and then just hands you a bunch of sources, right? Like what

1087
01:07:59,065 --> 01:08:02,945
Speaker 4:  I want from this Reddit thing is not Pat

1088
01:08:02,945 --> 01:08:06,905
Speaker 4:  answers in paragraphs. I want a bunch of links to like here are,

1089
01:08:06,905 --> 01:08:10,105
Speaker 4:  here are the 10, like you said Neli, here are the 10 most

1090
01:08:10,755 --> 01:08:14,265
Speaker 4:  loved and contentious and funny responses that we've ever gotten to this

1091
01:08:14,385 --> 01:08:16,885
Speaker 4:  question that everybody replied to. Oh my god, I tried this and it worked.

1092
01:08:16,995 --> 01:08:20,765
Speaker 4:  Yeah, that's how you do this, right? Not by saying here is the

1093
01:08:20,765 --> 01:08:24,725
Speaker 4:  rough summary of 12,000 Reddit posts and that's what too many of

1094
01:08:24,725 --> 01:08:25,885
Speaker 4:  these services are trying to do.

1095
01:08:26,115 --> 01:08:30,045
Speaker 3:  Yeah, that's it. I do think Reddit is, it's where I see the most opportunity.

1096
01:08:30,245 --> 01:08:34,125
Speaker 3:  'cause Reddit is vast, right? And like you can burn a lot of time hearing

1097
01:08:34,135 --> 01:08:37,485
Speaker 3:  every version of everything on Reddit and having a tool that helps you like

1098
01:08:37,485 --> 01:08:40,325
Speaker 3:  get through that better is kind of interesting. I'm just

1099
01:08:41,385 --> 01:08:43,885
Speaker 3:  it, you know, it's like that's one example. I'm sure there are other ones,

1100
01:08:43,885 --> 01:08:47,165
Speaker 3:  but the glory of Reddit is that it's a bunch of people

1101
01:08:48,005 --> 01:08:51,935
Speaker 3:  telling you what they experienced. It's very rarely that there's a right

1102
01:08:51,935 --> 01:08:55,575
Speaker 3:  answer, right? It's just like I'm gonna synthesize all this information from

1103
01:08:55,575 --> 01:08:58,175
Speaker 3:  all these people who are telling me about whatever, or just talking about

1104
01:08:58,305 --> 01:09:01,895
Speaker 3:  their fandom or whatever it is. And that makes me feel like part of a community.

1105
01:09:01,895 --> 01:09:02,615
Speaker 3:  It makes me feel,

1106
01:09:04,275 --> 01:09:06,535
Speaker 3:  it makes me feel like part of a community. It makes me feel validated that

1107
01:09:06,535 --> 01:09:10,495
Speaker 3:  like I might have some experiences too. Not I need to, I just shove in a

1108
01:09:10,495 --> 01:09:10,855
Speaker 3:  car seat.

1109
01:09:13,765 --> 01:09:17,295
Speaker 3:  Okay, we gotta take a break. We got we back with lightning round Boy this

1110
01:09:17,295 --> 01:09:19,455
Speaker 3:  is, this is gonna be, this is an action packed lightning round. We'll be

1111
01:09:19,455 --> 01:09:19,735
Speaker 3:  right back.

1112
01:14:12,265 --> 01:14:15,645
Speaker 3:  All right, we're back with the lightning round. Liam, who is the Lightning

1113
01:14:15,645 --> 01:14:16,045
Speaker 3:  Round sponsor,

1114
01:14:16,435 --> 01:14:20,005
Speaker 1:  This week's lightning round is presented by Amazon Queue, the new

1115
01:14:20,005 --> 01:14:22,245
Speaker 1:  generative AI assistant from AWS

1116
01:14:22,745 --> 01:14:26,445
Speaker 3:  Oh, it's so good. Have we started adding the, the cash register sound effect?

1117
01:14:27,565 --> 01:14:31,365
Speaker 1:  I think we only did it on the first one since you asked for the effect, but

1118
01:14:31,385 --> 01:14:33,285
Speaker 1:  If you, If you want that to be a regular thing, we could,

1119
01:14:34,125 --> 01:14:36,925
Speaker 3:  I feel like we got in a lot of trouble with an episode recently by asking

1120
01:14:36,985 --> 01:14:38,405
Speaker 3:  for sound effects that we didn't deliver.

1121
01:14:38,825 --> 01:14:42,765
Speaker 4:  So also if you'd like to make a Jen Moji of Neli being showered in cash and

1122
01:14:42,765 --> 01:14:44,805
Speaker 4:  send it to me, I'm good with that. Right

1123
01:14:44,805 --> 01:14:48,725
Speaker 3:  Next to our, our emotional zebra. Alright, lightning round.

1124
01:14:49,325 --> 01:14:52,845
Speaker 3:  David, you wrote about this this week. YouTube is still growing fast on TVs

1125
01:14:52,845 --> 01:14:53,445
Speaker 3:  in the living room. What's going

1126
01:14:53,445 --> 01:14:57,085
Speaker 4:  On here? Yeah, so I would say statistically, If you are

1127
01:14:57,835 --> 01:15:00,845
Speaker 4:  absorbing this podcast right now, I don't even know what word to use anymore.

1128
01:15:01,125 --> 01:15:04,525
Speaker 4:  If you are experiencing this podcast, there is an

1129
01:15:04,525 --> 01:15:07,925
Speaker 4:  increasingly large chance you're watching us on your television in your living

1130
01:15:07,925 --> 01:15:09,525
Speaker 4:  room. If you are, hello?

1131
01:15:10,105 --> 01:15:13,845
Speaker 3:  Oh, it's only Love you if you're watching this on some janky side lid lcd,

1132
01:15:13,885 --> 01:15:16,245
Speaker 3:  I still love you. You should actually watch it. Watch it like several times

1133
01:15:16,605 --> 01:15:16,685
Speaker 3:  actually.

1134
01:15:16,955 --> 01:15:20,485
Speaker 4:  Yeah, watch it on all of your different TVs just to check the resolution

1135
01:15:20,485 --> 01:15:24,165
Speaker 4:  differences I would say. Yeah, but no, basically YouTube put out a bunch

1136
01:15:24,165 --> 01:15:27,525
Speaker 4:  of stats this week about how fast its living room stuff is growing.

1137
01:15:28,675 --> 01:15:32,405
Speaker 4:  400 million hours a month of podcast viewing is happening

1138
01:15:32,585 --> 01:15:36,285
Speaker 4:  on TVs, sports stuff is way up. They launched this new feature

1139
01:15:36,435 --> 01:15:39,765
Speaker 4:  that I think is kind of cool, it's called Watch with. And essentially

1140
01:15:40,155 --> 01:15:44,125
Speaker 4:  what they discovered is that a lot of people are watching a sporting

1141
01:15:44,125 --> 01:15:48,005
Speaker 4:  event with live commentary from somebody streaming and talking about it

1142
01:15:48,105 --> 01:15:50,485
Speaker 4:  and they're now smooshing those things together. So creators are gonna be

1143
01:15:50,485 --> 01:15:54,245
Speaker 4:  able to actually like do their own commentary over top of something that

1144
01:15:54,245 --> 01:15:56,605
Speaker 4:  they're watching and they're starting with sports but also made it pretty

1145
01:15:56,605 --> 01:15:59,765
Speaker 4:  clear that that's gonna come to other things. So like Kurt Wilms, the guy

1146
01:15:59,765 --> 01:16:02,885
Speaker 4:  I was talking to who runs product for YouTube's living room stuff

1147
01:16:03,525 --> 01:16:06,165
Speaker 4:  specifically mentioned like what about an Apple keynote where there are all

1148
01:16:06,165 --> 01:16:09,725
Speaker 4:  these creators who want to talk about it and comment on it. That's just a

1149
01:16:09,725 --> 01:16:13,045
Speaker 4:  thing we can offer that you can get all these different commentary streams.

1150
01:16:13,845 --> 01:16:14,605
Speaker 4:  I think that's very cool.

1151
01:16:14,865 --> 01:16:17,085
Speaker 3:  I'm sure Apple will have some, some very interesting thoughts about that.

1152
01:16:17,085 --> 01:16:17,525
Speaker 3:  Yeah, what

1153
01:16:17,525 --> 01:16:21,285
Speaker 4:  Could possibly go wrong? Who's gonna be mad about new commentary on

1154
01:16:21,285 --> 01:16:25,245
Speaker 4:  every movie that exists in the world, but more broadly

1155
01:16:25,355 --> 01:16:29,285
Speaker 4:  like YouTube is now huge on TVs

1156
01:16:29,505 --> 01:16:33,125
Speaker 4:  and has gone undergone this shift. I think really in the last year, like

1157
01:16:33,125 --> 01:16:36,405
Speaker 4:  I follow this company pretty closely and they used to talk about the TVs

1158
01:16:36,405 --> 01:16:40,285
Speaker 4:  as just kind of another place. People watched YouTube and now they're like

1159
01:16:40,285 --> 01:16:44,045
Speaker 4:  really thinking about it as like a primary platform

1160
01:16:44,185 --> 01:16:48,005
Speaker 4:  for YouTube and they're building features for the TV kind of for the first

1161
01:16:48,005 --> 01:16:51,485
Speaker 4:  time. Like they have this shows thing where you can put up a bunch of videos

1162
01:16:51,785 --> 01:16:54,685
Speaker 4:  in seasons and episodes that makes it feel more like a streaming service,

1163
01:16:55,035 --> 01:16:58,965
Speaker 4:  that're doing the watch with stuff. They're doing a lot of work

1164
01:16:59,065 --> 01:17:02,845
Speaker 4:  to sync your phone with your tv. Like YouTube has

1165
01:17:02,845 --> 01:17:05,845
Speaker 4:  always kind of wanted to be Netflix in a certain way and I think

1166
01:17:06,735 --> 01:17:10,305
Speaker 4:  we're starting to see it push to be a little more premium, a little higher

1167
01:17:10,365 --> 01:17:14,145
Speaker 4:  end, a little more TV centric. They're convinced they can do this

1168
01:17:14,145 --> 01:17:18,065
Speaker 4:  without killing YouTube as a product elsewhere. But that,

1169
01:17:18,135 --> 01:17:20,865
Speaker 4:  that tension feels very real to me and I think it's really fascinating.

1170
01:17:21,095 --> 01:17:25,065
Speaker 3:  Yeah, I will, can I say one thing on YouTube very quickly? We've like

1171
01:17:25,065 --> 01:17:27,505
Speaker 3:  talked about enough about a business in the past couple weeks. I don't wanna

1172
01:17:27,505 --> 01:17:30,945
Speaker 3:  overdo it, but I have a story on the website this week.

1173
01:17:32,585 --> 01:17:35,625
Speaker 3:  A lot of people complain that our YouTube videos on our website don't let

1174
01:17:35,625 --> 01:17:38,865
Speaker 3:  you click the title to go to YouTube and everyone thinks that's our fault.

1175
01:17:40,295 --> 01:17:43,255
Speaker 3:  'cause it makes sense that the publisher would want you to keep you on the

1176
01:17:43,255 --> 01:17:46,235
Speaker 3:  website. That it just, it makes sense that people

1177
01:17:46,345 --> 01:17:49,555
Speaker 4:  Well and because forever every YouTube video on the internet, you've been

1178
01:17:49,555 --> 01:17:52,195
Speaker 4:  able to, you, you hover over it, you click the title and it takes you

1179
01:17:52,195 --> 01:17:54,235
Speaker 3:  YouTube, YouTube, YouTube and in particular on phone. You wanna open the

1180
01:17:54,235 --> 01:17:56,515
Speaker 3:  YouTube app, there's all this stuff you wanna do, right? I, I hope people

1181
01:17:56,515 --> 01:17:59,955
Speaker 3:  know that The Verge is like very pro links. We have links everywhere, like

1182
01:17:59,955 --> 01:18:03,515
Speaker 3:  links on the homepage all the time that's on our fault. It is a YouTube decision

1183
01:18:03,535 --> 01:18:07,395
Speaker 3:  to disable those links and if we want to re-enable them, we

1184
01:18:07,395 --> 01:18:10,195
Speaker 3:  have to make less money in our YouTube videos you can read the whole story.

1185
01:18:11,235 --> 01:18:14,355
Speaker 3:  I was just very annoyed that I spent months being like, can you turn on these

1186
01:18:14,355 --> 01:18:17,915
Speaker 3:  links back on? And the answer at the end of the day was a hard no

1187
01:18:18,135 --> 01:18:21,355
Speaker 3:  unless we choose to either not use YouTube at all or make less money

1188
01:18:22,275 --> 01:18:24,115
Speaker 3:  and I'm not really part of the business side, made the business side make

1189
01:18:24,115 --> 01:18:26,715
Speaker 3:  a different decision. But I was like, that sucks. I'm just gonna tell everyone

1190
01:18:26,715 --> 01:18:29,235
Speaker 3:  about this. And all the YouTube people just made a very sad face at me.

1191
01:18:29,415 --> 01:18:33,115
Speaker 4:  If the internet goes back to every publisher having their own like

1192
01:18:33,125 --> 01:18:36,995
Speaker 4:  awful bespoke video player, again, I'm gonna be so angry,

1193
01:18:38,185 --> 01:18:41,595
Speaker 4:  like feel however you want about YouTube. It's a good video player. And the

1194
01:18:41,795 --> 01:18:44,635
Speaker 4:  internet was filled with bad video players for so long

1195
01:18:44,735 --> 01:18:48,515
Speaker 3:  By the way the links worked and we made the more money since 2016.

1196
01:18:48,775 --> 01:18:52,635
Speaker 3:  It was all fine. And then in the start of this year they

1197
01:18:52,635 --> 01:18:55,995
Speaker 3:  changed it to quote, remove their branding and they think the link

1198
01:18:56,415 --> 01:18:57,955
Speaker 3:  to YouTube counts as branding.

1199
01:18:58,675 --> 01:18:59,295
Speaker 4:  That's dumb

1200
01:19:00,005 --> 01:19:03,775
Speaker 3:  Dumb. Let me tell you how much of Brad I've been for the past

1201
01:19:04,075 --> 01:19:06,735
Speaker 3:  two and a half months and the answer was a hard no. I was like, I'm gonna

1202
01:19:06,735 --> 01:19:07,015
Speaker 3:  wrap. Is

1203
01:19:07,015 --> 01:19:09,095
Speaker 4:  Vimeo still around? Can Vimeo fix this? Sure.

1204
01:19:09,095 --> 01:19:11,775
Speaker 3:  Alright, here's a really weird one. I'm obsessed with this one.

1205
01:19:12,605 --> 01:19:16,375
Speaker 3:  Instagram is gonna let creators test reels

1206
01:19:17,035 --> 01:19:20,855
Speaker 3:  on people that don't follow them. So like, Adamis

1207
01:19:21,015 --> 01:19:23,815
Speaker 3:  Harry made this release, you go watch it. It's very interesting as a, as

1208
01:19:23,855 --> 01:19:27,295
Speaker 3:  a sociological document of how Adam thinks

1209
01:19:27,605 --> 01:19:31,575
Speaker 3:  Instagram creators think about Instagram. Like watch

1210
01:19:31,575 --> 01:19:35,455
Speaker 3:  this video through that lens. He's like, I know a lot of you are stressed

1211
01:19:35,455 --> 01:19:38,815
Speaker 3:  when you upload a reel that it won't perform. So now we're gonna let you

1212
01:19:38,815 --> 01:19:42,295
Speaker 3:  upload a reel. We won't show it to anyone who follows you. We'll show it

1213
01:19:42,295 --> 01:19:45,975
Speaker 3:  to people who don't follow you. So you can see how it'll do before

1214
01:19:46,435 --> 01:19:49,815
Speaker 3:  you publish it to everyone. And it's like, that's not how,

1215
01:19:50,235 --> 01:19:53,995
Speaker 3:  that's not how you should feel about your creative work, right?

1216
01:19:53,995 --> 01:19:56,915
Speaker 3:  Like I'm gonna focus group my video

1217
01:19:58,305 --> 01:20:01,685
Speaker 3:  before I show it to the people who follow me is a weird thing.

1218
01:20:02,605 --> 01:20:05,545
Speaker 3:  It is correct. I think if you're making like advertising, you run the pressure

1219
01:20:05,545 --> 01:20:07,925
Speaker 3:  washing business, you're like, is my pressure washing business gonna do better?

1220
01:20:08,365 --> 01:20:12,245
Speaker 3:  'cause they made, maybe that makes sense. But that's what I'm saying. Adam

1221
01:20:12,245 --> 01:20:15,765
Speaker 3:  thinks everyone is making commercial videos and they're trying to optimize

1222
01:20:15,765 --> 01:20:18,885
Speaker 3:  them for reach as opposed to making stuff on Instagram.

1223
01:20:18,995 --> 01:20:21,325
Speaker 4:  Richard, what do you make of this? This makes perfect sense to me.

1224
01:20:21,965 --> 01:20:25,605
Speaker 5:  I think it explains, like you said, it explains a lot about the way that

1225
01:20:25,625 --> 01:20:29,125
Speaker 5:  at Moser and Instagram kind of see the world, especially what we've seen

1226
01:20:29,125 --> 01:20:33,045
Speaker 5:  with threads. For example, the way that you could have a post that you, you

1227
01:20:33,045 --> 01:20:36,925
Speaker 5:  would put up and it would go viral with strangers but not be

1228
01:20:36,925 --> 01:20:40,285
Speaker 5:  seen by the people who follow you and in particular on threads go viral with

1229
01:20:40,365 --> 01:20:44,045
Speaker 5:  strangers who don't like what you said it, it was very, very,

1230
01:20:44,075 --> 01:20:47,325
Speaker 5:  very good at identifying people who do not like what you said and making

1231
01:20:47,325 --> 01:20:51,245
Speaker 5:  sure that they see your posts. But, but this is kind of

1232
01:20:51,245 --> 01:20:54,685
Speaker 5:  just the way that they see it. Okay, so you made something, you would love

1233
01:20:54,685 --> 01:20:58,565
Speaker 5:  to be seen by whoever, random

1234
01:20:58,565 --> 01:21:02,445
Speaker 5:  people, not your community because you would be de you'd be

1235
01:21:02,565 --> 01:21:04,525
Speaker 5:  deeming them in your channel If you wanted that.

1236
01:21:05,065 --> 01:21:07,525
Speaker 3:  No, but they're saying you don't wanna blow it with the people who follow

1237
01:21:07,545 --> 01:21:10,925
Speaker 3:  you so we'll like focus group it with people who don't

1238
01:21:11,425 --> 01:21:15,085
Speaker 3:  and then you can decide if that data makes you

1239
01:21:15,085 --> 01:21:18,925
Speaker 3:  confident to show the people that follow you and like maybe that that

1240
01:21:18,965 --> 01:21:22,685
Speaker 3:  I think that's interesting. I'm like, that is legitimately one of the most

1241
01:21:22,685 --> 01:21:26,205
Speaker 3:  novel features a social network has released in forever. Super

1242
01:21:26,565 --> 01:21:26,805
Speaker 3:  interesting.

1243
01:21:27,245 --> 01:21:30,085
Speaker 5:  I think the, the idea that you kind of can't trust the people who follow

1244
01:21:30,105 --> 01:21:34,045
Speaker 5:  you to judge whether or not your content is good is just a really weird way

1245
01:21:34,045 --> 01:21:34,605
Speaker 5:  to think about things.

1246
01:21:34,785 --> 01:21:37,685
Speaker 3:  And that might permanently down rank you in the algorithm. Like that's what

1247
01:21:37,685 --> 01:21:41,285
Speaker 3:  I mean like the, when I say watch this video from the perspective of how

1248
01:21:41,285 --> 01:21:44,965
Speaker 3:  does Adam think about creators thinking about Instagram, it's just purely

1249
01:21:44,965 --> 01:21:48,955
Speaker 3:  commercial. It's like what you want as a creator is to do

1250
01:21:48,955 --> 01:21:51,635
Speaker 3:  great in our algorithm. So here's some tools that will help you do great

1251
01:21:51,635 --> 01:21:55,355
Speaker 3:  in the algorithm. And it's like actually what I want as a creator is to

1252
01:21:55,465 --> 01:21:59,185
Speaker 3:  make art. Like those are different things,

1253
01:21:59,735 --> 01:22:01,385
Speaker 3:  like wildly different things.

1254
01:22:01,855 --> 01:22:05,505
Speaker 4:  This is gonna sound more cynical than I mean it, but I would, I would take

1255
01:22:05,505 --> 01:22:09,465
Speaker 4:  pretty strong bets on that. More people

1256
01:22:09,485 --> 01:22:11,825
Speaker 4:  are interested in making money than making art. Oh

1257
01:22:11,825 --> 01:22:13,905
Speaker 3:  Yeah, that, that, that's what I think those platforms just where we are,

1258
01:22:14,105 --> 01:22:16,985
Speaker 3:  I think creator platforms are commercialized in that specific way. People

1259
01:22:17,185 --> 01:22:20,465
Speaker 3:  wanna win them. And here's a feature and you know, he even says in that video,

1260
01:22:20,465 --> 01:22:22,665
Speaker 3:  we made this with creators. Like we've gotten a lot of feedback, they've

1261
01:22:22,665 --> 01:22:25,425
Speaker 3:  been working with creators, but there's just something very commercial about

1262
01:22:25,425 --> 01:22:28,465
Speaker 3:  everything that's happening there that is very far away from this was a photo

1263
01:22:28,465 --> 01:22:31,705
Speaker 3:  sharing website for like me to talk to my friends. Yeah, go watch a video.

1264
01:22:31,705 --> 01:22:32,185
Speaker 3:  There's

1265
01:22:32,185 --> 01:22:35,705
Speaker 4:  Something really specifically instagramy about this that I find really fascinating.

1266
01:22:35,705 --> 01:22:39,585
Speaker 4:  Like Richard, like you were saying. What this says to me is that you are

1267
01:22:40,085 --> 01:22:43,985
Speaker 4:  always on a knife's edge of losing everything

1268
01:22:44,125 --> 01:22:48,105
Speaker 4:  on Instagram in the way that like on TikTok, everything you make

1269
01:22:48,285 --> 01:22:51,785
Speaker 4:  is like a new pull at the slot machine, right? Like Instagram cares so much

1270
01:22:51,785 --> 01:22:55,425
Speaker 4:  more about who follows you and who you follow. Then TikTok does.

1271
01:22:55,425 --> 01:22:58,225
Speaker 4:  TikTok is just happy to shove stuff into the algorithm and see what works

1272
01:22:58,565 --> 01:23:02,225
Speaker 4:  and you'll randomly go viral and sometimes you won't. Instagram lets you

1273
01:23:02,225 --> 01:23:06,205
Speaker 4:  build something that is like much more sort of understandable by you

1274
01:23:06,205 --> 01:23:09,485
Speaker 4:  as the creator, but also If you make two videos in a row that your audience

1275
01:23:09,485 --> 01:23:12,645
Speaker 4:  doesn't like and respond, doesn't respond to, you're toast. Yeah. And that

1276
01:23:12,645 --> 01:23:16,205
Speaker 4:  this is like every creator's desperate fear is If you miss

1277
01:23:16,395 --> 01:23:20,205
Speaker 4:  once it can, it can kill the whole thing for you. And, and what this is,

1278
01:23:20,425 --> 01:23:23,885
Speaker 4:  is ary basically saying, yeah, that's true. So we're gonna give you more

1279
01:23:23,885 --> 01:23:25,525
Speaker 4:  tools to make sure you never do that.

1280
01:23:25,755 --> 01:23:29,565
Speaker 3:  It's weird. Yeah, it's weird. Speaking of it can all vanish an instant.

1281
01:23:30,145 --> 01:23:34,005
Speaker 3:  TikTok lost its court case, challenging the law

1282
01:23:34,005 --> 01:23:37,925
Speaker 3:  that would force it to either ban itself or be sold. Lauren

1283
01:23:37,955 --> 01:23:41,525
Speaker 3:  Feer wrote up the decision on that case complicated, but the court basically

1284
01:23:41,685 --> 01:23:45,405
Speaker 3:  said the Congress made a a national security decision. We're not gonna

1285
01:23:45,405 --> 01:23:48,405
Speaker 3:  override that many complicated First Amendment questions. We'll deal with

1286
01:23:48,405 --> 01:23:52,285
Speaker 3:  them probably in the quarter episode down the line. But Donald Trump, incoming

1287
01:23:52,285 --> 01:23:56,245
Speaker 3:  president of the United States asked about this by Kristen Weer on NBC news

1288
01:23:56,305 --> 01:23:59,965
Speaker 3:  and an hour and a half long interview. I'm just gonna read this quote because

1289
01:24:00,635 --> 01:24:04,485
Speaker 3:  it's a lot of words from Trump. I use TikTok very successfully in my campaign.

1290
01:24:04,565 --> 01:24:07,925
Speaker 3:  I have a man named TikTok Jack. He was very effective obviously because I

1291
01:24:07,925 --> 01:24:11,685
Speaker 3:  won the youth vote by 30%, which he did not do, but whatever. I just wanted

1292
01:24:11,685 --> 01:24:14,325
Speaker 3:  to say TikTok jack. Anyway, TikTok

1293
01:24:14,325 --> 01:24:16,165
Speaker 4:  Jack email us, I need to know everything about you.

1294
01:24:16,505 --> 01:24:20,405
Speaker 3:  And then he said, I use TikTok So I can't really, you know, I can't

1295
01:24:20,405 --> 01:24:23,805
Speaker 3:  totally hate it. It was very effective. But I will say this, If you do that

1296
01:24:23,835 --> 01:24:26,485
Speaker 3:  meaning ban it, something else will come along and take its place. And maybe

1297
01:24:26,485 --> 01:24:29,485
Speaker 3:  that's unfair. And what the judge actually said was, you can't have Chinese

1298
01:24:29,885 --> 01:24:32,685
Speaker 3:  companies, they have the right to ban it If you can prove that Chinese companies

1299
01:24:32,685 --> 01:24:35,805
Speaker 3:  own it. That's what the judge actually said. She has to follow up. She said,

1300
01:24:35,805 --> 01:24:39,405
Speaker 3:  well you protect TikTok. And he said, I'm gonna try and make it so that other

1301
01:24:39,645 --> 01:24:42,445
Speaker 3:  companies don't become an even bigger monopoly. It's specifically what he

1302
01:24:42,445 --> 01:24:45,005
Speaker 3:  means is Facebook. He has often complained that Facebook would just become

1303
01:24:45,005 --> 01:24:48,885
Speaker 3:  a bigger monopoly if TikTok is banned. And then during the campaign,

1304
01:24:48,885 --> 01:24:52,845
Speaker 3:  he obviously said to a bunch of young people, I'll protect TikTok, Biden

1305
01:24:52,845 --> 01:24:56,645
Speaker 3:  wants to ban TikTok, I'll protect it. This is in my mind a massive walk back.

1306
01:24:57,215 --> 01:25:00,965
Speaker 3:  Right? All he is saying is, I don't wanna face a monopoly, not I will protect

1307
01:25:01,075 --> 01:25:04,875
Speaker 3:  bite dance owning TikTok in America. And I am,

1308
01:25:05,215 --> 01:25:08,635
Speaker 3:  I'm just betting, I think we, we've done this a million times to David, like

1309
01:25:08,805 --> 01:25:12,395
Speaker 3:  who's gonna buy TikTok because that's the out, the out is

1310
01:25:12,815 --> 01:25:16,115
Speaker 3:  he negotiates a deal for Amazon or Walmart or

1311
01:25:16,805 --> 01:25:20,145
Speaker 3:  Subway to buy TikTok. There you go.

1312
01:25:20,645 --> 01:25:24,425
Speaker 3:  And he gets to say he saved it. I made a great deal to save TikTok.

1313
01:25:24,425 --> 01:25:27,545
Speaker 3:  Aren't you proud of me? I'm doing what I said I would do. And I am, I just,

1314
01:25:27,545 --> 01:25:30,425
Speaker 3:  you can watch the clip. We have it on the website. It feels very much like

1315
01:25:30,425 --> 01:25:31,505
Speaker 3:  that is the door he's opening

1316
01:25:32,795 --> 01:25:36,235
Speaker 4:  I think. I mean it's it's it's the escape hatch, right? It's, it's the closest

1317
01:25:36,245 --> 01:25:40,115
Speaker 4:  thing he's gonna find to a win-win at the end of this he gets a, he gets

1318
01:25:40,115 --> 01:25:43,235
Speaker 4:  an America first win. He gets TikTok to still be here.

1319
01:25:44,825 --> 01:25:48,445
Speaker 4:  He gets to like poke a knife at Mark Zuckerberg

1320
01:25:49,055 --> 01:25:52,925
Speaker 3:  Times running out though B goes into effect on January 19th, TikTok has

1321
01:25:52,925 --> 01:25:55,965
Speaker 3:  filed with the Supreme Court to appeal. We'll see what happens.

1322
01:25:56,435 --> 01:25:59,725
Speaker 3:  Notably January 19th is the day before January 20th,

1323
01:26:00,535 --> 01:26:02,045
Speaker 3:  which is when he takes office. So

1324
01:26:02,595 --> 01:26:05,405
Speaker 4:  Wait, Neela and I have have been on the record about what we think is gonna

1325
01:26:05,405 --> 01:26:07,685
Speaker 4:  happen many times. Richard, you have to make a prediction right now. What

1326
01:26:07,685 --> 01:26:08,765
Speaker 4:  happens on January 19th?

1327
01:26:10,345 --> 01:26:10,565
Speaker 5:  Oh,

1328
01:26:12,995 --> 01:26:15,455
Speaker 5:  TikTok actually turns into a new app called

1329
01:26:15,525 --> 01:26:16,055
Speaker 3:  Talk Tick

1330
01:26:17,565 --> 01:26:20,935
Speaker 5:  Something. It won't be, it won't be owned in China. Americans be owned in

1331
01:26:20,935 --> 01:26:22,375
Speaker 5:  like Malaysia or something like.

1332
01:26:24,075 --> 01:26:27,415
Speaker 5:  But you know by a very mysterious company that just happened to come into

1333
01:26:27,415 --> 01:26:29,375
Speaker 5:  a lot of money, who knows where it came

1334
01:26:29,375 --> 01:26:33,175
Speaker 3:  From. Next one. Chris Welch reviewed the Sonos Arc Ultra.

1335
01:26:33,175 --> 01:26:36,895
Speaker 3:  He says the hardware is great, particularly the new transducers from

1336
01:26:36,895 --> 01:26:40,335
Speaker 3:  mate that's a company in Sonos bought to make smaller transducers and make

1337
01:26:40,335 --> 01:26:43,215
Speaker 3:  more base loves those app is still

1338
01:26:44,315 --> 01:26:47,095
Speaker 3:  little, little something. Richard, what do you think?

1339
01:26:49,535 --> 01:26:52,455
Speaker 5:  I don't know. I mean it's, it's Sonos and they still have the problem of

1340
01:26:52,675 --> 01:26:56,655
Speaker 5:  the app that they messed up and made everyone mad. So how's

1341
01:26:56,655 --> 01:26:57,015
Speaker 5:  that going?

1342
01:26:58,795 --> 01:27:00,895
Speaker 3:  Medium? I, I would say the answer is medium

1343
01:27:02,715 --> 01:27:06,135
Speaker 3:  but I will say that as a soundbar seems very cool

1344
01:27:06,645 --> 01:27:10,465
Speaker 3:  If I, Chris was basically like this is worth upgrading. Which upgrading

1345
01:27:10,535 --> 01:27:14,425
Speaker 3:  from one expensive soundbar to another expensive soundbar is almost

1346
01:27:14,605 --> 01:27:18,585
Speaker 3:  not like no one has that thought. Like I'm a person who

1347
01:27:18,585 --> 01:27:21,665
Speaker 3:  likes to buy speakers and I'm never like I should upgrade soundbar like

1348
01:27:22,365 --> 01:27:24,985
Speaker 3:  and Chris thought it was worth upgrading, which is fascinating. So hopefully,

1349
01:27:25,255 --> 01:27:28,105
Speaker 3:  hopefully this is the beginning of the turnaround. This was big news this

1350
01:27:28,105 --> 01:27:31,945
Speaker 3:  week. GM shut down the cruise Robo taxii service laid

1351
01:27:31,945 --> 01:27:35,745
Speaker 3:  off a bunch of people. The former CEO of cruise just flat out

1352
01:27:35,745 --> 01:27:37,225
Speaker 3:  posted GM is a stupid company

1353
01:27:39,745 --> 01:27:41,415
Speaker 3:  which is great but it feels like

1354
01:27:41,415 --> 01:27:44,575
Speaker 4:  Think already used was dummies, which I like very much. Dummies is a, is

1355
01:27:44,615 --> 01:27:48,495
Speaker 4:  a surprisingly powerful. That's powerful mean thing to say to somebody. I

1356
01:27:48,495 --> 01:27:51,815
Speaker 3:  Know what David, there's just like a lot of action in this world and obviously

1357
01:27:51,875 --> 01:27:53,975
Speaker 3:  GM is like up and down. What do you, what do you think is going on here?

1358
01:27:54,735 --> 01:27:58,095
Speaker 4:  I think it, it just is starting to seem like

1359
01:27:58,835 --> 01:28:02,695
Speaker 4:  all of these car companies got out over their skis

1360
01:28:03,075 --> 01:28:06,455
Speaker 4:  on technological revolutions, right? There was the sense that

1361
01:28:07,555 --> 01:28:11,455
Speaker 4:  EVs and self-driving were gonna happen really fast

1362
01:28:11,675 --> 01:28:14,135
Speaker 4:  and that they were going to be immediately mainstream and they were gonna

1363
01:28:14,135 --> 01:28:18,095
Speaker 4:  be become a big business. And everybody got really excited about the

1364
01:28:18,095 --> 01:28:21,015
Speaker 4:  idea of robo taxis because then you have a really interesting like

1365
01:28:21,405 --> 01:28:25,295
Speaker 4:  diversified business from your own vehicles.

1366
01:28:25,325 --> 01:28:28,415
Speaker 4:  Like you can see how they get there,

1367
01:28:29,785 --> 01:28:33,645
Speaker 4:  but we're not there. And I and like unless you're Tesla and

1368
01:28:33,645 --> 01:28:37,525
Speaker 4:  people just keep giving you stock price money

1369
01:28:37,525 --> 01:28:40,885
Speaker 4:  with which to have weird ideas about where to go from here,

1370
01:28:42,175 --> 01:28:45,835
Speaker 4:  you're still fundamentally running a car business which is not getting higher

1371
01:28:45,835 --> 01:28:49,675
Speaker 4:  margin, it's not getting less complex. You

1372
01:28:49,675 --> 01:28:52,155
Speaker 4:  still have to make the things that people are buying now and it just feels

1373
01:28:52,155 --> 01:28:55,955
Speaker 4:  like one by one these car companies are starting to say, okay we,

1374
01:28:55,975 --> 01:28:59,595
Speaker 4:  we made these big giant future bets that just aren't coming true as quickly

1375
01:28:59,655 --> 01:29:03,125
Speaker 4:  as we thought and we just have to get out of it. I also think

1376
01:29:05,205 --> 01:29:08,205
Speaker 4:  a lot of people who cover this space more closely than I do think there is

1377
01:29:08,205 --> 01:29:11,765
Speaker 4:  something weird going on with this one in particular that the way

1378
01:29:12,025 --> 01:29:15,765
Speaker 4:  GM is handling this suggests that something else is going on inside of cruise.

1379
01:29:16,235 --> 01:29:19,925
Speaker 4:  That it basically just like pulled the plug all at once.

1380
01:29:20,125 --> 01:29:23,525
Speaker 4:  Which is a very odd thing to do when you're this kind of company that is

1381
01:29:23,525 --> 01:29:27,325
Speaker 4:  this invested in cruise. But the macro thing

1382
01:29:27,605 --> 01:29:30,125
Speaker 4:  actually kinda makes sense to me. It's just everybody made a bet

1383
01:29:31,655 --> 01:29:34,835
Speaker 4:  10 years ago now that they thought was gonna be true in five years and now

1384
01:29:34,835 --> 01:29:36,835
Speaker 4:  looks like it might be more like 25. I

1385
01:29:36,835 --> 01:29:40,195
Speaker 3:  Think it's just hard to lose money the way Google has chosen to lose money

1386
01:29:40,195 --> 01:29:43,395
Speaker 3:  on Waymo for this whole time. Yeah, right. I mean that's just Google's like,

1387
01:29:43,395 --> 01:29:46,235
Speaker 3:  we're just gonna lose money until this thing can make it through Phoenix.

1388
01:29:47,225 --> 01:29:50,595
Speaker 3:  Alright, we're gonna keep losing money until we can make it through Austin.

1389
01:29:50,975 --> 01:29:51,755
Speaker 3:  And it's just like,

1390
01:29:51,945 --> 01:29:54,475
Speaker 4:  Yeah, GM just doesn't, literally doesn't have that money. Yeah,

1391
01:29:54,625 --> 01:29:57,675
Speaker 3:  It's just not a thing. Richard, you're the one who put quantum computing

1392
01:29:57,675 --> 01:30:00,915
Speaker 3:  on this list. Google reveals quantum computing chip with breakthrough achievements.

1393
01:30:01,345 --> 01:30:04,875
Speaker 3:  Tell me, tell me why this one made Richard happy. 'cause most of the other

1394
01:30:04,875 --> 01:30:05,475
Speaker 3:  stuff did not.

1395
01:30:06,395 --> 01:30:10,315
Speaker 5:  I fully understand quantum computing. I understand everything about

1396
01:30:10,315 --> 01:30:13,635
Speaker 5:  it. Actually the quantum computing chip Richard is

1397
01:30:13,635 --> 01:30:15,315
Speaker 4:  Currently both here is called not here.

1398
01:30:16,755 --> 01:30:20,675
Speaker 5:  I, as I've said many times, my coin RJCC coin is quantum locked. It

1399
01:30:20,675 --> 01:30:24,555
Speaker 5:  is both launched and mint and premin at all times. That's how it works. That's

1400
01:30:24,555 --> 01:30:28,395
Speaker 5:  how quantum works. That's what we do here at RJCC coin. And we're gonna do

1401
01:30:28,395 --> 01:30:31,755
Speaker 5:  it on Willow because it can perform a task in five minutes that will take

1402
01:30:31,835 --> 01:30:35,755
Speaker 5:  a supercomputer 10 septillion years to complete, which may be

1403
01:30:35,915 --> 01:30:39,595
Speaker 5:  evidence that we live in a simulation or a multiverse

1404
01:30:39,925 --> 01:30:43,515
Speaker 5:  maybe, or maybe not. I don't really know what any of these words mean, but

1405
01:30:43,515 --> 01:30:46,275
Speaker 5:  it happened. They have a chip, they're doing a thing and now they're trying

1406
01:30:46,275 --> 01:30:49,075
Speaker 5:  to find a, basically, now that they've done this thing, they're trying to

1407
01:30:49,075 --> 01:30:51,835
Speaker 5:  find something to do with it that you will actually find the use for. So

1408
01:30:51,835 --> 01:30:54,475
Speaker 5:  they could prove how fast. It's because right now all the stuff they can

1409
01:30:54,475 --> 01:30:55,475
Speaker 5:  do is theoretical.

1410
01:30:56,055 --> 01:30:57,515
Speaker 3:  The answer is breaking crypto

1411
01:30:58,715 --> 01:30:59,555
Speaker 5:  RJCC coin.

1412
01:30:59,695 --> 01:31:00,675
Speaker 3:  That's what's gonna happen.

1413
01:31:01,135 --> 01:31:04,235
Speaker 5:  You're already in, its quantum you, you are already in it.

1414
01:31:05,375 --> 01:31:05,955
Speaker 3:  You already have

1415
01:31:05,955 --> 01:31:09,915
Speaker 4:  It. I have three very brief things to say about this whole thing. One, everybody

1416
01:31:09,915 --> 01:31:13,675
Speaker 4:  should read the research paper that Google did because even

1417
01:31:13,715 --> 01:31:17,195
Speaker 4:  Google's own researchers are basically like it, we, this thing we did is

1418
01:31:17,195 --> 01:31:19,355
Speaker 4:  cool, but I don't know what it is that we just did here.

1419
01:31:19,675 --> 01:31:20,915
Speaker 3:  They're like, that's all quantum computing.

1420
01:31:20,915 --> 01:31:24,355
Speaker 4:  Yeah. They're like, we did an amazing unprecedented

1421
01:31:24,675 --> 01:31:27,915
Speaker 4:  mathematical calculation that no one cares about and has no bearing on the

1422
01:31:27,915 --> 01:31:31,835
Speaker 4:  real world. Like they, they say that like, it's amazing that we

1423
01:31:31,835 --> 01:31:35,315
Speaker 4:  did it. We don't know why we did it or what it actually accomplishes. And

1424
01:31:35,315 --> 01:31:39,195
Speaker 4:  then they're like, the main thing that we've solved

1425
01:31:39,195 --> 01:31:42,835
Speaker 4:  here in our real breakthrough is that we've made it make fewer errors.

1426
01:31:42,945 --> 01:31:46,355
Speaker 4:  Yeah, like, like imagine if your computer was just like,

1427
01:31:46,865 --> 01:31:50,635
Speaker 4:  sometimes when you try to move a file, it just

1428
01:31:50,755 --> 01:31:54,715
Speaker 4:  explodes. But now our computer does that less. It still does it,

1429
01:31:55,065 --> 01:31:58,715
Speaker 3:  It's very good. It's less. I'm just saying the the main purpose of quantum

1430
01:31:58,715 --> 01:32:02,585
Speaker 3:  computing is gonna be to break cryptography in specific ways. Get

1431
01:32:02,585 --> 01:32:03,745
Speaker 3:  ready. Yeah, get ready.

1432
01:32:04,005 --> 01:32:06,785
Speaker 4:  But then the third thing is that they just very casually are like, this might

1433
01:32:06,785 --> 01:32:09,385
Speaker 4:  be evidence that we're living in a simulation and that's just like the end

1434
01:32:09,385 --> 01:32:12,105
Speaker 4:  of a, they just, they don't address that anymore. They just say it.

1435
01:32:12,445 --> 01:32:15,145
Speaker 3:  All right, here's the last one, which is pure evidence. We live in a simulation.

1436
01:32:15,405 --> 01:32:17,705
Speaker 3:  You can now buy Hyundai on Amazon.

1437
01:32:19,255 --> 01:32:22,985
Speaker 3:  It's very weird. You could, you like, it's very weird to go on

1438
01:32:22,985 --> 01:32:25,945
Speaker 3:  Amazon and see something listed for a price of $67,000,

1439
01:32:27,125 --> 01:32:30,785
Speaker 3:  but there's no one click buy. It's start the, the button is like start now

1440
01:32:30,965 --> 01:32:34,665
Speaker 3:  and then you get kicked to a dealer. And I feel like we're, we have to buy

1441
01:32:34,665 --> 01:32:37,305
Speaker 3:  a car to test out if the dealer actually plays ball.

1442
01:32:37,825 --> 01:32:41,475
Speaker 4:  Wait, that's less exciting. So I was really hoping there was gonna be like

1443
01:32:41,535 --> 01:32:44,675
Speaker 4:  an Amazon warehouse somewhere that's just full of cars.

1444
01:32:45,015 --> 01:32:48,355
Speaker 3:  That's Carvana, right? There's the, the vending machine.

1445
01:32:48,355 --> 01:32:49,355
Speaker 4:  Yeah, the car vending machine.

1446
01:32:49,745 --> 01:32:52,155
Speaker 3:  This is, they're, they're, I think they're just doing some pricing games

1447
01:32:52,155 --> 01:32:55,555
Speaker 3:  with Hyundai, right? Where Hyundai sets a price, Amazon sets a price and

1448
01:32:55,555 --> 01:32:57,835
Speaker 3:  the dealer just delivers a car and everyone gets a cut and is happy. But

1449
01:32:58,125 --> 01:33:02,115
Speaker 3:  there are still car dealers involved and I assure you at some

1450
01:33:02,115 --> 01:33:04,875
Speaker 3:  point they're gonna be like, and we've, we've marked this one up. We filled

1451
01:33:04,875 --> 01:33:08,635
Speaker 3:  the tires with nitrogen. So now that's an extra $6,000. Like that's car dealer

1452
01:33:08,635 --> 01:33:12,555
Speaker 3:  stuff, right? If anyone wants to buy a car on Amazon and tell us

1453
01:33:12,555 --> 01:33:15,875
Speaker 3:  how it goes. I'm dying to know If you just want to cut me a check for

1454
01:33:15,875 --> 01:33:18,675
Speaker 3:  $65,000 so we can buy on Amazon,

1455
01:33:19,605 --> 01:33:22,235
Speaker 3:  we'll accept that money as well. But I'm kind of dying to do this 'cause

1456
01:33:22,235 --> 01:33:24,915
Speaker 3:  I wanna see if the dealers play ball just,

1457
01:33:25,025 --> 01:33:28,755
Speaker 4:  This is so bizarre. It's weird man. Amazon will give you a $2,300 gift card

1458
01:33:28,775 --> 01:33:31,195
Speaker 4:  If you buy a car through Amazon.

1459
01:33:32,855 --> 01:33:33,825
Speaker 3:  It's the future man.

1460
01:33:33,855 --> 01:33:35,705
Speaker 4:  It's like kind of like a tax break, but it isn't,

1461
01:33:35,845 --> 01:33:39,625
Speaker 3:  The weird thing is the car only works

1462
01:33:39,685 --> 01:33:41,505
Speaker 3:  on Kindles in that,

1463
01:33:42,485 --> 01:33:45,385
Speaker 5:  But I want, I want them to link up to bring a trailer So I can just go on

1464
01:33:45,385 --> 01:33:48,705
Speaker 5:  Amazon like somebody's more sell XR four ti I is up there. I'm just like,

1465
01:33:48,705 --> 01:33:51,705
Speaker 5:  yeah, buy it now. And then it shows up at the house the next day and I have

1466
01:33:51,705 --> 01:33:53,705
Speaker 5:  to explain that that's what we need.

1467
01:33:53,775 --> 01:33:54,465
Speaker 3:  They just, they

1468
01:33:54,465 --> 01:33:57,385
Speaker 4:  Bring you the whole thing and then the delivery driver just hands you the

1469
01:33:57,385 --> 01:33:58,265
Speaker 4:  keys and walks away.

1470
01:33:58,565 --> 01:34:02,465
Speaker 3:  My cousin in New Jersey has absolutely impulse bought a car and bring a trailer

1471
01:34:02,845 --> 01:34:06,665
Speaker 3:  and had like a 1980s Mercedes roll up in his driveway

1472
01:34:06,965 --> 01:34:09,505
Speaker 3:  and his his wife is like, what's going on here?

1473
01:34:11,165 --> 01:34:13,665
Speaker 3:  But you can absolutely, I've been so pissed. Impulse by a car and bring a

1474
01:34:13,665 --> 01:34:15,545
Speaker 3:  trailer. That's a real thing that happens. All right, we gotta get outta

1475
01:34:15,545 --> 01:34:19,065
Speaker 3:  here. This is a great one, Richard. Thanks for being here. And I wanna call

1476
01:34:19,065 --> 01:34:22,945
Speaker 3:  out one story, which is maybe my favorite story of the month

1477
01:34:23,045 --> 01:34:26,985
Speaker 3:  so far. Christian Radkey, our creative director and Amelia Hall

1478
01:34:27,255 --> 01:34:30,625
Speaker 3:  kras, our photographer went to Pantone's launch of the Color of the Year,

1479
01:34:31,155 --> 01:34:34,745
Speaker 3:  which is called Mocha Moose, which is brown. So they went to a party for

1480
01:34:34,745 --> 01:34:37,665
Speaker 3:  the color brown. Christian was horrified 'cause she was accidentally wore

1481
01:34:37,665 --> 01:34:41,105
Speaker 3:  brown on that day, but then she got to the party, everyone else was wearing

1482
01:34:41,105 --> 01:34:43,705
Speaker 3:  brown. It's like a perfect VERGE story. Everything's great about it. Go read

1483
01:34:43,705 --> 01:34:47,505
Speaker 3:  that story. The photos are amazing. That's it. That's for cha. Rock and Roll.

1484
01:34:51,925 --> 01:34:54,985
Speaker 1:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1485
01:34:55,135 --> 01:34:59,045
Speaker 1:  Give us a call at eight six six VERGE one one. The Vergecast is a

1486
01:34:59,045 --> 01:35:02,685
Speaker 1:  production of The Verge and Vox Media podcast network. Our show is produced

1487
01:35:02,685 --> 01:35:06,605
Speaker 1:  by Liam James, will Poor and Eric Gomez. And that's it. We'll

1488
01:35:06,605 --> 01:35:07,165
Speaker 1:  see you next week.

