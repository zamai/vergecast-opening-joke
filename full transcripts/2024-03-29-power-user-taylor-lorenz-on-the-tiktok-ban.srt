1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 1883e16b-4334-4bac-bbdb-cb8ebba97fe9
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-298828691265574928/-6558360280728599402/s93290-US-3465s-1711706659.mp3
Description: Today we're sharing an episode of a new podcast called Power User, which explores how technology and the internet are upending our lives and the world around us, hosted by tech journalist Taylor Lorenz.
Follow Power User with Taylor Lorenz wherever you get your podcasts:Â https://link.chtbl.com/poweruserpod
Learn more about your ad choices. Visit podcastchoices.com/adchoices
Ad Filtering: Enabled (18 ads detected)

2
00:01:08,585 --> 00:01:12,155
Speaker 4:  Welcome To The Vergecast, the flagship podcast of the TikTok Ban.

3
00:01:12,415 --> 00:01:16,155
Speaker 4:  I'm your friend David Pierce and I am the only one here with

4
00:01:16,515 --> 00:01:20,395
Speaker 4:  vacations and sickness and stuff. We are not doing a normal Friday

5
00:01:20,395 --> 00:01:23,435
Speaker 4:  episode this week. Everybody's back next week we will be back to our regularly

6
00:01:23,435 --> 00:01:26,715
Speaker 4:  scheduled programming, but for Today we have something much better than NELI

7
00:01:26,715 --> 00:01:28,235
Speaker 4:  and Alex. Taylor Lorenza here. Hi Taylor.

8
00:01:28,735 --> 00:01:28,955
Speaker 5:  Hi.

9
00:01:29,415 --> 00:01:33,195
Speaker 4:  You're a podcaster now. Last time we did this you had like ear pods on. Now

10
00:01:33,195 --> 00:01:35,595
Speaker 4:  you have a light, you have a mic. Like this is a whole thing.

11
00:01:36,635 --> 00:01:40,395
Speaker 5:  I know. I have the Vox Media setup. Shout out to

12
00:01:41,175 --> 00:01:44,315
Speaker 5:  the producers that sent me all this gear. I have the official mic.

13
00:01:44,455 --> 00:01:47,315
Speaker 4:  Is there just like a box that they sent you that's like Welcome To podcast?

14
00:01:47,745 --> 00:01:48,035
Speaker 4:  Yeah,

15
00:01:48,055 --> 00:01:51,715
Speaker 5:  It was a box. They sent someone over to put up a lights in my kitchen, which

16
00:01:51,715 --> 00:01:52,235
Speaker 5:  is where I am.

17
00:01:52,575 --> 00:01:56,355
Speaker 4:  So you just launched a podcast. It's called Power User. Tell the people about

18
00:01:56,355 --> 00:01:56,675
Speaker 4:  your podcast.

19
00:01:57,265 --> 00:02:01,195
Speaker 5:  It's a weekly tech news show, but it focuses more on

20
00:02:01,195 --> 00:02:04,835
Speaker 5:  the things that I cover, which is, is the user side of tech. A lot of

21
00:02:04,995 --> 00:02:08,675
Speaker 5:  internet culture, media, landscape stuff, you know, some

22
00:02:08,675 --> 00:02:12,435
Speaker 5:  creator industry stuff. Mr. Beast. Also touching on things like

23
00:02:12,435 --> 00:02:15,995
Speaker 5:  Netflix, you know, Disney's some entertainment media

24
00:02:16,635 --> 00:02:20,515
Speaker 5:  business and just wild, weird internet stories and trends. This week we're

25
00:02:20,515 --> 00:02:24,475
Speaker 5:  talking about kindness influencers, which are a plague on TikTok. I don't

26
00:02:24,475 --> 00:02:28,355
Speaker 5:  know, If. you see those? The guys that exploit unhoused people for

27
00:02:28,355 --> 00:02:31,875
Speaker 5:  content basically. Yeah. And there's this feud between two big kindness

28
00:02:31,875 --> 00:02:35,755
Speaker 5:  influencers over who's essentially helping this one family in West Virginia

29
00:02:35,825 --> 00:02:39,515
Speaker 5:  more. you know, we're talking about the Andrew Huberman expose as well.

30
00:02:39,815 --> 00:02:43,355
Speaker 5:  And so yeah, it's, I think that's a good, listen, we just launched it last

31
00:02:43,355 --> 00:02:43,515
Speaker 5:  week.

32
00:02:43,825 --> 00:02:46,915
Speaker 4:  Yeah, you picked an interesting time to do this. There is like not a shortage

33
00:02:46,915 --> 00:02:49,155
Speaker 4:  of weird stuff happening on the internet at this moment.

34
00:02:50,255 --> 00:02:53,435
Speaker 5:  No, and I want people to better understand it. I think we talk so much about

35
00:02:53,455 --> 00:02:57,195
Speaker 5:  big tech and sort of the traditional Silicon Valley side of tech and there's

36
00:02:57,195 --> 00:03:00,485
Speaker 5:  not as many, I think tech podcasts focused on the tech world outside Silicon

37
00:03:00,485 --> 00:03:04,045
Speaker 5:  Valley and just more of the cultural side of things. So

38
00:03:04,185 --> 00:03:05,445
Speaker 5:  that's what I'm hoping to get into.

39
00:03:05,635 --> 00:03:08,605
Speaker 4:  Yeah, I, this is actually one of the things I was thinking about as, as you

40
00:03:08,635 --> 00:03:10,645
Speaker 4:  were, you know, we talked a little bit as you were getting ready to launch

41
00:03:10,645 --> 00:03:13,565
Speaker 4:  the podcast and it seems like we're in this weird space where there are a

42
00:03:13,645 --> 00:03:17,565
Speaker 4:  ton of creators who have podcasts, right? Like it's, I feel like it's the

43
00:03:17,725 --> 00:03:20,165
Speaker 4:  same way in like the sports world, right? Where there used to be lots of

44
00:03:20,165 --> 00:03:23,565
Speaker 4:  people who talked about athletes and now the athletes are just doing the

45
00:03:23,565 --> 00:03:27,405
Speaker 4:  talking. But I feel like they're in this sort of space of like

46
00:03:27,465 --> 00:03:31,005
Speaker 4:  people talking about this business. It's either the people who are in it

47
00:03:31,005 --> 00:03:34,085
Speaker 4:  every day living it or nobody really talks about it. It's very weird. And

48
00:03:34,085 --> 00:03:36,645
Speaker 4:  I feel like this has been like the needle, you've been threading for a really

49
00:03:36,645 --> 00:03:40,565
Speaker 4:  long time where you're like, no, this matters. But also if it's just

50
00:03:40,645 --> 00:03:43,365
Speaker 4:  a creator talking about their own life, that's not the quite, not quite the

51
00:03:43,365 --> 00:03:46,085
Speaker 4:  same thing as like doing journalism about it. Exactly.

52
00:03:46,315 --> 00:03:49,765
Speaker 5:  Yeah. And I think also, I mean, I have a big content creator on

53
00:03:50,315 --> 00:03:53,885
Speaker 5:  this week. Yeah. So, you know, I've, I've love creators and I wanna talk

54
00:03:53,885 --> 00:03:57,325
Speaker 5:  to them, but I, I think it's worth giving a little bit more critical thought.

55
00:03:57,445 --> 00:04:00,405
Speaker 5:  I think a lot of times when you're in an industry one, you can't speak as

56
00:04:00,405 --> 00:04:03,325
Speaker 5:  freely about it sometimes. 'cause you're actually in this industry, you know,

57
00:04:03,325 --> 00:04:06,485
Speaker 5:  a lot of their podcasts focus more on sort of like interpersonal drama, like

58
00:04:06,485 --> 00:04:10,325
Speaker 5:  drama sort of stuff. Or they do culture. I mean, there's a lot of internet

59
00:04:10,325 --> 00:04:14,005
Speaker 5:  culture YouTuber podcasts that I listen to all the time. I think I'm coming

60
00:04:14,025 --> 00:04:18,005
Speaker 5:  at it more from like a business and cultural and journalistic angle. But

61
00:04:18,005 --> 00:04:21,205
Speaker 5:  I love those YouTubers and I hope to have them on. And I'm a big commentary

62
00:04:21,205 --> 00:04:22,725
Speaker 5:  channel fan, so,

63
00:04:23,305 --> 00:04:26,165
Speaker 4:  Oh yeah, it, there's lots of good stuff out there, don't get me wrong. There's

64
00:04:26,165 --> 00:04:28,725
Speaker 4:  lots of different ways to come at this. So the episode we're gonna play here

65
00:04:28,825 --> 00:04:32,445
Speaker 4:  is about the TikTok band. We picked this one because we talked a bunch about

66
00:04:32,445 --> 00:04:36,365
Speaker 4:  the TikTok ban on The Vergecast two weeks ago. And everyone got

67
00:04:36,365 --> 00:04:40,165
Speaker 4:  very mad at us. We, we disagreed vehemently about what should happen

68
00:04:40,165 --> 00:04:43,325
Speaker 4:  here. Everybody got very upset. We got a lot of emails. I'm being accused

69
00:04:43,345 --> 00:04:47,045
Speaker 4:  of being a a Chinese communist basically. And a lot of people think Mila

70
00:04:47,045 --> 00:04:50,645
Speaker 4:  hates free speech and that think Alex is also

71
00:04:50,825 --> 00:04:54,245
Speaker 4:  mad at everybody. It's really good times. I'm just curious how you feel about

72
00:04:54,245 --> 00:04:56,685
Speaker 4:  where we are with the TikTok ban right now. We're gonna hear a bunch about

73
00:04:56,685 --> 00:05:00,125
Speaker 4:  this in the episode, but where is your head with what you're thinking about

74
00:05:00,125 --> 00:05:01,685
Speaker 4:  the TikTok ban kind of right in this moment?

75
00:05:01,985 --> 00:05:05,565
Speaker 5:  Oh gosh. I said very earnestly to a friend the other day. I was like, we're

76
00:05:05,565 --> 00:05:09,445
Speaker 5:  living in an era of mass censorship. And she was like, okay, you're starting

77
00:05:09,465 --> 00:05:10,765
Speaker 5:  to sound like Elon Musk,

78
00:05:12,785 --> 00:05:16,325
Speaker 5:  But as a journalist, and I would imagine you guys feel the same way, but

79
00:05:16,385 --> 00:05:19,125
Speaker 5:  it, you know, I care a lot about civil liberties. I've always cared a lot

80
00:05:19,125 --> 00:05:21,965
Speaker 5:  about civil liberties. I've spent a enormous time reporting on TikTok, and

81
00:05:21,965 --> 00:05:25,845
Speaker 5:  not just TikTok, but musically. And I remember when there

82
00:05:25,845 --> 00:05:29,645
Speaker 5:  was this fight when Facebook was driving musically into the ground

83
00:05:29,825 --> 00:05:33,805
Speaker 5:  and they had to sell. And Viacom, you know, there was a minute that

84
00:05:33,805 --> 00:05:37,325
Speaker 5:  we thought Viacom might buy, ance came in and sort of bought it and, and

85
00:05:37,325 --> 00:05:40,965
Speaker 5:  just the la you know, of course no one cared back then that the fast

86
00:05:41,105 --> 00:05:43,725
Speaker 5:  one of the, I would've called it one of the most relevant, fast growing video

87
00:05:43,725 --> 00:05:47,565
Speaker 5:  social networks on the planet was being bought by Ance. And then now there's

88
00:05:47,565 --> 00:05:50,885
Speaker 5:  this panic five years later. It's a little hard for me to take it quite as

89
00:05:50,885 --> 00:05:54,685
Speaker 5:  seriously. Not that I'm like, where was this energy when you allowed

90
00:05:54,685 --> 00:05:58,405
Speaker 5:  this deal to happen five years ago? But also I write a lot

91
00:05:58,405 --> 00:06:01,245
Speaker 5:  about a lot of other up and coming apps and there's just Chinese influence

92
00:06:01,245 --> 00:06:05,205
Speaker 5:  in so many things, gaming particularly. And you know, so I find a lot

93
00:06:05,205 --> 00:06:08,485
Speaker 5:  of these arguments that these lawmakers are making, it doesn't seem like

94
00:06:08,545 --> 00:06:12,125
Speaker 5:  the ban as it currently stands, the current bill would address any of these

95
00:06:12,125 --> 00:06:15,205
Speaker 5:  issues, right. Doesn't address, address any of the problems with data brokers.

96
00:06:15,205 --> 00:06:18,605
Speaker 5:  Doesn't address a lot of the civil liberties free speech stuff. And as I've

97
00:06:18,605 --> 00:06:20,965
Speaker 5:  said before, and I was also accused of being a communist and I understand

98
00:06:21,145 --> 00:06:25,045
Speaker 5:  making this comment, but even if TikTok was a Chinese propaganda app,

99
00:06:25,295 --> 00:06:28,205
Speaker 5:  we're supposed to have a free and open internet. What if I am a communist

100
00:06:28,205 --> 00:06:32,005
Speaker 5:  and I wanna consume Chinese propaganda all day, it's this is America. I should

101
00:06:32,005 --> 00:06:33,285
Speaker 5:  be allowed to do that. Yeah.

102
00:06:33,285 --> 00:06:35,965
Speaker 4:  We got a lot of emails from people after the episode a couple of weeks ago

103
00:06:35,995 --> 00:06:38,165
Speaker 4:  that were basically like, well, China bans it.

104
00:06:38,425 --> 00:06:41,765
Speaker 5:  Oh, so we should mimic an authoritarian state. Oh, well this

105
00:06:41,765 --> 00:06:44,325
Speaker 5:  authoritarian should more like China State over there. China, yeah. What?

106
00:06:45,185 --> 00:06:48,765
Speaker 5:  No, I actually don't want the government to control our app store. It's so

107
00:06:48,765 --> 00:06:52,165
Speaker 5:  funny the way this is split around political lines, and I think, I mean,

108
00:06:52,365 --> 00:06:55,405
Speaker 5:  I'm planning a future episode about this, so I really hope people tune in.

109
00:06:55,405 --> 00:06:59,165
Speaker 5:  I don't know if anyone's been following the Jonathan Haight Anti phone

110
00:06:59,295 --> 00:07:03,245
Speaker 5:  press tour recently. A little bit, yeah. But there's just this, you can push

111
00:07:03,445 --> 00:07:07,365
Speaker 5:  a lot of bad legislation and dangerous things through and sell people

112
00:07:07,365 --> 00:07:10,965
Speaker 5:  on things when you tell them it's protecting children and you know, here's

113
00:07:10,965 --> 00:07:13,925
Speaker 5:  what your children are up to and they're being brainwashed, you know, by

114
00:07:13,925 --> 00:07:17,765
Speaker 5:  this Chinese communist app or whatever. And it's just

115
00:07:17,765 --> 00:07:21,125
Speaker 5:  not true. I, I mean, I'm very skeptical of all of the, the Save the Children

116
00:07:21,275 --> 00:07:25,045
Speaker 5:  type messaging around phones generally, and especially around TikTok because

117
00:07:25,045 --> 00:07:28,925
Speaker 5:  TikTok is quite, you know, liberating for a lot of kids. It's not like Instagram

118
00:07:28,925 --> 00:07:31,925
Speaker 5:  and a lot of other apps which have actually done the things that they're

119
00:07:32,045 --> 00:07:35,645
Speaker 5:  accusing TikTok of doing, like manipulate, you know, affecting our elections,

120
00:07:35,955 --> 00:07:39,885
Speaker 5:  causing a lot of self harm, eating disorders, whatever. Not to say,

121
00:07:39,885 --> 00:07:42,485
Speaker 5:  you know, TikTok is a very different TikTok iss much more akin to YouTube.

122
00:07:42,565 --> 00:07:45,645
Speaker 5:  I always call say it's a, it's a combination of Twitter and YouTube.

123
00:07:46,205 --> 00:07:49,805
Speaker 4:  I think that's right. All right, well before we get into the episode, how

124
00:07:49,805 --> 00:07:52,725
Speaker 4:  can people find the show? It's all the places you get podcasts, right? It's

125
00:07:52,725 --> 00:07:55,645
Speaker 4:  called Power User. You're also on YouTube, right? Doing video stuff?

126
00:07:55,905 --> 00:07:59,685
Speaker 5:  Yes, it's on my YouTube channel, so please subscribe. Taylor Lorenz. I'm

127
00:07:59,685 --> 00:08:03,005
Speaker 5:  doing lots more videos, lots more tech videos. It's just Taylor Lorenz on

128
00:08:03,005 --> 00:08:06,885
Speaker 5:  YouTube. And you can find us on Spotify Apple podcast. Please review it.

129
00:08:07,085 --> 00:08:10,925
Speaker 5:  If you like it. And Instagram. I'm gonna be sharing your Power User pod on

130
00:08:10,925 --> 00:08:11,925
Speaker 5:  Instagram. There

131
00:08:11,925 --> 00:08:15,245
Speaker 4:  You go. Awesome. All right, well here is the first episode of Power User.

132
00:08:15,245 --> 00:08:15,765
Speaker 4:  Thanks Taylor.

133
00:08:15,865 --> 00:08:16,845
Speaker 5:  Thanks for having me.

134
00:08:22,585 --> 00:08:26,475
Speaker 5:  This week we're talking about the TikTok ban, how shrimp Jesus is taking

135
00:08:26,475 --> 00:08:29,875
Speaker 5:  over Facebook and Mr. Beast's record breaking new reality show.

136
00:08:30,295 --> 00:08:33,395
Speaker 5:  I'm Taylor Lorenz. And that's all coming up right now on Power. User

137
00:11:24,775 --> 00:11:25,265
Speaker 6:  Welcome

138
00:11:25,325 --> 00:11:29,185
Speaker 5:  To Power User. I'm Taylor Lorenz, a technology journalist, and I'm so excited

139
00:11:29,185 --> 00:11:32,305
Speaker 5:  to be launching this show today. I've been covering internet culture, online

140
00:11:32,465 --> 00:11:35,505
Speaker 5:  creators, and the tech world for over a decade. Every week I'm gonna be breaking

141
00:11:35,505 --> 00:11:39,025
Speaker 5:  down the biggest stories in tech and online culture. And this week the big

142
00:11:39,025 --> 00:11:42,425
Speaker 5:  story is definitely TikTok. Recently the House of

143
00:11:42,425 --> 00:11:45,585
Speaker 5:  Representatives moved a bill forward that would effectively ban the app and

144
00:11:45,585 --> 00:11:49,465
Speaker 5:  The future of the platform is looking dicey. So I'm bringing in Evan Greer,

145
00:11:49,465 --> 00:11:53,185
Speaker 5:  the director of Fight for The future, a nonpartisan nonprofit organization

146
00:11:53,435 --> 00:11:56,965
Speaker 5:  that fights for civil liberties online. We're gonna break down the concerns

147
00:11:56,965 --> 00:12:00,485
Speaker 5:  around TikTok, what lawmakers are hoping to achieve by banning it, and whether

148
00:12:00,485 --> 00:12:04,045
Speaker 5:  this app is really something we should all be worried about. Hi Evan, thank

149
00:12:04,045 --> 00:12:06,445
Speaker 5:  you so much for joining me on the first episode of Power User.

150
00:12:06,955 --> 00:12:09,685
Speaker 8:  Yeah, for sure. Taylor, thanks so much for having me on. You've

151
00:12:09,685 --> 00:12:12,925
Speaker 5:  Been super critical of this legislation, so I wanna talk about some of the

152
00:12:12,925 --> 00:12:15,765
Speaker 5:  concerns with this bill and what you feel like Congress could actually be

153
00:12:15,765 --> 00:12:19,405
Speaker 5:  doing to make our lives better online. First of all, a lot of lawmakers are

154
00:12:19,405 --> 00:12:22,965
Speaker 5:  trying to claim that this is not a real ban, it's a forced sale, and there's

155
00:12:22,965 --> 00:12:26,405
Speaker 5:  some sort of big distinction between the two things. Is it fair to characterize

156
00:12:26,405 --> 00:12:30,005
Speaker 5:  this legislation as a forced sale or is this an effective ban?

157
00:12:30,755 --> 00:12:33,725
Speaker 8:  This bill is a ban by any other name.

158
00:12:34,665 --> 00:12:38,605
Speaker 8:  I'm not a a business lawyer. I've never tried to buy or

159
00:12:38,635 --> 00:12:42,565
Speaker 8:  sell a multi-billion dollar company, but it doesn't take an

160
00:12:42,565 --> 00:12:45,685
Speaker 8:  expert to understand that. Like those types of things don't tend to happen

161
00:12:45,875 --> 00:12:49,845
Speaker 8:  overnight. And all of the experts that I've talked to have

162
00:12:49,845 --> 00:12:53,765
Speaker 8:  suggested that the six month timeline that's laid out

163
00:12:53,785 --> 00:12:57,685
Speaker 8:  in this bill that would attempt to force TikTok to effectively sell

164
00:12:57,745 --> 00:13:01,085
Speaker 8:  itself or get banned is just completely

165
00:13:01,475 --> 00:13:02,805
Speaker 8:  impractical and ridiculous.

166
00:13:02,955 --> 00:13:06,765
Speaker 5:  Well, yeah, it's actually 180 days, right? Within that six

167
00:13:06,765 --> 00:13:10,605
Speaker 5:  month timeline, they have to sell within 180 days, which as my

168
00:13:10,605 --> 00:13:14,125
Speaker 5:  colleague Drew reported, would be one of the thorniest and most complicated

169
00:13:14,125 --> 00:13:18,085
Speaker 5:  transactions in all of corporate history. And it would almost

170
00:13:18,085 --> 00:13:21,845
Speaker 5:  certainly pose insurmountable financial, technical and geopolitical challenges

171
00:13:22,025 --> 00:13:24,285
Speaker 5:  making this sale basically untenable.

172
00:13:24,975 --> 00:13:28,515
Speaker 8:  That's exactly right. That's what I've heard from every expert that I've

173
00:13:28,515 --> 00:13:32,395
Speaker 8:  talked to about it as well, which is why the A CLU and many other

174
00:13:32,395 --> 00:13:36,355
Speaker 8:  civil liberties organizations have said clearly that this does amount

175
00:13:36,355 --> 00:13:39,315
Speaker 8:  to a ban. And because of that, it's clearly

176
00:13:39,855 --> 00:13:43,755
Speaker 8:  unconstitutional. In the United States, we have the first amendment, it

177
00:13:44,075 --> 00:13:48,035
Speaker 8:  prohibits the government from engaging in acts of censorship unless

178
00:13:48,035 --> 00:13:51,755
Speaker 8:  they can prove they have no other option, no other way to

179
00:13:51,755 --> 00:13:55,675
Speaker 8:  address the concerns that they're raising. And you know, we'll get

180
00:13:55,675 --> 00:13:58,915
Speaker 8:  into it a little bit more later, but they have many options, many ways that

181
00:13:58,915 --> 00:14:02,635
Speaker 8:  they could address legitimate concerns with TikTok and other big tech

182
00:14:02,915 --> 00:14:05,475
Speaker 8:  companies short of actually banning them outright.

183
00:14:05,865 --> 00:14:09,275
Speaker 5:  Yeah, and also, I mean you mentioned these lawmakers themselves have been

184
00:14:09,275 --> 00:14:12,675
Speaker 5:  referring to it as ban. I just wanna read a couple quotes from some of those

185
00:14:12,955 --> 00:14:16,035
Speaker 5:  lawmakers. These are the people that championed the bill and ushered it through

186
00:14:16,035 --> 00:14:19,635
Speaker 5:  Representative Elise Deon says quote, I'm proud to join Chairman

187
00:14:19,665 --> 00:14:23,035
Speaker 5:  Mike Gallagher to finally ban TikTok in the United States.

188
00:14:23,695 --> 00:14:27,595
Speaker 5:  And then representative Bob Lata says, I'm proud to help lead the bipartisan

189
00:14:27,975 --> 00:14:31,915
Speaker 5:  act, which will ban the app from the United States. Representative Chip

190
00:14:32,075 --> 00:14:35,195
Speaker 5:  Roy says, I'm proud to partner with the representatives on this bipartisan

191
00:14:35,305 --> 00:14:39,155
Speaker 5:  bill to ban the distribution of TikTok in the us. These lawmakers might

192
00:14:39,175 --> 00:14:43,115
Speaker 5:  try to sort of spin it in the media like, oh, it's not a real ban, but this

193
00:14:43,115 --> 00:14:46,835
Speaker 5:  is an effective ban and that is how they themselves are

194
00:14:46,835 --> 00:14:47,235
Speaker 5:  promoting it.

195
00:14:47,295 --> 00:14:50,675
Speaker 8:  That's exactly right. And I think in their minds, you know, I don't know

196
00:14:50,675 --> 00:14:54,165
Speaker 8:  that they've actually like gamed this out until the end, right? They may

197
00:14:54,165 --> 00:14:58,085
Speaker 8:  very much end up as the dogs who caught the car because I think they

198
00:14:58,155 --> 00:15:02,085
Speaker 8:  perhaps really are hoping to bully TikTok into selling themselves to a US

199
00:15:02,085 --> 00:15:05,005
Speaker 8:  company or something like that. But the timeline that they've laid out is

200
00:15:05,005 --> 00:15:08,845
Speaker 8:  so ridiculous that they might end up sort of accidentally banning TikTok

201
00:15:09,185 --> 00:15:13,045
Speaker 8:  and then dealing with the political consequences of that since we

202
00:15:13,045 --> 00:15:16,845
Speaker 8:  know it is an app that is wildly popular, especially with young

203
00:15:16,845 --> 00:15:20,725
Speaker 8:  people, which both parties are struggling to speak to in

204
00:15:20,725 --> 00:15:24,485
Speaker 8:  terms of what the issues that they care about. So I think unfortunately this

205
00:15:24,485 --> 00:15:28,445
Speaker 8:  is a lot of like tech policy made for tv and I think it's bad politics

206
00:15:28,445 --> 00:15:29,645
Speaker 8:  on both sides all around.

207
00:15:29,985 --> 00:15:32,965
Speaker 5:  Not to mention that musically, which is what TikTok was named before they

208
00:15:33,125 --> 00:15:36,685
Speaker 5:  rebranded it as TikTok was sold to Bite Dance in 2017.

209
00:15:37,075 --> 00:15:40,325
Speaker 5:  None of these lawmakers had any issue with that sale at the time, of course.

210
00:15:40,825 --> 00:15:43,645
Speaker 5:  But the reason that Musically sale went through and bite dance was the one

211
00:15:43,645 --> 00:15:47,045
Speaker 5:  to buy it as well is because there were no viable

212
00:15:47,425 --> 00:15:51,165
Speaker 5:  US purchasers. Google and Meta were essentially written out

213
00:15:51,165 --> 00:15:55,005
Speaker 5:  because of antitrust concerns. So I just think

214
00:15:55,005 --> 00:15:58,245
Speaker 5:  it's very telling that like who is this app gonna sell to? you know, if this

215
00:15:58,245 --> 00:16:02,005
Speaker 5:  alleged forced sale is supposed to happen, any major

216
00:16:02,005 --> 00:16:04,725
Speaker 5:  company that it would sell to, I think there would be serious antitrust concerns.

217
00:16:04,785 --> 00:16:06,925
Speaker 5:  So it's just totally unfeasible.

218
00:16:07,315 --> 00:16:08,805
Speaker 8:  Yeah, that's exactly right.

219
00:16:09,075 --> 00:16:12,405
Speaker 5:  Alright Evan, so let's get into some of the civil liberties concerns with

220
00:16:12,405 --> 00:16:15,485
Speaker 5:  the ban. How is this a free speech and First Amendment issue?

221
00:16:16,345 --> 00:16:19,845
Speaker 8:  So what this really comes down to is do you believe that your government

222
00:16:19,945 --> 00:16:23,725
Speaker 8:  should be able to tell you where you can get news from and where

223
00:16:23,905 --> 00:16:27,405
Speaker 8:  you can express yourself? In the United States, we have the First Amendment.

224
00:16:27,705 --> 00:16:31,525
Speaker 8:  The First Amendment prohibits the government from telling me I can't give

225
00:16:31,525 --> 00:16:35,285
Speaker 8:  an interview to Al Jazeera, even though Al Jazeera is not

226
00:16:35,285 --> 00:16:39,045
Speaker 8:  owned by a US company. Just like the government can't stop me from writing

227
00:16:39,105 --> 00:16:42,325
Speaker 8:  an op-ed, or a letter to the editor to a newspaper,

228
00:16:42,985 --> 00:16:46,765
Speaker 8:  TikTok is a platform that millions of Americans use to express themselves.

229
00:16:47,145 --> 00:16:50,765
Speaker 8:  So banning it would amount to silencing the voices of

230
00:16:50,765 --> 00:16:54,365
Speaker 8:  millions of people. The First Amendment requires that if the US

231
00:16:54,365 --> 00:16:58,285
Speaker 8:  government is going to engage in that type of act of restricting speech,

232
00:16:58,555 --> 00:17:02,365
Speaker 8:  they have to prove that they have no other way to

233
00:17:02,365 --> 00:17:06,205
Speaker 8:  address their concerns. Now, the concerns the US government has stated in

234
00:17:06,205 --> 00:17:09,805
Speaker 8:  this case, as they say, TikTok is collecting your data. It could be shared

235
00:17:09,805 --> 00:17:13,365
Speaker 8:  with the Chinese government and or the Chinese government could be manipulating

236
00:17:13,475 --> 00:17:17,445
Speaker 8:  tiktoks algorithm to spread propaganda. Let's take both of those concerns

237
00:17:17,445 --> 00:17:18,805
Speaker 8:  at face value. Let's say it's true.

238
00:17:19,365 --> 00:17:23,325
Speaker 5:  I mean, my feeling is even if it was a Chinese propaganda app say

239
00:17:23,325 --> 00:17:27,045
Speaker 5:  it was, don't we have the right to consume that if we want? Isn't that supposed

240
00:17:27,045 --> 00:17:29,525
Speaker 5:  to be a freedom that we're allowed?

241
00:17:30,445 --> 00:17:33,485
Speaker 8:  Absolutely. you know, there's nothing that says we shouldn't be able to read

242
00:17:33,555 --> 00:17:37,445
Speaker 8:  lies. And you know, unfortunately there's more

243
00:17:37,445 --> 00:17:40,925
Speaker 8:  of them floating around on the internet than any of us would perhaps like.

244
00:17:41,385 --> 00:17:44,925
Speaker 8:  But yes, absolutely. Even if we take the government's concerns completely

245
00:17:44,985 --> 00:17:48,885
Speaker 8:  at face value and they believe the Chinese Communist Party is literally has

246
00:17:48,885 --> 00:17:52,565
Speaker 8:  their their hand on the lever and is like controlling tiktoks algorithm at

247
00:17:52,565 --> 00:17:56,205
Speaker 8:  a granular level. And let's be clear, there's no actual evidence

248
00:17:56,465 --> 00:17:59,765
Speaker 8:  to suggest that that is happening. But it's a valid concern.

249
00:17:59,795 --> 00:18:03,725
Speaker 8:  Governments do things, the Chinese government certainly does engage in

250
00:18:03,725 --> 00:18:07,445
Speaker 8:  authoritarian acts, you know, worth asking. That

251
00:18:07,595 --> 00:18:11,565
Speaker 8:  said, there's absolutely ways to address that. We could pass algorithmic

252
00:18:11,565 --> 00:18:14,965
Speaker 8:  transparency legislation and require that companies like TikTok

253
00:18:15,315 --> 00:18:18,845
Speaker 8:  open up their algorithms to third party auditors that would

254
00:18:18,975 --> 00:18:22,765
Speaker 8:  catch a government that was engaging in that kind of manipulation. We could

255
00:18:22,765 --> 00:18:26,445
Speaker 8:  pass privacy legislation to prevent them from collecting so much data about

256
00:18:26,445 --> 00:18:30,005
Speaker 8:  all of us in the first place and using that data to recommend us content.

257
00:18:30,375 --> 00:18:33,805
Speaker 8:  Those are meaningful ways that we could address the types of concerns that

258
00:18:33,805 --> 00:18:37,405
Speaker 8:  the government is raising without banning an app that millions of people

259
00:18:37,465 --> 00:18:40,685
Speaker 8:  use to express themselves. That's why banning TikTok is blatantly

260
00:18:41,025 --> 00:18:45,005
Speaker 8:  unconstitutional and frankly a huge distraction from the types

261
00:18:45,005 --> 00:18:48,925
Speaker 8:  of real measures that would actually lead to a better internet with more

262
00:18:48,925 --> 00:18:50,885
Speaker 8:  free expression and more human rights for everyone.

263
00:18:51,435 --> 00:18:55,045
Speaker 5:  Yeah, I mean, I just think it's so telling that while these lawmakers claim,

264
00:18:55,225 --> 00:18:58,685
Speaker 5:  you know, concerns around national security and everything, and we'll definitely

265
00:18:58,685 --> 00:19:01,605
Speaker 5:  get into that. When you listen to what they said at the hearing,

266
00:19:02,585 --> 00:19:06,365
Speaker 5:  you realize that repeatedly they express issue with the

267
00:19:06,365 --> 00:19:08,205
Speaker 5:  content they believe to be on the app

268
00:19:08,555 --> 00:19:11,365
Speaker 9:  From the data it collects to the content it controls.

269
00:19:12,185 --> 00:19:16,165
Speaker 9:  TikTok is a grave threat of foreign influence

270
00:19:16,185 --> 00:19:20,005
Speaker 9:  in American life. It's been said, it's like allowing the Soviet Union the

271
00:19:20,005 --> 00:19:23,445
Speaker 9:  power to produce Saturday morning cartoons during the Cold War,

272
00:19:24,265 --> 00:19:26,925
Speaker 9:  but much more powerful and much more dangerous.

273
00:19:28,355 --> 00:19:31,155
Speaker 9:  Banning your platform will address the immediate threats.

274
00:19:32,495 --> 00:19:36,195
Speaker 5:  And that to me is really scary and dystopian because once we have these lawmakers

275
00:19:36,355 --> 00:19:40,235
Speaker 5:  starting to sort of legislate around the content that they

276
00:19:40,235 --> 00:19:43,955
Speaker 5:  think is okay, you know, to expose people to online or the news and information,

277
00:19:44,055 --> 00:19:47,115
Speaker 5:  you know, that young people have access to through apps like TikTok or other

278
00:19:47,115 --> 00:19:51,075
Speaker 5:  social media platforms, it just feels scary. Like that feels very

279
00:19:51,075 --> 00:19:55,035
Speaker 5:  much like an infringement on our ability to freely

280
00:19:55,035 --> 00:19:56,795
Speaker 5:  consume information and express ideas.

281
00:19:57,995 --> 00:20:01,645
Speaker 8:  Yeah. And unfortunately that kind of authoritarian

282
00:20:01,645 --> 00:20:05,485
Speaker 8:  impulse of going after online speech and content is

283
00:20:05,485 --> 00:20:09,245
Speaker 8:  not just limited to TikTok and it's unfortunately very bipartisan in

284
00:20:09,345 --> 00:20:12,885
Speaker 8:  DC right now. Both Democrats and Republicans have really leaned into this

285
00:20:12,915 --> 00:20:16,845
Speaker 8:  idea that it's like online content and

286
00:20:16,845 --> 00:20:20,645
Speaker 8:  speech that is driving these harms in our society rather than

287
00:20:20,645 --> 00:20:24,605
Speaker 8:  the underlying business practices of these companies that we can actually

288
00:20:25,165 --> 00:20:28,525
Speaker 8:  regulate. And so what you see is Republicans sort of being like, look at

289
00:20:28,525 --> 00:20:31,565
Speaker 8:  all this content that our kids are consuming and they're talking about like

290
00:20:31,805 --> 00:20:35,485
Speaker 8:  RuPaul's Drag race videos or whatever. But then you see Democrats leading

291
00:20:35,485 --> 00:20:38,605
Speaker 8:  into it and saying, look at these Andrew Tate videos and like they're being

292
00:20:38,605 --> 00:20:42,045
Speaker 8:  recommended to kids and like, that's terrible. And like they're right. That's

293
00:20:42,205 --> 00:20:45,605
Speaker 8:  terrible. The way that we address that is by addressing

294
00:20:45,605 --> 00:20:49,525
Speaker 8:  patriarchy in our society, by attacking the underlying business

295
00:20:49,525 --> 00:20:53,205
Speaker 8:  model that makes it profitable for companies to artificially amplify

296
00:20:53,465 --> 00:20:57,125
Speaker 8:  the most extreme content. We cannot address this issue

297
00:20:57,125 --> 00:21:00,965
Speaker 8:  through government censorship that just leads to more harm, particularly

298
00:21:00,985 --> 00:21:01,605
Speaker 8:  for young people.

299
00:21:01,985 --> 00:21:05,485
Speaker 5:  And say China did wanna interfere with America through, you know,

300
00:21:05,485 --> 00:21:09,445
Speaker 5:  programming social media. Is there anything that TikTok

301
00:21:09,885 --> 00:21:13,725
Speaker 5:  specifically allows that other American owned apps like Facebook and

302
00:21:13,725 --> 00:21:15,325
Speaker 5:  Twitter and YouTube don't allow?

303
00:21:16,325 --> 00:21:20,265
Speaker 8:  No, these companies operate in almost the exact same manner. They

304
00:21:20,265 --> 00:21:24,105
Speaker 8:  all employ the same basic surveillance capitalist business

305
00:21:24,185 --> 00:21:28,025
Speaker 8:  model, which is about collecting your data, monitoring what you do

306
00:21:28,025 --> 00:21:31,905
Speaker 8:  on the app, and then using that information to recommend you content in order

307
00:21:31,925 --> 00:21:35,825
Speaker 8:  to keep you clicking and scrolling and generating advertising revenue. They're

308
00:21:35,825 --> 00:21:39,025
Speaker 8:  all basically the same app and they all sort of increasingly become more

309
00:21:39,025 --> 00:21:41,705
Speaker 8:  and more like the same app as they just sort of copy each other.

310
00:21:42,305 --> 00:21:46,225
Speaker 5:  Although one thing I'll say about that is that there has been this

311
00:21:46,625 --> 00:21:49,825
Speaker 5:  refrain from a lot of people of, oh well because these are just the same

312
00:21:49,895 --> 00:21:53,545
Speaker 5:  apps. If we delete TikTok, everybody can just go use, you know,

313
00:21:53,575 --> 00:21:57,545
Speaker 5:  Instagram or Twitter or whatever. And I do think that there's

314
00:21:57,545 --> 00:22:00,425
Speaker 5:  something that's different about TikTok, which is that it's not owned by

315
00:22:00,425 --> 00:22:03,865
Speaker 5:  an American company. And also these other platforms have made very

316
00:22:03,955 --> 00:22:07,905
Speaker 5:  aggressive moves to censor speech. I mean, I'm just thinking of

317
00:22:07,905 --> 00:22:11,585
Speaker 5:  meta Instagram's recent policies saying that any content that touches on

318
00:22:11,585 --> 00:22:14,945
Speaker 5:  political issues or social issues will be down ranked. And that is the type

319
00:22:14,945 --> 00:22:17,905
Speaker 5:  of stuff that they're trying to deprioritize on Instagram, Facebook, and

320
00:22:17,905 --> 00:22:21,385
Speaker 5:  threads. So it's really hard to talk about things like

321
00:22:21,415 --> 00:22:25,385
Speaker 5:  revenge porn or the election or just these really critical issues that we

322
00:22:25,385 --> 00:22:29,265
Speaker 5:  should be talking about in society on meta owned apps. Look at Twitter, right?

323
00:22:29,655 --> 00:22:33,185
Speaker 5:  Elon Musk has spent the past week banning any journalists that reported on

324
00:22:33,185 --> 00:22:36,905
Speaker 5:  this Nazi comic book writer. And YouTube is much more of an

325
00:22:36,905 --> 00:22:39,505
Speaker 5:  entertainment platform in the sense that I don't think a lot of average users

326
00:22:39,535 --> 00:22:42,825
Speaker 5:  post on there. It's also not a great place for politics. So

327
00:22:43,505 --> 00:22:46,945
Speaker 5:  I do think that TikTok allowed a certain group of people, especially young

328
00:22:47,185 --> 00:22:50,825
Speaker 5:  progressives, a space to really have these urgent political conversations

329
00:22:50,835 --> 00:22:54,305
Speaker 5:  about things like the war on Gaza or climate change and things like that.

330
00:22:54,925 --> 00:22:58,105
Speaker 5:  And it seems that seems to have really irked these lawmakers.

331
00:22:59,055 --> 00:23:02,545
Speaker 8:  Yeah, I think that's definitely a part of the picture here. Although I think

332
00:23:02,545 --> 00:23:06,065
Speaker 8:  in the long run we just have to recognize that like TikTok may be a

333
00:23:06,065 --> 00:23:09,705
Speaker 8:  friendlier space for some of the politics that we care about right now,

334
00:23:10,145 --> 00:23:14,025
Speaker 8:  but in the end, as long as we're reliant on a very small handful

335
00:23:14,445 --> 00:23:17,985
Speaker 8:  of privately owned companies, whether they're owned by US

336
00:23:18,385 --> 00:23:22,305
Speaker 8:  companies or companies based in other countries to be

337
00:23:22,325 --> 00:23:26,025
Speaker 8:  the kind of digital public square, we're always going to be sort of at the

338
00:23:26,025 --> 00:23:29,905
Speaker 8:  whim of whoever our digital landlord is, whether it's Elon Musk or Mark

339
00:23:29,905 --> 00:23:33,705
Speaker 8:  Zuckerberg or by dance. And so I think again, that's why

340
00:23:33,905 --> 00:23:37,865
Speaker 8:  we need more thoughtful and comprehensive policies that get us past the current

341
00:23:37,965 --> 00:23:40,865
Speaker 8:  era that we're in and toward The future where we have meaningful choices

342
00:23:41,085 --> 00:23:44,985
Speaker 8:  so that If, you are concerned about TikTok or Instagram or any

343
00:23:45,050 --> 00:23:48,965
Speaker 8:  other company and their business practices, you can actually go find somewhere

344
00:23:48,965 --> 00:23:52,005
Speaker 8:  else. And just to your other point of this idea that like, oh, well if we

345
00:23:52,125 --> 00:23:56,045
Speaker 8:  ban TikTok, everyone can just immediately switch. Big tech companies have

346
00:23:56,045 --> 00:23:59,885
Speaker 8:  intentionally made it as difficult as possible to switch

347
00:23:59,885 --> 00:24:03,165
Speaker 8:  from one platform to another to bring your followers with you.

348
00:24:03,825 --> 00:24:07,325
Speaker 8:  And you know, that really affects marginalized people the most. Maybe if

349
00:24:07,325 --> 00:24:11,165
Speaker 8:  you're a big celebrity and you, you know, the app you're on gets banned,

350
00:24:11,165 --> 00:24:14,005
Speaker 8:  you can switch to a different app, everyone's gonna follow you there. If

351
00:24:14,005 --> 00:24:17,965
Speaker 8:  you're an L-G-B-T-Q content creator or a musician or someone

352
00:24:17,965 --> 00:24:21,805
Speaker 8:  who's spent a lot of time building an audience on one platform and then

353
00:24:21,965 --> 00:24:25,485
Speaker 8:  that platform gets ripped out from underneath you, that can have a profound

354
00:24:25,485 --> 00:24:28,565
Speaker 8:  impact on your ability to make living your ability to make rent that month,

355
00:24:28,795 --> 00:24:32,605
Speaker 8:  your ability to express yourself and connect with your community and your

356
00:24:32,885 --> 00:24:36,845
Speaker 8:  audience. So I think we shouldn't dismiss or ignore the real

357
00:24:36,845 --> 00:24:40,605
Speaker 8:  human impacts of, you know, these types of measures or

358
00:24:40,805 --> 00:24:44,325
Speaker 8:  thinking about banning an entire app or the impact it has on people when

359
00:24:44,325 --> 00:24:48,285
Speaker 8:  an app gets bought or sold. I mean, you know it, it personally impacted

360
00:24:48,385 --> 00:24:51,725
Speaker 8:  me when Elon Musk bought Twitter, it was someplace I spent a fair amount

361
00:24:51,725 --> 00:24:55,325
Speaker 8:  of time building an audience and speaking particularly to other L-G-B-T-Q

362
00:24:55,325 --> 00:24:59,245
Speaker 8:  people and then it became a much less friendly place for my community.

363
00:24:59,385 --> 00:25:02,525
Speaker 8:  That's a reality that you know, many people have had to deal with and we

364
00:25:02,525 --> 00:25:03,405
Speaker 8:  should take it seriously.

365
00:25:03,905 --> 00:25:07,045
Speaker 5:  And I think a lot of people also misunderstand the ownership structure of

366
00:25:07,065 --> 00:25:11,005
Speaker 5:  TikTok. TikTok is not majority owned by dance, okay? This is

367
00:25:11,025 --> 00:25:14,565
Speaker 5:  not how TikTok is owned. TikTok was in a company that was incorporated in

368
00:25:14,565 --> 00:25:18,125
Speaker 5:  America, and it is based in Los Angeles

369
00:25:18,385 --> 00:25:22,325
Speaker 5:  and Singapore. It does not operate in China. It is owned

370
00:25:22,375 --> 00:25:26,165
Speaker 5:  60% by foreign investors, including many Americans,

371
00:25:26,225 --> 00:25:29,965
Speaker 5:  20% owned by employees, including over 7,000 American

372
00:25:30,205 --> 00:25:33,125
Speaker 5:  employees and 20% owned by the company's founders. So

373
00:25:34,035 --> 00:25:37,965
Speaker 5:  does Bite Dance have a stake in TikTok? Yes, but this is

374
00:25:37,965 --> 00:25:41,925
Speaker 5:  not an app that is solely owned by a Chinese corporation. So back in

375
00:25:41,925 --> 00:25:45,725
Speaker 5:  2019, the US government forced another Chinese company to give

376
00:25:45,825 --> 00:25:49,245
Speaker 5:  up a 60% stake in Grindr, a gay dating app.

377
00:25:49,835 --> 00:25:52,445
Speaker 5:  It's bothers me a little bit when people try to make these comparisons because

378
00:25:52,585 --> 00:25:56,005
Speaker 5:  Grindr is not a media platform, it is not a social media platform. It's a

379
00:25:56,005 --> 00:25:59,445
Speaker 5:  completely different product. Also, the ownership structure is

380
00:25:59,445 --> 00:26:03,205
Speaker 5:  completely different. In that case, a Chinese company had majority

381
00:26:03,205 --> 00:26:07,005
Speaker 5:  ownership. I it's truly apples to oranges and it is

382
00:26:07,005 --> 00:26:09,965
Speaker 5:  very frustrating to sort of debunk all the different ways. This is different.

383
00:26:10,195 --> 00:26:13,245
Speaker 5:  Grinder had a completely different corporate ownership structure.

384
00:26:14,255 --> 00:26:17,825
Speaker 8:  Well and even more absurdly when the US

385
00:26:17,825 --> 00:26:21,785
Speaker 8:  government forced Grindr to sell itself to US company, which they did,

386
00:26:22,085 --> 00:26:25,625
Speaker 8:  it wasn't that long ago that data brokers were caught

387
00:26:25,755 --> 00:26:29,585
Speaker 8:  collecting and selling information from Grindr to a right wing

388
00:26:29,825 --> 00:26:32,905
Speaker 8:  religious group that was using it to track down priests that they thought

389
00:26:32,905 --> 00:26:36,665
Speaker 8:  might be gay. So here's an example of where if what we're really

390
00:26:36,665 --> 00:26:40,465
Speaker 8:  concerned about is like bad actors might get this data from

391
00:26:40,565 --> 00:26:43,645
Speaker 8:  TikTok, then we need to regulate the

392
00:26:44,405 --> 00:26:48,085
Speaker 8:  industry of collecting and selling people's data because the Chinese government,

393
00:26:48,185 --> 00:26:51,725
Speaker 8:  if you're concerned about them, could just go buy a lot of the same data

394
00:26:52,115 --> 00:26:55,565
Speaker 8:  from data brokers in the US that they could get from TikTok if that's even

395
00:26:55,565 --> 00:26:58,125
Speaker 8:  happening, which again, there's no evidence to suggest that it actually is.

396
00:26:58,185 --> 00:26:58,405
Speaker 8:  So

397
00:26:58,405 --> 00:27:01,645
Speaker 5:  Let's dig into some of the national security concerns because I think a lot

398
00:27:01,645 --> 00:27:05,205
Speaker 5:  of people are scared about the data privacy. TikTok has said

399
00:27:05,485 --> 00:27:09,005
Speaker 5:  repeatedly that US user data is stored in Texas on Oracle

400
00:27:09,055 --> 00:27:12,645
Speaker 5:  controlled servers. So in order for TikTok to make changes

401
00:27:12,865 --> 00:27:15,925
Speaker 5:  or access that data or the Chinese government was gonna try to access this

402
00:27:15,925 --> 00:27:19,485
Speaker 5:  data, it would have to go through Oracle, which is a US based company to

403
00:27:19,485 --> 00:27:23,085
Speaker 5:  access that data, which again is housed in the us. There is no evidence that

404
00:27:23,865 --> 00:27:27,605
Speaker 5:  any Chinese entity has even tried to gain access to that data.

405
00:27:28,155 --> 00:27:31,725
Speaker 5:  Meanwhile, we know that back in 2016 and many other times,

406
00:27:32,035 --> 00:27:35,805
Speaker 5:  foreign governments actually were able to manipulate our social media landscape

407
00:27:36,105 --> 00:27:39,845
Speaker 5:  not through any sort of secret data obtaining but

408
00:27:39,845 --> 00:27:43,805
Speaker 5:  through buying things like public Facebook ads or buying data publicly.

409
00:27:43,865 --> 00:27:47,565
Speaker 5:  So tell me a little bit about these concerns about data privacy and

410
00:27:47,665 --> 00:27:51,245
Speaker 5:  TikTok and whether you think that any of this is warranted.

411
00:27:52,365 --> 00:27:56,335
Speaker 8:  Sure. So people are right to be concerned about TikTok collecting their

412
00:27:56,365 --> 00:28:00,215
Speaker 8:  data. They should just also be as concerned about

413
00:28:01,015 --> 00:28:04,775
Speaker 8:  companies like Instagram and Uber and Postmates and your

414
00:28:04,795 --> 00:28:08,615
Speaker 8:  dog walking app and almost every other free online

415
00:28:08,615 --> 00:28:12,575
Speaker 8:  service that you use collecting your data. And what we know is that

416
00:28:12,605 --> 00:28:16,255
Speaker 8:  once a company collects your data, it doesn't necessarily

417
00:28:16,565 --> 00:28:20,455
Speaker 8:  stay with that company. There is a massive unregulated data

418
00:28:20,455 --> 00:28:24,415
Speaker 8:  broker industry in the us. Companies collect your data and then sell it

419
00:28:24,415 --> 00:28:28,095
Speaker 8:  to other companies who will then sell it to the highest bidder. It's a

420
00:28:28,295 --> 00:28:32,135
Speaker 8:  national embarrassment that we do not have basic data privacy laws

421
00:28:32,135 --> 00:28:35,935
Speaker 8:  here in the us. So it's almost sort of a funny talking point for TikTok I

422
00:28:35,935 --> 00:28:39,815
Speaker 8:  would say to be like, our data is housed in Texas where there's basically

423
00:28:39,875 --> 00:28:43,775
Speaker 8:  no privacy protections or laws to protect your data. And we

424
00:28:43,775 --> 00:28:47,415
Speaker 8:  have like the Attorney General of Texas, Ken Paxton is like making lists

425
00:28:47,415 --> 00:28:51,255
Speaker 8:  of trans people. So it doesn't make me feel particularly safer that my

426
00:28:51,255 --> 00:28:55,215
Speaker 8:  data is stored there than if it restored anywhere else. In the end, what

427
00:28:55,215 --> 00:28:58,975
Speaker 8:  we need is policies that protect everyone's data, which would start

428
00:28:58,975 --> 00:29:02,695
Speaker 8:  with a strong national privacy law here in the US and ideally

429
00:29:02,765 --> 00:29:04,375
Speaker 8:  laws like that all around the world.

430
00:29:05,265 --> 00:29:08,205
Speaker 5:  Why hasn't a law like that been passed? Because it seems like such a no-brainer.

431
00:29:08,205 --> 00:29:10,925
Speaker 5:  This is something that consumers have repeatedly asked for.

432
00:29:11,885 --> 00:29:15,825
Speaker 8:  It is, and it's where there's so much consensus between human

433
00:29:15,825 --> 00:29:19,465
Speaker 8:  rights experts and the business community and others. Like

434
00:29:20,025 --> 00:29:23,865
Speaker 8:  everyone agrees a privacy law would be a good idea. The reason it hasn't

435
00:29:23,865 --> 00:29:27,505
Speaker 8:  moved forward is basically the same reasons that a lot of other

436
00:29:27,505 --> 00:29:31,265
Speaker 8:  legislation doesn't move. It's about corruption, corporate

437
00:29:31,265 --> 00:29:35,065
Speaker 8:  influence, and congressional dysfunction. So the corporate

438
00:29:35,065 --> 00:29:38,745
Speaker 8:  influence part is probably the biggest factor here. Certainly big tech

439
00:29:39,025 --> 00:29:42,645
Speaker 8:  companies have been very active in lobbying against strong privacy

440
00:29:42,835 --> 00:29:46,685
Speaker 8:  legislation, but it's not just the like Metas and Instagrams

441
00:29:46,685 --> 00:29:50,485
Speaker 8:  and YouTubes of the world that care about this, it's Bank of America and

442
00:29:50,485 --> 00:29:54,205
Speaker 8:  Walmart and Target and effectively every other large

443
00:29:54,385 --> 00:29:58,285
Speaker 8:  US company that at this point has some interest in being able

444
00:29:58,285 --> 00:30:02,165
Speaker 8:  to conduct the type of commercial surveillance that allows them

445
00:30:02,165 --> 00:30:05,965
Speaker 8:  to gain a competitive edge. So unfortunately, because of a

446
00:30:05,965 --> 00:30:09,725
Speaker 8:  lack of regulation, almost every big company in the US has sort of

447
00:30:09,725 --> 00:30:13,485
Speaker 8:  become a surveillance capitalist company in some capacity

448
00:30:13,505 --> 00:30:17,245
Speaker 8:  or other. And that's a big reason why privacy legislation

449
00:30:17,245 --> 00:30:20,725
Speaker 8:  hasn't moved forward because it's not just opposed by the tech industry,

450
00:30:21,075 --> 00:30:24,365
Speaker 8:  it's sort of posed by Wall Street and the broader

451
00:30:24,755 --> 00:30:28,365
Speaker 8:  behemoths of the American economy. And that's what needs to be overcome

452
00:30:28,785 --> 00:30:32,605
Speaker 8:  to get something done on this, even though it's overwhelmingly supported

453
00:30:32,745 --> 00:30:36,405
Speaker 8:  people from across the political spectrum. Overwhelmingly agree, we

454
00:30:36,725 --> 00:30:40,485
Speaker 8:  don't want companies to be able to collect so much of our private information

455
00:30:40,625 --> 00:30:44,605
Speaker 8:  and use it to enrich themselves at the cost of our basic rights

456
00:30:44,605 --> 00:30:44,965
Speaker 8:  and safety.

457
00:30:45,705 --> 00:30:48,725
Speaker 5:  I'm curious what you think about Biden's reversal and all this. you know,

458
00:30:48,725 --> 00:30:52,645
Speaker 5:  back in 2020 it was Trump pushing this ban. Trump has now recognized correctly

459
00:30:52,645 --> 00:30:56,605
Speaker 5:  that a ban would actually embolden Facebook, which I

460
00:30:56,605 --> 00:31:00,085
Speaker 5:  guess is his enemy now. And now we have Biden saying that he would sign a

461
00:31:00,085 --> 00:31:03,285
Speaker 5:  bill into law. What do you think led to that flip flop?

462
00:31:04,245 --> 00:31:08,005
Speaker 8:  I think it just shows that, again, this is really all about politics, right?

463
00:31:08,075 --> 00:31:11,565
Speaker 8:  More than it's about the substance, more than it's about any specific business

464
00:31:11,925 --> 00:31:15,725
Speaker 8:  practice. and more than it's about any legitimate concerns, TikTok

465
00:31:15,745 --> 00:31:19,485
Speaker 8:  has become a bit of a political football that's being kicked back and forth

466
00:31:19,755 --> 00:31:23,605
Speaker 8:  between two parties that are sort of trying to outcompete each other on who

467
00:31:23,605 --> 00:31:27,445
Speaker 8:  can be the toughest on China. And all of us in our basic

468
00:31:27,505 --> 00:31:30,845
Speaker 8:  rights and free expression are sort of caught in the middle of this

469
00:31:30,955 --> 00:31:34,685
Speaker 8:  political football game. But I think, you know, just to say it like, look,

470
00:31:34,845 --> 00:31:38,525
Speaker 8:  I am an advocate. I run a nonpartisan nonprofit. We focus

471
00:31:38,625 --> 00:31:42,165
Speaker 8:  on advocating for people's rights. So I don't get involved in

472
00:31:42,165 --> 00:31:45,925
Speaker 8:  electoral politics, but just it doesn't take an expert to sit back and be

473
00:31:45,925 --> 00:31:49,285
Speaker 8:  like, it would be bad politics for any lawmaker

474
00:31:49,825 --> 00:31:53,245
Speaker 8:  or certainly any sitting president or presidential hopeful

475
00:31:53,745 --> 00:31:57,445
Speaker 8:  to get on board with the idea of banning an app that is disproportionately

476
00:31:57,475 --> 00:32:01,405
Speaker 8:  used by young people who are the voters that both parties are desperately

477
00:32:01,405 --> 00:32:05,125
Speaker 8:  trying to court. And so it just strikes me as like deeply

478
00:32:05,215 --> 00:32:09,205
Speaker 8:  silly and absurd, and especially now that Trump has come out and

479
00:32:09,205 --> 00:32:12,845
Speaker 8:  said that he's against a TikTok ban, Biden would just be walking into the

480
00:32:12,845 --> 00:32:16,485
Speaker 8:  most obvious trap ever. If he were to sign a bill banning

481
00:32:16,485 --> 00:32:20,285
Speaker 8:  TikTok and hand Trump this talking point to beat him over the head with,

482
00:32:20,455 --> 00:32:24,205
Speaker 8:  especially with the young voters that he is really struggling to

483
00:32:24,275 --> 00:32:28,205
Speaker 8:  keep support from, especially given his ongoing support of

484
00:32:28,505 --> 00:32:31,965
Speaker 8:  the violence in Gaza and a lot of other issues that young people

485
00:32:32,625 --> 00:32:36,285
Speaker 8:  are concerned about and talking about on TikTok primarily.

486
00:32:36,755 --> 00:32:40,525
Speaker 5:  Yeah, you know, speaking of playing politics, TikTok was also accused of

487
00:32:40,525 --> 00:32:44,125
Speaker 5:  playing politics by quote unquote forcing users to contact their

488
00:32:44,125 --> 00:32:47,925
Speaker 5:  representatives and express their opinion on the van. Just to be clear,

489
00:32:48,025 --> 00:32:51,405
Speaker 5:  TikTok didn't force users to do anything despite what Congress people are

490
00:32:51,565 --> 00:32:55,045
Speaker 5:  claiming. How do you think that push alert that they sent helped or hurt

491
00:32:55,045 --> 00:32:56,045
Speaker 5:  them? Yeah,

492
00:32:56,065 --> 00:32:59,685
Speaker 8:  You know, I mean I think it's hard to say, right? Certainly many members

493
00:32:59,705 --> 00:33:03,085
Speaker 8:  of Congress have leaned into that talking point and said like, see, look,

494
00:33:03,085 --> 00:33:06,965
Speaker 8:  here's the proof that they have all this power to manipulate people. I

495
00:33:06,965 --> 00:33:10,885
Speaker 8:  mean, I'm old enough to remember the SOPA blackout when Wikipedia and

496
00:33:11,125 --> 00:33:15,085
Speaker 8:  thousands of other websites blacked out their homepage to protest internet

497
00:33:15,085 --> 00:33:18,725
Speaker 8:  censorship legislation that could have led to widespread harm

498
00:33:18,785 --> 00:33:22,765
Speaker 8:  online. So this is not a brand new thing that said,

499
00:33:22,825 --> 00:33:25,965
Speaker 8:  you know, I couldn't tell you whether it was, you know, helpful or hurtful

500
00:33:25,965 --> 00:33:29,645
Speaker 8:  to their case in Washington DC because in the end, I think this does come

501
00:33:29,645 --> 00:33:33,445
Speaker 8:  down to politics and sort of what lawmakers think is gonna help or

502
00:33:33,445 --> 00:33:36,885
Speaker 8:  hurt them in the election. I should have mentioned this earlier, but just

503
00:33:36,885 --> 00:33:40,765
Speaker 8:  like the timeline that has been laid out in this bill of when a

504
00:33:40,765 --> 00:33:44,485
Speaker 8:  sale would have to be forced is suspiciously timed to be

505
00:33:44,705 --> 00:33:47,925
Speaker 8:  before the election, right? And so I think again, that does show us that

506
00:33:47,925 --> 00:33:51,845
Speaker 8:  like more than this is about China and more than this is about data

507
00:33:51,995 --> 00:33:55,565
Speaker 8:  more than this is about speech, this is about 2024,

508
00:33:56,065 --> 00:34:00,005
Speaker 8:  and politicians sort of jockeying for position with TikTok as

509
00:34:00,005 --> 00:34:01,405
Speaker 8:  a ball that they're kicking back and forth.

510
00:34:01,985 --> 00:34:05,685
Speaker 5:  I'm so glad you brought up SOPA and that blackout because there's been so

511
00:34:05,685 --> 00:34:09,565
Speaker 5:  many efforts over the years from tech companies and startups

512
00:34:09,745 --> 00:34:13,405
Speaker 5:  to push certain pieces of legislation through Congress and otherwise. I'm

513
00:34:13,405 --> 00:34:16,525
Speaker 5:  also thinking of all of the work that companies did around net neutrality.

514
00:34:16,605 --> 00:34:19,325
Speaker 5:  I mean, for a while you couldn't log onto Tumblr back in the day without

515
00:34:19,325 --> 00:34:23,085
Speaker 5:  being hit with this big message about this net neutrality legislation that

516
00:34:23,285 --> 00:34:27,045
Speaker 5:  Congress wanted to pass. More recently, other tech companies have pushed

517
00:34:27,045 --> 00:34:30,525
Speaker 5:  messaging to users about legislation they felt would threaten their businesses

518
00:34:30,765 --> 00:34:34,725
Speaker 5:  directly. Instacart, Uber, Lyft, Postmates, DoorDash have all engaged in

519
00:34:34,835 --> 00:34:38,765
Speaker 5:  this type of behavior. In 2020, Uber users in California had to confirm

520
00:34:38,765 --> 00:34:42,125
Speaker 5:  that they'd seen a message before calling the ride, which told them that

521
00:34:42,125 --> 00:34:45,965
Speaker 5:  wait times and prices would rise if this legislation called Prop

522
00:34:45,965 --> 00:34:49,765
Speaker 5:  22 wasn't passed. And Uber actually sent repeated push

523
00:34:49,765 --> 00:34:52,885
Speaker 5:  notifications to users warning them about Prop 22.

524
00:34:53,315 --> 00:34:57,285
Speaker 5:  Instacart workers were also even instructed to insert stickers and flyers

525
00:34:57,315 --> 00:35:01,245
Speaker 5:  endorsing a pretty controversial California ballot measure into

526
00:35:01,405 --> 00:35:05,045
Speaker 5:  customers shopping bags at grocery stores. I really agreed with The Verge

527
00:35:05,045 --> 00:35:08,565
Speaker 5:  as Miato who said, you know, it's really weird to act like this type of behavior

528
00:35:08,655 --> 00:35:12,645
Speaker 5:  isn't standard. And I think that these lawmakers trying to spin it as like

529
00:35:12,935 --> 00:35:16,685
Speaker 5:  China pushing its agenda on the American public are ignoring a

530
00:35:16,685 --> 00:35:20,445
Speaker 5:  long history of tech companies and startups doing exactly this.

531
00:35:20,755 --> 00:35:24,685
Speaker 8:  Yeah, absolutely. And look like this is how companies operate

532
00:35:25,345 --> 00:35:28,805
Speaker 8:  in a capitalist system. We should reckon with that and then

533
00:35:29,025 --> 00:35:33,005
Speaker 8:  decide what we want to do as a society and what sorts of rules should be

534
00:35:33,005 --> 00:35:36,725
Speaker 8:  governing technology and recognize that companies are gonna

535
00:35:36,785 --> 00:35:40,685
Speaker 8:  try to influence that. And it's up to us as a society to push

536
00:35:40,685 --> 00:35:43,165
Speaker 8:  back on that influence and do what actually needs to be done.

537
00:35:43,675 --> 00:35:47,405
Speaker 5:  Yeah, and I certainly would never argue, well Uber did something, therefore

538
00:35:47,635 --> 00:35:51,045
Speaker 5:  it's totally fine that TikTok is to it. I think it's pretty dark that it

539
00:35:51,045 --> 00:35:54,485
Speaker 5:  is so common. But to sort of ignore that history to me is

540
00:35:54,805 --> 00:35:58,765
Speaker 5:  disingenuous. What should average users do who are against

541
00:35:58,795 --> 00:35:59,805
Speaker 5:  this type of bill?

542
00:36:00,675 --> 00:36:04,255
Speaker 8:  You know, I actually think it's still really important that people do

543
00:36:04,405 --> 00:36:07,935
Speaker 8:  call and email their members of Congress about this. And I would even encourage

544
00:36:07,935 --> 00:36:11,855
Speaker 8:  you to call and say, I am not calling from that TikTok push notification.

545
00:36:12,535 --> 00:36:16,335
Speaker 8:  I really care about this because I care about free speech

546
00:36:16,395 --> 00:36:20,335
Speaker 8:  and human rights. I want you to pass privacy legislation instead of

547
00:36:20,495 --> 00:36:24,335
Speaker 8:  banning TikTok. We have a campaign, my organization fight for The future

548
00:36:24,675 --> 00:36:28,655
Speaker 8:  at don't ban TikTok dot com that has that exact messaging. So it's not just

549
00:36:28,655 --> 00:36:32,495
Speaker 8:  saying, Hey, Congress, don't do anything. It's saying pass

550
00:36:32,615 --> 00:36:36,295
Speaker 8:  privacy laws if you're concerned about this, rather than banning TikTok.

551
00:36:36,315 --> 00:36:39,695
Speaker 8:  And I think that that is really important because again, there's some of

552
00:36:39,695 --> 00:36:43,575
Speaker 8:  this that is just about political posturing and stoking

553
00:36:43,625 --> 00:36:47,575
Speaker 8:  xenophobia and you know, kind of saying whatever politicians think

554
00:36:47,575 --> 00:36:51,335
Speaker 8:  is gonna sound good on Fox News and M-S-N-B-C. But there's some

555
00:36:51,535 --> 00:36:55,335
Speaker 8:  amount of it that is like lawmakers having genuine concerns. And to the

556
00:36:55,335 --> 00:36:58,815
Speaker 8:  extent there are lawmakers with genuine concerns, it's important that we

557
00:36:59,005 --> 00:37:02,455
Speaker 8:  tell them that there are alternatives, that there are things that they can

558
00:37:02,455 --> 00:37:06,295
Speaker 8:  do to address those concerns, but that banning TikTok is not one of them.

559
00:37:06,295 --> 00:37:10,215
Speaker 8:  So I would encourage everyone to write an email their senators since

560
00:37:10,215 --> 00:37:13,935
Speaker 8:  this is now really a Senate issue, and tell them to

561
00:37:13,965 --> 00:37:17,655
Speaker 8:  pass a privacy law instead of banning TikTok. The more they hear from people,

562
00:37:17,795 --> 00:37:21,575
Speaker 8:  it does matter. I know sometimes it sounds, it feels like they're just ignoring

563
00:37:21,595 --> 00:37:25,335
Speaker 8:  you, they never listen. But I can tell you that congressional staffers,

564
00:37:25,485 --> 00:37:29,015
Speaker 8:  when their phone starts ringing off the hook, it does make a difference.

565
00:37:29,035 --> 00:37:33,015
Speaker 8:  It does give them pause. It may not always translate into what their boss

566
00:37:33,165 --> 00:37:37,095
Speaker 8:  says on tv, but it does have an impact on how they end up voting or

567
00:37:37,095 --> 00:37:40,855
Speaker 8:  whether the legislation even goes to the floor. The more that

568
00:37:40,855 --> 00:37:44,735
Speaker 8:  particularly Chuck Schumer Senate majority leader gets the sense that this

569
00:37:44,735 --> 00:37:48,215
Speaker 8:  is controversial and would be a bad look, the less likely

570
00:37:48,555 --> 00:37:52,015
Speaker 8:  he is to move it forward. I could very much see a scenario where they just

571
00:37:52,015 --> 00:37:55,375
Speaker 8:  to kind of decide, yeah, we'll keep talking about this TikTok thing, but

572
00:37:55,375 --> 00:37:58,255
Speaker 8:  we're just gonna kind of like hope that people forget about it and never

573
00:37:58,255 --> 00:38:01,895
Speaker 8:  bring it to the floor. I wouldn't be surprised if that happens, but

574
00:38:02,235 --> 00:38:06,215
Speaker 8:  if people don't speak out, we could end up in a really dangerous

575
00:38:06,535 --> 00:38:10,295
Speaker 8:  situation here and I could see an environment where they sort of accidentally

576
00:38:10,935 --> 00:38:14,535
Speaker 8:  actually succeed in banning TikTok and the political consequences of that

577
00:38:14,535 --> 00:38:15,495
Speaker 8:  could be really dire.

578
00:38:16,215 --> 00:38:18,935
Speaker 5:  I think a lot of the conversations around this app and the way that people

579
00:38:18,935 --> 00:38:22,495
Speaker 5:  have talked about China, and I'm thinking of when Tiktoks

580
00:38:22,575 --> 00:38:26,255
Speaker 5:  CEO was hauled in front of Congress and was repeatedly asked if he was an

581
00:38:26,255 --> 00:38:29,575
Speaker 5:  agent of the Chinese Communist Party, despite the fact that he's Singaporean

582
00:38:29,635 --> 00:38:31,895
Speaker 5:  and served in the Singaporean military.

583
00:38:31,895 --> 00:38:33,495
Speaker 8:  Have you ever been a member of the Chinese Communist Party?

584
00:38:33,645 --> 00:38:35,405
Speaker 10:  Senator? I'm Singaporean, no.

585
00:38:36,035 --> 00:38:39,005
Speaker 11:  Have you ever been associated or affiliated with the Chinese Communist Party?

586
00:38:39,225 --> 00:38:41,005
Speaker 10:  No. Senator, again, I'm Singaporean.

587
00:38:41,465 --> 00:38:44,685
Speaker 5:  It just makes it seem like a lot of this stuff is motivated by

588
00:38:44,895 --> 00:38:48,525
Speaker 5:  xenophobia. How much of a role do you think that's playing in the way that

589
00:38:48,525 --> 00:38:49,645
Speaker 5:  these conversations are happening?

590
00:38:50,405 --> 00:38:54,205
Speaker 8:  I think it plays a huge role and I think it's unserious

591
00:38:54,345 --> 00:38:58,205
Speaker 8:  to pretend that it doesn't, right? Like there are legitimate

592
00:38:58,205 --> 00:39:01,685
Speaker 8:  concerns and they can be separated from this kind of

593
00:39:01,715 --> 00:39:04,925
Speaker 8:  generalized xenophobia and anti-Chinese

594
00:39:05,095 --> 00:39:09,005
Speaker 8:  anti-Asian sentiment. But it's just foolish and ridiculous to

595
00:39:09,005 --> 00:39:12,405
Speaker 8:  pretend that that's not a huge factor that is

596
00:39:12,715 --> 00:39:16,685
Speaker 8:  animating this attack on TikTok. And let's be real that it's not

597
00:39:16,685 --> 00:39:20,125
Speaker 8:  just Republicans. There are unfortunately many

598
00:39:20,445 --> 00:39:24,405
Speaker 8:  Democrats that are happy to lean into those types of xenophobic

599
00:39:24,585 --> 00:39:28,525
Speaker 8:  and anti-Asian narratives so that they can again kind of be one-upping

600
00:39:28,525 --> 00:39:31,965
Speaker 8:  their Republican opponents on who's tougher on China.

601
00:39:32,505 --> 00:39:36,485
Speaker 8:  And look, you know, I'm an anti-authoritarian. I care a

602
00:39:36,485 --> 00:39:39,725
Speaker 8:  lot about civil liberties and civil rights. I have a lot of concerns about

603
00:39:39,725 --> 00:39:43,325
Speaker 8:  the Chinese government and their policies. Just like I have a lot of concerns

604
00:39:43,325 --> 00:39:47,205
Speaker 8:  about most other governments and their policies. So this doesn't

605
00:39:47,205 --> 00:39:50,645
Speaker 8:  mean you can't criticize the Chinese government or that you can't raise

606
00:39:50,685 --> 00:39:54,365
Speaker 8:  concerns about the Chinese government's very small stake

607
00:39:54,465 --> 00:39:58,405
Speaker 8:  in by dance, which has a stake in TikTok. But there are ways to

608
00:39:58,405 --> 00:40:01,485
Speaker 8:  do that without leaning into these xenophobic and anti-Asian

609
00:40:01,735 --> 00:40:04,605
Speaker 8:  narratives. But unfortunately that's not what lawmakers have been doing.

610
00:40:04,605 --> 00:40:08,325
Speaker 8:  They're very much pouring gasoline on those types of flames.

611
00:40:08,505 --> 00:40:11,765
Speaker 8:  And we should be honest about the fact that that has deadly consequences.

612
00:40:12,265 --> 00:40:16,245
Speaker 8:  We know there has been a massive uptick in anti-Asian hate crimes

613
00:40:16,885 --> 00:40:20,645
Speaker 8:  here in the United States over the last number of years as more and more

614
00:40:20,885 --> 00:40:24,765
Speaker 8:  lawmakers have leaned into this type of rhetoric. And we should be honest

615
00:40:24,815 --> 00:40:28,565
Speaker 8:  about the fact that folks that are kind of jumping on this bandwagon of

616
00:40:28,565 --> 00:40:32,365
Speaker 8:  let's ban TikTok because China are either knowingly

617
00:40:32,365 --> 00:40:36,285
Speaker 8:  or or unwittingly helping pour gasoline on those flames.

618
00:40:36,705 --> 00:40:40,365
Speaker 8:  And it could really have terrible consequences for some really

619
00:40:40,365 --> 00:40:42,605
Speaker 8:  vulnerable communities here in the US and around the world.

620
00:40:43,225 --> 00:40:47,205
Speaker 5:  We were talking about corporate lobbying, and I just think back to

621
00:40:47,205 --> 00:40:51,045
Speaker 5:  the story my colleague Drew Harwell and I did a year or two ago about meta

622
00:40:51,265 --> 00:40:55,165
Speaker 5:  hiring this Republican political firm to go out and smear

623
00:40:55,265 --> 00:40:58,925
Speaker 5:  TikTok in local news across the country, which they were able to do quite

624
00:40:58,925 --> 00:41:02,485
Speaker 5:  successfully. We know that they are one of the biggest lobbyists in

625
00:41:02,705 --> 00:41:06,525
Speaker 5:  DC political reported that they had outspent many other companies, most

626
00:41:06,525 --> 00:41:10,325
Speaker 5:  other tech companies in terms of lobbying dollars. What role

627
00:41:10,345 --> 00:41:13,285
Speaker 5:  is meta playing in all of this? Because they love to say, well, we haven't

628
00:41:13,645 --> 00:41:17,525
Speaker 5:  actually asked for an outright ban, but I can't imagine what else this

629
00:41:17,525 --> 00:41:18,885
Speaker 5:  type of lobbying would lead to.

630
00:41:19,765 --> 00:41:23,325
Speaker 8:  Absolutely. I mean, meta has been incredibly aggressive at

631
00:41:23,385 --> 00:41:27,085
Speaker 8:  trying to attain total monopoly status. Their

632
00:41:27,345 --> 00:41:30,805
Speaker 8:  MO has been, they either buy or try to kill off

633
00:41:31,185 --> 00:41:35,005
Speaker 8:  any competitor that comes along that poses a meaningful challenge to their

634
00:41:35,005 --> 00:41:38,525
Speaker 8:  dominance. And they have absolutely been behind some of this

635
00:41:38,805 --> 00:41:42,485
Speaker 8:  sentiment and attacks on TikTok. I just saw on TV the other day,

636
00:41:43,025 --> 00:41:46,245
Speaker 8:  the head of the Chamber of Progress, which is one of the sort of lobbying

637
00:41:46,245 --> 00:41:49,965
Speaker 8:  front groups that works for Meta and other big tech companies coming

638
00:41:50,025 --> 00:41:53,965
Speaker 8:  out in support of this bill that would ban TikTok even though they push

639
00:41:53,965 --> 00:41:57,725
Speaker 8:  back against basically every other type of regulation that

640
00:41:57,895 --> 00:42:01,605
Speaker 8:  could actually meaningfully address this issue like antitrust laws for

641
00:42:01,605 --> 00:42:05,285
Speaker 8:  example. And so again, that really just exposes the fact that

642
00:42:05,675 --> 00:42:09,125
Speaker 8:  this is not really about addressing concerns, it's about

643
00:42:09,925 --> 00:42:13,445
Speaker 8:  entrenching US dominance. And frankly, it could lead to a world where

644
00:42:13,745 --> 00:42:17,725
Speaker 8:  we have an even more concentrated tech market in the us which would

645
00:42:17,725 --> 00:42:21,685
Speaker 8:  be insane. And again, exposes some of the hypocrisy here. Joe Biden

646
00:42:21,785 --> 00:42:25,605
Speaker 8:  has been very outspoken about how there's too much power in the

647
00:42:25,605 --> 00:42:29,045
Speaker 8:  hands of these big corporations and we should break them up and you know,

648
00:42:29,045 --> 00:42:32,965
Speaker 8:  pass antitrust reforms and take on big tech. And in the meantime, he's

649
00:42:32,965 --> 00:42:36,725
Speaker 8:  basically saying he's gonna sign a bill that would kill off

650
00:42:36,745 --> 00:42:40,485
Speaker 8:  one of Facebook's biggest competitors and lead to even less

651
00:42:41,085 --> 00:42:44,725
Speaker 8:  choice and less options for us consumers who wanna find a place

652
00:42:44,905 --> 00:42:46,365
Speaker 8:  online to express themselves.

653
00:42:47,025 --> 00:42:51,005
Speaker 5:  And a huge reason why musically was forced to sell to Bite Dance in

654
00:42:51,005 --> 00:42:54,925
Speaker 5:  2017 is because it was having an increasingly difficult time going up

655
00:42:54,925 --> 00:42:58,845
Speaker 5:  against Meta and Instagram musically was being run dry

656
00:42:58,945 --> 00:43:02,685
Speaker 5:  by Instagram, who was poaching all of music, Lee's top talent, cloning a

657
00:43:02,685 --> 00:43:06,405
Speaker 5:  lot of their video features, and musically at that time

658
00:43:06,665 --> 00:43:09,605
Speaker 5:  did not have the resources to compete with Meta part. That is part of the

659
00:43:09,605 --> 00:43:13,405
Speaker 5:  reason it sold to such a big company. And you know, as the Wall Street Journal

660
00:43:13,725 --> 00:43:17,485
Speaker 5:  reported, TikTok spent a billion dollars in app download marketing just to

661
00:43:17,485 --> 00:43:20,885
Speaker 5:  sort of get that initial traction and boost after the rebranded 2018.

662
00:43:21,585 --> 00:43:25,485
Speaker 5:  So that is the level of resources that you have to have

663
00:43:25,585 --> 00:43:29,205
Speaker 5:  to go up against these tech giants. And I think it's quite telling that

664
00:43:29,785 --> 00:43:33,685
Speaker 5:  the only app that can even remotely give meta and Google a

665
00:43:33,685 --> 00:43:36,965
Speaker 5:  run for their money has to be backed by another giant

666
00:43:37,195 --> 00:43:40,245
Speaker 5:  multi-billion dollar tech conglomerate because that's the level of resources

667
00:43:40,245 --> 00:43:42,005
Speaker 5:  that you need to compete with this duopoly.

668
00:43:42,805 --> 00:43:46,735
Speaker 8:  Well, what's even funnier too is that, you know, tiktoks format of

669
00:43:46,735 --> 00:43:50,655
Speaker 8:  these short form videos is not actually brand new. That some of

670
00:43:50,655 --> 00:43:54,575
Speaker 8:  you may be old enough to remember Vine, right, which was

671
00:43:54,575 --> 00:43:58,135
Speaker 8:  basically killed off by meta when they revoked their

672
00:43:58,295 --> 00:44:02,055
Speaker 8:  API access to intentionally crush a

673
00:44:02,055 --> 00:44:06,015
Speaker 8:  competitor, right? And so we could have had a American owned

674
00:44:06,015 --> 00:44:09,895
Speaker 8:  version of TikTok as apparently lawmakers say they want,

675
00:44:10,195 --> 00:44:14,095
Speaker 8:  but because we have no meaningful anti-monopoly protections here in the

676
00:44:14,115 --> 00:44:18,015
Speaker 8:  us, we allowed meta to kill off what could have become the US version of

677
00:44:18,155 --> 00:44:21,775
Speaker 8:  TikTok. And so I think again, that just exposes that like these

678
00:44:21,775 --> 00:44:25,455
Speaker 8:  discussions are not very serious right now, that there are real

679
00:44:25,455 --> 00:44:28,655
Speaker 8:  measures we could take to address the concerns we have about the current

680
00:44:28,655 --> 00:44:32,485
Speaker 8:  social media ecosystem. That's not what's being talked about in Washington

681
00:44:32,745 --> 00:44:34,325
Speaker 8:  DC right now, unfortunately.

682
00:44:35,465 --> 00:44:39,025
Speaker 5:  I think that is such a good point about Vine and I did a lot of reporting

683
00:44:39,165 --> 00:44:42,785
Speaker 5:  on Instagram's aggressive moves to crush Vine

684
00:44:42,835 --> 00:44:46,265
Speaker 5:  Court, their talent steal all of their video features. That was wild.

685
00:44:46,805 --> 00:44:50,625
Speaker 5:  All right, Evan, well thank you so much for chatting with us today. I really

686
00:44:50,625 --> 00:44:54,305
Speaker 5:  appreciate you taking your time. Where can people follow the work that you're

687
00:44:54,305 --> 00:44:54,465
Speaker 5:  doing?

688
00:44:55,045 --> 00:44:57,985
Speaker 8:  You can follow me on Twitter, blue sky

689
00:44:59,305 --> 00:45:02,985
Speaker 8:  mastodon, Instagram, just find my name, Evan

690
00:45:03,075 --> 00:45:06,985
Speaker 8:  Greer, E-V-A-N-G-R-E-E-R, and then go to fight for The

691
00:45:06,985 --> 00:45:10,265
Speaker 8:  future dot org, which is the organization I help run. And you can learn about

692
00:45:10,325 --> 00:45:13,825
Speaker 8:  our various campaigns protecting people's basic rights in the digital age.

693
00:45:14,125 --> 00:45:15,425
Speaker 5:  Thanks Evan. Thank

694
00:45:15,425 --> 00:45:15,545
Speaker 8:  You.

695
00:45:20,055 --> 00:45:23,465
Speaker 5:  When we come back, we are gonna find out why Shrimp Jesus has taken over

696
00:45:23,745 --> 00:45:24,625
Speaker 5:  Facebook and more

697
00:47:41,105 --> 00:47:42,885
Speaker 5:  and we're gonna run through some stories from the week.

698
00:47:43,265 --> 00:47:45,525
Speaker 15:  All right, hey, let's, let's do it. I put

699
00:47:45,585 --> 00:47:49,445
Speaker 17:  100 people inside of a giant circle and whoever leaves the

700
00:47:49,445 --> 00:47:53,405
Speaker 17:  circle last wins $500,000. But If, you touch the red line,

701
00:47:54,395 --> 00:47:54,885
Speaker 17:  it's game

702
00:47:54,885 --> 00:47:58,485
Speaker 5:  Over. I wrote a story about this this week, but Mr. Beast struck a deal with

703
00:47:58,485 --> 00:48:02,045
Speaker 5:  Amazon MGM to produce a massive reality TV competition show.

704
00:48:02,595 --> 00:48:06,485
Speaker 5:  This is pretty big news because it's Jimmy Donaldson's first foray into

705
00:48:06,485 --> 00:48:10,205
Speaker 5:  traditional entertainment. The show is gonna be called Beast Games and it'll

706
00:48:10,205 --> 00:48:14,045
Speaker 5:  feature a thousand contestants competing for a $5 million cash payout,

707
00:48:14,095 --> 00:48:17,325
Speaker 5:  which is actually the biggest prize offered in TV history.

708
00:48:18,065 --> 00:48:21,645
Speaker 5:  The Hollywood reporter said that Amazon outbid at least one other major streaming

709
00:48:21,645 --> 00:48:25,325
Speaker 5:  service to land this deal. Mr. Beast has become the most subscribed to

710
00:48:25,325 --> 00:48:28,765
Speaker 5:  individual on YouTube in recent years, and he has a massive audience of children.

711
00:48:29,395 --> 00:48:32,765
Speaker 5:  He's really known for these outrageous stunts like burying himself alive

712
00:48:33,065 --> 00:48:37,045
Speaker 5:  or recreating the show Squid Game as this reality TV style competition.

713
00:48:37,605 --> 00:48:40,085
Speaker 5:  I think this is a really notable moment for the entertainment industry because

714
00:48:40,085 --> 00:48:42,885
Speaker 5:  it shows how these streaming platforms are willing to pay a lot of money

715
00:48:43,025 --> 00:48:47,005
Speaker 5:  to court these big online content creators in an effort to absorb some

716
00:48:47,005 --> 00:48:50,645
Speaker 5:  of their audience. I think what remains to be seen is whether Jimmy

717
00:48:50,645 --> 00:48:54,205
Speaker 5:  Donaldson's audience will actually hop over and watch this show on Amazon.

718
00:48:54,875 --> 00:48:58,405
Speaker 5:  There's been a lot of other efforts to take internet talent and move them

719
00:48:58,405 --> 00:49:01,925
Speaker 5:  into more traditional forms of entertainment, mostly linear TV or

720
00:49:02,085 --> 00:49:05,365
Speaker 5:  traditional movies. And that has always flopped. He set his channel between

721
00:49:05,365 --> 00:49:08,685
Speaker 5:  makes between 600 million and $700 million a year, and he

722
00:49:08,965 --> 00:49:12,845
Speaker 5:  reinvests most of that back into his stunts, building

723
00:49:13,145 --> 00:49:16,645
Speaker 5:  his channel and his media empire even more. So, who knows? I doubt that Amazon

724
00:49:16,645 --> 00:49:19,965
Speaker 5:  will give him exactly the same production budget as he usually has, but maybe,

725
00:49:20,265 --> 00:49:23,005
Speaker 5:  you know, they're, they're investing quite a lot in him and I'm sure they

726
00:49:23,005 --> 00:49:24,085
Speaker 5:  want the show to be successful.

727
00:49:24,825 --> 00:49:28,205
Speaker 15:  Amazon famously spends a lot of money on their shows

728
00:49:28,785 --> 00:49:32,645
Speaker 15:  and really does a poor job of promoting them. I

729
00:49:32,645 --> 00:49:36,205
Speaker 15:  haven't seen them with any like breakout reality or game shows.

730
00:49:36,645 --> 00:49:40,245
Speaker 15:  A lot of their scripted stuff has, has just not done that well. They've,

731
00:49:40,245 --> 00:49:44,115
Speaker 15:  they spent like a billion dollars on Lord of the Rings and it just

732
00:49:44,115 --> 00:49:47,355
Speaker 15:  didn't really make a, make a splash. So I think a lot of stuff tends to

733
00:49:47,355 --> 00:49:51,195
Speaker 15:  get buried on Amazon. Netflix has had quite a bit of success with

734
00:49:51,195 --> 00:49:53,835
Speaker 15:  reality shows, game shows, stuff like that, that that seems like it probably

735
00:49:53,835 --> 00:49:57,595
Speaker 15:  would've been a better fit for him audience wise. I'm sure Amazon

736
00:49:57,735 --> 00:50:01,675
Speaker 15:  outbid whoever they were up against. But you know, the thing

737
00:50:01,675 --> 00:50:05,235
Speaker 15:  that happens over and over again is when your audience is in one place,

738
00:50:05,385 --> 00:50:07,395
Speaker 15:  it's really hard to get them to another place.

739
00:50:07,705 --> 00:50:11,355
Speaker 5:  Well, he's also not stopping production on YouTube, right? So he's gonna

740
00:50:11,515 --> 00:50:15,435
Speaker 5:  continue to post on YouTube throughout this deal. And

741
00:50:15,435 --> 00:50:19,275
Speaker 5:  so yeah, it just remains to be seen whether the audience will hop over

742
00:50:19,275 --> 00:50:23,065
Speaker 5:  and also watch on Amazon. That said, I mean, other, other

743
00:50:23,265 --> 00:50:26,785
Speaker 5:  children's shows have had success. Coco Mellon jumped over.

744
00:50:27,035 --> 00:50:29,885
Speaker 5:  It's more of a sort of cartoon children's

745
00:50:30,785 --> 00:50:34,485
Speaker 5:  series and Mr. Beast audience is also children. So, you know, If, you can

746
00:50:34,485 --> 00:50:38,445
Speaker 5:  plop your child down in front of the TV and turn on the Mr. Beast game show

747
00:50:39,065 --> 00:50:41,445
Speaker 5:  for an hour and distract them. I, you know, maybe,

748
00:50:41,825 --> 00:50:45,725
Speaker 15:  And I, I do feel like every parent I know has Amazon, like If. you have

749
00:50:45,725 --> 00:50:49,485
Speaker 15:  kids, you probably have an Amazon subscription, so it,

750
00:50:49,715 --> 00:50:51,765
Speaker 15:  it's, it's all right there for you potentially.

751
00:50:53,105 --> 00:50:56,445
Speaker 5:  If you've been on Facebook at all recently, you might have noticed that more

752
00:50:56,445 --> 00:51:00,285
Speaker 5:  and more AI generated images are creeping into your feed. These images can

753
00:51:00,285 --> 00:51:03,605
Speaker 5:  be strange, bizarre sometimes they're very beautiful and they're usually

754
00:51:03,765 --> 00:51:07,645
Speaker 5:  posted by anonymously run pages. But since last week, one

755
00:51:07,885 --> 00:51:11,645
Speaker 5:  specific type of image has become pervasive and that is AI generated

756
00:51:11,645 --> 00:51:15,605
Speaker 5:  portrayals of Jesus Christ made out of live shrimp. These pictures

757
00:51:15,625 --> 00:51:19,085
Speaker 5:  are really crazy and slightly grotesque, and they've been generating a massive

758
00:51:19,085 --> 00:51:22,685
Speaker 5:  amount of likes and shares every single time they're posted shrimp. Jesus

759
00:51:22,985 --> 00:51:26,925
Speaker 5:  is actually just the latest engagement hack by these spammy types of pages,

760
00:51:27,255 --> 00:51:30,805
Speaker 5:  doing what's called engagement farming. Basically they're posting something

761
00:51:30,865 --> 00:51:34,645
Speaker 5:  so bad and crazy and absurd that it makes people stop, comment, or

762
00:51:34,645 --> 00:51:38,605
Speaker 5:  share. This then pushes that page's reach up and allows them

763
00:51:38,605 --> 00:51:42,525
Speaker 5:  to get other content more easily into your feed. After they

764
00:51:42,525 --> 00:51:45,805
Speaker 5:  boost their reach with shrimp Jesus, they start pushing other low quality

765
00:51:45,805 --> 00:51:49,725
Speaker 5:  content into your feed, like fake news websites full of Google ads, or

766
00:51:49,725 --> 00:51:53,605
Speaker 5:  they try to sell you low quality products. This is how they effectively monetize.

767
00:51:53,745 --> 00:51:57,605
Speaker 5:  So when you see these really weird AI generated images all over, you might

768
00:51:57,605 --> 00:52:01,005
Speaker 5:  just think that they're fun or weird or being shared all in good fun.

769
00:52:01,435 --> 00:52:05,125
Speaker 5:  This is all actually part of a big moneymaking scheme that's increasingly

770
00:52:05,125 --> 00:52:06,205
Speaker 5:  common on Facebook.

771
00:52:07,245 --> 00:52:10,525
Speaker 15:  Honestly, I get the rubber necker approach to this. Like, you're, you're

772
00:52:10,605 --> 00:52:14,125
Speaker 15:  cruising through Facebook and then you just halt and you, you share or you

773
00:52:14,125 --> 00:52:17,925
Speaker 15:  comment. What the fuck is going on? Seeing Jesus as a shrimp, is

774
00:52:18,425 --> 00:52:21,005
Speaker 15:  it, it is uncomfortable to look at this image.

775
00:52:21,595 --> 00:52:25,445
Speaker 5:  Yeah, it's quite terrifying. I think scammers are always ahead of the game

776
00:52:25,445 --> 00:52:29,165
Speaker 5:  when it comes to what performs online. I've seen a lot of other AI

777
00:52:29,165 --> 00:52:32,405
Speaker 5:  character driven pages as well on Facebook, which are essentially

778
00:52:32,795 --> 00:52:36,765
Speaker 5:  anonymously run pages that purport to be a specific person. I saw

779
00:52:36,905 --> 00:52:40,365
Speaker 5:  one woman saying, I'm 120 1-year-old baker

780
00:52:40,705 --> 00:52:44,405
Speaker 5:  and I love to bake these peachy confections, and it's just

781
00:52:44,705 --> 00:52:48,165
Speaker 5:  AI generated images of this old woman

782
00:52:48,505 --> 00:52:52,365
Speaker 5:  and various types of cakes. Yeah. And this whole Facebook page

783
00:52:52,425 --> 00:52:55,645
Speaker 5:  is ostensibly supposed to be run by this

784
00:52:55,785 --> 00:52:59,005
Speaker 5:  120 1-year-old woman, which I think would make her the oldest woman in the

785
00:52:59,005 --> 00:53:02,925
Speaker 5:  world. Yeah, and it's, it looks so obviously fake, but the comments

786
00:53:03,025 --> 00:53:05,845
Speaker 5:  on the post make it seem like, I don't even think people really care.

787
00:53:07,345 --> 00:53:10,805
Speaker 5:  I'm currently reporting on a similar phenomenon on Instagram, so my story

788
00:53:10,805 --> 00:53:13,685
Speaker 5:  will be out soon. But I really do think that we're just seeing more and more

789
00:53:13,685 --> 00:53:16,405
Speaker 5:  of this AI engagement bait all over the internet.

790
00:53:16,675 --> 00:53:17,725
Speaker 15:  Yeah. Boomer bait

791
00:53:19,115 --> 00:53:22,525
Speaker 5:  Also happening on Facebook. Meta redesigned aspects of the app to

792
00:53:22,525 --> 00:53:26,285
Speaker 5:  emphasize the poke button, again, for people that don't remember Poke was

793
00:53:26,285 --> 00:53:29,085
Speaker 5:  this really popular feature back in the aughts where you could really just

794
00:53:29,085 --> 00:53:31,925
Speaker 5:  send little pokes to people, which just, you'd just get a little message

795
00:53:31,945 --> 00:53:35,685
Speaker 5:  saying, I poked you. Or, so-and-so poked you. This redesign

796
00:53:35,925 --> 00:53:39,845
Speaker 5:  apparently works 'cause there's been a 13 x jump in use of the feature. I

797
00:53:39,845 --> 00:53:43,725
Speaker 5:  think this is just Facebook trying to bait people into engaging on the

798
00:53:43,725 --> 00:53:47,405
Speaker 5:  app in any way that they can. I think there's tons of nostalgia

799
00:53:47,585 --> 00:53:51,525
Speaker 5:  for the aughts, especially among millennials. Nostalgia stuff plays really

800
00:53:51,525 --> 00:53:55,445
Speaker 5:  well. This is a feature that a lot of millennials associate with their

801
00:53:55,445 --> 00:53:59,045
Speaker 5:  youth back when we had Facebook in college and would poke each other and

802
00:53:59,045 --> 00:54:02,605
Speaker 5:  things in our dorm, and so I think this is kind of a, a naked

803
00:54:02,915 --> 00:54:06,765
Speaker 5:  play for engagement. Zuckerberg posted after they announced the uptick

804
00:54:06,765 --> 00:54:08,925
Speaker 5:  in the feature that quote, nature is healing.

805
00:54:09,645 --> 00:54:13,605
Speaker 15:  I don't know why we need this. I, I don't use Facebook anymore, so

806
00:54:13,755 --> 00:54:17,365
Speaker 15:  it's not for me, but I don't know why we need this. And it, it just feels

807
00:54:17,365 --> 00:54:21,085
Speaker 15:  like an excuse for more notifications and I'm just like

808
00:54:21,085 --> 00:54:23,725
Speaker 15:  pretty Anti notification at these at this point.

809
00:54:23,905 --> 00:54:25,805
Speaker 5:  Me too. I have all my notifications turned off.

810
00:54:26,235 --> 00:54:26,525
Speaker 15:  Same

811
00:54:28,245 --> 00:54:31,845
Speaker 5:  LinkedIn, which recently surpassed a billion users and is primarily known

812
00:54:31,845 --> 00:54:35,205
Speaker 5:  as a place to find your next job, is now trying to court gamers in an effort

813
00:54:35,205 --> 00:54:39,165
Speaker 5:  to boost time on the platform. They're rolling out in-app games. The company

814
00:54:39,265 --> 00:54:43,125
Speaker 5:  is testing three puzzle type games so far, and I think this

815
00:54:43,125 --> 00:54:46,245
Speaker 5:  move is really smart because it taps into the puzzle mania that helped other

816
00:54:46,485 --> 00:54:49,725
Speaker 5:  platforms like the New York Times Boost engagement with things like Wordle

817
00:54:49,725 --> 00:54:53,525
Speaker 5:  and Connections. I know people that play every day. Facebook also launched

818
00:54:53,525 --> 00:54:57,085
Speaker 5:  Farmville back in 2009, and time spent on the platform skyrocketed.

819
00:54:57,565 --> 00:55:00,845
Speaker 5:  I think a lot of these social media companies have integrated gaming aspects

820
00:55:00,845 --> 00:55:04,765
Speaker 5:  over the years with quite positive results. It's also worth noting

821
00:55:04,765 --> 00:55:07,925
Speaker 5:  that the company is owned by Microsoft, a gaming giant that made over

822
00:55:07,925 --> 00:55:11,885
Speaker 5:  $7 billion last quarter in gaming alone. So this just

823
00:55:11,885 --> 00:55:15,245
Speaker 5:  seems like smart synergy and if people can spend a little bit more time on

824
00:55:15,925 --> 00:55:19,125
Speaker 5:  LinkedIn playing Word games or whatever, that's good for the company.

825
00:55:19,955 --> 00:55:23,765
Speaker 15:  Yeah. I totally get why LinkedIn would wanna do this

826
00:55:23,785 --> 00:55:27,285
Speaker 15:  and why it makes business sense, but if you're spending a ton of time

827
00:55:28,065 --> 00:55:31,445
Speaker 15:  on LinkedIn playing games, I don't think you're using LinkedIn correctly.

828
00:55:31,505 --> 00:55:35,005
Speaker 15:  You're probably not finding that new job you're after, you know?

829
00:55:35,085 --> 00:55:38,165
Speaker 5:  But that's not really what LinkedIn is anymore. I mean, LinkedIn is a pretty

830
00:55:38,265 --> 00:55:42,045
Speaker 5:  robust social network. People spend a lot of time on their

831
00:55:42,405 --> 00:55:45,845
Speaker 5:  learning and yeah, talking with people and sharing news.

832
00:55:46,135 --> 00:55:50,125
Speaker 5:  Since Twitter declined, LinkedIn has become a huge hub for news and

833
00:55:50,125 --> 00:55:53,245
Speaker 5:  information. People go there in the morning, you check in on your LinkedIn,

834
00:55:53,345 --> 00:55:57,325
Speaker 5:  see what your connections are up to. It's not only looking for work, obviously

835
00:55:57,325 --> 00:56:00,925
Speaker 5:  that's sort of was the initial point of the platform, but I do think

836
00:56:01,075 --> 00:56:04,885
Speaker 5:  it's sort of just this robust professional networking platform

837
00:56:04,945 --> 00:56:05,165
Speaker 5:  now.

838
00:56:05,835 --> 00:56:09,805
Speaker 15:  Yeah, I, I've had LinkedIn probably 12 years. I don't think I've

839
00:56:09,805 --> 00:56:13,685
Speaker 15:  ever had a shred of fun on that platform, and I think

840
00:56:13,685 --> 00:56:17,525
Speaker 15:  it's useful. I think it's like super useful and yeah, I wouldn't

841
00:56:17,525 --> 00:56:18,285
Speaker 15:  associate it with games.

842
00:56:18,645 --> 00:56:22,445
Speaker 5:  I mean, I think it's smart because it keeps people engaged. Yeah. You might

843
00:56:22,445 --> 00:56:26,365
Speaker 5:  be on LinkedIn, networking, chatting, sending some

844
00:56:26,365 --> 00:56:30,325
Speaker 5:  messages, waiting for some messages to come through and wanna play a

845
00:56:30,325 --> 00:56:33,805
Speaker 5:  crossword or puzzle or something, you know? Yeah. I think it makes sense.

846
00:56:33,945 --> 00:56:37,925
Speaker 5:  As you mentioned, it's not always the most entertaining platform. Microsoft

847
00:56:38,225 --> 00:56:41,645
Speaker 5:  has a lot of staking in gaming and this makes sense.

848
00:56:42,155 --> 00:56:45,645
Speaker 15:  Yeah. This reminds me of a tweet I saw that was like the New York Times.

849
00:56:46,145 --> 00:56:50,045
Speaker 15:  You mean that place where I play my little games? It's like maybe that's

850
00:56:50,205 --> 00:56:50,685
Speaker 15:  LinkedIn's future.

851
00:56:54,715 --> 00:56:57,925
Speaker 5:  Alright, that's the show. If. you like this episode, give us a rating and

852
00:56:57,925 --> 00:57:01,885
Speaker 5:  review on Apple Podcast, Spotify, or wherever you listen. Power User

853
00:57:01,905 --> 00:57:05,605
Speaker 5:  is produced by Travis Uch and Jelani Carter. We were mixed and mastered by

854
00:57:05,605 --> 00:57:09,485
Speaker 5:  Brendan McFarland. Our video producer is Brandon Keefer. You can watch full

855
00:57:09,485 --> 00:57:13,125
Speaker 5:  episodes on my YouTube channel at Taylor Lorenz. Our executive producers

856
00:57:13,125 --> 00:57:16,925
Speaker 5:  are Zach Mack and Nisha Kwa. Power User is part of the Vox Media podcast

857
00:57:16,925 --> 00:57:19,765
Speaker 5:  network. We'll be back next week. See you then.

