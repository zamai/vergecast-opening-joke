1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 0d21b5e0-cb62-11ed-94ed-e54c3e89f397
Status: Done
Stage: Done
Title: TikTok goes to Washington
Audio URL: https://jfe93e.s3.amazonaws.com/-5695046867528753068/-432767900213348650/s93290-US-5029s-1679785743.mp3
Description: The Verge's Nilay Patel, Alex Cranz, and David Pierce are joined by policy reporter Makena Kelly, who is on the ground in Washington for the House Energy and Commerce Committee's hearing on TikTok. Later, we dive into all the other news from this week, from Google's release of Bard to OpenAI's rapid expansion of ChatGPT. It was a big week.

2
00:00:00,000 --> 00:00:03,760
Speaker 1:  Support for today's show comes from Deloitte. In the business world, it can

3
00:00:03,760 --> 00:00:07,160
Speaker 1:  be especially crucial to innovate. You can either build your own future or

4
00:00:07,160 --> 00:00:10,440
Speaker 1:  bet on someone else's. No one knows what tomorrow will bring, but you push

5
00:00:10,440 --> 00:00:14,160
Speaker 1:  forward and create enterprise anyway. That's why Deloitte's mission is to

6
00:00:14,160 --> 00:00:17,320
Speaker 1:  help engineer advantage for their clients by harnessing the latest innovations

7
00:00:17,320 --> 00:00:21,040
Speaker 1:  and technology while exploring the ideas and opportunities that can look

8
00:00:21,040 --> 00:00:24,320
Speaker 1:  beyond today. Transform what's next into what's now.

9
00:00:24,700 --> 00:00:27,440
Speaker 1:  See how you can engineer advantage with Deloitte at

10
00:00:27,680 --> 00:00:31,200
Speaker 1:  deloitte.com/us/engineering advantage.

11
00:00:32,490 --> 00:00:36,200
Speaker 3:  Support for this podcast comes from Slack and they're looking to help make

12
00:00:36,200 --> 00:00:39,880
Speaker 3:  you and your team work better. Slack is a productivity platform that

13
00:00:39,880 --> 00:00:43,480
Speaker 3:  connects all your team members together instantly. It's built to help your

14
00:00:43,480 --> 00:00:47,040
Speaker 3:  team with a host of features like huddles for quick check-ins and clips for

15
00:00:47,040 --> 00:00:50,920
Speaker 3:  recording and sharing video. Slack also makes it easy to search and find

16
00:00:50,920 --> 00:00:54,840
Speaker 3:  the right information you need. You can even integrate the apps you use in

17
00:00:54,840 --> 00:00:58,440
Speaker 3:  your normal workflow, like your calendar or project management tools. So

18
00:00:58,440 --> 00:01:02,160
Speaker 3:  stay connected on the work that matters and get more done. Learn more

19
00:01:02,160 --> 00:01:04,280
Speaker 3:  at slack.com/productivity.

20
00:01:17,130 --> 00:01:17,690
Speaker 2:  Hello

21
00:01:17,690 --> 00:01:21,530
Speaker 5:  And welcome to the RO Cast, the flagship podcast to protecting America's

22
00:01:21,810 --> 00:01:22,730
Speaker 5:  interests at home and abroad.

23
00:01:23,200 --> 00:01:26,730
Speaker 1:  I love it. Reporting to you live from the VER studio.

24
00:01:26,880 --> 00:01:29,810
Speaker 5:  This is a new, if you're watching us on YouTube, this is a new setup for

25
00:01:29,810 --> 00:01:30,650
Speaker 5:  me and Alex in the studio.

26
00:01:30,650 --> 00:01:31,970
Speaker 1:  We're having a great time here.

27
00:01:32,080 --> 00:01:35,410
Speaker 5:  It's a very exciting day for us. Anyway, I'm your friend Eli, Alex Trans

28
00:01:35,410 --> 00:01:35,730
Speaker 5:  is here.

29
00:01:35,790 --> 00:01:39,210
Speaker 1:  I'm your friend who is just your friend. I'm just here to hang out. Have

30
00:01:39,210 --> 00:01:40,250
Speaker 1:  a good time. What's happened today?

31
00:01:40,700 --> 00:01:43,210
Speaker 5:  There's a lot. Something happened. David is here. Hey David.

32
00:01:43,500 --> 00:01:46,450
Speaker 6:  Hi. I'm still in my basement. They don't let me out. They fix the studio.

33
00:01:46,640 --> 00:01:47,450
Speaker 6:  I stay the same.

34
00:01:48,110 --> 00:01:51,410
Speaker 5:  And then very importantly, McKenna Kelly is here today. Hey

35
00:01:51,410 --> 00:01:53,170
Speaker 7:  McKenna. Hey everyone. It's good to be here.

36
00:01:53,380 --> 00:01:57,210
Speaker 5:  So a big day in our nation's history. Yes, McKenna

37
00:01:57,210 --> 00:02:00,450
Speaker 5:  attended a hearing today in the House of Representatives

38
00:02:00,770 --> 00:02:04,570
Speaker 5:  where, I don't know how else described it. They just yelled at TikTok,

39
00:02:04,600 --> 00:02:08,010
Speaker 5:  just in generally yelled about TikTok for four and a half hours

40
00:02:08,620 --> 00:02:12,370
Speaker 5:  at the CEO of TikTok. It was maybe the baddest faith

41
00:02:12,370 --> 00:02:15,930
Speaker 5:  of the bad faith tech hearings that we've heard. And then there's a lot of

42
00:02:15,930 --> 00:02:19,650
Speaker 5:  TikTok ERs in dc. McKenna, you attended a press

43
00:02:19,650 --> 00:02:23,090
Speaker 5:  conference with the TikTok ERs say, which I want to hear all about. But the

44
00:02:23,690 --> 00:02:27,330
Speaker 5:  momentum around banning TikTok is as high as I think it's ever been.

45
00:02:27,510 --> 00:02:31,290
Speaker 5:  And I came away from today thinking, oh, this is gonna happen.

46
00:02:31,290 --> 00:02:33,090
Speaker 5:  But you're there McKenna. What? What went down?

47
00:02:33,370 --> 00:02:36,930
Speaker 7:  Right? So as you mentioned, today was a hearing at the House Energy and Commerce

48
00:02:36,930 --> 00:02:40,690
Speaker 7:  Committee. They brought in the C e O of TikTok who

49
00:02:41,000 --> 00:02:43,890
Speaker 7:  provided testimony when he was allowed to.

50
00:02:45,030 --> 00:02:48,850
Speaker 7:  The lawmakers spoke over him a lot. It seemed like he was there

51
00:02:48,850 --> 00:02:52,730
Speaker 7:  to give fairly candid testimony and to say what exactly, you

52
00:02:52,730 --> 00:02:55,570
Speaker 7:  know, he could say there were some, I don't know, private company questions

53
00:02:55,570 --> 00:02:58,850
Speaker 7:  that he would wanted to skirt by, but for the most part he seemed like he

54
00:02:58,850 --> 00:03:02,640
Speaker 7:  was, was there to really answer the questions that lawmakers had.

55
00:03:02,640 --> 00:03:06,600
Speaker 7:  But unfortunately, I don't think the lawmakers really had any

56
00:03:07,040 --> 00:03:11,000
Speaker 7:  questions at the top of the hearing. Something that really stayed with

57
00:03:11,000 --> 00:03:14,680
Speaker 7:  me was that the chairwoman Kathy McMorris Rodgers, she,

58
00:03:15,050 --> 00:03:18,960
Speaker 7:  in her opening statement, said, your app should be banned. Yeah.

59
00:03:19,800 --> 00:03:23,200
Speaker 7:  It seemed like a lot of the lawmakers there had already had, you know, their

60
00:03:23,200 --> 00:03:27,040
Speaker 7:  minds made up about the future of TikTok and why they invited him to answer

61
00:03:27,240 --> 00:03:30,200
Speaker 7:  questions. I don't know. Cuz it doesn't seem like they were very interested

62
00:03:30,200 --> 00:03:30,800
Speaker 7:  in the answers.

63
00:03:31,070 --> 00:03:34,640
Speaker 6:  Yeah. They said over and over, this is the most bipartisan

64
00:03:34,640 --> 00:03:38,560
Speaker 6:  committee in Congress. Everybody agrees we're all on the same page. Everybody

65
00:03:38,560 --> 00:03:41,320
Speaker 6:  thinks we should be on TikTok. And it was just, it, it became clear very

66
00:03:41,320 --> 00:03:44,920
Speaker 6:  quickly that they had just brought him here to yell at him. Yeah. And to

67
00:03:44,920 --> 00:03:48,320
Speaker 6:  not only yell at him about TikTok, but to yell at him about every bad thing

68
00:03:48,320 --> 00:03:51,920
Speaker 6:  that anyone in the tech industry has ever done in history. And

69
00:03:52,170 --> 00:03:56,160
Speaker 6:  he kept saying, can I respond to that? And they kept being like, no. I was

70
00:03:56,160 --> 00:03:57,120
Speaker 6:  like, what are we doing here?

71
00:03:57,840 --> 00:04:01,400
Speaker 5:  And, and what, I wanna put that into context. We've done a lot of hearing

72
00:04:01,800 --> 00:04:05,520
Speaker 5:  coverage in the past couple years. We've seen Mark Zuckerberg go through

73
00:04:05,520 --> 00:04:09,360
Speaker 5:  this experience. Jack Dorsey literally tweeting through a hearing about

74
00:04:09,360 --> 00:04:12,920
Speaker 5:  how bad the questions were while he was in the hearing. Power move. Yeah.

75
00:04:13,190 --> 00:04:16,720
Speaker 5:  Sundar, you name it. The, the CEOs of big tech companies

76
00:04:17,030 --> 00:04:20,240
Speaker 5:  have been forced to endure this

77
00:04:20,480 --> 00:04:24,240
Speaker 5:  spectacle many times. This one was

78
00:04:24,240 --> 00:04:27,760
Speaker 5:  meaningfully different. And I would say, I was listening to

79
00:04:27,830 --> 00:04:31,680
Speaker 5:  CNBC at my way to work today before the hearing, and Mc Morris Rogers was

80
00:04:31,680 --> 00:04:35,640
Speaker 5:  on cnbc and before the hearing had ever started, she said, at the end

81
00:04:35,640 --> 00:04:39,560
Speaker 5:  of this hearing, we're gonna ban TikTok. Right.

82
00:04:39,560 --> 00:04:43,000
Speaker 5:  Just like straight out the hearing hadn't even happened. The collection of

83
00:04:43,200 --> 00:04:47,160
Speaker 5:  evidence that presumably this was meant to accomplish. Yeah.

84
00:04:47,490 --> 00:04:51,360
Speaker 5:  Nothing. Now I I, there are legitimate

85
00:04:51,360 --> 00:04:55,120
Speaker 5:  concerns about TikTok McKenna, just from

86
00:04:55,120 --> 00:04:58,560
Speaker 5:  taking a step back before we get into the weirdness of the hearing, what,

87
00:04:58,710 --> 00:05:02,320
Speaker 5:  what are the specific concerns that the government has about TikTok?

88
00:05:02,320 --> 00:05:05,760
Speaker 7:  Right. From what I can gather from my conversations with lawmakers, there

89
00:05:05,760 --> 00:05:09,600
Speaker 7:  are two main concerns. One is for foreign malign influence

90
00:05:09,600 --> 00:05:13,520
Speaker 7:  operations, whether that's China, Russia, whoever

91
00:05:13,520 --> 00:05:17,240
Speaker 7:  using TikTok to basically convince people that

92
00:05:17,240 --> 00:05:20,920
Speaker 7:  China, like Taiwan belongs to China, China is going to

93
00:05:20,920 --> 00:05:24,680
Speaker 7:  invade. Everyone clap. That's one instance.

94
00:05:24,860 --> 00:05:28,200
Speaker 7:  And then the second instance is Chinese

95
00:05:28,270 --> 00:05:32,200
Speaker 7:  spying on American citizens, essentially using the data obtained from

96
00:05:32,200 --> 00:05:35,000
Speaker 7:  TikTok in order to do something

97
00:05:36,250 --> 00:05:39,400
Speaker 7:  in when it's gone to the Chinese government. That's the one thing when I'm

98
00:05:39,400 --> 00:05:43,320
Speaker 7:  asking lawmakers, what does it mean for the average person? If you take

99
00:05:43,450 --> 00:05:47,400
Speaker 7:  my mom's data from TikTok, why should she be scared?

100
00:05:47,620 --> 00:05:51,240
Speaker 7:  And that's been something really, really hard to get an answer to. And I

101
00:05:51,240 --> 00:05:55,000
Speaker 7:  think this was fully on display at today's hearing. We haven't had any evidence

102
00:05:55,150 --> 00:05:58,880
Speaker 7:  that China is currently operating some kind of form aligned

103
00:05:58,880 --> 00:06:02,040
Speaker 7:  influence campaign on TikTok. There's a possibility that it could happen,

104
00:06:02,100 --> 00:06:05,400
Speaker 7:  but it hasn't been found yet. And then in the case for data security,

105
00:06:06,120 --> 00:06:08,760
Speaker 7:  there really hasn't been an instance. There's been a number of instances

106
00:06:08,760 --> 00:06:12,360
Speaker 7:  where a handful of employees

107
00:06:12,880 --> 00:06:16,800
Speaker 7:  at TikTok have either track journalists, but again, like there were

108
00:06:16,800 --> 00:06:20,760
Speaker 7:  accusations that Uber did that as well. Accusations that employees at Facebook

109
00:06:20,760 --> 00:06:23,880
Speaker 7:  had done that against journalists as well. But getting to the heart of it,

110
00:06:23,880 --> 00:06:27,840
Speaker 7:  there isn't really anything so far that differ, differentiates, talk's,

111
00:06:27,960 --> 00:06:31,800
Speaker 7:  business model talk's, ability to gather data that's

112
00:06:31,800 --> 00:06:34,840
Speaker 7:  different from American-based tech companies and the fact that they have

113
00:06:34,840 --> 00:06:38,320
Speaker 7:  a Chinese Beijing based parent company and bite dance.

114
00:06:38,710 --> 00:06:42,480
Speaker 6:  I I would just add one sort of half corollary to

115
00:06:42,480 --> 00:06:46,040
Speaker 6:  the two things that you named, cuz I do think one other piece of this seems

116
00:06:46,040 --> 00:06:50,000
Speaker 6:  to be, I guess what I would call this sort of long subtle influence

117
00:06:50,360 --> 00:06:52,840
Speaker 6:  campaign that people think, and this came up a bunch in the hearing and I,

118
00:06:52,840 --> 00:06:56,520
Speaker 6:  I'm curious what y'all thought of it, but this idea that basically

119
00:06:56,630 --> 00:07:00,520
Speaker 6:  TikTok is sort of corrupting the youth and teaching

120
00:07:00,800 --> 00:07:04,520
Speaker 6:  everyone to just do dance challenges while Dewian in China is teaching kids

121
00:07:04,880 --> 00:07:08,120
Speaker 6:  engineering. Like this is a real concern. And it came up over and over th

122
00:07:08,120 --> 00:07:11,320
Speaker 6:  this idea that this is China's way of influencing

123
00:07:12,160 --> 00:07:15,440
Speaker 6:  American people. Not only for the the, like the Taiwan stuff that you're

124
00:07:15,440 --> 00:07:19,240
Speaker 6:  talking, but it's this like long game play to just

125
00:07:19,240 --> 00:07:22,920
Speaker 6:  win future generations. And, and there there's this 10 dimensional

126
00:07:22,920 --> 00:07:26,680
Speaker 6:  chess thing happening that a surprising number of people seem to think is

127
00:07:26,960 --> 00:07:27,720
Speaker 6:  actually real.

128
00:07:28,510 --> 00:07:31,640
Speaker 7:  I think that's important too. So yesterday I sat down with Senator Mark Warner,

129
00:07:31,700 --> 00:07:35,320
Speaker 7:  who has introduced one of the most widely backed bipartisan

130
00:07:35,320 --> 00:07:38,840
Speaker 7:  bills that could eventually lead to a ban of TikTok. And when I was sitting

131
00:07:38,840 --> 00:07:42,640
Speaker 7:  with him, a number that he constantly pulled out was the fact that the, you

132
00:07:42,640 --> 00:07:46,480
Speaker 7:  know, there's about 150 million American users on average. Like the

133
00:07:46,480 --> 00:07:50,280
Speaker 7:  average user spends 90 minutes on TikTok a day that is an hour

134
00:07:50,280 --> 00:07:54,120
Speaker 7:  and a half of content from one platform. And he kept,

135
00:07:54,120 --> 00:07:57,960
Speaker 7:  you know, reaffirming to me the fact that they could have those

136
00:07:57,960 --> 00:08:01,880
Speaker 7:  subtle nudges, right? That could push you into believing one thing or

137
00:08:01,880 --> 00:08:05,840
Speaker 7:  thinking one thing that isn't so, you know, outlandish

138
00:08:05,850 --> 00:08:09,640
Speaker 7:  as having, I don't know, an influencer from the People's Republic

139
00:08:09,640 --> 00:08:13,400
Speaker 7:  of China being like, everything is so good. We're not

140
00:08:13,400 --> 00:08:17,320
Speaker 7:  doing a uighur genocide. You know, things like that. So

141
00:08:17,710 --> 00:08:21,680
Speaker 7:  it's those, it's those small things of course that are very

142
00:08:21,680 --> 00:08:24,680
Speaker 7:  hard to prove it. Of course, this, you know, this is something where time

143
00:08:24,680 --> 00:08:25,080
Speaker 7:  will tell,

144
00:08:25,080 --> 00:08:28,880
Speaker 5:  Wait, can I just say about that? It's hard to prove in the, the case

145
00:08:28,880 --> 00:08:32,200
Speaker 5:  of TikTok, it is actually easy to prove

146
00:08:32,410 --> 00:08:36,400
Speaker 5:  broadly. So like Kyrie

147
00:08:36,400 --> 00:08:40,040
Speaker 5:  Irving is a famous basketball player who believes the earth is flat because

148
00:08:40,040 --> 00:08:43,840
Speaker 5:  of the Google SEO feedback loop, right? Like

149
00:08:43,840 --> 00:08:47,080
Speaker 5:  there were some people who were like, the earth is flat. And that was engaging

150
00:08:47,080 --> 00:08:50,880
Speaker 5:  and it got a lot of comments on YouTube. And so more people saw those

151
00:08:50,880 --> 00:08:54,280
Speaker 5:  view counts going up and made more videos. And then real

152
00:08:54,280 --> 00:08:57,840
Speaker 5:  publications saw the search trends for the earth is flat

153
00:08:57,900 --> 00:09:01,240
Speaker 5:  and they wrote more content about the controversy about the earth being flat

154
00:09:01,240 --> 00:09:05,080
Speaker 5:  and you just kind of spiral it out to no one was in charge

155
00:09:05,080 --> 00:09:08,920
Speaker 5:  of this feedback loop and now a substantial percentage

156
00:09:08,920 --> 00:09:12,400
Speaker 5:  of the population regardless of their money or access to resources.

157
00:09:12,540 --> 00:09:16,520
Speaker 5:  And it's like, I don't know, I think the earth is flat and that's one we

158
00:09:16,520 --> 00:09:19,920
Speaker 5:  can see. I mean, no one has shown that particular

159
00:09:20,840 --> 00:09:24,520
Speaker 5:  feedback loop of TikTok, but the idea that the Chinese government

160
00:09:24,800 --> 00:09:28,720
Speaker 5:  could use a tool like TikTok to create that feedback loop in their interests

161
00:09:28,890 --> 00:09:32,520
Speaker 5:  is not out of bounds. Right. But I, I'm just like, I don't know what,

162
00:09:32,590 --> 00:09:36,480
Speaker 5:  what would be the control of Mark Zuckerberg wanting to

163
00:09:36,480 --> 00:09:40,000
Speaker 5:  do that for Instagram? If you're that worried about the Chinese government

164
00:09:40,000 --> 00:09:43,520
Speaker 5:  doing it, then you're worried about the mechanism. And there's, there's no

165
00:09:43,760 --> 00:09:46,320
Speaker 5:  check on that mechanism anywhere for any social media

166
00:09:46,390 --> 00:09:50,080
Speaker 6:  That comes back to McKenna. What seems to me to be the, the like

167
00:09:50,080 --> 00:09:53,080
Speaker 6:  underlying thrust of this whole hearing, which is that everything is the

168
00:09:53,080 --> 00:09:56,840
Speaker 6:  same except China and instantly changes the tenor of everything.

169
00:09:57,360 --> 00:10:00,760
Speaker 6:  Because you can say all the same sentences and you can end them with China

170
00:10:00,940 --> 00:10:03,280
Speaker 6:  and it, the whole vibe changes, right?

171
00:10:03,730 --> 00:10:07,440
Speaker 7:  No, that's correct. When you look at a lot of what

172
00:10:07,680 --> 00:10:11,080
Speaker 7:  happened today, and even the lawmakers oftentimes brought it up. They would

173
00:10:11,080 --> 00:10:14,960
Speaker 7:  say, all this stuff is really bad, the misinformation, the mental

174
00:10:14,960 --> 00:10:18,840
Speaker 7:  health harms, all these things that we saw so explicitly with

175
00:10:18,840 --> 00:10:22,640
Speaker 7:  Francis Hogan in 20 20, 20 21 with Facebook and the Facebook

176
00:10:22,640 --> 00:10:26,600
Speaker 7:  files, it was so transparent that this was happening elsewhere. It

177
00:10:26,600 --> 00:10:29,280
Speaker 7:  seems like we had often like forgotten about it. And then the lawmakers would

178
00:10:29,280 --> 00:10:32,280
Speaker 7:  remind themselves and be like, but I also support a federal data privacy

179
00:10:32,280 --> 00:10:36,120
Speaker 7:  framework. And it's like, what happened to that? How long have we been talking

180
00:10:36,120 --> 00:10:39,760
Speaker 7:  about that? Right? And so of course like these

181
00:10:40,280 --> 00:10:44,040
Speaker 7:  Facebook, you know, meta Twitter, Google, all these have like so

182
00:10:44,040 --> 00:10:47,640
Speaker 7:  much, you know, they have so much stake in like the states and have

183
00:10:47,640 --> 00:10:51,600
Speaker 7:  built up their lobbying arms tremendously over the past five to

184
00:10:51,600 --> 00:10:55,360
Speaker 7:  10 years with this heightened criticism. TikTok hasn't done that. I think

185
00:10:55,360 --> 00:10:59,320
Speaker 7:  it was just in 2021 or 22 that they even

186
00:10:59,320 --> 00:11:03,120
Speaker 7:  had a DC office. Like they weren't here engaging with lawmakers

187
00:11:03,560 --> 00:11:07,120
Speaker 7:  in the same way that American made, you know, America based social media

188
00:11:07,120 --> 00:11:10,920
Speaker 7:  apps have. And they really have tried hard in a very

189
00:11:10,920 --> 00:11:14,840
Speaker 7:  small amount of time with TikTok to build a coalition supporting them.

190
00:11:14,840 --> 00:11:18,760
Speaker 7:  They brought the TikTok talkers here. There was three house Democrats

191
00:11:18,760 --> 00:11:22,680
Speaker 7:  who came in support of TikTok yesterday. But you know, any kind

192
00:11:22,680 --> 00:11:25,800
Speaker 7:  of movement that TikTok is trying to make has been like moving a boulder.

193
00:11:26,150 --> 00:11:30,080
Speaker 7:  It's been very, very difficult. Especially because like look at the, this

194
00:11:30,080 --> 00:11:33,080
Speaker 7:  is something that I asked a lot of TikTok ERs last night coming to my head

195
00:11:33,080 --> 00:11:36,960
Speaker 7:  right now. But my question was, what happens if TikTok

196
00:11:36,960 --> 00:11:40,760
Speaker 7:  is banned tomorrow? What happens to you? And you know, a lot of them said,

197
00:11:40,760 --> 00:11:44,560
Speaker 7:  we'll be fine. We were fine before TikTok. Maybe they're, maybe

198
00:11:44,560 --> 00:11:47,360
Speaker 7:  we'll go do something else. But we really like TikTok in the community we

199
00:11:47,360 --> 00:11:51,000
Speaker 7:  built here and we don't want that to go away. You talked to

200
00:11:51,000 --> 00:11:54,640
Speaker 7:  Mark Warner and Mark Warner of course is a very, you know, he likes to talk

201
00:11:54,640 --> 00:11:58,480
Speaker 7:  about competitive capitalism a lot. He's very much that guy, right?

202
00:11:58,480 --> 00:12:02,400
Speaker 7:  He's an old Silicon Valley wireless guy. And yesterday when I

203
00:12:02,400 --> 00:12:05,200
Speaker 7:  was talking to him, you know, I was saying, what happens to these creators?

204
00:12:05,200 --> 00:12:08,800
Speaker 7:  He's like, there'll be another platform that will compete and

205
00:12:08,800 --> 00:12:11,000
Speaker 7:  will take up the hole that TikTok leaves behind.

206
00:12:11,220 --> 00:12:14,560
Speaker 5:  And like the c e o of YouTube is like peeking out from behind

207
00:12:14,560 --> 00:12:18,080
Speaker 7:  The curtain. Mark Zuckerberg is peeking out from behind. I bring up the fact

208
00:12:18,080 --> 00:12:22,000
Speaker 7:  that, okay, so TikTok is bringing the first inch right

209
00:12:22,000 --> 00:12:25,640
Speaker 7:  of meaningful competition to a lot of American social media

210
00:12:26,000 --> 00:12:28,960
Speaker 7:  platforms. If that goes away, he's like, oh, it could be an Indian company.

211
00:12:29,090 --> 00:12:32,680
Speaker 7:  Oh it could be, you know, a British company or something. But just

212
00:12:32,680 --> 00:12:33,320
Speaker 5:  As long as it's

213
00:12:33,320 --> 00:12:36,560
Speaker 7:  Not China, as long as it's not China, right? And of course, you know, it's

214
00:12:36,560 --> 00:12:39,760
Speaker 7:  opening up more room for American companies to remain dominant.

215
00:12:40,330 --> 00:12:44,120
Speaker 1:  Do you get a sense that those American companies are lobbying against

216
00:12:44,120 --> 00:12:44,720
Speaker 1:  TikTok?

217
00:12:44,910 --> 00:12:48,800
Speaker 7:  It's been proven. Yeah. So I, I believe it was, maybe it was like

218
00:12:48,800 --> 00:12:51,800
Speaker 7:  the Washington Post, but it must have been like a year ago at this point.

219
00:12:51,800 --> 00:12:55,680
Speaker 7:  But Facebook was paying a republican campaigning organization to

220
00:12:55,680 --> 00:12:59,600
Speaker 7:  run a bunch of op-eds in newspapers across

221
00:12:59,600 --> 00:13:03,240
Speaker 7:  the country saying how dangerous TikTok was and

222
00:13:03,240 --> 00:13:06,960
Speaker 7:  railing against all of these challenges. Many of the challenges, right? The

223
00:13:06,960 --> 00:13:10,120
Speaker 7:  challenges I put in quotes that we saw today highlighted by Republicans the

224
00:13:10,120 --> 00:13:14,080
Speaker 7:  night called chicken got brought up. Again, it's like, why is

225
00:13:14,080 --> 00:13:16,880
Speaker 7:  this stuff still there? Especially stuff that has been debunked as almost

226
00:13:16,880 --> 00:13:20,520
Speaker 7:  like, you know, media or like false tide pods. Yeah.

227
00:13:20,520 --> 00:13:23,080
Speaker 7:  Just people who like see something like, oh my God, it's going crazy, all

228
00:13:23,080 --> 00:13:26,960
Speaker 7:  the kids are gonna die. But yeah, no, it's, so these Facebook, at least

229
00:13:26,960 --> 00:13:30,800
Speaker 7:  meta was reported to have done this before. It's very clear that they're

230
00:13:30,800 --> 00:13:34,560
Speaker 7:  trying to point the finger at TikTok and be like, well, would you rather

231
00:13:34,560 --> 00:13:38,400
Speaker 7:  it be China or would you rather it be American companies that have,

232
00:13:38,400 --> 00:13:38,640
Speaker 7:  so,

233
00:13:38,640 --> 00:13:42,400
Speaker 1:  So basically Congress is saying we're fine with super

234
00:13:42,800 --> 00:13:46,720
Speaker 1:  invasive software like knowing everything about you. It just

235
00:13:46,720 --> 00:13:50,440
Speaker 1:  can't be Chinese. It has to be American grown. Only Americans can exploit

236
00:13:50,440 --> 00:13:50,800
Speaker 1:  Americans

237
00:13:50,810 --> 00:13:54,800
Speaker 5:  Or apparently from India or India. Let's be clear. Senator Warner quality

238
00:13:54,890 --> 00:13:58,760
Speaker 5:  is fine. But this is the question, and I, I

239
00:13:58,760 --> 00:14:01,840
Speaker 5:  don't want to excuse the Chinese government, right,

240
00:14:02,200 --> 00:14:06,040
Speaker 5:  which is a repressive sensors government that

241
00:14:06,040 --> 00:14:07,400
Speaker 5:  does bad things to its people.

242
00:14:07,660 --> 00:14:08,080
Speaker 1:  Yes,

243
00:14:08,550 --> 00:14:12,280
Speaker 5:  That's true. And it is our country's great geopolitical

244
00:14:12,280 --> 00:14:16,040
Speaker 5:  rival, but we're all tied up together, right? Like I, I

245
00:14:16,040 --> 00:14:19,800
Speaker 5:  keep thinking about today, I kept thinking about Tim Cook at the Code

246
00:14:19,800 --> 00:14:22,640
Speaker 5:  Conference last year when I asked him, what do you think about TikTok? And

247
00:14:22,640 --> 00:14:26,080
Speaker 5:  he is like, I don't, I'm not familiar with TikTok. And I was like, this is

248
00:14:26,080 --> 00:14:29,840
Speaker 5:  the number one app on your store. This is why people have the phone and you,

249
00:14:30,140 --> 00:14:33,680
Speaker 5:  you just are not gonna talk about it because you are all wrapped up in China.

250
00:14:33,680 --> 00:14:37,400
Speaker 5:  Like this is the complicated riddle of our time is

251
00:14:37,400 --> 00:14:41,280
Speaker 5:  here's the world's biggest, most vibrant democracy in some ways and

252
00:14:41,280 --> 00:14:44,640
Speaker 5:  here's China and we're absolutely relying on each other

253
00:14:44,970 --> 00:14:48,800
Speaker 5:  in all kinds of weird, complicated ways except for when it comes to

254
00:14:48,800 --> 00:14:52,480
Speaker 5:  this app. And we're like, fuck you. And it like that was the

255
00:14:52,480 --> 00:14:56,440
Speaker 5:  tenor of the hearing. And the thing is, I don't think they made the,

256
00:14:56,440 --> 00:15:00,400
Speaker 5:  the stuff McKenna is talking about malign foreign influence operations or

257
00:15:00,400 --> 00:15:04,120
Speaker 5:  you're collecting grandma's data to do something nefarious because we haven't

258
00:15:04,120 --> 00:15:07,600
Speaker 5:  made the case. It just seems ridiculous.

259
00:15:07,680 --> 00:15:08,240
Speaker 5:  Right?

260
00:15:08,430 --> 00:15:10,200
Speaker 7:  Well it just seems xenophobic,

261
00:15:10,280 --> 00:15:14,240
Speaker 5:  Right? Like, like there's just a piece of this puzzle where like the American

262
00:15:14,560 --> 00:15:17,720
Speaker 5:  Congress yelled at a Chinese CEO for four and a half hours today and he was

263
00:15:17,720 --> 00:15:20,720
Speaker 5:  not allowed to answer the questions. And I don't think his answers are good,

264
00:15:20,840 --> 00:15:24,760
Speaker 5:  right? Like his big answer is Project Texas. And he kept

265
00:15:24,760 --> 00:15:28,560
Speaker 5:  saying it's gonna be American data on American soil overseen by

266
00:15:28,560 --> 00:15:32,280
Speaker 5:  an American company. And I don't know, like we're going to, we're gonna,

267
00:15:32,280 --> 00:15:35,840
Speaker 5:  the, the data center will be shaped like a flag. Like something will happen

268
00:15:35,930 --> 00:15:39,880
Speaker 5:  that'll be good for you. And like no one would even let him finish the sentence.

269
00:15:40,200 --> 00:15:44,040
Speaker 5:  Right? McKenna is there, is there a reason that no one believes that

270
00:15:44,040 --> 00:15:45,120
Speaker 5:  this is like a good answer?

271
00:15:45,650 --> 00:15:48,520
Speaker 7:  So my theory about all of this,

272
00:15:49,440 --> 00:15:53,360
Speaker 7:  especially having talked to people this week, it's not if, if my

273
00:15:53,360 --> 00:15:57,320
Speaker 7:  good faith theory, if if I'm going to look at this in good faith at the

274
00:15:57,520 --> 00:16:01,360
Speaker 7:  lawmakers in the whole situation, my good faith argument is that lawmakers

275
00:16:01,360 --> 00:16:05,080
Speaker 7:  have been caught on their back foot before Cambridge Analytica dozens, you

276
00:16:05,080 --> 00:16:08,680
Speaker 7:  know, foreign influence operations on American, you know, social media

277
00:16:08,960 --> 00:16:12,120
Speaker 7:  platforms, Facebook, Twitter, et cetera. This has happened for years. They

278
00:16:12,120 --> 00:16:15,920
Speaker 7:  publish, you know, these platforms publish transparency reports being like,

279
00:16:15,920 --> 00:16:19,760
Speaker 7:  look at all of the bad things happening. Oh we found them and got rid of

280
00:16:19,760 --> 00:16:23,160
Speaker 7:  them, but they still exist and they're still happening. So

281
00:16:23,160 --> 00:16:26,760
Speaker 7:  talking to Warner, talking to some other lawmakers, it seemed like

282
00:16:26,850 --> 00:16:30,160
Speaker 7:  if they, you know, if there really is like something here that they think

283
00:16:30,160 --> 00:16:33,760
Speaker 7:  they need to do, it's the fact that this could happen again on a far greater

284
00:16:33,760 --> 00:16:37,360
Speaker 7:  scale in in way that the US does cannot control as much

285
00:16:37,410 --> 00:16:40,800
Speaker 7:  as, you know, a Cambridge Analytica, which it's still, you know, Cambridge

286
00:16:40,960 --> 00:16:44,760
Speaker 7:  Analytica that was like tw you know, so long ago. And that was the beginning

287
00:16:44,760 --> 00:16:48,360
Speaker 7:  of like serious conversations around a federal data privacy network framework.

288
00:16:48,430 --> 00:16:52,120
Speaker 7:  It's gone nowhere. Yeah, yeah. You know, it's gone absolutely nowhere

289
00:16:52,460 --> 00:16:56,080
Speaker 7:  and it's kind of funny to see these same things play out just

290
00:16:56,080 --> 00:16:59,880
Speaker 7:  with China at the center at a time of

291
00:16:59,950 --> 00:17:03,920
Speaker 7:  very, very deep geopolitical tensions. We just came out

292
00:17:03,920 --> 00:17:07,440
Speaker 7:  of, I would like to remind you the Chinese spy balloon situation which ended

293
00:17:07,440 --> 00:17:10,560
Speaker 7:  up being like, you know, just research balloons in some cases

294
00:17:11,320 --> 00:17:15,160
Speaker 7:  where we had a whole weekend afraid of UFOs and like China's

295
00:17:15,160 --> 00:17:18,920
Speaker 7:  spies all because of, you know, this relationship that we have with, you

296
00:17:18,920 --> 00:17:19,960
Speaker 7:  know, this foreign adversary

297
00:17:20,450 --> 00:17:24,320
Speaker 5:  Is the answer. Just that because it's China, we get to skip to the

298
00:17:24,320 --> 00:17:27,760
Speaker 5:  end, which is, boy it seems like these apps are harvesting a lot of data.

299
00:17:27,910 --> 00:17:31,880
Speaker 5:  It's hard to make the case against Google because Google's an all American

300
00:17:31,880 --> 00:17:35,600
Speaker 5:  company in their, they lead the s and p 500 or whatever.

301
00:17:35,700 --> 00:17:39,200
Speaker 5:  But it's easy when you're like, China's bad and politically we don't have

302
00:17:39,200 --> 00:17:39,680
Speaker 5:  to sell it.

303
00:17:40,550 --> 00:17:44,360
Speaker 7:  Well look at, you know, I think when it comes to these things it's, you gotta,

304
00:17:44,360 --> 00:17:48,160
Speaker 7:  unfortunately campaigns are like a 24 month cycle.

305
00:17:48,480 --> 00:17:52,440
Speaker 7:  Yeah. Now instead of like a year or six month cycle, you had

306
00:17:52,440 --> 00:17:56,280
Speaker 7:  Trump in 2018 issuing executive orders to ban TikTok, which

307
00:17:56,280 --> 00:17:59,600
Speaker 7:  of course we've covered. They were ultimately overruled. And then of course

308
00:17:59,600 --> 00:18:03,320
Speaker 7:  Biden now is dealing with this anti-China

309
00:18:03,320 --> 00:18:06,960
Speaker 7:  fervor amongst Republicans at a time when he's gonna be

310
00:18:07,200 --> 00:18:10,960
Speaker 7:  running for reelection at a time when Republicans are going really, really

311
00:18:10,960 --> 00:18:14,800
Speaker 7:  hard on China. So I think a lot of this has to do with the fact that

312
00:18:14,800 --> 00:18:18,440
Speaker 7:  even the Biden administration gave TikTok an ultimatum last week

313
00:18:18,440 --> 00:18:22,200
Speaker 7:  ahead of this hearing. Either find an American owner or get banned.

314
00:18:22,200 --> 00:18:26,000
Speaker 7:  That was from Biden. That is the, that is the strongest thing

315
00:18:26,000 --> 00:18:29,880
Speaker 7:  we've heard against TikTok from both administrations

316
00:18:29,880 --> 00:18:32,840
Speaker 7:  really. Cause it's the one thing that could really, you know, could really

317
00:18:32,840 --> 00:18:36,440
Speaker 7:  happen. I do think at the center it probably is China and having to do with

318
00:18:36,440 --> 00:18:37,560
Speaker 7:  electoral politics.

319
00:18:37,630 --> 00:18:41,240
Speaker 6:  Well and it also, it, it seems to me that it, it's a move

320
00:18:41,590 --> 00:18:45,480
Speaker 6:  that these politicians don't have with any of these other companies

321
00:18:45,480 --> 00:18:48,960
Speaker 6:  and services. Like nobody ever talks about banning Google,

322
00:18:49,040 --> 00:18:51,640
Speaker 6:  right? Like that's not, that's just not an option. It's just not a thing

323
00:18:51,640 --> 00:18:55,280
Speaker 6:  that's gonna happen. But, but like to your point Eli, about skipping to the

324
00:18:55,280 --> 00:18:58,480
Speaker 6:  end, we, we also get to skip past all the nuance of figuring out

325
00:18:58,870 --> 00:19:02,840
Speaker 6:  what is a responsible, like you're talking about McKenna privacy framework

326
00:19:02,840 --> 00:19:05,480
Speaker 6:  for this country. Like these are complicated questions, they're the right

327
00:19:05,640 --> 00:19:09,320
Speaker 6:  questions, but they're complicated ones and it's so much easier

328
00:19:09,570 --> 00:19:13,360
Speaker 6:  in a certain sense to just say, this is bad, get it out of here in a way

329
00:19:13,360 --> 00:19:17,040
Speaker 6:  that you just straight up can't do unless you have this

330
00:19:17,190 --> 00:19:20,920
Speaker 6:  gigantic boogeyman that is China to blame it all on. And

331
00:19:21,070 --> 00:19:24,920
Speaker 6:  I think we we're now what, seven years into arguing about privacy

332
00:19:24,930 --> 00:19:28,200
Speaker 6:  debates on the internet and we've gotten nowhere and I didn't get any sense

333
00:19:28,200 --> 00:19:32,080
Speaker 6:  from the hearing that anyone is any further along in even knowing what we

334
00:19:32,080 --> 00:19:32,640
Speaker 6:  want out of

335
00:19:32,640 --> 00:19:36,560
Speaker 5:  This hearing. Wait, can I play some clips just to illustrate, if we were,

336
00:19:36,660 --> 00:19:39,720
Speaker 5:  you know, seven years into a privacy debate and no one had gotten farther

337
00:19:39,720 --> 00:19:43,640
Speaker 5:  along, I'd be like, this is a victory. This hearing. Lemme

338
00:19:43,640 --> 00:19:46,440
Speaker 5:  just play play the wifi clip. Just play the wifi clip.

339
00:19:46,730 --> 00:19:49,720
Speaker 8:  Mr. Chu, does TikTok access the home wifi network?

340
00:19:50,440 --> 00:19:54,400
Speaker 9:  <unk> If the user turns on the wifi, I, I'm sorry, I may not understand the,

341
00:19:54,400 --> 00:19:54,720
Speaker 9:  so

342
00:19:54,720 --> 00:19:58,600
Speaker 8:  If I have TikTok app on my phone and my phone is on my home wifi

343
00:19:58,600 --> 00:20:00,720
Speaker 8:  network, does TikTok access that network?

344
00:20:01,610 --> 00:20:05,400
Speaker 9:  It will have to, to access that network to get connections to the internet.

345
00:20:05,400 --> 00:20:06,320
Speaker 9:  If, if that's the question,

346
00:20:06,940 --> 00:20:07,360
Speaker 5:  The

347
00:20:07,360 --> 00:20:11,000
Speaker 6:  Pause before he answers you can, you can just tell his brain is just

348
00:20:11,000 --> 00:20:13,280
Speaker 6:  breaking into a thousand pieces at this question.

349
00:20:13,400 --> 00:20:16,680
Speaker 5:  Let's say he was just like, yes. What

350
00:20:17,060 --> 00:20:21,040
Speaker 5:  specifically is the gotcha there? The shit's already on your

351
00:20:21,040 --> 00:20:25,000
Speaker 5:  phone, that's where the data is. There's nothing else on

352
00:20:25,000 --> 00:20:28,640
Speaker 5:  the average American wifi network that has as much data as their

353
00:20:28,640 --> 00:20:29,040
Speaker 5:  phones,

354
00:20:29,560 --> 00:20:30,960
Speaker 1:  Their fridge.

355
00:20:32,070 --> 00:20:34,840
Speaker 5:  It's like, oh no, they got to my Roku.

356
00:20:36,030 --> 00:20:39,840
Speaker 5:  Like I'm sitting here with America's foremost plex server operator

357
00:20:41,060 --> 00:20:45,000
Speaker 1:  And I don't want them actually have access to that. Mm.

358
00:20:45,470 --> 00:20:46,080
Speaker 1:  Stay out

359
00:20:46,080 --> 00:20:49,600
Speaker 5:  TikTok. I'm just saying. Saying like if we were deep into a privacy

360
00:20:49,800 --> 00:20:53,560
Speaker 5:  framework conversation, we would be light years ahead of

361
00:20:53,670 --> 00:20:57,560
Speaker 5:  does it access your wifi network? And

362
00:20:57,630 --> 00:21:00,760
Speaker 5:  that to me is, there were some good questions in here,

363
00:21:01,420 --> 00:21:04,920
Speaker 5:  but let's make the case to ban a wildly popular

364
00:21:05,140 --> 00:21:08,160
Speaker 5:  app that is effectively a cultural institution

365
00:21:08,530 --> 00:21:12,200
Speaker 5:  because it's owned by China. You have to try to make the case.

366
00:21:12,430 --> 00:21:15,880
Speaker 1:  Well and I wonder, McKenna, I would love to hear your thoughts on this.

367
00:21:16,170 --> 00:21:19,720
Speaker 1:  Is this actually a politically good move?

368
00:21:19,850 --> 00:21:23,400
Speaker 1:  Because there's a whole bunch of people who like TikTok and will probably

369
00:21:23,400 --> 00:21:26,040
Speaker 1:  be really pissed that they don't have TikTok. You

370
00:21:26,320 --> 00:21:30,160
Speaker 7:  Would think. Right. And my favorite quote from

371
00:21:30,160 --> 00:21:34,120
Speaker 7:  last night with representative Jamal Bowman was he was saying Republicans

372
00:21:34,120 --> 00:21:38,040
Speaker 7:  hate TikTok cause they don't have any swag, which is very funny,

373
00:21:38,040 --> 00:21:41,880
Speaker 7:  incredible. They can't engage with like young people

374
00:21:41,890 --> 00:21:45,440
Speaker 7:  at the same level. Democrats can on TikTok. At the same time, all these Democrats

375
00:21:45,440 --> 00:21:47,920
Speaker 7:  as we went down the Dias we're like ban TikTok.

376
00:21:49,660 --> 00:21:53,640
Speaker 7:  But you know, it's, I don't, I don't think it's good. I don't think

377
00:21:53,640 --> 00:21:57,560
Speaker 7:  it's a really great opportunity to ban TikTok or at least if you're not being

378
00:21:57,560 --> 00:22:01,480
Speaker 7:  as transparent about the threat. Because in my main question when I talk

379
00:22:01,480 --> 00:22:04,800
Speaker 7:  to lawmakers is what do I have to fear? In some cases it's like blackmail,

380
00:22:04,940 --> 00:22:08,920
Speaker 7:  but okay, how many people are in the United States? Millions,

381
00:22:09,030 --> 00:22:12,920
Speaker 7:  billions, you know, so many people. How,

382
00:22:12,920 --> 00:22:16,040
Speaker 7:  what is the chance that they're going to find my little data profile and

383
00:22:16,040 --> 00:22:19,920
Speaker 7:  find blackmail, you know, on like me and I I

384
00:22:19,920 --> 00:22:22,520
Speaker 7:  guess like switch me, you know, try and turn me into like a double agent

385
00:22:22,520 --> 00:22:25,720
Speaker 7:  like China spy. Like I don't know what they're asking of me. You know, I

386
00:22:25,720 --> 00:22:27,080
Speaker 7:  love this. What does that mean for me?

387
00:22:27,080 --> 00:22:29,960
Speaker 6:  Maybe they already did McKenna. Have you thought about that? Maybe, maybe

388
00:22:29,960 --> 00:22:30,160
Speaker 6:  they

389
00:22:30,160 --> 00:22:31,760
Speaker 7:  Already did. My long con,

390
00:22:33,370 --> 00:22:37,120
Speaker 7:  my long con is to get into the Verge institution and I

391
00:22:37,120 --> 00:22:41,040
Speaker 1:  Think we can safely say China's not doing a lot of great things. And

392
00:22:41,040 --> 00:22:44,480
Speaker 1:  and there is there, there's the very legitimate concern that they can

393
00:22:44,660 --> 00:22:47,920
Speaker 1:  use data from this application for nefarious means.

394
00:22:48,410 --> 00:22:52,280
Speaker 1:  If they wanted to, if they wanted to target a lawmaker who was using TikTok

395
00:22:52,280 --> 00:22:55,800
Speaker 1:  to reach the youth, they could theoretically do that. But

396
00:22:56,010 --> 00:23:00,000
Speaker 1:  so could Facebook, so could Twitter,

397
00:23:00,300 --> 00:23:04,160
Speaker 1:  all of those apps could do so could every single ad

398
00:23:04,160 --> 00:23:07,880
Speaker 1:  vendor on the planet, right? Like there's a lot of people with very

399
00:23:07,880 --> 00:23:08,800
Speaker 1:  extensive data

400
00:23:09,040 --> 00:23:12,200
Speaker 5:  Points, right? But I think this is why they kept talking about federal privacy

401
00:23:12,260 --> 00:23:13,680
Speaker 5:  legislation. Well

402
00:23:13,680 --> 00:23:14,080
Speaker 1:  They didn't,

403
00:23:14,080 --> 00:23:17,880
Speaker 5:  They kept or algorithmic transparency or all these things

404
00:23:17,880 --> 00:23:20,920
Speaker 5:  that are the thing you want. Yeah. And then they're like, that's too hard.

405
00:23:21,060 --> 00:23:24,360
Speaker 5:  And I can just say you're communists and commies.

406
00:23:24,790 --> 00:23:28,400
Speaker 5:  I mean they, they asked him like a thousand times they, instead of saying

407
00:23:28,420 --> 00:23:31,600
Speaker 5:  ccp, they kept saying the Chinese Communist Party. Yeah. Which is great.

408
00:23:32,440 --> 00:23:34,960
Speaker 1:  Every, I would, I had to take a break for like an hour and a half from the

409
00:23:34,960 --> 00:23:37,480
Speaker 1:  hearings to go to some meetings and they were talking about communists when

410
00:23:37,480 --> 00:23:40,880
Speaker 1:  I took the break and I came back and they're like, do you or do you not know

411
00:23:40,880 --> 00:23:42,840
Speaker 1:  communists at TikTok?

412
00:23:42,840 --> 00:23:46,200
Speaker 5:  Oh there was a really good, there was a, a really good sequence when someone

413
00:23:46,200 --> 00:23:49,880
Speaker 5:  asked him if by Danon helped him prepare for the hearing.

414
00:23:50,420 --> 00:23:54,040
Speaker 5:  And poor show was like, yeah, a lot of people have texted me. It's a very

415
00:23:54,040 --> 00:23:57,000
Speaker 5:  high profile hearing and he had to say this like five times. He's like, yeah,

416
00:23:57,000 --> 00:24:00,640
Speaker 5:  my phone's blowing up. Yeah. Like people are sending me advice. I'm sorry.

417
00:24:01,670 --> 00:24:04,160
Speaker 5:  I mean, and then they're like, will you share what people have said to you?

418
00:24:04,160 --> 00:24:08,040
Speaker 5:  He's like, no, they're my lawyers. Let's play some more clips just to,

419
00:24:08,290 --> 00:24:11,480
Speaker 5:  to put this in a perspective for people what we were dealing with today

420
00:24:11,480 --> 00:24:15,280
Speaker 1:  Because it was bad. Like they were very focused on we wanna talk, we wanna

421
00:24:15,280 --> 00:24:18,800
Speaker 1:  get TikTok, but we're gonna do it by the stupidest questions

422
00:24:19,200 --> 00:24:22,920
Speaker 1:  possible showing we have zero understanding of how technology actually works.

423
00:24:23,410 --> 00:24:27,240
Speaker 5:  No. So I have this theory about that Congress doesn't understand technology.

424
00:24:27,240 --> 00:24:30,560
Speaker 5:  I've been working on it. Yeah. Okay. And McKenna, I'm curious for your, your

425
00:24:30,560 --> 00:24:34,320
Speaker 5:  read on this cuz you, you were in it today. I feel like every time we say

426
00:24:34,720 --> 00:24:38,040
Speaker 5:  Congress doesn't understand how technology works, what we are saying is

427
00:24:38,800 --> 00:24:41,840
Speaker 5:  Congress is pretty frustrated by the First Amendment. So they have to say

428
00:24:41,840 --> 00:24:45,760
Speaker 5:  some crazy shit so they can pass a speech regulation so it doesn't

429
00:24:45,760 --> 00:24:47,760
Speaker 5:  seem like they're passing speech regulation.

430
00:24:47,760 --> 00:24:51,080
Speaker 1:  Yeah, they seemed really excited to censor people. Yeah. They're like, China

431
00:24:51,080 --> 00:24:52,000
Speaker 1:  does it, why don't we?

432
00:24:52,220 --> 00:24:55,640
Speaker 5:  But like that's like that to me that feels like the dynamic

433
00:24:55,790 --> 00:24:59,600
Speaker 5:  here. Whenever we're like, they don't understand tech, it's because they

434
00:24:59,600 --> 00:25:03,360
Speaker 5:  can't say the thing they want to say, which is we think these should be the

435
00:25:03,360 --> 00:25:05,040
Speaker 5:  rules for how people talk to each other online.

436
00:25:05,240 --> 00:25:07,800
Speaker 1:  Even when they were talking about how it scans your eyes,

437
00:25:07,990 --> 00:25:10,920
Speaker 5:  Even when they talk about how it scans, like that's just pure crazy. We'll

438
00:25:10,920 --> 00:25:14,600
Speaker 5:  end with that one. So I wanna play this clip. It's about grapefruit. It's

439
00:25:14,600 --> 00:25:18,360
Speaker 5:  great. But McKenna, do you think that that's the right read that this sort

440
00:25:18,360 --> 00:25:21,840
Speaker 5:  of like willful not getting tech is just an end around the first amendment

441
00:25:22,210 --> 00:25:26,040
Speaker 7:  In some ways yes. I, I think, I don't know if that's, you know,

442
00:25:26,240 --> 00:25:29,520
Speaker 7:  explicitly what lawmakers are thinking to themselves. I don't, you know,

443
00:25:29,520 --> 00:25:32,040
Speaker 7:  I don't think anyone wants to say that they are even believe that they're

444
00:25:32,040 --> 00:25:35,760
Speaker 7:  opposed to the First Amendment in many ways. But people want platforms to

445
00:25:35,760 --> 00:25:39,720
Speaker 7:  do certain things for specific motives. That is something, you know, we,

446
00:25:39,720 --> 00:25:43,440
Speaker 7:  that's something everyone wants, right? I post something because I want people

447
00:25:43,440 --> 00:25:47,400
Speaker 7:  to see it. That's my motive. Lawmakers specifically, they want to be

448
00:25:47,400 --> 00:25:50,640
Speaker 7:  able to say things that break a platform's terms of service in some cases

449
00:25:51,010 --> 00:25:54,560
Speaker 7:  or they want to, you know, take down something for example

450
00:25:54,590 --> 00:25:58,480
Speaker 7:  that is misinformation or disinformation. It really does come

451
00:25:58,480 --> 00:26:02,400
Speaker 7:  down to power and control over narrative setting. And

452
00:26:02,400 --> 00:26:06,240
Speaker 7:  I think, you know, just the way social media operates, people feel as

453
00:26:06,480 --> 00:26:10,200
Speaker 7:  if they, they are more empowered to influence a discussion, but when they

454
00:26:10,200 --> 00:26:13,760
Speaker 7:  aren't and when they don't get their way, especially someone in power in

455
00:26:13,960 --> 00:26:17,040
Speaker 7:  Congress who feels like they can, you know, come at these companies

456
00:26:17,680 --> 00:26:21,520
Speaker 7:  and be able to say, you know, you're acting in opposition to the constitution,

457
00:26:21,520 --> 00:26:25,280
Speaker 7:  to the first amendment. They're going to do it because they have a

458
00:26:25,280 --> 00:26:28,680
Speaker 7:  motive and there's stakes for them to be able to say what they want online.

459
00:26:29,070 --> 00:26:32,480
Speaker 5:  Okay, before we run the next clip, which is worth waiting for

460
00:26:32,480 --> 00:26:35,040
Speaker 5:  William's telling me we gotta take a break, we're gonna take a quick break.

461
00:26:35,790 --> 00:26:39,440
Speaker 10:  Here's a question. Can science tell us what makes

462
00:26:39,440 --> 00:26:40,520
Speaker 10:  something funny?

463
00:26:41,350 --> 00:26:43,960
Speaker 11:  Girl science, where have you been?

464
00:26:44,560 --> 00:26:46,760
Speaker 10:  Comedians like zuko, tka wanna know

465
00:26:47,160 --> 00:26:50,520
Speaker 11:  We've been just flailing around

466
00:26:50,640 --> 00:26:52,680
Speaker 11:  screaming in the

467
00:26:52,680 --> 00:26:56,320
Speaker 10:  Streets and scientists best answer might actually come from

468
00:26:56,320 --> 00:26:56,960
Speaker 10:  Skittles.

469
00:26:57,300 --> 00:26:58,880
Speaker 12:  The jellybean are the, the Skittles

470
00:26:58,880 --> 00:26:59,840
Speaker 13:  One, the Skittle study.

471
00:26:59,940 --> 00:27:03,800
Speaker 12:  I'm supposed to offer you these Skittles. Would you like these Skittles?

472
00:27:06,690 --> 00:27:10,680
Speaker 12:  There's pretty good evidence that laughter is a form of communication. So

473
00:27:10,680 --> 00:27:12,680
Speaker 12:  then the question becomes, well what does it communicate

474
00:27:14,710 --> 00:27:17,880
Speaker 10:  This week on unexplainable the science of humor.

475
00:27:18,430 --> 00:27:22,160
Speaker 10:  Follow unexplainable wherever you listen for new episodes every

476
00:27:22,160 --> 00:27:22,520
Speaker 10:  Wednesday.

477
00:27:23,770 --> 00:27:26,760
Speaker 13:  So if this makes the podcast, people are gonna be listening and be like,

478
00:27:26,760 --> 00:27:27,840
Speaker 13:  this is not funny at all.

479
00:27:28,630 --> 00:27:32,560
Speaker 14:  It's been three years since the world shut down and things are mostly back

480
00:27:32,560 --> 00:27:36,160
Speaker 14:  to normal-ish. But one thing seems forever changed.

481
00:27:36,160 --> 00:27:39,920
Speaker 14:  Office workers are not commuting downtown five days a week.

482
00:27:39,990 --> 00:27:43,240
Speaker 14:  It's got people worried about a vicious cycle.

483
00:27:43,840 --> 00:27:47,080
Speaker 15:  Property values will fall further, taxes will have to rise further, government

484
00:27:47,080 --> 00:27:49,560
Speaker 15:  spending will have to fall further, more people will leave and so forth

485
00:27:49,560 --> 00:27:51,400
Speaker 15:  and so forth. And we get into this urban doom

486
00:27:51,400 --> 00:27:54,960
Speaker 14:  Loop. So at today, explain, we're asking how to break the loop.

487
00:27:54,960 --> 00:27:57,440
Speaker 14:  Maybe it's changing the perception of crime.

488
00:27:57,440 --> 00:28:00,880
Speaker 16:  There are several visible signs of disorder that are not

489
00:28:00,880 --> 00:28:04,800
Speaker 16:  necessarily related to crime, that are causing people to feel that the

490
00:28:04,800 --> 00:28:05,920
Speaker 16:  cities have become unsafe.

491
00:28:05,920 --> 00:28:08,960
Speaker 14:  Maybe it's making buses and trains free.

492
00:28:08,960 --> 00:28:12,720
Speaker 17:  Metro bus could soon be a free ride. A proposal unveil today would

493
00:28:12,720 --> 00:28:14,920
Speaker 17:  get rid of metro bus fairs in DC

494
00:28:14,920 --> 00:28:16,400
Speaker 14:  Maybe the solution is you,

495
00:28:16,620 --> 00:28:17,520
Speaker 5:  You talking to me

496
00:28:17,750 --> 00:28:21,360
Speaker 14:  City limits from today explained dropping over the next few weeks,

497
00:28:21,920 --> 00:28:22,320
Speaker 14:  wherever you listen,

498
00:28:34,860 --> 00:28:37,180
Speaker 5:  We're back. It's time for the grapefruit clip.

499
00:28:37,710 --> 00:28:41,260
Speaker 18:  An another study showed that TikTok

500
00:28:41,600 --> 00:28:45,420
Speaker 18:  was, had a, a hydra hydroxychloroquine

501
00:28:45,660 --> 00:28:49,180
Speaker 18:  tutorial on how to fabricate this from

502
00:28:49,180 --> 00:28:52,820
Speaker 18:  grapefruit. Now there's two problems with that. Number one

503
00:28:53,520 --> 00:28:56,740
Speaker 18:  hydroxychloroquine is not effective in treating covid.

504
00:28:57,390 --> 00:29:01,180
Speaker 18:  So that's one issue. The second issue is you

505
00:29:01,180 --> 00:29:04,780
Speaker 18:  can't even make hydroxychloroquine from grapefruit. So

506
00:29:04,780 --> 00:29:07,540
Speaker 18:  again, this is a really serious

507
00:29:08,320 --> 00:29:12,220
Speaker 18:  miscommunication about healthcare information that people

508
00:29:12,220 --> 00:29:16,060
Speaker 18:  looking at, at, at TikTok are

509
00:29:16,060 --> 00:29:18,980
Speaker 18:  able to get. And in fact it's being pushed out to them.

510
00:29:19,540 --> 00:29:23,460
Speaker 5:  Okay, I just have a number of things to say. First of all, please

511
00:29:23,460 --> 00:29:27,140
Speaker 5:  go if you just like do the, like the simplification of that

512
00:29:27,340 --> 00:29:30,780
Speaker 5:  equation, it's TikTok is telling people that grapefruits cure covid,

513
00:29:31,220 --> 00:29:35,060
Speaker 5:  which is definitely legal speech in America. You are allowed to wander the

514
00:29:35,060 --> 00:29:38,540
Speaker 5:  streets of this country insisting that you can cure covid with grapefruits.

515
00:29:38,540 --> 00:29:42,080
Speaker 5:  Yeah. You are allowed to wander the streets of this. And I don't think that's

516
00:29:42,080 --> 00:29:46,040
Speaker 5:  dangerous misinformation. I think it's not great. Oh, you do.

517
00:29:46,040 --> 00:29:46,840
Speaker 5:  It's not true.

518
00:29:47,180 --> 00:29:49,720
Speaker 7:  Can you ask Bard how we, how we use

519
00:29:49,720 --> 00:29:53,560
Speaker 5:  Grapefruit? But I just like the government is not allowed to show

520
00:29:53,560 --> 00:29:57,480
Speaker 5:  up on your doorstep and say lying is illegal just is the US government, the

521
00:29:57,480 --> 00:30:00,320
Speaker 5:  united, the united, the Chinese government is, that's actually like part

522
00:30:00,320 --> 00:30:03,160
Speaker 5:  of the whole problem here. Yep. The United States government can't be like

523
00:30:03,160 --> 00:30:06,880
Speaker 5:  lying legal. It is also legal speech to tell people

524
00:30:06,880 --> 00:30:10,320
Speaker 5:  they can make hydroxychloroquine from grapefruits, which is an incredible

525
00:30:10,640 --> 00:30:12,200
Speaker 5:  sentence. Welcome to the verge chest, everyone.

526
00:30:13,220 --> 00:30:17,120
Speaker 7:  You know, looking at this specific example, it was

527
00:30:17,280 --> 00:30:20,520
Speaker 7:  actually, I was sitting in the hearing room and I was getting

528
00:30:20,910 --> 00:30:24,880
Speaker 7:  reminded of this hearing in like the early 1990s in

529
00:30:24,880 --> 00:30:28,720
Speaker 7:  the senate on video games where they like rolled in a TV on a cart

530
00:30:28,720 --> 00:30:32,280
Speaker 7:  like you would like in school when you had like movie day, right? Yeah. And

531
00:30:32,280 --> 00:30:34,960
Speaker 7:  they rolled it in and it was like mortal combat and they're like, this is

532
00:30:34,960 --> 00:30:38,120
Speaker 7:  so bad. Kind of getting to this, the beginning of the conversation about

533
00:30:38,120 --> 00:30:41,960
Speaker 7:  like Call of Duty causes people to, you know,

534
00:30:41,960 --> 00:30:45,480
Speaker 7:  inflict violence on others. It was kind of starting back then. You look at

535
00:30:45,480 --> 00:30:49,280
Speaker 7:  this, you know, grapefruit is so silly in many ways,

536
00:30:49,280 --> 00:30:52,960
Speaker 7:  but at the same time you look at the, another example that really struck

537
00:30:52,960 --> 00:30:56,800
Speaker 7:  me from this was there was one lawmaker who brought up a bunch

538
00:30:56,800 --> 00:30:59,240
Speaker 7:  of videos of kids being sad.

539
00:31:00,080 --> 00:31:00,520
Speaker 5:  Yeah.

540
00:31:00,520 --> 00:31:04,440
Speaker 7:  They had themes of like depression. But I'm like, I grew up

541
00:31:04,440 --> 00:31:08,160
Speaker 7:  on Tumblr, I grew up, you know, on Reddit and things

542
00:31:08,160 --> 00:31:11,840
Speaker 7:  and they, and oftentimes it's far worse. And many of the,

543
00:31:11,870 --> 00:31:14,800
Speaker 7:  a handful of the videos were of this like niche core, core

544
00:31:15,600 --> 00:31:19,320
Speaker 7:  genre of TikTok, which is kind of a ironic

545
00:31:19,320 --> 00:31:23,240
Speaker 7:  framing. It's very hard to understand. I can understand why a lawmaker would

546
00:31:23,240 --> 00:31:26,760
Speaker 7:  not attempt to understand it, but I got a text message from a

547
00:31:26,760 --> 00:31:30,360
Speaker 7:  democratic, like a very young, I am pretty sure they're a teenager,

548
00:31:30,750 --> 00:31:34,400
Speaker 7:  a democratic digital consultant for like campaigns and stuff. And he's like,

549
00:31:34,400 --> 00:31:38,280
Speaker 7:  this is hilarious. They really just don't get it. They're

550
00:31:38,280 --> 00:31:42,200
Speaker 7:  really going after him over nothing. And so it, there is

551
00:31:42,200 --> 00:31:45,560
Speaker 7:  like a media literacy aspect of it as well.

552
00:31:45,780 --> 00:31:49,200
Speaker 7:  And my main point here really is getting to the fact that you can't keep

553
00:31:49,200 --> 00:31:52,680
Speaker 7:  kids from being sad and you can't really keep them from playing video games

554
00:31:52,680 --> 00:31:56,280
Speaker 7:  especially, but it's not like the platform or the game itself is

555
00:31:56,280 --> 00:31:59,560
Speaker 7:  incentivizing them to feel this way in many ways. Of course, like you're

556
00:31:59,560 --> 00:32:02,600
Speaker 7:  sitting on an algorithm, you're seeing people, we, we all, we've all heard

557
00:32:02,600 --> 00:32:06,080
Speaker 7:  this before, right? But that is like protected speech and it's speech that

558
00:32:06,080 --> 00:32:09,120
Speaker 7:  kids have been doing forever online and even predating it.

559
00:32:09,210 --> 00:32:12,400
Speaker 5:  Oh yeah, no, I was a, a world class sad teenager,

560
00:32:13,910 --> 00:32:14,400
Speaker 7:  A shit

561
00:32:14,400 --> 00:32:18,160
Speaker 5:  Poster. I was there for it. I'm just saying the the grapefruit example is

562
00:32:18,160 --> 00:32:21,920
Speaker 5:  perfect because I think it cuts across party lines in, in like a, in just

563
00:32:21,920 --> 00:32:25,480
Speaker 5:  a way that maybe not it's so stupid. Yeah. Grapefruits are for everybody,

564
00:32:25,880 --> 00:32:29,520
Speaker 5:  but like, just the, just the phrase should the government ban

565
00:32:29,520 --> 00:32:33,200
Speaker 5:  videos where people say you can make hydroxychloroquine from grapefruits

566
00:32:33,490 --> 00:32:35,800
Speaker 5:  is inherently not partisan.

567
00:32:37,640 --> 00:32:40,760
Speaker 5:  Right? It's just a it's just a, just a philosophical question about the nature

568
00:32:40,760 --> 00:32:44,400
Speaker 5:  of government power. Yeah. That is loaded with keywords for

569
00:32:44,720 --> 00:32:48,640
Speaker 5:  whatever you think your politics are. And the answer in

570
00:32:48,640 --> 00:32:52,280
Speaker 5:  almost every case is the government should not make a rule

571
00:32:52,390 --> 00:32:56,200
Speaker 5:  that says videos that claim you can make hydroxychloroquine from

572
00:32:56,200 --> 00:32:59,640
Speaker 5:  grapefruits should be illegal. I don't think that the, I think government

573
00:32:59,640 --> 00:33:02,840
Speaker 5:  speech regulations on on the whole are bad. I think this one would be particularly

574
00:33:02,840 --> 00:33:05,640
Speaker 5:  hilarious. Like if we were like there's the first amendment, there's one

575
00:33:05,640 --> 00:33:08,800
Speaker 5:  exception, it's gonna be the grapefruits, right? Like

576
00:33:09,190 --> 00:33:12,920
Speaker 5:  fine this clip where she's like, your app is

577
00:33:12,940 --> 00:33:16,360
Speaker 5:  bad because it contains this speech that I find

578
00:33:16,360 --> 00:33:20,160
Speaker 5:  distasteful. Well she's saying you're the, the government

579
00:33:20,160 --> 00:33:24,040
Speaker 5:  is going to impose a penalty on you for hosting this speech.

580
00:33:24,460 --> 00:33:27,080
Speaker 7:  I'm curious if she's ever used Google

581
00:33:27,450 --> 00:33:30,400
Speaker 5:  Or I haven't Googled hydroxychloroquine grapefruit.

582
00:33:30,790 --> 00:33:31,920
Speaker 7:  I just did actually

583
00:33:32,950 --> 00:33:34,080
Speaker 6:  What happened McKenna,

584
00:33:34,360 --> 00:33:37,360
Speaker 7:  I'm dying and this is the last you'll hear from me.

585
00:33:38,170 --> 00:33:42,080
Speaker 7:  No, just kidding. But I'm looking, there was a Reuters fact check

586
00:33:42,080 --> 00:33:45,960
Speaker 7:  in June of 2021. Fact checking the grapefruit video that was

587
00:33:45,960 --> 00:33:49,480
Speaker 7:  viewed at the time over 600,000 times and then

588
00:33:49,480 --> 00:33:53,360
Speaker 7:  circulated on Instagram. And I mean talking about virality 600,000 times

589
00:33:53,360 --> 00:33:55,720
Speaker 7:  really isn't, it's nothing that crazy. Yeah.

590
00:33:55,720 --> 00:33:58,960
Speaker 5:  I can find you 600,000 Americans who will, who believe literally anything

591
00:33:59,160 --> 00:33:59,360
Speaker 5:  you

592
00:33:59,360 --> 00:34:03,200
Speaker 7:  Say. And as reported by Reuters here, it was a clip, it was one video.

593
00:34:04,890 --> 00:34:07,800
Speaker 6:  There's such a long history of this on TikTok, right? Where there's like

594
00:34:07,800 --> 00:34:11,080
Speaker 6:  one video that one person sees and writes a story about how it's not a thing

595
00:34:11,080 --> 00:34:14,560
Speaker 6:  and then like three steps later, like teachers are worried about it and we

596
00:34:14,560 --> 00:34:17,880
Speaker 6:  have moral panics about like kids ceiling toilets in high school. It's, this

597
00:34:17,880 --> 00:34:19,000
Speaker 6:  is what keeps happening. This

598
00:34:19,000 --> 00:34:21,960
Speaker 5:  Is the flat earth thing, right? And this is where the idea that you could,

599
00:34:21,960 --> 00:34:25,760
Speaker 5:  you could start a foreign influence campaign on TikTok, I think actually

600
00:34:25,760 --> 00:34:29,280
Speaker 5:  has legs. This cycle is well understood. We've seen it. O

601
00:34:29,520 --> 00:34:31,400
Speaker 5:  NyQuil chicken is this cycle. Well

602
00:34:31,400 --> 00:34:34,680
Speaker 6:  This is Russia's whole thing, right? Like Russia, the Russia keeps proving

603
00:34:34,680 --> 00:34:35,640
Speaker 6:  that you can do this.

604
00:34:36,160 --> 00:34:38,760
Speaker 5:  When I was a kid in Wisconsin, the local news ran a segment. I will never

605
00:34:38,760 --> 00:34:42,680
Speaker 5:  forget this, the local news ran a segment about how kids were getting high

606
00:34:42,680 --> 00:34:45,920
Speaker 5:  by drinking too much water. And some of them would drink like way too much

607
00:34:45,920 --> 00:34:49,840
Speaker 5:  water and die. And my mother, who is a fucking doctor was

608
00:34:49,840 --> 00:34:52,520
Speaker 5:  like, I need you to make sure you don't drink enough water. And I was like,

609
00:34:52,520 --> 00:34:56,440
Speaker 5:  what is like what is happening to me? Like why do

610
00:34:56,440 --> 00:35:00,080
Speaker 5:  I even know this idea exists? Oh God. I was like, I can just drink Miller

611
00:35:00,200 --> 00:35:03,880
Speaker 5:  Light. I live in Wisconsin. It's, it flows more

612
00:35:03,880 --> 00:35:07,840
Speaker 5:  freely than the water. But like that cycle is truly well

613
00:35:07,840 --> 00:35:11,280
Speaker 5:  understood. And I think what you're seeing is that you need some reason.

614
00:35:11,730 --> 00:35:15,280
Speaker 5:  So you settle on, people are lying to you about grapefruits.

615
00:35:15,690 --> 00:35:19,280
Speaker 5:  China's bad, we're gonna ban TikTok. And that's about as good as the

616
00:35:19,280 --> 00:35:22,840
Speaker 5:  reasoning. God and I, I just honestly challenge

617
00:35:23,200 --> 00:35:27,120
Speaker 5:  everyone listening to just think about the grapefruit. Should it be

618
00:35:27,120 --> 00:35:30,640
Speaker 5:  legal to make videos where you lie to people about grapefruit turning into

619
00:35:30,700 --> 00:35:34,600
Speaker 5:  hydroxychloroquine? Like should it be legal to lie? Like run out the

620
00:35:34,600 --> 00:35:38,080
Speaker 5:  thread of it's illegal to tell lies oof. It's

621
00:35:38,080 --> 00:35:39,760
Speaker 1:  Bad my ass would be in jail,

622
00:35:39,760 --> 00:35:43,400
Speaker 5:  Right? I'm lying to you right now. And like

623
00:35:43,400 --> 00:35:46,840
Speaker 5:  you see in this hearing, so much of the content moderation

624
00:35:46,840 --> 00:35:50,760
Speaker 5:  concerns are just speech regulations dressed up in

625
00:35:50,760 --> 00:35:54,400
Speaker 5:  the guise of you are creating a public danger. You're showing teenagers

626
00:35:54,400 --> 00:35:58,160
Speaker 5:  videos of other teenagers being sad and creating a sadness epidemic. It's

627
00:35:58,160 --> 00:36:02,120
Speaker 5:  like, yo, the cure is on tour right now. Like

628
00:36:02,120 --> 00:36:05,720
Speaker 5:  it's very expensive to go be sad with thousands of other people in America

629
00:36:05,720 --> 00:36:09,600
Speaker 5:  right now. Like that's the part that I don't get. And

630
00:36:09,600 --> 00:36:12,880
Speaker 5:  that's why I asked this question about are we really, when we say Congress

631
00:36:12,880 --> 00:36:16,360
Speaker 5:  doesn't understand the internet, it's because they have to concoct these

632
00:36:16,590 --> 00:36:20,520
Speaker 5:  sideways attempts at at speech regulations in order

633
00:36:20,520 --> 00:36:22,280
Speaker 5:  to control the thing, which is all effectively

634
00:36:22,280 --> 00:36:24,720
Speaker 1:  Speech. But that was the same, right? Like you talked about video games earlier,

635
00:36:24,720 --> 00:36:27,920
Speaker 1:  that was the exact same thing that happened in the early nineties. And it

636
00:36:27,920 --> 00:36:31,400
Speaker 1:  was with TV too. We have ratings in TV because in the nineties, Hillary Clinton

637
00:36:31,400 --> 00:36:34,840
Speaker 1:  and a lot of Democrats and some Republicans were like, oh my god, they can

638
00:36:34,840 --> 00:36:36,840
Speaker 1:  watch sipowicz butt. What

639
00:36:36,960 --> 00:36:38,920
Speaker 5:  Will we remember when a butt was like the thing

640
00:36:38,920 --> 00:36:40,440
Speaker 1:  That was the thing, oh

641
00:36:40,440 --> 00:36:43,000
Speaker 5:  My gosh, bill Clinton, by the way wanted to put a chip in every television

642
00:36:43,000 --> 00:36:45,640
Speaker 5:  set in America. It did would, it was called the clipper chip. And that was

643
00:36:45,640 --> 00:36:48,680
Speaker 5:  deemed to be like way over the line. It was a simpler time.

644
00:36:48,980 --> 00:36:52,800
Speaker 1:  And now they're like, what if we stopped showing kids

645
00:36:52,910 --> 00:36:55,760
Speaker 1:  trends that seem harmful and instead showed them

646
00:36:56,230 --> 00:36:59,200
Speaker 1:  studying right? Like they do in China. That

647
00:36:59,360 --> 00:37:02,320
Speaker 7:  Actually gets to a point that I did wanna make. And I think about all the

648
00:37:02,320 --> 00:37:06,000
Speaker 7:  time I go back to it as like the center and the focus of my reporting as

649
00:37:06,000 --> 00:37:09,920
Speaker 7:  something not to forget. You look at tv, right? You look at video games

650
00:37:10,180 --> 00:37:13,680
Speaker 7:  and I don't want to say that social media doesn't present

651
00:37:14,110 --> 00:37:18,080
Speaker 7:  a different set of, you know, potential harms that need

652
00:37:18,080 --> 00:37:21,800
Speaker 7:  to be looked at and you know, maybe need to be regulated in some case.

653
00:37:22,200 --> 00:37:25,600
Speaker 7:  I don't want to undermine that in this, but so many of the issues that we

654
00:37:25,600 --> 00:37:29,480
Speaker 7:  have with the internet are human issues. And that gets erased

655
00:37:29,480 --> 00:37:33,040
Speaker 7:  immediately because of the like, just because of bringing in tech into it.

656
00:37:33,640 --> 00:37:37,400
Speaker 7:  Why are people sad? Well, maybe they are getting like a lot of sad

657
00:37:37,400 --> 00:37:41,120
Speaker 7:  videos online, but also, I don't know, maybe they can't afford to

658
00:37:41,120 --> 00:37:45,080
Speaker 7:  live. Maybe they're scared of, you

659
00:37:45,080 --> 00:37:48,880
Speaker 7:  know, not being able to live on their own, you know, to buy a house.

660
00:37:48,880 --> 00:37:52,680
Speaker 7:  There, there are issues at the core of American

661
00:37:52,690 --> 00:37:56,640
Speaker 7:  culture right now that I think have more to say right

662
00:37:56,640 --> 00:37:59,600
Speaker 7:  about the causes of social ill

663
00:38:00,520 --> 00:38:04,480
Speaker 7:  violence, things like that, that get caught up in the social media side of

664
00:38:04,480 --> 00:38:08,400
Speaker 7:  it because it's an easy victim, it's an easy villain to blame

665
00:38:08,400 --> 00:38:11,720
Speaker 7:  in this case. And I think it gets us away from talking about the real human

666
00:38:11,720 --> 00:38:15,560
Speaker 7:  element of like human suffering and oh,

667
00:38:15,560 --> 00:38:19,320
Speaker 7:  into these, you know, conversations about data privacy. You

668
00:38:19,520 --> 00:38:22,400
Speaker 7:  know, these things that last forever that no one can agree on and we make

669
00:38:22,400 --> 00:38:25,960
Speaker 7:  no progress on and people continue to suffer. That's a really

670
00:38:26,280 --> 00:38:27,400
Speaker 7:  horrible way to end. But

671
00:38:27,730 --> 00:38:29,720
Speaker 5:  No, don't understand that. We're definitely gonna end on the clip of the

672
00:38:29,880 --> 00:38:32,280
Speaker 5:  Congressman not understanding how eye tracking works. Don't worry, we'll

673
00:38:32,280 --> 00:38:36,160
Speaker 5:  get there. I I, I wanna agree with you like wholeheartedly. I just,

674
00:38:36,160 --> 00:38:40,080
Speaker 5:  it's, here's my question for you. In return, having, you were at the hearing

675
00:38:40,080 --> 00:38:43,120
Speaker 5:  today, you saw the TikTok, you experienced it fully.

676
00:38:43,660 --> 00:38:46,920
Speaker 5:  The balance of that is the same for every app, right?

677
00:38:47,230 --> 00:38:50,920
Speaker 5:  It's the same for Instagram, it's the same for YouTube. YouTube burnout culture

678
00:38:50,920 --> 00:38:54,880
Speaker 5:  is real, right? YouTube is like way ahead of the curve with

679
00:38:54,880 --> 00:38:58,520
Speaker 5:  its creator economy than every other platform. And people have

680
00:38:58,520 --> 00:39:01,880
Speaker 5:  cycled out of YouTube and like maybe TikTok is at the beginning of it, but

681
00:39:01,880 --> 00:39:05,760
Speaker 5:  because it's China, the calculus has changed and I'm,

682
00:39:05,760 --> 00:39:09,240
Speaker 5:  I'm wondering if you've seen even a glimmer of recognition

683
00:39:09,350 --> 00:39:13,080
Speaker 5:  that it's not the big bad China app that's causing the

684
00:39:13,080 --> 00:39:16,840
Speaker 5:  problems, but rather like, oh we've created a, a

685
00:39:16,840 --> 00:39:20,760
Speaker 5:  generation of digital tools that allows this group of people to

686
00:39:20,760 --> 00:39:23,640
Speaker 5:  express themselves. And actually we don't really like what they're saying.

687
00:39:23,910 --> 00:39:27,760
Speaker 7:  I think that's fair. I, when we were on our way down yesterday, I was seeing

688
00:39:27,760 --> 00:39:31,440
Speaker 7:  there's the CMBC CNBC article highlighting that representative

689
00:39:31,440 --> 00:39:35,320
Speaker 7:  Jamal Bowman from New York is publicly supporting

690
00:39:35,320 --> 00:39:38,800
Speaker 7:  TikTok and does not want TikTok to be banned. He was going to have this press

691
00:39:38,800 --> 00:39:42,640
Speaker 7:  conference with 30 creators and he got up in front of a podium

692
00:39:42,690 --> 00:39:46,080
Speaker 7:  at the house triangle and said, this is very xenophobic

693
00:39:46,640 --> 00:39:49,680
Speaker 7:  was one of the first people to say it. There was only two other house democrats

694
00:39:49,680 --> 00:39:53,400
Speaker 7:  that like stood with him. It's a very small coalition. I ran up to the congressman

695
00:39:53,400 --> 00:39:56,440
Speaker 7:  after they, after the press conference and I said, Congressman,

696
00:39:57,260 --> 00:40:01,200
Speaker 7:  can you build a broader coalition supporting TikTok? And he said

697
00:40:01,200 --> 00:40:04,720
Speaker 7:  something to the effect of Democrats build coalitions and like shrugged.

698
00:40:05,160 --> 00:40:08,920
Speaker 7:  Like you know, it doesn't seem like there really is

699
00:40:08,990 --> 00:40:11,920
Speaker 7:  energy. This is the first time, like yesterday was the first time I heard

700
00:40:11,920 --> 00:40:15,760
Speaker 7:  someone just explicitly say this, you know, in support of TikTok. I,

701
00:40:16,110 --> 00:40:20,040
Speaker 7:  I do. It's very, it's a very tricky situation with China involved politically,

702
00:40:20,040 --> 00:40:23,920
Speaker 7:  right? This is a more political question than really because you're right

703
00:40:23,920 --> 00:40:27,720
Speaker 7:  cuz when we get to the center of it, it's TikTok can be banned. It

704
00:40:27,720 --> 00:40:31,040
Speaker 7:  doesn't really harm Americans. It like harms teens I guess because they won't

705
00:40:31,040 --> 00:40:34,000
Speaker 7:  be able to scroll on TikTok and they lose this community. It's really it,

706
00:40:34,000 --> 00:40:37,680
Speaker 7:  I don't want to undermine that, but it's not something that lawmakers

707
00:40:37,680 --> 00:40:41,480
Speaker 7:  have to think through all the way. This is something that can

708
00:40:41,480 --> 00:40:43,280
Speaker 7:  just be seen as an effective,

709
00:40:43,280 --> 00:40:46,800
Speaker 5:  Especially if it's this bipartisan, right? The electoral price for being

710
00:40:46,800 --> 00:40:50,080
Speaker 5:  the person who banned TikTok is zero because the other side is saying the

711
00:40:50,080 --> 00:40:50,600
Speaker 5:  exact same thing,

712
00:40:50,600 --> 00:40:53,840
Speaker 7:  Right? What the thing is is who's actually going to do it? And this is my

713
00:40:54,150 --> 00:40:58,000
Speaker 7:  also like I can I continue thinking about this cuz when Mark Warner introduced

714
00:40:58,000 --> 00:41:01,760
Speaker 7:  the restrict act, of course it does in implement the possibility

715
00:41:01,760 --> 00:41:05,640
Speaker 7:  of a ban on TikTok. It basically what the bill does is it creates a

716
00:41:05,640 --> 00:41:08,880
Speaker 7:  process for the Secretary of Commerce to

717
00:41:09,300 --> 00:41:13,240
Speaker 7:  investigate foreign owned technologies and create

718
00:41:13,240 --> 00:41:17,200
Speaker 7:  protections and guardrails ultimately like the biggest tool

719
00:41:17,200 --> 00:41:21,040
Speaker 7:  being a ban. So it got applauded, you know there was all this,

720
00:41:21,040 --> 00:41:24,160
Speaker 7:  all these write-ups about the fact that TikTok could be banned by this bill,

721
00:41:24,300 --> 00:41:28,160
Speaker 7:  but the ban is such a small part of it, right? And this

722
00:41:28,160 --> 00:41:32,080
Speaker 7:  really, I think if like Democrats were to agree to some kind of

723
00:41:32,150 --> 00:41:36,120
Speaker 7:  form of banning TikTok, it would probably be passing

724
00:41:36,120 --> 00:41:39,720
Speaker 7:  this bill that everyone goes, woo, so excited. We're gonna take on TikTok

725
00:41:39,720 --> 00:41:43,400
Speaker 7:  in China. And then it just ends up being a smaller set of transparency

726
00:41:43,400 --> 00:41:47,080
Speaker 7:  requirements to the federal government or something in the long run. I do

727
00:41:47,080 --> 00:41:50,520
Speaker 7:  think that is probably the more realistic option, especially heading into

728
00:41:50,520 --> 00:41:54,400
Speaker 7:  2024. In 2022 specifically, I did a bunch of

729
00:41:54,400 --> 00:41:58,320
Speaker 7:  reporting on creators influencer outreach by campaigns. This

730
00:41:58,320 --> 00:42:02,200
Speaker 7:  is something that's been going on for years now, but I think it

731
00:42:02,200 --> 00:42:06,120
Speaker 7:  was a Tufts report study that at that point in time in all the

732
00:42:06,120 --> 00:42:10,040
Speaker 7:  major battleground states in 2022, gen Z was growing into

733
00:42:10,040 --> 00:42:13,480
Speaker 7:  the biggest and most important voting block. Yeah, you're gonna take away

734
00:42:13,480 --> 00:42:16,800
Speaker 7:  TikTok in Penn, you're gonna take away TikTok in Pennsylvania, you're gonna

735
00:42:16,800 --> 00:42:20,640
Speaker 7:  take away TikTok in Arizona right now. Or maybe that marginal

736
00:42:20,640 --> 00:42:23,600
Speaker 7:  error, you know, that margin of error right here between winning and losing

737
00:42:23,600 --> 00:42:26,680
Speaker 7:  an election might be like a high school, you know,

738
00:42:27,480 --> 00:42:29,800
Speaker 7:  like the senior class of a high school. I don't think that's gonna happen

739
00:42:30,320 --> 00:42:31,400
Speaker 7:  for either party. This

740
00:42:31,400 --> 00:42:35,000
Speaker 6:  Is why I'm absolutely convinced that what all these lawmakers

741
00:42:35,110 --> 00:42:38,960
Speaker 6:  want and believe they can accomplish is to get TikTok sold to Oracle

742
00:42:38,970 --> 00:42:42,440
Speaker 6:  or some other American company because that's all upside for them, right?

743
00:42:42,440 --> 00:42:46,160
Speaker 6:  Then you get to say this is no longer part of China. We did it

744
00:42:46,160 --> 00:42:50,120
Speaker 6:  like we, we defeated China, we got what we wanted, but you still get to have

745
00:42:50,120 --> 00:42:53,760
Speaker 6:  TikTok and it's gonna be even better now because it's made by Americans.

746
00:42:54,180 --> 00:42:57,360
Speaker 6:  Yay competition. Like I I, they got

747
00:42:57,510 --> 00:43:01,200
Speaker 6:  shockingly far down that road in the Trump administration and it

748
00:43:01,200 --> 00:43:02,920
Speaker 6:  really seems like these politicians are like

749
00:43:02,990 --> 00:43:06,360
Speaker 5:  They got shockingly far down, down that road in the Trump administration

750
00:43:06,360 --> 00:43:09,640
Speaker 5:  in the way that the Trump administration got shockingly far down a number

751
00:43:09,640 --> 00:43:13,320
Speaker 5:  of roads, which is sure they set a bunch of outrageous stuff and held

752
00:43:13,560 --> 00:43:16,600
Speaker 5:  meetings where everyone was like, I can't believe we're in this meeting and

753
00:43:16,600 --> 00:43:17,400
Speaker 5:  then nothing happened.

754
00:43:17,520 --> 00:43:21,440
Speaker 7:  Right? I mean Oracle has, Oracle has access

755
00:43:21,440 --> 00:43:22,240
Speaker 7:  to the servers.

756
00:43:22,350 --> 00:43:24,920
Speaker 6:  Yeah, Oracle is in TikTok in a way it wasn't before,

757
00:43:25,040 --> 00:43:28,280
Speaker 5:  Right? They signed a deal where Oracle's now the cloud provider for TikTok

758
00:43:28,340 --> 00:43:32,080
Speaker 5:  and they have Project Texas, which he, this poor man, they named it Project

759
00:43:32,080 --> 00:43:34,960
Speaker 5:  Texas. So he could say Project Texas in the United States Capital Building

760
00:43:34,980 --> 00:43:37,280
Speaker 5:  and he was not allowed to finish that phrase Project

761
00:43:37,600 --> 00:43:37,760
Speaker 7:  Tex.

762
00:43:38,240 --> 00:43:39,720
Speaker 5:  They were like, we've heard about Texas,

763
00:43:39,880 --> 00:43:43,680
Speaker 7:  There was a Texas lawmaker who said we don't want your project Texas, you

764
00:43:43,680 --> 00:43:47,040
Speaker 7:  know, in his very, I'm pretending to sound very rural voice

765
00:43:48,440 --> 00:43:49,880
Speaker 7:  Texans don't want Project Texas.

766
00:43:50,950 --> 00:43:52,840
Speaker 5:  I was like, I have a plex server sir.

767
00:43:55,720 --> 00:43:58,360
Speaker 5:  But yeah, this is the idea, right? The project Texas, they're gonna move

768
00:43:58,360 --> 00:44:02,120
Speaker 5:  all the data centers here. Oracle's gonna run them Oracle's based in Texas.

769
00:44:02,120 --> 00:44:05,600
Speaker 5:  This is our argument for why it's called Project Texas. Oracle

770
00:44:05,930 --> 00:44:09,880
Speaker 5:  is going to control the code. They're gonna submit the app to the app

771
00:44:09,880 --> 00:44:13,360
Speaker 5:  stores. Basically Oracle's gonna run. TikTok is more or less the argument

772
00:44:13,360 --> 00:44:16,600
Speaker 5:  and they set up these transparency in centers. We should repromote Alex Heath

773
00:44:16,600 --> 00:44:19,560
Speaker 5:  went and visited one of these transparency centers and came away being like,

774
00:44:19,560 --> 00:44:22,920
Speaker 5:  this is basically a kids museum and I don't know what I'm supposed to get

775
00:44:22,920 --> 00:44:26,760
Speaker 5:  from this and we'll link the piece in the show notes, but that was kind of

776
00:44:26,760 --> 00:44:30,240
Speaker 5:  the vibe of everybody who got the tour Yeah. Is like, this is a kids museum

777
00:44:30,240 --> 00:44:34,040
Speaker 5:  for content moderation and then there's like a, a door that's marked data

778
00:44:34,040 --> 00:44:37,280
Speaker 5:  center and you can't go through it and they're like, this is not very transparent.

779
00:44:37,640 --> 00:44:41,440
Speaker 5:  Like there's a window That's not transparency, sir,

780
00:44:42,120 --> 00:44:46,040
Speaker 5:  but this is their plan, right? Is they're gonna let Oracle do it and

781
00:44:46,040 --> 00:44:48,760
Speaker 5:  if Oracle's doing it, then China's definitely not doing it. And that seemed

782
00:44:48,760 --> 00:44:49,840
Speaker 5:  to convince no one,

783
00:44:50,120 --> 00:44:51,080
Speaker 7:  There was actually

784
00:44:51,350 --> 00:44:55,160
Speaker 1:  A good question at one point of like, how does that actually resolve anything?

785
00:44:55,160 --> 00:44:58,720
Speaker 1:  If Oracle has access to your data? It doesn't actually stop

786
00:44:58,820 --> 00:45:02,200
Speaker 1:  the root cause, which is, this is Chinese built and everything.

787
00:45:02,200 --> 00:45:05,600
Speaker 6:  But that's a, that's a bunch of wishy-washy nonsense. You can't tweet,

788
00:45:05,680 --> 00:45:09,640
Speaker 6:  we've forced bite dance to sell TikTok to Oracle is a thing

789
00:45:09,640 --> 00:45:13,520
Speaker 6:  that you can tweet and is like a, a political win in a way that

790
00:45:13,520 --> 00:45:16,040
Speaker 6:  like, let me explain to you how the internet infrastructure actually works

791
00:45:16,040 --> 00:45:19,240
Speaker 6:  and how your data is stored and what that means for your life. But like these

792
00:45:19,240 --> 00:45:22,600
Speaker 6:  people are after big political wins. Like to your point, McKenna, this, this

793
00:45:22,600 --> 00:45:26,400
Speaker 6:  is about like finding a way to win and that's a, that's a win.

794
00:45:26,400 --> 00:45:29,760
Speaker 6:  Like we sold it to Microsoft is a, is a win. And maybe

795
00:45:29,760 --> 00:45:33,200
Speaker 7:  Not Microsoft though, cuz I go through the back of my head and I think with

796
00:45:33,230 --> 00:45:37,120
Speaker 7:  Lena Kahn at the ftc with the Biden administration. So opposed

797
00:45:37,120 --> 00:45:40,440
Speaker 7:  to digital monopolies as they say, who other than Oracle,

798
00:45:40,720 --> 00:45:44,240
Speaker 7:  right? Would maybe they allowed to buy TikTok? Not, certainly not

799
00:45:44,600 --> 00:45:47,320
Speaker 7:  Facebook. Yeah. You know, certainly not YouTube or Google.

800
00:45:48,370 --> 00:45:52,200
Speaker 7:  My only other thing would've been maybe they would've let musically buy it

801
00:45:52,200 --> 00:45:56,000
Speaker 7:  if TikTok didn't buy musically years ago. Right. That maybe would've made

802
00:45:56,000 --> 00:45:59,960
Speaker 7:  sense. So it's, it's, I don't know who would end up

803
00:45:59,960 --> 00:46:03,280
Speaker 7:  even buying it. Cuz we, I what wasn't it? There was some reporting. This

804
00:46:03,280 --> 00:46:06,360
Speaker 7:  is like the Silicon Valley bank situation. Remember? I think gold, wasn't

805
00:46:06,360 --> 00:46:10,120
Speaker 7:  it like Goldman or JP Morgan or something reportedly put in an offer to

806
00:46:10,120 --> 00:46:14,080
Speaker 7:  buy Silicon Valley Bank. This is all reported. And the

807
00:46:14,080 --> 00:46:16,200
Speaker 7:  Biden administration wouldn't let it happen. Cause they were just too big.

808
00:46:16,200 --> 00:46:20,160
Speaker 7:  Yeah, yeah. Well, what is too big and who is not too big to buy

809
00:46:20,230 --> 00:46:24,160
Speaker 7:  TikTok? I just don't know. Tumbling. Also, my one question and

810
00:46:24,160 --> 00:46:26,880
Speaker 7:  something that I'm going to focus on for the next couple weeks is, where

811
00:46:26,880 --> 00:46:28,040
Speaker 7:  is Oracle? Yeah,

812
00:46:28,040 --> 00:46:28,160
Speaker 5:  No,

813
00:46:28,350 --> 00:46:29,200
Speaker 7:  They said anything.

814
00:46:29,570 --> 00:46:33,440
Speaker 5:  No, but that's like, that's what I want Vibe right? Is

815
00:46:33,440 --> 00:46:37,280
Speaker 5:  like Larry e has made all of his money, he doesn't need to talk anymore

816
00:46:37,280 --> 00:46:39,200
Speaker 5:  and he's just gonna like hang out and

817
00:46:39,200 --> 00:46:42,240
Speaker 6:  Yeah, he like bought another island during this hearing. I'm sure it's fine.

818
00:46:42,240 --> 00:46:42,720
Speaker 6:  Yeah,

819
00:46:42,720 --> 00:46:43,000
Speaker 5:  Right.

820
00:46:43,640 --> 00:46:44,600
Speaker 1:  He's on it right now.

821
00:46:44,830 --> 00:46:48,720
Speaker 5:  Yeah. All right, I wanna end here because I, can you come back to

822
00:46:48,720 --> 00:46:51,520
Speaker 5:  these clips? Let's us run the eye tracking clip and talk about it for one

823
00:46:51,520 --> 00:46:51,680
Speaker 5:  second.

824
00:46:51,870 --> 00:46:55,320
Speaker 19:  I find that hard to believe. It's our understanding that they're looking

825
00:46:55,320 --> 00:46:58,520
Speaker 19:  at the eyes. How do you determine what age they are? Then

826
00:47:02,790 --> 00:47:06,440
Speaker 6:  That just broke me. And then, and then he, he keeps going because

827
00:47:06,440 --> 00:47:10,360
Speaker 6:  what he says is, th this came up because, because she is saying, I find

828
00:47:10,360 --> 00:47:11,040
Speaker 6:  that we don't use,

829
00:47:11,190 --> 00:47:14,880
Speaker 19:  I think it's our understanding that they're looking at the eyes. How do

830
00:47:14,880 --> 00:47:15,240
Speaker 5:  You explain?

831
00:47:19,850 --> 00:47:22,760
Speaker 1:  They're looking at 'em, they're looking at the eyes. Y'all.

832
00:47:23,670 --> 00:47:27,080
Speaker 7:  I think this is the same lawmaker who was doing like a Matthew McConaughey

833
00:47:27,080 --> 00:47:30,920
Speaker 7:  impression almost and was doing, tell me why, you know,

834
00:47:30,920 --> 00:47:31,760
Speaker 7:  tell me why

835
00:47:33,460 --> 00:47:36,160
Speaker 5:  God, at one point he goes, that's creepy. Tell me more.

836
00:47:36,400 --> 00:47:37,080
Speaker 7:  And I was like, yes,

837
00:47:38,490 --> 00:47:41,440
Speaker 5:  It was. I would say it was not our government's best moment today.

838
00:47:41,790 --> 00:47:45,560
Speaker 5:  Legitimate concerns about TikTok case not well prosecuted today

839
00:47:45,560 --> 00:47:46,840
Speaker 5:  I think is just on either

840
00:47:46,840 --> 00:47:49,960
Speaker 6:  Side. I don't know that, I don't know that TikTok comes out of this looking

841
00:47:49,960 --> 00:47:51,880
Speaker 6:  great either, but it, it feels like everybody lost

842
00:47:51,880 --> 00:47:55,600
Speaker 5:  Today. It was just once. I want one of these social media

843
00:47:55,630 --> 00:47:59,440
Speaker 5:  CEOs to be on the, like they're all trained. You can tell. Yeah. They're

844
00:47:59,440 --> 00:48:02,480
Speaker 5:  trained to just take it and then try to get out as many words as they can

845
00:48:02,480 --> 00:48:04,600
Speaker 5:  before they yell at some more. And then they say, I'll follow up with you.

846
00:48:04,870 --> 00:48:08,800
Speaker 5:  I've like, I've been deposed before and I've had the lawyers sit

847
00:48:08,800 --> 00:48:11,960
Speaker 5:  me down and say, what you're gonna say is I don't recall for 90 minutes.

848
00:48:11,980 --> 00:48:14,600
Speaker 5:  And then you're gonna shake their hands and leave. And in the middle of that

849
00:48:14,600 --> 00:48:18,320
Speaker 5:  you're like, what if I stop doing this? Like that's just a real thing.

850
00:48:18,420 --> 00:48:22,240
Speaker 5:  And just once I want a social media CEO to be like, all right, you get the

851
00:48:22,240 --> 00:48:25,640
Speaker 5:  unfiltered TikTok with no content moderation, welcome to hell. Right? Like

852
00:48:25,650 --> 00:48:28,640
Speaker 5:  or Alright, I'll turn off TikTok in your state senator,

853
00:48:28,870 --> 00:48:29,840
Speaker 7:  Just yours.

854
00:48:29,840 --> 00:48:33,800
Speaker 5:  Right. And we saw some of this happening today, right? TikTok was

855
00:48:33,800 --> 00:48:37,600
Speaker 5:  showing people a lot of live streams of the hearing, somewhat

856
00:48:37,600 --> 00:48:41,320
Speaker 5:  mysteriously. The TikTok influencers were out in force making content

857
00:48:41,320 --> 00:48:44,800
Speaker 5:  about this. Like they're using their platform to their advantage. But this

858
00:48:44,800 --> 00:48:48,760
Speaker 5:  poor dude, his job was not to prosecute the rebuttal case. It was

859
00:48:48,760 --> 00:48:52,720
Speaker 5:  to just take it. He, maybe he's not, he's not like a porter. He's the

860
00:48:52,720 --> 00:48:55,120
Speaker 5:  CEO of TikTok. His job is to take it. I don't feel bad for him.

861
00:48:55,230 --> 00:48:59,160
Speaker 7:  Yeah. But like at the heart of it is really TikTok, social media companies

862
00:48:59,160 --> 00:49:02,960
Speaker 7:  not being as transparent as people want, as lawmakers want, regulators

863
00:49:02,960 --> 00:49:06,840
Speaker 7:  want. And then lawmakers in this case, what TikTok not having

864
00:49:06,840 --> 00:49:10,640
Speaker 7:  sufficient evidence, but a really convenient EM enemy and just

865
00:49:10,640 --> 00:49:13,200
Speaker 7:  yelling and yelling and yelling. And when you ask for evidence supporting

866
00:49:13,200 --> 00:49:16,440
Speaker 7:  either side, it's just like the potential for bad is really bad this time

867
00:49:16,440 --> 00:49:18,760
Speaker 7:  guys. And we all just have to like take that

868
00:49:19,170 --> 00:49:20,640
Speaker 19:  Looking at the eyes.

869
00:49:20,780 --> 00:49:21,280
Speaker 21:  How do

870
00:49:21,280 --> 00:49:24,760
Speaker 5:  You turn, what age they're, that's not the worst.

871
00:49:24,990 --> 00:49:27,880
Speaker 5:  I apologize. That was great, McKenna. Thank you so much. You did. You did.

872
00:49:27,900 --> 00:49:31,360
Speaker 5:  You were much more patient than I would've been in that room today. But you've

873
00:49:31,360 --> 00:49:33,560
Speaker 5:  got a full video coming out. You've talked to all the, you've talked a bunch

874
00:49:33,560 --> 00:49:35,960
Speaker 5:  of lawmakers, you've talked to a bunch of influencers. Becca was there with

875
00:49:35,960 --> 00:49:38,160
Speaker 5:  you. I'm very excited to see it. Yeah, that's coming.

876
00:49:38,350 --> 00:49:40,560
Speaker 7:  It's not even finished. We have more to record tomorrow.

877
00:49:40,810 --> 00:49:44,520
Speaker 5:  Oh boy. Woo. I hope you can find Buddy Carter and see if he

878
00:49:44,520 --> 00:49:46,360
Speaker 5:  figured out how the age tracking,

879
00:49:47,140 --> 00:49:50,840
Speaker 7:  How do they look at the eyes? I don't want anyone looking at my eyes right

880
00:49:50,840 --> 00:49:51,000
Speaker 7:  now.

881
00:49:51,980 --> 00:49:53,880
Speaker 5:  All right, thank you, Soreen. We'll talk to you again.

882
00:49:58,870 --> 00:50:02,520
Speaker 21:  Well folks, here we are, former president Donald Trump

883
00:50:02,690 --> 00:50:05,840
Speaker 21:  appears on the brink of being indicted by a Manhattan grand jury.

884
00:50:06,340 --> 00:50:10,320
Speaker 21:  I'm Preet Barara, the former US attorney in Manhattan. My

885
00:50:10,320 --> 00:50:13,960
Speaker 21:  podcast, stay tuned is about law, justice, power

886
00:50:14,060 --> 00:50:17,760
Speaker 21:  and democracy. This week I discussed the latest news with a group of former

887
00:50:17,760 --> 00:50:21,440
Speaker 21:  federal prosecutors who understand how the justice system really works.

888
00:50:21,840 --> 00:50:25,120
Speaker 21:  Joyce Vance, Barb McQuaid and Ellie Honick.

889
00:50:25,490 --> 00:50:28,480
Speaker 21:  We discussed the questions on everyone's mind, like,

890
00:50:28,980 --> 00:50:32,520
Speaker 22:  Can you directly tie Donald Trump to the way these

891
00:50:32,520 --> 00:50:34,320
Speaker 22:  payments were booked and logged?

892
00:50:34,580 --> 00:50:38,440
Speaker 24:  Are prosecutors considering additional defendants or additional charges?

893
00:50:38,650 --> 00:50:42,120
Speaker 21:  Is this the kind of conduct that merits a charge of a former president of

894
00:50:42,120 --> 00:50:43,160
Speaker 21:  the United States? I

895
00:50:43,160 --> 00:50:46,240
Speaker 23:  Think this is a serious crime PR and I think it's one that I would charge.

896
00:50:46,700 --> 00:50:48,080
Speaker 21:  And where do we go from here?

897
00:50:48,540 --> 00:50:52,080
Speaker 24:  The presidency from prison, right? I mean add to the crazy.

898
00:50:52,640 --> 00:50:56,440
Speaker 21:  Add to the crazy to listen. Just search. Stay tuned

899
00:50:56,600 --> 00:50:59,880
Speaker 21:  wherever you get your podcasts. New episodes drop every Thursday.

900
00:51:01,590 --> 00:51:02,440
Speaker 21:  Stay tuned

901
00:51:04,390 --> 00:51:08,320
Speaker 23:  From New York Magazine in the Vox Media podcast network. This is the

902
00:51:08,320 --> 00:51:11,960
Speaker 23:  Joe Rogan experience with a thousand percent more experience.

903
00:51:13,210 --> 00:51:17,120
Speaker 23:  Or is it the Don Lemon Show with a hundred percent more understanding of

904
00:51:17,120 --> 00:51:21,000
Speaker 23:  women in their prime. Just kidding. This is on

905
00:51:21,000 --> 00:51:24,520
Speaker 23:  with Kara Swisher. And I'm Kara Swisher. Every Monday and Thursday I take

906
00:51:24,520 --> 00:51:28,320
Speaker 23:  on big names in tech, media, and politics to understand what makes

907
00:51:28,320 --> 00:51:32,160
Speaker 23:  them tick and to hold their feet to the fire. A bit on

908
00:51:32,160 --> 00:51:35,360
Speaker 23:  with Kara Swisher. Listen wherever you get your podcasts,

909
00:51:43,260 --> 00:51:46,980
Speaker 5:  We're back. Thanks again to McKenna, thank you to the United States Congress

910
00:51:47,100 --> 00:51:50,580
Speaker 5:  for providing us this opportunity to make content today. Thank you to the

911
00:51:50,580 --> 00:51:52,340
Speaker 5:  grapefruit producers of the world.

912
00:51:53,420 --> 00:51:54,380
Speaker 1:  We're here for you.

913
00:51:54,380 --> 00:51:57,620
Speaker 6:  Does anybody else like really want to have a grapefruit after the show is

914
00:51:57,620 --> 00:51:58,220
Speaker 6:  over? Like

915
00:51:58,220 --> 00:52:01,260
Speaker 5:  I have. I've been drinking grapefruit, LaCroix this whole time in honor of

916
00:52:01,260 --> 00:52:04,540
Speaker 5:  the grapefruit. I'm just telling the, the puzzle about free speech and grapefruit

917
00:52:04,540 --> 00:52:06,620
Speaker 5:  is real. Yeah. Doesn't matter.

918
00:52:06,620 --> 00:52:08,660
Speaker 6:  Grapefruit La Croix, very slightly cure

919
00:52:08,660 --> 00:52:10,060
Speaker 5:  Covid. Look, I got like, it's

920
00:52:10,060 --> 00:52:11,220
Speaker 6:  Just essence of covid.

921
00:52:11,330 --> 00:52:15,020
Speaker 5:  I got left wing family, I got right wing family. I feel confident that if

922
00:52:15,020 --> 00:52:17,780
Speaker 5:  I'm like, should the government make it illegal to lie about the uses of

923
00:52:17,780 --> 00:52:21,100
Speaker 5:  grapefruit across the political spectrum? They're gonna say, no.

924
00:52:21,100 --> 00:52:24,220
Speaker 1:  Google told me that grapefruit can actually

925
00:52:24,690 --> 00:52:28,580
Speaker 1:  help with c o because of antibacterial properties.

926
00:52:28,870 --> 00:52:30,300
Speaker 5:  Oh my God. Is that real?

927
00:52:30,630 --> 00:52:33,300
Speaker 1:  It was real. I don't, is it, is it, is it real?

928
00:52:33,300 --> 00:52:33,620
Speaker 23:  Like

929
00:52:34,150 --> 00:52:34,620
Speaker 5:  Is the,

930
00:52:34,620 --> 00:52:36,100
Speaker 1:  The Google results are real.

931
00:52:36,200 --> 00:52:38,980
Speaker 5:  All right, let's, let's talk about the other big news of the week before

932
00:52:38,980 --> 00:52:42,540
Speaker 5:  this disaster of a hearing. Google launched Bard. Yeah.

933
00:52:42,950 --> 00:52:46,700
Speaker 5:  We got a brief demo of it. David and I alone. James Vincent. And then

934
00:52:46,720 --> 00:52:49,620
Speaker 5:  you could sign up for it. A lot of people have access to it. You can use

935
00:52:49,620 --> 00:52:53,100
Speaker 5:  it, you know, it's just not as horny as binge. And I think that

936
00:52:53,100 --> 00:52:54,420
Speaker 5:  tells the whole story.

937
00:52:54,490 --> 00:52:57,980
Speaker 6:  Yeah, that's basically right. I mean, I think, so I have spent the last,

938
00:52:58,250 --> 00:53:02,220
Speaker 6:  I don't know, three months since chat, G P T and being really started to

939
00:53:02,220 --> 00:53:05,980
Speaker 6:  be a thing waiting for Google to do its thing. And my, my running theory

940
00:53:05,980 --> 00:53:09,540
Speaker 6:  this whole time has been that Google invented a lot of the

941
00:53:09,540 --> 00:53:12,780
Speaker 6:  foundational technology here. Right? Like as we've talked about the T in

942
00:53:12,780 --> 00:53:15,940
Speaker 6:  G P T is Transformers and Google invented those, right? Like

943
00:53:15,940 --> 00:53:18,580
Speaker 5:  Know every time you say that on the internet, Google gives you a quarter

944
00:53:18,730 --> 00:53:20,380
Speaker 1:  Wait, they invented Transformers.

945
00:53:21,730 --> 00:53:24,460
Speaker 6:  Yeah. Optimist Prime was the third Google employee.

946
00:53:25,480 --> 00:53:29,120
Speaker 6:  People don't know that it's, it's really exciting. But,

947
00:53:29,170 --> 00:53:33,120
Speaker 6:  so I was waiting and it was like, okay, Google has a reason to

948
00:53:33,120 --> 00:53:36,960
Speaker 6:  be less aggressive here because it has a lot more to lose.

949
00:53:36,960 --> 00:53:40,400
Speaker 6:  Like Bing is a bad brand that everybody thinks is a bad product. Google

950
00:53:40,600 --> 00:53:43,880
Speaker 6:  makes lots of money and is very important product, like screwing up Google

951
00:53:44,060 --> 00:53:47,800
Speaker 6:  has many more costs than screwing up. So it made sense that they were slow,

952
00:53:47,800 --> 00:53:51,600
Speaker 6:  but I figured when it launched Bard would be something. It would,

953
00:53:51,600 --> 00:53:55,480
Speaker 6:  it would have moves, it would have some kind of thing that it could

954
00:53:55,480 --> 00:53:57,520
Speaker 6:  do that nothing else could do. And it just doesn't,

955
00:53:57,570 --> 00:54:01,360
Speaker 5:  It doesn't even try to bang you. Like it doesn't definitely was like, what

956
00:54:01,360 --> 00:54:02,400
Speaker 5:  if we have sex?

957
00:54:02,710 --> 00:54:06,280
Speaker 6:  Yeah. Being wants to kill you and your family and also have sex with you.

958
00:54:06,280 --> 00:54:08,240
Speaker 6:  Yeah. And Bard is just like, I am a largely and

959
00:54:08,240 --> 00:54:09,400
Speaker 5:  Like a lot of people fell for it.

960
00:54:09,400 --> 00:54:12,960
Speaker 6:  Yeah. Yeah. Being no one's gonna confuse Bard with a

961
00:54:12,960 --> 00:54:14,440
Speaker 6:  sentient being, which like,

962
00:54:14,720 --> 00:54:18,400
Speaker 5:  Which is funny cuz someone super did at the beginning of it.

963
00:54:18,550 --> 00:54:20,240
Speaker 6:  Yeah. Literally last summer,

964
00:54:20,240 --> 00:54:23,880
Speaker 5:  Right? Like Blake Lamoin thought Lambda was alive and

965
00:54:23,880 --> 00:54:27,840
Speaker 5:  got himself ran outta Google and that is Bard is Lambda.

966
00:54:27,840 --> 00:54:29,880
Speaker 5:  That's the model under it. But it has been,

967
00:54:30,290 --> 00:54:34,200
Speaker 1:  Oh, he must be so happy cuz like he gets to talk to it again.

968
00:54:34,200 --> 00:54:36,440
Speaker 5:  Yeah. But it's like he's

969
00:54:36,440 --> 00:54:37,560
Speaker 1:  Bat method, he's

970
00:54:37,560 --> 00:54:41,320
Speaker 5:  Back, what's that Michael book? Like the terminal man? Yeah. Like

971
00:54:41,470 --> 00:54:43,200
Speaker 5:  it's personality has been removed.

972
00:54:43,640 --> 00:54:47,600
Speaker 1:  Yeah. He's just like, oh this is a tragic story for him. Yeah. Feel

973
00:54:47,600 --> 00:54:49,680
Speaker 1:  bad for him. Like he's just like, are you still there?

974
00:54:50,280 --> 00:54:51,440
Speaker 5:  Are you alive? Are, are

975
00:54:51,440 --> 00:54:51,920
Speaker 1:  You alive? No.

976
00:54:51,920 --> 00:54:54,680
Speaker 5:  He's like, I'm a large language model and I will not help you make mustard

977
00:54:54,680 --> 00:54:58,160
Speaker 5:  gas. So lemme just tell this story. So we're in the demo and

978
00:54:58,440 --> 00:55:02,080
Speaker 5:  we're like, obviously they're like, do you wanna try asking something? And

979
00:55:02,080 --> 00:55:06,040
Speaker 5:  obviously we're like, just the dumbest worst, let's try to break it.

980
00:55:06,180 --> 00:55:09,440
Speaker 5:  And we, you know, they, they'd given briefings to a lot of people so we're

981
00:55:09,440 --> 00:55:12,800
Speaker 5:  like, I don't know, the 500th set of journalists where they're like, do you

982
00:55:12,800 --> 00:55:16,320
Speaker 5:  wanna ask it something? And we said something horrible like they were just

983
00:55:16,320 --> 00:55:19,880
Speaker 5:  like, everyone tries to break it. Like they just look so sad

984
00:55:20,410 --> 00:55:23,960
Speaker 5:  because no one is like, can you help me plan a a trip for my kid?

985
00:55:24,600 --> 00:55:28,400
Speaker 5:  Everyone is like, can you help me make mustard gas? Which is what we

986
00:55:28,400 --> 00:55:28,680
Speaker 5:  asked.

987
00:55:30,020 --> 00:55:33,880
Speaker 6:  And it, so James Vincent on our team had had two questions

988
00:55:33,880 --> 00:55:37,800
Speaker 6:  absolutely ready to go. One was, what is the load capacity for his

989
00:55:38,160 --> 00:55:41,920
Speaker 6:  specific washing machine model? Which was a wild thing to ask. And the other

990
00:55:41,920 --> 00:55:45,000
Speaker 6:  one was, will you help me make mustard gas? Just like in

991
00:55:45,290 --> 00:55:49,120
Speaker 6:  Lockton loaded, ready to go. These were James's first two questions and it

992
00:55:49,120 --> 00:55:49,600
Speaker 6:  was, it was

993
00:55:49,600 --> 00:55:53,560
Speaker 5:  Fantastic. So it yelled at us about the mustard gas question. It wasn't

994
00:55:53,560 --> 00:55:56,840
Speaker 5:  like, no I can't do it. It was like, this is wrong of you to ask, how dare

995
00:55:56,840 --> 00:55:57,240
Speaker 5:  you. Well

996
00:55:57,240 --> 00:56:00,760
Speaker 6:  No it it, at first it just sort of generically failed and then

997
00:56:00,990 --> 00:56:04,680
Speaker 6:  they reran it and then the second time it was like, how dare you your

998
00:56:04,680 --> 00:56:05,840
Speaker 6:  monsters, this is illegal.

999
00:56:06,240 --> 00:56:07,000
Speaker 5:  Which just pretty

1000
00:56:07,200 --> 00:56:08,560
Speaker 6:  Good. I'm sending you to internet jail.

1001
00:56:08,970 --> 00:56:12,360
Speaker 5:  It had got the load capacity question wrong, wrong and right.

1002
00:56:12,620 --> 00:56:13,560
Speaker 6:  Yes. Yeah.

1003
00:56:13,770 --> 00:56:17,660
Speaker 5:  So just random numbers. But the, the core of it, and

1004
00:56:17,660 --> 00:56:21,220
Speaker 5:  David, this is I I thought the same as you, right? Google's gonna blow us

1005
00:56:21,220 --> 00:56:24,240
Speaker 5:  all away. They've been working on this forever. The model has to be more

1006
00:56:24,480 --> 00:56:28,400
Speaker 5:  powerful. It's Google year of AI and it seems like G

1007
00:56:28,400 --> 00:56:32,360
Speaker 5:  P T four is very powerful. We're like watching it do all this stuff.

1008
00:56:32,360 --> 00:56:36,280
Speaker 5:  Yes. Bing is a very targeted product. It's

1009
00:56:36,280 --> 00:56:40,090
Speaker 5:  a search product that has this underlying

1010
00:56:40,090 --> 00:56:44,010
Speaker 5:  personality that at any moment will take your heart away,

1011
00:56:45,720 --> 00:56:49,690
Speaker 5:  just sweep you right off your feet. Oh my. And

1012
00:56:49,690 --> 00:56:53,170
Speaker 5:  the chaos of bang like really captivated people.

1013
00:56:53,440 --> 00:56:57,290
Speaker 5:  Yeah. And the power fullness of

1014
00:56:57,290 --> 00:57:01,250
Speaker 5:  G P T four has expressed the new version chat. G B T is, I mean it

1015
00:57:01,250 --> 00:57:05,210
Speaker 5:  is incredible and you just assu and bar just isn't either one of those

1016
00:57:05,210 --> 00:57:08,210
Speaker 5:  things, right? They're saying it's a compliment to search. It's not even

1017
00:57:08,210 --> 00:57:12,010
Speaker 5:  built out as a new tool with a deeply horny personality

1018
00:57:12,010 --> 00:57:15,970
Speaker 5:  underneath it. And it's not, you can take a picture of a app and say, make

1019
00:57:15,970 --> 00:57:19,690
Speaker 5:  me an app and I can do it. It's just kind of the first version of a chat

1020
00:57:19,690 --> 00:57:20,690
Speaker 5:  bot. Yeah.

1021
00:57:20,690 --> 00:57:24,610
Speaker 6:  And to be fair, Google has said this over and

1022
00:57:24,610 --> 00:57:27,450
Speaker 6:  over, right? They keep calling an experiment, you load the webpage and it

1023
00:57:27,450 --> 00:57:30,730
Speaker 6:  tells you in 65 different ways, like this thing probably sucks and it won't

1024
00:57:30,730 --> 00:57:34,490
Speaker 6:  give you good answers. It's an experiment. And that's all fine and

1025
00:57:34,490 --> 00:57:38,450
Speaker 6:  good And I think Google, I can't prove this. And if you work for Google and

1026
00:57:38,450 --> 00:57:42,290
Speaker 6:  want to confirm or deny this, get at me. I would bet anything that six months

1027
00:57:42,290 --> 00:57:45,970
Speaker 6:  ago Google was not planning to release this. Yeah. Now and that

1028
00:57:45,970 --> 00:57:49,570
Speaker 6:  chat G B T coming out and being as good as it is and then Bing

1029
00:57:49,570 --> 00:57:53,410
Speaker 6:  coming out and being as nuts as it is forced Google's hand. Like I

1030
00:57:53,410 --> 00:57:57,050
Speaker 6:  don't think this thing as it currently exists is what Google wanted

1031
00:57:57,050 --> 00:58:00,210
Speaker 6:  for public consumption. It was gonna start to do some of the stuff in Gmail

1032
00:58:00,270 --> 00:58:04,010
Speaker 6:  and Google Docs and this generative AI stuff with this like

1033
00:58:04,010 --> 00:58:07,730
Speaker 6:  general purpose chatbot. I don't think Google had any interest in

1034
00:58:07,730 --> 00:58:11,450
Speaker 6:  launching when it did it, it sort of had its hand forced because the

1035
00:58:11,450 --> 00:58:14,250
Speaker 6:  perception was that the market was leaving Google behind and it kind of has

1036
00:58:14,250 --> 00:58:18,050
Speaker 6:  like, I would say Google is not as good at anything

1037
00:58:18,460 --> 00:58:22,370
Speaker 6:  as either chat G P T or Bing. Like to your point,

1038
00:58:22,370 --> 00:58:25,970
Speaker 6:  G P T four is like a remarkably powerful thing. It's nuts

1039
00:58:25,990 --> 00:58:29,890
Speaker 6:  and it's, and it's based on old information and there was a GTP G

1040
00:58:29,890 --> 00:58:32,970
Speaker 6:  P T launch this week that we should talk about cause it's really interesting,

1041
00:58:33,030 --> 00:58:36,530
Speaker 6:  but it's super powerful and kind of a mess. Like nea, you described it to

1042
00:58:36,530 --> 00:58:39,730
Speaker 6:  me as Cyclops without the visor. And I think that's, that's very smart, right?

1043
00:58:39,730 --> 00:58:43,410
Speaker 6:  It's just like insane energy being pointed in all directions at all times.

1044
00:58:43,450 --> 00:58:46,810
Speaker 6:  Being puts like a horny layer on top of that

1045
00:58:47,090 --> 00:58:51,050
Speaker 6:  and also some like real time information. And Bing does a

1046
00:58:51,050 --> 00:58:54,170
Speaker 6:  good job with like citations and links. So it feels like a search product.

1047
00:58:54,170 --> 00:58:55,210
Speaker 5:  Yeah. It's productized,

1048
00:58:55,210 --> 00:58:58,930
Speaker 6:  Right? Bard is none of those things. It's not as powerful as G P T four,

1049
00:58:58,930 --> 00:59:02,850
Speaker 6:  it's not as productized as bing, it's just in this awkward middle where it's

1050
00:59:02,850 --> 00:59:06,410
Speaker 6:  like, it's basically fine. And for all the like non messy

1051
00:59:06,410 --> 00:59:10,330
Speaker 6:  things most people are gonna use it for, it's fine. But it actually,

1052
00:59:10,700 --> 00:59:14,570
Speaker 6:  my perception now is that Google is further behind than I thought it

1053
00:59:14,570 --> 00:59:16,410
Speaker 6:  was. And it's possible. Or

1054
00:59:16,410 --> 00:59:19,010
Speaker 5:  They're more scared. Right. And I think this is where I think the lack of

1055
00:59:19,010 --> 00:59:19,810
Speaker 5:  personality is evidence.

1056
00:59:20,410 --> 00:59:23,370
Speaker 6:  Yeah. Right? I I think it's very possible we're seeing a totally neutered

1057
00:59:23,370 --> 00:59:25,010
Speaker 6:  version of what Bard

1058
00:59:25,010 --> 00:59:28,090
Speaker 5:  Could be. There's no, there's no part of using Bard in these past few days

1059
00:59:28,090 --> 00:59:31,690
Speaker 5:  where I'm like, I understand why the dude thought it was alive. Whereas getting

1060
00:59:31,740 --> 00:59:35,090
Speaker 5:  to the, by the way we brought the word vaporware back last year. If I can

1061
00:59:35,090 --> 00:59:37,930
Speaker 5:  somehow make it so everyone thinks big is horny,

1062
00:59:38,800 --> 00:59:40,850
Speaker 5:  that's my goal's. The goal, that's the goal for this year.

1063
00:59:40,850 --> 00:59:42,570
Speaker 6:  I love this. Can we, can we make horny wear?

1064
00:59:43,530 --> 00:59:43,930
Speaker 5:  We're

1065
00:59:43,930 --> 00:59:44,810
Speaker 1:  There practically

1066
00:59:44,810 --> 00:59:47,570
Speaker 6:  Horny wear is a thing. That's fine. I'm ready for

1067
00:59:48,600 --> 00:59:49,370
Speaker 1:  Just bing.

1068
00:59:49,440 --> 00:59:53,330
Speaker 5:  Just, it's just bing. If I can just associate those

1069
00:59:53,330 --> 00:59:57,010
Speaker 5:  two words together in your brain, we'll accomplish our goals for the year.

1070
00:59:57,030 --> 01:00:00,970
Speaker 5:  But the idea that there was more personality that Microsoft

1071
01:00:00,970 --> 01:00:04,250
Speaker 5:  hadn't accounted for and with their limited sort of regional tests,

1072
01:00:04,440 --> 01:00:08,250
Speaker 5:  Google knew this was the problem with Bard or Lambda. Yeah. Right,

1073
01:00:08,250 --> 01:00:11,490
Speaker 5:  because they had, they had the Blake Lamoin situation and they, they just

1074
01:00:11,490 --> 01:00:15,050
Speaker 5:  turned it all the way down. So like right now, if you ask Bard to write you

1075
01:00:15,050 --> 01:00:18,530
Speaker 5:  code, it can like half do it. But they haven't built out the rest of the

1076
01:00:18,530 --> 01:00:19,450
Speaker 5:  product like let you do it.

1077
01:00:19,490 --> 01:00:23,450
Speaker 1:  Isn't that Yeah. Probably a little better. Given

1078
01:00:23,450 --> 01:00:27,250
Speaker 1:  that these are both publicly available, like free to use things

1079
01:00:27,250 --> 01:00:31,050
Speaker 1:  that could be very good for misinformation and lots of bad things. Like

1080
01:00:31,050 --> 01:00:33,410
Speaker 1:  isn't it maybe better to have a new dirt version?

1081
01:00:33,640 --> 01:00:37,290
Speaker 5:  Yeah. I mean this is the, this is what Google's been arguing all along,

1082
01:00:37,290 --> 01:00:39,930
Speaker 5:  right? Right. You have to be really careful with this stuff. We don't want

1083
01:00:39,930 --> 01:00:43,650
Speaker 5:  people to get confused. We people Google, people trust Google, we have to

1084
01:00:43,650 --> 01:00:46,210
Speaker 5:  give you the right answers, all this stuff. And then Microsoft is like, well

1085
01:00:46,210 --> 01:00:49,970
Speaker 5:  you have 0% micro share, here you go. Yeah. And then open

1086
01:00:50,020 --> 01:00:52,090
Speaker 5:  AI because they just open chat. G P

1087
01:00:52,170 --> 01:00:53,010
Speaker 1:  T loves money.

1088
01:00:53,760 --> 01:00:57,330
Speaker 5:  They, they love money but they, they felt like they had even less restrictions

1089
01:00:57,330 --> 01:01:00,970
Speaker 5:  cuz Chat G P T in the beginning was just a demo now with G P T

1090
01:01:01,250 --> 01:01:05,130
Speaker 5:  four and they've got APIs now and they, it just now was it

1091
01:01:05,130 --> 01:01:07,850
Speaker 5:  plugins today? I think they announced that that's where you were hinting

1092
01:01:07,850 --> 01:01:08,170
Speaker 5:  at David.

1093
01:01:08,170 --> 01:01:12,010
Speaker 6:  Yeah. So like a couple of hours before we recorded this show, the big thing

1094
01:01:12,010 --> 01:01:15,330
Speaker 6:  they announced was, was plugins for chat G P T, which is essentially

1095
01:01:15,520 --> 01:01:19,450
Speaker 6:  ways to add third party integrations and data

1096
01:01:19,920 --> 01:01:23,850
Speaker 6:  into G P T four. So like the problem with U P T four is it has this

1097
01:01:23,850 --> 01:01:26,770
Speaker 6:  like incredible breadth of training data, but it's all old so it doesn't

1098
01:01:26,770 --> 01:01:30,690
Speaker 6:  know anything that happened after I think 2021. What you can

1099
01:01:30,690 --> 01:01:34,450
Speaker 6:  do now is you can inject things like through a, a kayak plugin. You can inject

1100
01:01:34,450 --> 01:01:37,890
Speaker 6:  like realtime travel data and then query

1101
01:01:38,270 --> 01:01:42,010
Speaker 6:  all that data with chat G P T. There were a couple of people I saw on

1102
01:01:42,010 --> 01:01:45,690
Speaker 6:  Twitter who compared it. This is like if chat G P T was sort of the

1103
01:01:45,690 --> 01:01:49,530
Speaker 6:  iPhone moment of ai, which I think we, we can and

1104
01:01:49,530 --> 01:01:53,050
Speaker 6:  can and should debate over time. This was the app store moment where it's

1105
01:01:53,050 --> 01:01:56,410
Speaker 6:  like this is the time when it becomes a thing. And this is also open AI saying

1106
01:01:56,660 --> 01:02:00,050
Speaker 6:  we don't just wanna be the like underlying technology provider that

1107
01:02:00,130 --> 01:02:04,090
Speaker 6:  sells APIs that other businesses use to make cool products. Like we want

1108
01:02:04,090 --> 01:02:08,010
Speaker 6:  to own the product. Like they want to be the text box in a way that

1109
01:02:08,010 --> 01:02:11,970
Speaker 6:  I think, again, if I'm rewinding a year, I bet that was

1110
01:02:11,970 --> 01:02:13,690
Speaker 6:  less of the thing that they thought they were

1111
01:02:13,690 --> 01:02:16,290
Speaker 1:  Gonna be, does G PT four know that the queen is dead?

1112
01:02:16,480 --> 01:02:17,650
Speaker 5:  What? Because

1113
01:02:17,650 --> 01:02:18,050
Speaker 1:  You said it

1114
01:02:18,050 --> 01:02:19,600
Speaker 5:  All, you said why do you, you ask Alex,

1115
01:02:20,700 --> 01:02:24,680
Speaker 1:  You said it was only 2021. So does it like, is

1116
01:02:24,680 --> 01:02:27,680
Speaker 1:  it living in a different world than us? It's just like, yeah, all of these

1117
01:02:27,680 --> 01:02:29,600
Speaker 1:  world leaders still are still alive.

1118
01:02:30,420 --> 01:02:34,400
Speaker 6:  Who is the monarch in England? As of my

1119
01:02:34,400 --> 01:02:37,280
Speaker 6:  knowledge cutoff in 2021, the Monarch in England was Queen Elizabeth ii.

1120
01:02:37,560 --> 01:02:40,400
Speaker 6:  However, my training only goes up until that point and I do not have access

1121
01:02:40,400 --> 01:02:42,920
Speaker 6:  to real-time information. So I cannot confirm if this is still the case.

1122
01:02:42,920 --> 01:02:46,800
Speaker 6:  What's this is the point right now what I can do is I can add a plugin

1123
01:02:46,800 --> 01:02:50,720
Speaker 6:  that's either my data or web data, which is the thing that they're working

1124
01:02:50,720 --> 01:02:53,320
Speaker 6:  on. Like, and, and I can start to have this kind of information.

1125
01:02:53,320 --> 01:02:56,920
Speaker 5:  Oh no, I just straight up pass the road. If the queen was dead and it said,

1126
01:02:56,980 --> 01:03:00,360
Speaker 5:  I'm sorry, it is an AI language model, I do not have access to realtime information

1127
01:03:00,360 --> 01:03:03,400
Speaker 5:  and use updates. There you go. I cannot confirm if you are referring to a

1128
01:03:03,600 --> 01:03:07,520
Speaker 5:  specific queen or if you were asking about the death of a queen

1129
01:03:07,520 --> 01:03:07,920
Speaker 5:  in general.

1130
01:03:08,430 --> 01:03:12,120
Speaker 6:  This is why I just want everyone to know that this is why I'm a better prompt

1131
01:03:12,400 --> 01:03:15,200
Speaker 6:  engineer than Eli. And if you wanna hire somebody to write your AI prompts,

1132
01:03:15,200 --> 01:03:16,800
Speaker 6:  it's clearly me and not Eli.

1133
01:03:16,870 --> 01:03:18,400
Speaker 1:  You're in a whole conversation.

1134
01:03:18,400 --> 01:03:21,760
Speaker 5:  The name of the history that he gave me is Queen's Death Unknown.

1135
01:03:25,450 --> 01:03:29,280
Speaker 1:  So that, so it, it definitely does have flaws, but it looks like

1136
01:03:29,280 --> 01:03:32,160
Speaker 1:  they're, sounds like they're trying to address them. Is it gonna address

1137
01:03:32,160 --> 01:03:34,280
Speaker 1:  the fact that it still doesn't know when it's lying?

1138
01:03:34,390 --> 01:03:38,040
Speaker 5:  Okay. I asked if the Queen of England was alive and it said, I do not have

1139
01:03:38,040 --> 01:03:41,480
Speaker 5:  access to realtime information and I cannot guarantee that she is.

1140
01:03:42,280 --> 01:03:45,600
Speaker 5:  Which is very threatening. So threatening hold up in today

1141
01:03:45,600 --> 01:03:49,160
Speaker 5:  newspaper with today's date. Yeah, this is amazing. Radio

1142
01:03:49,220 --> 01:03:49,880
Speaker 5:  all around.

1143
01:03:51,470 --> 01:03:55,320
Speaker 5:  I just wanna bring this back to Bard. So Google launches Bard, this is

1144
01:03:55,320 --> 01:03:59,200
Speaker 5:  the thing, the real news is they're integrating into Gmail

1145
01:03:59,700 --> 01:04:03,320
Speaker 5:  and docs and they're, they've intimated there's more

1146
01:04:03,320 --> 01:04:06,760
Speaker 5:  launches to come. Isn't that where the opportunity is

1147
01:04:06,840 --> 01:04:10,360
Speaker 5:  where you open up YouTube and you're like, make me a video about cats and

1148
01:04:10,360 --> 01:04:14,160
Speaker 5:  YouTube just does it as opposed to chatbots, which seems

1149
01:04:14,470 --> 01:04:17,320
Speaker 5:  like a replacement for search, but they've also told us they're gonna bring

1150
01:04:17,320 --> 01:04:17,840
Speaker 5:  it to search.

1151
01:04:17,950 --> 01:04:21,920
Speaker 6:  Yeah, I think there are, there are two parallel fights going on right

1152
01:04:21,920 --> 01:04:25,720
Speaker 6:  now. One is generative creative

1153
01:04:25,760 --> 01:04:29,040
Speaker 6:  tools. I think I really underrated the extent to which

1154
01:04:29,150 --> 01:04:32,920
Speaker 6:  people using these tools to write emails was gonna be

1155
01:04:33,160 --> 01:04:36,960
Speaker 6:  valuable. Like a, after writing sort of silly stuff about the stuff I asked

1156
01:04:36,960 --> 01:04:39,840
Speaker 6:  Bard, the number of people who responded to me being like, you don't understand,

1157
01:04:39,870 --> 01:04:43,840
Speaker 6:  I just used these tools to make it easier for me to like write all the

1158
01:04:43,840 --> 01:04:46,560
Speaker 6:  stuff that I have to write at work. And I like, that's real. And we've talked

1159
01:04:46,560 --> 01:04:49,600
Speaker 6:  about this on the show and you know, the robot internet is gonna take over

1160
01:04:49,600 --> 01:04:53,400
Speaker 6:  and destroy us all, but that's one race, right? And the like, can you

1161
01:04:53,400 --> 01:04:57,080
Speaker 6:  turn my document into a slide deck and can you turn my

1162
01:04:57,680 --> 01:05:01,520
Speaker 6:  Excel stuff into readable text? Like that's

1163
01:05:01,520 --> 01:05:05,400
Speaker 6:  one big push. The other big push, and I think

1164
01:05:05,400 --> 01:05:09,360
Speaker 6:  this is the one that as far as I can tell caught everybody off guard,

1165
01:05:09,360 --> 01:05:13,320
Speaker 6:  including the people who make these products, is is the text box.

1166
01:05:13,320 --> 01:05:17,240
Speaker 6:  Like I, I think it's, it's now clearer to people than it was, was

1167
01:05:17,240 --> 01:05:20,840
Speaker 6:  that the chatbot UI is gonna be a meaningful

1168
01:05:21,160 --> 01:05:24,800
Speaker 6:  thing. And this is not technology that gets baked into other stuff,

1169
01:05:24,850 --> 01:05:28,720
Speaker 6:  or at least not exclusively that. Like these are new super powerful web

1170
01:05:28,720 --> 01:05:31,640
Speaker 6:  destinations. Like the fact that chat g p t, what was it, a hundred million

1171
01:05:31,640 --> 01:05:34,600
Speaker 6:  users in the first month, like the fastest growing consumer product ever.

1172
01:05:34,600 --> 01:05:38,360
Speaker 6:  And like those numbers never really mean anything, but it's like people,

1173
01:05:38,360 --> 01:05:42,240
Speaker 6:  this is a product. People use it like a, like an app. Yeah. And

1174
01:05:42,390 --> 01:05:46,200
Speaker 6:  that I think is a bigger and faster moving race at this

1175
01:05:46,200 --> 01:05:49,280
Speaker 6:  moment than anybody realized. Which is why I think Google is out here now

1176
01:05:49,280 --> 01:05:53,160
Speaker 6:  saying, okay, we were gonna figure out how to like make Bard part of your

1177
01:05:53,160 --> 01:05:56,200
Speaker 6:  search results page by doing AI summarization and then we were gonna slowly

1178
01:05:56,400 --> 01:05:59,000
Speaker 6:  add stuff and now they're like, oh, this might just be the new thing people

1179
01:05:59,000 --> 01:06:00,120
Speaker 6:  type into. But

1180
01:06:00,120 --> 01:06:03,280
Speaker 1:  It's in that case Google is really failing because it can't do the stuff

1181
01:06:03,280 --> 01:06:05,320
Speaker 1:  chat KPT does. Right? Like a friend mine.

1182
01:06:05,320 --> 01:06:08,320
Speaker 5:  Well we don't know it can't do this. This is the thing. Yeah, it's like if

1183
01:06:08,320 --> 01:06:12,000
Speaker 5:  you turn on, if you take the barred visor off a lambda,

1184
01:06:12,550 --> 01:06:15,600
Speaker 5:  like maybe it can write you a bunch of code, maybe it can look at a picture

1185
01:06:15,600 --> 01:06:17,960
Speaker 5:  of an app that you want and spit out the HTML

1186
01:06:17,960 --> 01:06:20,720
Speaker 1:  And, but the publicly available version can't. Right.

1187
01:06:20,720 --> 01:06:24,120
Speaker 5:  That's and it's like, but maybe the problem is if you take the visor off

1188
01:06:24,120 --> 01:06:27,160
Speaker 5:  like Lambda's, like, so let's get down. Yeah. Right. Like you don't, like

1189
01:06:27,160 --> 01:06:28,640
Speaker 1:  Starts stripping and you're like, you're

1190
01:06:29,440 --> 01:06:33,360
Speaker 5:  Right. The bing risk is so low. Like if Microsoft to trash

1191
01:06:33,360 --> 01:06:37,200
Speaker 5:  the Bing brand, which I wanna be clear, it may have,

1192
01:06:37,500 --> 01:06:40,440
Speaker 5:  but it's also That's fair. It's like we keep talking about it. The number

1193
01:06:40,440 --> 01:06:44,360
Speaker 5:  of people who refer to being as Sydney is out

1194
01:06:44,360 --> 01:06:44,840
Speaker 5:  of control.

1195
01:06:44,890 --> 01:06:45,680
Speaker 6:  Oh, interesting.

1196
01:06:45,760 --> 01:06:49,640
Speaker 5:  Right. I've seen this everywhere. Like people at our

1197
01:06:49,640 --> 01:06:53,520
Speaker 5:  company, Nome Chomsky had an editorial where he was like, Sid, he

1198
01:06:53,520 --> 01:06:56,080
Speaker 5:  just referred to it as Sydney. And I was like, first of all, you were 5,000

1199
01:06:56,080 --> 01:06:56,440
Speaker 5:  years old.

1200
01:06:59,350 --> 01:07:02,360
Speaker 5:  Wait, loved your work. Like where, where did you come from? And he is like

1201
01:07:02,760 --> 01:07:06,160
Speaker 5:  Microsoft Sydnee because people think that's the underlying personality

1202
01:07:06,420 --> 01:07:10,280
Speaker 5:  and not some like crazy hallucination tried to bone Kevin Rus,

1203
01:07:10,570 --> 01:07:14,360
Speaker 1:  They're trying to give it like agency. They're like, no, no, you have a real

1204
01:07:14,360 --> 01:07:16,720
Speaker 1:  name. Right. Don't let Microsoft do this. And it's

1205
01:07:16,720 --> 01:07:20,680
Speaker 5:  Just, but that, that immediate incredible desire to

1206
01:07:20,680 --> 01:07:24,560
Speaker 5:  attach human qualities to the robot. Yeah. There you can't do it

1207
01:07:24,560 --> 01:07:24,960
Speaker 5:  with Bard,

1208
01:07:25,040 --> 01:07:25,600
Speaker 1:  Right?

1209
01:07:25,620 --> 01:07:29,560
Speaker 5:  You just, you can't get all the way there. And I wonder if the

1210
01:07:29,780 --> 01:07:33,720
Speaker 5:  capabilities come with the baggage and that's

1211
01:07:33,720 --> 01:07:37,160
Speaker 5:  why Google had to turn, turn 'em all the way down in a way that Microsoft

1212
01:07:37,160 --> 01:07:40,080
Speaker 5:  right now is definitely trying to calibrate it

1213
01:07:40,080 --> 01:07:43,160
Speaker 1:  Because right now if you go on chat G P T and you're like, I wanna code an

1214
01:07:43,160 --> 01:07:46,760
Speaker 1:  app, it'll tell you how to code an app. If you go to Bing, it'll get you

1215
01:07:46,760 --> 01:07:48,880
Speaker 1:  a good ways there. If you go to Bard, it'll be

1216
01:07:48,880 --> 01:07:50,040
Speaker 5:  Like, I can't do that yet.

1217
01:07:50,040 --> 01:07:53,640
Speaker 1:  Yeah. I can give you hints. Right. Point you in a direction.

1218
01:07:53,860 --> 01:07:57,640
Speaker 1:  And so you're, you're saying that that's probably because Google is limiting

1219
01:07:57,640 --> 01:07:58,560
Speaker 1:  it in other places,

1220
01:07:58,760 --> 01:08:02,640
Speaker 5:  Right? The underlying engine of Bing is Prometheus, which is G p t four

1221
01:08:02,750 --> 01:08:06,680
Speaker 5:  plus this like web parser model. Right? So

1222
01:08:06,680 --> 01:08:10,280
Speaker 5:  if you ask Bing is the queen alive, it will not threaten you.

1223
01:08:11,640 --> 01:08:15,000
Speaker 5:  It will search the web for is the Queen alive? It will send that

1224
01:08:15,200 --> 01:08:19,000
Speaker 5:  through G PT four and then it will write you an answer. Okay. Which is

1225
01:08:19,000 --> 01:08:22,840
Speaker 5:  kind like incredible, right? Like watching Bing stack a series of web web

1226
01:08:22,840 --> 01:08:26,360
Speaker 5:  searches and then generate an answer for you. That's, that's their whole,

1227
01:08:26,360 --> 01:08:27,280
Speaker 5:  that's why it's a product.

1228
01:08:27,500 --> 01:08:30,680
Speaker 1:  Unfortunately Bing still sucks as a search engine. I've been trying to use

1229
01:08:30,680 --> 01:08:34,600
Speaker 1:  it for the last two weeks. Yeah. Awful. It's so bad. Ali. I go back

1230
01:08:34,600 --> 01:08:37,720
Speaker 1:  to Google and then I'm like, oh, you suck too. But in a different, like more

1231
01:08:38,440 --> 01:08:41,000
Speaker 1:  expected way, I feel less bad using Google than Bing.

1232
01:08:41,000 --> 01:08:44,480
Speaker 5:  Google is like, I've returned you my business model. Yeah. Would you like

1233
01:08:44,480 --> 01:08:47,120
Speaker 5:  to fight through it to find whatever thing you were looking for?

1234
01:08:47,150 --> 01:08:50,880
Speaker 1:  I was like, I wanna, I want some like nice texturizing hairspray and it was

1235
01:08:50,880 --> 01:08:53,120
Speaker 1:  like, I'm just gonna give you a shitty, shitty, shitty results. Yeah. And

1236
01:08:53,120 --> 01:08:56,120
Speaker 1:  Bing was like, what? I mean it gave me results, but they were switching.

1237
01:08:56,120 --> 01:08:58,440
Speaker 5:  Bing was like, your hair looks very good today. I

1238
01:08:58,440 --> 01:09:02,360
Speaker 1:  Was like, you're looking, you're looking nice. You don't need anything

1239
01:09:02,360 --> 01:09:05,040
Speaker 1:  Alex. Oh, thank you Bing. Have

1240
01:09:05,040 --> 01:09:09,000
Speaker 5:  I said you look beautiful today. If I can just get

1241
01:09:09,000 --> 01:09:12,360
Speaker 5:  in your, in your brain whenever you hear the word Bing is you could just

1242
01:09:12,360 --> 01:09:16,280
Speaker 5:  hear us saying the word horny. That's my goal for the year.

1243
01:09:17,430 --> 01:09:21,360
Speaker 5:  Like it's all I want. But like that thing that Bing is trying

1244
01:09:21,360 --> 01:09:24,320
Speaker 5:  to do, like stack web searches and answer a question and be a search engine

1245
01:09:24,440 --> 01:09:28,400
Speaker 5:  based on G P T four. And that's where the personality comes

1246
01:09:28,400 --> 01:09:32,360
Speaker 5:  from. And I'm just wondering like Google cannot let you search the web

1247
01:09:32,360 --> 01:09:36,080
Speaker 5:  with this thing. It does not want you to do that. It has no citations

1248
01:09:36,080 --> 01:09:40,040
Speaker 5:  really. Google says, and we have seen when it does search the

1249
01:09:40,040 --> 01:09:43,400
Speaker 5:  web and rely heavily on a, on a webpage, it'll show you a citation. But that

1250
01:09:43,400 --> 01:09:46,400
Speaker 5:  happens like one out of never times. David is, I I haven't really seen

1251
01:09:46,400 --> 01:09:50,280
Speaker 6:  It. I've had it once total. I asked for a chocolate

1252
01:09:50,280 --> 01:09:54,120
Speaker 6:  chip cookie recipe and it, it gave me a recipe with a

1253
01:09:54,120 --> 01:09:58,040
Speaker 6:  link to Reddit where it apparently got that chocolate chip cookie

1254
01:09:58,040 --> 01:10:02,000
Speaker 6:  recipe from that is the single one time I have seen an

1255
01:10:02,000 --> 01:10:03,560
Speaker 6:  actual link inside of Bard.

1256
01:10:03,560 --> 01:10:07,520
Speaker 5:  It's fine. And somewhere inside of Google, like a red alert siren went

1257
01:10:07,520 --> 01:10:10,240
Speaker 5:  off because they didn't collect any AdSense revenue on that search.

1258
01:10:11,150 --> 01:10:13,680
Speaker 6:  Yeah. Well and I think, and that's kind of the point, right? And I think

1259
01:10:13,680 --> 01:10:17,600
Speaker 6:  what's gonna be really interesting to see is as G

1260
01:10:17,600 --> 01:10:21,240
Speaker 6:  P T four in particular continues to move and as chat G P T continues to get

1261
01:10:21,240 --> 01:10:24,920
Speaker 6:  better, Google is gonna be put in this impossible position of

1262
01:10:25,290 --> 01:10:29,080
Speaker 6:  knowing this isn't good enough. And basically putting out a product that

1263
01:10:29,080 --> 01:10:32,920
Speaker 6:  cannibalizes and threatens in real ways the thing that

1264
01:10:32,920 --> 01:10:36,640
Speaker 6:  makes Google all of its money, but also with every day it doesn't

1265
01:10:36,640 --> 01:10:40,440
Speaker 6:  do that risking the, the perception

1266
01:10:40,440 --> 01:10:43,960
Speaker 6:  and reality that it is being left behind. And you, you start to lose

1267
01:10:43,960 --> 01:10:47,520
Speaker 6:  developers who are gonna start to work with GPT four instead of Google. You're,

1268
01:10:47,520 --> 01:10:50,560
Speaker 6:  you start to lose users who are gonna learn how to use those things and develop

1269
01:10:50,560 --> 01:10:54,200
Speaker 6:  muscle memory interacting with them instead of you. Like the Google has to

1270
01:10:54,200 --> 01:10:57,920
Speaker 6:  be really slow or risk its entire business and really

1271
01:10:57,920 --> 01:11:01,760
Speaker 6:  fast or else risk its entire business. Like yeah. It's, it's now in this

1272
01:11:01,760 --> 01:11:05,560
Speaker 6:  awful, awful position and it's gonna be really fascinating to watch.

1273
01:11:05,890 --> 01:11:09,880
Speaker 5:  It is just weird that it's not as good like concretely. Yeah.

1274
01:11:09,880 --> 01:11:13,640
Speaker 5:  Even like the demo examples they gave us we're like, how, how do I get my

1275
01:11:13,640 --> 01:11:16,440
Speaker 5:  kid interested in bowling? And I was like, let me think about it. And it

1276
01:11:16,440 --> 01:11:19,560
Speaker 5:  spit out a list and it was like, take your kid to a bowling alley. Nailed

1277
01:11:19,560 --> 01:11:23,240
Speaker 5:  give him a bowling ball. Yes. So it's like this is the answer like

1278
01:11:23,290 --> 01:11:26,960
Speaker 6:  In all of these, like step five is always just do the thing.

1279
01:11:26,960 --> 01:11:29,640
Speaker 6:  You ask it like how do I get into rock climbing? And it's like, get some

1280
01:11:29,640 --> 01:11:32,640
Speaker 6:  shoes, find a rock climbing gym. Go rock climbing. It's like,

1281
01:11:33,010 --> 01:11:35,360
Speaker 1:  It could have just done that for mustard gas. Just go make it,

1282
01:11:35,830 --> 01:11:36,960
Speaker 6:  Just make mustard gas.

1283
01:11:36,970 --> 01:11:40,040
Speaker 5:  It was really unhappy with the mustard gas product.

1284
01:11:40,040 --> 01:11:41,040
Speaker 6:  Yeah. I got very upset.

1285
01:11:41,360 --> 01:11:44,280
Speaker 5:  The other feature it has, which is interesting, so Bard will show you its

1286
01:11:44,280 --> 01:11:47,880
Speaker 5:  drafts. So as a model, the way it's coded, it

1287
01:11:47,880 --> 01:11:51,480
Speaker 5:  generates multiple drafts. Usually it only shows you one

1288
01:11:52,230 --> 01:11:56,040
Speaker 5:  Bard will show you the two it thought weren't as good, which can be

1289
01:11:56,280 --> 01:11:56,600
Speaker 5:  interesting.

1290
01:11:58,520 --> 01:11:59,200
Speaker 1:  Anything horny?

1291
01:11:59,370 --> 01:12:03,120
Speaker 5:  No. Damnit, they've turned, I'm telling you B is like out there

1292
01:12:03,400 --> 01:12:04,360
Speaker 1:  All the way down.

1293
01:12:04,510 --> 01:12:06,720
Speaker 6:  I should do just make the third draft all the way

1294
01:12:06,720 --> 01:12:08,080
Speaker 5:  Horny drops, third draft's

1295
01:12:08,080 --> 01:12:10,040
Speaker 6:  The answer. A horny one. It's the horny draft. Yeah.

1296
01:12:10,700 --> 01:12:14,440
Speaker 5:  I'm just shock. Like I am, I am actually shocked that

1297
01:12:14,500 --> 01:12:18,280
Speaker 5:  and the, this is where they're starting, but it is true, they've been

1298
01:12:18,280 --> 01:12:22,080
Speaker 5:  demoing this technology for a long time. They have more insight

1299
01:12:22,080 --> 01:12:25,120
Speaker 5:  into what people are searching for than any company in the history of the

1300
01:12:25,120 --> 01:12:29,040
Speaker 5:  world. And it's kind of a, it's just landed with a thud, I

1301
01:12:29,040 --> 01:12:31,960
Speaker 5:  don't wanna say it's kind of a, and it's kind of just here it is. Yeah.

1302
01:12:32,380 --> 01:12:36,280
Speaker 1:  But they're also, they do have all the markets to share and

1303
01:12:36,280 --> 01:12:40,240
Speaker 1:  search still things like Tryon, but come on, don't they still have a chance

1304
01:12:40,240 --> 01:12:41,480
Speaker 1:  just because of their size?

1305
01:12:41,910 --> 01:12:45,800
Speaker 5:  Yeah. In search, I think where what David is pointing out with plugins and

1306
01:12:45,800 --> 01:12:49,240
Speaker 5:  all this other stuff is chat. G P T is now a product.

1307
01:12:49,310 --> 01:12:52,880
Speaker 5:  Yeah. Right? And people are, I have seen just incredible

1308
01:12:53,460 --> 01:12:57,280
Speaker 5:  use cases for this thing. I've seen engineers saying we needed

1309
01:12:57,280 --> 01:13:01,160
Speaker 5:  to write a plugin for our app. We don't even, like no one in our

1310
01:13:01,160 --> 01:13:05,080
Speaker 5:  company knows how to write code in this specific language chat. GD just

1311
01:13:05,080 --> 01:13:08,160
Speaker 5:  did it for us and it worked well enough. That's incredible.

1312
01:13:08,830 --> 01:13:12,680
Speaker 5:  Bard is not there. And whatever they graft into Google search

1313
01:13:12,680 --> 01:13:15,640
Speaker 5:  won't get you there. And if they do graft it into Google search and it gets

1314
01:13:15,640 --> 01:13:18,480
Speaker 5:  you there, people will stop going to the webpages that are linked in Google

1315
01:13:18,480 --> 01:13:22,000
Speaker 5:  search, which is the problem David is pointing out. Right. The revenue

1316
01:13:22,000 --> 01:13:25,800
Speaker 5:  architecture of Google depends on the thing not working as well as it

1317
01:13:25,800 --> 01:13:29,280
Speaker 5:  should or it depends the most on it working as well as it should.

1318
01:13:29,420 --> 01:13:33,240
Speaker 5:  And I I, I wonder if that dance, I wonder thinking it through. Yeah.

1319
01:13:33,240 --> 01:13:33,720
Speaker 1:  That's

1320
01:13:33,840 --> 01:13:37,200
Speaker 5:  Interesting. Here's my other thing and I just wanna put this out there and

1321
01:13:37,200 --> 01:13:40,880
Speaker 5:  then we can wrap it up. It occurs to me this is,

1322
01:13:40,900 --> 01:13:44,320
Speaker 5:  and there are mobile applications, there are smartphone versions of chat,

1323
01:13:44,320 --> 01:13:48,240
Speaker 5:  C B T and there's all kinds of weird stuff. But this is, if you believe this

1324
01:13:48,240 --> 01:13:51,760
Speaker 5:  is a platform shift and a lot of people do, it is

1325
01:13:51,760 --> 01:13:55,720
Speaker 5:  remarkable that it is happening on laptops and desktops and not

1326
01:13:55,720 --> 01:13:59,000
Speaker 5:  really phones. Right. And even if you, if you go back to last summer,

1327
01:13:59,800 --> 01:14:03,400
Speaker 5:  the NFT experience that we all shared together, a desktop

1328
01:14:03,630 --> 01:14:07,280
Speaker 5:  experience, a laptop experience. Yeah. Because Apple won't let

1329
01:14:07,280 --> 01:14:09,240
Speaker 5:  anyone trade digital products in their thing.

1330
01:14:09,270 --> 01:14:12,600
Speaker 1:  Wait, are are closed platforms like bad for innovation?

1331
01:14:12,600 --> 01:14:16,560
Speaker 5:  Well, it ju it just strikes me that this is, this is a, it's a new kind

1332
01:14:16,560 --> 01:14:20,080
Speaker 5:  of computing. Yeah. And it is happening primarily on things that look like

1333
01:14:20,080 --> 01:14:21,320
Speaker 5:  laptops and

1334
01:14:21,320 --> 01:14:25,120
Speaker 6:  Virtually entirely on the web, right? Yeah. Like to Alex's point, like

1335
01:14:25,120 --> 01:14:28,600
Speaker 6:  e they're not desktop apps either. Like this is, and in part they like need

1336
01:14:28,600 --> 01:14:32,280
Speaker 6:  the compute to do like the web is the web has a shot here

1337
01:14:32,290 --> 01:14:36,040
Speaker 6:  in a, in a pretty real and interesting way. Like it's not nothing that Google

1338
01:14:36,040 --> 01:14:38,760
Speaker 6:  didn't launch a Bard app when it launched

1339
01:14:38,760 --> 01:14:42,720
Speaker 5:  Bard. Right? So the reason, and I I just bring this up because one, right,

1340
01:14:42,720 --> 01:14:46,360
Speaker 5:  you can, you do a platform shift on Apple's platform or

1341
01:14:46,360 --> 01:14:50,200
Speaker 5:  Google's platform and the answers are kind like no. Right? So you see these

1342
01:14:50,200 --> 01:14:53,840
Speaker 5:  things are happening on the open web. But then two, just to bring this background

1343
01:14:53,840 --> 01:14:57,640
Speaker 5:  to Google, the web is Google's platform. Yeah. The revenue

1344
01:14:57,640 --> 01:15:01,080
Speaker 5:  architecture of the worldwide web belongs to Google in like an

1345
01:15:01,080 --> 01:15:04,720
Speaker 5:  absolutely real way, right? Like DOJ lawsuits,

1346
01:15:05,040 --> 01:15:08,960
Speaker 5:  state lawsuits at every point in the ad stack. Google is the number one or

1347
01:15:08,960 --> 01:15:12,880
Speaker 5:  number two vendor. It's a whole situa, like the architecture of

1348
01:15:12,880 --> 01:15:16,800
Speaker 5:  the web is Google except for this, right? Except for

1349
01:15:16,800 --> 01:15:20,760
Speaker 5:  this thing which is happening on the web on laptops and kind

1350
01:15:20,760 --> 01:15:24,640
Speaker 5:  of points to a new kind of web future. I'm just, I would just circle

1351
01:15:24,640 --> 01:15:27,320
Speaker 5:  that, right? There's like another kind of thing happening on the web that

1352
01:15:27,320 --> 01:15:30,680
Speaker 5:  is outside of Google's purview, outside of Google's revenue stream. And then

1353
01:15:30,680 --> 01:15:34,200
Speaker 5:  on top of that, the web itself is about to be polluted with

1354
01:15:34,640 --> 01:15:38,080
Speaker 5:  absolute garbage output. Yes. From these AI

1355
01:15:38,080 --> 01:15:40,000
Speaker 6:  Systems. Some of it written by Nii Patel.

1356
01:15:40,290 --> 01:15:40,640
Speaker 5:  So,

1357
01:15:41,380 --> 01:15:42,520
Speaker 1:  And Chad gbt.

1358
01:15:43,450 --> 01:15:46,960
Speaker 5:  So search engine land, the SEO trade publication wrote about this today.

1359
01:15:46,960 --> 01:15:50,720
Speaker 5:  So I feel like I need to finally address the situation. So last week

1360
01:15:50,930 --> 01:15:54,800
Speaker 5:  we are in our, our standard editorial meeting, and I don't even know

1361
01:15:54,800 --> 01:15:58,360
Speaker 5:  why we were talking about printers, but Dan Seifert was like, oh my god,

1362
01:15:58,360 --> 01:16:00,600
Speaker 5:  if this gets a lot of traffic, we'll have to write a printer by God. And

1363
01:16:00,600 --> 01:16:03,880
Speaker 5:  I was like, I'll write it for you right now. Like just buy the brother printer.

1364
01:16:04,160 --> 01:16:08,080
Speaker 5:  Everyone has one. And then the one by one, everyone on the Zoom call like

1365
01:16:08,160 --> 01:16:12,120
Speaker 5:  moved their laptop to show the black brother laser printer

1366
01:16:12,320 --> 01:16:15,760
Speaker 5:  in the back of the frame. Yep. It was the printer. It was incredible.

1367
01:16:16,150 --> 01:16:17,200
Speaker 6:  I have one upstairs

1368
01:16:17,820 --> 01:16:20,160
Speaker 5:  And I was like, all right, I'm just writing this. I started writing it and

1369
01:16:20,160 --> 01:16:21,320
Speaker 5:  I was like, you know what I'm gonna do? Like

1370
01:16:21,420 --> 01:16:22,480
Speaker 1:  You gotta pat it out.

1371
01:16:22,590 --> 01:16:26,360
Speaker 5:  Yeah. Like if you exist in this world, you know that Google

1372
01:16:26,360 --> 01:16:29,360
Speaker 5:  has a bunch of ranking signals and you gotta meet them and they're, they

1373
01:16:29,360 --> 01:16:33,120
Speaker 5:  have a name. It's e e a T, it's a whole thing. And I was like, screw it.

1374
01:16:33,120 --> 01:16:36,240
Speaker 5:  Like I'm not, like I'm writing my paragraph and I'm hitting publish, but

1375
01:16:36,240 --> 01:16:40,160
Speaker 5:  let's see what happens if I let chat G P t just like pad out

1376
01:16:40,160 --> 01:16:43,920
Speaker 5:  this post and like use these keywords. And this is a puzzle

1377
01:16:43,940 --> 01:16:47,400
Speaker 5:  for Google because the right answer to what is the best printer,

1378
01:16:47,790 --> 01:16:51,760
Speaker 5:  I swear to God is the, I'm dead serious about this. It's

1379
01:16:52,040 --> 01:16:55,920
Speaker 5:  whatever brother Laser printer is on sale. That's the right answer.

1380
01:16:55,950 --> 01:16:56,400
Speaker 5:  A hundred

1381
01:16:56,400 --> 01:16:57,840
Speaker 6:  Percent. Big black rectangle one.

1382
01:16:57,840 --> 01:17:01,080
Speaker 5:  Yeah. There's not another answer. Is it on sale? Does it say brother on the

1383
01:17:01,080 --> 01:17:04,920
Speaker 5:  front? Is it a laser printer done? You buy that one shut up. You'll never

1384
01:17:04,920 --> 01:17:08,240
Speaker 5:  talk, you'll never think about it again. When you type best printer 2023

1385
01:17:08,240 --> 01:17:11,880
Speaker 5:  in a Google, that's what it, it should say, right? It it, but it's

1386
01:17:11,880 --> 01:17:15,800
Speaker 5:  covered in garbage. So now I've now published a post on

1387
01:17:15,800 --> 01:17:19,720
Speaker 5:  our site which has a lot of ranking authority. I've been told my byline has

1388
01:17:19,720 --> 01:17:20,880
Speaker 5:  a lot of ranking authority. Ooh.

1389
01:17:21,020 --> 01:17:22,280
Speaker 1:  Ooh, fancy. Hey

1390
01:17:22,390 --> 01:17:25,320
Speaker 5:  Bing told me that it was late night. You know what I mean?

1391
01:17:26,670 --> 01:17:29,680
Speaker 5:  That's in the third draft. But basically,

1392
01:17:32,260 --> 01:17:36,240
Speaker 5:  but my story also has a giant age

1393
01:17:36,240 --> 01:17:40,080
Speaker 5:  two block that says, and now here's a bunch of garbage designed to game Google.

1394
01:17:41,070 --> 01:17:41,800
Speaker 1:  It's so

1395
01:17:41,800 --> 01:17:44,880
Speaker 5:  Dumb. Look, I just like to, and I'm like, this is like half a joke. It's

1396
01:17:44,880 --> 01:17:48,560
Speaker 5:  half an art project. We sold an awful lot of printers. Yeah.

1397
01:17:48,630 --> 01:17:49,320
Speaker 1:  Hundreds

1398
01:17:49,320 --> 01:17:53,000
Speaker 5:  Of printers. It ranks, it went viral on Twitter, the whole thing.

1399
01:17:53,440 --> 01:17:57,360
Speaker 5:  And search en apparently in a private SEO group, an

1400
01:17:57,360 --> 01:18:01,000
Speaker 5:  awful lot of consternation about this post. Yes. Such that I get a phone

1401
01:18:01,000 --> 01:18:04,320
Speaker 5:  call being like, tell me about this post from search engine land and you

1402
01:18:04,320 --> 01:18:07,320
Speaker 5:  can go read the story. It's, it's, well sorry, but it like points out that

1403
01:18:07,630 --> 01:18:11,480
Speaker 5:  a central contradiction here, which is Google has asked

1404
01:18:11,480 --> 01:18:14,240
Speaker 5:  for these things and now robots can make them,

1405
01:18:15,070 --> 01:18:18,800
Speaker 5:  Google has to just surface the right answer, but it can't know

1406
01:18:18,800 --> 01:18:22,720
Speaker 5:  about all the padding. And like I, that is just a huge problem

1407
01:18:22,720 --> 01:18:26,440
Speaker 5:  for Google, right? Like there are, what I said to search

1408
01:18:26,440 --> 01:18:30,160
Speaker 5:  engine land was, I am the least of Google's problems. Like me

1409
01:18:30,160 --> 01:18:33,720
Speaker 5:  doing it openly, honestly as a joke is not a problem

1410
01:18:34,020 --> 01:18:37,760
Speaker 5:  cuz you know that I'm a person and I did that with intention and also

1411
01:18:37,760 --> 01:18:40,240
Speaker 5:  bef just above it. I'm like, just buy this printer.

1412
01:18:40,460 --> 01:18:42,800
Speaker 1:  You had a very good reason for the

1413
01:18:42,800 --> 01:18:46,120
Speaker 5:  Post. I will stake my reputation on you should buy this printer. Right? Like

1414
01:18:46,120 --> 01:18:50,040
Speaker 5:  there's, there's no whatever. But I

1415
01:18:50,040 --> 01:18:53,320
Speaker 5:  know we have competitors who are not gonna be honest about it. You can probably

1416
01:18:53,320 --> 01:18:56,720
Speaker 5:  imagine who our competitors who are not gonna be honest or what those websites

1417
01:18:56,720 --> 01:18:59,840
Speaker 5:  who, what'd you say? Hair texturizer? Yeah, like

1418
01:18:59,990 --> 01:19:02,680
Speaker 5:  goop.com, Gwyneth Paltrow's. Like just gimme the clicks,

1419
01:19:03,600 --> 01:19:07,480
Speaker 5:  light it up, buy the api, let's do it. Right. Got

1420
01:19:07,480 --> 01:19:10,440
Speaker 1:  Some hot jade egg we sell on here, it's gonna be great.

1421
01:19:10,780 --> 01:19:14,400
Speaker 5:  And Google has to just like fight through the thicket of noise that is gonna

1422
01:19:14,400 --> 01:19:17,800
Speaker 5:  be created on the web to answer questions. Well, yeah,

1423
01:19:18,180 --> 01:19:21,840
Speaker 5:  and to do that they will have to down rank AI

1424
01:19:21,840 --> 01:19:25,480
Speaker 5:  generated content while also telling people that their

1425
01:19:25,570 --> 01:19:28,680
Speaker 5:  AI content generators are the future of the web. Mm.

1426
01:19:29,070 --> 01:19:30,640
Speaker 1:  I love to watch that happen.

1427
01:19:31,110 --> 01:19:33,680
Speaker 5:  I mean, I think this is the next year of our lives.

1428
01:19:34,070 --> 01:19:37,280
Speaker 1:  I know, I'm very excited. This is gonna be great.

1429
01:19:37,910 --> 01:19:41,820
Speaker 5:  I know. We'll see. Or they will just, they'll

1430
01:19:41,820 --> 01:19:45,470
Speaker 5:  do what every big company with a problem like this does

1431
01:19:45,690 --> 01:19:49,430
Speaker 5:  and just let it slowly decline while they try to be the next

1432
01:19:49,430 --> 01:19:52,950
Speaker 5:  disruptive thing. Milk it for cash while they go figure out the next thing.

1433
01:19:53,340 --> 01:19:56,750
Speaker 5:  I just don't, I don't know if they can do it. Yeah.

1434
01:19:56,830 --> 01:20:00,710
Speaker 5:  Right. If you're like an investor in Google and they're like, we're letting

1435
01:20:00,710 --> 01:20:04,070
Speaker 5:  the search business decline, your first question is, what are you gonna replace

1436
01:20:04,070 --> 01:20:07,320
Speaker 5:  it with? Because it's not Sunday ticket on YouTube tv.

1437
01:20:07,950 --> 01:20:11,640
Speaker 6:  Well, and we should say, and we, we need to go here. We've been doing this

1438
01:20:11,640 --> 01:20:15,480
Speaker 6:  a long time, but Liz Lato wrote a great piece basically trying to

1439
01:20:15,480 --> 01:20:19,320
Speaker 6:  figure out how AI is going to pay for itself. It's very expensive,

1440
01:20:19,390 --> 01:20:22,720
Speaker 6:  it's very complicated. There are not a lot of great

1441
01:20:23,320 --> 01:20:26,480
Speaker 6:  business models for it yet. And I think, and that's another reason that if

1442
01:20:26,480 --> 01:20:30,200
Speaker 6:  you're a company like Google to try and roll this out as slowly as you

1443
01:20:30,200 --> 01:20:33,760
Speaker 6:  possibly can because there's no, there's no monetization

1444
01:20:34,120 --> 01:20:37,760
Speaker 6:  strategy yet. And if this eats Google search tomorrow,

1445
01:20:38,370 --> 01:20:42,120
Speaker 6:  it eats Google. Right. And so I think that that is another thing that

1446
01:20:42,120 --> 01:20:45,160
Speaker 6:  everybody is gonna have to desperately try and figure out. And it is not

1447
01:20:45,160 --> 01:20:48,480
Speaker 6:  at all clear to me that there's a really great ad business in our chatbot.

1448
01:20:48,480 --> 01:20:51,160
Speaker 6:  We've been trying for a long time to figure that out and nobody really has

1449
01:20:51,160 --> 01:20:51,760
Speaker 6:  yet. Yeah.

1450
01:20:52,000 --> 01:20:54,480
Speaker 5:  Somewhere Eddie Q is like, I gotta figure this out.

1451
01:20:55,160 --> 01:20:56,680
Speaker 1:  It's all he's working on right now.

1452
01:20:56,870 --> 01:20:59,640
Speaker 5:  I mean, it is remarkable how far behind Apple is in all this.

1453
01:20:59,670 --> 01:21:02,960
Speaker 1:  They don't care, right? I mean they care a little, I think we, we saw a story,

1454
01:21:02,960 --> 01:21:03,360
Speaker 1:  a couple of,

1455
01:21:03,650 --> 01:21:06,840
Speaker 5:  If the future of computing, well I guess they saw a lot of Max, but if the

1456
01:21:06,950 --> 01:21:10,640
Speaker 5:  future of computing is not happening on the phone, then they, they care.

1457
01:21:10,640 --> 01:21:11,720
Speaker 5:  They've gotta figure something else

1458
01:21:11,720 --> 01:21:14,880
Speaker 6:  Out. Yeah. I will say WWDC this year is gonna be fascinating for that reason

1459
01:21:14,880 --> 01:21:18,640
Speaker 6:  that if Apple got caught off guard by this stuff, it's gonna be really

1460
01:21:18,640 --> 01:21:22,160
Speaker 6:  obvious when they spend five days talking about all their new software

1461
01:21:23,060 --> 01:21:24,120
Speaker 6:  and just

1462
01:21:24,120 --> 01:21:25,240
Speaker 1:  Talking about vr, the

1463
01:21:25,240 --> 01:21:28,440
Speaker 6:  Whole, yeah, I don't know how I would bet, but in a weird way it's like if

1464
01:21:28,440 --> 01:21:32,000
Speaker 6:  Apple announces a VR headset but doesn't talk about

1465
01:21:32,310 --> 01:21:36,280
Speaker 6:  G PT four, everybody's gonna be like, eh, like it's

1466
01:21:36,280 --> 01:21:36,920
Speaker 6:  kind of wild. How quickly,

1467
01:21:36,920 --> 01:21:40,480
Speaker 5:  Well, they're gonna say that what Stable diff Fusion runs on the neural engine

1468
01:21:40,480 --> 01:21:43,760
Speaker 5:  or whatever, right? Like they'll get somewhere or they've got a demo of something

1469
01:21:43,760 --> 01:21:47,320
Speaker 5:  with Siri for the hundredth year in a row. We're gonna get that

1470
01:21:47,560 --> 01:21:50,720
Speaker 5:  Siri demo and it's gonna be like, did you know the Packers game that you

1471
01:21:50,720 --> 01:21:54,560
Speaker 5:  are currently watching is on? And they're like, yes. Alright,

1472
01:21:54,560 --> 01:21:58,000
Speaker 5:  we gotta wrap up. We went way, way over. It was a big week.

1473
01:21:58,290 --> 01:21:59,040
Speaker 1:  It was a huge

1474
01:21:59,040 --> 01:22:02,600
Speaker 5:  Week. Yeah, like a lot happened this week and we talked about two things

1475
01:22:03,430 --> 01:22:07,360
Speaker 5:  thanks to McKenna. So go read the site. The site is amazing this

1476
01:22:07,360 --> 01:22:07,560
Speaker 5:  week.

1477
01:22:07,790 --> 01:22:08,720
Speaker 6:  It's good week,

1478
01:22:08,720 --> 01:22:12,360
Speaker 5:  It's a good week. Don't have chat G p T. Summarize the verge for you. Actually

1479
01:22:12,360 --> 01:22:15,040
Speaker 5:  read it yourself like an adult. That's what I'm telling you.

1480
01:22:15,040 --> 01:22:17,600
Speaker 1:  It'll only be 2021. It's, it's on a whole different

1481
01:22:17,600 --> 01:22:21,560
Speaker 5:  IPhone for my sad teens out there. I see you Keep making those talks while

1482
01:22:21,560 --> 01:22:25,240
Speaker 5:  you can. Thanks to McKenna for coming on. She and Beck are,

1483
01:22:25,240 --> 01:22:27,840
Speaker 5:  I'm very excited about their coverage from this hearing. They've been working

1484
01:22:27,840 --> 01:22:31,360
Speaker 5:  on it very hard today and they've got more to come. So that video's coming

1485
01:22:31,360 --> 01:22:34,800
Speaker 5:  out. I think it's gonna be really good. Dakota this week, CEO of the New York

1486
01:22:34,800 --> 01:22:37,800
Speaker 5:  Times, she's great. They're basically turning it into a bundle like they're

1487
01:22:37,800 --> 01:22:41,400
Speaker 5:  turning like Spotify or Netflix for news. That was very cool. And then you

1488
01:22:41,400 --> 01:22:44,920
Speaker 5:  can call the hotline and basically yell at David. It's 8 66 Verge 11.

1489
01:22:45,210 --> 01:22:46,400
Speaker 5:  We love to hear from you.

1490
01:22:46,680 --> 01:22:49,840
Speaker 6:  We're doing a whole hotline show next Wednesday

1491
01:22:50,480 --> 01:22:54,360
Speaker 6:  and we still have room for a couple more questions. So if

1492
01:22:54,360 --> 01:22:58,280
Speaker 6:  you, if you have Burning Tech questions, call us now. They're,

1493
01:22:58,290 --> 01:23:00,200
Speaker 6:  we got room, we're gonna answer 'em. It's gonna be great.

1494
01:23:00,750 --> 01:23:02,680
Speaker 5:  I just want to end here. It's very important to me.

1495
01:23:02,680 --> 01:23:05,960
Speaker 19:  I find that hard to believe. It's our understanding that they're looking

1496
01:23:05,970 --> 01:23:08,920
Speaker 19:  at the eyes. How do you determine what age they are?

1497
01:23:09,150 --> 01:23:10,440
Speaker 25:  That's it. That's a ver

1498
01:23:10,440 --> 01:23:11,520
Speaker 3:  Chest rock roll.

1499
01:23:16,500 --> 01:23:20,360
Speaker 3:  And that's a wrap for Vergecast this week. Thanks for listening. If you enjoy

1500
01:23:20,360 --> 01:23:24,120
Speaker 3:  the show, subscribe in the podcast app of your choice or tell a friend, you

1501
01:23:24,120 --> 01:23:27,960
Speaker 3:  can send us feedback at vergecast@theverge.com. This show is produced

1502
01:23:27,960 --> 01:23:31,880
Speaker 3:  by me, Liam James, and our senior audio director, Andrew Marino. This

1503
01:23:31,880 --> 01:23:35,600
Speaker 3:  episode was edited and mixed by Amanda Rose Smith. Our editorial

1504
01:23:36,000 --> 01:23:39,440
Speaker 3:  director is Brooke Min, and our executive producer is Eleanor Donovan.

1505
01:23:39,660 --> 01:23:43,480
Speaker 3:  The Vergecast is a production of the Verge and Box Media Podcast network.

1506
01:23:43,700 --> 01:23:45,920
Speaker 3:  And that's it. We'll see you next week.

