1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 76ddbe5d-842b-4825-b3cc-4a10b101a15b
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-1827136009249556856/-1609564094464774717/s93290-US-6263s-1738318961.mp3
Description: Nilay and David dig into the week's biggest story: the new Intel-powered Surface Pro. Kidding! They talk about DeepSeek, the out-of-nowhere AI company that sent both Silicon Valley and the stock market into uproar this week. Then, after the hosts debate what the real killer app for AI is — and whether we've even found one yet — we follow up on our question from last week about how people are actually using AI. We got so many good answers, and we talk through what to make of them all. Finally, in the lightning round, we talk about Brendan Carr being a dummy, the return of the Pebble, the continued rise of Bluesky and Threads, and Meta's $25 million check to Trump.

2
00:01:37,585 --> 00:01:41,295
Speaker 1:  Hello and welcome to our chest, the flagship podcast Stealing

3
00:01:41,545 --> 00:01:43,175
Speaker 1:  stuff. Sam Altman has already stolen.

4
00:01:45,355 --> 00:01:49,215
Speaker 1:  You're on notice, Sam, you steal a car. I'm coming to steal that car. Look,

5
00:01:49,255 --> 00:01:52,015
Speaker 1:  I haven't been a lawyer in a long time. Isn't that double jeopardy? Like

6
00:01:52,015 --> 00:01:54,895
Speaker 1:  if somebody else has done a crime and then you do the crime to them,

7
00:01:55,675 --> 00:01:56,095
Speaker 1:  you're

8
00:01:56,095 --> 00:01:59,295
Speaker 5:  Good. Yeah, if it's additive crime, I think it it is fine.

9
00:01:59,805 --> 00:02:03,495
Speaker 1:  Yeah. I don't look, I passed the bar exam a long time ago.

10
00:02:05,185 --> 00:02:09,035
Speaker 1:  I've drank a lot since then. Just in the past five days of the Trump

11
00:02:09,035 --> 00:02:12,675
Speaker 1:  administration, I've drank away one full bar exam. But I do believe, and

12
00:02:12,675 --> 00:02:15,635
Speaker 1:  you correct and correct me if I'm wrong, that if someone does a crime and

13
00:02:15,635 --> 00:02:19,205
Speaker 1:  then you do that same crime to them, the court's like, you're good.

14
00:02:19,425 --> 00:02:22,885
Speaker 5:  Listen, if we've learned anything in the last two weeks, it's that you can

15
00:02:22,885 --> 00:02:23,965
Speaker 5:  in fact just say things

16
00:02:25,365 --> 00:02:26,165
Speaker 1:  Whenever you want. Yeah.

17
00:02:26,345 --> 00:02:30,205
Speaker 5:  And sometimes those things turn out to have huge weight

18
00:02:30,275 --> 00:02:32,925
Speaker 5:  over how America works. So yeah, here we are.

19
00:02:33,095 --> 00:02:36,405
Speaker 1:  We're fully in the, I declare bankruptcy stage as the American experiment,

20
00:02:36,625 --> 00:02:40,565
Speaker 1:  as, as I believe George Washington prophesied in his

21
00:02:40,565 --> 00:02:44,325
Speaker 1:  first inaugural. He is like, one day people are just gonna start saying stuff

22
00:02:44,825 --> 00:02:47,885
Speaker 1:  and you should row a boat away from Harris as fast as you can. Those are

23
00:02:47,885 --> 00:02:49,965
Speaker 1:  all the things I know about George Washington. All right. It's about right.

24
00:02:50,485 --> 00:02:53,805
Speaker 1:  There's a lot going on this week. I'm actually very excited about our second

25
00:02:53,805 --> 00:02:57,725
Speaker 1:  segment this week. Last week we asked a bunch of you to tell us

26
00:02:57,725 --> 00:03:01,605
Speaker 1:  what you were using AI for because there's a big gap, right? In

27
00:03:01,985 --> 00:03:05,205
Speaker 1:  how people feel about it, what the companies say about it, the amount of

28
00:03:05,205 --> 00:03:08,775
Speaker 1:  money we're spending on it, and then how people are actually using it. And

29
00:03:08,775 --> 00:03:10,735
Speaker 1:  it turns out a lot of you are using ai.

30
00:03:10,795 --> 00:03:14,695
Speaker 5:  We got so much feed, like, so much feedback, including from a bunch

31
00:03:14,695 --> 00:03:18,495
Speaker 5:  of people who like, were outraged at the idea that

32
00:03:18,555 --> 00:03:21,975
Speaker 5:  not everyone uses AI for everything all the time. And I really actually appreciate

33
00:03:21,975 --> 00:03:24,775
Speaker 5:  that. It was, it was very cool to go through and be like, oh, these are like

34
00:03:24,775 --> 00:03:28,455
Speaker 5:  real people who use it in their real lives and not just like

35
00:03:29,015 --> 00:03:31,655
Speaker 5:  somebody trying to convince me to give them billions of dollars.

36
00:03:32,515 --> 00:03:35,055
Speaker 1:  Are you sure that Sam Altman wasn't outraged and not everybody's

37
00:03:35,055 --> 00:03:38,495
Speaker 5:  Using ai? Do I think Sam Altman has a lot of burners and is in our inbox

38
00:03:38,515 --> 00:03:40,055
Speaker 5:  on occasion? I I do think that, yeah,

39
00:03:40,115 --> 00:03:44,095
Speaker 1:  He does have access to chat. Bt I'll say that. Sweet. We're gonna go

40
00:03:44,095 --> 00:03:46,335
Speaker 1:  through all those. I think that's gonna be really fun. We got a lightning

41
00:03:46,335 --> 00:03:49,855
Speaker 1:  round, but we gotta start, I think with the, the tech news.

42
00:03:50,695 --> 00:03:53,945
Speaker 1:  It's the tech news and it's obviously that Microsoft is continuing to launch

43
00:03:54,035 --> 00:03:55,385
Speaker 1:  Intel powered surface devices.

44
00:03:57,745 --> 00:03:58,645
Speaker 1:  No, David?

45
00:03:58,645 --> 00:04:02,405
Speaker 5:  Yeah. For 500 more dollars, you can have a surface

46
00:04:02,835 --> 00:04:06,405
Speaker 5:  that has more words in the name and a chip you don't really want

47
00:04:06,865 --> 00:04:07,485
Speaker 1:  And worse battery

48
00:04:07,485 --> 00:04:09,285
Speaker 5:  Line. Congratulations. You did it.

49
00:04:10,165 --> 00:04:13,325
Speaker 1:  There's something there. They, they did announce they're gonna do this. They

50
00:04:13,325 --> 00:04:17,285
Speaker 1:  also announced that Microsoft is running DeepSeek now on copilot PCs

51
00:04:17,285 --> 00:04:20,085
Speaker 1:  that run Qualcomm chips. We're gonna talk about DeepSeek in a second, but

52
00:04:20,085 --> 00:04:22,565
Speaker 1:  it's just interesting that there's still,

53
00:04:23,845 --> 00:04:27,685
Speaker 1:  Microsoft has to still carry Intel in this specific way because that

54
00:04:27,685 --> 00:04:31,085
Speaker 1:  partnership is so long and so important to everyone. But it's like,

55
00:04:31,775 --> 00:04:35,605
Speaker 1:  everything about Microsoft releasing new service devices has changed

56
00:04:35,705 --> 00:04:39,685
Speaker 1:  in the past 18 months. Yeah. To where two years

57
00:04:39,685 --> 00:04:43,045
Speaker 1:  ago they're like, here's some service devices with arm chips. I think the

58
00:04:43,245 --> 00:04:47,005
Speaker 1:  reaction would be, eh, and now it's, here's some service devices

59
00:04:47,005 --> 00:04:49,805
Speaker 1:  with Intel chips and it's like, oh man, you're still doing this. And that's

60
00:04:49,885 --> 00:04:50,525
Speaker 1:  a big change.

61
00:04:51,115 --> 00:04:54,965
Speaker 5:  Yeah. And it, it really did happen a really fast and B,

62
00:04:55,585 --> 00:04:58,965
Speaker 5:  almost without anybody noticing, like, I feel like every step along the way

63
00:04:59,385 --> 00:05:02,725
Speaker 5:  was also kind of meh, right? Like when that first round of

64
00:05:03,115 --> 00:05:06,605
Speaker 5:  copilot p copilot plus PCs, still the worst name in

65
00:05:06,755 --> 00:05:10,655
Speaker 5:  laptops came out. They they were

66
00:05:10,655 --> 00:05:14,335
Speaker 5:  fine, right? Yeah. Like the overwhelming response was like, Qualcomm did

67
00:05:14,355 --> 00:05:18,295
Speaker 5:  pretty good work here, but the emulation stuff is still pretty bad. The

68
00:05:18,295 --> 00:05:22,095
Speaker 5:  performance isn't what we, it's like, it's just not, it's, it's fine.

69
00:05:22,475 --> 00:05:26,335
Speaker 5:  And then somehow between there and here, we've flipped

70
00:05:26,435 --> 00:05:30,375
Speaker 5:  all the way to, like you said, I see Intel Surface Pros and go, that's

71
00:05:30,375 --> 00:05:31,895
Speaker 5:  not what anybody wants. Like, it's crazy,

72
00:05:32,555 --> 00:05:35,815
Speaker 1:  But I don't, and I, to be clear, I don't think the arm powered chips are

73
00:05:35,815 --> 00:05:39,015
Speaker 1:  doing everything yet. There's a, a reason to sell Intel powered laptops.

74
00:05:39,085 --> 00:05:42,135
Speaker 1:  Lots of people are still buying Intel powered Windows PCs obviously. But

75
00:05:42,135 --> 00:05:45,775
Speaker 1:  it's just interesting to see where the bleeding edge has gone to.

76
00:05:46,165 --> 00:05:49,935
Speaker 1:  Like, it's obvious that that thing has runway and intel and you know,

77
00:05:50,065 --> 00:05:51,695
Speaker 1:  along the way they did fire their CEO,

78
00:05:52,285 --> 00:05:52,895
Speaker 5:  They did do

79
00:05:52,895 --> 00:05:55,935
Speaker 1:  That. So maybe it's just vibes all the way down. That's not actually the

80
00:05:55,935 --> 00:05:58,240
Speaker 1:  biggest news. That was a joke. A joke. And we ended up talking about it because

81
00:05:58,240 --> 00:06:01,845
Speaker 1:  it's forecast. The big news is DeepSeek. Yeah. It has turned

82
00:06:02,025 --> 00:06:05,645
Speaker 1:  the tech world upside down in just a very, the only way to describe, it's

83
00:06:05,645 --> 00:06:09,325
Speaker 1:  a very bubbly way. Like everyone was over invested and

84
00:06:09,325 --> 00:06:13,245
Speaker 1:  overhyping AI and then everyone overreacted to another AI tool.

85
00:06:14,435 --> 00:06:15,165
Speaker 1:  What do you think, David?

86
00:06:15,785 --> 00:06:19,245
Speaker 5:  So I actually think that's the right frame for it, because the more I've

87
00:06:19,245 --> 00:06:22,245
Speaker 5:  learned about this, and the more I've talked to people about it, the more

88
00:06:22,795 --> 00:06:25,245
Speaker 5:  I've come to believe there are sort of three

89
00:06:26,005 --> 00:06:29,885
Speaker 5:  simultaneous things going on. On the one hand, there's the

90
00:06:29,885 --> 00:06:33,805
Speaker 5:  stock market freak out, which is just a stock market freak out a bunch

91
00:06:33,805 --> 00:06:37,045
Speaker 5:  of people, I would say like half correctly

92
00:06:37,255 --> 00:06:40,685
Speaker 5:  understood what DeepSeek is and made a bunch of

93
00:06:41,055 --> 00:06:44,445
Speaker 5:  weird semi thoughtful bets

94
00:06:44,725 --> 00:06:48,165
Speaker 5:  against Nvidia as a result. And there's been some really interesting

95
00:06:48,355 --> 00:06:51,965
Speaker 5:  reporting over the last few days that a big part of the selloff was triggered

96
00:06:51,965 --> 00:06:55,805
Speaker 5:  by this one super deep, super thoughtful

97
00:06:55,835 --> 00:06:59,765
Speaker 5:  blog post about somebody who was shorting Nvidia. And he, he like writes

98
00:06:59,765 --> 00:07:03,405
Speaker 5:  this big thing and lands on short Nvidia, it has no moat.

99
00:07:03,475 --> 00:07:06,925
Speaker 5:  This thing is not, this thing is massively overvalued,

100
00:07:07,825 --> 00:07:11,725
Speaker 5:  got shared by a bunch of people in tech and in you can kind of argue that

101
00:07:12,265 --> 00:07:16,205
Speaker 5:  that's what caused a lot of that. So on the one hand, there is just a

102
00:07:16,205 --> 00:07:20,125
Speaker 5:  lot of money was lost very quickly. So a lot

103
00:07:20,125 --> 00:07:23,645
Speaker 5:  of people got in a like CNBC tizzy about that, right? So that's like one

104
00:07:23,645 --> 00:07:26,805
Speaker 5:  thing happening over here. The other thing is

105
00:07:27,435 --> 00:07:31,165
Speaker 5:  very much a story about technology and, and

106
00:07:31,305 --> 00:07:34,285
Speaker 5:  the actual questions of like, okay, how do we think about

107
00:07:35,835 --> 00:07:39,725
Speaker 5:  what DeepSeek did here that is interesting and unique and how do we

108
00:07:39,725 --> 00:07:43,205
Speaker 5:  think about open source AI being really at the bleeding edge? A lot of this

109
00:07:43,205 --> 00:07:45,925
Speaker 5:  stuff and what did DeepSeek do that is interesting, that's gonna roll back

110
00:07:45,925 --> 00:07:49,245
Speaker 5:  to some of these companies and what does it mean for OpenAI? That's all interesting

111
00:07:49,245 --> 00:07:51,405
Speaker 5:  and I think we should spend most of our time talking about that. And then

112
00:07:51,405 --> 00:07:54,325
Speaker 5:  there's a third thing, which is just China. It's just the China of it all.

113
00:07:54,705 --> 00:07:58,485
Speaker 5:  And I think everybody sort of twists themselves

114
00:07:58,545 --> 00:08:01,565
Speaker 5:  in knots about everything to do with China in part, for good reasons and

115
00:08:01,565 --> 00:08:05,085
Speaker 5:  in part for bad reasons. But like coming off of all this stuff with

116
00:08:05,185 --> 00:08:09,005
Speaker 5:  TikTok and Red Note and the, the ideas about

117
00:08:09,035 --> 00:08:11,685
Speaker 5:  what these companies are collecting about us and what it means that there

118
00:08:11,685 --> 00:08:14,925
Speaker 5:  is like technical leadership happening in those countries and like

119
00:08:16,545 --> 00:08:20,395
Speaker 5:  everybody is just really spun up to have feelings about China.

120
00:08:20,455 --> 00:08:24,395
Speaker 5:  And so there was a lot of feelings about China in their altogether.

121
00:08:24,395 --> 00:08:28,155
Speaker 5:  So it just became this like perfect storm of, I think under

122
00:08:28,475 --> 00:08:32,155
Speaker 5:  a couple of really small changes, this would've been like an

123
00:08:32,395 --> 00:08:35,995
Speaker 5:  interesting story and not like the only thing anyone in my group chats cared

124
00:08:35,995 --> 00:08:38,915
Speaker 5:  about all week. But because it was those three things together,

125
00:08:39,795 --> 00:08:43,015
Speaker 5:  it became everything. Like my high school friends are talking about DeepSeek,

126
00:08:43,605 --> 00:08:45,095
Speaker 5:  this has just been happening this week.

127
00:08:45,205 --> 00:08:49,175
Speaker 1:  Well, once you turn the entire economy into gambling, which is

128
00:08:49,175 --> 00:08:53,005
Speaker 1:  what we're doing, yeah. Everyone cares a lot about the gambling and to,

129
00:08:53,145 --> 00:08:56,765
Speaker 1:  to like whatever you mentioned the CNBC of it all. Like most

130
00:08:56,925 --> 00:09:00,765
Speaker 1:  people's retirement savings are like in big index funds. Yeah.

131
00:09:00,765 --> 00:09:03,845
Speaker 1:  Which are all just big tech stocks, right? And so it's like eight

132
00:09:04,045 --> 00:09:04,405
Speaker 5:  Companies now,

133
00:09:04,895 --> 00:09:07,805
Speaker 1:  Right? So when Nvidia goes down or Apple goes down or meta goes down, like

134
00:09:08,285 --> 00:09:11,775
Speaker 1:  actually the whole economy kind of goes with them, which is not great. There's

135
00:09:11,815 --> 00:09:15,055
Speaker 1:  a, there's a lot of reading and reporting you can do about that. That's not

136
00:09:15,525 --> 00:09:18,775
Speaker 1:  what we care about here at The Verge, but it's just notable that that's why

137
00:09:18,775 --> 00:09:20,095
Speaker 1:  it breaks through so often now.

138
00:09:20,245 --> 00:09:23,975
Speaker 5:  Yeah. Oh, I agree. And I think in, in, in NVIDIA's case, I mean the, the

139
00:09:24,005 --> 00:09:27,855
Speaker 5:  like, what people have been saying about Nvidia for months is that it has

140
00:09:27,855 --> 00:09:30,975
Speaker 5:  essentially propped up the stock market on its own for a very long time now.

141
00:09:30,975 --> 00:09:33,895
Speaker 5:  And it has gone through the roof. Like it's, it's grown like crazy. And so

142
00:09:34,245 --> 00:09:38,215
Speaker 5:  yeah, everybody has been saying there is a bubble and just pointing

143
00:09:38,215 --> 00:09:41,175
Speaker 5:  it Nvidia. By the way, total side note, one of the funniest things about

144
00:09:41,175 --> 00:09:44,615
Speaker 5:  every time Nvidia is in the news is the number of different ways people pronounce

145
00:09:44,615 --> 00:09:48,015
Speaker 5:  the name of that company. Yes. There's a lot of Nvidia happening like

146
00:09:49,115 --> 00:09:50,895
Speaker 5:  Nvidia. Yes. Which I like. There's a lot of, we gotta

147
00:09:51,035 --> 00:09:53,815
Speaker 1:  Got if VERGE has, listen, if you're paying attention to The Verge, there's

148
00:09:53,815 --> 00:09:55,935
Speaker 1:  some of that floating around the bir lately. Listen, we're we're gonna fix

149
00:09:55,935 --> 00:09:56,055
Speaker 1:  it.

150
00:09:56,235 --> 00:09:59,135
Speaker 5:  The thing is, I'm not sure anybody's wrong, you know, who knows? It's, it's

151
00:09:59,135 --> 00:10:01,135
Speaker 5:  nuclear and nuclear like, you know. No,

152
00:10:01,485 --> 00:10:05,335
Speaker 1:  It's the, there's wait, hold on. But there's a, we we can't slide this far.

153
00:10:06,125 --> 00:10:08,895
Speaker 1:  There's an answer. We're gonna, we're gonna clean it up. That's all I'm saying.

154
00:10:09,005 --> 00:10:12,775
Speaker 5:  Okay. So it's like we agree that you, you would pronounce it like Nvidia,

155
00:10:12,775 --> 00:10:16,445
Speaker 5:  like there's a sort of silent, there's an I at the beginning, right? Yeah.

156
00:10:16,475 --> 00:10:17,645
Speaker 5:  Like a, like a gentle I

157
00:10:17,645 --> 00:10:21,285
Speaker 1:  Nvidia has, their name is so stupid that they have pronunciation

158
00:10:21,285 --> 00:10:22,845
Speaker 1:  guidelines online. Do

159
00:10:22,845 --> 00:10:23,125
Speaker 5:  They really?

160
00:10:23,315 --> 00:10:26,685
Speaker 1:  Yeah, they have for years. Years and years and years. It doesn't matter because

161
00:10:26,685 --> 00:10:30,205
Speaker 1:  like the CNBC people will just say in Nvidia,

162
00:10:30,385 --> 00:10:34,125
Speaker 5:  But also Nvidia might be the company that most violates my theory that you

163
00:10:34,125 --> 00:10:38,005
Speaker 5:  can't overcome a stupid company name. Mm. And it's a stupid company name.

164
00:10:38,235 --> 00:10:41,885
Speaker 5:  It's hard to pronounce. It doesn't mean anything. It has nothing. It's just,

165
00:10:41,955 --> 00:10:45,885
Speaker 5:  it's, it's just like an Amazon brand of letters that they happen

166
00:10:45,885 --> 00:10:46,885
Speaker 5:  to turn into. Oh that's,

167
00:10:46,885 --> 00:10:48,805
Speaker 1:  That's one of the biggest companies on our Oh, that's rough. But it's good.

168
00:10:48,825 --> 00:10:52,525
Speaker 1:  That's a rough chuckle. That's really good. I bought this Nvidia battery

169
00:10:52,555 --> 00:10:56,325
Speaker 1:  bank. That's what I'm saying, $39 with a 10% coupon code.

170
00:10:56,515 --> 00:11:00,445
Speaker 5:  Yeah. Like in a parallel universe, you green makes all of the GPUs that people

171
00:11:00,465 --> 00:11:04,445
Speaker 5:  buy in the world. And this is where we are. But yeah, so the the,

172
00:11:04,545 --> 00:11:08,245
Speaker 5:  the money of it is, is real. And I think to the extent that it's also tied

173
00:11:08,245 --> 00:11:11,765
Speaker 5:  up in all of this other stuff, it, it's, that's important. But to me that

174
00:11:11,765 --> 00:11:15,485
Speaker 5:  is actually like, that is the thing people got most sort of

175
00:11:15,485 --> 00:11:19,165
Speaker 5:  bent outta shape about and took it to mean that there is all this other

176
00:11:19,165 --> 00:11:22,205
Speaker 5:  equally important stuff happening in these other ways like with the tech

177
00:11:22,205 --> 00:11:25,885
Speaker 5:  itself. And the more I learn about it, the less I feel like that's actually

178
00:11:25,945 --> 00:11:26,325
Speaker 5:  the case.

179
00:11:26,825 --> 00:11:30,765
Speaker 1:  So I keep making that comparison to Bluetooth with ai. It's

180
00:11:30,765 --> 00:11:31,965
Speaker 1:  just, it's just my go-to. You know, you've

181
00:11:31,965 --> 00:11:34,925
Speaker 5:  Been doing that a lot recently. Every, you compare a lot of things to Bluetooth

182
00:11:35,205 --> 00:11:36,405
Speaker 1:  Recently. Everything is Bluetooth,

183
00:11:38,195 --> 00:11:41,185
Speaker 1:  right? Like we had to, you don't remember this when we

184
00:11:41,185 --> 00:11:44,185
Speaker 5:  Do it first cast coffee table book, by the way, that's the title, just throwing

185
00:11:44,185 --> 00:11:44,305
Speaker 5:  that

186
00:11:44,305 --> 00:11:47,225
Speaker 1:  Everything is Bluetooth. Everything is Bluetooth. When, when Bluetooth first

187
00:11:47,325 --> 00:11:51,255
Speaker 1:  hit the scene, they were so proud of themselves and they're like,

188
00:11:51,255 --> 00:11:54,565
Speaker 1:  we, everything is gonna be Bluetooth, Bluetooth is gonna change the economy.

189
00:11:55,285 --> 00:11:58,205
Speaker 1:  They were, they, they were out here talking about personal area networks

190
00:11:58,205 --> 00:12:02,045
Speaker 1:  or pans. Oh yeah, I, I wrote a lot of blog posts in gadget about

191
00:12:02,205 --> 00:12:05,685
Speaker 1:  personal area networks. How's your pan doing? This is awesome dude.

192
00:12:06,555 --> 00:12:08,005
Speaker 1:  It's my Apple watch and my hair fox.

193
00:12:08,955 --> 00:12:09,245
Speaker 5:  Yeah,

194
00:12:09,245 --> 00:12:12,565
Speaker 1:  My pants crush. This is a real thing that was gonna happen, right? Like the,

195
00:12:12,625 --> 00:12:16,085
Speaker 1:  we were gonna blow up the world with Bluetooth and then the

196
00:12:16,345 --> 00:12:19,845
Speaker 1:  actual in products didn't ma match the investment for years.

197
00:12:21,015 --> 00:12:24,995
Speaker 1:  And so like, it's a, just a good comparison. FI 5G actually is

198
00:12:24,995 --> 00:12:28,395
Speaker 1:  another great comparison. We were in a race for that you will recall.

199
00:12:28,865 --> 00:12:32,675
Speaker 1:  Yeah. Just to what? To what finish line. No one knew the

200
00:12:32,675 --> 00:12:34,075
Speaker 1:  answer. Robot surgery

201
00:12:35,775 --> 00:12:39,645
Speaker 1:  seems unlikely. Autonomous cars there, there's some of 'em driving

202
00:12:39,645 --> 00:12:43,565
Speaker 1:  around. Sure, sure. But like that was the big imp

203
00:12:43,705 --> 00:12:47,645
Speaker 1:  and you will also, with 5G you'll call, we had to beat China because

204
00:12:47,665 --> 00:12:51,605
Speaker 1:  if China got 5G networks, first America

205
00:12:51,745 --> 00:12:54,725
Speaker 1:  was done. Right? You can't, can't have it.

206
00:12:54,895 --> 00:12:57,005
Speaker 5:  Which is very much what people are saying about AI right now.

207
00:12:57,005 --> 00:13:00,325
Speaker 1:  Right? It all just tracks like the, the, these things rhyme, right?

208
00:13:01,145 --> 00:13:04,925
Speaker 1:  The difference is when at and t and Verizon spent

209
00:13:05,005 --> 00:13:08,925
Speaker 1:  billions of dollars to build 5G networks against whatever hype

210
00:13:09,385 --> 00:13:13,245
Speaker 1:  and whatever graft, they at least knew they were gonna sell people cell

211
00:13:13,245 --> 00:13:17,205
Speaker 1:  phones and cell service true at numbers that people

212
00:13:17,205 --> 00:13:21,135
Speaker 1:  were willing to pay, right? And the people who put Bluetooth everywhere,

213
00:13:21,325 --> 00:13:24,695
Speaker 1:  they didn't know that there would be AirPods, but they could imagine

214
00:13:25,325 --> 00:13:29,135
Speaker 1:  AirPods and they were like, one day there will be AirPods and then

215
00:13:29,135 --> 00:13:31,375
Speaker 1:  eventually Apple invented AirPods. But they had

216
00:13:31,375 --> 00:13:33,495
Speaker 5:  This, yeah, you could get all the things with wires and be like, what if

217
00:13:33,495 --> 00:13:36,975
Speaker 5:  no wires and like that, that worked, right? You could, you could see how

218
00:13:36,975 --> 00:13:37,415
Speaker 5:  you'd get there.

219
00:13:38,165 --> 00:13:41,465
Speaker 1:  But with AI we just keep, we're like, what if it bangs you?

220
00:13:43,045 --> 00:13:46,865
Speaker 1:  Is that worth $5 billion? For some people the answer is

221
00:13:46,975 --> 00:13:50,865
Speaker 1:  decidedly yes. We're like, what if it's agents and it does some clicking

222
00:13:50,865 --> 00:13:53,385
Speaker 1:  around the web for you? What if the rabbit R one is great? What if the humane

223
00:13:53,385 --> 00:13:57,305
Speaker 1:  pin is great to quote David Pierce, is this a thing? And there's not

224
00:13:57,405 --> 00:14:00,585
Speaker 1:  the, there's not the finish line, right? Right. Where everyone's gonna pay

225
00:14:00,725 --> 00:14:04,385
Speaker 1:  all of the money for the products that justifies all of the investment.

226
00:14:04,765 --> 00:14:08,225
Speaker 1:  And I think my version of the DeepSeek story is like,

227
00:14:09,625 --> 00:14:13,125
Speaker 1:  it just showed that all of this big upfront investment

228
00:14:14,085 --> 00:14:15,635
Speaker 1:  maybe isn't necessary.

229
00:14:17,285 --> 00:14:21,245
Speaker 1:  And so meta doing dick measuring by

230
00:14:21,245 --> 00:14:25,125
Speaker 1:  how many Nvidia GPUs it was buying kind of is like are, wait, are you

231
00:14:25,125 --> 00:14:29,055
Speaker 1:  just wasting money? Like is there, is there an end result

232
00:14:29,055 --> 00:14:32,895
Speaker 1:  to this or can or can you just do it much more cheaply now? And I don't

233
00:14:32,895 --> 00:14:36,615
Speaker 1:  like, I don't know the answer to that, the, these questions because I don't

234
00:14:36,615 --> 00:14:40,295
Speaker 1:  think anybody has really delivered a product

235
00:14:41,225 --> 00:14:43,875
Speaker 1:  that is gonna pay it all back

236
00:14:44,335 --> 00:14:47,915
Speaker 5:  In the context of DeepSeek. That conversation has been super interesting

237
00:14:47,915 --> 00:14:50,395
Speaker 5:  this week because like the, the,

238
00:14:51,905 --> 00:14:55,205
Speaker 5:  for all of this being good news for the, the

239
00:14:55,875 --> 00:14:59,565
Speaker 5:  Mark Zuckerberg, you know, talking about the size of his data center

240
00:15:00,465 --> 00:15:04,245
Speaker 5:  of the world is, okay, I lemme see if I can explain

241
00:15:04,385 --> 00:15:07,485
Speaker 5:  how the theory goes. The theory goes like this, If you make

242
00:15:08,545 --> 00:15:12,245
Speaker 5:  AI cheaper to make two things will happen. One,

243
00:15:12,425 --> 00:15:16,365
Speaker 5:  you'll be able to make better AI for the same price. So you get the, the

244
00:15:16,365 --> 00:15:20,005
Speaker 5:  performance scale goes up, right? I can now make something

245
00:15:20,015 --> 00:15:23,845
Speaker 5:  50% better for the same price as I was planning to do it before. So

246
00:15:23,915 --> 00:15:27,005
Speaker 5:  thus good ai then all the stuff that I've already been doing

247
00:15:27,715 --> 00:15:31,565
Speaker 5:  gets cheaper. So it becomes more accessible to more people who use it

248
00:15:31,565 --> 00:15:35,405
Speaker 5:  for more things, thus also better. So it's like thi this flywheel of,

249
00:15:35,545 --> 00:15:39,525
Speaker 5:  of scaling is what everybody is very excited. Dario Amay, the CEO of

250
00:15:39,525 --> 00:15:43,405
Speaker 5:  Anthropic made essentially that argument, right? That now, because all

251
00:15:43,405 --> 00:15:45,725
Speaker 5:  this stuff is getting cheaper, and this has been happening for a while. Like

252
00:15:45,725 --> 00:15:49,245
Speaker 5:  DeepSeek didn't invent making AI cheaper to run like this has been happening

253
00:15:49,245 --> 00:15:49,645
Speaker 5:  for a while.

254
00:15:50,145 --> 00:15:54,005
Speaker 1:  One of the very first Google iOS in this whole AI tumult like two years

255
00:15:54,025 --> 00:15:57,965
Speaker 1:  ago, literally Sindar Phai was saying the thing that we're best

256
00:15:57,965 --> 00:15:59,325
Speaker 1:  at is making things cheaper,

257
00:15:59,695 --> 00:16:03,045
Speaker 5:  Right? And so, so now what you're thinking is like, okay, if you're Sam Altman,

258
00:16:03,045 --> 00:16:06,405
Speaker 5:  instead of spending $10 billion to do

259
00:16:07,275 --> 00:16:11,165
Speaker 5:  this thing, I can spend $10 billion to do the next thing. Right?

260
00:16:11,245 --> 00:16:15,005
Speaker 5:  I can, I can make the thing that I'm working on with all this money, 50%

261
00:16:15,005 --> 00:16:15,725
Speaker 5:  better. But what

262
00:16:15,725 --> 00:16:16,085
Speaker 1:  Is he making?

263
00:16:16,155 --> 00:16:19,565
Speaker 5:  This is what I'm saying. So all I I actually buy that theory, right? Like

264
00:16:19,565 --> 00:16:22,525
Speaker 5:  the, the, it, it all makes sense, but the question continues to be what are

265
00:16:22,525 --> 00:16:26,005
Speaker 5:  we laddering all of this up to? What is the pot of gold at the end of this

266
00:16:26,005 --> 00:16:29,605
Speaker 5:  rainbow? And the, my favorite thing that happened this week was did you read

267
00:16:29,805 --> 00:16:33,605
Speaker 5:  Reid Hoffman's New York Times opinion essay that everybody

268
00:16:33,625 --> 00:16:37,285
Speaker 5:  was dunking on all week? Yes. Can I just read out loud two paragraphs from

269
00:16:37,285 --> 00:16:40,765
Speaker 5:  it please. He says, imagine AI models that are trained on comprehensive

270
00:16:40,765 --> 00:16:43,885
Speaker 5:  collections of your own digital activities and behaviors. This kind of AI

271
00:16:43,885 --> 00:16:46,845
Speaker 5:  could possess total recall of your Venmo transactions and Instagram likes

272
00:16:46,845 --> 00:16:49,565
Speaker 5:  and Google calendar appointments. The more you choose to share, the more

273
00:16:49,565 --> 00:16:52,645
Speaker 5:  this AI would be able to identify patterns in your life and surface insights

274
00:16:52,645 --> 00:16:55,965
Speaker 5:  that you may find useful decades from now. As you try to remember exactly

275
00:16:55,965 --> 00:16:58,805
Speaker 5:  what sequence of events and life circumstances made you finally decide to

276
00:16:58,805 --> 00:17:02,685
Speaker 5:  go all in on Bitcoin. Your AI could develop an informed hypothesis based

277
00:17:02,685 --> 00:17:06,205
Speaker 5:  on a detailed record of your status updates, invites dms and other potentially

278
00:17:06,205 --> 00:17:09,725
Speaker 5:  enduring ephemera that we're often barely aware of as we create them much

279
00:17:09,725 --> 00:17:13,605
Speaker 5:  less days, months, or years after the fact. This is Reid Hoffman writing

280
00:17:13,605 --> 00:17:17,405
Speaker 5:  about how AI is going to make humanity better. And the best use case

281
00:17:17,665 --> 00:17:20,525
Speaker 5:  can dream about is it's gonna tell you

282
00:17:21,615 --> 00:17:24,645
Speaker 5:  which DM made you wanna buy into Bitcoin. Like

283
00:17:26,275 --> 00:17:30,165
Speaker 5:  what, what's the plan here? Like, and, and now, and

284
00:17:30,165 --> 00:17:32,565
Speaker 5:  I I I've been thinking about this a lot with the questions of like, okay,

285
00:17:32,565 --> 00:17:34,725
Speaker 5:  well then if all these companies don't need this much money, why are they

286
00:17:34,725 --> 00:17:36,845
Speaker 5:  raising all this much money? And it's like, well, 'cause money's good to

287
00:17:36,845 --> 00:17:40,725
Speaker 5:  have and you should, If you can raise it, you might as well. And so Sam

288
00:17:40,885 --> 00:17:43,845
Speaker 5:  Altman is out there raising unheard of amounts of money and there are reports

289
00:17:43,845 --> 00:17:47,445
Speaker 5:  that they're about to raise anywhere from 15 to 25 billion more dollars

290
00:17:47,625 --> 00:17:51,475
Speaker 5:  now. Like Sam Altman is as

291
00:17:51,475 --> 00:17:54,155
Speaker 5:  good a fundraiser as we have ever had in this industry. And so he's gonna

292
00:17:54,155 --> 00:17:57,835
Speaker 5:  go out and raise money. But it turns out I'm not sure anyone knows

293
00:17:58,265 --> 00:18:01,435
Speaker 5:  what we're building towards anymore. And I'm less and less sure every day

294
00:18:01,435 --> 00:18:05,075
Speaker 5:  that people know what the answer is to what problem they are solving with

295
00:18:05,295 --> 00:18:09,155
Speaker 5:  ai. So we're in this place where like DeepSeek is very interesting in

296
00:18:09,155 --> 00:18:13,075
Speaker 5:  that it is going to make, it is going to help make a lot of this stuff easier

297
00:18:13,215 --> 00:18:16,475
Speaker 5:  and cheaper to make. The fact that it's open source I actually think is on

298
00:18:16,475 --> 00:18:20,315
Speaker 5:  balance. Very cool. There are scary things

299
00:18:20,315 --> 00:18:23,435
Speaker 5:  about open source ai, but whatever, I think by and large, I this stuff should

300
00:18:23,435 --> 00:18:25,355
Speaker 5:  be in the open and not in the hands of three companies.

301
00:18:27,065 --> 00:18:29,835
Speaker 5:  It's going to change the way that this stuff gets developed. People are going

302
00:18:29,835 --> 00:18:33,755
Speaker 5:  to like take the things that DeepSeek did, bring it in, make it

303
00:18:33,755 --> 00:18:37,355
Speaker 5:  better, the whole industry will get better. And to what end has not become

304
00:18:37,495 --> 00:18:38,795
Speaker 5:  any clearer at all.

305
00:18:39,015 --> 00:18:42,675
Speaker 1:  You know, it's funny, the Reid Hoffen thing, I read that too in, you

306
00:18:43,435 --> 00:18:46,435
Speaker 1:  know, like you do in Trump administrations, I I spend a, I've been spending

307
00:18:46,475 --> 00:18:49,715
Speaker 1:  a lot of time think about the inner lives of rich, rich men.

308
00:18:50,225 --> 00:18:54,115
Speaker 1:  Sure. Like very rich men. What do they, what do they want from me?

309
00:18:54,115 --> 00:18:57,515
Speaker 1:  Because they seem to be in charge of everything. And it's just like dude

310
00:18:57,515 --> 00:19:01,165
Speaker 1:  wants a friend. Like I don't, I think a lot of these guys don't have

311
00:19:01,785 --> 00:19:05,005
Speaker 1:  all their relationships are just people who want things from that, right?

312
00:19:05,705 --> 00:19:09,045
Speaker 1:  And so when they're like, what, what could this robot be for me? They just

313
00:19:09,045 --> 00:19:11,005
Speaker 1:  imagine just a friend who's known them since kindergarten.

314
00:19:11,505 --> 00:19:14,325
Speaker 5:  But then do you think there's also a little part of them that's like, wants

315
00:19:14,325 --> 00:19:16,685
Speaker 5:  their AI friend to know how rich they are? Yeah.

316
00:19:16,705 --> 00:19:16,925
Speaker 1:  Oh

317
00:19:17,245 --> 00:19:20,325
Speaker 5:  Absolutely. They're like, why don't you think my ideas are good? I don't,

318
00:19:20,325 --> 00:19:22,925
Speaker 5:  I don't want your ideas Ai I want you to like my ideas.

319
00:19:23,525 --> 00:19:26,165
Speaker 1:  I just think I, if I have a piece of life advice for everybody listening,

320
00:19:26,165 --> 00:19:28,805
Speaker 1:  it's like you should have some non-transactional relationships in your life

321
00:19:28,805 --> 00:19:32,325
Speaker 1:  with people who just like, like you and you like them and that's it. Like

322
00:19:32,325 --> 00:19:34,965
Speaker 1:  you're not trying to buy and sell That's right. Everything, yeah. All the

323
00:19:34,965 --> 00:19:38,445
Speaker 1:  time. That might be good for humanity. I always think about Steve Jobs,

324
00:19:38,745 --> 00:19:41,805
Speaker 1:  you know, he would end all the keynotes with like apples at the intersection

325
00:19:41,805 --> 00:19:45,605
Speaker 1:  of liberal arts and technology and like, these dudes are like,

326
00:19:45,875 --> 00:19:46,965
Speaker 1:  fuck the liberal arts.

327
00:19:48,475 --> 00:19:50,965
Speaker 1:  It's like, no, that's the the important thing. Like that's why that, that's

328
00:19:50,965 --> 00:19:54,165
Speaker 1:  why that company is important. 'cause they, they like care about culture.

329
00:19:54,165 --> 00:19:56,245
Speaker 1:  They're like the the computers are to make art with,

330
00:19:56,385 --> 00:19:58,165
Speaker 5:  That's why they can't make AI for shit though.

331
00:19:58,785 --> 00:20:02,565
Speaker 1:  But that's, that's so funny. Like I, I dunno what Steve Jobs thought about

332
00:20:02,625 --> 00:20:06,485
Speaker 1:  ai, but it's like, you look at the output and it's like, it's

333
00:20:06,485 --> 00:20:10,445
Speaker 1:  pretty bad and it's like, is this good? Like is this, do you wanna

334
00:20:10,445 --> 00:20:14,125
Speaker 1:  make more stuff with this? Right? I will say DeepSeek,

335
00:20:15,115 --> 00:20:18,245
Speaker 1:  there's some copy going around that it had written about like the nature

336
00:20:18,245 --> 00:20:21,165
Speaker 1:  of tools, which is a very VERGE thing to write about. Dieter made an entire

337
00:20:21,165 --> 00:20:24,165
Speaker 1:  video about whether or not computers or tools or instruments once amount

338
00:20:24,165 --> 00:20:27,805
Speaker 1:  of time. It was like pretty good, you know, it's like not great,

339
00:20:28,185 --> 00:20:30,325
Speaker 1:  but that it had some bangers in there, you know,

340
00:20:30,425 --> 00:20:33,525
Speaker 5:  It, it didn't sound like chat GPT, which I mean is a compliment right there.

341
00:20:33,525 --> 00:20:36,365
Speaker 5:  Is this like, yeah, I think we're all kind of learning how to

342
00:20:37,455 --> 00:20:41,085
Speaker 5:  sense when something is written by ai and it didn't sound like that.

343
00:20:41,305 --> 00:20:45,285
Speaker 5:  It was weird and kind of overdone and overwrought and it was very much

344
00:20:45,285 --> 00:20:49,005
Speaker 5:  like a overly pretentious college sophomore writing a paper

345
00:20:49,065 --> 00:20:52,685
Speaker 5:  for their English class. But like that's a lot better than chat gt

346
00:20:52,925 --> 00:20:53,605
Speaker 5:  so I'll take it.

347
00:20:53,955 --> 00:20:57,365
Speaker 1:  Yeah. It's not like a, a receptionist who's kind of annoyed that you're there,

348
00:20:57,365 --> 00:21:01,045
Speaker 1:  which is chat GT's default tone. Like I'll be so corporate that you will

349
00:21:01,045 --> 00:21:04,765
Speaker 1:  go away is kind of how Chad p writes. But I just think that's like

350
00:21:04,835 --> 00:21:08,605
Speaker 1:  very fascinating. Like what are these, what is this tool for? And the first

351
00:21:08,625 --> 00:21:12,485
Speaker 1:  answer with the image generators was art, right? And then the

352
00:21:12,485 --> 00:21:16,005
Speaker 1:  second answer was the chat bots will write me 10 more episodes of

353
00:21:16,005 --> 00:21:19,325
Speaker 1:  Seinfeld and then it was like, they'll kill Hollywood. And then we just kind

354
00:21:19,325 --> 00:21:23,165
Speaker 1:  of like brought it all the way down to like, it'll be a friend that'll, that'll

355
00:21:23,165 --> 00:21:26,925
Speaker 1:  book an Uber for you and maybe buy a sandwich. And it's like, this is a

356
00:21:26,925 --> 00:21:30,805
Speaker 1:  really small vision at the end of the day, just as we're talking,

357
00:21:30,885 --> 00:21:33,765
Speaker 1:  I think you, you just kind of mentioned it, David, open eyes like in talks

358
00:21:33,765 --> 00:21:37,445
Speaker 1:  to raise more money at a $340 billion

359
00:21:37,445 --> 00:21:40,925
Speaker 1:  valuation. This is right on top of them announcing

360
00:21:40,955 --> 00:21:44,485
Speaker 1:  $500 billion to do Stargate, which is not real,

361
00:21:45,065 --> 00:21:48,255
Speaker 1:  right? That's the most like Trump talk about Trump administration staying

362
00:21:48,255 --> 00:21:51,095
Speaker 1:  in the White House announcing a project that you're not gonna build as like

363
00:21:51,095 --> 00:21:53,175
Speaker 1:  the heart of the Trump administration experience.

364
00:21:53,485 --> 00:21:56,415
Speaker 5:  It's a giant novelty check. Like it really is. Yeah. That, that whole thing

365
00:21:56,415 --> 00:21:59,495
Speaker 5:  was a giant novelty check for $500 billion. Like sure,

366
00:21:59,755 --> 00:22:02,935
Speaker 1:  But the reality check on this whole industry right now is you're spending

367
00:22:02,995 --> 00:22:06,855
Speaker 1:  all this money 'cause no one was stopping you, everyone wanted

368
00:22:06,855 --> 00:22:10,615
Speaker 1:  crypto to be it and crypto wasn't it? So then maybe I'll be it. And

369
00:22:10,885 --> 00:22:13,775
Speaker 1:  because it can do something as opposed to crypto,

370
00:22:14,605 --> 00:22:17,935
Speaker 1:  like all the big companies are like, look at this new thing our our phones

371
00:22:17,955 --> 00:22:21,055
Speaker 1:  can do. It's Apple intelligence. Like, great,

372
00:22:21,955 --> 00:22:25,865
Speaker 1:  but it hasn't done enough yet. And then Deepsea

373
00:22:25,865 --> 00:22:29,785
Speaker 1:  comes along and they're doing it for way cheaper and

374
00:22:29,785 --> 00:22:33,385
Speaker 1:  I still don't think it's good enough, but it's so much cheaper that it's

375
00:22:33,385 --> 00:22:37,345
Speaker 1:  like wiping out this like investment thesis that you need all of the

376
00:22:37,345 --> 00:22:38,265
Speaker 1:  money to make a friend,

377
00:22:38,795 --> 00:22:42,785
Speaker 5:  Right? And again, it goes back to the thing of you have to start to

378
00:22:42,785 --> 00:22:46,625
Speaker 5:  explain to people what you're making. And I think like the this,

379
00:22:46,655 --> 00:22:50,435
Speaker 5:  this, this we are making God thing isn't

380
00:22:50,565 --> 00:22:53,715
Speaker 5:  gonna work forever. It's already starting to come unwound. Like a thing I've

381
00:22:53,715 --> 00:22:54,435
Speaker 5:  really noticed is

382
00:22:56,615 --> 00:23:00,275
Speaker 5:  as Sam Altman and others have started to pull back on

383
00:23:00,465 --> 00:23:04,355
Speaker 5:  what they claim a GI is this idea that

384
00:23:04,355 --> 00:23:07,755
Speaker 5:  we are like marching towards a singularity where these things are going to

385
00:23:07,755 --> 00:23:11,035
Speaker 5:  be as good as humans at everything is like

386
00:23:11,265 --> 00:23:15,035
Speaker 5:  getting more nebulous and weird. And so like if

387
00:23:15,395 --> 00:23:19,195
Speaker 5:  anything, the goal posts are getting fuzzier for these companies,

388
00:23:19,295 --> 00:23:22,515
Speaker 5:  not more clear. And so it's, it's wild that

389
00:23:23,095 --> 00:23:26,795
Speaker 5:  it continues to be this frothy when it feels

390
00:23:27,025 --> 00:23:30,995
Speaker 5:  less and less like anybody has any idea what they're going towards here

391
00:23:31,575 --> 00:23:34,795
Speaker 5:  except it will reinvent the economy.

392
00:23:35,775 --> 00:23:39,395
Speaker 5:  And that's what you have to say because that's the only way to raise 25 billion

393
00:23:39,465 --> 00:23:42,795
Speaker 5:  more dollars is to claim you are going to change the way that people live

394
00:23:42,795 --> 00:23:46,475
Speaker 5:  their lives. And like that's how Adam Neuman convinced

395
00:23:46,585 --> 00:23:49,435
Speaker 5:  Masa Stone to give him a lot of money. And that went super great for WeWork

396
00:23:49,655 --> 00:23:52,675
Speaker 5:  and we're just, we're just doing it again at even more scale now.

397
00:23:52,775 --> 00:23:56,675
Speaker 1:  It is notable that SoftBank is investing into at large

398
00:23:56,675 --> 00:23:59,955
Speaker 1:  valuations. It's also notable that, you know, Sam Altman is, you know, making

399
00:23:59,955 --> 00:24:03,635
Speaker 1:  videos where he is saying AI will require us to renegotiate a social

400
00:24:03,955 --> 00:24:07,195
Speaker 1:  contract. And Mark Andreessen is like, absolutely not. And then in the next

401
00:24:07,195 --> 00:24:10,795
Speaker 1:  breath, mark Andreessen is tweeting, like imagine a world in which wages

402
00:24:10,795 --> 00:24:13,955
Speaker 1:  go to zero and everything is free. And it's like, well that's renegotiating

403
00:24:13,955 --> 00:24:17,235
Speaker 1:  the social contract mark. Yeah. I don't what are you talking like that you're

404
00:24:17,235 --> 00:24:20,515
Speaker 5:  Also, none of that is going to, I cannot say clearly enough. None of that

405
00:24:20,515 --> 00:24:24,315
Speaker 5:  is going to happen. It's just not like some if,

406
00:24:24,315 --> 00:24:27,995
Speaker 5:  if, if that does happen, play this clip for me and I will be suitably

407
00:24:27,995 --> 00:24:31,555
Speaker 5:  embarrassed as I sit on the yacht that I got for free because of ai. Like,

408
00:24:31,705 --> 00:24:35,275
Speaker 5:  this is not going to happen. And so, and until then it's like, okay,

409
00:24:36,225 --> 00:24:39,875
Speaker 5:  sure, let's work backwards from reshape the way humans live our lives.

410
00:24:40,095 --> 00:24:43,995
Speaker 5:  What's the, what's the first killer app that you're

411
00:24:43,995 --> 00:24:47,355
Speaker 5:  gonna, like, what are you gonna make for me that is great that I like and

412
00:24:47,355 --> 00:24:47,955
Speaker 5:  use all the time.

413
00:24:49,535 --> 00:24:53,385
Speaker 1:  It's, it's Apple intelligence too. God, you can, you can summarize

414
00:24:53,385 --> 00:24:54,985
Speaker 1:  any email you want at the click of a button.

415
00:24:56,015 --> 00:24:56,305
Speaker 5:  Yeah,

416
00:24:57,585 --> 00:25:01,425
Speaker 1:  I saw a a, an unnamed big tech executive and

417
00:25:01,785 --> 00:25:05,185
Speaker 1:  I just said the words Apple intelligence and we just started laughing.

418
00:25:07,425 --> 00:25:07,985
Speaker 5:  I think that's right.

419
00:25:08,535 --> 00:25:11,945
Speaker 1:  That was basically where, where they were at with it. There are two other

420
00:25:12,035 --> 00:25:15,185
Speaker 1:  funny parts of the DeepSeek story. Interesting, but funny

421
00:25:16,285 --> 00:25:19,425
Speaker 1:  one is that DeepSeek is the result of,

422
00:25:20,245 --> 00:25:23,105
Speaker 1:  you know, Biden administration policies to restrict chips in China.

423
00:25:24,125 --> 00:25:27,985
Speaker 1:  And so it really feels like a, a bunch of US AI researchers

424
00:25:28,085 --> 00:25:31,945
Speaker 1:  got lazy and they were not optimizing their code enough.

425
00:25:32,405 --> 00:25:36,225
Speaker 1:  And the Chinese researchers with H 800 chips, which

426
00:25:36,345 --> 00:25:40,065
Speaker 1:  are nerfed H one hundreds over-optimize

427
00:25:40,065 --> 00:25:43,945
Speaker 1:  those chips to get better results than the H one hundreds, which is

428
00:25:43,945 --> 00:25:47,625
Speaker 1:  just a like a classic of computers story, right?

429
00:25:47,805 --> 00:25:50,465
Speaker 1:  Yes. Like you start out with 64 kilobytes of Ram and you just like worked

430
00:25:50,465 --> 00:25:52,905
Speaker 1:  your way up from there and then the old head programmers are like, these

431
00:25:52,905 --> 00:25:56,705
Speaker 1:  kids today are wasted cycles on animations. Like it's all,

432
00:25:56,705 --> 00:25:58,785
Speaker 1:  it's it's just there. It it's the same story.

433
00:25:59,095 --> 00:26:02,505
Speaker 5:  Yeah. I don't know if it's laziness. I think, I mean there's, there's certainly

434
00:26:02,505 --> 00:26:03,985
Speaker 5:  some laziness they were

435
00:26:04,145 --> 00:26:07,985
Speaker 1:  Counting on an infinite supply of processing power. Right? Right.

436
00:26:08,405 --> 00:26:11,385
Speaker 5:  But that's like, it's, it's, you know, what is it? Constraints breed creativity.

437
00:26:11,385 --> 00:26:14,745
Speaker 5:  Like yeah, there is something to that. But I also think, I don't know that

438
00:26:14,745 --> 00:26:18,625
Speaker 5:  they were sitting around being like, well we don't have to try hard because

439
00:26:18,645 --> 00:26:21,625
Speaker 5:  the chips will do it for us. Actually, now that I say that I do, I I know

440
00:26:21,625 --> 00:26:24,505
Speaker 5:  for sure they were kind of doing that. See what I'm saying? Yeah, no, you're

441
00:26:24,505 --> 00:26:24,865
Speaker 5:  right. Once

442
00:26:24,865 --> 00:26:27,385
Speaker 1:  You worked it all way out, you're like, oh, they didn't, they were just like,

443
00:26:27,385 --> 00:26:30,425
Speaker 1:  we'll just buy more chips like Sam, get more money and buy more chips. Right.

444
00:26:30,595 --> 00:26:30,945
Speaker 1:  There

445
00:26:30,945 --> 00:26:34,905
Speaker 5:  Was, there was a brute forcing to this problem that everyone

446
00:26:34,905 --> 00:26:38,745
Speaker 5:  just assumed was going to be how we get better was that like, and

447
00:26:38,745 --> 00:26:41,785
Speaker 5:  they've been making changes to the foundational technology and like transformers

448
00:26:41,785 --> 00:26:44,445
Speaker 5:  are better than they were in 20, 20 17 when people started talking about

449
00:26:44,445 --> 00:26:47,885
Speaker 5:  this stuff. Like they're doing that, but nobody

450
00:26:49,105 --> 00:26:52,725
Speaker 5:  at these companies spent a lot of time thinking about how do I do this on

451
00:26:52,725 --> 00:26:54,645
Speaker 5:  worse chips because they just had the good chips.

452
00:26:54,935 --> 00:26:58,165
Speaker 1:  Right. That's true. And you know, the, you know, a month ago we were talking

453
00:26:58,165 --> 00:27:01,405
Speaker 1:  about scaling laws. I'm like, can you just throw more horsepower and more

454
00:27:01,485 --> 00:27:04,525
Speaker 1:  data at this problem to make the models more efficient? And the answer was

455
00:27:04,525 --> 00:27:08,125
Speaker 1:  like, kind of no. Right. And that might have been when the bottom should

456
00:27:08,125 --> 00:27:11,365
Speaker 1:  have fallen out of all these stock prices. Yeah. 'cause that answer was no

457
00:27:11,365 --> 00:27:13,685
Speaker 1:  then this moment was coming inevitably. Well,

458
00:27:13,685 --> 00:27:16,805
Speaker 5:  But then what they all did in response was say, okay, well we're, we're running

459
00:27:16,835 --> 00:27:20,245
Speaker 5:  into walls with our, you know, frontier models at these

460
00:27:20,525 --> 00:27:24,445
Speaker 5:  unbelievably huge things. GPT five is reportedly not

461
00:27:24,515 --> 00:27:28,365
Speaker 5:  what OpenAI wanted it to be. Like, nobody can find kind of the next

462
00:27:28,395 --> 00:27:32,285
Speaker 5:  huge turn. And then they were like, but if we do even

463
00:27:32,285 --> 00:27:33,365
Speaker 5:  bigger data centers,

464
00:27:34,265 --> 00:27:34,485
Speaker 1:  And

465
00:27:34,485 --> 00:27:37,325
Speaker 5:  It's, it's the same thing to your point. It wasn't like, let's go back down

466
00:27:37,545 --> 00:27:41,125
Speaker 5:  to the metal here and figure out like from first

467
00:27:41,125 --> 00:27:44,485
Speaker 5:  principles how to do this. It was what if we throw even more chips at it?

468
00:27:44,635 --> 00:27:46,045
Speaker 5:  Yeah. Which is how you get Nvidia.

469
00:27:46,285 --> 00:27:49,965
Speaker 1:  Which is, and, and, and very much like in, you know, and in Liz and I have

470
00:27:49,965 --> 00:27:52,325
Speaker 1:  been talking about parts of the Nvidia story for a while now. I think she's

471
00:27:52,325 --> 00:27:55,925
Speaker 1:  even written it up. Like there are startups that are financing their

472
00:27:56,165 --> 00:27:59,885
Speaker 1:  purchase of Nvidia chips with the Nvidia chips is the collateral on the loan.

473
00:27:59,955 --> 00:28:03,485
Speaker 1:  Yeah. Because their assumption is that there will be so much demand for usage,

474
00:28:03,485 --> 00:28:07,005
Speaker 1:  they'll be able to just like make it work. And it's like, that's a little,

475
00:28:08,185 --> 00:28:10,965
Speaker 1:  that's got some NFT vibes to it. You know what I'm seriously?

476
00:28:12,225 --> 00:28:15,685
Speaker 1:  And you just see, okay, well they didn't, the Biden administration

477
00:28:16,225 --> 00:28:20,165
Speaker 1:  put restrictions on the chips we could sell in China. And the

478
00:28:20,165 --> 00:28:23,685
Speaker 1:  Chinese researchers at DeepSeek were able to

479
00:28:24,535 --> 00:28:28,445
Speaker 1:  ultra optimize those chips in a way that US engineers

480
00:28:28,445 --> 00:28:32,325
Speaker 1:  and researchers had no incentive to do. And then this is so that's funny,

481
00:28:32,325 --> 00:28:36,125
Speaker 1:  right? That's like a unintended consequence of a restriction.

482
00:28:36,155 --> 00:28:39,685
Speaker 1:  Yeah. That like fine, like it's funny

483
00:28:40,105 --> 00:28:43,925
Speaker 1:  in the sense that like trying to Nerf the Chinese AI market

484
00:28:43,935 --> 00:28:47,725
Speaker 1:  ended up tanking the US stock market. Weird. It's pretty good.

485
00:28:47,745 --> 00:28:51,485
Speaker 1:  That's not what you want there. And then the second extremely funny

486
00:28:51,595 --> 00:28:54,845
Speaker 1:  part is this idea of distillation

487
00:28:55,255 --> 00:28:58,805
Speaker 1:  where part of DeepSeek was trained by just like ripping off open AI models

488
00:28:58,825 --> 00:29:02,645
Speaker 1:  in various ways. That's pretty cool. And like, you know, Sam Alman is like

489
00:29:02,685 --> 00:29:06,325
Speaker 1:  a little bit outraged by it and it's like, Sam, you stole the entire internet.

490
00:29:07,605 --> 00:29:09,245
Speaker 1:  Everyone's so mad at you. Yeah.

491
00:29:09,285 --> 00:29:12,685
Speaker 5:  I mean DeepSeek is, is apparently was built on top of

492
00:29:13,085 --> 00:29:17,045
Speaker 5:  llama meta's model and used vast quantities

493
00:29:17,045 --> 00:29:21,005
Speaker 5:  of output from chat GPT to train itself, which

494
00:29:21,005 --> 00:29:24,845
Speaker 5:  is just, it's just perfect. Like, it's just if every single stereotype you

495
00:29:24,845 --> 00:29:27,565
Speaker 5:  wanna believe about the AI industry, every single stereotype you wanna believe

496
00:29:27,565 --> 00:29:30,845
Speaker 5:  about China, like it's all right there. It's great. Yeah.

497
00:29:30,845 --> 00:29:33,685
Speaker 1:  And at least meta to its credit is like, yeah, we made the model open source.

498
00:29:33,685 --> 00:29:37,325
Speaker 1:  Yeah. Like that's what it's here for. Yeah. Like, you know, open,

499
00:29:37,505 --> 00:29:40,765
Speaker 1:  but the open eye making noises that like they have evidence that this was

500
00:29:40,785 --> 00:29:42,005
Speaker 1:  and it's like, yeah, who cares?

501
00:29:42,245 --> 00:29:44,165
Speaker 5:  Yeah. Like, don't know they broke your terms of service.

502
00:29:44,385 --> 00:29:47,925
Speaker 1:  How awful. Lot of evidence about you my friends. Like, you know, what else

503
00:29:47,925 --> 00:29:51,325
Speaker 1:  should take the bottom out of the AI industry losing the number of copier

504
00:29:51,325 --> 00:29:54,725
Speaker 1:  lawsuits that you are currently engaged in. Yep. I feel like I need to disclose

505
00:29:54,725 --> 00:29:58,605
Speaker 1:  at this time that Fox Media has signed some kind of licensing deal with

506
00:29:58,605 --> 00:30:02,165
Speaker 1:  open AI to make our stuff show up, show up and search GPT. And

507
00:30:03,365 --> 00:30:05,485
Speaker 1:  honestly no one uses it, so it's fine. It

508
00:30:05,485 --> 00:30:05,805
Speaker 5:  Seems right.

509
00:30:06,855 --> 00:30:08,915
Speaker 1:  That's way above our heads. But there it is. There's a disclosure.

510
00:30:08,985 --> 00:30:12,875
Speaker 5:  They just turned Gemini on for our Google workspace. That's a thing that

511
00:30:12,955 --> 00:30:13,915
Speaker 5:  happened this week. Disclosure,

512
00:30:14,455 --> 00:30:18,355
Speaker 1:  We had to have a meeting trying to turn it off and keeping our data

513
00:30:18,355 --> 00:30:21,395
Speaker 1:  inside our own house, even that Right. We're gonna take a big bite outta

514
00:30:21,395 --> 00:30:25,155
Speaker 1:  the search market and knock Google off it like it hasn't played out.

515
00:30:25,335 --> 00:30:29,275
Speaker 1:  And that's the biggest identifiable market so far. Right? You

516
00:30:29,275 --> 00:30:32,715
Speaker 1:  will ask the robot questions and it'll deliver you answers. Right. We're

517
00:30:32,715 --> 00:30:36,635
Speaker 1:  gonna make Google dance all of that to I that doesn't seem to have

518
00:30:36,635 --> 00:30:36,995
Speaker 1:  played out.

519
00:30:37,495 --> 00:30:41,475
Speaker 5:  No, and I think that's, it's why everybody's betting so big on agents, right?

520
00:30:41,545 --> 00:30:45,395
Speaker 5:  It's like, okay, somewhere between we're gonna take a cut of

521
00:30:45,395 --> 00:30:49,275
Speaker 5:  your DoorDash order and we are going to be the payments

522
00:30:49,275 --> 00:30:53,115
Speaker 5:  processor is like, that's the idea. Everybody has had like, I think

523
00:30:53,115 --> 00:30:55,995
Speaker 5:  it was you that said to me at one point on the show that everybody's business

524
00:30:55,995 --> 00:30:59,515
Speaker 5:  model ultimately is just payments processing. Like it's really, it's really

525
00:30:59,665 --> 00:31:03,235
Speaker 5:  extremely true. Yeah. Everybody is just desperately trying to do payments

526
00:31:03,235 --> 00:31:06,595
Speaker 5:  processing, including now x this week, which we can talk about. But

527
00:31:08,295 --> 00:31:11,475
Speaker 5:  the, the, these ideas just aren't working like it is true. And we're gonna

528
00:31:11,545 --> 00:31:14,995
Speaker 5:  talk about a lot of the stuff that people are doing with these tools.

529
00:31:15,335 --> 00:31:18,915
Speaker 5:  It is true that people like using these tools and that that is a meaningful

530
00:31:19,245 --> 00:31:23,115
Speaker 5:  thing. But the question of what are they

531
00:31:23,135 --> 00:31:26,915
Speaker 5:  for? And maybe even more importantly, what is the business here?

532
00:31:28,725 --> 00:31:31,755
Speaker 5:  Super duper unknown. Like it's, it's important to keep reminding you that

533
00:31:32,935 --> 00:31:36,675
Speaker 5:  the bigger open AI gets, the more money it loses and that

534
00:31:36,995 --> 00:31:40,875
Speaker 5:  continues to be true. And like there was like the Uber thing

535
00:31:40,875 --> 00:31:44,675
Speaker 5:  that was like, you know, when when we win we will, we'll take over and we'll

536
00:31:44,675 --> 00:31:47,915
Speaker 5:  raise our prices. And like they, they did win, they did raise their prices.

537
00:31:48,625 --> 00:31:52,115
Speaker 5:  They had to retrench in like a huge way and now they're making

538
00:31:52,705 --> 00:31:56,635
Speaker 5:  this much money. Yeah. That's not coming for any of these AI companies

539
00:31:56,635 --> 00:32:00,035
Speaker 5:  Anytime soon, no matter how many billions of dollars you raise, it's just

540
00:32:00,035 --> 00:32:00,355
Speaker 5:  not coming.

541
00:32:00,685 --> 00:32:03,595
Speaker 1:  Right. 'cause they, their product isn't as useful as when you push a button

542
00:32:03,675 --> 00:32:05,355
Speaker 1:  a Toyota Highlander shows up. Right?

543
00:32:06,005 --> 00:32:09,835
Speaker 5:  Right. And it's, it's being commoditized even faster than that.

544
00:32:09,905 --> 00:32:13,195
Speaker 5:  Like the, the thing everybody was worried about, about Uber 12 years ago

545
00:32:13,375 --> 00:32:16,875
Speaker 5:  was robo taxis are gonna come and just end, just throw all this into chaos

546
00:32:17,135 --> 00:32:20,835
Speaker 5:  that's happening in the AI world. Like all this stuff, whatever you build

547
00:32:21,295 --> 00:32:25,115
Speaker 5:  as an AI model company is going to be copied

548
00:32:25,135 --> 00:32:28,875
Speaker 5:  and bested like immediately by six other companies.

549
00:32:28,975 --> 00:32:30,195
Speaker 5:  And this is, this is just what we're seeing.

550
00:32:30,295 --> 00:32:31,155
Speaker 1:  So let me ask you this.

551
00:32:32,685 --> 00:32:35,245
Speaker 1:  Microsoft had earnings this week. They're, they're doing, they're doing fine.

552
00:32:36,645 --> 00:32:39,805
Speaker 1:  But you know, it seems like the Microsoft Open AI relationship has, has been

553
00:32:39,805 --> 00:32:43,045
Speaker 1:  rocky for a minute. Yep. You know, they were like part of the Stargate announcement,

554
00:32:43,105 --> 00:32:45,605
Speaker 1:  not really part of the Stargate announcement that was mostly funded by Oracle

555
00:32:45,785 --> 00:32:49,445
Speaker 1:  and SoftBank, Nadela was asked about

556
00:32:49,445 --> 00:32:52,605
Speaker 1:  $500 billion and he is like, we've already committed $80 billion to Azure

557
00:32:52,605 --> 00:32:55,165
Speaker 1:  build outs and I'm good for my 80 billion. And we just like wouldn't talk

558
00:32:55,165 --> 00:32:58,725
Speaker 1:  about Stargate. Yep. And then this week, Altman and Nadella, they Altman

559
00:32:58,725 --> 00:33:01,605
Speaker 1:  post a selfie of the two of them. So the next chapter of this relationship

560
00:33:01,625 --> 00:33:04,765
Speaker 1:  is gonna be great. But, but then they, the companies had also announced that

561
00:33:04,765 --> 00:33:07,925
Speaker 1:  like Microsoft's deal basically goes until 2030.

562
00:33:09,305 --> 00:33:12,225
Speaker 1:  I can't tell what's going on with these two companies right now. And especially

563
00:33:12,725 --> 00:33:16,585
Speaker 1:  he posted that after DeepSeek and after Nadella is trying to

564
00:33:16,585 --> 00:33:19,585
Speaker 1:  calm down the market by talking about Jevons Paradox, which is what you're

565
00:33:19,585 --> 00:33:22,025
Speaker 1:  describing where you make the thing cheaper so people use it more.

566
00:33:24,025 --> 00:33:25,435
Speaker 1:  What is going on with these two companies?

567
00:33:25,955 --> 00:33:29,285
Speaker 5:  I I think it's as simple as they are.

568
00:33:30,105 --> 00:33:33,365
Speaker 5:  It it they're sort of in a mutually assured destruction thing where like

569
00:33:35,045 --> 00:33:38,245
Speaker 5:  Microsoft benefits so much from open AI being

570
00:33:39,395 --> 00:33:43,035
Speaker 5:  a huge success because it has these agreements for the all the

571
00:33:43,085 --> 00:33:46,075
Speaker 5:  cloud stuff that's happening because it's such a huge investor in the company.

572
00:33:46,345 --> 00:33:50,315
Speaker 5:  Like as OpenAI gets bigger, it uses more Azure,

573
00:33:50,525 --> 00:33:53,715
Speaker 5:  which is more money for Microsoft. Like Microsoft literally wins coming and

574
00:33:53,875 --> 00:33:57,815
Speaker 5:  going when it comes to open ai. So if I'm Satya Nadella,

575
00:33:57,815 --> 00:34:01,735
Speaker 5:  it makes sense to be doing both of these things. You both want to shore

576
00:34:01,735 --> 00:34:05,655
Speaker 5:  this stuff up in house in case this company implodes in some

577
00:34:05,655 --> 00:34:08,295
Speaker 5:  meaningful way, which a, it has already done once and B

578
00:34:09,385 --> 00:34:12,935
Speaker 5:  could just happen again. Like I, the the people

579
00:34:13,295 --> 00:34:17,055
Speaker 5:  a lot this week have been talking about the fact that most of open AI's

580
00:34:17,055 --> 00:34:20,775
Speaker 5:  co-founders aren't there anymore. There are many litigious people circling

581
00:34:20,775 --> 00:34:24,055
Speaker 5:  around this company. There are lawsuits circling like OpenAI is both

582
00:34:25,495 --> 00:34:29,135
Speaker 5:  absolutely chockfull of money and like pretty fragile in a lot of ways. Yeah.

583
00:34:29,135 --> 00:34:32,695
Speaker 5:  So I think if you're Sacie Nadella, you're like, okay, I I need to

584
00:34:32,915 --> 00:34:36,695
Speaker 5:  not be fully reliant on it, but I also am going to make so much

585
00:34:36,695 --> 00:34:40,335
Speaker 5:  money from OpenAI if it hits that it's,

586
00:34:40,445 --> 00:34:43,695
Speaker 5:  it's worth a selfie and like a nice tweet a couple of times a year. Yeah.

587
00:34:43,695 --> 00:34:47,615
Speaker 5:  Like that's a pretty low risk way to at least keep the peace enough

588
00:34:48,965 --> 00:34:52,455
Speaker 5:  that open. Nobody gets super nervous about open

589
00:34:53,005 --> 00:34:56,335
Speaker 1:  Like so one selfie. Yeah. Like he's got a chart in his office. It's like

590
00:34:56,335 --> 00:34:59,615
Speaker 1:  options one selfie. Right. Invest

591
00:34:59,635 --> 00:35:01,255
Speaker 1:  $200 billion. Yeah.

592
00:35:01,645 --> 00:35:05,335
Speaker 5:  Leak phone call to Sam. Like Yeah. Be caught

593
00:35:05,335 --> 00:35:08,655
Speaker 5:  Having lunch with Sam at Sweet Green is like

594
00:35:08,675 --> 00:35:12,295
Speaker 5:  $200 billion. Like it's, I I really, I think, I think these companies hate

595
00:35:12,295 --> 00:35:16,125
Speaker 5:  each other and I think they know it's better to be stuck

596
00:35:16,365 --> 00:35:20,285
Speaker 5:  together for a while. Yeah. And to root for each other because it will be

597
00:35:20,285 --> 00:35:24,045
Speaker 5:  successful for everybody. But I think they will be in, in they,

598
00:35:24,285 --> 00:35:27,885
Speaker 5:  I think Sam and Satya both go home at the end of day and like throw

599
00:35:27,935 --> 00:35:30,245
Speaker 5:  darts at a, at a the other person's

600
00:35:30,245 --> 00:35:34,125
Speaker 1:  Face. The really interesting thing is, you know, Microsoft's core

601
00:35:34,445 --> 00:35:36,205
Speaker 1:  business is Azure, like in the realist way.

602
00:35:37,755 --> 00:35:41,735
Speaker 1:  Azure's doing great, you know, their earnings are out. There are

603
00:35:41,735 --> 00:35:45,335
Speaker 1:  parts of the Azure business that are like different than the others. Like

604
00:35:45,335 --> 00:35:48,055
Speaker 1:  their new customers are not signing up for ai, they're just signing up to

605
00:35:48,055 --> 00:35:51,015
Speaker 1:  be cloud customers. And they kind of announced during the earning call like,

606
00:35:51,285 --> 00:35:54,455
Speaker 1:  yeah, we, we stopped trying to do that. Like we're we're back to just trying

607
00:35:54,455 --> 00:35:57,135
Speaker 1:  to get cloud customers. Yeah. I mean

608
00:35:57,135 --> 00:36:00,015
Speaker 5:  That's what that, that has happened in a real way like that that's actually

609
00:36:00,015 --> 00:36:03,455
Speaker 5:  been a big moment that both Microsoft and Google were like, okay, we're gonna

610
00:36:03,455 --> 00:36:07,415
Speaker 5:  charge $20 a month extra on top of the normal like

611
00:36:07,725 --> 00:36:11,575
Speaker 5:  workspace or office subscription and you're gonna get all the AI stuff

612
00:36:12,135 --> 00:36:16,095
Speaker 5:  and people didn't like $20 per month per person for

613
00:36:16,935 --> 00:36:20,335
Speaker 5:  nebulous AI features. Turned out to be an awful lot to ask. And

614
00:36:21,065 --> 00:36:24,695
Speaker 5:  these companies, like you said, they need more cloud customers and they need

615
00:36:24,695 --> 00:36:28,615
Speaker 5:  people to use the AI in order for the AI to get any good. And so they

616
00:36:28,615 --> 00:36:32,375
Speaker 5:  have just walked it all the way back and what they want is they

617
00:36:32,375 --> 00:36:36,255
Speaker 5:  want you to use their tools that make them money and then they wanna

618
00:36:36,415 --> 00:36:39,855
Speaker 5:  shove AI at you so that you use it and maybe someday we'll pay for it.

619
00:36:40,405 --> 00:36:44,055
Speaker 1:  Yeah. We'll see how that goes. But that's like a really

620
00:36:44,335 --> 00:36:47,215
Speaker 1:  interesting piece that Microsoft is at least reselling whatever capability

621
00:36:47,215 --> 00:36:51,175
Speaker 1:  it has. Google's reselling it, Amazon is reselling it in Nvidia

622
00:36:51,185 --> 00:36:55,055
Speaker 1:  sells the chips and then they, you know, like Jensen Flash on the world on

623
00:36:55,055 --> 00:36:58,815
Speaker 1:  is jet made of money. Meta is really interesting in here. 'cause they're

624
00:36:58,815 --> 00:37:02,125
Speaker 1:  the only ones who don't resell the capacity. They're, they are buying all

625
00:37:02,125 --> 00:37:05,605
Speaker 1:  the GPUs. They're building the huge data center the size of Manhattan like

626
00:37:05,775 --> 00:37:09,765
Speaker 1:  crazy. But they sell none of it. It's all for meta

627
00:37:10,585 --> 00:37:14,245
Speaker 1:  and either that's a lot of risk, right. They have to get the value out of

628
00:37:14,245 --> 00:37:17,935
Speaker 1:  it or they figured it out and in their

629
00:37:18,215 --> 00:37:18,375
Speaker 1:  earnings,

630
00:37:19,905 --> 00:37:23,795
Speaker 1:  this isn't great, I don't love it, but they're like, we are, we've increased

631
00:37:23,975 --> 00:37:27,835
Speaker 1:  ad targeting yield by sorting through all of the ads. We can show

632
00:37:27,835 --> 00:37:31,795
Speaker 1:  people faster and that's what the GPUs are for. And it's like,

633
00:37:31,815 --> 00:37:35,745
Speaker 1:  oh they actually figured it out. They know why they're spending

634
00:37:35,745 --> 00:37:38,145
Speaker 1:  the money. They're not just trying to resell capacity to somebody who's gonna

635
00:37:38,145 --> 00:37:39,985
Speaker 1:  make a killer app or

636
00:37:41,675 --> 00:37:45,395
Speaker 1:  assuming that they can make Digital co Yeah. They're like, oh, there's a

637
00:37:45,395 --> 00:37:48,475
Speaker 1:  thing we do with this that actually goes faster because we're doing it.

638
00:37:48,665 --> 00:37:51,595
Speaker 5:  Yeah. I mean the killer app for AI might be ad targeting.

639
00:37:51,865 --> 00:37:55,435
Speaker 1:  Yeah. Which is terrifying. Like, I, I wanna be clear. That sucks. Yeah. Yeah.

640
00:37:55,895 --> 00:37:59,155
Speaker 5:  And, and Meta would love for you to not think about that and instead think

641
00:37:59,155 --> 00:38:02,915
Speaker 5:  about smart glasses. But like Meta is building those data

642
00:38:02,915 --> 00:38:05,515
Speaker 5:  centers to serve you better ads, not to

643
00:38:06,625 --> 00:38:10,595
Speaker 5:  like let you wear glasses on your face if they can do all that

644
00:38:10,595 --> 00:38:14,475
Speaker 5:  stuff. Terrific. But the business case for that Manhattan sized data

645
00:38:14,475 --> 00:38:17,355
Speaker 5:  center is advertising. Yeah. And I think that's, that's what is happening

646
00:38:17,575 --> 00:38:20,675
Speaker 5:  across the board. It's true for Google, it's true for a lot of these companies.

647
00:38:20,675 --> 00:38:24,555
Speaker 5:  Like ads are going to be how a lot of

648
00:38:24,555 --> 00:38:28,235
Speaker 5:  these things support themselves in a way that I think people are not yet

649
00:38:28,235 --> 00:38:28,595
Speaker 5:  ready for

650
00:38:29,055 --> 00:38:32,675
Speaker 1:  In and in the case of Meta and Google specifically, they have already started

651
00:38:33,185 --> 00:38:37,165
Speaker 1:  down this road where the end result is you're

652
00:38:37,195 --> 00:38:41,165
Speaker 1:  some advertiser and you just type in who you want to

653
00:38:41,165 --> 00:38:44,965
Speaker 1:  reach and you put in the first version of your ad and then

654
00:38:44,985 --> 00:38:48,725
Speaker 1:  it makes infinite variations of that ad targeted at

655
00:38:48,925 --> 00:38:52,405
Speaker 1:  specific people. So everyone is seeing their own ad. Right. And that's what

656
00:38:52,405 --> 00:38:56,285
Speaker 1:  the generative AI is for. And this is either good or bad, you know, like

657
00:38:56,835 --> 00:38:57,805
Speaker 1:  it's bad, but,

658
00:39:01,045 --> 00:39:04,905
Speaker 1:  but like, you know, from those company's perspective, they're,

659
00:39:04,905 --> 00:39:08,705
Speaker 1:  they might spend less money in advertising but meta might make more because

660
00:39:08,705 --> 00:39:12,665
Speaker 1:  they're showing more ads that convert more clicks and purchases. And you

661
00:39:12,665 --> 00:39:16,185
Speaker 1:  can just see how you get to this is all good. And then you can also see how

662
00:39:16,185 --> 00:39:19,625
Speaker 1:  the user experience of the platforms will go to hell

663
00:39:19,855 --> 00:39:23,705
Speaker 1:  because they're gonna be just choked to death with AI slop.

664
00:39:23,925 --> 00:39:27,585
Speaker 1:  Yep. And extremely weird AI ads and it's already happening. Like

665
00:39:27,865 --> 00:39:31,745
Speaker 1:  I don't want to pretend, I don't wanna be like, this is some

666
00:39:31,745 --> 00:39:35,425
Speaker 1:  like vaporware idea. Like TikTok has some of this capability in their

667
00:39:35,695 --> 00:39:39,265
Speaker 1:  adec today. Meta and Google are already

668
00:39:39,285 --> 00:39:42,985
Speaker 1:  saying to creators, let people chat with AI avatars of yourself. Yep. Like

669
00:39:43,215 --> 00:39:47,105
Speaker 1:  they are training the user base to see AI

670
00:39:47,105 --> 00:39:50,745
Speaker 1:  content and think of it as the regular part of the user experience. And then

671
00:39:50,775 --> 00:39:54,705
Speaker 1:  it's just, it's just one turn to Hey Ford, give us

672
00:39:54,745 --> 00:39:58,155
Speaker 1:  a bunch of money and we'll have infinite ad for

673
00:39:58,515 --> 00:40:02,315
Speaker 1:  explorers that like we know everything about this person and

674
00:40:02,315 --> 00:40:05,235
Speaker 1:  we'll be like, Hey, we saw that you bought a lot of stuff at the store today

675
00:40:06,105 --> 00:40:09,955
Speaker 1:  that would fit better in a Ford Explorer. Like down to that level of

676
00:40:09,955 --> 00:40:10,275
Speaker 1:  targeting.

677
00:40:10,335 --> 00:40:13,875
Speaker 5:  Or it'll show me the color of Ford Explorer that I'm most likely

678
00:40:14,175 --> 00:40:17,475
Speaker 5:  to like in the setting that is most likely to make me wanna buy it because

679
00:40:17,475 --> 00:40:20,835
Speaker 5:  that is data that these companies have. Like Yeah. It's, it's gonna get

680
00:40:21,335 --> 00:40:24,915
Speaker 5:  really weird. And I think the, the like uncanny value of

681
00:40:25,435 --> 00:40:28,515
Speaker 5:  targeted advertising is about to be explored in a really fascinating way.

682
00:40:28,795 --> 00:40:32,595
Speaker 5:  I will say one of the funniest things about this is that it is going to make

683
00:40:32,735 --> 00:40:36,515
Speaker 5:  Amazon look even stupider because Amazon still does the ad where

684
00:40:36,515 --> 00:40:40,235
Speaker 5:  you're like, I buy a hat on Amazon and then for six months Amazon's ads are

685
00:40:40,235 --> 00:40:42,515
Speaker 5:  just like, do you wanna buy that exact hat that you already bought? Yeah.

686
00:40:42,655 --> 00:40:46,515
Speaker 5:  How about a hat? Do you like hats? Here's a hat. And I'm like,

687
00:40:46,535 --> 00:40:49,475
Speaker 5:  and it's just like Amazon. If, if AI can fix that,

688
00:40:50,315 --> 00:40:51,935
Speaker 5:  I'm in Gimme ai

689
00:40:52,445 --> 00:40:55,975
Speaker 1:  Just boil, just stop with the boil the ocean. Yeah. Take this hat away from

690
00:40:55,975 --> 00:40:59,815
Speaker 1:  me. I it's weird. Look, okay. We've done enough negativity around

691
00:40:59,915 --> 00:41:03,855
Speaker 1:  ai. I do think that this thing is a bubble and it has popped

692
00:41:03,855 --> 00:41:07,415
Speaker 1:  to some extent and it might bubble again, but

693
00:41:07,445 --> 00:41:11,175
Speaker 1:  there's just, I don't, it's not just DeepSeek,

694
00:41:11,325 --> 00:41:15,295
Speaker 1:  it's the products. It's I'll I'm gonna compare it to, to

695
00:41:15,295 --> 00:41:18,895
Speaker 1:  cell networks again. 'cause it, this is just, this is just the world I was

696
00:41:18,895 --> 00:41:22,735
Speaker 1:  raised in. The money we spent on LTE was

697
00:41:22,735 --> 00:41:26,215
Speaker 1:  good. Like If you, If you, we have a lot of newer

698
00:41:26,215 --> 00:41:29,855
Speaker 1:  listeners, but If you are with here with us from the beginning, you'll

699
00:41:30,175 --> 00:41:34,125
Speaker 1:  remember that the iPhone getting LTE was a big deal. Yeah. Right. And

700
00:41:34,125 --> 00:41:38,085
Speaker 1:  the iPhone launching without 3G was like a, a legitimate criticism

701
00:41:38,105 --> 00:41:42,045
Speaker 1:  of the product at that time. It had edge, it had two g networking, it was

702
00:41:42,045 --> 00:41:45,645
Speaker 1:  too slow and putting wifi in it was the big, like the big

703
00:41:45,645 --> 00:41:49,605
Speaker 1:  innovation. This is all impossible to think about right now. Yeah. But the,

704
00:41:49,605 --> 00:41:53,405
Speaker 1:  when the iPhone got 3G, that was a big, big deal

705
00:41:53,715 --> 00:41:57,045
Speaker 1:  because it meant that it could access the internet faster and was usable

706
00:41:57,045 --> 00:42:00,945
Speaker 1:  on the go as an internet device when it got LTE that meant it

707
00:42:00,945 --> 00:42:04,865
Speaker 1:  could do video and that's when a bunch of mobile video like took off

708
00:42:04,965 --> 00:42:08,785
Speaker 1:  and like became a thing when it got 5G

709
00:42:09,505 --> 00:42:12,105
Speaker 1:  everyone was like, that's gonna happen again. Right. And it

710
00:42:12,105 --> 00:42:13,305
Speaker 5:  Just didn't, it didn't,

711
00:42:13,725 --> 00:42:14,945
Speaker 1:  It super didn't.

712
00:42:15,135 --> 00:42:18,705
Speaker 5:  What is this for is a question people should ask much more often

713
00:42:18,835 --> 00:42:22,705
Speaker 5:  about technology. I will say, I just wanna say one

714
00:42:22,815 --> 00:42:25,745
Speaker 5:  nice thing about DeepSeek and then we should take a break and talk about

715
00:42:25,855 --> 00:42:27,025
Speaker 5:  cool things people do with ai.

716
00:42:28,825 --> 00:42:32,665
Speaker 5:  Learning about DeepSeek sent me down a deep rabbit hole on

717
00:42:32,665 --> 00:42:36,465
Speaker 5:  chain of thought reasoning. Yeah. Which is one of the technologies that,

718
00:42:36,535 --> 00:42:40,305
Speaker 5:  that DeepSeek did a really good job with. Basically like these reasoning

719
00:42:40,305 --> 00:42:43,865
Speaker 5:  models, all these companies are coming out with that kind of show you the

720
00:42:43,865 --> 00:42:47,105
Speaker 5:  way that they think. Truthfully I thought of that as kind of a gimmick that

721
00:42:47,105 --> 00:42:50,505
Speaker 5:  was like less doing something different and more just like

722
00:42:51,775 --> 00:42:55,745
Speaker 5:  play pretending to be human so that you trust it more. But what it actually

723
00:42:55,745 --> 00:42:58,705
Speaker 5:  is is DeepSeek and other companies have found ways to

724
00:42:59,135 --> 00:43:02,985
Speaker 5:  basically, instead of generating one token that is like the answer to your

725
00:43:03,145 --> 00:43:07,105
Speaker 5:  question, it's able to generate a bunch at a time and actually go step

726
00:43:07,125 --> 00:43:10,385
Speaker 5:  by step and try to think through a problem. And what DeepSeek figured out

727
00:43:10,385 --> 00:43:14,305
Speaker 5:  how to do is how to use a technique called reinforcement learning, which

728
00:43:14,305 --> 00:43:17,625
Speaker 5:  basically rewards and punishes a model when it does things right and wrong

729
00:43:18,085 --> 00:43:22,025
Speaker 5:  and teach it how to do that kind of step by step

730
00:43:22,025 --> 00:43:25,185
Speaker 5:  reasoning inside of the model. And so there's a, there's a great moment in

731
00:43:25,185 --> 00:43:29,145
Speaker 5:  one of the technical papers where the model starts to

732
00:43:29,145 --> 00:43:32,985
Speaker 5:  answer a question, goes step by step, gets it wrong, realizes that

733
00:43:32,985 --> 00:43:36,905
Speaker 5:  it is going down the wrong road and is going to make a mistake backtracks

734
00:43:36,905 --> 00:43:40,785
Speaker 5:  and starts over. And that is like a powerful moment in

735
00:43:40,885 --> 00:43:43,825
Speaker 5:  the history of ai. 'cause all this stuff is like If you just think about

736
00:43:43,825 --> 00:43:46,585
Speaker 5:  it as like it's generating the next word every time, it doesn't actually

737
00:43:46,585 --> 00:43:49,505
Speaker 5:  have the capability to go back and be like, wait, where did I go off track?

738
00:43:49,765 --> 00:43:53,545
Speaker 5:  It just keeps going off track. And with something like this, like

739
00:43:53,895 --> 00:43:56,625
Speaker 5:  this is the kind of thing you will start to see in every other model very

740
00:43:56,625 --> 00:44:00,465
Speaker 5:  quickly. I think because the, the reasoning model is a, they have there,

741
00:44:00,465 --> 00:44:04,345
Speaker 5:  there are pretty strong studies to suggest that it's vastly

742
00:44:04,345 --> 00:44:08,185
Speaker 5:  more accurate when you ask these things to go step by step. Because instead

743
00:44:08,185 --> 00:44:11,385
Speaker 5:  of trying to answer the question all at once, it answers it step by step,

744
00:44:11,385 --> 00:44:14,505
Speaker 5:  which is like the correct way to do things. But that also,

745
00:44:15,205 --> 00:44:19,145
Speaker 5:  the more you let the model do that, like

746
00:44:19,145 --> 00:44:22,105
Speaker 5:  the, the longer you let it take and the more steps you let it take and the

747
00:44:22,105 --> 00:44:25,705
Speaker 5:  more you let it think and try and poke around and do stuff, the more accurate

748
00:44:25,705 --> 00:44:29,625
Speaker 5:  it actually is. So there, there is a path toward we can do this

749
00:44:29,625 --> 00:44:33,585
Speaker 5:  better. And it looks like these reasoning models and there

750
00:44:33,585 --> 00:44:36,585
Speaker 5:  are huge compute challenges there. There are huge latency problems there.

751
00:44:36,585 --> 00:44:40,345
Speaker 5:  Like that stuff is gonna be hard to do in a way that is like good user experience.

752
00:44:40,645 --> 00:44:41,305
Speaker 5:  But the idea

753
00:48:43,145 --> 00:48:46,195
Speaker 1:  All right, so last week

754
00:48:47,405 --> 00:48:51,385
Speaker 1:  we pointed out a gap that I, I think we see in all of our AI coverage. So

755
00:48:51,385 --> 00:48:55,345
Speaker 1:  you might have even felt in the last segment that there's how people

756
00:48:55,345 --> 00:48:57,945
Speaker 1:  feel about ai, which in our comments is bad

757
00:48:59,905 --> 00:49:01,065
Speaker 1:  straightforwardly. There is a real like

758
00:49:01,275 --> 00:49:04,065
Speaker 5:  Principled stance against it. Yeah. As a whole that

759
00:49:04,065 --> 00:49:06,905
Speaker 1:  A lot of people, it's antiar on machine and maybe it is, like I said the

760
00:49:06,905 --> 00:49:10,145
Speaker 1:  other, the other leg of the stool that could get kicked out is the New York

761
00:49:10,145 --> 00:49:14,065
Speaker 1:  Times winning its copyright case against Open Act or Sarah

762
00:49:14,065 --> 00:49:15,345
Speaker 1:  Silverman or any of these other people. Wasn't

763
00:49:15,345 --> 00:49:19,305
Speaker 5:  It Mark Andreesen, who was like, If you don't let us steal from the whole

764
00:49:19,545 --> 00:49:20,705
Speaker 5:  internet, this all falls apart.

765
00:49:20,935 --> 00:49:24,665
Speaker 1:  Yeah. He wrote it in a letter to the copyright office. Perfect. Where he

766
00:49:24,665 --> 00:49:27,305
Speaker 1:  was like, I need you to, I need you to do me a favor. Then he brought the

767
00:49:27,305 --> 00:49:29,785
Speaker 1:  government. So we'll see what happens. Yeah. There's that by the copyright

768
00:49:29,785 --> 00:49:33,145
Speaker 1:  office. This week announced a clarification of its rules saying If you make

769
00:49:33,165 --> 00:49:36,785
Speaker 1:  art with AI, that gets copyrightable. But art made entirely by AI is not

770
00:49:37,125 --> 00:49:40,625
Speaker 1:  no gray area there that won't be confusing. Ten one. Super, super straightforward.

771
00:49:40,925 --> 00:49:44,745
Speaker 1:  Anyhow, there's a gap. The, the gap that I'm saying is people are mad about

772
00:49:44,745 --> 00:49:48,585
Speaker 1:  it, especially in our comments across the internet and various social media

773
00:49:48,865 --> 00:49:52,625
Speaker 1:  platforms. You, you, it's movie studio puts up a poster with ai, they get

774
00:49:52,625 --> 00:49:55,225
Speaker 1:  bullied right off the internet. Then there's what the companies tell us,

775
00:49:55,225 --> 00:49:58,625
Speaker 1:  which is people are using the stuff like crazy. DeepSeek is at the top of

776
00:49:58,625 --> 00:50:02,305
Speaker 1:  the app store chat, GPT most popular app by growth

777
00:50:02,805 --> 00:50:06,265
Speaker 1:  in the history of all applications. I think it's a little bit less than that,

778
00:50:06,265 --> 00:50:09,785
Speaker 1:  but you get what I'm saying. And then there's just the stats that we hear

779
00:50:09,785 --> 00:50:12,025
Speaker 1:  from the companies themselves that are sort of like less invested in the

780
00:50:12,025 --> 00:50:15,505
Speaker 1:  hype. Like the one I always call out, I call out last week Adobe

781
00:50:15,805 --> 00:50:18,745
Speaker 1:  Chanti new Ryan told me on decoder that people use Generat to fill at the

782
00:50:18,745 --> 00:50:22,465
Speaker 1:  same rate as layers in Photoshop, which means they just have opened Photoshop,

783
00:50:22,465 --> 00:50:26,305
Speaker 1:  right? Like that's just an identity. Yeah. There's this big gap

784
00:50:26,305 --> 00:50:29,885
Speaker 1:  in how people are saying they feel about ai, the hype

785
00:50:30,455 --> 00:50:33,765
Speaker 1:  we're gonna make digital God, and then the sort of like mundane reality of

786
00:50:33,805 --> 00:50:36,845
Speaker 1:  a lot of people are just using this stuff and a lot of people like it, but

787
00:50:36,845 --> 00:50:40,045
Speaker 1:  they don't perceive themselves as like using ai. So we asked

788
00:50:41,145 --> 00:50:44,755
Speaker 1:  listeners, send us what you're doing. And David, it sounds like

789
00:50:44,755 --> 00:50:46,195
Speaker 1:  everybody responded. Yeah,

790
00:50:46,275 --> 00:50:50,075
Speaker 5:  We got a lot. We heard from a bunch of folks on social. We got a, like

791
00:50:50,295 --> 00:50:54,115
Speaker 5:  dozens of calls to The Vergecast hotline. We got a whole bunch of

792
00:50:54,115 --> 00:50:57,795
Speaker 5:  emails. Everybody it turns out,

793
00:50:58,705 --> 00:51:02,595
Speaker 5:  uses AI and there are at least and a lot of people out there

794
00:51:02,615 --> 00:51:05,955
Speaker 5:  who, who had a use case that they wanted to share. So I figured

795
00:51:06,495 --> 00:51:09,835
Speaker 5:  we should just go through a bunch of 'em. And I think I tried to pull ones

796
00:51:09,835 --> 00:51:10,235
Speaker 5:  that are

797
00:51:11,795 --> 00:51:15,275
Speaker 5:  re reflective of trends that we saw in the responses.

798
00:51:16,335 --> 00:51:19,115
Speaker 5:  So if, if you're down, I'm gonna read you some emails and I'm gonna play

799
00:51:19,115 --> 00:51:22,675
Speaker 5:  you some hotlines and we're just gonna talk about it. 'cause they all made

800
00:51:22,835 --> 00:51:25,795
Speaker 5:  me feel a lot of feelings and we need to sort through them together.

801
00:51:26,225 --> 00:51:28,435
Speaker 1:  What is the show? But sorting through feelings today, that's, that's

802
00:51:28,435 --> 00:51:32,355
Speaker 5:  We're here for, so I'm gonna not say the emailers names

803
00:51:32,545 --> 00:51:36,435
Speaker 5:  because some of this stuff is very personal. And also most people in their

804
00:51:36,435 --> 00:51:40,275
Speaker 5:  emails don't explicitly give us instructions whether to say their

805
00:51:40,275 --> 00:51:44,155
Speaker 5:  names. I think a few of the folks on the hotline say their names, but I feel

806
00:51:44,155 --> 00:51:46,915
Speaker 5:  like If you say it into the voicemail knowing we're gonna play it on the

807
00:51:46,915 --> 00:51:50,835
Speaker 5:  show. That's fair game. So I'm so sorry if I'm outing you here on the first

808
00:51:50,835 --> 00:51:51,075
Speaker 5:  cast.

809
00:51:51,265 --> 00:51:52,915
Speaker 1:  This first email is from Sam A

810
00:51:55,335 --> 00:51:56,355
Speaker 5:  All right, let's start with

811
00:51:57,225 --> 00:52:00,155
Speaker 1:  Ilya S has a note for us about how good AI is.

812
00:52:01,475 --> 00:52:05,355
Speaker 5:  So here's one email we got. It says, I was recently diagnosed with

813
00:52:05,355 --> 00:52:08,235
Speaker 5:  a stomach issue. So it's been difficult for me to find and know things that

814
00:52:08,235 --> 00:52:11,035
Speaker 5:  trigger and don't trigger a flare up. So when it's really acting up, it's

815
00:52:11,035 --> 00:52:13,595
Speaker 5:  super easy for me to just take a picture of a menu at a restaurant or bar

816
00:52:13,695 --> 00:52:17,515
Speaker 5:  and just ask what, what can I have? And from what I've told chat GBT from

817
00:52:17,515 --> 00:52:20,395
Speaker 5:  the doctors, it can just tell me what's safest to eat and drink. Even telling

818
00:52:20,415 --> 00:52:23,635
Speaker 5:  me the exact ingredients that are safe in those dishes. That's awesome.

819
00:52:24,395 --> 00:52:27,475
Speaker 5:  Like unambiguously cool use case in my,

820
00:52:27,535 --> 00:52:31,075
Speaker 1:  That's a great use case. It's just like I, again, I'm just being negative.

821
00:52:32,895 --> 00:52:35,715
Speaker 1:  I don't know how the restaurant actually made the food. I don't know if the

822
00:52:35,715 --> 00:52:39,595
Speaker 1:  things hallucinate like that. It's just, this is what I mean. Like I'm

823
00:52:39,595 --> 00:52:43,555
Speaker 1:  just prone to see the risk. And I think that's an email where

824
00:52:43,555 --> 00:52:44,435
Speaker 1:  people see the benefit.

825
00:52:45,095 --> 00:52:48,995
Speaker 5:  So this is, you just brought up the exact thing that is going to come

826
00:52:48,995 --> 00:52:52,955
Speaker 5:  up an awful lot in these, which is you are

827
00:52:52,955 --> 00:52:56,665
Speaker 5:  going to see a trend in which there is a question of

828
00:52:56,805 --> 00:53:00,785
Speaker 5:  how much should I trust this? And one of the things that

829
00:53:00,785 --> 00:53:03,425
Speaker 5:  consistently surprised me is a lot of people saying it doesn't really matter.

830
00:53:03,615 --> 00:53:07,385
Speaker 5:  That like, I understand it has flaws. It might not be perfect. It's something

831
00:53:07,905 --> 00:53:11,745
Speaker 5:  like, here, let me, let me read you one. I've often felt intimidated by

832
00:53:11,745 --> 00:53:15,185
Speaker 5:  the professional and technical demands of public service. However, chat GBT

833
00:53:15,185 --> 00:53:17,985
Speaker 5:  has given me the ability to communicate more effectively, present my ideas

834
00:53:17,985 --> 00:53:21,585
Speaker 5:  professionally and quickly understand complex topics from rebuilding water

835
00:53:21,585 --> 00:53:24,545
Speaker 5:  treatment facilities and developing billing systems to ensuring compliance

836
00:53:24,545 --> 00:53:27,845
Speaker 5:  with the Environmental protection Agency regulations. I've been able to handle

837
00:53:28,165 --> 00:53:31,995
Speaker 5:  responsibilities I never imagined I could take on again. Very

838
00:53:31,995 --> 00:53:35,715
Speaker 5:  cool. To me, I immediately read that and go, how many of

839
00:53:35,935 --> 00:53:39,635
Speaker 5:  the EPA regulations do you think chat GPT just flatly it's wrong. Like

840
00:53:39,665 --> 00:53:43,515
Speaker 5:  yeah, just, just fully hallucinates some made

841
00:53:43,715 --> 00:53:47,515
Speaker 5:  up thing. And I feel a, with a lot of these things I keep coming

842
00:53:47,515 --> 00:53:51,355
Speaker 5:  back to like, it, it's gonna work for you until it blows up spectacularly

843
00:53:51,355 --> 00:53:54,155
Speaker 5:  in your face. Right? And it's like, it's the same with the eating in the

844
00:53:54,275 --> 00:53:58,155
Speaker 5:  restaurant. Like as long as it's right, and let's say it's right

845
00:53:58,155 --> 00:54:02,075
Speaker 5:  80% of the time, which is probably kind to

846
00:54:02,185 --> 00:54:05,555
Speaker 5:  most of these systems. But let's say it's you take a picture of your food

847
00:54:05,575 --> 00:54:09,235
Speaker 5:  or you, you upload a PDF of a regulatory document,

848
00:54:10,105 --> 00:54:13,765
Speaker 5:  ask it questions, it's often gonna be right. But when it's wrong,

849
00:54:14,145 --> 00:54:18,055
Speaker 5:  If you trust it, really bad things can happen. And so

850
00:54:18,135 --> 00:54:21,975
Speaker 5:  I, I feel like where I fall on the like, but what if

851
00:54:21,975 --> 00:54:25,815
Speaker 5:  it doesn't work? Scale might just be different than some

852
00:54:25,815 --> 00:54:28,215
Speaker 5:  other people. But that just is the thing I keep thinking about with all of

853
00:54:28,215 --> 00:54:28,335
Speaker 5:  these.

854
00:54:28,645 --> 00:54:32,545
Speaker 1:  Yeah. I, my, I don't trust computers. I have a weird job

855
00:54:32,885 --> 00:54:36,785
Speaker 1:  in that regard, but like, as David knows, when I record this podcast at home,

856
00:54:37,465 --> 00:54:41,105
Speaker 1:  I insist on recording locally into another

857
00:54:41,385 --> 00:54:45,065
Speaker 1:  recorder. Yeah. And not letting my Mac do the recording. 'cause I just don't

858
00:54:45,065 --> 00:54:48,505
Speaker 1:  trust the Mac, which is weird because I should,

859
00:54:49,565 --> 00:54:50,945
Speaker 5:  But I will say on the, on the flip side,

860
00:54:50,945 --> 00:54:53,865
Speaker 1:  But like, that's the, but I just like, that's the frame that I'm in. Right.

861
00:54:53,865 --> 00:54:57,585
Speaker 1:  Whereas it's like what I hear that email is

862
00:54:59,205 --> 00:55:00,905
Speaker 1:  not about trust. It's about

863
00:55:02,855 --> 00:55:06,615
Speaker 1:  I worked out this idea so that I would have the confidence to go have, like,

864
00:55:06,615 --> 00:55:09,615
Speaker 1:  have the idea. And that's really different and interesting.

865
00:55:10,505 --> 00:55:11,805
Speaker 5:  How, how So say more about that.

866
00:55:12,705 --> 00:55:16,145
Speaker 1:  I mean, I, that person sounds like they have a very complicated job. They,

867
00:55:16,145 --> 00:55:18,505
Speaker 1:  they know, they have to know how to do everything and they're like designing

868
00:55:18,505 --> 00:55:19,585
Speaker 1:  a water plant, right? Yeah,

869
00:55:22,055 --> 00:55:24,935
Speaker 1:  I hear that. And I think, well you already, you already did know how, right?

870
00:55:25,005 --> 00:55:28,095
Speaker 1:  Like you had the capability of doing it and what you needed

871
00:55:28,795 --> 00:55:32,495
Speaker 1:  was a conversation or a more easily

872
00:55:32,855 --> 00:55:36,535
Speaker 1:  adjusted reference material or whatever to get you over the hump.

873
00:55:37,435 --> 00:55:40,955
Speaker 1:  Like you just solved your imposter syndrome. You,

874
00:55:41,215 --> 00:55:45,125
Speaker 1:  the, the AI didn't do it for you, right. You just, it just helped

875
00:55:45,125 --> 00:55:47,725
Speaker 1:  you get to where you were inevitably gonna be.

876
00:55:48,845 --> 00:55:52,365
Speaker 5:  I, I think there's something to that, and I think there's also a, a recurring

877
00:55:52,365 --> 00:55:55,125
Speaker 5:  theme in a lot of these that is basically like, I

878
00:55:56,435 --> 00:56:00,295
Speaker 5:  just needed something, I didn't have a tool

879
00:56:00,295 --> 00:56:03,575
Speaker 5:  to do this thing and now I have a tool to do this thing. And even if it doesn't

880
00:56:03,575 --> 00:56:07,375
Speaker 5:  necessarily do it well, the fact that it does it at all is

881
00:56:07,375 --> 00:56:10,935
Speaker 5:  something right. And it's like I I I think that I started with the food one

882
00:56:10,935 --> 00:56:14,925
Speaker 5:  because that is one that there isn't a clean other answer to,

883
00:56:14,935 --> 00:56:18,005
Speaker 5:  right? Like I see on Reddit all the time, the people who go to a restaurant

884
00:56:18,005 --> 00:56:21,245
Speaker 5:  with like those huge placards of like, here's all the food I can't eat. And

885
00:56:21,245 --> 00:56:23,540
Speaker 5:  I hand them to the chef and then the chef posted it on Reddit being like,

886
00:56:23,540 --> 00:56:27,485
Speaker 5:  this asshole came to my restaurant. And to have something that is like,

887
00:56:27,525 --> 00:56:31,125
Speaker 5:  I can take a picture of this and it is going to give me even, even reasonably

888
00:56:31,445 --> 00:56:34,925
Speaker 5:  reliable information about what this is and whether I can safely eat it

889
00:56:36,135 --> 00:56:39,885
Speaker 5:  reasonably reliable and easy is probably a huge step up from the

890
00:56:39,885 --> 00:56:42,805
Speaker 5:  alternative. Yeah. In this case. And I think that's, that's something really

891
00:56:42,805 --> 00:56:46,245
Speaker 5:  meaningful. And so there's this sense that like, something is better than

892
00:56:46,245 --> 00:56:49,885
Speaker 5:  nothing is, is so pervasive through a lot of this. And I think is really

893
00:56:50,125 --> 00:56:52,245
Speaker 5:  interesting in a way that I didn't give it credit for. All right. Let me,

894
00:56:52,245 --> 00:56:53,725
Speaker 5:  let me play you a couple of voicemails. We got,

895
00:56:54,225 --> 00:56:58,045
Speaker 7:  Hi, this is Alex. And I wanted to call in to talk about how I

896
00:56:58,105 --> 00:57:02,005
Speaker 7:  use LLMs in my daily life. I use chat

897
00:57:02,125 --> 00:57:05,965
Speaker 7:  EBT as an on demand advisor of

898
00:57:05,975 --> 00:57:09,765
Speaker 7:  sorts. You know, how can I improve this recipe? How much

899
00:57:09,825 --> 00:57:13,765
Speaker 7:  cat lit should I have, be using per month? What are some good ways to save

900
00:57:13,765 --> 00:57:15,925
Speaker 7:  time at the gym if these are my priorities?

901
00:57:17,595 --> 00:57:20,675
Speaker 7:  A lot of these things are things I could use Google for,

902
00:57:21,625 --> 00:57:25,485
Speaker 7:  but it's just quicker to ask chat GBT. Another

903
00:57:25,485 --> 00:57:29,005
Speaker 7:  thing I use it for is as a way to

904
00:57:29,325 --> 00:57:32,725
Speaker 7:  organize my thoughts. So if I'm ruminating about something,

905
00:57:33,755 --> 00:57:37,165
Speaker 7:  I'll just talk to chat GBT It's no

906
00:57:37,285 --> 00:57:41,045
Speaker 7:  replacement for my therapist, but it helps reorient my thinking

907
00:57:41,045 --> 00:57:45,005
Speaker 7:  when I need it to. I know I could also get this from

908
00:57:45,005 --> 00:57:48,765
Speaker 7:  my friends and sometimes I do, but Chatt PP is nearly

909
00:57:48,945 --> 00:57:51,045
Speaker 7:  always available instantly.

910
00:57:51,605 --> 00:57:52,965
Speaker 5:  I find that very compelling, I have to say.

911
00:57:53,825 --> 00:57:57,595
Speaker 1:  Yeah. I mean, again, it's, it's hard to tell anyone

912
00:57:57,595 --> 00:58:01,315
Speaker 1:  they're like doing it wrong. Yeah. Like, that's that's great. I'm like very

913
00:58:01,315 --> 00:58:04,515
Speaker 1:  happy that that's working in that way. I, the thing that grabs me at that

914
00:58:04,515 --> 00:58:08,355
Speaker 1:  one in particular is it is the thing

915
00:58:08,465 --> 00:58:12,225
Speaker 1:  that Google is afraid of, right? But it's also the thing

916
00:58:12,225 --> 00:58:16,095
Speaker 1:  that these products have shown themselves to be the worst at.

917
00:58:16,805 --> 00:58:20,775
Speaker 1:  Yeah. Like how much cat litter should my, my cat use every month is,

918
00:58:21,405 --> 00:58:25,125
Speaker 1:  well, first of all it depends on the cat, but second of all, it's like that's

919
00:58:25,125 --> 00:58:28,605
Speaker 1:  the one where it's like, I you should, you should feed your cat rocks. Like,

920
00:58:28,605 --> 00:58:29,445
Speaker 1:  it's like that's what will

921
00:58:29,445 --> 00:58:31,925
Speaker 5:  Happen. Put glue in the cat litter. It's like it's just sitting there. Yeah.

922
00:58:32,665 --> 00:58:36,125
Speaker 1:  And that I think this is just really eyeopening for me in that

923
00:58:37,105 --> 00:58:40,365
Speaker 1:  you're to your point, it's just good enough. Yeah. It's just something. Yeah.

924
00:58:40,425 --> 00:58:44,125
Speaker 1:  And we, and we and as we have covered exhaustively typing that sort of thing

925
00:58:44,125 --> 00:58:46,645
Speaker 1:  into Google right now is like getting mugged in an alley.

926
00:58:46,795 --> 00:58:49,545
Speaker 5:  Seriously. Yeah. No, I, there there is a real

927
00:58:50,515 --> 00:58:54,305
Speaker 5:  thing that I I I keep hearing in a bunch of these that is just how

928
00:58:54,305 --> 00:58:57,745
Speaker 5:  aggressively Google fumbled the bag of search that like

929
00:58:58,095 --> 00:59:01,705
Speaker 5:  what people want is better Google not chat

930
00:59:01,865 --> 00:59:05,665
Speaker 5:  GPT and Google just threw all of that away. But I think

931
00:59:05,805 --> 00:59:08,385
Speaker 5:  one of the things that you say all the time is that the internet is about

932
00:59:08,385 --> 00:59:12,105
Speaker 5:  to be flooded with c plus stuff. And I think what we might be under

933
00:59:12,105 --> 00:59:15,945
Speaker 5:  rating is how valuable a c plus can be sometimes. Like maybe a

934
00:59:15,985 --> 00:59:19,705
Speaker 5:  c plus delivered to me easily and quickly at all times is

935
00:59:19,945 --> 00:59:20,705
Speaker 5:  actually something

936
00:59:22,715 --> 00:59:26,515
Speaker 5:  Hmm. Is that bleak as hell? Probably, but it's something.

937
00:59:26,535 --> 00:59:27,715
Speaker 5:  All right, lemme keep, lemme keep going.

938
00:59:28,775 --> 00:59:31,715
Speaker 1:  That's like the, that's like how they, why they invented legacy admissions

939
00:59:31,715 --> 00:59:34,195
Speaker 1:  at Harvard. Like that's what you're describing. Like what if we can just

940
00:59:34,195 --> 00:59:35,795
Speaker 1:  accept a flood of c plus applicants.

941
00:59:35,995 --> 00:59:38,835
Speaker 5:  Yeah. And that goes fine. What talking about, alright, here's the next one.

942
00:59:39,375 --> 00:59:39,595
Speaker 5:  Hey,

943
00:59:39,595 --> 00:59:43,355
Speaker 8:  This is Jake. I've listening to your recent podcast where you've mentioned

944
00:59:43,735 --> 00:59:47,115
Speaker 8:  how you want people to call in and say what they use Chad GBT for in the

945
00:59:47,115 --> 00:59:51,035
Speaker 8:  real world. I use it weekly to make my shopping list. I ask

946
00:59:51,035 --> 00:59:54,715
Speaker 8:  you to make recipes based off of what I have in my house and what I'm going

947
00:59:54,715 --> 00:59:58,635
Speaker 8:  to buy. Try to make it where I can get it as macro friendly as

948
00:59:58,835 --> 01:00:02,275
Speaker 8:  possible. One of the features that I've started using Mo more recently

949
01:00:02,655 --> 01:00:05,995
Speaker 8:  is whenever I make a shopping list, I ask it to make an optimized

950
01:00:06,635 --> 01:00:10,315
Speaker 8:  shopping pattern for a standard location of where I'm going. So I can go

951
01:00:10,315 --> 01:00:13,755
Speaker 8:  into Costco or Aldi, I can make sure that I'm shopping the most sufficient

952
01:00:14,495 --> 01:00:18,475
Speaker 8:  and then I ask it to find substitutes to make for recipes, top

953
01:00:18,475 --> 01:00:21,915
Speaker 8:  values. Use it for pretty much anything that it comes to

954
01:00:22,535 --> 01:00:26,435
Speaker 8:  for like vacations, trips you for planning. That's

955
01:00:26,435 --> 01:00:30,035
Speaker 8:  the, those are some of the main use cases that I use it for outside of

956
01:00:30,535 --> 01:00:31,115
Speaker 8:  all of my work.

957
01:00:31,535 --> 01:00:35,315
Speaker 1:  All right, so first of all, I just, I that's all great. I,

958
01:00:36,335 --> 01:00:39,755
Speaker 1:  me Google blew it. You just described everything Google has ever demoed at

959
01:00:39,755 --> 01:00:42,875
Speaker 1:  Google io right? I know, but you're doing it in Chachi. I know.

960
01:00:43,945 --> 01:00:47,635
Speaker 5:  Yeah, yeah. Like there, there's a, there are a hundred Google Keep features

961
01:00:47,635 --> 01:00:50,035
Speaker 5:  that you just described. No one uses.

962
01:00:51,625 --> 01:00:55,195
Speaker 5:  Yeah, no, I, this to me was the one that I was like the most

963
01:00:55,625 --> 01:00:59,355
Speaker 5:  sort of down the middle simple use case is like, I'm gonna write

964
01:00:59,555 --> 01:01:02,475
Speaker 5:  a list of stuff that I have in my fridge. You're gonna spit back some recipes

965
01:01:02,495 --> 01:01:05,275
Speaker 5:  and then I'm gonna say, can you optimize this list to go to Costco? Like

966
01:01:05,935 --> 01:01:08,595
Speaker 5:  ta GPT is great at that. Do it. Love it. Yeah.

967
01:01:08,775 --> 01:01:12,625
Speaker 1:  And then the next turn is go on Instacart with operator

968
01:01:12,685 --> 01:01:15,625
Speaker 1:  and order it for me. Right. Like you can see how you connect all those

969
01:01:15,655 --> 01:01:19,505
Speaker 5:  Dots. Next time I go to Costco, I'm going to tell Chad Gt

970
01:01:19,505 --> 01:01:23,265
Speaker 5:  to give me directions around Costco and I'm only going to follow Chad

971
01:01:23,345 --> 01:01:24,345
Speaker 5:  GBT. We're gonna see how it goes.

972
01:01:24,575 --> 01:01:28,105
Speaker 1:  This is actually the part where he said that I'm dying to know more here.

973
01:01:29,105 --> 01:01:32,965
Speaker 1:  It really implies that Chad GT knows how Costco's are

974
01:01:32,965 --> 01:01:35,965
Speaker 1:  organized and just based on my Costco in

975
01:01:36,915 --> 01:01:37,965
Speaker 1:  it's a war zone, I sure

976
01:01:38,025 --> 01:01:39,005
Speaker 5:  Costco knows how Costco

977
01:01:39,005 --> 01:01:41,965
Speaker 1:  Is organized. That's what I mean. It's like I've asked Costco employees where

978
01:01:41,965 --> 01:01:44,205
Speaker 1:  things are and they're like, no, no, no, no, no. Good luck to you sir.

979
01:01:45,355 --> 01:01:49,285
Speaker 5:  Yeah, it's a secret you're not allowed to know. But yeah, that kind

980
01:01:49,285 --> 01:01:53,165
Speaker 5:  of thing. Like I I, I am always talking to people about like what I, what

981
01:01:53,165 --> 01:01:56,965
Speaker 5:  do I think are good use cases for chat GBT And the one I always come up with

982
01:01:56,965 --> 01:02:00,845
Speaker 5:  is movie recommendations. 'cause it's like super low stakes, but actually

983
01:02:00,845 --> 01:02:03,845
Speaker 5:  a thing AI is unusually able to be good at. Where I'm like, I like these

984
01:02:03,845 --> 01:02:07,365
Speaker 5:  three movies, what else should I watch? And it's like, that's a thing LLMs

985
01:02:07,365 --> 01:02:11,205
Speaker 5:  do very well. And this kind of like, recipe remixing

986
01:02:11,205 --> 01:02:13,925
Speaker 5:  is another good example of that. So I just thought that one was great. All

987
01:02:13,925 --> 01:02:16,885
Speaker 5:  right. Let me read you another email. Real tone shift here. You ready for

988
01:02:16,885 --> 01:02:19,805
Speaker 5:  this? Yeah. We got an email from somebody who's going through a divorce

989
01:02:21,345 --> 01:02:25,285
Speaker 5:  and said, Chad GBT has been a surprising source of support. Let me

990
01:02:25,285 --> 01:02:28,965
Speaker 5:  be clear, I don't use it as a talk therapist parentheses gross, but rather

991
01:02:28,985 --> 01:02:32,085
Speaker 5:  as a tool to help me communicate effectively and with kindness, my wife is

992
01:02:32,085 --> 01:02:34,685
Speaker 5:  very sensitive to perceived criticism. So I draft emails to her and then

993
01:02:34,685 --> 01:02:38,285
Speaker 5:  use Chad GBT to help me soften the tone, make them warmer and remove anything

994
01:02:38,285 --> 01:02:40,805
Speaker 5:  that could be interpreted as critical while still ensuring my requests and

995
01:02:40,805 --> 01:02:43,805
Speaker 5:  boundaries are not diluted. Filtering emails this way has made a difficult

996
01:02:43,805 --> 01:02:46,845
Speaker 5:  process a little less hard. It's also helped me curb any instincts to be

997
01:02:46,845 --> 01:02:50,085
Speaker 5:  petty, sarcastic or passive aggressive. It helps me keep my side of the street

998
01:02:50,085 --> 01:02:50,365
Speaker 5:  clean.

999
01:02:52,245 --> 01:02:52,465
Speaker 1:  Wow.

1000
01:02:52,715 --> 01:02:53,065
Speaker 5:  Right.

1001
01:02:54,365 --> 01:02:58,145
Speaker 1:  So I'm, I'm married to a divorce lawyer. I have heard about

1002
01:02:58,325 --> 01:03:02,105
Speaker 1:  an infinite variety of passive aggressive emails between partners for

1003
01:03:02,405 --> 01:03:05,785
Speaker 1:  the past several years. I feel like I should just go tell her about this.

1004
01:03:06,055 --> 01:03:09,305
Speaker 5:  It's really interesting, right? Yeah. I, this one I was like, I don't know

1005
01:03:09,305 --> 01:03:13,025
Speaker 5:  how to feel on the one hand, it, it, it, there's like a piece of this that

1006
01:03:13,025 --> 01:03:16,985
Speaker 5:  kinda makes me sad that it's like, it's a bummer that there is a, a web app

1007
01:03:16,985 --> 01:03:20,825
Speaker 5:  that has to be your tool for this. But on the other hand, like again,

1008
01:03:20,825 --> 01:03:23,785
Speaker 5:  what's your alternative? Where else would someone go to do something like

1009
01:03:23,785 --> 01:03:23,905
Speaker 5:  this?

1010
01:03:24,125 --> 01:03:28,105
Speaker 1:  If you took this out of the bigger context of AI and all of

1011
01:03:28,105 --> 01:03:31,825
Speaker 1:  the noise and all of the hype and you were like, there's a new

1012
01:03:31,825 --> 01:03:35,625
Speaker 1:  feature in Grammarly, right. Five years ago. Yeah. Or

1013
01:03:35,625 --> 01:03:39,145
Speaker 1:  you're like, Google Docs has a new feature that just helps you, like

1014
01:03:39,405 --> 01:03:43,385
Speaker 1:  de passive aggressive in email. I don't know that anyone would have any

1015
01:03:43,625 --> 01:03:46,185
Speaker 1:  reaction to this other than that's actually pretty good. Yeah.

1016
01:03:46,375 --> 01:03:47,345
Speaker 5:  Yeah. I think that's right.

1017
01:03:47,725 --> 01:03:51,145
Speaker 1:  And it's only in the context of all the AI hype and you know, that horrible

1018
01:03:51,155 --> 01:03:54,545
Speaker 1:  Apple commercial where it's just like bad employee sends professional email.

1019
01:03:54,865 --> 01:03:56,745
Speaker 1:  Yeah. And like this is what Apple intelligence is for

1020
01:03:58,435 --> 01:04:02,265
Speaker 1:  there, there this is maybe the most human of the notes

1021
01:04:02,265 --> 01:04:05,865
Speaker 1:  that we've gotten. Yeah. Right. And that, I mean, not to say everyone else

1022
01:04:05,865 --> 01:04:09,385
Speaker 1:  isn't human, but like, this is a very emotional task that is being

1023
01:04:09,455 --> 01:04:12,865
Speaker 1:  performed. Totally. And I I I really do think If you took this one out of

1024
01:04:12,865 --> 01:04:16,585
Speaker 1:  the AI bubble in all of the baggage that comes with it,

1025
01:04:17,205 --> 01:04:19,745
Speaker 1:  you put this in any other product five years ago and everyone's like, oh,

1026
01:04:19,745 --> 01:04:20,105
Speaker 1:  this rules.

1027
01:04:20,535 --> 01:04:24,305
Speaker 5:  Yeah. Yeah. I mean, and I think it, it goes to show that

1028
01:04:24,905 --> 01:04:28,705
Speaker 5:  a, our, our vocabulary about AI is still so primitive. Like we call

1029
01:04:28,705 --> 01:04:32,465
Speaker 5:  everything ai and actually AI has lots of wildly

1030
01:04:32,465 --> 01:04:35,105
Speaker 5:  different things and we need to get to the point where we're better at talking

1031
01:04:35,105 --> 01:04:36,345
Speaker 5:  about them as different things.

1032
01:04:37,965 --> 01:04:41,665
Speaker 5:  But it also to some extent goes to show how versatile something like chat

1033
01:04:41,785 --> 01:04:45,305
Speaker 5:  GPT is, like all this stuff is happening in the same text box.

1034
01:04:45,435 --> 01:04:47,225
Speaker 5:  Right? Like it's a pretty remarkable thing.

1035
01:04:47,575 --> 01:04:50,825
Speaker 1:  Also very notable. Everyone has said Chacha Bt

1036
01:04:51,275 --> 01:04:52,265
Speaker 5:  Every single person. Yeah.

1037
01:04:52,265 --> 01:04:55,865
Speaker 1:  We haven't gotten one Dolly. We haven't got one Gemini. Nope. No,

1038
01:04:56,045 --> 01:04:57,385
Speaker 1:  no. Apple intelligence mix. A little bit of claw

1039
01:04:57,495 --> 01:04:58,585
Speaker 5:  Love in a, in a few of

1040
01:04:58,585 --> 01:04:59,785
Speaker 1:  These. Oh, it's common. Okay. Okay. A little bit

1041
01:04:59,785 --> 01:05:02,865
Speaker 5:  Of Claude love. I don't think I pulled any of the ones that people use Claude,

1042
01:05:02,865 --> 01:05:04,385
Speaker 5:  but you're

1043
01:05:04,385 --> 01:05:05,505
Speaker 1:  Just, just, sorry, calling

1044
01:05:05,505 --> 01:05:09,145
Speaker 5:  It out. Sorry Claude users. But it was, Claude was mentioned. I don't think

1045
01:05:09,145 --> 01:05:13,105
Speaker 5:  Gemini came up one time in the emails that we got brutal. But Claude

1046
01:05:13,105 --> 01:05:16,665
Speaker 5:  there people were like, oh, I mostly use chat GPT and like some Claude.

1047
01:05:17,125 --> 01:05:18,525
Speaker 5:  There was a lot of some Claude.

1048
01:05:20,725 --> 01:05:23,205
Speaker 5:  All right, lemme play you another voicemail. Here's a, again, we're just

1049
01:05:23,205 --> 01:05:25,565
Speaker 5:  gonna tone shift all over the place here 'cause that's what AI does. Here

1050
01:05:25,565 --> 01:05:25,765
Speaker 5:  we go.

1051
01:05:26,065 --> 01:05:29,885
Speaker 9:  Hey, this is Leo from Atlanta, Georgia. Neil, I asked what

1052
01:05:30,085 --> 01:05:33,925
Speaker 9:  everyone's using their AI assistance for and I

1053
01:05:33,925 --> 01:05:37,845
Speaker 9:  use mine almost exclusively to solve hard questions

1054
01:05:37,845 --> 01:05:41,605
Speaker 9:  in the word puzzle game word salad. Also to note,

1055
01:05:41,805 --> 01:05:45,445
Speaker 9:  I recently discovered that something about David's voice makes me super tired

1056
01:05:46,205 --> 01:05:49,125
Speaker 9:  and helps lull me to sleep. So I want to give a quick shout out and thank

1057
01:05:49,125 --> 01:05:52,965
Speaker 9:  you to Mr. David Pierce for improving my sleep

1058
01:05:52,965 --> 01:05:56,525
Speaker 9:  quality and helping fight my insomnia. Thanks guys. Love the show.

1059
01:05:56,785 --> 01:05:58,525
Speaker 1:  Som David's voice makes me super tired.

1060
01:06:00,495 --> 01:06:02,325
Speaker 5:  First question, was that a compliment or an insult?

1061
01:06:02,395 --> 01:06:06,325
Speaker 1:  Yeah, that's really, that's a tough one. You make me sleepy. It's like

1062
01:06:06,325 --> 01:06:06,605
Speaker 1:  a really

1063
01:06:06,875 --> 01:06:10,525
Speaker 5:  Yeah, he said thanks. I don't think he meant it, but that one

1064
01:06:10,635 --> 01:06:14,325
Speaker 5:  like another in the line of like movie recommendations, right? Like pretty

1065
01:06:14,325 --> 01:06:17,165
Speaker 5:  low stakes, pretty straightforward. This thing has a lot of information and

1066
01:06:17,165 --> 01:06:20,285
Speaker 5:  it can, it can spit some information back at me. Yeah, it works.

1067
01:06:20,425 --> 01:06:21,565
Speaker 1:  You're cheating, but it works.

1068
01:06:22,185 --> 01:06:26,125
Speaker 5:  You are cheating to be so clear. You are cheating. It's okay. I've

1069
01:06:26,125 --> 01:06:28,525
Speaker 5:  Googled, what's the word I'll answer before, like it's fine. It's

1070
01:06:28,525 --> 01:06:29,125
Speaker 1:  Bedtime for you, sir.

1071
01:06:29,225 --> 01:06:30,325
Speaker 5:  We all have bad days. Yeah.

1072
01:06:32,235 --> 01:06:35,285
Speaker 5:  Okay, let me, let me find the next one.

1073
01:06:36,635 --> 01:06:40,405
Speaker 5:  I've been fascinated by my mom's usage of ai. She's 49 and while she uses

1074
01:06:40,405 --> 01:06:43,245
Speaker 5:  it a lot for her small business writing posts, et cetera, I've noticed that

1075
01:06:43,245 --> 01:06:46,005
Speaker 5:  over the past year, she's also started to use it as a general Google replacement

1076
01:06:46,005 --> 01:06:49,405
Speaker 5:  in her personal life as well. She wants to know a fact. She asked chat GPT,

1077
01:06:49,625 --> 01:06:52,405
Speaker 5:  she has to write a WhatsApp message that requires minimum thinking. She asked

1078
01:06:52,405 --> 01:06:55,325
Speaker 5:  chat GPT last week. She found out one of her close friends is struggling

1079
01:06:55,325 --> 01:06:58,445
Speaker 5:  with her mental health and having suicidal ideations. And I shit you not,

1080
01:06:58,585 --> 01:07:01,645
Speaker 5:  she asked chat GPT what are the best ways to support someone who's depressed?

1081
01:07:03,095 --> 01:07:03,445
Speaker 5:  Again,

1082
01:07:05,125 --> 01:07:08,255
Speaker 5:  just Google fumbled the bag. Like people want better Google.

1083
01:07:08,965 --> 01:07:12,655
Speaker 1:  Yeah, but this is the one, again, I, these are real people.

1084
01:07:13,335 --> 01:07:17,235
Speaker 1:  Everyone use a computer however you want. I'm, I'm happy it's working,

1085
01:07:18,295 --> 01:07:21,715
Speaker 1:  but this is the one that's the, the blinking red danger sign,

1086
01:07:22,205 --> 01:07:25,315
Speaker 1:  right? It's like, oh, everyone just needs a friend. Like

1087
01:07:25,345 --> 01:07:25,635
Speaker 5:  Sure.

1088
01:07:25,885 --> 01:07:29,835
Speaker 1:  Right. There's, there's just an element here where this

1089
01:07:29,835 --> 01:07:33,755
Speaker 1:  is to me like this is where the robots hallucinate

1090
01:07:33,755 --> 01:07:36,515
Speaker 1:  all day long. And what you're getting is better Google

1091
01:07:38,215 --> 01:07:42,065
Speaker 1:  with worse results presumably, but in a better interface

1092
01:07:42,065 --> 01:07:44,745
Speaker 1:  or more reactive or just someone to talk to you and in a way that's natural.

1093
01:07:44,765 --> 01:07:45,265
Speaker 1:  And it's like,

1094
01:07:46,975 --> 01:07:50,385
Speaker 1:  this is the one where it's like, If you tell me this is the use case, I will

1095
01:07:50,385 --> 01:07:53,545
Speaker 1:  confidently tell you this does not support $500 billion data center built

1096
01:07:53,565 --> 01:07:54,305
Speaker 1:  out. Oh,

1097
01:07:54,345 --> 01:07:54,865
Speaker 5:  A hundred percent.

1098
01:07:54,935 --> 01:07:58,865
Speaker 1:  Because it, it, it doesn't scale like the thing

1099
01:07:58,865 --> 01:08:02,505
Speaker 1:  will happen that happened to Alexa in Siri, which is you'll do this

1100
01:08:02,515 --> 01:08:05,945
Speaker 1:  until it fails you and then you will stop thinking it can do anything more

1101
01:08:06,065 --> 01:08:09,885
Speaker 1:  than this. Right? Like you're like, Siri, tell me a joke.

1102
01:08:09,885 --> 01:08:13,565
Speaker 1:  And it's like, I don't, I give up. Right? You're like fine timers of music.

1103
01:08:13,585 --> 01:08:17,565
Speaker 1:  It is, and I'm not saying the ceiling is, the ceiling for Chacha piece

1104
01:08:17,565 --> 01:08:21,325
Speaker 1:  is so much higher, obviously, but there's still a ceiling. And the ceiling

1105
01:08:21,745 --> 01:08:25,675
Speaker 1:  is when it lies to you. Right? When it makes something up when

1106
01:08:25,675 --> 01:08:28,475
Speaker 1:  you ask it for movies and it gives you three real movies and one fake one.

1107
01:08:28,855 --> 01:08:31,475
Speaker 1:  And then people are literally showing up at public libraries asking for books

1108
01:08:31,475 --> 01:08:35,355
Speaker 1:  that don't exist. Yep. And that, that's what I mean. It's like when lawyers

1109
01:08:35,355 --> 01:08:38,635
Speaker 1:  show up in court citing cases that aren't real and they get disciplined by

1110
01:08:38,635 --> 01:08:42,595
Speaker 1:  the bar association. That's the pro, like, they

1111
01:08:42,595 --> 01:08:45,925
Speaker 1:  can't overcome. They don't know how to overcome that. And so, like this one

1112
01:08:45,925 --> 01:08:49,645
Speaker 1:  to me is like, yeah, I I I every experience is valid.

1113
01:08:49,645 --> 01:08:52,925
Speaker 1:  Like great. And that's really interesting to see how different people are

1114
01:08:53,085 --> 01:08:55,805
Speaker 1:  approaching it. But this is the one that hits the wall the fastest.

1115
01:08:56,885 --> 01:08:59,605
Speaker 5:  Interesting. Yeah. I mean I think I agree, but I also think there is a, a

1116
01:08:59,955 --> 01:09:02,085
Speaker 5:  more charitable way to look at that, that says

1117
01:09:03,865 --> 01:09:07,325
Speaker 5:  the human is still kind of in charge here, right? Like, I think it's, it's

1118
01:09:07,345 --> 01:09:08,725
Speaker 5:  one thing to

1119
01:09:10,795 --> 01:09:13,365
Speaker 5:  have these things do things on your behalf, right? And this is where I think

1120
01:09:13,365 --> 01:09:17,165
Speaker 5:  things like operator get really messy. Like, would you let

1121
01:09:17,835 --> 01:09:21,765
Speaker 5:  chat GPT text your wife for you? Like e

1122
01:09:21,765 --> 01:09:23,245
Speaker 5:  even something relatively inno obvious again,

1123
01:09:23,245 --> 01:09:25,805
Speaker 1:  David, I don't, I i I don't think I've made this clear enough. I am married

1124
01:09:25,805 --> 01:09:26,845
Speaker 1:  to a divorce lawyer.

1125
01:09:27,265 --> 01:09:29,125
Speaker 5:  No, but even something very simple where you're like,

1126
01:09:29,395 --> 01:09:32,805
Speaker 1:  That woman can have chat g bt divorce me. It's just a macro on her computer.

1127
01:09:33,435 --> 01:09:37,085
Speaker 1:  It's a work issued windowed laptop with a button that says D and then my

1128
01:09:37,085 --> 01:09:38,165
Speaker 1:  life is over. Fair.

1129
01:09:38,355 --> 01:09:41,005
Speaker 5:  Yeah. But like, okay, fine.

1130
01:09:42,335 --> 01:09:45,605
Speaker 5:  Helen Hala, our boss, our boss. Would you, would you like

1131
01:09:46,095 --> 01:09:49,125
Speaker 5:  voice mode chat GBT and be like, Hey, will you text Helen and tell her I'm

1132
01:09:49,125 --> 01:09:51,485
Speaker 5:  running late, but I'll be like 10 minutes late to the meeting. Why wouldn't

1133
01:09:51,485 --> 01:09:52,005
Speaker 1:  I just text

1134
01:09:52,005 --> 01:09:54,645
Speaker 5:  Her that? No, no. But, but just go thought, experiment with me here for a

1135
01:09:54,645 --> 01:09:58,525
Speaker 5:  second. Okay. Would you let it take that act like do

1136
01:09:58,525 --> 01:10:01,565
Speaker 5:  that on your behalf. Not say here's a message you might send, but be like,

1137
01:10:01,565 --> 01:10:03,925
Speaker 5:  send this message for me, chat. GBT, would you have it do that?

1138
01:10:05,255 --> 01:10:06,585
Speaker 1:  Yeah. Abstractly.

1139
01:10:07,205 --> 01:10:10,945
Speaker 5:  Yes. Really. See, I wouldn't, I like just the, there, there is a certain

1140
01:10:11,235 --> 01:10:14,665
Speaker 5:  thing where I'm like, okay, If you wanna write the text and then I'll look

1141
01:10:14,665 --> 01:10:17,285
Speaker 5:  at it and send it. That's something. Yeah, I guess

1142
01:10:17,285 --> 01:10:19,885
Speaker 1:  That's what I'm imagining. 'cause that's how I use Siri in the car.

1143
01:10:20,185 --> 01:10:22,845
Speaker 5:  No, I even get freaked out in the car because you say it and it reads it

1144
01:10:22,845 --> 01:10:26,245
Speaker 5:  back to you and I'm like, what if it like put weird punctuation and a period

1145
01:10:26,305 --> 01:10:27,765
Speaker 5:  at the end and wife is gonna think

1146
01:10:27,765 --> 01:10:30,765
Speaker 1:  I'm, there's middle emojis between every word and you just don't know. Yeah.

1147
01:10:30,765 --> 01:10:31,125
Speaker 1:  Okay. I'm

1148
01:10:31,125 --> 01:10:34,245
Speaker 5:  With you. And I still like, I I truly get nervous about doing a text that

1149
01:10:34,285 --> 01:10:37,805
Speaker 5:  I can't look at the words before I send them. Yeah. And I think that's where

1150
01:10:38,185 --> 01:10:41,525
Speaker 5:  all of this age agentic stuff gets really messy for me because I'm so, I'm

1151
01:10:41,525 --> 01:10:44,125
Speaker 5:  expecting you to do things. But again, just to finish this thought, in this

1152
01:10:44,445 --> 01:10:48,325
Speaker 5:  case, the idea that like I'm asking chat GBT to

1153
01:10:48,325 --> 01:10:51,565
Speaker 5:  give me ideas about how to support a friend, I think is

1154
01:10:52,275 --> 01:10:55,805
Speaker 5:  less terrifying than the idea of like, send my friend a supportive text message,

1155
01:10:55,895 --> 01:10:56,245
Speaker 5:  which

1156
01:10:56,245 --> 01:10:56,405
Speaker 1:  Would be

1157
01:10:56,575 --> 01:10:57,725
Speaker 5:  Horrifying. I

1158
01:10:57,725 --> 01:11:01,405
Speaker 1:  Scrolled down and I'm reading the rest of the same email Sure. About the

1159
01:11:01,405 --> 01:11:01,565
Speaker 1:  mom

1160
01:11:03,775 --> 01:11:07,075
Speaker 1:  and the second part of the email is the most important part of the email.

1161
01:11:07,185 --> 01:11:11,035
Speaker 1:  Okay, here's a quote. Most people I'm friends with or that listen

1162
01:11:11,035 --> 01:11:14,845
Speaker 1:  to the show are probably concerned with accuracy, environmental impact, development

1163
01:11:14,845 --> 01:11:18,605
Speaker 1:  of critical thinking, et cetera. She, the mom could frankly

1164
01:11:18,865 --> 01:11:20,325
Speaker 1:  not give less of a shit.

1165
01:11:22,245 --> 01:11:26,025
Speaker 1:  And I suspect people are like her are likely to be in the majority, not because

1166
01:11:26,035 --> 01:11:29,105
Speaker 1:  she's dumb, she's brilliant in other ways because she's just not wired that

1167
01:11:29,105 --> 01:11:33,065
Speaker 1:  way. And this is your point about good enough. Yeah. Right. It's just

1168
01:11:33,065 --> 01:11:36,105
Speaker 1:  the computer's talking to you and it sounds fine

1169
01:11:36,885 --> 01:11:39,105
Speaker 1:  and that's good enough. And I,

1170
01:11:41,025 --> 01:11:44,645
Speaker 1:  I'm just very convinced that we live in an age where just being confident

1171
01:11:45,465 --> 01:11:49,325
Speaker 1:  is like all you need, like just bluing through

1172
01:11:49,325 --> 01:11:53,165
Speaker 1:  it is very effective in a variety of domains. And maybe

1173
01:11:53,195 --> 01:11:56,845
Speaker 1:  that collapses on itself. We'll find out. But

1174
01:11:57,385 --> 01:12:01,285
Speaker 1:  that's like the whole population is primed to just be like confident

1175
01:12:01,285 --> 01:12:02,885
Speaker 1:  talking. That must be right. Totally.

1176
01:12:03,375 --> 01:12:06,725
Speaker 5:  Wired ran a headline the other day, something to the effect of the less people

1177
01:12:06,995 --> 01:12:08,725
Speaker 5:  know about AI the more they like it.

1178
01:12:10,265 --> 01:12:13,485
Speaker 5:  And I'm there based on a study that I think it was like a marketing firm

1179
01:12:13,625 --> 01:12:17,565
Speaker 5:  did, basically defined that the, If you don't understand

1180
01:12:17,565 --> 01:12:20,725
Speaker 5:  what's happening, AI seems great. Yeah. And that makes perfect sense to me.

1181
01:12:20,725 --> 01:12:23,645
Speaker 5:  That feels exactly right. And I think most people don't understand what's

1182
01:12:23,645 --> 01:12:26,725
Speaker 5:  happening. And maybe that's terrifying. But I think that's what's coming

1183
01:12:27,095 --> 01:12:27,445
Speaker 5:  based

1184
01:12:27,445 --> 01:12:31,205
Speaker 1:  On this. And that second piece, which is super interesting to me.

1185
01:12:31,765 --> 01:12:35,245
Speaker 1:  I understand why a bunch of companies are so excited about this because

1186
01:12:36,505 --> 01:12:39,885
Speaker 1:  it is the first time using a computer has felt different for people,

1187
01:12:40,725 --> 01:12:44,715
Speaker 1:  right? The, the jump from the command line to the graphical user

1188
01:12:44,715 --> 01:12:48,675
Speaker 1:  interface felt different. And you built an entire paradigm of computing

1189
01:12:48,675 --> 01:12:52,395
Speaker 1:  around it. And then the jump from desktop computers to touchscreen

1190
01:12:52,395 --> 01:12:55,555
Speaker 1:  mobile phones was an entire paradigm shift. And obviously phones could go

1191
01:12:55,555 --> 01:12:58,595
Speaker 1:  with you and all, all the other stuff, but that created an entire economy

1192
01:12:59,375 --> 01:13:02,315
Speaker 1:  and they've been searching for something else that makes computers feel different

1193
01:13:02,315 --> 01:13:05,755
Speaker 1:  to people. Are they gonna fold? You know, like

1194
01:13:06,135 --> 01:13:09,915
Speaker 1:  what's something, it's gotta be something. And this feels different to people

1195
01:13:10,035 --> 01:13:13,675
Speaker 1:  'cause they're just talking to them, but there's not an application layer

1196
01:13:13,675 --> 01:13:17,395
Speaker 1:  behind that that builds the whole economy or supports all this. Like, and

1197
01:13:17,395 --> 01:13:21,075
Speaker 1:  also I just come back to this here example. Like, eventually mom's gonna

1198
01:13:21,075 --> 01:13:23,915
Speaker 1:  hit a wall, right? It's gonna lie to her. She's gonna make a mistake, it

1199
01:13:23,915 --> 01:13:27,425
Speaker 1:  will embarrass her in some way, and then, and then we'll stop

1200
01:13:27,675 --> 01:13:30,605
Speaker 1:  because she can't trust it. And that trust once it's gone is gone.

1201
01:13:30,955 --> 01:13:34,885
Speaker 5:  Yeah. I, I definitely agree with that. And I think the question of a,

1202
01:13:34,885 --> 01:13:38,325
Speaker 5:  when that happens and B, what the stakes are is the stuff that makes me really

1203
01:13:38,325 --> 01:13:39,125
Speaker 5:  nervous about all of this.

1204
01:13:39,675 --> 01:13:39,965
Speaker 1:  Yeah.

1205
01:13:40,445 --> 01:13:44,265
Speaker 5:  And either I'm just a chicken or everyone else should be more afraid

1206
01:13:44,405 --> 01:13:47,305
Speaker 5:  or both. I don't know. All right, let's do one more and then, then we'll,

1207
01:13:47,305 --> 01:13:49,305
Speaker 5:  we'll get outta here. Yeah. Here's one more.

1208
01:13:49,525 --> 01:13:52,985
Speaker 10:  Hey, The Verge and or the Slack channel. My name's Florian. I'm calling

1209
01:13:52,985 --> 01:13:56,745
Speaker 10:  from New Jersey. Taking up your request on what I use chat GBT for.

1210
01:13:57,225 --> 01:14:00,945
Speaker 10:  I would say primarily I use it as a, a

1211
01:14:00,945 --> 01:14:04,425
Speaker 10:  more reliable resource than Google searches for informed decisions.

1212
01:14:04,765 --> 01:14:08,745
Speaker 10:  So whether that's in depth settings, menus, I can find Bluetooth.

1213
01:14:09,125 --> 01:14:13,045
Speaker 10:  But you know the other ones, I'm a recent college grad, so I've

1214
01:14:13,045 --> 01:14:16,925
Speaker 10:  used a lot to help friends with their career growth or updating resumes.

1215
01:14:17,075 --> 01:14:19,845
Speaker 10:  Kind of like the bad code, good code argument.

1216
01:14:21,285 --> 01:14:24,885
Speaker 10:  I come back for practical advice, namely curious

1217
01:14:25,025 --> 01:14:28,645
Speaker 10:  topics or other things like that to gain some

1218
01:14:28,645 --> 01:14:32,005
Speaker 10:  information on very quickly. But I would say primarily

1219
01:14:32,435 --> 01:14:36,285
Speaker 10:  it's for things like when I can't find the answer on AV forums for how to

1220
01:14:36,285 --> 01:14:40,165
Speaker 10:  set up the Blu ray player through a RC with the

1221
01:14:40,205 --> 01:14:44,045
Speaker 10:  soundbar and making sure that Dolby true HD plays from

1222
01:14:44,045 --> 01:14:47,975
Speaker 10:  my disc. Neil, I for you, but I love you.

1223
01:14:47,975 --> 01:14:51,615
Speaker 10:  Thanks for the great pod and hopefully someone in the slack see this, have

1224
01:14:51,615 --> 01:14:51,975
Speaker 10:  a great day.

1225
01:14:52,695 --> 01:14:56,455
Speaker 1:  I love you. I love this man. First of all, just call me, you know, you how

1226
01:14:56,455 --> 01:14:57,935
Speaker 1:  talk to the robot I'm around.

1227
01:14:58,205 --> 01:15:01,295
Speaker 5:  Yeah. This is why the verb cast exists. We will solve these problems for

1228
01:15:01,295 --> 01:15:01,375
Speaker 5:  you.

1229
01:15:02,235 --> 01:15:05,735
Speaker 1:  That's great. The, the in-depth technical problem. The issue that I've always

1230
01:15:05,735 --> 01:15:09,655
Speaker 1:  run into with that is the answer is buried in some forum posts from six

1231
01:15:09,655 --> 01:15:13,465
Speaker 1:  years ago and it's just been muddled. And I

1232
01:15:13,585 --> 01:15:16,105
Speaker 1:  actually wanna find the old forum post where the person actually has the

1233
01:15:16,105 --> 01:15:19,795
Speaker 1:  answer, not the, do you know what I mean? Yeah. Like not the, not the

1234
01:15:19,795 --> 01:15:21,275
Speaker 1:  synthesis over time. This is

1235
01:15:21,275 --> 01:15:25,035
Speaker 5:  Where I think like Reddit answers the, the Reddit AI search is, is

1236
01:15:25,275 --> 01:15:28,315
Speaker 5:  actually super exciting because it is geared toward that exact thing. Yeah.

1237
01:15:28,425 --> 01:15:32,275
Speaker 5:  That is like, what if we just did a smarter job of searching this

1238
01:15:32,275 --> 01:15:36,035
Speaker 5:  stuff, but we're still interested in finding actual people's actual

1239
01:15:36,035 --> 01:15:39,475
Speaker 5:  answers. It's like that's a, that's a cool mix if we can figure out how to

1240
01:15:39,475 --> 01:15:41,715
Speaker 5:  do it right. I lied. I have one more to play for you and then we should go

1241
01:15:41,715 --> 01:15:42,955
Speaker 5:  to break. But let's just wait.

1242
01:15:43,435 --> 01:15:46,675
Speaker 1:  I just wanna say to, to my guy, you're my best friend.

1243
01:15:47,855 --> 01:15:50,555
Speaker 1:  We don't know each other, but we we're in it now together.

1244
01:15:50,735 --> 01:15:53,595
Speaker 5:  You are one of us. Now we're family. Welcome to The Verge. All right, I got

1245
01:15:53,595 --> 01:15:53,915
Speaker 5:  one more

1246
01:15:54,535 --> 01:15:55,505
Speaker 1:  True HD for life.

1247
01:15:56,255 --> 01:15:59,425
Speaker 9:  This is Matt from Springfield, Missouri. I was just answering your question

1248
01:15:59,425 --> 01:16:03,105
Speaker 9:  about using AI daily. All these tools came out when I started

1249
01:16:03,105 --> 01:16:06,465
Speaker 9:  dating and I used it to help write my profile and

1250
01:16:06,815 --> 01:16:10,465
Speaker 9:  also compose my messages. 'cause I'm a machinist, I'm

1251
01:16:10,735 --> 01:16:14,625
Speaker 9:  have good ideas, but I'm not good at words. And AI helped me fill that gap.

1252
01:16:14,765 --> 01:16:18,385
Speaker 9:  It must have worked because now I'm in a relationship. I'm gonna have a

1253
01:16:18,565 --> 01:16:22,425
Speaker 9:  son this year and I still use AI today to compose

1254
01:16:22,435 --> 01:16:25,425
Speaker 9:  crack thoughtful messages to send to her throughout the week.

1255
01:16:26,125 --> 01:16:26,785
Speaker 1:  Oh, this is great.

1256
01:16:27,045 --> 01:16:28,105
Speaker 5:  AI is good. Neli,

1257
01:16:28,505 --> 01:16:31,105
Speaker 1:  I gotta say, I mean that's, we just got it there. We should shut down The

1258
01:16:31,185 --> 01:16:34,625
Speaker 1:  Verge Negative. Nancy is good outta here. That's fast. Yeah, that one made

1259
01:16:34,805 --> 01:16:35,025
Speaker 5:  Me

1260
01:16:35,025 --> 01:16:38,345
Speaker 1:  Really happy. Cha We only hit two years ago. We went from zero to sun.

1261
01:16:39,615 --> 01:16:42,665
Speaker 1:  Killing it. Pretty good. Yeah, pretty good. Pretty good.

1262
01:16:44,135 --> 01:16:46,515
Speaker 1:  You know, most people move in together for, I'm just saying anyway.

1263
01:16:47,215 --> 01:16:49,595
Speaker 5:  Not anymore. This is an AI world we live in. No time

1264
01:16:49,595 --> 01:16:53,395
Speaker 1:  For that. Okay. I'll give you, I'll give mine. And we, and it's very different

1265
01:16:53,395 --> 01:16:57,155
Speaker 1:  than that, but it's in the same zone. I recently sold something on

1266
01:16:57,225 --> 01:17:01,195
Speaker 1:  eBay and I let eBay's AI just write the description for me. Really just gave

1267
01:17:01,195 --> 01:17:04,675
Speaker 1:  it, it was so dumb. It was like we, we bought a new stove

1268
01:17:05,895 --> 01:17:08,955
Speaker 1:  and we bought different color handles for the stove. So I sold the old ones

1269
01:17:09,345 --> 01:17:11,765
Speaker 1:  and I, I just took a picture and I figured out what it was and I'm like,

1270
01:17:11,765 --> 01:17:13,805
Speaker 1:  do you wanna write a description? And it's like, this is the lowest stake

1271
01:17:13,805 --> 01:17:17,405
Speaker 1:  shit in the world. Like anybody who's searching for replacement stove handles

1272
01:17:17,945 --> 01:17:21,565
Speaker 1:  and sees a picture of this like they know. Yeah. Like it, you don't have

1273
01:17:21,565 --> 01:17:23,965
Speaker 1:  to describe them very much. And I was like, fine, here's some copy. And it

1274
01:17:23,965 --> 01:17:27,835
Speaker 1:  did it. Poshmark today actually announced that they're letting

1275
01:17:27,835 --> 01:17:30,955
Speaker 1:  people do AI descriptions. I think there's this world in which like weird

1276
01:17:31,135 --> 01:17:34,915
Speaker 1:  low stakes fields get filled in. Yeah. Obviously dating is a higher

1277
01:17:34,915 --> 01:17:38,795
Speaker 1:  stakes field, but I can, I can see why it's very tempting to

1278
01:17:38,795 --> 01:17:41,275
Speaker 1:  people. Yeah, it's good stuff. And by the way, that's a great story. Yeah.

1279
01:17:41,275 --> 01:17:42,795
Speaker 5:  Matt, congrats. We're very happy for

1280
01:17:42,795 --> 01:17:45,755
Speaker 1:  It. I hope you name your son Al. Think about it.

1281
01:17:49,735 --> 01:17:52,635
Speaker 1:  All right. We should take a break. And we got a lightning round. That was

1282
01:17:52,635 --> 01:17:53,915
Speaker 1:  great. Send us more of those.

1283
01:17:53,985 --> 01:17:57,595
Speaker 5:  Yeah. Thank you to everybody who, who wrote in It was super fun. I learned

1284
01:19:37,635 --> 01:19:41,595
Speaker 1:  All right, we're back. Unsponsored lightning round. That's how you know

1285
01:19:41,595 --> 01:19:42,035
Speaker 1:  it's wild.

1286
01:19:42,945 --> 01:19:43,875
Speaker 5:  It's crazy. Times out

1287
01:19:43,875 --> 01:19:47,535
Speaker 1:  There Unfiltered, uncensored. The First

1288
01:19:47,535 --> 01:19:51,295
Speaker 1:  Amendment's back people. And by back we mean Trump

1289
01:19:51,295 --> 01:19:52,215
Speaker 1:  will arrest you If you

1290
01:19:52,215 --> 01:19:52,775
Speaker 13:  Criticize it.

1291
01:19:54,035 --> 01:19:56,095
Speaker 1:  All right. There's actually a lot this lightning round. Let's start with

1292
01:19:56,095 --> 01:19:59,775
Speaker 1:  some very exciting, very niche Vergecast News.

1293
01:20:01,265 --> 01:20:04,765
Speaker 1:  Google is open sourcing the Pebble Source code, this original

1294
01:20:04,765 --> 01:20:08,705
Speaker 1:  smartwatch. And Eric Midkowski, the founder of Pebble, is gonna make

1295
01:20:08,705 --> 01:20:10,825
Speaker 1:  another watch. David, you like talk to him. What's going on here?

1296
01:20:10,855 --> 01:20:14,745
Speaker 5:  Yeah, so I saw Eric at CES, we had lunch and he

1297
01:20:14,745 --> 01:20:18,585
Speaker 5:  had a pebble on his wrist and I said, we were just talking

1298
01:20:18,595 --> 01:20:22,385
Speaker 5:  about, you know, life and the future and gadgets. And I was like, I was like,

1299
01:20:22,405 --> 01:20:25,665
Speaker 5:  so you, you've left automatic, he sold his company beeper the messaging app

1300
01:20:25,785 --> 01:20:28,905
Speaker 5:  that we talked about a bunch to automatic last year. He left the company

1301
01:20:28,975 --> 01:20:31,385
Speaker 5:  looking for something to do. I was like, so are you gonna, are you gonna

1302
01:20:31,385 --> 01:20:35,305
Speaker 5:  just like do Pebble again? Are you gonna re Pebble? And little

1303
01:20:35,365 --> 01:20:38,425
Speaker 5:  did I know, he asked me again. We talked this past week and he was like,

1304
01:20:38,425 --> 01:20:41,205
Speaker 5:  did did you know? And I was like, what are you talking about? And he was

1305
01:20:41,205 --> 01:20:44,965
Speaker 5:  like, we're calling it re Pebble. How? Like what? Why did you say So I, I'm

1306
01:20:44,965 --> 01:20:47,805
Speaker 5:  feeling very proud of myself except that I completely let him off the hook

1307
01:20:47,985 --> 01:20:51,285
Speaker 5:  and just moved on to something else. But anyway, so Eric,

1308
01:20:52,725 --> 01:20:56,445
Speaker 5:  I guess over the last year has been working with Google to open

1309
01:20:56,445 --> 01:20:59,885
Speaker 5:  source all of Pebble Os. So Pebble sold to Fitbit, which sold to Google and

1310
01:21:00,005 --> 01:21:03,645
Speaker 5:  a lot of the like Pebble ideas sort of percolated around those two

1311
01:21:03,885 --> 01:21:07,525
Speaker 5:  companies. And you can see some DNA in like the Pixel watch, but the

1312
01:21:07,525 --> 01:21:11,175
Speaker 5:  Pebble software itself was just not being used. And so

1313
01:21:11,855 --> 01:21:15,635
Speaker 5:  Google to its credit, I cannot report that

1314
01:21:15,635 --> 01:21:18,635
Speaker 5:  Dieter Bone was involved. But I also believe in my heart that Dieter Bone

1315
01:21:18,635 --> 01:21:19,155
Speaker 5:  was involved.

1316
01:21:21,295 --> 01:21:25,155
Speaker 5:  Dieter famous Pebble fan was just works

1317
01:21:25,155 --> 01:21:27,755
Speaker 5:  for Google. Now you know there, there's just sometimes there are pieces there.

1318
01:21:27,775 --> 01:21:28,155
Speaker 5:  But anyway,

1319
01:21:28,185 --> 01:21:30,235
Speaker 1:  Look, we don't do conspiracy theories here except for that

1320
01:21:30,235 --> 01:21:33,955
Speaker 5:  One. Except for that one. And Google

1321
01:21:33,955 --> 01:21:36,915
Speaker 5:  agreed to do it. So Google open sourced all of Pebble s which is

1322
01:21:37,735 --> 01:21:41,715
Speaker 5:  not everything. There are like third party drivers and stuff that

1323
01:21:41,715 --> 01:21:44,995
Speaker 5:  are occasionally complicated to open source. The name Pebble still belongs

1324
01:21:44,995 --> 01:21:48,835
Speaker 5:  to Google. But functionally all of the software is now available. And

1325
01:21:48,835 --> 01:21:52,595
Speaker 5:  so Eric is starting a company that will not be called Pebble, but the goal

1326
01:21:52,615 --> 01:21:56,275
Speaker 5:  is just to make Pebbles again. He's like, we got it right. His pebbles now

1327
01:21:56,275 --> 01:21:59,395
Speaker 5:  still work. He said he had to replace the battery on all of them, but they

1328
01:21:59,395 --> 01:22:01,595
Speaker 5:  still work and they still do the things he wanted. And so he's like, I'm

1329
01:22:01,595 --> 01:22:04,995
Speaker 5:  gonna keep making these like long lasting e ink

1330
01:22:05,425 --> 01:22:08,995
Speaker 5:  gadgety gadgets to put on people's wrists. And the plan is

1331
01:22:09,025 --> 01:22:12,595
Speaker 5:  hopefully to start making stuff quickly. Like even this year.

1332
01:22:13,655 --> 01:22:17,435
Speaker 5:  And there are a lot of pebble nerds out there who are very excited about

1333
01:22:17,435 --> 01:22:17,755
Speaker 5:  this. Yeah.

1334
01:22:18,095 --> 01:22:22,075
Speaker 1:  So the Pebble story as I recall it is that they just

1335
01:22:22,095 --> 01:22:25,835
Speaker 1:  ran into API problems with the iPhone. 'cause the

1336
01:22:26,075 --> 01:22:29,875
Speaker 1:  original Pebble could get notifications, right? Like Apple supported sending

1337
01:22:29,875 --> 01:22:33,595
Speaker 1:  notifications out that way. But you couldn't reply to a message like it.

1338
01:22:33,595 --> 01:22:36,835
Speaker 1:  They were just limited like in very real ways. You could just like look at

1339
01:22:36,835 --> 01:22:40,035
Speaker 1:  a text message but you gotta get your phone. Apple's never gonna open that

1340
01:22:40,035 --> 01:22:43,905
Speaker 1:  up. No. So probably Android phones, the Pebble app on the

1341
01:22:43,905 --> 01:22:45,585
Speaker 1:  Android phones, they could do some of that stuff more. It

1342
01:22:45,585 --> 01:22:48,185
Speaker 5:  Was, it always much better. Yeah, there's more to do on Android, but also

1343
01:22:48,545 --> 01:22:52,385
Speaker 5:  I think for Eric in particular, I think the idea of

1344
01:22:52,385 --> 01:22:56,305
Speaker 5:  it being like a messaging machine is, seems to be less compelling. He's

1345
01:22:56,305 --> 01:23:00,185
Speaker 5:  like, he's very into it as like it's an easy way to get kind of

1346
01:23:00,185 --> 01:23:03,905
Speaker 5:  glanceable information, but it's also like simple. And the battery lasts

1347
01:23:03,905 --> 01:23:06,420
Speaker 5:  a long time and it has buttons for music controls, like buttons for music

1348
01:23:06,420 --> 01:23:09,565
Speaker 5:  controls is one of the specific things he named as like a core

1349
01:23:10,095 --> 01:23:14,005
Speaker 5:  competency of Pebble. And it is like, love it. It does not have to be a messaging

1350
01:23:14,025 --> 01:23:17,765
Speaker 5:  system, which I actually kind of agree with. Like I, I, I

1351
01:23:17,765 --> 01:23:21,645
Speaker 5:  would like lots of features to exist but I find that I only

1352
01:23:21,705 --> 01:23:25,485
Speaker 5:  use a very small number of them on my wrist and I would trade a lot of them

1353
01:23:25,745 --> 01:23:29,405
Speaker 5:  for like five day battery life. And yeah. And I think it, it's gonna be interesting

1354
01:23:29,405 --> 01:23:30,365
Speaker 5:  to see what they do this time.

1355
01:23:30,825 --> 01:23:34,085
Speaker 1:  I'm excited. I love a good open source hardware hacky project.

1356
01:23:34,515 --> 01:23:36,965
Speaker 5:  It's gonna be cool. 'cause there there's like, there's been a pretty big

1357
01:23:36,965 --> 01:23:40,525
Speaker 5:  community of people keeping this stuff alive for a long time. And

1358
01:23:41,185 --> 01:23:45,125
Speaker 5:  the fact that now they have honest to God software to play with instead

1359
01:23:45,125 --> 01:23:49,085
Speaker 5:  of like hacking around the edges of existing pebbles. I think we're

1360
01:23:49,085 --> 01:23:51,845
Speaker 5:  gonna see some pretty interesting stuff here like that that community is,

1361
01:23:51,845 --> 01:23:54,485
Speaker 5:  is really engaged and I think is gonna get up to some stuff.

1362
01:23:54,955 --> 01:23:58,785
Speaker 1:  Okay, so that's Pebble next one. Oracle

1363
01:23:58,845 --> 01:24:01,465
Speaker 1:  and Microsoft reportedly and talks to take over TikTok.

1364
01:24:01,655 --> 01:24:02,865
Speaker 5:  This story makes me so tired.

1365
01:24:03,465 --> 01:24:06,865
Speaker 1:  Although Trump denied Oracle right in some press con, but he doesn't know.

1366
01:24:06,985 --> 01:24:10,825
Speaker 1:  I mean like when I say he doesn't know, I don't, I just mean be dismissive.

1367
01:24:11,185 --> 01:24:14,785
Speaker 1:  I mean literally he gave them a 75 day illegal

1368
01:24:14,855 --> 01:24:18,345
Speaker 1:  extension, which Apple and Google do not trust enough to put TikTok back

1369
01:24:18,345 --> 01:24:22,305
Speaker 1:  in the store. And then at a press conference he said Congress

1370
01:24:22,305 --> 01:24:25,505
Speaker 1:  has given me 90 days, which is 15 more days than he gave himself.

1371
01:24:26,015 --> 01:24:26,305
Speaker 5:  Yeah.

1372
01:24:27,225 --> 01:24:28,265
Speaker 1:  Congress has not given him 90 days.

1373
01:24:28,465 --> 01:24:29,465
Speaker 5:  Congress didn't give anybody anything.

1374
01:24:29,685 --> 01:24:33,465
Speaker 1:  The time's up like what he gave TikTok was a 75

1375
01:24:33,605 --> 01:24:37,505
Speaker 1:  day pause on enforcement of breaking the law. Right. Which

1376
01:24:37,505 --> 01:24:41,185
Speaker 1:  again, apple and Google did not trust. 'cause that's billions of dollars.

1377
01:24:41,275 --> 01:24:41,625
Speaker 5:  Right.

1378
01:24:42,125 --> 01:24:45,345
Speaker 1:  Anyway, so I'm just saying like the facts are confusing on the ground. Like

1379
01:24:45,345 --> 01:24:48,145
Speaker 1:  what does Trump even believe is going on here? Because he's just saying different

1380
01:24:48,145 --> 01:24:50,825
Speaker 1:  numbers and then, but he did say I don't think Oracle was involved, which

1381
01:24:50,825 --> 01:24:53,665
Speaker 1:  is weird. But Microsoft was a bid the first time

1382
01:24:54,535 --> 01:24:58,105
Speaker 1:  Nadella famously said it was like the weirdest deal he is ever been a part

1383
01:24:58,105 --> 01:24:58,225
Speaker 1:  of.

1384
01:24:58,615 --> 01:25:00,785
Speaker 5:  Yeah. A bidder might give

1385
01:25:02,345 --> 01:25:05,425
Speaker 5:  Microsoft too much agency in that. It seems a lot like Microsoft was like

1386
01:25:05,425 --> 01:25:08,945
Speaker 5:  dragged into a room and yelled at about buying TikTok for a while.

1387
01:25:10,285 --> 01:25:13,825
Speaker 1:  No one knows what's gonna happen. Yeah. Like truly no one knows what's gonna

1388
01:25:13,825 --> 01:25:17,465
Speaker 1:  happen. And then this other plan that Trump keeps

1389
01:25:17,785 --> 01:25:21,265
Speaker 1:  floating where someone does a joint venture and they give 50% of it

1390
01:25:21,565 --> 01:25:22,425
Speaker 1:  to the United States.

1391
01:25:24,445 --> 01:25:28,185
Speaker 1:  Our own big tech companies are extremely nervous

1392
01:25:28,275 --> 01:25:29,145
Speaker 1:  about that plan.

1393
01:25:29,245 --> 01:25:29,825
Speaker 5:  Oh, interesting.

1394
01:25:31,865 --> 01:25:35,705
Speaker 1:  'cause If you just think about it, that means India could shut down

1395
01:25:35,775 --> 01:25:39,625
Speaker 1:  YouTube and demand it get sold and 50%

1396
01:25:39,625 --> 01:25:43,465
Speaker 1:  of YouTube be given to Indi. Like that's a precedent you do not want

1397
01:25:43,465 --> 01:25:43,985
Speaker 1:  to play with.

1398
01:25:44,925 --> 01:25:46,035
Speaker 5:  Right. Interesting. Yeah.

1399
01:25:46,265 --> 01:25:49,715
Speaker 1:  Because all their governments are way less skittish about

1400
01:25:50,195 --> 01:25:53,995
Speaker 1:  nationalizing companies or doing speech regulations and

1401
01:25:54,195 --> 01:25:56,035
Speaker 1:  everyone's like, don't play this game. Right.

1402
01:25:56,355 --> 01:25:59,155
Speaker 5:  We'll see if any is actually a particularly interesting one in that it is

1403
01:25:59,275 --> 01:26:02,795
Speaker 5:  a government that I think would be excited to do that kind of thing. And

1404
01:26:02,855 --> 01:26:06,765
Speaker 5:  is such a crucial market as a growth engine for

1405
01:26:06,765 --> 01:26:07,845
Speaker 5:  so many of these companies. Oh

1406
01:26:07,845 --> 01:26:10,885
Speaker 1:  Yeah, I picked it. India has banned TikTok. Yeah. Like straight up and it

1407
01:26:11,185 --> 01:26:14,645
Speaker 1:  is very reliant on YouTube, but it has very draconian speech policies. The

1408
01:26:14,645 --> 01:26:18,045
Speaker 1:  Modi government does not enjoy descent in very specific ways.

1409
01:26:18,905 --> 01:26:22,005
Speaker 1:  You, you could Brazil and other country like that. Russia exists. Yeah. You

1410
01:26:22,005 --> 01:26:24,925
Speaker 1:  know like there's a million countries that would play these games.

1411
01:26:25,985 --> 01:26:29,125
Speaker 1:  And so like our own big tech companies are like, do not, you do not wanna

1412
01:26:29,125 --> 01:26:32,645
Speaker 1:  open this door. Mm. Like you're already in 50 different trade wars. Like

1413
01:26:32,645 --> 01:26:33,085
Speaker 1:  don't do this.

1414
01:26:34,345 --> 01:26:37,605
Speaker 5:  Is it possible that the door is opened now that just someone has set it out

1415
01:26:37,605 --> 01:26:41,445
Speaker 5:  loud? Like do you wonder if there are a bunch of people sitting around

1416
01:26:41,465 --> 01:26:44,325
Speaker 5:  in like government boardrooms being like, how did we never think of this?

1417
01:26:45,275 --> 01:26:48,605
Speaker 1:  Like they do, but like If you wanna roll up on the United States that way

1418
01:26:49,625 --> 01:26:51,645
Speaker 1:  the United States would be like No. Fair.

1419
01:26:51,675 --> 01:26:51,965
Speaker 5:  Yeah.

1420
01:26:52,345 --> 01:26:54,325
Speaker 1:  But then the United States starts doing it and they're like, okay, tit for

1421
01:26:54,325 --> 01:26:58,005
Speaker 1:  ta. Like here we go. Like we're off the races. Like we, the norm has been

1422
01:26:58,245 --> 01:27:01,885
Speaker 1:  shattered and like we're, because we're doing trade wars, you, you don't

1423
01:27:01,885 --> 01:27:04,765
Speaker 1:  already see it. You can see people be like, we dunno what's gonna happen.

1424
01:27:05,025 --> 01:27:07,805
Speaker 1:  But don't don't do that one. We'll see if any of that goes through. We'll

1425
01:27:07,805 --> 01:27:11,525
Speaker 1:  see if there's a buyer. We'll see what happens at 75 days. Like 75 days is

1426
01:27:11,525 --> 01:27:13,805
Speaker 1:  not a long time to close a deal of this magnitude. No.

1427
01:27:13,955 --> 01:27:17,565
Speaker 5:  Even If you had agreed on it already, it would be a short amount of time.

1428
01:27:17,775 --> 01:27:21,165
Speaker 1:  Right. And as far, so far as, you know, there's no lead buyer. And then there's

1429
01:27:21,165 --> 01:27:24,125
Speaker 1:  the big question of are you selling TikTok US

1430
01:27:25,075 --> 01:27:28,935
Speaker 1:  or are you selling by dance? And there's a lot of chatter that

1431
01:27:28,935 --> 01:27:30,615
Speaker 1:  the answer is they're gonna sell by dance.

1432
01:27:31,255 --> 01:27:31,335
Speaker 5:  Hmm.

1433
01:27:32,035 --> 01:27:35,735
Speaker 1:  And so that's a much bigger deal, right. With a much

1434
01:27:35,935 --> 01:27:39,415
Speaker 1:  different set of investors. One of whom is Jeff Ya Trump

1435
01:27:39,625 --> 01:27:43,575
Speaker 1:  donor, you know, like a lot of his money is set up in by dance not

1436
01:27:43,575 --> 01:27:47,455
Speaker 1:  TikTok us. And then there's the question of like, what

1437
01:27:47,455 --> 01:27:50,735
Speaker 1:  does control mean? Because the law specifies it can't be under the control

1438
01:27:50,735 --> 01:27:54,575
Speaker 1:  of a foreign adversary power and then Trump is supposed to run an

1439
01:27:54,575 --> 01:27:58,495
Speaker 1:  inter-agency process to blah blah blah blah blah. Yeah. Okay. Is he

1440
01:27:58,495 --> 01:28:02,455
Speaker 1:  gonna, is he gonna do that? So there's just, I don't know man. I think

1441
01:28:02,455 --> 01:28:05,735
Speaker 1:  we're, I think we're gonna run to the end of that seven, five days and then

1442
01:28:05,735 --> 01:28:08,255
Speaker 1:  we're all just gonna look at each other and it's, who knows

1443
01:28:09,115 --> 01:28:13,095
Speaker 5:  It, it really does seem like at the end of day 74 we're

1444
01:28:13,175 --> 01:28:15,815
Speaker 5:  still gonna look around and everybody's gonna be like, what happens now?

1445
01:28:16,075 --> 01:28:19,615
Speaker 5:  And yeah, no one is going to know like that. It feels like that's where we're

1446
01:28:19,615 --> 01:28:19,735
Speaker 5:  headed

1447
01:28:20,275 --> 01:28:23,535
Speaker 1:  And that is gonna, you know, not, not that Trump isn't running into

1448
01:28:24,075 --> 01:28:27,575
Speaker 1:  the separation of power's problem every single day, but you get to day 74

1449
01:28:27,575 --> 01:28:28,935
Speaker 1:  and he is like, I'm extending it again.

1450
01:28:30,905 --> 01:28:34,815
Speaker 1:  Bunch of Republican senators are gonna say, no, no, no, no, no.

1451
01:28:34,815 --> 01:28:37,535
Speaker 1:  Right. Tom Cotton is gonna be like, no, here's what we're gonna do. We're

1452
01:28:37,535 --> 01:28:41,175
Speaker 1:  gonna shoot TikTok in the street. Right. That's where he's already at. Yeah.

1453
01:28:41,275 --> 01:28:43,895
Speaker 1:  And so I, yeah, we're just barreling towards

1454
01:28:45,275 --> 01:28:48,755
Speaker 1:  a full sale of bite tents, which is nuts to

1455
01:28:48,945 --> 01:28:52,355
Speaker 1:  contemplate with the permission of the Chinese government.

1456
01:28:52,585 --> 01:28:56,395
Speaker 5:  Byan probably either the first, second or third most valuable

1457
01:28:56,465 --> 01:28:58,275
Speaker 5:  startup on earth right now. Yeah.

1458
01:28:58,655 --> 01:29:01,165
Speaker 1:  Way up there by April. Yeah. And by the time you're listening to to this,

1459
01:29:01,185 --> 01:29:03,885
Speaker 1:  it will be very close to February. Like this is just not gonna happen.

1460
01:29:03,955 --> 01:29:04,245
Speaker 5:  Yeah.

1461
01:29:07,395 --> 01:29:08,325
Speaker 1:  It's crazy to me.

1462
01:29:08,635 --> 01:29:12,325
Speaker 5:  Yeah. I I really like, we were, we were on the show for a couple of weeks

1463
01:29:12,325 --> 01:29:14,885
Speaker 5:  trying to like handicap possibilities and I have given up even trying to

1464
01:29:14,885 --> 01:29:18,845
Speaker 5:  guess like I, I couldn't possibly begin to make an

1465
01:29:19,085 --> 01:29:22,005
Speaker 5:  educated guess about what's gonna happen here, which is so interesting and

1466
01:29:22,005 --> 01:29:23,005
Speaker 5:  also so insane.

1467
01:29:23,845 --> 01:29:27,405
Speaker 1:  I do think there's some reasonable likelihood that Microsoft actually goes

1468
01:29:27,405 --> 01:29:30,205
Speaker 1:  through with this and tries to do it. They have the money to do it.

1469
01:29:31,715 --> 01:29:35,525
Speaker 1:  They have the ability to run the infrastructure. They have Azure, they

1470
01:29:35,525 --> 01:29:39,205
Speaker 1:  have consumer market with Xbox users.

1471
01:29:39,275 --> 01:29:42,765
Speaker 1:  Like they have a thesis for why they would run it. They have an

1472
01:29:43,245 --> 01:29:47,205
Speaker 1:  advertising business. It's fine, not great. Like sure. Meta

1473
01:29:47,205 --> 01:29:51,045
Speaker 1:  can't do it for sure. Can't do it for other reasons. We can talk about,

1474
01:29:51,065 --> 01:29:54,085
Speaker 1:  but like, you know, the FT C is still pursuing a breakup in that like they're

1475
01:29:54,085 --> 01:29:54,485
Speaker 1:  not gonna do it.

1476
01:29:54,485 --> 01:29:58,285
Speaker 5:  Would buying by dance be the biggest acquisition in history companies

1477
01:29:58,285 --> 01:30:01,405
Speaker 5:  worth, like upwards of $300 billion is the number that keeps being floated

1478
01:30:01,405 --> 01:30:02,565
Speaker 1:  Around. I know what the number is,

1479
01:30:02,825 --> 01:30:06,125
Speaker 5:  But then I, I've heard numbers for TikTok that are higher than $300 billion.

1480
01:30:06,185 --> 01:30:09,925
Speaker 5:  So like by dance by the way, also kind of in the

1481
01:30:09,925 --> 01:30:13,845
Speaker 5:  DeepSeek race of putting out much cheaper AI models that can be like,

1482
01:30:14,635 --> 01:30:16,325
Speaker 5:  there's a lot happening in that company.

1483
01:30:16,685 --> 01:30:20,395
Speaker 1:  There's a lot happening in that company. Also, just the, the

1484
01:30:20,465 --> 01:30:24,395
Speaker 1:  chatter that I've heard is the number changes based on who buys it. Mm.

1485
01:30:25,005 --> 01:30:28,875
Speaker 1:  Right. So like if Meta or Google

1486
01:30:29,025 --> 01:30:32,995
Speaker 1:  buys it, for example, they have gigantic

1487
01:30:33,235 --> 01:30:37,155
Speaker 1:  monetization engines that are really, really good. So they

1488
01:30:37,155 --> 01:30:40,955
Speaker 1:  don't have to spend any money like getting their money back out.

1489
01:30:41,265 --> 01:30:44,075
Speaker 1:  Sure. Right. They just have to deliver ads inside of TikTok and they already

1490
01:30:44,075 --> 01:30:45,995
Speaker 1:  have the clients and they already have the tech and they already have the

1491
01:30:45,995 --> 01:30:48,395
Speaker 1:  ai like whatever it is that you think they need, they've already, they already

1492
01:30:48,395 --> 01:30:48,635
Speaker 1:  have it.

1493
01:30:48,635 --> 01:30:51,155
Speaker 5:  It's just really expensive scale essentially. Yeah.

1494
01:30:51,155 --> 01:30:55,075
Speaker 1:  They just bought more scale. If you are Amazon even, you gotta

1495
01:30:55,095 --> 01:30:57,595
Speaker 1:  go build an entire monetization system, right. So you're gonna pay a lower

1496
01:30:57,595 --> 01:31:00,835
Speaker 1:  price 'cause you gotta allocate some money to that stuff. So like how much

1497
01:31:00,835 --> 01:31:04,595
Speaker 1:  you're willing to pay, how much it's worth is all based on what you can do

1498
01:31:04,595 --> 01:31:08,435
Speaker 1:  with it. And there's only so many companies like, and the two companies that

1499
01:31:08,435 --> 01:31:11,795
Speaker 1:  could do the most of it, pay the most are not gonna do it because the government's

1500
01:31:11,795 --> 01:31:15,275
Speaker 1:  trying to break 'em up. Yeah. So like literally they're trying to break up

1501
01:31:15,275 --> 01:31:15,515
Speaker 1:  Google.

1502
01:31:15,775 --> 01:31:19,715
Speaker 5:  Google has to sell Chrome but can have TikTok. Yeah. It's like a

1503
01:31:19,715 --> 01:31:20,475
Speaker 5:  very funny outcome.

1504
01:31:21,585 --> 01:31:25,515
Speaker 1:  It's nuts. Yeah. Alright, onto the next one. This is a new segment that you're

1505
01:31:25,515 --> 01:31:28,155
Speaker 1:  gonna have to get used to in a virtual cast for the next four years. I'm

1506
01:31:28,155 --> 01:31:30,355
Speaker 1:  just calling it Brendan Carr is a dummy. Ooh.

1507
01:31:30,665 --> 01:31:31,795
Speaker 5:  It's a podcast within a podcast.

1508
01:31:33,105 --> 01:31:36,795
Speaker 1:  He's a moron. He is. He is a total

1509
01:31:36,825 --> 01:31:40,635
Speaker 1:  political hack job with no conviction and no morals. That's my

1510
01:31:40,635 --> 01:31:43,195
Speaker 1:  belief. Brendan, if you're listening you can come on to coder and you can

1511
01:31:43,195 --> 01:31:46,235
Speaker 1:  try to prove me wrong, but good luck buddy. He

1512
01:31:46,235 --> 01:31:48,995
Speaker 5:  Doesn't even have a cool mug. Like if you're gonna be terrible at this job,

1513
01:31:49,015 --> 01:31:50,875
Speaker 5:  at least like have a funny mug. You know what I

1514
01:31:50,875 --> 01:31:54,635
Speaker 1:  Mean? You know Ajit Pi, he was a guy. We,

1515
01:31:54,695 --> 01:31:58,515
Speaker 1:  he was also a character in the first Trump administration. He had

1516
01:31:58,595 --> 01:32:02,405
Speaker 1:  a governor. Like he, there was a line of depravity

1517
01:32:02,405 --> 01:32:03,405
Speaker 1:  that he would not cross

1518
01:32:03,635 --> 01:32:03,925
Speaker 5:  Fair.

1519
01:32:04,675 --> 01:32:08,565
Speaker 1:  Like you could be like, I disagree with this irritatingly

1520
01:32:08,565 --> 01:32:11,645
Speaker 1:  Verizon captured conservative Republican commissioner.

1521
01:32:12,475 --> 01:32:15,965
Speaker 1:  Brendan is just a political animal. Yeah. Here of two

1522
01:32:15,965 --> 01:32:19,605
Speaker 1:  examples this week. And they're just gonna happen every week. 'cause the

1523
01:32:19,605 --> 01:32:20,565
Speaker 1:  man loves attention.

1524
01:32:22,865 --> 01:32:26,855
Speaker 1:  He is rolling back a rule that allowed people who

1525
01:32:26,855 --> 01:32:30,015
Speaker 1:  live in apartment buildings to go get their own internet service.

1526
01:32:30,905 --> 01:32:34,565
Speaker 1:  So like a lot of apartment buildings, your landlord would be like,

1527
01:32:34,825 --> 01:32:38,725
Speaker 1:  I'm just gonna sign everybody up for whatever ISP, Verizon

1528
01:32:38,785 --> 01:32:42,005
Speaker 1:  at t Comcast, whatever. And all of you have to use it and you can't go get

1529
01:32:42,005 --> 01:32:45,885
Speaker 1:  your own. And because I have collected this all up together, I'm gonna

1530
01:32:45,885 --> 01:32:49,725
Speaker 1:  get a deal. Right. We have, we have some scale English in the

1531
01:32:49,995 --> 01:32:53,925
Speaker 1:  ISPs and if you've experienced this, you're like, I don't what

1532
01:32:54,555 --> 01:32:58,485
Speaker 1:  like it's just the same number as everything else. Right, right. You're

1533
01:32:58,485 --> 01:33:01,485
Speaker 1:  not getting a deal, you're just being lazy or you're getting a kickback from

1534
01:33:01,485 --> 01:33:05,405
Speaker 1:  the ISP and you're not passing any of that on like, th this was my

1535
01:33:05,405 --> 01:33:07,925
Speaker 1:  experience in an apartment in Chicago was my experience in an apartment in

1536
01:33:07,925 --> 01:33:09,765
Speaker 1:  New York. Like same. Yeah. Several. A lot of people have had this experience.

1537
01:33:09,825 --> 01:33:13,485
Speaker 1:  Yep. Biden, FCC passes rule says

1538
01:33:13,785 --> 01:33:17,325
Speaker 1:  you cannot do bulk billing. You have to increase competition and let

1539
01:33:17,425 --> 01:33:21,365
Speaker 1:  people sign up for their own ISPs and apartments. Brendan Carr

1540
01:33:21,365 --> 01:33:24,125
Speaker 1:  has revoked this rule and his argument is this will lower prices,

1541
01:33:24,995 --> 01:33:27,865
Speaker 5:  Which just on its face doesn't make any sense. It's one of those things that

1542
01:33:27,865 --> 01:33:31,625
Speaker 5:  like, I, I can't even make the bad faith

1543
01:33:32,105 --> 01:33:35,305
Speaker 5:  argument that that makes sense. You know what I mean? Yeah.

1544
01:33:35,305 --> 01:33:38,705
Speaker 1:  Here's a quote. I have ended the F c's consideration of a bride and error

1545
01:33:38,985 --> 01:33:42,585
Speaker 1:  proposal that would, could have increased costs by 50% the price that some

1546
01:33:42,825 --> 01:33:45,585
Speaker 1:  Americans living in apartments pay for internet service. This is regulatory

1547
01:33:45,585 --> 01:33:47,905
Speaker 1:  overreach and it would've artificially raised the cost of our service. There

1548
01:33:47,905 --> 01:33:51,425
Speaker 1:  is not a world in which letting people choose their service provider

1549
01:33:51,605 --> 01:33:55,025
Speaker 1:  raises a surprise. There's a, I might choose world, world mean expensive.

1550
01:33:55,025 --> 01:33:58,145
Speaker 1:  One world in which you live in an apartment, that building that's so big.

1551
01:33:59,095 --> 01:34:02,625
Speaker 1:  Then you have a landlord that's so good that maybe they were

1552
01:34:02,625 --> 01:34:06,565
Speaker 1:  negotiating a terrific rate. I don't know anybody who lives in

1553
01:34:06,565 --> 01:34:07,685
Speaker 1:  an apartment. Yeah, no,

1554
01:34:08,395 --> 01:34:12,205
Speaker 5:  There's there's one of those in America and that's the one Brina car is

1555
01:34:12,205 --> 01:34:12,645
Speaker 5:  talking about.

1556
01:34:12,715 --> 01:34:15,885
Speaker 1:  Yeah. I just, I'm like, what this do you mean by a political animal? Like

1557
01:34:15,885 --> 01:34:19,445
Speaker 1:  he's straight up saying, reducing your experience of broadband

1558
01:34:19,445 --> 01:34:20,925
Speaker 1:  competition will lower prices.

1559
01:34:21,235 --> 01:34:24,565
Speaker 5:  Yeah. It's ridiculous. Alright. What was the other one you said? There were

1560
01:34:24,565 --> 01:34:25,085
Speaker 5:  two pure

1561
01:34:25,085 --> 01:34:27,605
Speaker 1:  Doublespeak. All right, well there's two actually there's two other ones.

1562
01:34:28,585 --> 01:34:31,885
Speaker 1:  The, the second one is he's reopened investigations

1563
01:34:32,355 --> 01:34:35,885
Speaker 1:  into N-B-C-A-B-C and CBS's broadcast license. It was very

1564
01:34:35,885 --> 01:34:39,485
Speaker 1:  complicated 'cause the stations have them, not the networks but Socos

1565
01:34:40,225 --> 01:34:44,125
Speaker 1:  and in particular he opened up the NBC one because he says they

1566
01:34:44,365 --> 01:34:48,205
Speaker 1:  violated the equal time rule by having Kamala Harris on and Tim Kane

1567
01:34:48,405 --> 01:34:49,165
Speaker 1:  on Saturday Night Live.

1568
01:34:49,945 --> 01:34:50,525
Speaker 5:  Oh right.

1569
01:34:51,265 --> 01:34:52,205
Speaker 1:  Do you remember this whole

1570
01:34:52,205 --> 01:34:54,725
Speaker 5:  Thing? This was like last fall that he got all fussy about it. Yep.

1571
01:34:54,725 --> 01:34:58,325
Speaker 1:  Yeah. And so on her way out the door, Biden's FCC chair Jessica

1572
01:34:58,435 --> 01:35:01,885
Speaker 1:  Rosenworcel just like wipes out these investigations. She's like, these are

1573
01:35:01,885 --> 01:35:04,925
Speaker 1:  stupid. I know they're politically motivated. We're not doing this. He's

1574
01:35:04,925 --> 01:35:05,445
Speaker 1:  reopened them,

1575
01:35:07,655 --> 01:35:10,175
Speaker 1:  whatever you could say, whatever. About two of them, they're, they're dumb,

1576
01:35:10,355 --> 01:35:13,895
Speaker 1:  but in particular the NBC one, he knows

1577
01:35:14,525 --> 01:35:18,415
Speaker 1:  that NBC complied with the rule because he has emailed people

1578
01:35:18,875 --> 01:35:22,855
Speaker 1:  and said that they have, this is such a brazen

1579
01:35:22,915 --> 01:35:26,715
Speaker 1:  lie. Yeah. Right. He's like, they had her

1580
01:35:26,735 --> 01:35:30,435
Speaker 1:  on, they complied with the rule, here's their notice. And then immediately

1581
01:35:30,435 --> 01:35:33,675
Speaker 1:  had Trump on a NASCAR race the next day because they gave the proper notice

1582
01:35:33,675 --> 01:35:37,555
Speaker 1:  and the campaign took adv advantage of it. He know, I

1583
01:35:37,555 --> 01:35:41,325
Speaker 1:  know he knows it. 'cause I've seen the email and he is reopened it to be

1584
01:35:41,325 --> 01:35:44,845
Speaker 1:  like I'm investigate. Like this is just total political theater for an audience

1585
01:35:45,065 --> 01:35:45,845
Speaker 5:  Of one. It's just, yeah.

1586
01:35:46,425 --> 01:35:50,205
Speaker 1:  And like this is dangerous First Amendment territory. I don't like this

1587
01:35:50,205 --> 01:35:53,925
Speaker 1:  speech, so I will cost you money is not what you want any government,

1588
01:35:54,065 --> 01:35:57,325
Speaker 1:  it doesn't matter If you like Kamala Harris, you love Donald Trump, you hate

1589
01:35:57,325 --> 01:36:01,285
Speaker 1:  one, you love, it doesn't matter. Unelected bureaucrat saying, I will cost

1590
01:36:01,315 --> 01:36:05,045
Speaker 1:  this money, I will make this painful because I don't like your

1591
01:36:05,045 --> 01:36:08,725
Speaker 1:  political speech is red flashing warning

1592
01:36:08,725 --> 01:36:12,485
Speaker 1:  light of free speech problems. Yeah. And then on top of that,

1593
01:36:12,795 --> 01:36:16,485
Speaker 1:  just as we sat down to record, Carr announced investigations into

1594
01:36:16,685 --> 01:36:20,205
Speaker 1:  NPR and PBS. Okay. 'cause he says they are illegally

1595
01:36:20,565 --> 01:36:23,965
Speaker 1:  advertising and what he specifically means is that they do fundraisers.

1596
01:36:25,165 --> 01:36:28,985
Speaker 1:  And so there his line is taxpayer should not pay

1597
01:36:29,085 --> 01:36:32,905
Speaker 1:  for commercial activity. And then, you know, the usual

1598
01:36:32,905 --> 01:36:36,305
Speaker 1:  garbage about woke Sesame Street. Right. Or woke NPR,

1599
01:36:36,705 --> 01:36:40,465
Speaker 1:  he's attacking NPR in particular because the CEO of NPR is

1600
01:36:40,465 --> 01:36:44,385
Speaker 1:  Katherine Maher who used to run Wikipedia and the conservatives

1601
01:36:44,385 --> 01:36:48,065
Speaker 1:  hate Wikipedia right now. And so this is just Brendan Carr

1602
01:36:48,065 --> 01:36:51,645
Speaker 1:  trying to chill speech. Right. He's like, I will defund

1603
01:36:51,765 --> 01:36:55,645
Speaker 1:  PBS and NPR because they, because I've constructed a legal reality in

1604
01:36:55,645 --> 01:36:59,405
Speaker 1:  which them doing fundraising drives or saying we're supported by viewers

1605
01:36:59,405 --> 01:37:02,645
Speaker 1:  like you call us to give us money, is advertising for an agenda.

1606
01:37:03,595 --> 01:37:07,145
Speaker 5:  Right. With, and all of that is transparently not since, but it maybe it

1607
01:37:07,145 --> 01:37:08,425
Speaker 5:  doesn't matter because he's in charge of

1608
01:37:08,425 --> 01:37:12,385
Speaker 1:  The ffc. It doesn't matter in, in like the alternate reality of x where this

1609
01:37:12,385 --> 01:37:16,305
Speaker 1:  man is trying to build a profile. And I'm tell like if he, Brendan,

1610
01:37:16,505 --> 01:37:20,145
Speaker 1:  I know you pay attention to us, you have for years, you wanna come on decoder

1611
01:37:20,145 --> 01:37:24,105
Speaker 1:  and explain this like craven political bullshit when

1612
01:37:24,105 --> 01:37:27,825
Speaker 1:  you are playing with the absolute fire of the First amendment and claiming

1613
01:37:27,825 --> 01:37:31,565
Speaker 1:  you'll do it. Come on, I'll talk to you. But it's not gonna,

1614
01:37:31,635 --> 01:37:35,285
Speaker 1:  it's not gonna be easy. 'cause I think this stuff is incredibly transparent.

1615
01:37:35,595 --> 01:37:35,885
Speaker 1:  Yeah.

1616
01:37:36,485 --> 01:37:39,925
Speaker 5:  I also, this is not important in the scheme of things, but like

1617
01:37:40,305 --> 01:37:43,965
Speaker 5:  If you find yourself against Sesame Street, like in any way business

1618
01:37:44,605 --> 01:37:48,565
Speaker 5:  politics, like if you're canceling Sesame Street or you're mad

1619
01:37:48,565 --> 01:37:51,405
Speaker 5:  at like you've done something wrong, like just,

1620
01:37:52,285 --> 01:37:55,200
Speaker 5:  it's not good. You shouldn't be against Sesame Street. That's, that's it.

1621
01:37:55,200 --> 01:37:57,085
Speaker 5:  That's all I have to say. Sesame Street is great.

1622
01:37:57,235 --> 01:37:58,005
Speaker 1:  It's pretty straightforward.

1623
01:37:58,095 --> 01:37:59,285
Speaker 5:  Leave Sesame Street alone.

1624
01:37:59,795 --> 01:38:01,365
Speaker 1:  I'll stop. It's lightning ground. We gotta move on.

1625
01:38:01,365 --> 01:38:04,925
Speaker 5:  Yeah. Oh, but I have another super fun, deeply craven

1626
01:38:04,925 --> 01:38:07,245
Speaker 5:  political speech thing. Oh good. Do you wanna talk about it real fast? Love

1627
01:38:07,245 --> 01:38:07,805
Speaker 1:  It. My favorite

1628
01:38:08,395 --> 01:38:12,205
Speaker 5:  Meta agreed to pay $25 million to settle

1629
01:38:12,385 --> 01:38:16,085
Speaker 5:  the, the Trump suit over suspending his account after January 6th,

1630
01:38:17,135 --> 01:38:20,605
Speaker 5:  which is just like in a sea of Mark

1631
01:38:20,605 --> 01:38:24,485
Speaker 5:  Zuckerberg nakedly sucking up to Trump for business reasons. This is the

1632
01:38:24,685 --> 01:38:28,605
Speaker 5:  simplest one yet. Yeah. Like, here's a bunch of money, please be nice. And

1633
01:38:28,605 --> 01:38:32,285
Speaker 5:  then there was some reporting that Trump was basically like,

1634
01:38:32,965 --> 01:38:35,765
Speaker 5:  I think what he said is like, we have to settle this lawsuit before I can

1635
01:38:35,765 --> 01:38:39,565
Speaker 5:  like bring you into the, into the fold. I think he called it the nest. Into

1636
01:38:39,565 --> 01:38:39,925
Speaker 5:  the nest.

1637
01:38:41,505 --> 01:38:45,445
Speaker 5:  And it's just like this, this was the price of admission, right? Like this

1638
01:38:45,445 --> 01:38:48,325
Speaker 5:  is what happened at the inauguration. They all spent a bunch of money to

1639
01:38:48,755 --> 01:38:51,805
Speaker 5:  hang out with him and go to parties and he would say nice things and, and

1640
01:38:51,905 --> 01:38:55,845
Speaker 5:  now meta is just spending another $25 million to make Donald Trump

1641
01:38:55,845 --> 01:38:58,205
Speaker 5:  like them, because that is worth it.

1642
01:38:58,385 --> 01:39:02,165
Speaker 1:  Can I just say why this one is like particularly galling to me? Meta

1643
01:39:02,165 --> 01:39:05,815
Speaker 1:  would've definitely won, right? The theory of this

1644
01:39:05,885 --> 01:39:08,965
Speaker 1:  case Yeah. Is that meta

1645
01:39:09,755 --> 01:39:13,685
Speaker 1:  when it banned Donald Trump was somehow the government and that

1646
01:39:14,045 --> 01:39:17,805
Speaker 1:  violated Trump's first amendment rights. Right. And I would Which is nonsense.

1647
01:39:18,015 --> 01:39:21,805
Speaker 1:  Which is nonsense for the very simple reason that the

1648
01:39:21,805 --> 01:39:25,605
Speaker 1:  president at the time met Aban Trump from its platform was

1649
01:39:25,615 --> 01:39:25,965
Speaker 1:  Trump.

1650
01:39:27,475 --> 01:39:27,765
Speaker 5:  Yeah.

1651
01:39:29,575 --> 01:39:32,355
Speaker 5:  He was the government at that it turns out. Yeah. He

1652
01:39:32,355 --> 01:39:35,515
Speaker 1:  Was super, the gov like, you don't even have to get to the like pedantic

1653
01:39:35,515 --> 01:39:38,155
Speaker 1:  forum stuff of like the First Amendment is about the government not slash

1654
01:39:38,305 --> 01:39:42,185
Speaker 1:  like I can do that all day. I I was, I was raised in that

1655
01:39:42,185 --> 01:39:45,305
Speaker 1:  fire. I'm just saying this one is particularly stupid

1656
01:39:45,775 --> 01:39:49,625
Speaker 1:  because you're like meta acting on orders from the government who was

1657
01:39:49,685 --> 01:39:52,265
Speaker 1:  me illegally abandoning. Yeah.

1658
01:39:53,585 --> 01:39:57,315
Speaker 1:  It's so stupid. He's lost other versions of this case.

1659
01:39:58,145 --> 01:40:02,085
Speaker 1:  But this, this is everyone is doing, there's other cases that he would've

1660
01:40:02,645 --> 01:40:05,965
Speaker 1:  A, B, C settled one, Disney, A, B, C, settled one. They're just trying to

1661
01:40:05,965 --> 01:40:09,285
Speaker 1:  get out of the line of fire. 'cause he controls the regulatory agencies.

1662
01:40:09,285 --> 01:40:11,925
Speaker 5:  Right. All these companies have decided it's worth the money. And the thing

1663
01:40:11,925 --> 01:40:15,735
Speaker 5:  is, they're, they're right. Right. Like I would say pretty transparently.

1664
01:40:16,515 --> 01:40:20,415
Speaker 5:  You, you can make the case that this is just a good use of

1665
01:40:20,415 --> 01:40:22,615
Speaker 5:  $25 million for meta, which is,

1666
01:40:22,615 --> 01:40:23,415
Speaker 1:  Yeah, this is a sneeze

1667
01:40:23,555 --> 01:40:27,335
Speaker 5:  Has a lot of complicated cases going on. There are antitrust questions about

1668
01:40:27,335 --> 01:40:30,495
Speaker 5:  meta. Trump has threatened to throw Mark Zuckerberg in jail a bunch of times.

1669
01:40:30,605 --> 01:40:34,255
Speaker 5:  Like I can see why you write this check. It just sucks.

1670
01:40:34,645 --> 01:40:38,455
Speaker 5:  Like it just sucks so much that this is the state of things

1671
01:40:38,455 --> 01:40:39,255
Speaker 5:  right now. I

1672
01:40:39,255 --> 01:40:42,855
Speaker 1:  Mean, yeah. The FTC is gonna try that. Their antitrust case to break up Metas

1673
01:40:42,855 --> 01:40:45,655
Speaker 1:  coming the various teen health cases about

1674
01:40:46,285 --> 01:40:49,895
Speaker 1:  Instagram causing mental health crises across the country.

1675
01:40:49,895 --> 01:40:53,725
Speaker 1:  They're all coming up. You can, If you have an angry

1676
01:40:53,725 --> 01:40:57,205
Speaker 1:  Trump who's like, break him up and you can get outta that for

1677
01:40:57,205 --> 01:41:00,685
Speaker 1:  $25 million. Yeah. Yes. Done. There's a ruthless cynicism here. Yeah.

1678
01:41:01,185 --> 01:41:04,925
Speaker 1:  But man, we are, I'm, we are just playing with fire with the First Amendment

1679
01:41:04,925 --> 01:41:08,685
Speaker 1:  right now. Like left and right. Yep. We're, we are doing weird

1680
01:41:08,685 --> 01:41:12,445
Speaker 1:  speech regulations and claiming to do them in the name of free speech.

1681
01:41:13,585 --> 01:41:16,735
Speaker 1:  Weird. Super weird. Yeah. All right. We gotta end with a happy one.

1682
01:41:17,085 --> 01:41:17,615
Speaker 5:  Blue sky.

1683
01:41:18,305 --> 01:41:18,595
Speaker 1:  Yeah.

1684
01:41:19,955 --> 01:41:23,855
Speaker 5:  I'm happy Blue Sky's working. I have, I'm all in on Blue Sky. I was was

1685
01:41:23,855 --> 01:41:24,015
Speaker 5:  wait.

1686
01:41:24,205 --> 01:41:25,895
Speaker 1:  Just the concept of Blue Sky. Last Blue sky.

1687
01:41:25,895 --> 01:41:27,135
Speaker 5:  Happy Sky has 30 million years there. You,

1688
01:41:27,375 --> 01:41:30,535
Speaker 1:  I was like, there's news, right? Not just knowledge, like I logged into Blue

1689
01:41:30,815 --> 01:41:31,015
Speaker 1:  Sky.

1690
01:41:31,205 --> 01:41:34,655
Speaker 5:  It's growing. Yeah. Threads is much bigger. Threads

1691
01:41:34,685 --> 01:41:37,895
Speaker 5:  massively bigger. Continues to add a million people a day. It has

1692
01:41:38,265 --> 01:41:42,055
Speaker 5:  320 million is the number Mark Zuckerberg gave on the earnings call this

1693
01:41:42,055 --> 01:41:44,095
Speaker 5:  week. That's a gigantic number.

1694
01:41:45,915 --> 01:41:49,455
Speaker 5:  But Blue sky blue Sky's the fun one now I think we can say pretty,

1695
01:41:50,045 --> 01:41:50,335
Speaker 1:  Yeah,

1696
01:41:50,445 --> 01:41:54,335
Speaker 5:  Certainly that like Blue Sky has replaced Twitter as the

1697
01:41:54,335 --> 01:41:57,615
Speaker 5:  place that is fun to hang out. And it is true. It's a bunch of like liberals

1698
01:41:57,615 --> 01:42:01,135
Speaker 5:  being annoying to each other. Super annoying. But in addition to that, this

1699
01:42:01,135 --> 01:42:01,495
Speaker 5:  is a group of

1700
01:42:01,495 --> 01:42:03,575
Speaker 1:  People. It's also a lot of that does not love jokes. No.

1701
01:42:03,605 --> 01:42:03,895
Speaker 5:  Like

1702
01:42:04,005 --> 01:42:05,935
Speaker 1:  Yeah. That's, at least that's why it's Experie. Yeah.

1703
01:42:06,115 --> 01:42:10,055
Speaker 5:  But like a, I'm psyched that there are multiple versions of this thing

1704
01:42:10,055 --> 01:42:13,015
Speaker 5:  that are succeeding. I think it would've been a bummer if we had just replaced

1705
01:42:13,085 --> 01:42:16,895
Speaker 5:  Twitter with one thing. The fact that there are a few different cuts at this

1706
01:42:16,895 --> 01:42:20,375
Speaker 5:  that are gonna work is really exciting. Mark Zuckerberg also made noise about

1707
01:42:20,375 --> 01:42:23,695
Speaker 5:  wanting to make the OG Facebook experience come back,

1708
01:42:24,295 --> 01:42:28,135
Speaker 5:  which that's not gonna happen. Like, sorry Mark, you ruined

1709
01:42:28,135 --> 01:42:32,015
Speaker 5:  that 15 years ago. But like Blue sky man,

1710
01:42:32,085 --> 01:42:35,215
Speaker 5:  blue sky I think has hit like, escape velocity in that this thing is gonna

1711
01:42:35,215 --> 01:42:38,935
Speaker 5:  work and it's here. People are building stuff on the protocol. People are

1712
01:42:39,025 --> 01:42:42,785
Speaker 5:  using the app. Are they? It's coming. Yeah. I don't

1713
01:42:43,025 --> 01:42:44,345
Speaker 5:  remember what, but they're doing it.

1714
01:42:45,825 --> 01:42:49,705
Speaker 1:  I would say I've, I, I'm a blue sky person. My harshest criticism Blue sky

1715
01:42:49,805 --> 01:42:52,985
Speaker 1:  is for all the promises of that protocol and how open it is.

1716
01:42:53,665 --> 01:42:55,865
Speaker 1:  There's not one other server.

1717
01:42:56,445 --> 01:42:58,895
Speaker 5:  No there's not. But there are people just gimme there. But people building,

1718
01:42:59,295 --> 01:43:03,095
Speaker 5:  somebody built a cool blue Sky EE app that just looks like Instagram

1719
01:43:03,095 --> 01:43:06,095
Speaker 5:  and it just takes photo posts. Like that's cool. That's the fedi verse baby.

1720
01:43:06,385 --> 01:43:09,055
Speaker 5:  Gimme weird ideas about how to look at Blue Sky. I'm in.

1721
01:43:09,055 --> 01:43:12,215
Speaker 1:  Yeah. But that's all just like API front end views. Listen,

1722
01:43:12,495 --> 01:43:12,695
Speaker 5:  I want

1723
01:43:12,995 --> 01:43:13,735
Speaker 1:  One other place to log

1724
01:43:13,735 --> 01:43:14,455
Speaker 5:  In one thing at a time.

1725
01:43:14,475 --> 01:43:16,615
Speaker 1:  Man. I wanna post. Maybe that's gonna be the ver

1726
01:43:16,755 --> 01:43:18,095
Speaker 5:  You said good news. Let's end on good news.

1727
01:43:18,365 --> 01:43:20,455
Speaker 1:  Okay. Good news. We're happy the Fedi verse is

1728
01:43:22,345 --> 01:43:22,765
Speaker 1:  around.

1729
01:43:26,275 --> 01:43:30,165
Speaker 1:  Here we go. I like it. I do like it. I do love an open source weird

1730
01:43:30,165 --> 01:43:33,965
Speaker 1:  pebble hack. And I do love a weird band of misfit,

1731
01:43:34,115 --> 01:43:37,965
Speaker 1:  decentralized social media hackers. That's the good stuff. Hell

1732
01:43:37,965 --> 01:43:41,885
Speaker 1:  yeah. That's the rebel stuff. Well, you know, the bad thing happens. That's

1733
01:43:41,885 --> 01:43:44,845
Speaker 1:  what we want. Send us more. This is your, send us more weird

1734
01:43:45,765 --> 01:43:49,525
Speaker 1:  rebel technology. Right. That stuff. That's your, that's your

1735
01:43:49,725 --> 01:43:50,845
Speaker 1:  homework for this week. And we, we'll

1736
01:43:50,845 --> 01:43:54,165
Speaker 6:  Play the voicemails next week. Love it. All right. That's it. We got, we're

1737
01:43:54,165 --> 01:43:56,565
Speaker 6:  way over time. That's The Vergecast. Bye Carl.

1738
01:44:02,065 --> 01:44:05,085
Speaker 6:  And that's it for The Vergecast this week. And hey, we'd love to hear from

1739
01:44:05,085 --> 01:44:08,845
Speaker 6:  you. Give us a call at eight six six VERGE one one.

1740
01:44:09,025 --> 01:44:12,565
Speaker 6:  The Vergecast is a production of The Verge and the Vox Media Podcast network.

1741
01:44:12,865 --> 01:44:16,805
Speaker 6:  Our show is produced by Will Por, Eric Gomez and Brandon Keefer. And

1742
01:44:16,805 --> 01:44:18,125
Speaker 6:  that's it. We'll see you next week.

