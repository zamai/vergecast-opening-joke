1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: af2f2431-4227-4421-80fc-963363cfeeda
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/7183752148907953753/-4112892553261797519/s93290-US-6344s-1733495599.mp3
Description: Nilay and David talk a bit about this week’s launch of the Verge subscription, plus what’s coming next. (There’s still time to send questions for next week! 866-VERGE11 or email vergecast@theverge.com.) Then they talk about the streaming news of the week, and the ways streaming services are continuing to act like cable companies. Then Kylie Robison joins to talk about the lowering stakes for AGI, shipmas at OpenAI, and more. Finally, in the lighting round, it’s crypto and browsers and Intel. And more crypto.

2
00:01:56,055 --> 00:01:59,525
Speaker 2:  Hello and welcome to broadcast the flagship podcast of paying us money

3
00:02:01,005 --> 00:02:04,205
Speaker 2:  directly. Just show up in my house with a check. Give

4
00:02:04,205 --> 00:02:04,685
Speaker 3:  It to me.

5
00:02:06,025 --> 00:02:09,605
Speaker 2:  Hi, I'm Neli and I would like you to pay us money. I'm also your friend

6
00:02:09,605 --> 00:02:10,445
Speaker 2:  David Pierce is here.

7
00:02:10,865 --> 00:02:14,805
Speaker 3:  Hi. I want slightly less of your money than Neli does. I feel like

8
00:02:14,805 --> 00:02:17,525
Speaker 3:  that's, that's my brand is like, like Neli only less.

9
00:02:18,705 --> 00:02:21,365
Speaker 2:  That's what I'm here for TV but not very good.

10
00:02:23,165 --> 00:02:24,455
Speaker 3:  Exactly. That's what I did here.

11
00:02:24,455 --> 00:02:28,135
Speaker 2:  Yeah, big Vir has today. Kylie Robinson's gonna join us to talk.

12
00:02:28,135 --> 00:02:32,015
Speaker 2:  There's just a lot of AI stuff and all that boils down to what if

13
00:02:32,015 --> 00:02:34,135
Speaker 2:  it's not as good as we said it was gonna be. And

14
00:02:34,135 --> 00:02:36,975
Speaker 3:  They're all talking this week. Yeah. Is the real thing. I've noticed a lot

15
00:02:36,975 --> 00:02:40,835
Speaker 3:  of people are talking and when they do that we have to

16
00:02:40,835 --> 00:02:41,315
Speaker 3:  talk about it.

17
00:02:41,465 --> 00:02:44,835
Speaker 2:  Yeah. So Kyle is gonna join us in a bit. David has put together a package

18
00:02:44,835 --> 00:02:48,515
Speaker 2:  called Cable Is So Back, it's so bad, which is a threat

19
00:02:48,515 --> 00:02:52,435
Speaker 2:  really. We have a lightning round with, I wanna

20
00:02:52,435 --> 00:02:56,195
Speaker 2:  just say this as as directly as I can. A hilariously

21
00:02:56,195 --> 00:02:59,845
Speaker 2:  amazing sponsor given the topics that we're gonna discuss today.

22
00:03:00,145 --> 00:03:03,805
Speaker 2:  All kinds of stuff going on. But we should start with our own news because

23
00:03:03,805 --> 00:03:07,085
Speaker 2:  what is the show about if not being totally self-indulgent about the making

24
00:03:07,145 --> 00:03:11,005
Speaker 2:  of The Verge dot com? Right? That's what, that's what everybody tunes

25
00:03:11,005 --> 00:03:14,325
Speaker 2:  in for, not tech News, just deep inside baseball

26
00:03:14,885 --> 00:03:18,805
Speaker 2:  about our website. So the news this week, you probably

27
00:03:18,805 --> 00:03:21,445
Speaker 2:  saw it if you're listening to Vergecast it, it's almost certain you saw our

28
00:03:21,445 --> 00:03:25,155
Speaker 2:  news this week, we launched a subscription on our website.

29
00:03:25,545 --> 00:03:29,235
Speaker 2:  It's $7 a month or it's $50 a year. If you're in the US and Canada and you

30
00:03:29,235 --> 00:03:32,515
Speaker 2:  give us the 50 bucks a year, we're gonna mail you a print zine, which is

31
00:03:32,515 --> 00:03:36,315
Speaker 2:  very fun called Content Goblins. It's very expensive to ship

32
00:03:36,315 --> 00:03:40,235
Speaker 2:  and mail things overseas. So we're figuring out a way to

33
00:03:40,695 --> 00:03:44,395
Speaker 2:  do something for our, our readers and our listeners overseas. I'm sorry that

34
00:03:44,395 --> 00:03:47,875
Speaker 2:  we didn't figure it out right away. It's our first time, you know,

35
00:03:48,105 --> 00:03:51,555
Speaker 2:  with a subscription product like this. So we're figuring it out. We, we have

36
00:03:51,555 --> 00:03:55,275
Speaker 2:  some ideas we'll solve it, but that's the, the, the shape of it,

37
00:03:55,275 --> 00:03:58,395
Speaker 2:  right? It's, yeah. If you've been listening to us, you've been reading the

38
00:03:58,395 --> 00:04:01,635
Speaker 2:  site, you know that the internet and how we distribute media on the internet

39
00:04:01,635 --> 00:04:05,625
Speaker 2:  is kind of broken. It's very hard to just make money giving

40
00:04:05,625 --> 00:04:08,985
Speaker 2:  away things for free, which is this, this sort of economic reality that

41
00:04:09,435 --> 00:04:12,985
Speaker 2:  makes sense and we all just pretended wasn't real for a long time.

42
00:04:13,255 --> 00:04:17,105
Speaker 2:  What if I worked really hard and then gave you the, the effort for free

43
00:04:17,885 --> 00:04:21,825
Speaker 2:  is that the internet is really founded on this idea and then it turns out

44
00:04:21,965 --> 00:04:24,785
Speaker 2:  the platforms get to make the money and the people who work really hard and

45
00:04:24,785 --> 00:04:27,625
Speaker 2:  give you the stuff for free mostly burn themselves out or go outta business.

46
00:04:27,795 --> 00:04:31,785
Speaker 3:  Right. I think just to put a fine point on that, that is

47
00:04:31,945 --> 00:04:35,905
Speaker 3:  a thing we've seen a lot of feedback to both with

48
00:04:36,005 --> 00:04:39,585
Speaker 3:  our own subscription launch and also just kind of the subscription

49
00:04:39,745 --> 00:04:43,505
Speaker 3:  ification of everything, right? Like the subscription fatigue is real. The

50
00:04:43,505 --> 00:04:47,305
Speaker 3:  idea that anything you want, whether it's a website or a calendar

51
00:04:47,365 --> 00:04:51,065
Speaker 3:  app or Netflix now costs some number of dollars a month instead of just

52
00:04:51,305 --> 00:04:55,185
Speaker 3:  some number of dollars or nothing and ads and like, and

53
00:04:55,185 --> 00:04:58,745
Speaker 3:  there's this idea that okay, why doesn't the ad

54
00:04:58,745 --> 00:05:02,625
Speaker 3:  supported model work anymore? And and the reason is what you just

55
00:05:02,625 --> 00:05:06,025
Speaker 3:  said, the reason is that it does, except it's been completely disintermediated

56
00:05:06,025 --> 00:05:09,825
Speaker 3:  essentially by two companies who just took all of the money. Yeah. And so

57
00:05:10,135 --> 00:05:13,465
Speaker 3:  what that thing where it was like, oh you put the ads in the newspaper and

58
00:05:13,465 --> 00:05:16,825
Speaker 3:  I look at the ads and so the ads are how the things get that that is not

59
00:05:16,825 --> 00:05:20,745
Speaker 3:  how it works anymore because of largely Google and Facebook who are not the

60
00:05:20,745 --> 00:05:24,105
Speaker 3:  only companies that did this, but are mostly the companies that changed that

61
00:05:24,115 --> 00:05:25,065
Speaker 3:  model for everybody.

62
00:05:25,445 --> 00:05:28,825
Speaker 2:  And then you look at how people are making up the gap. And I spent a lot

63
00:05:28,825 --> 00:05:32,465
Speaker 2:  of time thinking about our competitors in the creator economy on YouTube,

64
00:05:32,525 --> 00:05:36,425
Speaker 2:  on TikTok what, whatever. And the platforms don't pay them enough

65
00:05:36,425 --> 00:05:40,265
Speaker 2:  money directly. So they've all almost universally made the same

66
00:05:40,385 --> 00:05:44,145
Speaker 2:  decision, which is to go take brand deals and we just won't. Like I am

67
00:05:44,505 --> 00:05:47,625
Speaker 2:  allergic and like genetically incompatible

68
00:05:48,255 --> 00:05:51,505
Speaker 2:  with a big company paying me money to tell me what to say. We're just not

69
00:05:51,505 --> 00:05:55,345
Speaker 2:  gonna do it. So we, we just landed it. How do we solve

70
00:05:55,345 --> 00:05:57,605
Speaker 2:  this problem? How do we make up the gap? And the answer is subscriptions.

71
00:05:58,445 --> 00:06:01,565
Speaker 2:  I am blown away by how many people

72
00:06:01,995 --> 00:06:05,885
Speaker 2:  immediately sign up for a subscription to our To The Verge. Yeah. Like

73
00:06:06,185 --> 00:06:10,085
Speaker 2:  it blew our estimates and our goals outta the water. We ran outta print zines

74
00:06:10,145 --> 00:06:11,445
Speaker 2:  and we were ordering a second round

75
00:06:12,805 --> 00:06:16,045
Speaker 3:  Printing stuff is so expensive. Did you know this? I had no idea. Printing

76
00:06:16,045 --> 00:06:16,925
Speaker 3:  stuff is so expensive

77
00:06:17,065 --> 00:06:19,485
Speaker 2:  And we are totally used to internet economics, which is like just makes,

78
00:06:19,635 --> 00:06:21,645
Speaker 2:  just make a million more page views. Website

79
00:06:21,995 --> 00:06:22,285
Speaker 3:  More

80
00:06:22,285 --> 00:06:25,165
Speaker 2:  Website. Yeah. And we can do it on demand, you know, like we'll just add

81
00:06:25,165 --> 00:06:28,925
Speaker 2:  more servers to the AWS or whatever like it making digital

82
00:06:28,925 --> 00:06:32,845
Speaker 2:  products is so easy and then print is like, we're gonna put

83
00:06:32,845 --> 00:06:36,285
Speaker 2:  up a bunch of money and hopefully enough people sign up and then lots more

84
00:06:36,285 --> 00:06:40,165
Speaker 2:  people did. So we have to sort out. So I'm just Thank you If you subscribed.

85
00:06:40,845 --> 00:06:43,485
Speaker 2:  I really appreciate it. We really appreciate it. The number of people who

86
00:06:43,645 --> 00:06:47,205
Speaker 2:  subscribed just to support our work, because you've been reading us for 13

87
00:06:47,205 --> 00:06:50,685
Speaker 2:  years, is it's humbling. Like truly humbling. Yeah.

88
00:06:51,385 --> 00:06:55,205
Speaker 2:  We have staved off that we have to charge for our work for 13 years

89
00:06:56,025 --> 00:07:00,005
Speaker 2:  and that is mostly because we do have a big audience, like a audience of

90
00:07:00,005 --> 00:07:03,165
Speaker 2:  people who visits the site every day. Helen, our publisher, pulled the stat

91
00:07:03,495 --> 00:07:07,365
Speaker 2:  55,000 people read The Verge every single day and have for the past year.

92
00:07:07,665 --> 00:07:11,605
Speaker 2:  That's pretty wild. Like every single day for a year they have hit

93
00:07:11,605 --> 00:07:14,405
Speaker 2:  the site, logged in and we can see them. That's not even counting the people

94
00:07:14,405 --> 00:07:18,165
Speaker 2:  who hit us without being logged in. And we can't like see that. It's that

95
00:07:18,165 --> 00:07:22,045
Speaker 2:  user account that is a crazy number. Like in the context of digital

96
00:07:22,055 --> 00:07:25,925
Speaker 2:  media every single day. People who spend the average amount

97
00:07:25,925 --> 00:07:29,085
Speaker 2:  of time on our homepage is six minutes. That is a crazy number. And that's

98
00:07:29,085 --> 00:07:32,725
Speaker 2:  gone up since our redesign, which is a number that almost any other

99
00:07:32,725 --> 00:07:36,165
Speaker 2:  publisher finds impossible to move. So we've made some bets about making

100
00:07:36,165 --> 00:07:40,085
Speaker 2:  our own products really valuable and now we think it's valuable to pay for.

101
00:07:40,505 --> 00:07:43,485
Speaker 2:  So hopefully more of you pay for us. A lot of you have asked us for an ad

102
00:07:43,485 --> 00:07:46,725
Speaker 2:  free Vergecast. David, I don't know If you had the same disconnect in your

103
00:07:46,725 --> 00:07:50,605
Speaker 2:  head here, that the ad experience on the website and the ad experience on

104
00:07:50,605 --> 00:07:52,925
Speaker 2:  the podcast are the same ad experience for a lot of people.

105
00:07:55,605 --> 00:07:58,975
Speaker 2:  It's all Yeah. And like that's not how, that's just not how my brain works.

106
00:07:59,175 --> 00:08:02,095
Speaker 2:  'cause we are so divorced from advertise, like that's literally not the part

107
00:08:02,095 --> 00:08:04,815
Speaker 2:  of the business that we are in. We, we just make this stuff and the ad show

108
00:08:04,815 --> 00:08:08,015
Speaker 2:  up so it, it didn't even like really occur to me like

109
00:08:08,535 --> 00:08:11,295
Speaker 2:  intellectually The Verge, like we should do an ad free VERGE cast. Sure.

110
00:08:11,295 --> 00:08:14,135
Speaker 2:  Like it was on the list and then the immediate feedback is I'm paying you

111
00:08:14,135 --> 00:08:16,175
Speaker 2:  make an ad free VERGE cast so we'll figure out something.

112
00:08:16,725 --> 00:08:19,575
Speaker 3:  Well it is cool. I will say one of the things that I've taken away from it

113
00:08:19,595 --> 00:08:23,495
Speaker 3:  is that like we spend all of our time inside of this thinking

114
00:08:23,495 --> 00:08:27,295
Speaker 3:  about like individual things and individual products and individual

115
00:08:27,365 --> 00:08:31,175
Speaker 3:  sort of pieces of the puzzle and the idea that

116
00:08:31,515 --> 00:08:35,455
Speaker 3:  to most people it all feels like one thing is very cool, right? Like

117
00:08:35,455 --> 00:08:39,135
Speaker 3:  I think to me the, the risk we run at The Verge sometimes is we just like

118
00:08:39,155 --> 00:08:42,055
Speaker 3:  do a lot of things and it all becomes disconnected and weird and everybody's

119
00:08:42,055 --> 00:08:45,895
Speaker 3:  like, why is this happening? And I think part of the thing we've gotten away

120
00:08:45,895 --> 00:08:49,295
Speaker 3:  with for 13 years is we do a lot of that and we have this like big audience

121
00:08:49,295 --> 00:08:52,615
Speaker 3:  of people who think it's fun. And so we get to like all do that together.

122
00:08:53,175 --> 00:08:56,180
Speaker 3:  But it was very cool to have a bunch of people immediately understand like,

123
00:08:56,180 --> 00:09:00,005
Speaker 3:  oh here are all the other things that I consume of yours. We got a ton of

124
00:09:00,005 --> 00:09:03,005
Speaker 3:  people who were like, now that it's subscript a subscription, make everything

125
00:09:03,045 --> 00:09:05,845
Speaker 3:  a social feed, do all of your quick posts on the activity pub. And I was

126
00:09:05,845 --> 00:09:08,285
Speaker 3:  just like, yes, you get it. This is the thing. Yeah. Like we're gonna do

127
00:09:08,285 --> 00:09:11,645
Speaker 3:  it now. This is, this is, this is all in service of that. And it was very

128
00:09:11,645 --> 00:09:15,565
Speaker 3:  cool to see how many people just like instantly saw the whole size of

129
00:09:15,565 --> 00:09:19,405
Speaker 3:  the whole thing. Ad free podcasts very complicated. Also

130
00:09:19,455 --> 00:09:23,165
Speaker 3:  allow me to plug, we're gonna talk about a lot of this stuff in

131
00:09:23,195 --> 00:09:26,805
Speaker 3:  more depth on Tuesday's show both with you and with Helen Havelock, our

132
00:09:26,805 --> 00:09:30,365
Speaker 3:  publisher. We got a lot of questions. If you have questions,

133
00:09:30,455 --> 00:09:33,925
Speaker 3:  email us vergecast at The Verge dot com. Call the hotline eight six six version

134
00:09:33,945 --> 00:09:37,845
Speaker 3:  one one. You have, I would say like a few hours from when you're hearing

135
00:09:37,845 --> 00:09:39,365
Speaker 3:  this to get the questions in Pull over

136
00:09:39,365 --> 00:09:39,925
Speaker 2:  In your car right

137
00:09:39,925 --> 00:09:43,885
Speaker 3:  Now. Yeah, like do it right now because I have to record as neli in order

138
00:09:43,885 --> 00:09:47,565
Speaker 3:  to get this all done. But we're gonna answer lots more questions in

139
00:09:47,635 --> 00:09:51,205
Speaker 3:  lots more depth and we've gotten a a lot of really good interesting ones

140
00:09:51,205 --> 00:09:54,885
Speaker 3:  that are like kind of about The Verge but are also just about like what does

141
00:09:54,885 --> 00:09:58,365
Speaker 3:  it mean to be on the internet in 2024? Which is really interesting.

142
00:09:59,265 --> 00:10:03,205
Speaker 2:  And I you're, if you're, and the one thing I'll point out is we're

143
00:10:03,205 --> 00:10:06,685
Speaker 2:  always saying we have no idea how the business works. Helen is the business.

144
00:10:06,835 --> 00:10:07,125
Speaker 2:  Yeah.

145
00:10:07,125 --> 00:10:08,965
Speaker 3:  Helen super knows how the business works. She's,

146
00:10:08,965 --> 00:10:12,725
Speaker 2:  She's the money. Yeah. And Helen's backstory is really great. She

147
00:10:12,835 --> 00:10:16,765
Speaker 2:  came to us, she was on the Samsung account at Edelman and she joined

148
00:10:16,765 --> 00:10:20,125
Speaker 2:  us as our engagement editor. And my pitch to her was, If you can,

149
00:10:20,625 --> 00:10:23,565
Speaker 2:  If you can make people watch Samsung's branded content, you can definitely

150
00:10:23,565 --> 00:10:27,085
Speaker 2:  make them watch VERGE stuff on these platforms. That was years ago. And then

151
00:10:27,085 --> 00:10:30,365
Speaker 2:  she was so much better at running our company than me that I just made her

152
00:10:30,365 --> 00:10:34,245
Speaker 2:  my boss because that was simpler than me asking her what to do and then

153
00:10:34,245 --> 00:10:38,045
Speaker 2:  pretending that was my idea, which other

154
00:10:38,045 --> 00:10:40,485
Speaker 2:  people find very easy and I find very challenging. So I was like, you're

155
00:10:40,485 --> 00:10:42,525
Speaker 2:  the boss now go be the boss. So If you have questions about our business,

156
00:10:42,525 --> 00:10:46,445
Speaker 2:  Helen is a person who will answer them. And I a lot of

157
00:10:46,585 --> 00:10:50,565
Speaker 2:  the how do we charge, how much do we charge? What is the model

158
00:10:50,665 --> 00:10:54,525
Speaker 2:  for this thing that is Helen and I going back and forth and like what does

159
00:10:54,525 --> 00:10:58,085
Speaker 2:  it mean to run a website in 2024? The one thing I'll add before we stop talking

160
00:10:58,085 --> 00:11:01,685
Speaker 2:  about ourselves is our approach to this subscription is different than a

161
00:11:01,685 --> 00:11:05,405
Speaker 2:  lot of other publications. So I want our homepage to remain a

162
00:11:05,675 --> 00:11:09,165
Speaker 2:  very useful free utility to a lot of people.

163
00:11:09,945 --> 00:11:13,725
Speaker 2:  And if we have a lot of folks who listen to us who build things, where does

164
00:11:13,785 --> 00:11:17,165
Speaker 2:  my customer come from is a very hard problem to solve on the internet. Yeah.

165
00:11:17,425 --> 00:11:21,005
Speaker 2:  And for most people the answer is like, I paid for Facebook ads or something.

166
00:11:21,805 --> 00:11:25,725
Speaker 2:  I don't, we just don't wanna do that. And for publishers in particular, it's

167
00:11:25,725 --> 00:11:29,325
Speaker 2:  like you are trying to get people to convert into subscribers on one story

168
00:11:29,325 --> 00:11:32,885
Speaker 2:  at a time on social media that went viral and that I think the incentives

169
00:11:32,915 --> 00:11:36,565
Speaker 2:  that are pretty broken, I want people to be our, become our subscribers

170
00:11:36,565 --> 00:11:40,205
Speaker 2:  because we've just been really useful to them for a long time. So we are

171
00:11:40,595 --> 00:11:44,565
Speaker 2:  very focused on The Verge dot com is a thing you can come to a bunch of times

172
00:11:44,645 --> 00:11:47,965
Speaker 2:  a day with a lot of news on it, a lot of quick posts, a lot of free stuff.

173
00:11:48,545 --> 00:11:52,485
Speaker 2:  And hopefully that's just useful and that's fine. And when we

174
00:11:52,485 --> 00:11:56,045
Speaker 2:  are proven ourselves to be useful and good practitioners of

175
00:11:56,045 --> 00:11:59,845
Speaker 2:  journalism and good stewards of our community, then you'll pay us money

176
00:12:00,675 --> 00:12:03,925
Speaker 2:  already. Like some people are happy and they think we've done that, we have

177
00:12:03,925 --> 00:12:06,765
Speaker 2:  to earn it. For the rest of you, I, we have some product stuff to do. I think

178
00:12:06,925 --> 00:12:09,165
Speaker 2:  I agree with everybody. We should make it clear what stuff is freed, what

179
00:12:09,165 --> 00:12:12,365
Speaker 2:  stuff is not. Yep. That's complicated. We can get into that with Helen. Like

180
00:12:12,395 --> 00:12:15,605
Speaker 2:  there's reasons why that's complicated just on, based on how the paywall

181
00:12:15,605 --> 00:12:19,245
Speaker 2:  works. It's not some conspiracy, it's just like literally

182
00:12:20,605 --> 00:12:24,445
Speaker 2:  exposing that logic to people might be more confusing than not so we'll

183
00:12:24,445 --> 00:12:27,925
Speaker 2:  we'll just like walk through it. But that's the goal that we have is to make

184
00:12:27,985 --> 00:12:31,965
Speaker 2:  the heart of the thing a really useful free

185
00:12:31,965 --> 00:12:34,765
Speaker 2:  service to people where you're like, I would like to go somewhere on the

186
00:12:34,885 --> 00:12:38,765
Speaker 2:  internet where smart human beings are curating the

187
00:12:38,765 --> 00:12:41,565
Speaker 2:  news and explaining how things work,

188
00:12:42,625 --> 00:12:46,605
Speaker 2:  not AI slop. Right. And I, I feel like that we should just make that

189
00:12:46,725 --> 00:12:50,365
Speaker 2:  a big useful free website and then our

190
00:12:50,365 --> 00:12:54,005
Speaker 2:  reporting and analysis and reviews and all that other stuff, if that is useful

191
00:12:54,005 --> 00:12:57,005
Speaker 2:  to you, if that is interesting to you, you can pay to support us. So that's

192
00:12:57,005 --> 00:13:00,855
Speaker 2:  the plan. It has been two days, right. So

193
00:13:01,165 --> 00:13:04,535
Speaker 2:  it's a little messy parts. Oh, and we cut down the ads by the way, when we,

194
00:13:04,535 --> 00:13:07,655
Speaker 2:  we ask people what they wanna pay for we, when we've done surveys in the

195
00:13:07,655 --> 00:13:11,255
Speaker 2:  past years, turn off the ads or gimme fewer ads.

196
00:13:11,675 --> 00:13:15,615
Speaker 2:  It it's such, it's so by far the the first

197
00:13:15,635 --> 00:13:18,575
Speaker 2:  answer. Yeah. That we take it off of the charts.

198
00:13:19,605 --> 00:13:23,135
Speaker 2:  Like it's so overwhelmingly the thing that people wanted to pay for that

199
00:13:23,135 --> 00:13:26,695
Speaker 2:  we had to remove it from the charts to see all the other stuff.

200
00:13:27,785 --> 00:13:31,405
Speaker 2:  So we, we, I was, if I was a brat about anything, I was a huge brat about

201
00:13:31,405 --> 00:13:35,325
Speaker 2:  that. And that is a new system. It's a new like ads

202
00:13:35,325 --> 00:13:39,285
Speaker 2:  service that we had to build. It's a little buggy. It just give us a

203
00:13:39,285 --> 00:13:42,525
Speaker 2:  little bit of grace. We're gonna figure it all out. But those are the things

204
00:13:42,525 --> 00:13:45,365
Speaker 2:  that we really wanted to do and that's the model we really wanna have.

205
00:13:46,885 --> 00:13:47,855
Speaker 2:  Give us your feedback.

206
00:13:48,205 --> 00:13:52,095
Speaker 3:  Yeah. Yeah. We talked a lot about not wanting to just like throw

207
00:13:52,175 --> 00:13:55,655
Speaker 3:  a wall up in front of The Verge but build software. Yeah. And I feel like

208
00:13:55,745 --> 00:13:58,815
Speaker 3:  we're, we're, we have a lot of work to do on that front, but that feels still

209
00:13:58,815 --> 00:14:01,055
Speaker 3:  like the right goal as I think about all this stuff.

210
00:14:01,365 --> 00:14:04,135
Speaker 2:  Yeah. And again, the thing that I'm just thinking about is like how do I,

211
00:14:04,395 --> 00:14:07,855
Speaker 2:  how do we earn your money? Like I think we have to earn it and honestly we

212
00:14:07,855 --> 00:14:11,295
Speaker 2:  have to earn it once a month or once a year. And I just wanna make sure that

213
00:14:11,295 --> 00:14:15,055
Speaker 2:  the core service we provide continues to be useful and fun

214
00:14:15,075 --> 00:14:19,015
Speaker 2:  and high quality and like people made it people with points of view and beliefs,

215
00:14:19,555 --> 00:14:23,255
Speaker 2:  not just ai. That honestly as we will discover

216
00:14:23,345 --> 00:14:25,935
Speaker 2:  later in the show, mostly it doesn't know what it's talking about

217
00:14:27,635 --> 00:14:29,095
Speaker 3:  But it's doing stuff. Yeah.

218
00:14:29,155 --> 00:14:31,815
Speaker 2:  So that's the plan. We'll see how it works. And the, by the way, the activity

219
00:14:31,875 --> 00:14:35,855
Speaker 2:  bub stuff, the can we integrate with at Protocol in Blue Sky, it's

220
00:14:35,855 --> 00:14:38,735
Speaker 2:  just so high on our list now. Now that I don't have to figure out how to

221
00:14:38,735 --> 00:14:42,295
Speaker 2:  monetize that like inside of itself and I can just be like, this is for

222
00:14:42,495 --> 00:14:45,375
Speaker 2:  subscribers. It's gonna get way easier to just be like, here, here's some

223
00:14:45,375 --> 00:14:48,255
Speaker 2:  stuff to try out. Let us know how it works. Totally. So we're very excited

224
00:14:48,255 --> 00:14:48,495
Speaker 2:  about that.

225
00:14:49,085 --> 00:14:52,925
Speaker 3:  Yeah, it's gonna be good. Now let's talk about cable because I

226
00:14:53,045 --> 00:14:54,525
Speaker 3:  realized I just have subscriptions on the brand.

227
00:14:54,875 --> 00:14:58,605
Speaker 2:  This is the next thing that we're doing. Yeah. We're Cable Network 24 7

228
00:14:58,615 --> 00:14:59,285
Speaker 2:  cable news.

229
00:15:00,385 --> 00:15:03,645
Speaker 3:  Listen, it, it could work. We're gonna buy Spinco from

230
00:15:04,795 --> 00:15:05,845
Speaker 3:  show whatever company that is.

231
00:15:06,395 --> 00:15:09,285
Speaker 2:  Comcast. Oh my God. Disclosure Disclosure.

232
00:15:10,745 --> 00:15:14,405
Speaker 3:  NBC Universal is a minority investor in Vox Media. Hey, you did it. I have

233
00:15:14,405 --> 00:15:18,205
Speaker 3:  Peacock. Yeah. The day of the Jackal is pretty good on

234
00:15:18,205 --> 00:15:18,645
Speaker 3:  Peacock.

235
00:15:19,835 --> 00:15:23,485
Speaker 2:  This has nothing to do with NBC Universal just TV shows. And I wrote one

236
00:15:23,485 --> 00:15:26,405
Speaker 2:  up, I've been watching Landman on Paramount Plus. Okay. You, you're like

237
00:15:26,405 --> 00:15:26,525
Speaker 2:  the

238
00:15:26,525 --> 00:15:28,445
Speaker 3:  Eighth person who has told me the

239
00:15:28,445 --> 00:15:32,125
Speaker 2:  Show's horrible now, now that I'm like in it, I'm like, they just, Taylor

240
00:15:32,125 --> 00:15:36,005
Speaker 2:  Sheridan just wrote a show where he, he was like, here's all of my worst

241
00:15:36,405 --> 00:15:40,165
Speaker 2:  instincts as a writer. But like they're pretty fun. Okay. You know what I

242
00:15:40,165 --> 00:15:43,205
Speaker 2:  mean? Yeah. Like just do cowboy stuff. But what's it's like when

243
00:15:43,345 --> 00:15:45,685
Speaker 3:  Let Aaron Sorkin direct a movie and you're like, this isn't a good movie

244
00:15:45,685 --> 00:15:47,925
Speaker 3:  but it's it's you and I appreciate that. Yeah,

245
00:15:47,925 --> 00:15:51,845
Speaker 2:  Yeah. It's good. And it's like if more TV shows were

246
00:15:51,845 --> 00:15:55,495
Speaker 2:  just that like compelling character actor just fast talking,

247
00:15:55,995 --> 00:15:57,415
Speaker 2:  I'd be like, I should pay for cable again.

248
00:15:59,355 --> 00:16:02,095
Speaker 2:  But most, most TV shows are like, this is fine if you're on your phone.

249
00:16:04,575 --> 00:16:07,195
Speaker 2:  So I, that's a big setup for

250
00:16:09,175 --> 00:16:13,035
Speaker 2:  the TV industry is desperately trying to reinvent the TV industry of

251
00:16:13,065 --> 00:16:15,155
Speaker 2:  what, the late nineties, early two thousands

252
00:16:15,305 --> 00:16:18,955
Speaker 3:  Thereabouts. Yeah. Like right before it became

253
00:16:19,305 --> 00:16:23,075
Speaker 3:  easy to do things on the internet with is kind of what we're going back to

254
00:16:24,145 --> 00:16:27,955
Speaker 3:  like in the, in those days where like many of our listeners will not

255
00:16:28,155 --> 00:16:31,315
Speaker 3:  remember this, but like a webpage would literally load line by line as it

256
00:16:31,315 --> 00:16:34,275
Speaker 3:  would go. Like that's what we're headed back to. That's what these companies

257
00:16:34,275 --> 00:16:38,195
Speaker 3:  are trying to get. Again, the two big pieces of news this week

258
00:16:38,805 --> 00:16:42,035
Speaker 3:  along those lines were that Disney announced its adding

259
00:16:42,635 --> 00:16:46,595
Speaker 3:  ESPN into the Disney Plus app. They did this with Hulu earlier

260
00:16:46,665 --> 00:16:50,595
Speaker 3:  this year and just basically like stuck Hulu as a tab and also

261
00:16:50,695 --> 00:16:54,035
Speaker 3:  all of the Hulu content inside of Disney Plus. And then

262
00:16:54,575 --> 00:16:58,475
Speaker 3:  at the same time Max is getting these like

263
00:16:58,495 --> 00:17:01,075
Speaker 3:  always on HBO channels,

264
00:17:02,365 --> 00:17:05,515
Speaker 3:  which I think are fascinating and it's doing them exactly wrong and we should

265
00:17:05,515 --> 00:17:09,195
Speaker 3:  talk about that. But there is, there is this like incredible amount

266
00:17:09,335 --> 00:17:13,035
Speaker 3:  of bundling A happening and

267
00:17:13,275 --> 00:17:17,035
Speaker 3:  B, we're just going back to something should be

268
00:17:17,035 --> 00:17:19,875
Speaker 3:  playing when you open the app again. Which is the thing I've been saying

269
00:17:19,875 --> 00:17:23,755
Speaker 3:  about streaming for forever is like why, why do I turn

270
00:17:23,755 --> 00:17:27,715
Speaker 3:  on my TV to a grid of icons and then click into one

271
00:17:27,715 --> 00:17:31,195
Speaker 3:  of the icons and I'm presented with another grid of icons and then I click

272
00:17:31,195 --> 00:17:34,395
Speaker 3:  into that and I'm presented with a big picture and then I press play and

273
00:17:34,395 --> 00:17:37,875
Speaker 3:  I'm watching something like, just show me something. And it's like you, you

274
00:17:37,875 --> 00:17:41,795
Speaker 3:  hear all the people who kept cable for forever. That

275
00:17:41,795 --> 00:17:45,035
Speaker 3:  is the reason. It's because you turn the TV on and it shows you television.

276
00:17:45,515 --> 00:17:49,485
Speaker 3:  Like there's, there are things happening when I turn my TV on is pretty compelling.

277
00:17:49,865 --> 00:17:53,765
Speaker 3:  And like the, the fast channels, the, you know, two Bs and

278
00:17:53,765 --> 00:17:56,725
Speaker 3:  Pluto's of the world have done a good job of replicating some of that where

279
00:17:56,725 --> 00:17:59,165
Speaker 3:  you can just, you don't have to log in, you just flip it on and pick a thing

280
00:17:59,165 --> 00:18:03,125
Speaker 3:  and it starts playing. And we're now starting to see that roll back into

281
00:18:03,125 --> 00:18:07,005
Speaker 3:  these premium services in a really big way, which I find totally fascinating.

282
00:18:07,825 --> 00:18:10,525
Speaker 3:  But between those two, it's like this is just cable that is, those are the

283
00:18:10,525 --> 00:18:12,285
Speaker 3:  main two tenets of cable television.

284
00:18:12,425 --> 00:18:15,005
Speaker 2:  So let's start with Disney Plus 'cause they added Hulu and now they're adding

285
00:18:15,305 --> 00:18:19,205
Speaker 2:  ESP N and ESPN's their big bet. There's just a lot of weirdness there

286
00:18:19,205 --> 00:18:23,025
Speaker 2:  because I, the ESPN app has a lot of live

287
00:18:23,025 --> 00:18:26,905
Speaker 2:  sports in it. I watch NFL Prime to and on ESPN plus every

288
00:18:26,905 --> 00:18:30,785
Speaker 2:  week. It's like the only reason to have ESPN plus my opinion. Sure. But it,

289
00:18:31,055 --> 00:18:34,945
Speaker 2:  it's like hard to understand what is coming to Disney

290
00:18:34,975 --> 00:18:38,265
Speaker 2:  Plus from SPN because it's like select live events

291
00:18:38,845 --> 00:18:42,545
Speaker 2:  and that includes the Simpsons Fun Day football animated Monday night

292
00:18:42,785 --> 00:18:46,465
Speaker 2:  football game. And it's like I don't, do I want that? Like

293
00:18:46,845 --> 00:18:50,065
Speaker 2:  is that what I want? And then you look at what's on ESPN Plus and it's like

294
00:18:50,065 --> 00:18:53,985
Speaker 2:  30,000 live events and it's like, okay, so in Disney plus

295
00:18:54,105 --> 00:18:57,985
Speaker 2:  I get the cartoon NFL and on regular ES ESP NI get

296
00:18:58,155 --> 00:19:01,945
Speaker 2:  every sport they have. It's weird, it's like a weird cutdown

297
00:19:02,695 --> 00:19:06,625
Speaker 2:  version of ESPN lives inside of Disney Plus even their

298
00:19:06,625 --> 00:19:10,425
Speaker 2:  studio shows, which presumably can do whatever they want. It's only select

299
00:19:10,425 --> 00:19:13,705
Speaker 2:  studio shows. And I, by the way, I've never really looked at the full list

300
00:19:13,805 --> 00:19:17,625
Speaker 2:  of ESPN studio shows. So the full list, you know, it's college

301
00:19:17,735 --> 00:19:21,385
Speaker 2:  dame, it's college game day NFL matchup, the Pat

302
00:19:21,385 --> 00:19:24,545
Speaker 2:  McAfee show. Everybody knows about pardon the Interruption, which

303
00:19:26,105 --> 00:19:28,665
Speaker 2:  I think Vergecast listeners may not know what pardon the interruption is,

304
00:19:28,665 --> 00:19:30,785
Speaker 2:  but every time David and I talk about what The Vergecast should be, we're

305
00:19:30,785 --> 00:19:32,505
Speaker 2:  like what was just, pardon the interruption.

306
00:19:32,505 --> 00:19:32,865
Speaker 3:  That's real.

307
00:19:32,975 --> 00:19:36,745
Speaker 2:  Yeah. And then what I think is the ultimate distillation

308
00:19:36,805 --> 00:19:39,865
Speaker 2:  of what every single ESPN studio show is. And I didn't realize they had a

309
00:19:39,865 --> 00:19:43,545
Speaker 2:  show called this. It's just good guy, bad guy. I'm

310
00:19:43,545 --> 00:19:44,945
Speaker 3:  So glad you noticed this too.

311
00:19:45,895 --> 00:19:46,185
Speaker 2:  I've

312
00:19:46,185 --> 00:19:47,385
Speaker 3:  Never heard of the show in my life.

313
00:19:47,765 --> 00:19:50,105
Speaker 2:  And it's like, oh that's every s ESPN show.

314
00:19:51,735 --> 00:19:53,265
Speaker 2:  It's just good guy, bad guy,

315
00:19:55,165 --> 00:19:57,425
Speaker 2:  but good guy, bad guy is not on Disney Plus.

316
00:19:58,405 --> 00:19:59,345
Speaker 3:  No good guy, bad guy

317
00:19:59,625 --> 00:20:01,745
Speaker 2:  Anywhere. It might just be a fake one. They might have just thrown that in

318
00:20:01,745 --> 00:20:04,945
Speaker 2:  the list like the, like the way that Van Halen used to do the Brown M and

319
00:20:04,945 --> 00:20:06,745
Speaker 2:  MSM thing just to see If you were paying attention.

320
00:20:08,785 --> 00:20:10,595
Speaker 3:  It's apparent it's an MMA show.

321
00:20:11,775 --> 00:20:14,275
Speaker 2:  But so like this idea that you have the Disney plus app and we should talk

322
00:20:14,275 --> 00:20:18,075
Speaker 2:  about the fact that it's Disney plus it's the app, right? Yeah. It's, and

323
00:20:18,075 --> 00:20:21,915
Speaker 2:  now Disney plus that plus is almost more important than the

324
00:20:21,915 --> 00:20:24,995
Speaker 2:  Disney part. Yeah because you're full of Hulu. You got all the

325
00:20:25,695 --> 00:20:28,515
Speaker 2:  ESP n stuff except for the most important ESP n show of all Good guy, bad

326
00:20:28,515 --> 00:20:28,715
Speaker 2:  guy.

327
00:20:30,575 --> 00:20:33,795
Speaker 2:  I'm not even gonna look up. I don't even want to know it's better in my imagination.

328
00:20:36,545 --> 00:20:40,195
Speaker 2:  It's weird, right, that you've got that is your new cable box.

329
00:20:40,705 --> 00:20:44,515
Speaker 2:  Yeah. Like the idea is you opened Disney Plus and you are having like a cable-like

330
00:20:44,515 --> 00:20:47,890
Speaker 2:  experience in there with content from all of these, these networks shoveled

331
00:20:47,890 --> 00:20:51,765
Speaker 2:  through Hulu or shoveled through sports with ESPN or on the different Disney

332
00:20:51,935 --> 00:20:55,565
Speaker 2:  tiles. Like they're very quickly getting back to this is just a cable box

333
00:20:55,565 --> 00:20:59,405
Speaker 2:  that happens to run the Apple TV and then Max is coming at it from the other

334
00:20:59,405 --> 00:21:02,445
Speaker 2:  direction, which is you open it up and there's some channels in here that

335
00:21:02,445 --> 00:21:05,885
Speaker 2:  you can just start with. Peacock has a version of this going on inside the

336
00:21:05,885 --> 00:21:09,685
Speaker 2:  app. Paramount Plus has CBS inside the app at some

337
00:21:09,685 --> 00:21:13,405
Speaker 2:  point. Don't the Rokus and Apple TVs and the

338
00:21:14,035 --> 00:21:17,085
Speaker 2:  Samsung ties in team have to just say stop it.

339
00:21:17,835 --> 00:21:20,685
Speaker 2:  Like just give us the content directly. Not everybody wants 10 different

340
00:21:20,685 --> 00:21:22,205
Speaker 2:  cable box apps on their cable box.

341
00:21:23,045 --> 00:21:26,965
Speaker 3:  I think that's what all of those platforms would be thrilled to

342
00:21:26,965 --> 00:21:29,725
Speaker 3:  do and are somewhat desperate to do. And I mean that's the animating principle

343
00:21:29,725 --> 00:21:33,645
Speaker 3:  behind like Amazon channels and the thing that Apple

344
00:21:33,705 --> 00:21:37,325
Speaker 3:  is trying to do where they're like, don't subscribe to Max,

345
00:21:37,835 --> 00:21:41,525
Speaker 3:  just subscribe to Max through Apple TV and you'll get all the content just

346
00:21:41,525 --> 00:21:45,365
Speaker 3:  natively built into the system that has like UI

347
00:21:45,435 --> 00:21:48,925
Speaker 3:  upsides and it's easy to do in a way that sort of makes sense. But it's also

348
00:21:49,675 --> 00:21:53,405
Speaker 3:  that gives this like precious user

349
00:21:53,475 --> 00:21:57,365
Speaker 3:  data and branding stuff and content to those companies.

350
00:21:57,385 --> 00:22:00,765
Speaker 3:  And so all of the streamers have absolutely no interest in doing it.

351
00:22:01,115 --> 00:22:01,405
Speaker 3:  Yeah.

352
00:22:01,755 --> 00:22:04,885
Speaker 2:  Also, you often get a degradation in video quality when you play when you

353
00:22:04,885 --> 00:22:05,325
Speaker 2:  do it that way.

354
00:22:05,585 --> 00:22:09,525
Speaker 3:  That's true too. Which I notice on my television that I for sure

355
00:22:09,905 --> 00:22:11,525
Speaker 3:  see the difference every single time that

356
00:22:11,525 --> 00:22:14,445
Speaker 2:  Happens. I'm saying If you buy the, If you buy, If you stream on Max,

357
00:22:15,235 --> 00:22:19,085
Speaker 2:  like you usually get the highest quality in the first party service. I

358
00:22:19,205 --> 00:22:23,045
Speaker 2:  actually don't know this is true because Max has forgotten that I got

359
00:22:23,045 --> 00:22:26,965
Speaker 2:  it for free from at and t during, during that Ill faded period.

360
00:22:27,425 --> 00:22:30,765
Speaker 2:  And So I just haven't been paying for it. 'cause it's part of my at ts for

361
00:22:30,765 --> 00:22:33,805
Speaker 2:  shit. And even in the fact that I'm saying it out loud, I don't think there's

362
00:22:33,805 --> 00:22:37,725
Speaker 2:  anybody at at and t or Max who can identify my account and undo

363
00:22:37,725 --> 00:22:40,245
Speaker 2:  this. Yeah. I challenge you right now to stop being

364
00:22:40,245 --> 00:22:42,845
Speaker 3:  Free. The owner of that spreadsheet for sure does not work at any of those

365
00:22:43,045 --> 00:22:43,445
Speaker 3:  companies. Oh

366
00:22:43,445 --> 00:22:47,325
Speaker 2:  Yeah. They're gone. Nobody even knows where this data lives. Go

367
00:22:47,325 --> 00:22:50,955
Speaker 2:  ahead. Turn off my free max subscription disclosure.

368
00:22:51,275 --> 00:22:53,955
Speaker 2:  I haven't paid for that shit in years, but it's an only in 10 80 p

369
00:22:55,225 --> 00:22:57,955
Speaker 2:  because I can't upgrade to 4K. Just just to torture

370
00:22:58,095 --> 00:22:59,155
Speaker 3:  You specifically. I

371
00:22:59,155 --> 00:23:02,995
Speaker 2:  Can't. Right. Anyway, I'm just saying like you notice that the, the first

372
00:23:02,995 --> 00:23:06,755
Speaker 2:  party app often has the best quality. Yeah. Because

373
00:23:06,755 --> 00:23:09,995
Speaker 2:  they're not giving the best quality to some partner. And so like it's like

374
00:23:09,995 --> 00:23:12,955
Speaker 2:  I don't wanna do that 'cause I'm the one person in America who cares about

375
00:23:12,955 --> 00:23:14,875
Speaker 2:  this. David is like, I buy everything through Amazon.

376
00:23:17,445 --> 00:23:19,535
Speaker 3:  Yeah. It's, it's fine. I probably,

377
00:23:19,875 --> 00:23:20,455
Speaker 2:  But it does feel like

378
00:23:20,575 --> 00:23:23,615
Speaker 3:  Passcode is very recognizable and easy to remember and it works out.

379
00:23:23,885 --> 00:23:27,815
Speaker 2:  I'll just point it out. The, the cable era was not a great era for user choice

380
00:23:27,915 --> 00:23:31,895
Speaker 2:  or sane pricing or user experience. Right.

381
00:23:31,895 --> 00:23:35,415
Speaker 2:  You, you got a cable box, it had a bad grid guide.

382
00:23:36,195 --> 00:23:39,655
Speaker 2:  You, you got a bunch of channels you didn't want. Right. Like none of that

383
00:23:39,655 --> 00:23:43,615
Speaker 2:  was great. But one you turned on your tv, it just

384
00:23:43,615 --> 00:23:47,575
Speaker 2:  worked and everything was there. Yep. And two, and I, this

385
00:23:47,595 --> 00:23:50,935
Speaker 2:  to me is like the most important thing. The industry was making a lot of

386
00:23:50,935 --> 00:23:54,375
Speaker 2:  money and a lot of people were employed and a lot of good TV was getting

387
00:23:54,375 --> 00:23:57,875
Speaker 2:  made. And that is because they controlled their distribution.

388
00:23:58,285 --> 00:24:01,915
Speaker 2:  Right. The the cable companies of the world, as evil as they were and The

389
00:24:02,155 --> 00:24:05,995
Speaker 2:  Verge has published a story called Comcast, the

390
00:24:05,995 --> 00:24:09,915
Speaker 2:  worst company in America that like, they didn't like, like Disclosure,

391
00:24:09,915 --> 00:24:13,275
Speaker 2:  Comcast is an investor we have called out com. I personally have spent a

392
00:24:13,275 --> 00:24:17,195
Speaker 2:  lot of my career calling out Comcast for not being a great company and

393
00:24:17,195 --> 00:24:20,915
Speaker 2:  for having bad user experience and over billing people all the stuff cable

394
00:24:21,115 --> 00:24:24,875
Speaker 2:  companies and is piece down the line. Right. But there was a thing that they

395
00:24:24,905 --> 00:24:28,835
Speaker 2:  made where they owned their distribution and we sent video programming

396
00:24:28,835 --> 00:24:32,515
Speaker 2:  over that distribution and everyone paid high rates and the prices were too

397
00:24:32,515 --> 00:24:35,955
Speaker 2:  high. Yes. But you didn't know that you were paying a bunch of money for

398
00:24:35,995 --> 00:24:38,355
Speaker 2:  ESPN, whether you wanted it or not. It just was like part of your cable bundle.

399
00:24:38,695 --> 00:24:41,235
Speaker 2:  And everyone was like, fine, they've got cable, here's all this stuff I can

400
00:24:41,235 --> 00:24:44,355
Speaker 2:  watch. And then we moved our distribution to the internet where everything

401
00:24:44,355 --> 00:24:47,995
Speaker 2:  is free and no one's making money. The Hollywood's like eating itself alive.

402
00:24:48,455 --> 00:24:51,995
Speaker 2:  And instead it's like a bunch of burned out 25-year-old influencers. And

403
00:24:51,995 --> 00:24:55,635
Speaker 2:  it just feels like all of this stuff is an attempt to regain

404
00:24:55,775 --> 00:24:59,635
Speaker 2:  the glory years of owning their distribution over here in some way.

405
00:25:00,455 --> 00:25:04,355
Speaker 2:  I'm not sure any of it will work, but you can see exactly what these

406
00:25:04,555 --> 00:25:07,795
Speaker 2:  companies are trying to do. They do not wanna put everything like

407
00:25:08,595 --> 00:25:12,445
Speaker 2:  HBO does not wanna put its content on YouTube. Right. They want you

408
00:25:12,445 --> 00:25:15,525
Speaker 2:  to go somewhere else and say it's worth more money 'cause it's over here.

409
00:25:16,505 --> 00:25:19,845
Speaker 2:  And I, I don't know if that's like a winnable fight, but you can see what

410
00:25:19,845 --> 00:25:20,405
Speaker 2:  they're trying to do.

411
00:25:20,715 --> 00:25:24,565
Speaker 3:  Yeah. And I think the, the thing that's gonna suck for users

412
00:25:25,065 --> 00:25:28,845
Speaker 3:  in this world is you're gonna, you're gonna go

413
00:25:28,915 --> 00:25:32,645
Speaker 3:  into Disney Plus as a Disney plus subscriber, right? And in in, in the world

414
00:25:32,945 --> 00:25:36,245
Speaker 3:  we lived in before you're right. You were just paying for ESPN whether you

415
00:25:36,245 --> 00:25:40,085
Speaker 3:  wanted to or not. You weren't paying what ESPN

416
00:25:40,085 --> 00:25:43,845
Speaker 3:  is actually worth to people who want to watch it because that was being sort

417
00:25:43,845 --> 00:25:47,165
Speaker 3:  of spread across everyone whether you wanted it or not. So the price was

418
00:25:47,165 --> 00:25:50,925
Speaker 3:  relatively lower. Now what's gonna happen is you're gonna get

419
00:25:51,445 --> 00:25:54,725
Speaker 3:  a teeny tiny sliver of ESPN content in your

420
00:25:55,225 --> 00:25:57,765
Speaker 3:  Disney Plus, like the stuff you're talking about. You're not gonna get good

421
00:25:57,765 --> 00:26:00,765
Speaker 3:  guy, bad guy, but you're gonna get, you're gonna get game day, you're gonna

422
00:26:00,765 --> 00:26:03,485
Speaker 3:  get a, you're gonna get the Simpsons football and then you are going to be

423
00:26:03,925 --> 00:26:07,885
Speaker 3:  relentlessly upsold on getting everything else in order to pull it all into

424
00:26:07,885 --> 00:26:11,485
Speaker 3:  this app. And so what they're doing is they're saying not only,

425
00:26:12,075 --> 00:26:15,005
Speaker 3:  like imagine If you had cable, but every single time you went to a channel,

426
00:26:15,145 --> 00:26:18,325
Speaker 3:  the first thing it tried to get you to do was buy more of that channel. Like

427
00:26:18,325 --> 00:26:22,085
Speaker 3:  that's, that's where we're headed. And that's the thing that feels bad. And

428
00:26:22,765 --> 00:26:26,405
Speaker 3:  I, this is clearly the thing Disney is setting itself up to do is

429
00:26:27,395 --> 00:26:31,285
Speaker 3:  just be able to do the upselling all in one place. And to a certain extent

430
00:26:31,285 --> 00:26:33,965
Speaker 3:  that makes sense. And if you're somebody who wants the bundle, which is what

431
00:26:33,965 --> 00:26:37,885
Speaker 3:  Disney is trying hard to sell you, that makes sense. But then we're

432
00:26:37,885 --> 00:26:41,685
Speaker 3:  just back to you're paying for the bundle and you're subsidizing ESPN for

433
00:26:41,685 --> 00:26:44,365
Speaker 3:  all the people who wanna watch it. So I, I'm kind of reaching a point where

434
00:26:44,365 --> 00:26:48,205
Speaker 3:  it's like If you just add up the amount of content available to you for what

435
00:26:48,205 --> 00:26:52,045
Speaker 3:  you used to pay for cable, it feels like the the deal we had for

436
00:26:52,045 --> 00:26:54,165
Speaker 3:  cable is looking better and better over time.

437
00:26:54,625 --> 00:26:57,405
Speaker 2:  And that's re remarkable to say. Yeah,

438
00:26:57,665 --> 00:27:00,805
Speaker 3:  It really is. Like I, it feels crazy, but I honestly am starting to feel

439
00:27:00,805 --> 00:27:01,045
Speaker 3:  that way.

440
00:27:01,545 --> 00:27:04,975
Speaker 2:  And it all of that is up against YouTube,

441
00:27:06,595 --> 00:27:10,535
Speaker 2:  TikTok, whatever, whose economics are a bunch of kids show

442
00:27:10,535 --> 00:27:14,335
Speaker 2:  up and make the work for free and then we pay them

443
00:27:14,395 --> 00:27:18,335
Speaker 2:  not enough money to run sustainable businesses and then we collect the

444
00:27:18,335 --> 00:27:22,135
Speaker 2:  ad dollars in the back of that. And that I, I

445
00:27:22,375 --> 00:27:26,045
Speaker 2:  honestly dunno how much longer that has in it. Like a, I mean there's an

446
00:27:26,165 --> 00:27:28,645
Speaker 2:  infinite supply of kids who wanna be famous who will work for you. YouTube.

447
00:27:28,785 --> 00:27:29,645
Speaker 3:  That's the that's the thing.

448
00:27:29,645 --> 00:27:33,365
Speaker 2:  Yeah. But it feels like some bubble will pop there because the rates

449
00:27:33,365 --> 00:27:36,005
Speaker 2:  aren't high enough. Like more and more people are watching YouTube and TikTok

450
00:27:36,005 --> 00:27:39,365
Speaker 2:  and whatever and advertisers are putting more money into it, but then the

451
00:27:39,365 --> 00:27:43,205
Speaker 2:  number of creators who want a piece of that pie is also going up.

452
00:27:44,055 --> 00:27:47,955
Speaker 2:  And I don't think the amount of money going into the system is keeping up

453
00:27:47,955 --> 00:27:50,515
Speaker 2:  with the amount of people who are taking money outta the system on the creator

454
00:27:50,515 --> 00:27:54,315
Speaker 2:  rates. And so they're all getting paid less money and like some,

455
00:27:54,315 --> 00:27:58,115
Speaker 2:  something will reconcile itself there. But none of that, like Mr.

456
00:27:58,205 --> 00:28:02,035
Speaker 2:  Beast did not make his like labor law

457
00:28:02,065 --> 00:28:05,275
Speaker 2:  violation, squid games. He made that for Netflix.

458
00:28:05,845 --> 00:28:09,615
Speaker 2:  Right. Because Netflix can pay him the rates to do it. And

459
00:28:09,615 --> 00:28:13,015
Speaker 2:  there's just something there that is like, If you wanna make that next turn,

460
00:28:13,555 --> 00:28:16,215
Speaker 2:  you have to go to a place where the distribution is more valuable or like

461
00:28:16,215 --> 00:28:20,015
Speaker 2:  people perceive it, it's worth real money. Not just free stuff on YouTube.

462
00:28:20,355 --> 00:28:22,015
Speaker 2:  But that is still the thing they're all competing with.

463
00:28:22,445 --> 00:28:25,975
Speaker 3:  Yeah, no, IIII think that's right and it, the challenge is it feels like

464
00:28:27,005 --> 00:28:30,775
Speaker 3:  it's not obvious where the people who turn out of that turn to

465
00:28:31,225 --> 00:28:33,975
Speaker 3:  right. It's, it's back to the like where are you gonna go? Question. And

466
00:28:33,975 --> 00:28:36,895
Speaker 3:  I think the Netflixes and Disneys of the world are

467
00:28:37,935 --> 00:28:41,775
Speaker 3:  increasingly content to sort of skim off the top right. And give Mr. Beast

468
00:28:41,775 --> 00:28:45,735
Speaker 3:  more money to go do the shiny things for Netflix. But for everybody else

469
00:28:45,835 --> 00:28:49,585
Speaker 3:  who is just in the grind, there aren't that many

470
00:28:49,635 --> 00:28:51,705
Speaker 3:  moves and the platforms know it. Yeah.

471
00:28:52,365 --> 00:28:56,265
Speaker 2:  By the way, this brings up the last thing on our list here of reasons Cable

472
00:28:56,325 --> 00:28:59,985
Speaker 2:  is back, which is Vol. Walmart bought Vizio. Yes. Which is

473
00:28:59,995 --> 00:29:03,665
Speaker 2:  weird. Vizio, we've covered Vizio a lot. If you're an old school

474
00:29:04,105 --> 00:29:07,785
Speaker 2:  VERGE person, you know that Vizio like burst onto the scene. They're, I don't

475
00:29:07,785 --> 00:29:10,185
Speaker 2:  think most people know this. They're, they're an American company. They're

476
00:29:10,185 --> 00:29:13,985
Speaker 2:  in California. Like I've been to their offices. I met their old CEO

477
00:29:14,015 --> 00:29:17,985
Speaker 2:  like ages ago. Met their old CTO, Matt McCrae, who's now I believe the CEO

478
00:29:17,985 --> 00:29:21,465
Speaker 2:  of Arlo, the video camera company. Huh. But their business is, they make

479
00:29:21,465 --> 00:29:25,225
Speaker 2:  really, really cheap TVs. They put a bunch of ad tracking

480
00:29:25,505 --> 00:29:29,185
Speaker 2:  software on them and they subsidize the cheap TV with the

481
00:29:29,465 --> 00:29:33,305
Speaker 2:  lifetime value of the advertising, which is not great. I I think most

482
00:29:33,305 --> 00:29:36,265
Speaker 2:  people just don't like this. They've gotten in trouble for before. 'cause

483
00:29:36,265 --> 00:29:39,865
Speaker 2:  it's intrusive. They don't surveillance stuff. The TVs are of

484
00:29:40,015 --> 00:29:43,805
Speaker 2:  fine quality. Like Vizio is like, you know, their bread and butter now is,

485
00:29:44,355 --> 00:29:47,845
Speaker 2:  it's the Super Bowl. We're gonna sell you a hundred inch TV for

486
00:29:47,845 --> 00:29:50,805
Speaker 2:  $5. Right, right. And people just buy it 'cause they're all,

487
00:29:50,965 --> 00:29:53,765
Speaker 3:  'cause you're then gonna hang it on your wall for a decade and we'll make

488
00:29:53,805 --> 00:29:54,845
Speaker 3:  a lot of money off your ads.

489
00:29:55,025 --> 00:29:58,365
Speaker 2:  Yep. And and they will just serve ads to you. Yeah. All of this really depends

490
00:29:58,905 --> 00:30:02,845
Speaker 2:  on them owning the interface. Right. Or charging

491
00:30:02,855 --> 00:30:06,845
Speaker 2:  money for you to run software and sell ads on that software. This

492
00:30:06,845 --> 00:30:10,725
Speaker 2:  is Roku's model as well. Roku will give you a Roku box for $15 or no dollars

493
00:30:10,755 --> 00:30:14,565
Speaker 2:  depending. And then If you wanna sell ads inside of Peacock on Roku,

494
00:30:14,685 --> 00:30:17,445
Speaker 2:  Roku takes 30%. It's the app store model. Right. So they're all doing this

495
00:30:17,445 --> 00:30:20,965
Speaker 2:  thing. I think Walmart realizes like connected TV is where a bunch of money

496
00:30:20,965 --> 00:30:24,605
Speaker 2:  is going. That's what, that's what advertising people call running ads on

497
00:30:24,605 --> 00:30:28,125
Speaker 2:  Smart TVs is connected tv. Yeah. And it's like, dude, it's the TVs. We're

498
00:30:28,125 --> 00:30:29,085
Speaker 2:  always connected to

499
00:30:29,085 --> 00:30:29,365
Speaker 3:  Something.

500
00:30:30,645 --> 00:30:31,645
Speaker 2:  I dunno what you're talking about.

501
00:30:31,645 --> 00:30:33,485
Speaker 3:  They're just connected to you now. Yeah.

502
00:30:34,625 --> 00:30:38,205
Speaker 2:  So CTV, which is what the industry calls it, is where a bunch of ad money

503
00:30:38,225 --> 00:30:41,925
Speaker 2:  is going. They're excited about it. And Walmart, if

504
00:30:42,315 --> 00:30:45,405
Speaker 2:  Walmart is where a bunch of transactions get processed at the end,

505
00:30:46,305 --> 00:30:50,095
Speaker 2:  right? You're like, you see a bunch of ads for Procter and Gamble products,

506
00:30:50,225 --> 00:30:53,935
Speaker 2:  where are you gonna buy 'em? Walmart is is almost always the answer

507
00:30:54,075 --> 00:30:57,335
Speaker 2:  for like a bunch of categories. You just end up buying this stuff at Walmart.

508
00:30:57,675 --> 00:31:01,295
Speaker 2:  So Walmart's like, great, now we own the ad network on the TVs and we can

509
00:31:01,295 --> 00:31:04,135
Speaker 2:  do all the data tracking. And you're like, listen to how they talk about

510
00:31:04,135 --> 00:31:07,615
Speaker 2:  it. You're like, guys, this is, this isn't as cool as you think It's,

511
00:31:09,075 --> 00:31:12,775
Speaker 3:  But it is also in a certain way, like a perfectly efficient system. Like

512
00:31:12,775 --> 00:31:16,175
Speaker 3:  the, the argument they're making works,

513
00:31:16,645 --> 00:31:20,475
Speaker 3:  it's that plan is going to work. And they're talking about like they,

514
00:31:20,475 --> 00:31:24,155
Speaker 3:  they're gonna run ads on the TVs in the stores

515
00:31:24,455 --> 00:31:28,155
Speaker 3:  as a revenue stream now that they own visit. Like it's just

516
00:31:28,385 --> 00:31:31,635
Speaker 3:  wild the set of things that suddenly become available when you Walmart and

517
00:31:31,635 --> 00:31:34,835
Speaker 3:  you can just show people ads on their television. It's like

518
00:31:35,455 --> 00:31:38,875
Speaker 3:  Amazon had to build a whole, you know, division of its company

519
00:31:39,415 --> 00:31:42,235
Speaker 3:  out of fire TV to do this. Walmart just walked over and bought Vizio.

520
00:31:42,575 --> 00:31:46,075
Speaker 2:  So Vizios platform Plus, which is its advertising business

521
00:31:46,535 --> 00:31:50,375
Speaker 2:  now accounts for all of the company's gross profit. So that's

522
00:31:50,405 --> 00:31:53,255
Speaker 2:  just one line from the press release. Yeah. How, how does Vizio make money?

523
00:31:53,355 --> 00:31:57,095
Speaker 2:  All of its profit comes from this connected TV advertising business. The

524
00:31:57,095 --> 00:32:00,895
Speaker 2:  quote from Walmart's chief growth officer, Seth Delaer, Vizio has

525
00:32:00,895 --> 00:32:03,175
Speaker 2:  expertly changed their business over time, like building and quickly scaling

526
00:32:03,335 --> 00:32:06,055
Speaker 2:  a profitable advertising business, pairing it with Walmart Connect, which

527
00:32:06,055 --> 00:32:09,095
Speaker 2:  is their advertising business will be impactful and it allow us to invest

528
00:32:09,095 --> 00:32:13,015
Speaker 2:  our business even further on behalf of our customers. And it's like, I don't

529
00:32:13,015 --> 00:32:15,415
Speaker 2:  think your customers want you to invest in advertising on their behalf.

530
00:32:16,055 --> 00:32:18,135
Speaker 3:  I don't think so either. But also it's gonna work.

531
00:32:19,695 --> 00:32:21,495
Speaker 3:  I mean, this is the thing, right? And I think the, the reason

532
00:32:23,055 --> 00:32:26,255
Speaker 3:  I I continue to find companies like Telly so fascinating is they're the ones

533
00:32:26,255 --> 00:32:30,055
Speaker 3:  just saying the quiet part out loud, right? Like, we don't make money

534
00:32:30,075 --> 00:32:33,975
Speaker 3:  on TVs because it's a big ass screen we can put

535
00:32:33,975 --> 00:32:37,175
Speaker 3:  in your living room that you're gonna look at. And it turns out that's actually

536
00:32:37,175 --> 00:32:40,765
Speaker 3:  pretty valuable. And e everybody now is

537
00:32:41,035 --> 00:32:44,405
Speaker 3:  just making their TVs cheaper and cheaper. Telly was the one who's just like,

538
00:32:44,405 --> 00:32:46,885
Speaker 3:  screw this, we're just gonna make it free. Telly also, I don't think has

539
00:32:46,885 --> 00:32:47,805
Speaker 3:  shipped anything yet. So

540
00:32:48,065 --> 00:32:51,685
Speaker 2:  No, people have tellies Do people have the TV under it? Yeah. Oh yeah. People

541
00:32:51,705 --> 00:32:52,085
Speaker 2:  got those.

542
00:32:52,395 --> 00:32:53,845
Speaker 3:  Okay. Where's mine? You

543
00:32:53,845 --> 00:32:56,245
Speaker 2:  Should, I mean, you should get yours. Yeah, but I, I've seen like Reddit

544
00:32:56,245 --> 00:32:59,445
Speaker 2:  Threads of like I have a TV with a TV under it. Okay. And then If you try

545
00:32:59,445 --> 00:33:02,885
Speaker 2:  to like hack the TV under the TV tell like tell a CO will arrest you personally,

546
00:33:02,895 --> 00:33:03,245
Speaker 2:  right?

547
00:33:03,665 --> 00:33:06,005
Speaker 3:  He comes to your house and just holds ads in front of you while you look

548
00:33:06,005 --> 00:33:06,365
Speaker 3:  at the tv.

549
00:33:06,365 --> 00:33:10,245
Speaker 2:  Yeah, I'm just saying this is all the mix, right? Like the, the

550
00:33:10,725 --> 00:33:14,565
Speaker 2:  distribution of all of this content is where the money is. Yeah. It is

551
00:33:14,585 --> 00:33:18,555
Speaker 2:  not the content, like more

552
00:33:18,555 --> 00:33:21,835
Speaker 2:  than anything, video on the internet is now defined by its distribution and

553
00:33:22,115 --> 00:33:24,955
Speaker 2:  everyone is competing against the best business model of all time, which

554
00:33:24,955 --> 00:33:28,835
Speaker 2:  is what if a bunch of 20 year olds make the content for free, right? And

555
00:33:28,835 --> 00:33:32,795
Speaker 2:  then we distribute it with ads around it. It it, its like an unbeatable business

556
00:33:32,795 --> 00:33:36,715
Speaker 2:  model. And so you have Walmart being like, well we

557
00:33:36,715 --> 00:33:40,555
Speaker 2:  wanna sell ads for Walmart stuff. We're gonna buy Vizio and that will quote

558
00:33:40,605 --> 00:33:44,035
Speaker 2:  allow us to serve our customers in new ways to enhance their shopping journeys.

559
00:33:44,175 --> 00:33:47,195
Speaker 2:  And it's like, I dunno what that means, but, but it sounds like you're gonna

560
00:33:47,195 --> 00:33:48,355
Speaker 2:  track the shit outta me.

561
00:33:48,785 --> 00:33:51,635
Speaker 3:  Well, I just, I think a lot about the, the thing Netflix keeps saying where

562
00:33:51,635 --> 00:33:55,595
Speaker 3:  it's, it's not trying to compete with YouTube,

563
00:33:55,625 --> 00:33:59,435
Speaker 3:  it's basically pitting itself against YouTube as like a a higher end

564
00:33:59,925 --> 00:34:03,875
Speaker 3:  thing. It's like, oh, do you want to be next to like, you know, crummy

565
00:34:03,875 --> 00:34:07,395
Speaker 3:  cat videos on YouTube, knock yourself out, but come here and get the high

566
00:34:07,395 --> 00:34:10,955
Speaker 3:  end stuff. And I, the the big

567
00:34:11,315 --> 00:34:14,715
Speaker 3:  question for this entire industry is how compelling that case turns out to

568
00:34:14,715 --> 00:34:18,635
Speaker 3:  be to advertisers. Because what is true is that there are billions

569
00:34:18,635 --> 00:34:22,515
Speaker 3:  and billions and billions of dollars of cable and broadcast TV advertising

570
00:34:22,625 --> 00:34:26,395
Speaker 3:  that are going to become available to somebody else very quickly, much faster

571
00:34:26,425 --> 00:34:30,155
Speaker 3:  than anybody thought. And that money has to go somewhere. And

572
00:34:30,355 --> 00:34:34,195
Speaker 3:  the question is going to be do they wanna plug it into YouTube because that's

573
00:34:34,195 --> 00:34:37,555
Speaker 3:  where the audience is Or do they wanna plug it into the most sort of

574
00:34:37,625 --> 00:34:41,315
Speaker 3:  similarly shiny thing to what this once was? And I don't think anybody

575
00:34:41,405 --> 00:34:44,715
Speaker 3:  knows yet, at least at the like scale we're talking about.

576
00:34:45,215 --> 00:34:48,275
Speaker 3:  But, but that is the, it's the same bet all of these folks are making. Like

577
00:34:48,385 --> 00:34:51,795
Speaker 3:  they want you to subscribe, they want you to, but mostly they want you to

578
00:34:51,795 --> 00:34:54,795
Speaker 3:  look at ads. And the thing they want you to do the most, especially in the

579
00:34:54,795 --> 00:34:58,715
Speaker 3:  next couple of years when all of this money has to move somewhere, is look

580
00:34:58,715 --> 00:35:01,435
Speaker 3:  at ads. And I think

581
00:35:02,815 --> 00:35:06,635
Speaker 3:  you can feel about ads supported streaming however you want. I mostly think

582
00:35:06,635 --> 00:35:10,395
Speaker 3:  it's fine to be totally honest. I I am, I am less annoyed by it

583
00:35:10,485 --> 00:35:14,435
Speaker 3:  every day than I expect that I'm going to be. But that is

584
00:35:14,435 --> 00:35:16,675
Speaker 3:  coming. It just is. That is the thing.

585
00:35:17,495 --> 00:35:20,975
Speaker 2:  I'm just gonna say this one time and then we can move on.

586
00:35:21,475 --> 00:35:25,195
Speaker 2:  It is still way easier to watch an NFL game

587
00:35:25,455 --> 00:35:29,425
Speaker 2:  and what you might call the stormy seas than it's to pay for.

588
00:35:32,325 --> 00:35:36,135
Speaker 2:  Yeah. And that is the real thing that they're all, they're all truly competing

589
00:35:36,135 --> 00:35:39,525
Speaker 2:  against is the user experience of their business models are getting away

590
00:35:39,525 --> 00:35:43,325
Speaker 2:  in the user experience to the point where it is deeply

591
00:35:43,415 --> 00:35:46,365
Speaker 2:  confu. It's like confusing and bad and people are like, whatever, I'll just

592
00:35:46,365 --> 00:35:46,605
Speaker 2:  take it.

593
00:35:46,795 --> 00:35:50,725
Speaker 3:  Well If you remember the thing that happened right after the glory days of,

594
00:35:50,745 --> 00:35:54,725
Speaker 3:  of the late nineties and the early internet was piracy and Hollywood

595
00:35:54,725 --> 00:35:57,165
Speaker 3:  basically just fell all over itself because of it.

596
00:35:58,545 --> 00:35:59,045
Speaker 3:  Who knows?

597
00:36:00,375 --> 00:36:04,325
Speaker 2:  We'll see, it's same by the way, If you have a telly, the TV with the TV

598
00:36:04,325 --> 00:36:08,125
Speaker 2:  under it, or you, you work at telly and you wanna arrest David Pierce, let

599
00:36:08,125 --> 00:36:09,405
Speaker 2:  us know. I'm very, I

600
00:36:09,405 --> 00:36:11,365
Speaker 3:  Look at the ads, NELI don't come from me.

601
00:36:13,405 --> 00:36:16,165
Speaker 2:  I look my phone during the show. The greatest greatest episode of this podcast

602
00:36:16,215 --> 00:36:18,645
Speaker 2:  would be David Pierce gets arrested by the CEO of Tel.

603
00:36:20,675 --> 00:36:23,205
Speaker 2:  Like that we'll send a camera crew out for that. All right, we gotta take

604
00:36:23,205 --> 00:36:26,005
Speaker 2:  break. We're gonna be back with Kylie to talk about what on earth is going

605
00:36:26,005 --> 00:36:28,245
Speaker 2:  on with artificial intelligence. We'll be right back.

606
00:40:37,825 --> 00:40:41,805
Speaker 2:  We are back. Kylie Robinson's here. Hey Kylie. Hello. There's a lot of stuff

607
00:40:41,825 --> 00:40:45,365
Speaker 2:  on your beat. You're our senior AI reporter and that means every week

608
00:40:45,835 --> 00:40:49,685
Speaker 2:  just a number of people say things about our robot overlords.

609
00:40:49,945 --> 00:40:53,045
Speaker 2:  Yes. And this week they just all at the same time

610
00:40:53,715 --> 00:40:57,645
Speaker 2:  decided that AGI was coming. That's what that feels like to me is like we're

611
00:40:57,645 --> 00:41:01,205
Speaker 2:  just gonna, we're just gonna say this is happening and then people can interpret

612
00:41:01,205 --> 00:41:03,445
Speaker 2:  that to mean whatever they want. Do

613
00:41:03,685 --> 00:41:07,005
Speaker 3:  You think there's a group chat like, like I remember in like the early days

614
00:41:07,005 --> 00:41:10,685
Speaker 3:  of the pandemic, there were a bunch of tech CEOs who were like in a WhatsApp

615
00:41:10,925 --> 00:41:14,565
Speaker 3:  together trying to figure out what to do and how to manage things and all

616
00:41:14,565 --> 00:41:18,205
Speaker 3:  that stuff. And I'm like, is there now just like a AGI when

617
00:41:18,845 --> 00:41:22,765
Speaker 3:  WhatsApp chat where where it's just like Sundar and Sam being like, is

618
00:41:22,825 --> 00:41:25,365
Speaker 3:  did we do it? Should we say it now? Is this the one? I

619
00:41:25,365 --> 00:41:27,885
Speaker 2:  Hope So. I mean I kind of, there should be, by the way, the who's in, who's

620
00:41:27,885 --> 00:41:31,325
Speaker 2:  outta that group chat is Oof. That's a, that's a good one. Spicy.

621
00:41:31,855 --> 00:41:34,320
Speaker 3:  Maybe that's the problem. Just let Elon back in. All the

622
00:41:34,320 --> 00:41:36,485
Speaker 2:  Lawsuits go away. You know how the New York Times is subscription product,

623
00:41:36,485 --> 00:41:39,965
Speaker 2:  but mostly for video games. Like we should transition our subscription to

624
00:41:39,965 --> 00:41:42,885
Speaker 2:  just being who's in or who's out of the AI group chat

625
00:41:43,125 --> 00:41:44,605
Speaker 6:  Like a MySpace top 10 for

626
00:41:44,825 --> 00:41:47,685
Speaker 2:  And it's, yeah, we just get to crowdsource that every day and then that happens

627
00:41:47,705 --> 00:41:51,445
Speaker 2:  in real life. Perfect. You know, like today the

628
00:41:51,645 --> 00:41:54,485
Speaker 2:  audience of the Virgin decided Sam's outta the group chat and he like wakes

629
00:41:54,485 --> 00:41:55,685
Speaker 2:  up and he is like, I'm not in the group chat.

630
00:41:56,605 --> 00:41:58,365
Speaker 6:  I feel like we need to ship this immediately. Actually

631
00:41:58,555 --> 00:42:02,365
Speaker 2:  This is very good. That's worth $7 a month. Lemme tell you. Alright,

632
00:42:02,365 --> 00:42:05,485
Speaker 2:  let's start with Sam Altman was on stage at the deal book conference.

633
00:42:06,225 --> 00:42:09,805
Speaker 2:  He announced something that I, he I think we had as a scoop and then he announced

634
00:42:09,965 --> 00:42:12,845
Speaker 2:  it and so we just like ran it. But let's start with this. Open Eyes is gonna

635
00:42:12,845 --> 00:42:16,485
Speaker 2:  do the 12 days of what they're calling ship Miss. Yeah. Get it David.

636
00:42:17,345 --> 00:42:21,305
Speaker 3:  I don't like it, so No, I don't get it. Explain it to me. You tell

637
00:42:21,685 --> 00:42:25,465
Speaker 2:  No. All right. There's a, there's a man who comes into your house.

638
00:42:26,785 --> 00:42:29,985
Speaker 2:  I ostensibly to give you gifts and I ship. Really eat all of your cookies.

639
00:42:30,705 --> 00:42:34,545
Speaker 2:  Okay. And we celebrate him anyway. So Ship Miss, that's gonna

640
00:42:34,545 --> 00:42:38,105
Speaker 2:  include soa, the video generator, a new reasoning model. And then Kylie

641
00:42:38,235 --> 00:42:41,825
Speaker 2:  today they announced that the first thing that they're announcing as part

642
00:42:41,825 --> 00:42:44,265
Speaker 2:  of SHIP Miss is a $200 a month

643
00:42:45,645 --> 00:42:47,185
Speaker 2:  OpenAI plan. What is going on here?

644
00:42:47,655 --> 00:42:51,505
Speaker 6:  Yeah. And you talked about us having the scoop. I made probably a dozen

645
00:42:51,505 --> 00:42:55,185
Speaker 6:  calls the day before and people are like, shit Miss. And I'm like, no,

646
00:42:55,365 --> 00:42:59,105
Speaker 6:  no, like shipping something. I'm sorry. Can we do this quickly? Yeah. So

647
00:42:59,105 --> 00:43:02,865
Speaker 6:  they just released a $200 a month plan for like a new

648
00:43:02,935 --> 00:43:06,585
Speaker 6:  special oh one model. And I got this

649
00:43:07,115 --> 00:43:11,105
Speaker 6:  heads up like, you know, not a lot of time before. So I'm still

650
00:43:11,105 --> 00:43:13,665
Speaker 6:  processing it and my first gut reaction is

651
00:43:14,965 --> 00:43:18,775
Speaker 6:  that feels like such a joke. Like give us $200 and you get something so special,

652
00:43:18,885 --> 00:43:22,775
Speaker 6:  just trust us. So yeah, it's like a special oh one

653
00:43:22,775 --> 00:43:26,415
Speaker 6:  model. It's supposed to be better at coding, better at like, you know,

654
00:43:27,105 --> 00:43:30,535
Speaker 6:  aggressive research if that's what you're into. Yeah.

655
00:43:31,075 --> 00:43:34,015
Speaker 2:  Is that like the, you know, when did the Apple watch Shanny? I was like,

656
00:43:34,015 --> 00:43:37,775
Speaker 2:  this one's made of diamonds $5,000 and it just kind of didn't matter

657
00:43:38,015 --> 00:43:41,335
Speaker 2:  'cause some people just wanted to pay $5,000 for Apple Watch. Is that, is

658
00:43:41,335 --> 00:43:43,615
Speaker 2:  that what's going on here or is it actually more capable or do we just not

659
00:43:43,615 --> 00:43:43,815
Speaker 2:  know

660
00:43:44,205 --> 00:43:47,935
Speaker 6:  They have internal testing that they released? I am always skeptical of

661
00:43:48,175 --> 00:43:51,495
Speaker 6:  internal testing, whether that's fair or not. It's just like my natural response

662
00:43:51,555 --> 00:43:55,295
Speaker 6:  is like, I would like a lot more testing and proof and of course I wanna

663
00:43:55,295 --> 00:43:58,215
Speaker 6:  try it myself and see how I feel. And I wanna hear from engineers who use

664
00:43:58,215 --> 00:44:01,095
Speaker 6:  it for coding. They're like, oh, I do actually feel a difference. So until

665
00:44:01,095 --> 00:44:04,935
Speaker 6:  I see some of these markers, I'm not entirely sure myself, but they say that

666
00:44:04,935 --> 00:44:08,695
Speaker 6:  yes it is better. But being an SF and being an AI

667
00:44:08,855 --> 00:44:11,855
Speaker 6:  reporter, I'm imagining all the dudes in Hayes Valley who are like, oh my

668
00:44:11,855 --> 00:44:15,455
Speaker 6:  God, I'm gonna pay $200 a month and would be so cool. Like yes, it is like

669
00:44:15,455 --> 00:44:19,215
Speaker 6:  that Diamond Apple watch. It's just to have it, I would just recently

670
00:44:19,275 --> 00:44:23,215
Speaker 6:  did an interview with the head of chat, GBT, Nick Hurley and he

671
00:44:23,215 --> 00:44:27,175
Speaker 6:  was telling me that some people have multiple like paid accounts because

672
00:44:27,365 --> 00:44:30,735
Speaker 6:  they like, they, it, it's, it's interesting. So some people, I don't know,

673
00:44:30,735 --> 00:44:33,045
Speaker 6:  they're really into it, I guess paying more money.

674
00:44:33,195 --> 00:44:37,165
Speaker 3:  There's such a like FOMO, fake arms race thing

675
00:44:37,345 --> 00:44:41,005
Speaker 3:  to all of this that I think this is just part of, right? It's like everybody's

676
00:44:41,005 --> 00:44:44,565
Speaker 3:  out here talking about the number of Nvidia chips that they're buying as

677
00:44:44,565 --> 00:44:48,525
Speaker 3:  like a badge of honor for how cool your startup is. And like, we're

678
00:44:48,525 --> 00:44:52,285
Speaker 3:  gonna get to the point now where come work at our startup and we'll give

679
00:44:52,285 --> 00:44:56,245
Speaker 3:  you chat GPT Pro for $200 a month to do work instead

680
00:44:56,245 --> 00:44:58,445
Speaker 3:  of going to that other startup that only gives you

681
00:45:00,175 --> 00:45:03,425
Speaker 3:  plus, I guess what it's called, by the way, OpenAI, remember that moment

682
00:45:03,425 --> 00:45:05,425
Speaker 3:  where you were like, we're gonna get better at branding and our names are

683
00:45:05,425 --> 00:45:09,305
Speaker 3:  gonna make more sense. Whoops. Whoops, you lost that real

684
00:45:09,305 --> 00:45:13,145
Speaker 3:  fast. But anyway, like I, it really does, again, I think it's,

685
00:45:13,175 --> 00:45:16,825
Speaker 3:  it's possible that this will be like meaningfully better, but it really feels

686
00:45:16,825 --> 00:45:20,745
Speaker 3:  to me like OpenAI is just like, we can,

687
00:45:20,805 --> 00:45:24,385
Speaker 3:  we can do this thing where everybody is looking for any kind of tiny edge

688
00:45:24,385 --> 00:45:26,385
Speaker 3:  and there are a lot of people with a lot of money who will pay for that edge.

689
00:45:26,415 --> 00:45:30,345
Speaker 3:  Yeah. Let's just see what happens. Like the the, the price,

690
00:45:30,885 --> 00:45:34,665
Speaker 3:  what's the, what's the word for when you try to figure out how much

691
00:45:35,005 --> 00:45:36,585
Speaker 3:  people are willing to pay for something

692
00:45:36,915 --> 00:45:37,585
Speaker 2:  Elasticity?

693
00:45:37,845 --> 00:45:41,785
Speaker 3:  The Yes, the price elasticity test is very much on in

694
00:45:41,785 --> 00:45:42,545
Speaker 3:  the AI world right now.

695
00:45:42,935 --> 00:45:46,785
Speaker 6:  Yeah. And I think the New York Times had reported they wanted to, you know,

696
00:45:46,855 --> 00:45:50,825
Speaker 6:  make tens of billions of dollars in revenue and like now we

697
00:45:50,825 --> 00:45:53,225
Speaker 6:  kind of see what their plan was. They're like $200.

698
00:45:53,575 --> 00:45:57,385
Speaker 2:  It's Apple Watch additions. Exactly. Carl Lagerfeld is gonna buy this

699
00:45:57,385 --> 00:46:01,145
Speaker 2:  model and never set it up and he is just gonna wear around his unset up

700
00:46:01,145 --> 00:46:04,665
Speaker 2:  Apple watch as a status symbol for a while and then move on.

701
00:46:05,725 --> 00:46:08,985
Speaker 2:  That's really interesting. Like, I just wanna put that, I wanted to start

702
00:46:08,985 --> 00:46:12,625
Speaker 2:  there, right? They're gonna ship a bunch of new products over the next 10

703
00:46:12,625 --> 00:46:16,385
Speaker 2:  days, 12 days. The first one is this like outrageously expensive

704
00:46:16,795 --> 00:46:20,385
Speaker 2:  model and we have to see how well it works. That is in the context of, hey,

705
00:46:20,465 --> 00:46:24,025
Speaker 2:  a bunch of people are saying the models aren't scaling in capability the

706
00:46:24,025 --> 00:46:27,225
Speaker 2:  way they were last year, right? Yeah. It's been two years since chat Vidi

707
00:46:27,225 --> 00:46:30,185
Speaker 2:  came out. They did 3.5 and four Oh

708
00:46:31,165 --> 00:46:35,065
Speaker 2:  and now oh one. Well these names are all horrible and like they're plateauing,

709
00:46:35,065 --> 00:46:37,465
Speaker 2:  right? And there's a report that Gemini is sort of plateauing and actually

710
00:46:37,465 --> 00:46:40,265
Speaker 2:  they need, Gemini is not as good as the old Gemini. Like there's a sense

711
00:46:40,265 --> 00:46:44,145
Speaker 2:  that, okay, like a lot of hype happened because of the first big

712
00:46:44,175 --> 00:46:48,065
Speaker 2:  step up in capability, but it's not just like a linear increase

713
00:46:48,065 --> 00:46:51,905
Speaker 2:  in capability over time. And it feels like open eyes just kinda like running

714
00:46:51,905 --> 00:46:54,745
Speaker 2:  at that and being like, no, this one's so good, it's worth 10 x more money.

715
00:46:55,485 --> 00:46:57,105
Speaker 2:  And I, I can't quite square those things,

716
00:46:57,635 --> 00:47:01,545
Speaker 6:  Right? I mean that's how I read the deal book story that

717
00:47:01,755 --> 00:47:05,745
Speaker 6:  Heath did about, you know, AGI is actually not gonna be that big of a

718
00:47:05,745 --> 00:47:09,545
Speaker 6:  deal. I just feel like they have drummed up so much hype, like

719
00:47:09,925 --> 00:47:13,185
Speaker 6:  an unnecessary amount of hype and now they all have to reckon with that.

720
00:47:14,205 --> 00:47:17,905
Speaker 6:  So like that's, that's the state is it plateauing a lot of

721
00:47:18,445 --> 00:47:21,665
Speaker 6:  AI leaders right now. Like their common thing to say is like we've just gotten

722
00:47:21,665 --> 00:47:25,625
Speaker 6:  used to it and now like companies can't keep up with like the hype and like

723
00:47:25,625 --> 00:47:28,425
Speaker 6:  the progress they've had in the past, but scaling laws have brought us to

724
00:47:28,425 --> 00:47:31,845
Speaker 6:  this point and they should take us more. You know, take us further.

725
00:47:32,385 --> 00:47:36,245
Speaker 6:  But that remains to be seen I think. Yeah, they are experiencing some

726
00:47:36,245 --> 00:47:39,725
Speaker 6:  plateauing. That's why we're seeing some incremental developments. And why

727
00:47:39,725 --> 00:47:42,645
Speaker 6:  we're not getting like GPT five right now as much as they probably wanna

728
00:47:42,645 --> 00:47:44,725
Speaker 6:  release it right now. It's clearly not ready.

729
00:47:45,405 --> 00:47:48,525
Speaker 2:  I just wanna read you that quote that Heath wrote up. So Sam Altman is on

730
00:47:48,525 --> 00:47:52,045
Speaker 2:  stage at the deal book conference this week with Andrew Erkin and he says,

731
00:47:52,585 --> 00:47:56,165
Speaker 2:  my guess is that we will hit AGI sooner than most people in the world think,

732
00:47:56,345 --> 00:48:00,285
Speaker 2:  and it will matter much less. Which is an incredible quote,

733
00:48:00,695 --> 00:48:04,445
Speaker 2:  right? Because he, he has said on like, Reddit, we're gonna hit

734
00:48:04,745 --> 00:48:08,685
Speaker 2:  AGI on current hardware. Yeah. Which is a remarkable claim

735
00:48:09,075 --> 00:48:12,125
Speaker 2:  because if that's what you think we should just, again, I think we should

736
00:48:12,125 --> 00:48:15,845
Speaker 2:  just stop all other work and get to that. But then he is like, and also it

737
00:48:15,845 --> 00:48:19,715
Speaker 2:  will suck. Like, we're gonna make this, we're gonna, we're gonna

738
00:48:19,715 --> 00:48:23,315
Speaker 2:  make AGI, artificial general intelligence can do any task as well as a person

739
00:48:24,345 --> 00:48:27,205
Speaker 2:  and it will not be as good as you think it is. And it's like, so you're gonna

740
00:48:27,205 --> 00:48:31,095
Speaker 2:  make a dumb person. Is that, what are you saying is

741
00:48:31,095 --> 00:48:32,375
Speaker 2:  going to happen here? They

742
00:48:32,375 --> 00:48:34,535
Speaker 6:  Said human intelligence, not, you know,

743
00:48:34,865 --> 00:48:35,815
Speaker 2:  Super intelligence.

744
00:48:36,245 --> 00:48:39,295
Speaker 6:  Sure, fair. But you know, they, they throw around human intelligence a lot.

745
00:48:39,315 --> 00:48:42,975
Speaker 6:  You know, we're getting there. It's right now they're saying like a smart

746
00:48:43,175 --> 00:48:46,935
Speaker 6:  college student, right? I don't know. I feel like they have just

747
00:48:46,935 --> 00:48:50,325
Speaker 6:  like, they've screwed themselves on this hype and that's why, that's what

748
00:48:50,325 --> 00:48:53,445
Speaker 6:  I read when I, when I saw that deal book line, I'm like, well, you know,

749
00:48:53,905 --> 00:48:57,285
Speaker 6:  you've got a 10% expectations now before you say that, the next thing is

750
00:48:57,785 --> 00:49:00,765
Speaker 6:  AGI. And everyone's like, this is AGI. You know, it feels like he's like,

751
00:49:01,075 --> 00:49:03,245
Speaker 6:  well it's actually not gonna be that big.

752
00:49:03,385 --> 00:49:06,805
Speaker 2:  And then some bigger context there is, there's reporting that

753
00:49:07,515 --> 00:49:11,045
Speaker 2:  opening I can get out of its deal with Microsoft if it

754
00:49:11,875 --> 00:49:15,205
Speaker 2:  decides all by itself that it's achieved AGI, which I think was related to

755
00:49:15,225 --> 00:49:19,205
Speaker 2:  its like old structure. Yeah. Where it was like a nonprofit board and all

756
00:49:19,205 --> 00:49:22,965
Speaker 2:  that board was there to do was D decide whether to achieve AGI

757
00:49:22,965 --> 00:49:26,685
Speaker 2:  safely and or fire. So Sam Altman and they did one and not the other.

758
00:49:26,785 --> 00:49:29,925
Speaker 2:  And Sam Altman got rid of them all and now they're gonna turn into a for-profit

759
00:49:29,925 --> 00:49:33,405
Speaker 2:  company. So Microsoft is saddled with this weird clause in their deal.

760
00:49:33,675 --> 00:49:37,645
Speaker 2:  Yeah. Where if Sam Altman decides it's AGI, he gets to leave the

761
00:49:37,845 --> 00:49:41,485
Speaker 2:  Microsoft deal, which is weird, right? That that's just a weird set of incentives.

762
00:49:41,925 --> 00:49:45,885
Speaker 2:  I will, by the way, I'll preview that on Monday. Decoder

763
00:49:45,885 --> 00:49:49,645
Speaker 2:  with Mustafa Soleman, who is the ex co-founder. Well he is,

764
00:49:49,645 --> 00:49:52,885
Speaker 2:  he's not the ex co-founder who co-founded DeepMind, which is now Google DeepMind.

765
00:49:52,945 --> 00:49:56,525
Speaker 2:  And he left and started inflection and then like weird reverse and we hired

766
00:49:56,545 --> 00:50:00,285
Speaker 2:  his way and now is the CEO of Microsoft ai. And I asked him, the first

767
00:50:00,565 --> 00:50:04,525
Speaker 2:  question I asked him was, are we gonna hit AGI in current hardware? And he

768
00:50:04,585 --> 00:50:08,525
Speaker 2:  did not agree. Yeah. Hmm. I repeat the whole episode, but he was

769
00:50:08,525 --> 00:50:12,165
Speaker 2:  kinda like, yeah, no, I mean maybe someday what he was like, what do you

770
00:50:12,165 --> 00:50:14,845
Speaker 2:  think current hardware means? Which is an incredible response to that question.

771
00:50:14,985 --> 00:50:18,725
Speaker 2:  And then he was like, what do you think AGI means? And I was like, I

772
00:50:19,525 --> 00:50:23,085
Speaker 2:  I am the interviewer. You tell me good questions for you.

773
00:50:23,305 --> 00:50:27,285
Speaker 2:  It was a good decoder. He's fun to talk to. We like each other. We like challenging

774
00:50:27,285 --> 00:50:30,085
Speaker 2:  each other. That's all fine. But like, I don't think he agreed with Sam,

775
00:50:30,105 --> 00:50:34,045
Speaker 2:  but I do think he was also moving the goalpost for

776
00:50:34,305 --> 00:50:37,605
Speaker 2:  AGI. He very clearly, I was like, if we're gonna build this the

777
00:50:38,005 --> 00:50:41,085
Speaker 2:  singularity, he was like, shouldn't we just stop and work on that? And he's

778
00:50:41,085 --> 00:50:44,645
Speaker 2:  like, AGI is not the singularity. Yeah. Which is not the hype that we've

779
00:50:44,645 --> 00:50:48,285
Speaker 2:  all been experiencing for a minute. Right. The hype was we have to stop this

780
00:50:48,285 --> 00:50:51,605
Speaker 2:  because if we make it before we're ready, we'll destroy the world. That's,

781
00:50:51,805 --> 00:50:55,525
Speaker 2:  remember that's Elon Musk. That is why all the people are leaving OpenAI

782
00:50:55,545 --> 00:50:58,805
Speaker 2:  to start literally this company's called safer superD intelligence.

783
00:50:59,075 --> 00:51:02,965
Speaker 3:  Yeah. I, Ilya s giver, who was a big part of this

784
00:51:02,965 --> 00:51:05,685
Speaker 3:  whole process in June of this year,

785
00:51:06,645 --> 00:51:10,495
Speaker 3:  described AGI in the context of the OpenAI charter, which

786
00:51:10,495 --> 00:51:14,415
Speaker 3:  presumably Sam Altman was part of and would agree with as a computer

787
00:51:14,415 --> 00:51:18,015
Speaker 3:  system which can automate the great majority of intellectual labor. He called

788
00:51:18,015 --> 00:51:21,735
Speaker 3:  that one useful definition. It is. So we cannot let

789
00:51:21,835 --> 00:51:25,535
Speaker 3:  Sam Altman off the hook for doing this. We just can't. This is ridiculous.

790
00:51:25,565 --> 00:51:29,335
Speaker 3:  Yeah. Like my man has spent a decade, he made AGI a thing

791
00:51:29,365 --> 00:51:33,295
Speaker 3:  like Yep. The fact that we all use the term AGI is Sam Altman's fault.

792
00:51:33,715 --> 00:51:37,575
Speaker 3:  And to now just come and, and try to just sort of offhandedly sit with

793
00:51:37,575 --> 00:51:40,815
Speaker 3:  Andrew Russ, we're gonna be like, ah, it's not that big a deal Is is

794
00:51:41,415 --> 00:51:45,375
Speaker 3:  bullshit. Yeah. And we, we should not let him do this. Totally. It

795
00:51:45,375 --> 00:51:48,175
Speaker 3:  it is, it's a bunch of like political maneuvering because it's gonna make

796
00:51:48,175 --> 00:51:52,095
Speaker 3:  his life easier to just declare it AGI and move on. You're right. That it'll

797
00:51:52,095 --> 00:51:55,775
Speaker 3:  help with the hype because there, I think it's very clear that whatever this

798
00:51:55,775 --> 00:51:59,655
Speaker 3:  moment where AI wins is not going to happen soon if it

799
00:51:59,655 --> 00:52:02,495
Speaker 3:  happens ever. Yeah. It also lets them get outta the Microsoft deal. So like

800
00:52:02,575 --> 00:52:06,535
Speaker 3:  I can see why you do this if you're Sam, we just all have to keep saying

801
00:52:06,535 --> 00:52:09,815
Speaker 3:  out loud that this is what he's doing and it's bullshit. Yeah,

802
00:52:09,815 --> 00:52:10,815
Speaker 6:  Totally. I

803
00:52:10,815 --> 00:52:14,135
Speaker 2:  Agree. He also, Andrew Erkin asked him about

804
00:52:15,235 --> 00:52:18,575
Speaker 2:  OpenAI and Microsoft Disentangling. He actually quoted Alex Heath, which

805
00:52:18,645 --> 00:52:22,255
Speaker 2:  yeah, Heath was very excited about this indeed. Immediate text from Alex

806
00:52:22,285 --> 00:52:26,175
Speaker 2:  when this happened. Live in the room. And Sam said, I don't think we're

807
00:52:26,175 --> 00:52:29,755
Speaker 2:  disentangling from Microsoft. Which again, I asked

808
00:52:30,055 --> 00:52:32,275
Speaker 2:  SFA the same thing. And he is like, we'll see how it goes. And he said that

809
00:52:32,275 --> 00:52:34,395
Speaker 2:  three times until I said, you said, we'll see how it goes three times now.

810
00:52:34,615 --> 00:52:35,875
Speaker 2:  And he is like, oh, it's fine. So

811
00:52:35,975 --> 00:52:39,355
Speaker 3:  His real, like Chris Martin and Gwyneth Paltrow being like, we didn't break

812
00:52:39,355 --> 00:52:42,475
Speaker 3:  up, we consciously uncoupled. Like that's, that's how this will go. Yeah.

813
00:52:42,475 --> 00:52:43,275
Speaker 3:  With Microsoft. And

814
00:52:43,415 --> 00:52:47,075
Speaker 6:  Can I add my own tinfoil hat that today, because I've been so wrapped up

815
00:52:47,075 --> 00:52:50,915
Speaker 6:  in OpenAI News, they obviously had this huge launch and a live stream and

816
00:52:50,915 --> 00:52:54,395
Speaker 6:  I have all of these CEOs notifications turned on on Twitter of course.

817
00:52:55,035 --> 00:52:58,995
Speaker 6:  And, and So I get like my own little feed of them and Satya like

818
00:52:59,235 --> 00:53:02,995
Speaker 6:  reposted a bunch of Microsoft announcements that that came today

819
00:53:03,095 --> 00:53:06,675
Speaker 6:  as well. That I think Tom Warren, he scooped some of those

820
00:53:07,135 --> 00:53:10,475
Speaker 6:  but, and like no repost from Sam. No. Like this is my

821
00:53:11,155 --> 00:53:14,715
Speaker 6:  favorite boy Yeah. Succession style. Like, it's like just, I was like, that's,

822
00:53:14,715 --> 00:53:18,115
Speaker 6:  that's really weird timing that they're doing this in Satya's. Like it seems

823
00:53:18,115 --> 00:53:20,075
Speaker 6:  like he's ignoring the whole AI

824
00:53:20,075 --> 00:53:22,635
Speaker 2:  Lunch outta of a group chat. Man, $7 a month you can kick Sam Altman outta

825
00:53:22,635 --> 00:53:23,075
Speaker 2:  the group chat.

826
00:53:23,885 --> 00:53:26,885
Speaker 3:  These things are not accidents. Like that is definitely a tinfoil hat theory.

827
00:53:26,905 --> 00:53:30,685
Speaker 3:  But like when you're Satya Nadella, you do not do that stuff Right. Without

828
00:53:30,685 --> 00:53:31,605
Speaker 3:  thought behind it.

829
00:53:31,605 --> 00:53:32,565
Speaker 2:  Yeah. There. Exactly.

830
00:53:32,565 --> 00:53:36,085
Speaker 3:  The only person who tweets without thinking is Elon Musk. Most other

831
00:53:36,475 --> 00:53:39,205
Speaker 3:  tech CEOs are doing stuff on purpose. Right.

832
00:53:40,945 --> 00:53:44,485
Speaker 2:  That's actually a very good point. Okay. So this is like,

833
00:53:44,735 --> 00:53:48,605
Speaker 2:  we're like laying up the context, right. OpenAI is gonna announce 12 days

834
00:53:48,605 --> 00:53:51,205
Speaker 2:  worth of stuff, including a model that they think is so good that's worth

835
00:53:51,205 --> 00:53:54,725
Speaker 2:  $200 a year. Sam Altman is moving the AGI

836
00:53:54,755 --> 00:53:58,685
Speaker 2:  goalposts to like tomorrow. Right. He is like, I

837
00:53:58,685 --> 00:54:02,405
Speaker 2:  can do this, I can do this on a Nvidia 40 60 for 20 bucks.

838
00:54:02,405 --> 00:54:06,365
Speaker 2:  Like, doesn't matter. Like he's like, you give me an Xbox

839
00:54:06,365 --> 00:54:10,285
Speaker 2:  enough gasoline, I'll give you AGI. Like that's where his head's at. And

840
00:54:10,285 --> 00:54:13,965
Speaker 2:  then the reality of the products right now is that they're nowhere

841
00:54:14,115 --> 00:54:17,965
Speaker 2:  near good enough. Yeah. So the Columbia Tao Center for

842
00:54:17,965 --> 00:54:21,845
Speaker 2:  Digital Journalism released a report this week and

843
00:54:21,845 --> 00:54:25,445
Speaker 2:  it basically asked Chacha Pi to identify the source of 200 quotes from 20

844
00:54:25,445 --> 00:54:29,325
Speaker 2:  publications. And it just couldn't do it. Like it just

845
00:54:29,325 --> 00:54:33,205
Speaker 2:  hallucinated its way through that it produced partially or entirely

846
00:54:33,205 --> 00:54:37,165
Speaker 2:  incorrect responses on 153 occasions. And it only acknowledged

847
00:54:37,165 --> 00:54:40,885
Speaker 2:  inability to accurately respond and inability to accurately respond

848
00:54:40,885 --> 00:54:41,405
Speaker 2:  seven times

849
00:54:42,495 --> 00:54:46,355
Speaker 3:  It it asked it just, just, just to say it again, it asked it

850
00:54:46,455 --> 00:54:50,195
Speaker 3:  200 questions and it got 153 of them wrong. Yeah.

851
00:54:50,655 --> 00:54:53,515
Speaker 2:  And it only admitted that it might be wrong seven times. Right.

852
00:54:53,515 --> 00:54:56,315
Speaker 3:  Didn't admit that it was wrong. Admitted like the possibility that potentially

853
00:54:56,315 --> 00:54:58,875
Speaker 3:  maybe it could be seven times.

854
00:55:00,135 --> 00:55:04,035
Speaker 2:  And then in the culture people are using these tools without

855
00:55:04,075 --> 00:55:05,515
Speaker 2:  a second thought. So Liz Lipato,

856
00:55:07,025 --> 00:55:09,755
Speaker 2:  this is just a Liz banger, like I think she turned this around in five minutes.

857
00:55:10,195 --> 00:55:13,315
Speaker 2:  I think there's all just poured out of her head in one stream of consciousness.

858
00:55:13,475 --> 00:55:16,115
Speaker 2:  'cause I didn't know she was writing this story. It just went up today. But

859
00:55:16,115 --> 00:55:19,915
Speaker 2:  it's very good because I get to say the following phrase on CNN

860
00:55:21,275 --> 00:55:24,835
Speaker 2:  Anna Navarro Cardenas cited a list of

861
00:55:24,835 --> 00:55:28,235
Speaker 2:  presidential family members who have been pardoned by presidents. And she

862
00:55:28,235 --> 00:55:31,755
Speaker 2:  said, president Woodrow Wilson pardoned his brother-in-law Hunter

863
00:55:31,975 --> 00:55:35,315
Speaker 2:  debuts. Which I

864
00:55:35,785 --> 00:55:39,435
Speaker 2:  like. It's like, I know the internet made the robots say hundred deButts.

865
00:55:39,435 --> 00:55:42,595
Speaker 2:  Like right, If you say there's a hundred deButts, I'm like, that's not real.

866
00:55:43,185 --> 00:55:46,595
Speaker 2:  Like when the internet produces the words hundred deButts, I'm like, I know

867
00:55:46,595 --> 00:55:50,395
Speaker 2:  what happened here a hundred of a hundred times. Right? Like there's a Reddit

868
00:55:50,395 --> 00:55:54,275
Speaker 2:  Threads, it's like hundred debuts, there's full, there's like

869
00:55:54,385 --> 00:55:57,035
Speaker 2:  lore, you know, it's like all that happened. Oh yeah. And the robots are

870
00:55:57,235 --> 00:56:01,075
Speaker 2:  confused. So Liz went through this exhaustively to figure out where

871
00:56:01,075 --> 00:56:04,915
Speaker 2:  this came from and she found out that like Chepe basically doesn't,

872
00:56:04,925 --> 00:56:07,755
Speaker 2:  can't answer the question, which presidents have pardoned their family members.

873
00:56:07,785 --> 00:56:11,475
Speaker 2:  Like you just read the piece and Never Cardena is

874
00:56:11,505 --> 00:56:15,195
Speaker 2:  says take it up with chat GPTI said this thing wrong thing on cable

875
00:56:15,225 --> 00:56:18,955
Speaker 2:  news about the words hunter debuts. Wow. And she's like,

876
00:56:19,505 --> 00:56:23,365
Speaker 2:  take it up with chat GPT. And it's like, that's not correct first

877
00:56:23,365 --> 00:56:26,325
Speaker 2:  of all. No. So she tweeted it and then she got it wrong and then she just

878
00:56:26,325 --> 00:56:29,245
Speaker 2:  take up a chat. CBT. I'm just saying that is

879
00:56:29,975 --> 00:56:33,485
Speaker 2:  crazy. Like we are racing towards

880
00:56:33,485 --> 00:56:37,365
Speaker 2:  replacing all of our search engines with these tools, answer

881
00:56:37,365 --> 00:56:40,365
Speaker 2:  engines, which is what Google has long wanted to think of it Google as

882
00:56:41,025 --> 00:56:44,005
Speaker 2:  Google as a search engine. The 10 blue links was like, here are sources.

883
00:56:44,905 --> 00:56:47,605
Speaker 2:  Go read these sources, you can trust them or not. And now they're, now it's,

884
00:56:47,605 --> 00:56:51,045
Speaker 2:  here's an answer and their out is, here are some

885
00:56:51,815 --> 00:56:54,925
Speaker 2:  links to footnotes. And it's like no one's ever gonna click the links. They're

886
00:56:54,925 --> 00:56:57,005
Speaker 2:  just gonna, they're just gonna tweet the words hunter to butts. I I

887
00:56:57,005 --> 00:56:59,085
Speaker 6:  Click the links. I have to defend myself because, but

888
00:56:59,185 --> 00:57:00,405
Speaker 2:  You're a professional journalist.

889
00:57:01,165 --> 00:57:05,085
Speaker 6:  I went on The Verge on our YouTube and I said, I really like

890
00:57:06,225 --> 00:57:10,045
Speaker 6:  AI search because I think it helps me get my answers quickly, but I'm not,

891
00:57:10,185 --> 00:57:12,685
Speaker 6:  I'm not paying, I don't think I'm even really paying close attention to what

892
00:57:12,685 --> 00:57:16,285
Speaker 6:  it's saying. I'm clicking the relevant links because Google is so bad at

893
00:57:16,285 --> 00:57:20,245
Speaker 6:  just telling me who said what, who reported what, and then I can just

894
00:57:20,245 --> 00:57:23,205
Speaker 6:  go and look. I'm, I guess I'm never really paying attention to the answers.

895
00:57:23,425 --> 00:57:27,005
Speaker 6:  And that's, and that's important with what we saw with that report

896
00:57:27,525 --> 00:57:31,245
Speaker 6:  I had, again, I had just talked to the head of chat GBT and I am

897
00:57:31,395 --> 00:57:34,965
Speaker 6:  writing about this, this story of how they started and how they started.

898
00:57:35,275 --> 00:57:39,005
Speaker 6:  They were like, no one's gonna like this product because it is wrong and

899
00:57:39,005 --> 00:57:41,885
Speaker 6:  so confidently wrong. They're like, we shouldn't release this. No one's gonna

900
00:57:41,885 --> 00:57:44,685
Speaker 6:  like this. They're gonna hate it actually 'cause it hallucinates so often.

901
00:57:44,985 --> 00:57:48,725
Speaker 6:  And they decided to go for it and re release it as a research preview

902
00:57:48,865 --> 00:57:52,565
Speaker 6:  to see how people would use it. And people really loved it. And two years

903
00:57:52,565 --> 00:57:56,125
Speaker 6:  later it is still confidently wrong and it is making

904
00:57:56,165 --> 00:57:59,245
Speaker 6:  billions of dollars in revenue. Obviously it's not profitable, but still

905
00:57:59,275 --> 00:58:02,965
Speaker 6:  like all of that besides the point, it still hallucinates like crazy and

906
00:58:02,965 --> 00:58:06,085
Speaker 6:  people still love it. And I can't really figure out why that's the case.

907
00:58:06,265 --> 00:58:09,085
Speaker 2:  If you'll recall a year ago everyone was like, we'll solve hallucinations,

908
00:58:09,295 --> 00:58:09,645
Speaker 2:  right?

909
00:58:09,785 --> 00:58:11,605
Speaker 3:  Oh yeah. It was just waved away. Yeah.

910
00:58:11,835 --> 00:58:14,325
Speaker 2:  Like we're we got this, we got this under control, we know what to do. And

911
00:58:14,325 --> 00:58:16,685
Speaker 2:  it's like, no, you super don't have this under control. Yeah. Because the

912
00:58:16,685 --> 00:58:19,525
Speaker 2:  words Hunter deButts have now appeared on the internet in its totally sincere

913
00:58:19,525 --> 00:58:23,405
Speaker 2:  way, which is, you know, like if anybody had any

914
00:58:23,405 --> 00:58:26,925
Speaker 2:  sense, they'd be like, okay, we should hit pause before we declare that this

915
00:58:26,925 --> 00:58:30,665
Speaker 2:  thing is AGI. Right. And I think this to me is just a fundamental

916
00:58:31,545 --> 00:58:35,105
Speaker 2:  question of all of these tools. Can this technology actually do the things

917
00:58:35,415 --> 00:58:38,785
Speaker 2:  that the people say it can do? And right now it can't even

918
00:58:39,495 --> 00:58:43,465
Speaker 2:  like deduce what is true and what is fake and what is just a

919
00:58:43,465 --> 00:58:45,665
Speaker 2:  pure hallucination. Right.

920
00:58:45,665 --> 00:58:49,025
Speaker 3:  Right. Well, and I think this is why the search piece of this is so interesting

921
00:58:49,025 --> 00:58:52,985
Speaker 3:  to me because If you just take chat GBT as

922
00:58:52,985 --> 00:58:56,825
Speaker 3:  a whole, right? And you say, okay, the idea that it gives you the incorrect

923
00:58:56,825 --> 00:59:00,785
Speaker 3:  answer to this one specific question is a bummer. But

924
00:59:00,785 --> 00:59:04,585
Speaker 3:  it is so sort of ancillary to what most people are using chat GBT

925
00:59:04,585 --> 00:59:08,405
Speaker 3:  for fine. I don't necessarily buy that argument, but it is

926
00:59:08,425 --> 00:59:10,885
Speaker 3:  an argument, right? That it's like, and this is what we hear with a lot of

927
00:59:10,945 --> 00:59:14,645
Speaker 3:  AI stuff. You, you people do attest with it and they're like, well, real

928
00:59:14,645 --> 00:59:17,645
Speaker 3:  users aren't doing that, so we're not actually that concerned with it. Feel

929
00:59:17,675 --> 00:59:20,565
Speaker 3:  that about that however you want. But that is, that is a thing you can say

930
00:59:21,035 --> 00:59:24,725
Speaker 3:  when it comes to search, the whole damn job is to find

931
00:59:25,565 --> 00:59:29,285
Speaker 3:  relevant and true information right on the internet. And so, so the fact

932
00:59:29,285 --> 00:59:33,085
Speaker 3:  that they've pivoted from like this is a good tool for writing code,

933
00:59:33,085 --> 00:59:35,605
Speaker 3:  which is, which is a real thing that is true and it makes mistakes, but it

934
00:59:35,605 --> 00:59:39,445
Speaker 3:  also writes good fast code. And that, that seems to be a

935
00:59:39,445 --> 00:59:43,005
Speaker 3:  trade worth making for a lot of people who write code for a living when the

936
00:59:43,005 --> 00:59:46,925
Speaker 3:  whole job is correct information retrieval, and you are presenting it

937
00:59:46,925 --> 00:59:50,445
Speaker 3:  as I have answered your question about the web.

938
00:59:51,075 --> 00:59:54,845
Speaker 3:  Then the, the stakes are different. And the idea that like this, this thing

939
00:59:54,845 --> 00:59:58,565
Speaker 3:  sometimes makes mistakes, I think just no longer becomes

940
00:59:58,565 --> 01:00:02,365
Speaker 3:  acceptable because you, you've now built a product on the idea

941
01:00:02,365 --> 01:00:05,045
Speaker 3:  that it can do this thing that it just demonstrably can't do.

942
01:00:05,765 --> 01:00:09,085
Speaker 6:  I don't think it's acceptable, but it's like also the people who are coding

943
01:00:09,105 --> 01:00:12,285
Speaker 6:  and using it want it to produce correct code. And then like I've been told

944
01:00:12,285 --> 01:00:15,165
Speaker 6:  junior engineers shouldn't be using it, but senior engineers can because

945
01:00:15,165 --> 01:00:18,565
Speaker 6:  they know what's wrong and they can fix it and, and the output that it's

946
01:00:18,565 --> 01:00:22,325
Speaker 6:  given. So I as like a senior reporter, I'm like, I, I know all of this, I

947
01:00:22,325 --> 01:00:25,845
Speaker 6:  just need to find this information quickly and Google is just absolute

948
01:00:26,155 --> 01:00:29,845
Speaker 6:  garbage at this. So I mean, I don't know, it just takes like

949
01:00:29,845 --> 01:00:33,125
Speaker 6:  knowing that, and that's something I'm gonna keep referencing this one interview,

950
01:00:33,725 --> 01:00:37,205
Speaker 6:  the head of chat BT said he was like, you know, the users at the beginning

951
01:00:37,635 --> 01:00:40,245
Speaker 6:  knew that they were gonna have to check this information. They knew how to

952
01:00:40,245 --> 01:00:43,405
Speaker 6:  prompt around things and they're fine with that. So like that's the people

953
01:00:43,615 --> 01:00:47,365
Speaker 6:  using chat GBT to be clear. But I agree, I don't think it's

954
01:00:47,365 --> 01:00:51,285
Speaker 6:  acceptable to be like we have the answers and we literally like 90%

955
01:00:51,285 --> 01:00:54,485
Speaker 6:  of the time don't actually have an accurate answer. I think that's also unacceptable.

956
01:00:54,715 --> 01:00:58,565
Speaker 3:  Like forget all the like bigger philosophical stakes. That's just a bad product.

957
01:00:58,795 --> 01:01:01,285
Speaker 3:  Like what If you went to Google and it just gave you a bunch of links that

958
01:01:01,285 --> 01:01:04,125
Speaker 3:  had nothing to do with your search, that would just be a bad product. Doesn't

959
01:01:04,125 --> 01:01:05,085
Speaker 3:  it already kinda do that.

960
01:01:05,125 --> 01:01:08,165
Speaker 2:  I fun links that make you feel good about yourself.

961
01:01:09,335 --> 01:01:09,685
Speaker 2:  Right?

962
01:01:10,035 --> 01:01:12,445
Speaker 3:  It's, it's just like slightly nicer to you than it should be.

963
01:01:12,445 --> 01:01:15,565
Speaker 2:  Lemme just give you an example of bad product. This also happened this week,

964
01:01:15,585 --> 01:01:19,205
Speaker 2:  and I just wanna put this all in the context of we're we're defining

965
01:01:19,625 --> 01:01:22,845
Speaker 2:  AGI down so we can say it happened, right? Yeah. And we're gonna charge $200

966
01:01:22,885 --> 01:01:25,365
Speaker 2:  a month for some of our models the same week.

967
01:01:27,115 --> 01:01:29,875
Speaker 2:  A misinformation researcher, the founder of the Stanford Social Media lab

968
01:01:30,095 --> 01:01:33,955
Speaker 2:  got in trouble because he wrote an affidavit in support of an anti

969
01:01:34,155 --> 01:01:37,975
Speaker 2:  deepfake law in Minnesota that is being challenged by court.

970
01:01:37,975 --> 01:01:40,335
Speaker 2:  So like there's a law in Minnesota that says you can't deep fake politicians

971
01:01:40,335 --> 01:01:43,775
Speaker 2:  during a political campaign. It seems reasonable. There's a lot of like First

972
01:01:43,775 --> 01:01:46,335
Speaker 2:  Amendment stuff, whatever, it's being challenged in court by a bunch of conservatives.

973
01:01:46,555 --> 01:01:50,535
Speaker 2:  And this guy, founder of the Stanford Social Media Lab wrote an affidavit

974
01:01:50,535 --> 01:01:53,655
Speaker 2:  in support of this law to the court. And it turns out he used ChatGPT BT

975
01:01:53,655 --> 01:01:57,055
Speaker 2:  to help him organize the citations and it hallucinated the citations.

976
01:01:57,125 --> 01:01:57,415
Speaker 6:  It's

977
01:01:57,415 --> 01:02:00,895
Speaker 2:  Nice and it's like, what are you doing? You of all people know, right?

978
01:02:01,145 --> 01:02:04,495
Speaker 2:  Right. You, you know that the duals hallucinate, but they're so confident

979
01:02:04,715 --> 01:02:08,655
Speaker 2:  and you're like, screw it. Here it is. I here's the quote. I

980
01:02:08,655 --> 01:02:11,495
Speaker 2:  wrote and reviewed the substance of the declaration and I stand firmly behind

981
01:02:11,495 --> 01:02:14,335
Speaker 2:  each of the claims made at all, which are supported by the most recent scholarly

982
01:02:14,575 --> 01:02:17,535
Speaker 2:  research in the field and reflect my opinion as an expert regarding the impact

983
01:02:17,535 --> 01:02:21,205
Speaker 2:  of AI technology and misinformation and societal effects. Oh God. And then

984
01:02:21,205 --> 01:02:24,885
Speaker 2:  he goes on to say that he used Google Scholar and ChatGPT four oh to quote,

985
01:02:25,165 --> 01:02:28,045
Speaker 2:  identify articles that were likely to be relevant to declaration. So I could

986
01:02:28,135 --> 01:02:31,885
Speaker 2:  merge that, which I already knew with new scholarship. And it's like the

987
01:02:31,885 --> 01:02:33,885
Speaker 2:  new scholarship was made up. Right.

988
01:02:34,175 --> 01:02:35,325
Speaker 3:  First of all, why

989
01:02:35,845 --> 01:02:39,605
Speaker 2:  I asked a robot to hallucinate and then I added my ideas to it is like,

990
01:02:39,665 --> 01:02:40,885
Speaker 2:  you know, that's not correct.

991
01:02:41,475 --> 01:02:45,365
Speaker 3:  Yeah. I also just, I'm convinced like somewhat

992
01:02:45,365 --> 01:02:49,045
Speaker 3:  sincerely that this is just a bit like it's all, it's all

993
01:02:49,305 --> 01:02:53,005
Speaker 3:  too perfect for this to have earnestly happened to this person. Like

994
01:02:53,345 --> 01:02:57,205
Speaker 3:  not only it's, it's like he, it's like he hid it at the bottom just

995
01:02:57,205 --> 01:02:59,285
Speaker 3:  to see if anybody would notice and somebody noticed

996
01:02:59,415 --> 01:03:02,405
Speaker 2:  Brown m and ms. This is the real theme of The Verge House is when Halen shows

997
01:03:02,405 --> 01:03:04,885
Speaker 2:  up and says, don't put brown m and ms in the dressing room. Right. To see

998
01:03:04,885 --> 01:03:07,725
Speaker 2:  if you're paying attention so that you know the pyros right. And everything

999
01:03:07,725 --> 01:03:09,965
Speaker 2:  in the world is explained by Brown m and ms. Yeah.

1000
01:03:10,285 --> 01:03:11,045
Speaker 3:  Honestly, potentially. Did I

1001
01:03:11,045 --> 01:03:13,365
Speaker 2:  Hallucinate these citations? Were you reading that? Did you read my affidavit?

1002
01:03:13,365 --> 01:03:13,645
Speaker 2:  Judge,

1003
01:03:14,585 --> 01:03:17,965
Speaker 3:  But also I will say my, my wife is currently in grad school and is spending

1004
01:03:18,185 --> 01:03:21,485
Speaker 3:  an enormous amount of time doing things like putting together

1005
01:03:21,755 --> 01:03:25,485
Speaker 3:  perfectly formatted citations and actually got knocked off

1006
01:03:25,785 --> 01:03:29,685
Speaker 3:  points on a paper she turned in because everything was

1007
01:03:29,685 --> 01:03:33,285
Speaker 3:  right, but she like put the page number and the source in the wrong order.

1008
01:03:33,285 --> 01:03:36,405
Speaker 3:  Nice. And so loss points. And so she, I I've had like a year and a half now

1009
01:03:36,405 --> 01:03:40,245
Speaker 3:  of her just like raging against citation systems and so I'm like,

1010
01:03:40,245 --> 01:03:42,885
Speaker 3:  listen, if AI just wants to cite some stuff for me and I don't even have

1011
01:03:42,885 --> 01:03:45,405
Speaker 3:  to worry about it, great. Done. Perfect. Lock it in

1012
01:03:46,305 --> 01:03:47,445
Speaker 2:  The David Pea doesn't

1013
01:03:47,445 --> 01:03:50,205
Speaker 3:  Have to be real. Probably not. Nobody's gonna check anyway. No one's,

1014
01:03:50,385 --> 01:03:50,925
Speaker 6:  No one cares

1015
01:03:51,395 --> 01:03:55,165
Speaker 2:  Last. Well, last couple ones. I wanna put in the context of Sam Altman to

1016
01:03:55,165 --> 01:03:58,605
Speaker 2:  find AGI now, but the, the robots are just drunk, right?

1017
01:03:58,865 --> 01:04:01,525
Speaker 2:  That's right. Is that, that's basically the shape of this segment. That's

1018
01:04:01,525 --> 01:04:05,285
Speaker 2:  right. Google Sendar trial also on stage deal book

1019
01:04:07,095 --> 01:04:10,665
Speaker 2:  interview basically is Andrew Oar being like, are you doing a good job? And

1020
01:04:10,665 --> 01:04:13,705
Speaker 2:  he was like, I'm doing a great job. That's the whole interview. You can watch

1021
01:04:13,705 --> 01:04:17,585
Speaker 2:  it. I mean, they're both very chill people. So even that was like five

1022
01:04:17,795 --> 01:04:21,065
Speaker 2:  ticks more hostile than the actual interview was. But that's basically the

1023
01:04:21,065 --> 01:04:23,545
Speaker 2:  shape of it. Are you doing a good job? I'm doing a great job. Okay. And then

1024
01:04:23,545 --> 01:04:27,345
Speaker 2:  he says, Sundar says, I would love to do a side

1025
01:04:27,345 --> 01:04:31,185
Speaker 2:  by side comparison of Microsoft's own models and our

1026
01:04:31,185 --> 01:04:34,785
Speaker 2:  models any day, anytime they're using someone

1027
01:04:35,005 --> 01:04:38,745
Speaker 2:  else's models, which is very catty. And what he means is Microsoft's own

1028
01:04:38,745 --> 01:04:40,505
Speaker 2:  models suck and they have to use OpenAI models.

1029
01:04:40,685 --> 01:04:41,505
Speaker 6:  The girls are fighting.

1030
01:04:41,525 --> 01:04:44,825
Speaker 2:  It also means that if Sam Altman thinks OpenAI models

1031
01:04:45,965 --> 01:04:49,425
Speaker 2:  are about to be AGI sort of definitionally, so are

1032
01:04:49,425 --> 01:04:49,985
Speaker 2:  Google's,

1033
01:04:51,825 --> 01:04:54,665
Speaker 6:  I think they've all admitted that. Like they're going to, like Elon said,

1034
01:04:54,665 --> 01:04:58,385
Speaker 6:  like if, if we were to hit AGI, like we're all hitting AGI at the same time

1035
01:04:58,385 --> 01:05:00,865
Speaker 6:  someone asked like, are you gonna get it to it first? He's like, probably

1036
01:05:00,865 --> 01:05:03,945
Speaker 6:  not. We're probably like all in lockstep gonna get to it at the same time.

1037
01:05:05,265 --> 01:05:08,235
Speaker 3:  Yeah. If the last two years are in the indication they're all within like

1038
01:05:08,235 --> 01:05:11,595
Speaker 3:  four weeks of each other Right. On every development side. Truly. Like

1039
01:05:12,015 --> 01:05:15,755
Speaker 3:  it just keeps happening. Like yeah, OpenAI does something and then Anthropic

1040
01:05:15,785 --> 01:05:18,560
Speaker 3:  does something that's, you know, 10 know 10% better on benchmarks and then

1041
01:05:18,675 --> 01:05:21,485
Speaker 3:  meta is something that's 10% better on benchmarks. And then Google's like,

1042
01:05:21,485 --> 01:05:24,205
Speaker 3:  what's up Gemini? And then we just start the cycle over again. The

1043
01:05:24,205 --> 01:05:25,925
Speaker 2:  Apple is like, there's 10 people at your front door,

1044
01:05:28,695 --> 01:05:32,565
Speaker 3:  Front door, back door and outside. Yeah. And this is

1045
01:05:32,565 --> 01:05:35,605
Speaker 3:  just like, it, it, this stuff is both

1046
01:05:36,385 --> 01:05:40,285
Speaker 3:  the most competitive and the most rapidly commoditizing

1047
01:05:40,415 --> 01:05:44,325
Speaker 3:  thing I can remember. Yeah. It's very strange. And So I think this is where

1048
01:05:44,325 --> 01:05:47,645
Speaker 3:  you get to what all of these companies are desperately trying to convince

1049
01:05:47,645 --> 01:05:51,405
Speaker 3:  you that they either have some thing in the works

1050
01:05:51,555 --> 01:05:55,245
Speaker 3:  that is so much bigger and better that yeah, it's gonna change it or

1051
01:05:56,115 --> 01:05:59,685
Speaker 3:  that they're gonna productize it so much faster. And I think for Google totally,

1052
01:06:00,095 --> 01:06:03,485
Speaker 3:  their whole move here is we can build these products faster than everybody

1053
01:06:03,485 --> 01:06:07,365
Speaker 3:  else, right? Like if it's Sundar is out here just being like, our models

1054
01:06:07,425 --> 01:06:11,245
Speaker 3:  are great, we're they're gonna ship more stuff this year, we think, but

1055
01:06:11,245 --> 01:06:14,965
Speaker 3:  they're like, he's like, whatever we're Google, I built it into Gmail

1056
01:06:15,025 --> 01:06:16,965
Speaker 3:  and Google Drive. Like, what do you got, Sam?

1057
01:06:17,235 --> 01:06:20,765
Speaker 6:  Totally. I think it's, it's interesting these incumbents, like

1058
01:06:20,825 --> 01:06:24,565
Speaker 6:  Amazon has its own like new set of models and like

1059
01:06:24,655 --> 01:06:28,165
Speaker 6:  there is reports about OpenAI trying to, you know, bake

1060
01:06:28,465 --> 01:06:32,405
Speaker 6:  you make, make chat GBT you know, a customizable

1061
01:06:32,975 --> 01:06:36,445
Speaker 6:  enterprise product for people to use. So it's like they're, they're

1062
01:06:36,445 --> 01:06:40,285
Speaker 6:  competing in the, in this way and the incumbent and the incumbents

1063
01:06:40,395 --> 01:06:44,165
Speaker 6:  have a huge advantage obviously, but in terms of

1064
01:06:44,165 --> 01:06:47,365
Speaker 6:  like this, this hype cycle, this constant hype cycle that's like the only

1065
01:06:47,365 --> 01:06:50,885
Speaker 6:  thing I've learned as a AI journalist is like, just do not trust a single

1066
01:06:50,885 --> 01:06:53,565
Speaker 6:  thing. Just don't trust a single thing. They're gonna tell you, you know,

1067
01:06:53,615 --> 01:06:56,885
Speaker 6:  we're not gonna raise prices, we're close to AGI, we're like all of these

1068
01:06:56,885 --> 01:07:00,725
Speaker 6:  things and IJ just only trust what is released

1069
01:07:00,785 --> 01:07:03,605
Speaker 6:  and you can use in front of you at, at the very least, if

1070
01:07:03,605 --> 01:07:07,085
Speaker 3:  Sam Altman really wanted to be cool, the thing he should have said is like,

1071
01:07:07,085 --> 01:07:10,805
Speaker 3:  oh yeah, we hit AGI like two years ago. ChatGPT was AGI. Like, who's gonna,

1072
01:07:10,815 --> 01:07:11,685
Speaker 3:  who's gonna stop it?

1073
01:07:12,185 --> 01:07:15,445
Speaker 2:  And people, people believe that it is. I I honestly think they're very much

1074
01:07:15,445 --> 01:07:18,245
Speaker 2:  trading on a bunch of people who

1075
01:07:19,235 --> 01:07:23,165
Speaker 2:  just love smooth talkers, you know what I mean? Like If you, If you

1076
01:07:23,165 --> 01:07:26,605
Speaker 2:  can talk real fast and real confidently, you can get pretty far in America

1077
01:07:27,465 --> 01:07:29,965
Speaker 2:  and chat TV is like how fast and how smooth, you know,

1078
01:07:32,465 --> 01:07:32,805
Speaker 3:  You know,

1079
01:07:32,875 --> 01:07:36,565
Speaker 2:  Like it can just do it and then people just buy it. My niece

1080
01:07:36,755 --> 01:07:40,285
Speaker 2:  just by the way, refers to it just as chat, so it's home for Thanksgiving

1081
01:07:40,305 --> 01:07:42,565
Speaker 2:  and she just kept saying, let me ask chat. And I'm like, you know, it's just

1082
01:07:42,565 --> 01:07:46,165
Speaker 2:  lying to you. She's like, whatever, like close enough. Which is

1083
01:07:46,165 --> 01:07:49,925
Speaker 2:  terrifying. But it's also just like, that's the relationship people have

1084
01:07:49,925 --> 01:07:53,365
Speaker 2:  with this product and at some point they're trading on

1085
01:07:54,325 --> 01:07:57,625
Speaker 2:  it hasn't done enough damage yet. Right. But something will happen,

1086
01:07:58,015 --> 01:08:01,705
Speaker 2:  another researcher will get caught, another lawyer will, you know,

1087
01:08:01,735 --> 01:08:04,825
Speaker 2:  show up with a, a fake citation

1088
01:08:05,325 --> 01:08:08,505
Speaker 2:  enough people are getting in trouble that the markets they that have the

1089
01:08:08,505 --> 01:08:11,585
Speaker 2:  money to spend will know, will stop spending the money until they can guarantee

1090
01:08:12,255 --> 01:08:15,645
Speaker 2:  reliability or accuracy or even just efficacy.

1091
01:08:36,835 --> 01:08:40,795
Speaker 3:  foreseeable future and maybe forever is being the

1092
01:08:41,085 --> 01:08:43,955
Speaker 3:  cloud provider of AI tools. And, and it's the

1093
01:09:04,205 --> 01:09:07,915
Speaker 6:  who's first beat in journalism was on the cloud and enterprise team. I love

1094
01:09:07,915 --> 01:09:09,155
Speaker 6:  it. I love what it comes up, comes just

1095
01:09:09,155 --> 01:09:09,315
Speaker 3:  Back at

1096
01:09:09,315 --> 01:09:13,235
Speaker 6:  It now. I it's a all roads lead back to SaaS is

1097
01:09:13,235 --> 01:09:16,595
Speaker 6:  where where it is. Yeah. I mean like that's the fight. That's where all of

1098
01:09:16,595 --> 01:09:19,595
Speaker 6:  the money is and if they wanna make tens of billions of dollars and train

1099
01:09:19,595 --> 01:09:23,395
Speaker 6:  their a hundred billion dollar models, they're gonna need some big chumps

1100
01:09:23,395 --> 01:09:25,875
Speaker 6:  to pay up for these enterprise contracts. Seriously.

1101
01:09:26,395 --> 01:09:30,035
Speaker 2:  I guess so let me just end here. One, I feel like we should go back and find

1102
01:09:30,055 --> 01:09:32,795
Speaker 2:  all the quotes of CS saying that would solve hallucinations and we like,

1103
01:09:32,795 --> 01:09:36,355
Speaker 2:  did this happen? Right? And then two, when, whenever, however we define AGI,

1104
01:09:36,615 --> 01:09:40,395
Speaker 2:  we should be like, if, If you made a somewhat smart college

1105
01:09:40,395 --> 01:09:43,155
Speaker 2:  student that just made shit up

1106
01:09:43,765 --> 01:09:47,675
Speaker 2:  40% of the time, 153 times out of 200 just lied

1107
01:09:47,675 --> 01:09:51,355
Speaker 2:  to you, would you be like, you drive a submarine,

1108
01:09:53,805 --> 01:09:57,635
Speaker 2:  right? Like there's something there, there's a disconnect there. That

1109
01:09:57,725 --> 01:10:01,475
Speaker 2:  still feels like the most important thing. As much fun as these tools are,

1110
01:10:02,035 --> 01:10:05,075
Speaker 2:  there's just a meaningful disconnect where they don't actually know anything.

1111
01:10:05,845 --> 01:10:08,635
Speaker 2:  We'll see. Totally. Kylie, what's your sense on like

1112
01:10:09,535 --> 01:10:13,315
Speaker 2:  12 days of shipments are coming, what's your sense on the next turn

1113
01:10:13,465 --> 01:10:17,315
Speaker 2:  here? Is it just opening eye barrels towards being a for-profit next year?

1114
01:10:17,575 --> 01:10:20,715
Speaker 2:  Oh yes. That's it. Like that's the main thing. This

1115
01:10:20,715 --> 01:10:23,795
Speaker 6:  Whole time my mind has just been screaming about defense contracts and how

1116
01:10:23,895 --> 01:10:27,755
Speaker 6:  if these, if these ais are consistently wrong and now

1117
01:10:27,755 --> 01:10:31,235
Speaker 6:  they're like signing like deals with and Andrew, it's like, oh my god. Like

1118
01:10:31,235 --> 01:10:34,395
Speaker 6:  that's, that's coming, that's coming. And it's, it's, you know, like the

1119
01:10:34,395 --> 01:10:37,675
Speaker 6:  stakes are getting higher and I don't know if the models are ready for the

1120
01:10:37,675 --> 01:10:41,115
Speaker 6:  stakes that are coming and that's really stressful. I mean if they, they

1121
01:10:41,115 --> 01:10:44,875
Speaker 6:  wanna throw agents onto the payroll of major companies

1122
01:10:45,135 --> 01:10:49,115
Speaker 6:  and Yeah. Get even deeper into defense and then build all these

1123
01:10:49,115 --> 01:10:52,635
Speaker 6:  data centers. I mean 20, 25, yeah the stakes are just gonna keep getting

1124
01:10:52,635 --> 01:10:55,995
Speaker 6:  higher and these people are gonna keep making money and I think we are gonna

1125
01:10:55,995 --> 01:10:59,875
Speaker 6:  see, as you said, some of these serious consequences that I,

1126
01:10:59,955 --> 01:11:03,475
Speaker 6:  I don't think, I mean obviously we have seen a lot of serious consequences,

1127
01:11:03,495 --> 01:11:06,275
Speaker 6:  but I don't think it's really snapped anyone in the industry out of what

1128
01:11:06,275 --> 01:11:07,955
Speaker 6:  they're building, if that makes sense. Yeah.

1129
01:11:08,615 --> 01:11:12,075
Speaker 2:  By way. And Andrew's, this is nothing with, and Andrew, it's more Palmer

1130
01:11:12,075 --> 01:11:15,285
Speaker 2:  lucky. Palmer Lucky made a game Boy, do you know about this Game boy? And

1131
01:11:15,315 --> 01:11:18,565
Speaker 2:  Sean Holster has the game boy, he like put up a video of it on like our TikTok

1132
01:11:18,565 --> 01:11:22,445
Speaker 2:  or something and he was like writing a review of this game boy. And Jake

1133
01:11:22,445 --> 01:11:25,885
Speaker 2:  cast our executive editor are like, you realize that your story is Lockheed

1134
01:11:25,885 --> 01:11:29,605
Speaker 2:  Martin made a Game Boy. Right? That's a, it's a very VERGE story, but you

1135
01:11:29,675 --> 01:11:33,405
Speaker 2:  just take another run at it from that angle. So soon we'll

1136
01:11:33,505 --> 01:11:37,245
Speaker 2:  run like it's a real art versus artist story, you know, it's like,

1137
01:11:37,305 --> 01:11:40,805
Speaker 2:  can you listen to Ignition remix anymore? Oh my god, Lockheed Martin made

1138
01:11:40,805 --> 01:11:44,485
Speaker 2:  a Game Boy. Like they're the same, those are the same ideas. So we'll soon

1139
01:11:44,485 --> 01:11:47,285
Speaker 2:  publish it. I, I dunno If you call it a review, we'll publish what

1140
01:11:47,315 --> 01:11:50,285
Speaker 3:  Such a, what if it's like a really pretty game boy a moral, it's like, it's

1141
01:11:50,285 --> 01:11:50,605
Speaker 3:  a really pretty

1142
01:11:50,605 --> 01:11:54,485
Speaker 2:  Game boy of this game boy at some point. Good point. As as soon

1143
01:11:54,485 --> 01:11:58,325
Speaker 2:  as you wrestle with the essential nature of art versus the artist. Alright,

1144
01:11:58,385 --> 01:12:01,965
Speaker 2:  we gotta wrap it up here. The AI is telling me our time has come to a close.

1145
01:12:02,015 --> 01:12:05,285
Speaker 2:  Kylie, this was great. Thanks for joining us. Thanks for having me. All right,

1146
01:12:05,285 --> 01:12:08,525
Speaker 2:  we'll be right back with lighting around and wait till you hear who the sponsor

1147
01:12:08,545 --> 01:12:08,765
Speaker 2:  is.

1148
01:12:13,335 --> 01:12:17,165
Speaker 2:  We're back with the lightning round. Liam tell the people who

1149
01:12:17,165 --> 01:12:18,325
Speaker 2:  sponsored the lightning round this week.

1150
01:12:18,675 --> 01:12:22,405
Speaker 1:  This week's lightning round is presented by Amazon Q, the new generative

1151
01:12:22,465 --> 01:12:24,045
Speaker 1:  AI assistant from AWS

1152
01:12:24,455 --> 01:12:26,445
Speaker 3:  Feels just as good the second time. It does have

1153
01:12:26,445 --> 01:12:26,565
Speaker 2:  To say

1154
01:12:26,805 --> 01:12:28,605
Speaker 3:  It does, it has not lost its shine.

1155
01:12:29,105 --> 01:12:29,845
Speaker 2:  Say it again Liam.

1156
01:12:30,795 --> 01:12:34,405
Speaker 1:  This week's lightning round is presented by Amazon Q, the new generative

1157
01:12:34,505 --> 01:12:35,525
Speaker 1:  AI assistant from a

1158
01:12:35,605 --> 01:12:39,045
Speaker 3:  WSI don't even think we get more money for that. It just feels so good. It

1159
01:12:39,045 --> 01:12:40,085
Speaker 3:  just feels so good. I don't care.

1160
01:12:41,775 --> 01:12:45,205
Speaker 2:  We're gonna find ways as part of the subscription to let, to let you, the

1161
01:12:45,205 --> 01:12:47,765
Speaker 2:  people sponsor the lightning round. This is one of the, one of the ideas

1162
01:12:47,825 --> 01:12:48,205
Speaker 2:  we have,

1163
01:12:49,785 --> 01:12:50,565
Speaker 2:  but that's pretty good.

1164
01:12:50,945 --> 01:12:54,805
Speaker 3:  Do you think people would like our ads more if they were all read by

1165
01:12:55,365 --> 01:12:59,205
Speaker 3:  Vergecast listeners every time? It's like, you can come perform the ads

1166
01:12:59,385 --> 01:13:00,005
Speaker 3:  on our podcast,

1167
01:13:00,485 --> 01:13:04,205
Speaker 2:  Right? 'cause I, David and I won't do it. Right. You got that

1168
01:13:04,255 --> 01:13:06,805
Speaker 2:  pesky journalism, which is why we make li do it.

1169
01:13:07,465 --> 01:13:08,035
Speaker 3:  It's pretty good.

1170
01:13:08,175 --> 01:13:10,315
Speaker 2:  But readers can do whatever they want. Yeah,

1171
01:13:11,285 --> 01:13:15,085
Speaker 3:  I guess I did just like steal an idea from every NPR show ever. So I probably

1172
01:13:15,085 --> 01:13:17,525
Speaker 3:  shouldn't be as proud of myself for that as I am. Wait,

1173
01:13:17,645 --> 01:13:19,845
Speaker 2:  NPR lets the listeners read the ads. Yeah, like

1174
01:13:20,125 --> 01:13:24,005
Speaker 3:  Radiolab like does the credits at the end and it's like the, it's it's

1175
01:13:24,005 --> 01:13:26,885
Speaker 3:  a thing. People do it like whatever they're brought to you by the

1176
01:13:27,965 --> 01:13:30,045
Speaker 3:  somebody Sloan Foundation that's like, and

1177
01:13:30,045 --> 01:13:30,805
Speaker 2:  They let people call

1178
01:13:30,905 --> 01:13:34,165
Speaker 3:  Its do it. Yeah. And people like it. We could do this. We

1179
01:13:34,635 --> 01:13:34,925
Speaker 2:  Sure.

1180
01:13:37,265 --> 01:13:39,965
Speaker 2:  I'm just saying lighting around sponsored. Oh, we got a subscription going.

1181
01:13:39,985 --> 01:13:43,765
Speaker 2:  The lighting around, sponsored. We spend this money by the way, on making

1182
01:13:43,765 --> 01:13:47,605
Speaker 2:  journalism. It's, you know, this is who we are. But

1183
01:13:47,605 --> 01:13:50,605
Speaker 2:  one day I'm gonna skim a little off the top and we're gonna get a boat.

1184
01:13:50,945 --> 01:13:54,285
Speaker 3:  I'm gonna, I'm gonna turn this into a real basketball hoop one day. It's

1185
01:13:54,285 --> 01:13:54,925
Speaker 3:  gonna blow mines.

1186
01:13:55,225 --> 01:13:58,485
Speaker 2:  You're, you're gonna be the guy who buys Michael Jordan's house in Chicago.

1187
01:13:59,995 --> 01:14:03,045
Speaker 2:  It's just been on sale. That's the new VERGE cast studio in the suburbs of

1188
01:14:03,045 --> 01:14:05,845
Speaker 2:  Chicago for years. So you go to Highland Park, you just drive by the gates

1189
01:14:05,905 --> 01:14:09,045
Speaker 2:  and part of the problem is the gates say 23 on this. Nobody wants to buy

1190
01:14:09,045 --> 01:14:09,485
Speaker 2:  this house.

1191
01:14:11,085 --> 01:14:14,845
Speaker 2:  It's like too much house for for Highland Park, you know, because

1192
01:14:14,985 --> 01:14:18,945
Speaker 2:  no one else there is Michael Jordan. It's true. It's just been on sale for

1193
01:14:18,945 --> 01:14:22,805
Speaker 2:  years. You can look at the pictures. You also have to commit to living in

1194
01:14:22,805 --> 01:14:26,095
Speaker 2:  Michael Jordan's house, which has, which does, by the way, David have a basketball

1195
01:14:26,095 --> 01:14:29,295
Speaker 2:  court in it. And also a room where Michael Jordan definitely got drunk and

1196
01:14:29,295 --> 01:14:31,405
Speaker 2:  smoke cigars and play poker.

1197
01:14:31,585 --> 01:14:34,765
Speaker 3:  But then the price keeps going down. You just keep saying good things and

1198
01:14:36,085 --> 01:14:37,245
Speaker 3:  I don't understand the problem.

1199
01:14:38,475 --> 01:14:41,085
Speaker 2:  Look, when we were looking for, as when it was time to leave the woods, you

1200
01:14:41,085 --> 01:14:43,325
Speaker 2:  know, and like get max into school, I was like, what if we lose in Michael

1201
01:14:43,325 --> 01:14:46,125
Speaker 2:  Jordan's house? And that idea was quickly rejected.

1202
01:14:47,225 --> 01:14:48,285
Speaker 3:  That's, that's

1203
01:14:48,285 --> 01:14:52,245
Speaker 2:  What I have to say. It wasn't on the table for a variety of reasons. Alright,

1204
01:14:52,275 --> 01:14:53,085
Speaker 2:  it's the lightning round

1205
01:14:54,605 --> 01:14:57,035
Speaker 2:  again, sponsored by whoever Liam said it was sponsored by because I'm too

1206
01:14:57,195 --> 01:14:59,475
Speaker 2:  precious to say it, but think about it, rewind it and listen to it one more

1207
01:14:59,475 --> 01:15:02,995
Speaker 2:  time. Okay, so here's the thing. We got a lot of feedback that our other

1208
01:15:02,995 --> 01:15:06,275
Speaker 2:  segment, which who was name was show and Tell was too much like the lightning

1209
01:15:06,275 --> 01:15:08,115
Speaker 2:  round. So to separate these

1210
01:15:08,225 --> 01:15:09,835
Speaker 3:  Good solid feedback, by the way. Yeah,

1211
01:15:10,375 --> 01:15:14,195
Speaker 2:  It was correct. It was fundamentally correct. So our idea here is lightning

1212
01:15:14,195 --> 01:15:16,875
Speaker 2:  round. We're just gonna run through headlines as fast as we can. Lightning

1213
01:15:16,875 --> 01:15:20,725
Speaker 2:  style and hopefully that is perceptually different than

1214
01:15:20,725 --> 01:15:24,405
Speaker 2:  everything else we do on the show. Yeah. Which is we talk about

1215
01:15:24,605 --> 01:15:28,005
Speaker 2:  headlines. Alright, so here's number one, Bitcoin just hit a hundred thousand

1216
01:15:28,005 --> 01:15:28,165
Speaker 2:  dollars.

1217
01:15:30,155 --> 01:15:33,355
Speaker 3:  I have nothing to say. I, I cannot describe to you how little I have to say

1218
01:15:33,355 --> 01:15:33,795
Speaker 3:  about this.

1219
01:15:34,675 --> 01:15:36,875
Speaker 2:  I will just, I will say the two things I've always said.

1220
01:20:23,705 --> 01:20:27,565
Speaker 2:  How about Bitcoin? I've said them on CNBC to an audience

1221
01:20:27,565 --> 01:20:30,885
Speaker 2:  of people who did not wanna hear them from me. One, the only reason people

1222
01:20:30,885 --> 01:20:33,925
Speaker 2:  care about Bitcoin is dollars, which is why this headline is Bitcoin is now

1223
01:20:33,925 --> 01:20:37,845
Speaker 2:  worth a hundred thousand dollars. Not it's useful, right? I like not

1224
01:20:37,845 --> 01:20:41,125
Speaker 2:  more transactions are taking place in Bitcoin, right? You only care about

1225
01:20:41,125 --> 01:20:44,885
Speaker 2:  this 'cause of dollars. So at some point you're, you're, you've just

1226
01:20:44,885 --> 01:20:48,485
Speaker 2:  given up the game that Bitcoin is worth any, like a Bitcoin

1227
01:20:48,885 --> 01:20:52,325
Speaker 2:  qua. Bitcoin means nothing, right? 'cause you only care about how many dollars

1228
01:20:52,355 --> 01:20:56,045
Speaker 2:  it's worth. Second, If you have a Bitcoin, your incentive to

1229
01:20:56,045 --> 01:20:59,935
Speaker 2:  spend it is effectively zero. Why would you ever spend a Bitcoin

1230
01:21:00,115 --> 01:21:02,415
Speaker 2:  if the numbers just gonna be go up? It's

1231
01:21:02,415 --> 01:21:06,335
Speaker 3:  A, it's an asset class and like that's fine, right? And I think the,

1232
01:21:06,475 --> 01:21:09,095
Speaker 3:  the, the thing that I have come to appreciate over the last couple of years

1233
01:21:09,155 --> 01:21:12,935
Speaker 3:  is that the number of people who want to talk to me about how

1234
01:21:13,005 --> 01:21:16,935
Speaker 3:  this is the future of the internet has gone way down. And the number of people

1235
01:21:16,935 --> 01:21:19,615
Speaker 3:  who just wanna talk about how much money they're making has gone way up.

1236
01:21:19,755 --> 01:21:23,215
Speaker 3:  And to me, like I'm fine with that, right? Like we should talk about the

1237
01:21:23,215 --> 01:21:26,455
Speaker 3:  price of Bitcoin, the way that we talk about the price of baseball cards.

1238
01:21:26,725 --> 01:21:29,975
Speaker 3:  Like it's, it's a, it's a thing that you have

1239
01:21:30,175 --> 01:21:32,015
Speaker 2:  A fake idea for children. It's a,

1240
01:21:33,495 --> 01:21:37,055
Speaker 3:  I wouldn't gonna say quite those words, but basically, yeah. And like congrats

1241
01:21:37,055 --> 01:21:39,775
Speaker 3:  to everybody who has made a lot of money on Bitcoin, sincerely. Like great

1242
01:21:39,975 --> 01:21:43,455
Speaker 3:  job. I'm happy for all of you. I am increasingly

1243
01:21:43,455 --> 01:21:47,335
Speaker 3:  unconvinced that there is any kind of text story in Bitcoin and I am

1244
01:21:47,335 --> 01:21:49,815
Speaker 3:  more suspicious every day of people who tell me that there is because

1245
01:21:49,815 --> 01:21:53,575
Speaker 2:  There's no reason to spend one, right? It is a fundamentally UNT transactable

1246
01:21:53,575 --> 01:21:56,895
Speaker 2:  thing because you are just wasting money because all you care about is dollars.

1247
01:21:57,155 --> 01:22:00,295
Speaker 3:  The only good stories about Bitcoin are the people who like spent a bunch

1248
01:22:00,295 --> 01:22:03,335
Speaker 3:  of stuff on one 12 years ago and now it's like we

1249
01:22:03,335 --> 01:22:05,935
Speaker 2:  Are those people. I want to also wanna be very clear, you can assign us a

1250
01:22:05,935 --> 01:22:09,535
Speaker 2:  disclosure. We might be bitter because one time Adrian Jeffries

1251
01:22:09,835 --> 01:22:11,455
Speaker 2:  bought pizza with Bitcoin. Oh

1252
01:22:11,455 --> 01:22:12,695
Speaker 3:  That's right. I forgot that.

1253
01:22:13,335 --> 01:22:15,175
Speaker 2:  You know, you know, look up that story. It was a great story.

1254
01:22:15,285 --> 01:22:15,695
Speaker 3:  It's tough.

1255
01:22:16,215 --> 01:22:19,935
Speaker 2:  Adrian was one of our best reporters. She was a great writer

1256
01:22:19,995 --> 01:22:23,615
Speaker 2:  and she went to a, went to a bar and bought pizza Bitcoin. And every day

1257
01:22:23,655 --> 01:22:27,535
Speaker 2:  I think, boy, we could have sponsored our own lightning round. We

1258
01:22:27,535 --> 01:22:29,615
Speaker 2:  could have afforded the lightning round if we just held onto those Bitcoins.

1259
01:22:30,855 --> 01:22:34,455
Speaker 2:  I would just say, and even like we, you know, I think then Trump too is gonna,

1260
01:22:34,455 --> 01:22:38,335
Speaker 2:  there's gonna be a lot of crypto talk and infinite, the crypto lobby is

1261
01:22:38,335 --> 01:22:41,735
Speaker 2:  all very excited. And fundamentally it's all because of dollars.

1262
01:22:42,365 --> 01:22:45,415
Speaker 2:  They're just excited about how many dollars they will be. They will, the

1263
01:22:45,415 --> 01:22:48,935
Speaker 2:  things will be worth. Let's see what that means. Okay, this one David, I

1264
01:22:48,935 --> 01:22:52,665
Speaker 2:  need you to explain to me, okay, DIA is the

1265
01:22:52,665 --> 01:22:55,505
Speaker 2:  browser company's AI powered follow up to Arc.

1266
01:22:55,815 --> 01:22:58,865
Speaker 3:  Yeah. So the browser company made a browser called Arc.

1267
01:22:59,575 --> 01:23:02,505
Speaker 3:  It's very good and very cool and a lot of people liked it. So of course the

1268
01:23:02,505 --> 01:23:06,245
Speaker 3:  browser company decided to stop making it. That's, that's that

1269
01:23:06,245 --> 01:23:09,925
Speaker 3:  entire story. And now they're onto their second

1270
01:23:09,925 --> 01:23:13,885
Speaker 3:  product, which they are trying to make into something that is

1271
01:23:13,885 --> 01:23:17,685
Speaker 3:  like thoroughly everyday mainstream. And

1272
01:23:17,685 --> 01:23:21,645
Speaker 3:  they just put up a video this week that was like half recruiting video,

1273
01:23:21,915 --> 01:23:25,805
Speaker 3:  half kind of vision video released the

1274
01:23:25,805 --> 01:23:28,325
Speaker 3:  name of the thing showed off a little bit about what they're talking about,

1275
01:23:28,345 --> 01:23:32,045
Speaker 3:  but it is like, this thing is still in its very early days, but basically

1276
01:23:32,045 --> 01:23:34,965
Speaker 3:  they're making a browser that is like mostly

1277
01:23:36,145 --> 01:23:39,405
Speaker 3:  AI. And there's some neat stuff in here. Like you can, anywhere there's a

1278
01:23:39,405 --> 01:23:42,085
Speaker 3:  text box. And this is the thing you can do when you're building a browser,

1279
01:23:42,085 --> 01:23:46,005
Speaker 3:  right? Is you, you just have access to the whole web as long as somebody

1280
01:23:46,005 --> 01:23:48,685
Speaker 3:  has loaded it in a tab, which is a very powerful thing to be able to do.

1281
01:23:49,625 --> 01:23:53,605
Speaker 3:  And so they're, they're doing a thing where you can like click on the cursor

1282
01:23:53,625 --> 01:23:57,045
Speaker 3:  in any text box to bring up like a contextual AI menu

1283
01:23:57,525 --> 01:24:01,045
Speaker 3:  and it'll do stuff inside of that page on that

1284
01:24:01,475 --> 01:24:05,205
Speaker 3:  text box. Like whatever you want. And the, the big idea here

1285
01:24:05,205 --> 01:24:09,125
Speaker 3:  again is to like reimagine the way that you do all of

1286
01:24:09,125 --> 01:24:12,845
Speaker 3:  the things that you want to do online with ai. I will say the

1287
01:24:13,045 --> 01:24:16,765
Speaker 3:  reaction to this launch has not been super positive. Yeah. The two

1288
01:24:17,195 --> 01:24:20,605
Speaker 3:  Threads that I've seen most are why couldn't you build this into arc the

1289
01:24:20,605 --> 01:24:24,525
Speaker 3:  browser that lots of people already like and no thank you. I am

1290
01:24:24,525 --> 01:24:28,245
Speaker 3:  not interested in having more AI things pop up every time I go to a

1291
01:24:28,245 --> 01:24:32,005
Speaker 3:  webpage. I have to say I find both of those arguments very compelling.

1292
01:24:32,115 --> 01:24:32,405
Speaker 3:  Yeah.

1293
01:24:32,725 --> 01:24:36,565
Speaker 2:  I mean this is a hint at we're gonna do agents, right? You're gonna say

1294
01:24:36,565 --> 01:24:40,045
Speaker 2:  order me a sandwich and then something is gonna go browse the web and order

1295
01:24:40,045 --> 01:24:40,845
Speaker 2:  a sandwich on my behalf.

1296
01:24:41,275 --> 01:24:45,245
Speaker 3:  Yeah. And you can make a pretty good argument that the best way to do

1297
01:24:45,245 --> 01:24:47,725
Speaker 3:  that is with a web browser, right? Because it's like, right,

1298
01:24:47,845 --> 01:24:48,405
Speaker 2:  'cause you have the web,

1299
01:24:48,735 --> 01:24:52,645
Speaker 3:  Right? But only like you, you have the web and also you have me to

1300
01:24:52,645 --> 01:24:56,085
Speaker 3:  work with you, right? Like you can, I can open up, you can open up a tab

1301
01:24:56,325 --> 01:24:58,565
Speaker 3:  and show it to me. That's like, is this the thing that you want? And I say,

1302
01:24:58,565 --> 01:25:01,805
Speaker 3:  yeah, and it orders the thing for me. So like there's a, there's a useful

1303
01:25:02,085 --> 01:25:05,285
Speaker 3:  feedback loop inside of the browser that can actually really work. It's also

1304
01:25:05,285 --> 01:25:08,805
Speaker 3:  just a way to get access to everything without having to build

1305
01:25:09,035 --> 01:25:12,725
Speaker 3:  APIs, right? Like we've talked about the things that the rabbits and

1306
01:25:12,735 --> 01:25:16,045
Speaker 3:  human of the world are having to jump through to get access to DoorDash.

1307
01:25:16,665 --> 01:25:20,245
Speaker 3:  My browser can just go to doordash.com. Like yeah, it just can do it.

1308
01:25:20,665 --> 01:25:24,485
Speaker 3:  And So I think I, I have always thought the browser company is

1309
01:25:24,485 --> 01:25:27,005
Speaker 3:  like directionally interested in the right things,

1310
01:25:28,385 --> 01:25:32,315
Speaker 3:  but I, I am, I'm, I don't know, somewhere between curious

1311
01:25:32,335 --> 01:25:36,235
Speaker 3:  and skeptical that branching off to build this whole other thing that

1312
01:25:36,235 --> 01:25:40,155
Speaker 3:  is just like an AI bot on top of your browser is actually the thing

1313
01:25:40,155 --> 01:25:40,835
Speaker 3:  that they think it's

1314
01:25:41,385 --> 01:25:44,475
Speaker 2:  Yeah, and it's funny, Josh Miller was just on decoder as the CEO o the browser

1315
01:25:44,475 --> 01:25:47,275
Speaker 2:  company. He is like, our goal is to make an operating system for the web

1316
01:25:47,275 --> 01:25:50,795
Speaker 2:  that runs your web apps in it, which is a big idea. Yeah. Right? Most of

1317
01:25:50,795 --> 01:25:54,195
Speaker 2:  our, my apps are web apps anyway. I'm gonna, the browser will be more operating

1318
01:25:54,195 --> 01:25:57,155
Speaker 2:  system me and connect all those apps. And this is like a totally different

1319
01:25:57,155 --> 01:25:57,355
Speaker 2:  idea,

1320
01:25:57,645 --> 01:26:01,535
Speaker 3:  Right? And this is what they've been saying. Like you, you could

1321
01:26:01,805 --> 01:26:05,325
Speaker 3:  sort of push this towards that thing you're describing and that vision you're

1322
01:26:05,325 --> 01:26:08,685
Speaker 3:  describing has been their vision all along. Right? And in a way ARC became

1323
01:26:08,685 --> 01:26:12,325
Speaker 3:  like a diversion on that. It's like a power user redesign of a browser,

1324
01:26:12,455 --> 01:26:16,365
Speaker 3:  which is not an operating system for the internet. Yeah. So it'll, we don't

1325
01:26:16,365 --> 01:26:19,325
Speaker 3:  know a ton about what this browser's actually gonna be. They didn't show

1326
01:26:19,325 --> 01:26:22,445
Speaker 3:  that much. It was just a lot of like high minded talk about the internet.

1327
01:26:22,445 --> 01:26:26,325
Speaker 3:  Oh boy. So we'll see. But I will say the, the large number

1328
01:26:26,325 --> 01:26:30,045
Speaker 3:  of people out there who are not psyched about what is happening to Arc did

1329
01:26:30,045 --> 01:26:32,845
Speaker 3:  not seem to be satisfied by what deal looks like so far.

1330
01:26:33,425 --> 01:26:37,285
Speaker 2:  All right, next one. Threads continues to respond to Blue Sky

1331
01:26:37,285 --> 01:26:41,245
Speaker 2:  in a variety of ways, but then also just on their own pace. Another big

1332
01:26:41,245 --> 01:26:45,125
Speaker 2:  step towards Fed Verse integration. Now you can follow people from MA on

1333
01:26:45,125 --> 01:26:45,485
Speaker 2:  servers

1334
01:26:45,875 --> 01:26:46,285
Speaker 3:  Kind of

1335
01:26:46,545 --> 01:26:50,445
Speaker 2:  Fed verse servers, which is 99% activity

1336
01:26:50,465 --> 01:26:54,365
Speaker 2:  hub servers or 99% ma on. Yeah. So they won't, the post won't show

1337
01:26:54,365 --> 01:26:58,245
Speaker 2:  in your feeds, but their profile and posts will do appear on Threads

1338
01:26:58,245 --> 01:26:59,525
Speaker 2:  and you can get notifications and they

1339
01:26:59,525 --> 01:27:03,405
Speaker 3:  Publish. This one is so funny to me 'cause it is genuine. I

1340
01:27:03,405 --> 01:27:07,325
Speaker 3:  think I've been hard on the Threads team for not doing this

1341
01:27:07,325 --> 01:27:11,245
Speaker 3:  better and faster, but like credit where credit's due, they continue to

1342
01:27:11,245 --> 01:27:14,925
Speaker 3:  do the work and, and I believe sincerely that this is a thing that they want

1343
01:27:14,925 --> 01:27:18,525
Speaker 3:  to do Right and well, but the middle

1344
01:27:18,665 --> 01:27:20,605
Speaker 3:  phases are so weird. Yeah.

1345
01:27:20,955 --> 01:27:21,285
Speaker 2:  It's so

1346
01:27:21,285 --> 01:27:23,165
Speaker 3:  Complicated. So this thing where it's like you can follow somebody on the

1347
01:27:23,165 --> 01:27:26,085
Speaker 3:  Fedi verse, but you'll never see it unless you go to their profile page.

1348
01:27:26,085 --> 01:27:29,845
Speaker 3:  Like it's like 2006 era. Facebook is just a very weird

1349
01:27:30,965 --> 01:27:34,205
Speaker 3:  solution to a problem. And you get the sense they only announced this publicly

1350
01:27:34,205 --> 01:27:38,045
Speaker 3:  because somebody would've noticed it and it would've been a confusing

1351
01:27:38,045 --> 01:27:41,925
Speaker 3:  thing otherwise, you know what I mean? But again, like this is

1352
01:27:43,635 --> 01:27:46,715
Speaker 3:  a signal that they've done the hardest part, right. Which is pull that stuff

1353
01:27:46,865 --> 01:27:50,715
Speaker 3:  into Threads, how to co-mingle it with everything else

1354
01:27:50,715 --> 01:27:54,595
Speaker 3:  that's being posted to Threads is a difficult problem. But

1355
01:27:54,595 --> 01:27:58,315
Speaker 3:  that's like a thread specific problem. It's a big sign that

1356
01:27:58,315 --> 01:28:01,915
Speaker 3:  they've done a lot of the like really important protocol work that it shows

1357
01:28:01,915 --> 01:28:04,035
Speaker 3:  up in Threads at all. So I think that's really exciting.

1358
01:28:04,145 --> 01:28:07,515
Speaker 2:  Yeah. And so, you know, we want to federate quick posts and we are figuring

1359
01:28:07,515 --> 01:28:09,675
Speaker 2:  out how to do that. Actually don't wanna federate, I don't know how we're

1360
01:28:09,675 --> 01:28:13,195
Speaker 2:  doing any of that. And our people they call what you're describing the inbox.

1361
01:28:13,465 --> 01:28:17,195
Speaker 2:  Yeah. So like outbox is easy, right? Like everyone has solved RSS

1362
01:28:17,195 --> 01:28:20,835
Speaker 2:  that's outbox. It's when somebody hits likes and it comes to you or you're

1363
01:28:20,835 --> 01:28:23,595
Speaker 2:  pulling in posts from other sites, right? So the inbox is really hard and

1364
01:28:23,595 --> 01:28:26,155
Speaker 2:  they've obviously gotten there and Meta has like big data

1365
01:28:27,615 --> 01:28:31,415
Speaker 2:  regulatory problems as well. Right? So I, I'm excited for 'em to do it. A

1366
01:28:31,535 --> 01:28:35,415
Speaker 2:  a big question I have is whether they will find a way to bridge

1367
01:28:35,735 --> 01:28:39,335
Speaker 2:  activity pub with app protocol such that Threads can interoperate with Blue

1368
01:28:39,335 --> 01:28:43,125
Speaker 2:  Sky. Because once you build that bridge it kind of doesn't matter. Like all

1369
01:28:43,125 --> 01:28:44,245
Speaker 2:  bets are off, you know, and,

1370
01:28:44,305 --> 01:28:48,245
Speaker 3:  And you gotta figure if Threads is truly threatened by the rise of

1371
01:28:48,245 --> 01:28:52,085
Speaker 3:  blue sky, that is one way to, if not kneecap

1372
01:28:52,155 --> 01:28:56,045
Speaker 3:  blue sky to at least like enrich yourself on its

1373
01:28:56,415 --> 01:28:56,765
Speaker 3:  gains.

1374
01:28:57,075 --> 01:28:59,805
Speaker 2:  Yeah. And If you're meta, like why do you care about what protocol you've

1375
01:28:59,805 --> 01:29:00,805
Speaker 2:  chosen to support? Yeah.

1376
01:29:01,085 --> 01:29:04,005
Speaker 3:  I think that that work gets done either way, right? Like there are already

1377
01:29:04,005 --> 01:29:07,685
Speaker 3:  people bridging app protocol and activity pub in some

1378
01:29:07,845 --> 01:29:11,125
Speaker 3:  interesting ways, but it would be interesting to see meta just

1379
01:29:11,705 --> 01:29:14,165
Speaker 3:  try to try to eat the whole thing all at once. Yeah.

1380
01:29:14,725 --> 01:29:17,285
Speaker 2:  They also, that's a search feature with filters, but like whatever they,

1381
01:29:17,635 --> 01:29:18,685
Speaker 2:  this is like baby steps.

1382
01:29:18,905 --> 01:29:22,085
Speaker 3:  We made search usable. It was literally that announcement like

1383
01:29:23,575 --> 01:29:27,405
Speaker 3:  great, I'm, I'm happy to be able to finally find a thing I was looking

1384
01:29:27,405 --> 01:29:28,125
Speaker 3:  for on Threads.

1385
01:29:28,195 --> 01:29:32,005
Speaker 2:  Yeah. Also on Threads meta Nick Clegg came out and said they're

1386
01:29:32,375 --> 01:29:35,905
Speaker 2:  moderating too much. I don't, I don't want to say about that. He is like,

1387
01:29:35,905 --> 01:29:39,625
Speaker 2:  we mistakenly moderate too much, which is true Threads is like a notoriously

1388
01:29:39,625 --> 01:29:43,545
Speaker 2:  overmoderated place. He says too often harmless content gets taken

1389
01:29:43,545 --> 01:29:46,745
Speaker 2:  down or restricted and too many people get penalized unfairly. This is a

1390
01:29:46,745 --> 01:29:49,265
Speaker 2:  true, like many people have had this experience, particularly in Threads

1391
01:29:49,445 --> 01:29:52,905
Speaker 2:  at the same time. Donald Trump is President Marza is at Mar-a-Lago,

1392
01:29:53,375 --> 01:29:57,305
Speaker 2:  Elon Musk probably also just skulking around Mar-a-Lago as he has want to

1393
01:29:57,305 --> 01:30:01,265
Speaker 2:  do talking about free speech while suing his critics as

1394
01:30:01,265 --> 01:30:04,705
Speaker 2:  he has want to do. If you're Mark Zuckerberg, you're like, ah, we're gonna

1395
01:30:04,825 --> 01:30:06,745
Speaker 2:  moderate a little less Donald like free speech for everybody.

1396
01:30:07,295 --> 01:30:11,265
Speaker 3:  There's also a really interesting pandemic era apology

1397
01:30:11,525 --> 01:30:13,825
Speaker 3:  in this, in this stuff from Nick Clegg talking about,

1398
01:30:15,425 --> 01:30:19,065
Speaker 3:  I think Facebook in particular was really aggressive about

1399
01:30:20,045 --> 01:30:24,035
Speaker 3:  moderating what was perceived for one reason or another to bel

1400
01:30:24,095 --> 01:30:27,915
Speaker 3:  to be incorrect information about the pandemic in the early days of the pandemic.

1401
01:30:28,135 --> 01:30:31,955
Speaker 3:  And it was super controversial and it wasn't that long ago that

1402
01:30:32,605 --> 01:30:36,595
Speaker 3:  Zuckerberg basically said the Biden administration told him to do

1403
01:30:36,595 --> 01:30:39,835
Speaker 3:  that or pressured him. Yeah. I mean, again, right? I

1404
01:30:39,835 --> 01:30:42,435
Speaker 2:  Just give a thumbs up. You're in your car, just imagine you're giving a thumbs

1405
01:30:42,435 --> 01:30:45,115
Speaker 2:  down. Yeah. That is a pure SOP to this administration.

1406
01:30:45,415 --> 01:30:48,875
Speaker 3:  So that's what I mean. Right? But this is like, it, it's pretty clear

1407
01:30:49,225 --> 01:30:52,755
Speaker 3:  what this push is. It is, it is like we take no responsibility

1408
01:30:53,015 --> 01:30:56,355
Speaker 3:  for what happened during Covid and there there is like a, a giant strain

1409
01:30:56,355 --> 01:31:00,195
Speaker 3:  of the right wing that is like furious about how information was shared during

1410
01:31:00,455 --> 01:31:04,395
Speaker 3:  the early days of the pandemic and radicalized a lot of people in a lot of

1411
01:31:04,395 --> 01:31:05,115
Speaker 3:  directions. Hey,

1412
01:31:05,785 --> 01:31:09,755
Speaker 2:  Just a quick question, David. Remind me, who

1413
01:31:09,755 --> 01:31:11,515
Speaker 2:  was president in 2020?

1414
01:31:12,605 --> 01:31:14,315
Speaker 3:  Who's to say who can

1415
01:31:14,555 --> 01:31:14,635
Speaker 2:  Remember?

1416
01:31:15,255 --> 01:31:16,235
Speaker 3:  It was so long ago.

1417
01:31:17,845 --> 01:31:21,115
Speaker 3:  Biden Biden made him do it. Yeah. Is I think what what was hard, he showed

1418
01:31:21,115 --> 01:31:21,275
Speaker 3:  up,

1419
01:31:21,625 --> 01:31:22,875
Speaker 2:  He's like, come to Delaware.

1420
01:31:24,465 --> 01:31:26,955
Speaker 2:  It's just, there's a real flattening, I don't wanna, it's the lightning round,

1421
01:31:27,415 --> 01:31:30,115
Speaker 2:  you know, it's like a, this is a good time. We got a sponsor today.

1422
01:31:31,755 --> 01:31:35,115
Speaker 2:  I don't wanna get into it. I, it's painful to re-litigate, but there's, there's

1423
01:31:35,115 --> 01:31:38,515
Speaker 2:  some real smashing of ideas down into

1424
01:31:38,665 --> 01:31:42,635
Speaker 2:  stories that don't make sense. Correct. And all of these platforms in the

1425
01:31:42,635 --> 01:31:46,595
Speaker 2:  early part of the pandemic all took the same steps and you can

1426
01:31:46,665 --> 01:31:50,435
Speaker 2:  love them or hate them, but they, Biden was not the president at that time.

1427
01:31:50,575 --> 01:31:53,235
Speaker 2:  And that is a thing that they're apologizing for is their early missteps,

1428
01:31:53,255 --> 01:31:54,195
Speaker 2:  not the later ones.

1429
01:31:54,195 --> 01:31:58,035
Speaker 3:  Right. But it is also very clear that Facebook is not the only company that

1430
01:31:58,035 --> 01:32:00,565
Speaker 3:  is essentially just gonna walk away from caring about content moderation.

1431
01:32:00,715 --> 01:32:01,005
Speaker 3:  Yeah.

1432
01:32:01,585 --> 01:32:04,005
Speaker 2:  No, that, that seems, that's just where we're headed. And it's, what's funny

1433
01:32:04,005 --> 01:32:07,405
Speaker 2:  about that is the blue sky, right? They, they moderate pretty aggressively.

1434
01:32:07,485 --> 01:32:11,445
Speaker 2:  I, I would say, but they're, they're out is that If you want just

1435
01:32:11,665 --> 01:32:15,245
Speaker 2:  the full fire hose, you can have it and you can experience an

1436
01:32:15,245 --> 01:32:18,165
Speaker 2:  unmoderated blue sky. Right. Or If you want an even more moderate blue sky,

1437
01:32:18,165 --> 01:32:22,125
Speaker 2:  you can have it. And that, that is a very powerful out that I think is ultimately

1438
01:32:22,125 --> 01:32:24,325
Speaker 2:  gonna be the future. And part of the reason I think they're doing activity

1439
01:32:24,385 --> 01:32:27,685
Speaker 2:  pop right to say you can leave, but you can, people here can still find you

1440
01:32:27,685 --> 01:32:31,525
Speaker 2:  over there. Right. Alright, we should talk about Intel for one second.

1441
01:32:32,355 --> 01:32:36,045
Speaker 2:  Sean has a big explainer. What happened to Gelsinger, the Intel CEO

1442
01:32:36,785 --> 01:32:37,845
Speaker 2:  out after three years?

1443
01:32:39,575 --> 01:32:42,445
Speaker 2:  David, do you buy this conspiracy theory that they were like, you have to

1444
01:32:42,445 --> 01:32:46,335
Speaker 2:  spin off the foundry. And he basically was like, no, I mean, I'm a

1445
01:32:46,335 --> 01:32:47,335
Speaker 2:  proud Intel man. And no,

1446
01:32:47,675 --> 01:32:50,415
Speaker 3:  It certainly makes sense. I I don't pretend to be like America's most source

1447
01:32:50,415 --> 01:32:53,455
Speaker 3:  chip reporter, but the, the thing that I I think

1448
01:32:54,235 --> 01:32:58,135
Speaker 3:  is most odd about the way this is going is that

1449
01:32:58,135 --> 01:33:01,935
Speaker 3:  If you want something that feels like a stable company and

1450
01:33:01,935 --> 01:33:05,695
Speaker 3:  stable transition, this is not how you get rid of your CEO. It's just

1451
01:33:05,715 --> 01:33:09,175
Speaker 3:  not like he, he re he quote unquote retired

1452
01:33:09,445 --> 01:33:13,135
Speaker 3:  effective immediately and then a source

1453
01:33:13,355 --> 01:33:17,335
Speaker 3:  who pretty clearly was like on or near the board of directors just ran

1454
01:33:17,335 --> 01:33:20,135
Speaker 3:  around telling reporters that they fired him. Like that's just what happened.

1455
01:33:20,135 --> 01:33:23,815
Speaker 2:  That's how I'm gonna go by the way, when it's my time, it's gonna be exactly

1456
01:33:24,445 --> 01:33:27,415
Speaker 3:  John Legend board member of Vox Media. Exactly. Is gonna go tell people how

1457
01:33:27,415 --> 01:33:31,215
Speaker 3:  he fired me. Like, but so and

1458
01:33:31,215 --> 01:33:34,055
Speaker 3:  so the, the, the conclusion that a lot of people including Sean have drawn

1459
01:33:34,075 --> 01:33:37,855
Speaker 3:  is that there must have been a fork in the road somewhere. Right? Yeah. And

1460
01:33:37,855 --> 01:33:40,535
Speaker 3:  if you're, If you Intel, that is the biggest fork sitting in front of you

1461
01:33:40,715 --> 01:33:43,975
Speaker 3:  is we've spent all of Pat Gelsinger 10 years so far

1462
01:33:44,815 --> 01:33:48,695
Speaker 3:  spending, I mean spending like money is going outta style on becoming

1463
01:33:49,415 --> 01:33:53,055
Speaker 3:  a foundry, reshaping the way that we make chips, reshaping the kinds of chips

1464
01:33:53,055 --> 01:33:55,975
Speaker 3:  that we make. Like they've bet on everything simultaneously all at once.

1465
01:33:56,475 --> 01:34:00,255
Speaker 3:  And there is a real case to be made that actually splitting it

1466
01:34:00,255 --> 01:34:02,855
Speaker 3:  into two so that you have a company that designs chips and a company that

1467
01:34:02,855 --> 01:34:05,935
Speaker 3:  makes them. And having those be separate things can be really useful.

1468
01:34:06,715 --> 01:34:10,695
Speaker 3:  And Pat Inger has pretty loudly said he thinks Intel needs to

1469
01:34:10,695 --> 01:34:14,615
Speaker 3:  be both of those things at the same time. So it, it, it, it

1470
01:34:14,615 --> 01:34:18,495
Speaker 3:  certainly holds up to like a logical reasoning that that

1471
01:34:18,495 --> 01:34:22,175
Speaker 3:  would be the thing where eventually they're like, well, Intel is

1472
01:34:22,405 --> 01:34:26,215
Speaker 3:  running into the ground faster than you can turn it around. We have to make

1473
01:34:26,215 --> 01:34:29,815
Speaker 3:  this change. And he was like, no, I have to finish the plan. Like, I don't

1474
01:34:29,815 --> 01:34:31,135
Speaker 3:  know the answer, but boy, does it make sense?

1475
01:34:31,275 --> 01:34:35,135
Speaker 2:  It does feel like he also got caught holding the bag for a bunch of bad decisions

1476
01:34:35,135 --> 01:34:36,135
Speaker 2:  that happened before he showed up.

1477
01:34:36,765 --> 01:34:40,575
Speaker 3:  Yeah. I mean, yes. He, I think the mistake he made was he gave

1478
01:34:40,575 --> 01:34:44,535
Speaker 3:  himself four years and he should have given himself 10 seriously. Like

1479
01:34:44,755 --> 01:34:48,415
Speaker 3:  he came in guns blazing and was like, we're gonna do, we're gonna do

1480
01:34:48,565 --> 01:34:51,935
Speaker 3:  five nodes in four years, which would've, which is like an unprecedented,

1481
01:34:52,225 --> 01:34:56,175
Speaker 3:  outrageous pace of chip development and did pretty well.

1482
01:34:56,525 --> 01:34:56,815
Speaker 3:  Well

1483
01:34:56,815 --> 01:34:59,865
Speaker 2:  Also a bunch of the chips had a bunch of bugs in them. Yeah,

1484
01:35:00,595 --> 01:35:04,425
Speaker 2:  right. We, we made some chips that self-destruct is a thing that

1485
01:35:04,585 --> 01:35:05,505
Speaker 2:  happened under Yeah.

1486
01:35:05,505 --> 01:35:09,185
Speaker 3:  Pretty well is pretty well was two kind. He did some things and

1487
01:35:09,835 --> 01:35:12,185
Speaker 2:  Right. Their best chips are, I believe made by tsmc.

1488
01:35:13,015 --> 01:35:15,835
Speaker 3:  Yes. Currently. Right. And and there's a bunch of really interesting history

1489
01:35:16,175 --> 01:35:20,115
Speaker 3:  in both Sean's piece, which links to an interview you did

1490
01:35:20,455 --> 01:35:23,915
Speaker 3:  on decoder a while ago with the author of a book called Chip War, which gets

1491
01:35:24,065 --> 01:35:27,875
Speaker 3:  into a all of this stuff. And there's like some Intel TSMC history.

1492
01:35:27,875 --> 01:35:31,275
Speaker 3:  That's really fascinating. We'll put all that stuff in the show notes. People

1493
01:35:31,275 --> 01:35:31,875
Speaker 3:  should go see it. But

1494
01:35:33,415 --> 01:35:36,515
Speaker 3:  it, it really, I just keep coming back to the idea that like

1495
01:35:37,265 --> 01:35:40,395
Speaker 3:  CEOs of companies where things are going well and they are stable, don't

1496
01:35:40,395 --> 01:35:43,195
Speaker 3:  resign effective immediately. Yeah. It just doesn't happen. And

1497
01:35:43,195 --> 01:35:45,035
Speaker 2:  Then, and then other ly they find

1498
01:35:45,275 --> 01:35:46,755
Speaker 3:  Right to everyone who will win.

1499
01:35:47,335 --> 01:35:49,955
Speaker 2:  All right, I'm gonna smash these two together. All right. Trump has picked

1500
01:35:49,955 --> 01:35:53,155
Speaker 2:  his SEC head and his antitrust enforcement head of

1501
01:35:53,275 --> 01:35:57,115
Speaker 2:  D-O-J-S-E-C head is Paul Atkins. He likes

1502
01:35:57,115 --> 01:36:00,115
Speaker 2:  crypto. Everyone's very excited. By the way, I love it when the crypto industry

1503
01:36:00,225 --> 01:36:03,755
Speaker 2:  says they oppose regulation by enforcement, which is a great phrase. If you

1504
01:36:03,755 --> 01:36:07,515
Speaker 2:  really unpack it, it means we're doing crimes and they're enforcing the laws

1505
01:36:07,515 --> 01:36:11,155
Speaker 2:  against the crimes. And they're like, what If you made real laws that said

1506
01:36:11,155 --> 01:36:14,595
Speaker 2:  these weren't crimes? It is very good. That is good. And I, there's more

1507
01:36:14,595 --> 01:36:17,675
Speaker 2:  nuance that it's just every time they say it, like I, every, like, the next

1508
01:36:17,675 --> 01:36:21,035
Speaker 2:  time I get caught speeding, I'm be like, officer, you're doing regulation

1509
01:36:21,035 --> 01:36:22,955
Speaker 2:  by enforcement. Like, I don't dunno what you're talking about

1510
01:36:25,075 --> 01:36:28,875
Speaker 2:  anyway. Gail Slater is the more important one. And I

1511
01:36:29,045 --> 01:36:32,635
Speaker 2:  trust person. You know, there's a big case against Google.

1512
01:36:32,825 --> 01:36:36,795
Speaker 2:  There's a big case against Apple. There's a lot of antitrust

1513
01:36:36,795 --> 01:36:37,235
Speaker 2:  in this world.

1514
01:36:37,635 --> 01:36:38,395
Speaker 3:  Microsoft lingering.

1515
01:36:38,615 --> 01:36:42,595
Speaker 2:  Yep. Gail Slater was on JD Vance's team is pretty well

1516
01:36:42,595 --> 01:36:46,515
Speaker 2:  respected. I have been told that a bunch of the antitrust folks in the world,

1517
01:36:46,515 --> 01:36:50,435
Speaker 2:  like the antitrust agitators are pretty happy about her. The

1518
01:36:50,435 --> 01:36:54,235
Speaker 2:  reason I'm bringing up JD Vance, he has given speeches where

1519
01:36:54,235 --> 01:36:57,435
Speaker 2:  he's like, Lena Kahn's doing a good job. Yeah. Google is a threat to the

1520
01:36:57,435 --> 01:37:00,955
Speaker 2:  kinds of companies that I'm aware of in my, his like, you know, two year

1521
01:37:01,465 --> 01:37:04,235
Speaker 2:  fake job as a vc. But you know,

1522
01:37:05,345 --> 01:37:09,115
Speaker 2:  when Trump appointed Gail Slater, long

1523
01:37:09,335 --> 01:37:13,235
Speaker 2:  rambly Trump Post on True social and he said, big tech is outta

1524
01:37:13,235 --> 01:37:16,795
Speaker 2:  control and it's hurting Americans and Little Tech And Little Tech

1525
01:37:17,175 --> 01:37:20,955
Speaker 2:  is words straight out of Mark Andreessen's mouth. Yeah. Andreesen

1526
01:37:20,955 --> 01:37:23,435
Speaker 2:  Horowitz. Right now it's, I think their whole thing is the little tech agenda.

1527
01:37:23,435 --> 01:37:27,315
Speaker 2:  Like that's what they branded it as. So you have, and they're

1528
01:37:27,625 --> 01:37:31,235
Speaker 2:  railing and Google and Apple as well. So you just have this person, we don't

1529
01:37:31,235 --> 01:37:35,155
Speaker 2:  know her, we're gonna try to talk to her, but she's inheriting a very aggressive

1530
01:37:35,185 --> 01:37:39,115
Speaker 2:  antitrust program. The people in and around that program, it seems

1531
01:37:39,385 --> 01:37:42,955
Speaker 2:  like are reasonably happy with this choice. We're doing some more reporting

1532
01:37:43,025 --> 01:37:46,275
Speaker 2:  that it's just a vibe. It's not, I am not gonna source that too much. Just

1533
01:37:46,275 --> 01:37:49,955
Speaker 2:  a, just a vibe I've caught. And then you have Donald Trump himself using

1534
01:37:49,955 --> 01:37:53,395
Speaker 2:  the words of his many, many wealthy benefactors

1535
01:37:54,605 --> 01:37:58,585
Speaker 2:  saying, who have like, long since been critical of the power of

1536
01:37:58,585 --> 01:38:00,825
Speaker 2:  the Apple and Google wield. It's gonna be weird in

1537
01:38:00,825 --> 01:38:03,625
Speaker 3:  Large part because it makes it harder for them to fund companies that are

1538
01:38:03,625 --> 01:38:06,865
Speaker 3:  hugely successful. Like, just, just to pull that thought all the way out.

1539
01:38:06,865 --> 01:38:07,025
Speaker 3:  Yeah.

1540
01:38:07,025 --> 01:38:09,785
Speaker 2:  I'm not saying any of this is like some good faith dealing, I'm saying this

1541
01:38:09,785 --> 01:38:12,745
Speaker 2:  is about to be some of the most gangster tech regulation that any of us have

1542
01:38:12,745 --> 01:38:16,185
Speaker 2:  ever, ever experienced in the horse trading is outta control, right? Yes.

1543
01:38:16,185 --> 01:38:19,785
Speaker 2:  Like If you, one of the, though, If you are a crypto

1544
01:38:21,105 --> 01:38:24,545
Speaker 2:  salesman, like you, what you wanna do is sell NFTs on the iPhone.

1545
01:38:24,935 --> 01:38:27,905
Speaker 2:  That would be a huge market. If you could unlock buying and selling NFTs

1546
01:38:27,905 --> 01:38:31,065
Speaker 2:  on mobile, that'd be great. Right Now Apple's gonna take 30% of every digital

1547
01:38:31,065 --> 01:38:34,945
Speaker 2:  transaction. You want that to go away. You know how you can do that? A

1548
01:38:34,945 --> 01:38:38,915
Speaker 2:  big antitrust case against Apple. Apple does not want tariffs on every iPhone.

1549
01:38:39,345 --> 01:38:43,115
Speaker 2:  They, Tim Cook needs to go walk up down. Like that's just, we're all gonna

1550
01:38:43,115 --> 01:38:46,555
Speaker 2:  sit around the table at Mar-a-Lago and reshape the American economy.

1551
01:38:46,975 --> 01:38:50,555
Speaker 2:  Yep. Somehow like, I don't know. And that it's not gonna

1552
01:38:50,955 --> 01:38:54,515
Speaker 2:  be like a process based, evidence-based agency

1553
01:38:54,515 --> 01:38:57,635
Speaker 2:  rulemaking. It's, it's gonna be whoever talked to Trump last.

1554
01:38:57,985 --> 01:39:01,835
Speaker 3:  Yeah. Like truly sitting around a table making these decisions is,

1555
01:39:01,855 --> 01:39:05,635
Speaker 3:  is a not particularly like euphemistic way to describe

1556
01:39:05,635 --> 01:39:07,835
Speaker 3:  what's about to happen. Yeah. Like that, that is how it's going to happen

1557
01:39:08,815 --> 01:39:09,435
Speaker 3:  in so many ways.

1558
01:39:09,575 --> 01:39:13,555
Speaker 2:  But I think the, the, a lot of people assumed that Trump

1559
01:39:13,555 --> 01:39:16,875
Speaker 2:  would come into office and, you know, pro business, pro big business especially,

1560
01:39:17,135 --> 01:39:21,115
Speaker 2:  he would insult an antitrust person That what the, the, the took all

1561
01:39:21,115 --> 01:39:25,035
Speaker 2:  these cases away. And I think there's a greater than even odds that these

1562
01:39:25,035 --> 01:39:26,915
Speaker 2:  cases continue and remain as aggressive as there.

1563
01:39:27,535 --> 01:39:31,475
Speaker 3:  It, it, yeah. I would say the, the early signals on this do

1564
01:39:31,545 --> 01:39:34,115
Speaker 3:  push to more towards that than I would've expected. And

1565
01:39:34,115 --> 01:39:37,795
Speaker 2:  Let me just wrap all this together. Lemme just bring this together in Trump's

1566
01:39:37,795 --> 01:39:40,955
Speaker 2:  first term. Macan del Rahim was his antitrust enforcer. And you recall that

1567
01:39:40,955 --> 01:39:44,115
Speaker 2:  instead of preventing T-Mobile from buying Sprint, he brokered a deal in

1568
01:39:44,115 --> 01:39:47,515
Speaker 2:  which T-Mobile was allowed to buy Sprint, thus reducing the number of national

1569
01:39:47,515 --> 01:39:51,315
Speaker 2:  carriers from four to three in America. And he sold Sprint's assets to

1570
01:39:51,315 --> 01:39:55,115
Speaker 2:  Dish Network, America's vibrant fourth

1571
01:39:55,115 --> 01:39:58,875
Speaker 2:  carrier. And that has resulted in this week tr Verizon just raising

1572
01:39:58,975 --> 01:40:02,635
Speaker 2:  its prices because there's no competition. A a market turn,

1573
01:40:02,995 --> 01:40:03,475
Speaker 2:  I would say.

1574
01:40:05,345 --> 01:40:08,355
Speaker 3:  Yeah. Alright, well we're dangerously close to a Snyder cut conversation

1575
01:40:08,355 --> 01:40:10,675
Speaker 3:  here. So last one.

1576
01:40:11,635 --> 01:40:15,155
Speaker 2:  Alright, Gail, what are you gonna let Zach Snyder do in your

1577
01:40:15,185 --> 01:40:16,115
Speaker 2:  antitrust regime?

1578
01:40:16,745 --> 01:40:20,555
Speaker 3:  Yeah, that's the, the big questions we ask here on the first cast. How was

1579
01:40:20,555 --> 01:40:21,595
Speaker 3:  your Spotify Wrapped this year?

1580
01:40:22,615 --> 01:40:25,995
Speaker 2:  It was mostly full of the Moana soundtrack, little Encanto

1581
01:40:26,775 --> 01:40:29,995
Speaker 2:  and more Frozen two than anyone has the the right

1582
01:40:30,775 --> 01:40:34,275
Speaker 2:  to, to really deal with Boy More Frozen Two. Mine is

1583
01:40:34,395 --> 01:40:35,075
Speaker 3:  Becoming a alarmingly

1584
01:40:35,075 --> 01:40:37,995
Speaker 2:  Similar to that straight Peter sat song on the Frozen two song. I, I don't

1585
01:40:37,995 --> 01:40:40,555
Speaker 2:  get into it, but I listen to it. I was like, that was just Peter Satra. And

1586
01:40:40,555 --> 01:40:43,875
Speaker 2:  then yeah, it turns out they were very inspired by the musical styles of

1587
01:40:43,875 --> 01:40:44,235
Speaker 2:  Chicago.

1588
01:40:44,485 --> 01:40:48,395
Speaker 3:  There you go. Mine, my number one artist was The Wiggles, which was pretty

1589
01:40:48,595 --> 01:40:52,035
Speaker 3:  exciting. So was my wife's. And between us, we were both top

1590
01:40:52,035 --> 01:40:56,005
Speaker 3:  3% Wiggles listeners. Very good. Yeah. Mine was

1591
01:40:56,005 --> 01:40:59,845
Speaker 3:  all kids music and then one unreleased Broadway

1592
01:40:59,875 --> 01:41:02,925
Speaker 3:  show song that I got really obsessed with because I found it on TikTok and

1593
01:41:02,925 --> 01:41:06,565
Speaker 3:  that's been my yearend music. But anyway, I bring this up to say two things.

1594
01:41:06,705 --> 01:41:10,245
Speaker 3:  One, sincere thank you to every single person who

1595
01:41:10,455 --> 01:41:14,285
Speaker 3:  sends us their Spotify Wrapped with us on it. Yeah. Makes sense. It is

1596
01:41:14,625 --> 01:41:18,405
Speaker 3:  the best. Every single one of them just rules. And like,

1597
01:41:18,575 --> 01:41:21,725
Speaker 3:  we've gotten people who are like, I spent five days listening to The Vergecast

1598
01:41:21,725 --> 01:41:25,125
Speaker 3:  last year and like, I cannot, it's, it's the best. That is like the best

1599
01:41:25,125 --> 01:41:28,685
Speaker 3:  feeling every year is when people are like, you're at the top. Unless we're

1600
01:41:28,685 --> 01:41:31,085
Speaker 3:  second to hard fork and then, ha, dare

1601
01:41:31,085 --> 01:41:31,245
Speaker 2:  You,

1602
01:41:31,705 --> 01:41:34,885
Speaker 3:  You're dead to me. But the second thing is,

1603
01:41:35,835 --> 01:41:39,365
Speaker 3:  everybody's doing rap now. Like Reddit has the thing,

1604
01:41:39,665 --> 01:41:43,565
Speaker 3:  the other music services have it. Podcast services have it. Like they're

1605
01:41:43,565 --> 01:41:47,045
Speaker 3:  all kind of doing their year interview thing. Spotify for its new innovation

1606
01:41:47,045 --> 01:41:50,765
Speaker 3:  this year, partnered with Notebook LM from Google

1607
01:41:51,225 --> 01:41:54,725
Speaker 3:  to do an AI podcast of your rap.

1608
01:41:55,355 --> 01:41:58,525
Speaker 3:  Have you listened to yours? No. They're very funny. Mine is like four and

1609
01:41:58,525 --> 01:42:01,805
Speaker 3:  a half minutes long and it's both awful and

1610
01:42:01,855 --> 01:42:05,685
Speaker 3:  delightful. It's just like, like imagine a sort of bouncy

1611
01:42:05,925 --> 01:42:09,765
Speaker 3:  stranger just reading your raps to you who

1612
01:42:09,765 --> 01:42:13,725
Speaker 3:  has never met you before. It's so fun and so funny. And you come out

1613
01:42:13,725 --> 01:42:17,405
Speaker 3:  of it being like, this is somehow the person that Spotify thinks I am. And

1614
01:42:17,405 --> 01:42:20,685
Speaker 3:  I don't know what to do about that. But like, we're, we're in a real like

1615
01:42:20,685 --> 01:42:22,965
Speaker 3:  wrapped arms race now and I'm extremely here for it.

1616
01:42:23,435 --> 01:42:26,005
Speaker 2:  It's like funny how like most of the time people are like, don't surveil

1617
01:42:26,005 --> 01:42:28,365
Speaker 2:  me. And at the end of the year they're like, tell me everything about myself.

1618
01:42:28,605 --> 01:42:31,125
Speaker 3:  Yeah. Listen, if you're gonna collect all this data on me, at least, like

1619
01:42:31,125 --> 01:42:33,205
Speaker 3:  give it back to me once a year. In a funny way.

1620
01:42:33,465 --> 01:42:36,285
Speaker 2:  I'm very excited for the, the Vizio, Walmart wrapped

1621
01:42:38,435 --> 01:42:39,725
Speaker 3:  Here are all the ads you engaged

1622
01:42:39,725 --> 01:42:40,285
Speaker 2:  With this year.

1623
01:42:41,385 --> 01:42:43,005
Speaker 3:  That's actually a good idea. Here's,

1624
01:42:43,005 --> 01:42:46,405
Speaker 2:  Here's exactly how much laundry detergent we prodded you into buying.

1625
01:42:47,975 --> 01:42:50,635
Speaker 3:  Here's how much you overspent on Amazon this year. Yeah.

1626
01:42:50,635 --> 01:42:53,355
Speaker 2:  Midyear you switched car insurance providers. That was us.

1627
01:42:57,105 --> 01:42:57,395
Speaker 2:  Same.

1628
01:42:57,475 --> 01:43:00,515
Speaker 3:  I mean, you know, they do that for their advertisers, right? All the clients

1629
01:43:00,515 --> 01:43:01,755
Speaker 3:  are getting wrapped right now.

1630
01:43:04,455 --> 01:43:07,795
Speaker 2:  All right. My, I just looked at my Apple Music wrapped and my number one

1631
01:43:08,075 --> 01:43:11,235
Speaker 2:  playlist was Taylor Swift Essentials. And my number two playlist was two

1632
01:43:11,435 --> 01:43:14,275
Speaker 2:  thousands indie rock. And you can really tell perfect that there's a battle

1633
01:43:14,335 --> 01:43:18,235
Speaker 2:  in our house over who gets to ask for music. It's very

1634
01:43:18,235 --> 01:43:21,355
Speaker 2:  good. Very good. All right. That's it. We're way over. We're like six hours

1635
01:43:21,355 --> 01:43:24,555
Speaker 2:  over. We've been podcasting for a full year now.

1636
01:43:24,925 --> 01:43:27,915
Speaker 3:  We're also both sick right now, So I just wanna say congratulations to both

1637
01:43:27,915 --> 01:43:28,235
Speaker 3:  of us. We

1638
01:43:28,235 --> 01:43:30,875
Speaker 2:  Did. If you, yeah. If you've heard it was about sniffling and coughing and

1639
01:43:30,875 --> 01:43:33,715
Speaker 2:  my and Max just like screaming in the hallway. Yeah. It's just one of those

1640
01:43:34,035 --> 01:43:36,435
Speaker 2:  s at the end of the year. It's a time. Once again, I wanna say thank you

1641
01:43:36,435 --> 01:43:40,315
Speaker 2:  to everybody who's subscribed, particularly Vergecast listeners who are telling

1642
01:43:40,315 --> 01:43:43,475
Speaker 2:  us or are subscribing just to support us. That means a lot. Like we've been

1643
01:43:43,475 --> 01:43:46,075
Speaker 2:  doing this for a long time. Yeah. 13 years is a long time to be doing a thing

1644
01:43:46,455 --> 01:43:50,115
Speaker 2:  and to have so many of you say, I can't believe I wasn't paying for this

1645
01:43:50,115 --> 01:43:53,565
Speaker 2:  already. Which is a real, real note. We have gotten mind blowing, so thank

1646
01:43:53,565 --> 01:43:57,045
Speaker 2:  you. We're gonna keep at it. We're very stubborn. No one ever, no one can

1647
01:43:57,105 --> 01:44:00,925
Speaker 2:  pay to tell us what to do except for you, apparently. So we're gonna figure

1648
01:44:00,925 --> 01:44:04,365
Speaker 2:  that out and also send us more questions. Call the hotline, send us, send

1649
01:44:04,365 --> 01:44:07,445
Speaker 2:  us questions. Yeah. Helen's on next week, so send us questions, we'll answer

1650
01:44:07,445 --> 01:44:09,965
Speaker 2:  them. We're gonna be as transparent with our business as we can be. We think

1651
01:44:09,965 --> 01:44:13,245
Speaker 2:  that's an important part of the whole puzzle here. And then we have a big

1652
01:44:13,245 --> 01:44:17,205
Speaker 2:  launch feature for the subscription Josh Jeza wrote about like

1653
01:44:17,205 --> 01:44:19,605
Speaker 2:  falling in love with AI and the companies that do it, and the people that

1654
01:44:19,725 --> 01:44:23,445
Speaker 2:  actually fall in love. Go read that. It is incredible. It is easily worth

1655
01:44:23,445 --> 01:44:27,125
Speaker 2:  your $7. Like, oh, unquestionably, straightforwardly read out stories worth

1656
01:44:27,125 --> 01:44:30,565
Speaker 2:  seven. Yeah. So go, go check that out. It's, it's good. And the art is amazing.

1657
01:44:30,865 --> 01:44:34,645
Speaker 2:  Oh, and you haven't read our magazine content goblins. Yeah, subscribe for

1658
01:44:34,645 --> 01:44:37,005
Speaker 2:  that. It's good. The photos are amazing. All right. That's it. That's for

1659
01:44:37,005 --> 01:44:37,565
Speaker 2:  Chest rock.

1660
01:44:42,065 --> 01:44:44,925
Speaker 1:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1661
01:44:45,115 --> 01:44:48,885
Speaker 1:  Give us a call at eight six six VERGE one one. The Vergecast

1662
01:44:48,905 --> 01:44:52,885
Speaker 1:  is a production of The Verge and Vox Media podcast network. Our show is produced

1663
01:44:52,885 --> 01:44:56,765
Speaker 1:  by Liam James Will Pour and Eric Gomez. And that's it. We'll

1664
01:44:56,765 --> 01:44:57,325
Speaker 1:  see you next week.

