1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 9e8328b3-e1a5-47cd-bd97-a7b686b55cdd
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/-8621159635500505288/-8742694857511658866/s93290-US-7310s-1743155668.mp3
Description: In this episode, we do a Studio Ghibli-like rendition of The Vergecast. First, Nilay and David discuss some big news in the gadget world, from the mysteriously viral midrange Canon camera to the upgrades we're expecting out of Apple in the next few months. Plus, is it over for Amazon's Echo brand? After all that, The Verge's Kylie Robison joins the show to discuss everything happening at OpenAI: the company launched a new image generator inside of ChatGPT, and it immediately became both a huge hit and a big mess. (Par for the course with OpenAI, really.) Kylie also explains why Perplexity is probably not buying TikTok, no matter how much it might want to. Finally, in the lightning round, it's time for everyone's favorite segment, Brendan Carr Is a Dummy, followed by the latest on the Signal attack-planning chaos in the government, some news about Elon Musk pressuring Reddit CEO Steve Huffmann, and what's next for the car industry with huge tariffs looming. Oh, and a little bit of exciting e-bike news


2
00:00:29,385 --> 00:00:29,605
Speaker 1:  Now

3
00:01:28,895 --> 00:01:32,605
Speaker 6:  Hello and welcome to Red Chest, the flagship podcast with viral cameras from

4
00:01:33,035 --> 00:01:33,845
Speaker 6:  five years ago.

5
00:01:34,555 --> 00:01:37,485
Speaker 7:  This has all made me feel really good because I have a, the only camera I

6
00:01:37,485 --> 00:01:41,165
Speaker 7:  have is from like 2012. It's a, it's a Sony a

7
00:01:41,265 --> 00:01:45,165
Speaker 7:  6,000 and it sucks. And I'm like, give it, give it 18 more months. And this

8
00:01:45,165 --> 00:01:46,485
Speaker 7:  is gonna be the camera everybody wants.

9
00:01:47,065 --> 00:01:49,645
Speaker 6:  I'm your friend Neli David Pierce is here. First of all, I wanna point out

10
00:01:49,645 --> 00:01:52,365
Speaker 6:  that everyone thought I was gonna make a joke about Signal, right? You all,

11
00:01:52,485 --> 00:01:55,965
Speaker 6:  you all thought it. I said the flagship podcast of, and you all thought

12
00:01:56,545 --> 00:01:59,765
Speaker 6:  of something, right? Adding somebody to a group chat, you all thought it

13
00:01:59,765 --> 00:02:02,685
Speaker 6:  and I went another way and I just wanna take the credit for that.

14
00:02:03,685 --> 00:02:06,035
Speaker 6:  We're gonna talk about Signal later in the show. Do you have

15
00:02:06,035 --> 00:02:07,475
Speaker 7:  Any good, I've been added to a group chat

16
00:02:08,215 --> 00:02:11,905
Speaker 6:  Stories. I have a good, I didn't know

17
00:02:12,005 --> 00:02:15,995
Speaker 6:  how deleting a message worked. So like

18
00:02:16,195 --> 00:02:19,355
Speaker 6:  I, this is a long time ago, actually. This is a really good story. Okay.

19
00:02:20,705 --> 00:02:24,265
Speaker 6:  A long time ago, the night before Uber

20
00:02:24,475 --> 00:02:28,145
Speaker 6:  fired Travis Kanick from this, from being the CEO, do you remember this?

21
00:02:28,145 --> 00:02:32,025
Speaker 6:  It is like a big deal. Yeah, sure. Everyone heard about it? I can't, I

22
00:02:32,025 --> 00:02:35,825
Speaker 6:  got a message from, let's just say a

23
00:02:35,825 --> 00:02:36,065
Speaker 6:  dancer

24
00:02:39,855 --> 00:02:43,635
Speaker 6:  and the dancer said, there's a VC crying in my lap

25
00:02:43,635 --> 00:02:47,515
Speaker 6:  right now because they're gonna fire Travis Knick. Whoa. And I

26
00:02:47,515 --> 00:02:51,315
Speaker 6:  had, what am I gonna do with this? Like, right. I'm like,

27
00:02:51,475 --> 00:02:55,355
Speaker 6:  this is weird. I dunno. Like, can you take a picture? Like this is a

28
00:02:55,355 --> 00:02:58,995
Speaker 6:  true story. A hundred percent true story. And so I went to Slack or transportation

29
00:02:59,005 --> 00:03:02,645
Speaker 6:  editor Andy Hawkins being like, I think dudes, I think it's gonna happen.

30
00:03:03,645 --> 00:03:07,335
Speaker 6:  Like here's we all, this noise is happening. And I just got this insane message.

31
00:03:09,435 --> 00:03:12,975
Speaker 6:  And in the middle of that, I accidentally texted her what I meant to text

32
00:03:13,005 --> 00:03:16,855
Speaker 6:  Andy. And I went to delete it. And it doesn't, it didn't delete right?

33
00:03:16,925 --> 00:03:19,775
Speaker 6:  Because it works. It's like locally, so you can hide your cheating from like

34
00:03:19,775 --> 00:03:22,855
Speaker 6:  your wife. But the person who got the message to has it,

35
00:03:25,925 --> 00:03:28,325
Speaker 6:  I thought I was being so smart. I was like, oh, I deleted it. And later,

36
00:03:28,435 --> 00:03:31,365
Speaker 6:  like she, the, the person was like, did you,

37
00:03:32,225 --> 00:03:35,645
Speaker 6:  please don't, please don't mention my name. And this why i's like, I don't

38
00:03:35,645 --> 00:03:39,005
Speaker 6:  have the story. That's, we don't know. It's true. This all It is all years

39
00:03:39,005 --> 00:03:42,405
Speaker 6:  later, we, we did not have the story at a single source the next morning.

40
00:03:42,405 --> 00:03:46,205
Speaker 6:  It turned out to be true. But you know, we have standards here,

41
00:03:46,205 --> 00:03:49,485
Speaker 6:  right? We, we need multiple sources to run a story of, of that nature. And

42
00:03:49,545 --> 00:03:52,325
Speaker 6:  so we could, I, so I knew it and the next morning I was like, God damnit,

43
00:03:52,475 --> 00:03:55,925
Speaker 7:  Does our ethics policy cover how many dancers have to

44
00:03:56,285 --> 00:03:59,205
Speaker 7:  corroborate the VC crying in their lap before it's portable

45
00:03:59,205 --> 00:04:02,845
Speaker 6:  Journalism? This was also, this was happening like at

46
00:04:02,845 --> 00:04:06,655
Speaker 6:  7:00 PM East Coast time, right? So it was like,

47
00:04:06,735 --> 00:04:10,715
Speaker 6:  I had to, I had to call like everyone was home. So they'd be like,

48
00:04:10,755 --> 00:04:13,075
Speaker 6:  I need you to walk away from your children. I need you to tell you a story

49
00:04:13,075 --> 00:04:17,065
Speaker 6:  about a t. He was like, what are you talking about? So he

50
00:04:17,065 --> 00:04:19,945
Speaker 6:  changed it for a while and then the next morning it happened and I was like,

51
00:04:19,945 --> 00:04:22,185
Speaker 6:  I should have done something that I couldn't figure out what to do. That's

52
00:04:22,185 --> 00:04:24,425
Speaker 6:  my, that's my texting fail story. What's yours? Good stuff.

53
00:04:24,705 --> 00:04:27,345
Speaker 7:  I like that. I don't really have a good one, to be honest. I have, I've had

54
00:04:27,385 --> 00:04:31,225
Speaker 7:  a lot of very near misses. Most of mine are actually more

55
00:04:31,465 --> 00:04:34,305
Speaker 7:  dangerous in Slack than in messages where

56
00:04:35,365 --> 00:04:39,185
Speaker 7:  in Slack everything looks the same. And I have sent, I have almost sent

57
00:04:39,185 --> 00:04:42,545
Speaker 7:  like truly alarming things to the person

58
00:04:43,305 --> 00:04:44,385
Speaker 7:  I was talking shit about.

59
00:04:46,225 --> 00:04:49,685
Speaker 7:  And I don't think I have ever actually done it. And it's a real like there,

60
00:04:49,685 --> 00:04:53,245
Speaker 7:  but for the grace of God kind of situation, watching all of this unfold.

61
00:04:53,835 --> 00:04:57,405
Speaker 7:  Yeah. Like It is, It, is it out of the realm of possibility that I have

62
00:04:57,605 --> 00:05:00,725
Speaker 7:  accidentally invited the editor in chief of the Atlantic to a group chat?

63
00:05:01,155 --> 00:05:03,725
Speaker 7:  It's not, but luckily I never have. As far as,

64
00:05:05,105 --> 00:05:08,525
Speaker 6:  The thing that gets me the most often is clipboard fails

65
00:05:09,955 --> 00:05:13,705
Speaker 6:  where I hit, I paced and I hit return. Like before I

66
00:05:13,705 --> 00:05:16,625
Speaker 6:  verify that I've pasted the thing I wanna paste. Yep. And that happens to

67
00:05:16,625 --> 00:05:19,465
Speaker 6:  me all the time. Like just random sentences from old documents just show

68
00:05:19,465 --> 00:05:19,865
Speaker 6:  up. Like

69
00:05:19,865 --> 00:05:22,345
Speaker 7:  That's right for me always. It's always a really, it's always a link. I didn't

70
00:05:22,345 --> 00:05:26,265
Speaker 7:  mean to post. Yeah. Like I, I copied the, the wrong tab out of Chrome

71
00:05:26,265 --> 00:05:30,185
Speaker 7:  instead of the right one. And so I'm like, Hey, here's a cool thing

72
00:05:30,185 --> 00:05:34,145
Speaker 7:  we should do. And then it's just like a, a weird like cheat code for a video

73
00:05:34,175 --> 00:05:37,705
Speaker 7:  game that I should not be thinking about at work. It's good times.

74
00:05:38,055 --> 00:05:40,585
Speaker 6:  This happens to me all the time with TikTok, which is my segue. Ooh,

75
00:05:40,935 --> 00:05:41,225
Speaker 7:  Okay.

76
00:05:41,595 --> 00:05:43,985
Speaker 6:  Right. Because I'm always like, I wanna share TikTok, but I always have to

77
00:05:43,985 --> 00:05:46,545
Speaker 6:  double check. It's not a totally bananas TikTok.

78
00:05:46,795 --> 00:05:50,185
Speaker 7:  Right? There's a real, like you have to watch the whole thing through just

79
00:05:50,185 --> 00:05:51,145
Speaker 7:  in case. Yeah.

80
00:05:51,205 --> 00:05:55,025
Speaker 6:  Is this creator secretly like a milkshake duck situation? It's like a real

81
00:05:55,025 --> 00:05:58,245
Speaker 6:  thing. So here's the mystery I wanna start with, which explains the joke

82
00:05:58,245 --> 00:06:01,565
Speaker 6:  at the top of the show. My TikTok feed for the past three days

83
00:06:02,305 --> 00:06:06,045
Speaker 6:  has just been people talking about the Canon G seven X mark

84
00:06:06,045 --> 00:06:09,625
Speaker 6:  three. And this to me, this is a gadget mystery.

85
00:06:10,375 --> 00:06:13,235
Speaker 6:  So the Canon G seven X mark three is a camera from 2019.

86
00:06:14,895 --> 00:06:17,425
Speaker 6:  It's just old. It is an old camera.

87
00:06:17,725 --> 00:06:18,625
Speaker 7:  Oh, I remember this camera.

88
00:06:19,055 --> 00:06:22,905
Speaker 6:  Yeah. It's just, it's a G seven X. Like it, you know, there's the RX 100,

89
00:06:22,905 --> 00:06:26,425
Speaker 6:  which I talk about on the show all the time. 'cause I have one and it's competitors,

90
00:06:26,455 --> 00:06:29,865
Speaker 6:  sort of the G seven X, but like, they're different. I dunno it whatever pop,

91
00:06:29,865 --> 00:06:30,025
Speaker 6:  it's got

92
00:06:30,045 --> 00:06:31,625
Speaker 7:  Aon pop. It's fun. Yeah.

93
00:06:32,295 --> 00:06:36,225
Speaker 6:  This thing is apparently mega popular with influencers and people wanna take

94
00:06:36,225 --> 00:06:40,065
Speaker 6:  be better pictures. They're all taking flash photos with it,

95
00:06:40,595 --> 00:06:44,585
Speaker 6:  which is bonkers. But they're, that's what they're doing. And

96
00:06:44,585 --> 00:06:48,165
Speaker 6:  It is selling out every day. Like I have gotten

97
00:06:48,665 --> 00:06:52,605
Speaker 6:  TikTok videos from people who are like, here's how to get a G seven X mark

98
00:06:52,605 --> 00:06:56,165
Speaker 6:  three, or even a mark two, which is even older. And the answer is like,

99
00:06:56,755 --> 00:07:00,725
Speaker 6:  wait up until 5:00 AM until Target restocks, its online like a

100
00:07:00,745 --> 00:07:01,405
Speaker 6:  PS five drop.

101
00:07:01,905 --> 00:07:02,125
Speaker 7:  Wow.

102
00:07:02,465 --> 00:07:06,005
Speaker 6:  So we've assigned the story, the virtue. I think it's Alison Johnson's like

103
00:07:06,005 --> 00:07:09,365
Speaker 6:  working on what, where, what's happening here. Like why,

104
00:07:09,945 --> 00:07:13,045
Speaker 6:  why is the G seven X mark three A camera again from 2019?

105
00:07:13,785 --> 00:07:16,605
Speaker 6:  Why is it going viral and why isn't Canon making enough of them?

106
00:07:17,105 --> 00:07:20,565
Speaker 7:  The second question I think is super fascinating. The, the macro

107
00:07:20,675 --> 00:07:24,525
Speaker 7:  mystery of this is, is pretty straightforward, right? This, this like

108
00:07:24,845 --> 00:07:28,525
Speaker 7:  resurgence in digital cameras and point and shoots in particular

109
00:07:29,745 --> 00:07:33,365
Speaker 7:  is a thing, right? We're in this very like, turn of the century

110
00:07:33,635 --> 00:07:37,005
Speaker 7:  nostalgia moment in so much of culture and like

111
00:07:37,625 --> 00:07:41,125
Speaker 7:  crappy pre-Instagram photos are like

112
00:07:41,705 --> 00:07:45,685
Speaker 7:  the thing everybody wants, which is very funny because they're bad. But that

113
00:07:45,685 --> 00:07:48,725
Speaker 7:  is like, there's something in that look that a lot of people are after. And

114
00:07:48,725 --> 00:07:51,845
Speaker 7:  so we, we've talked about this before with like there, there was a run on

115
00:07:51,845 --> 00:07:55,685
Speaker 7:  like the Nikon Cool picks from 2004 Yeah. For a long time. But

116
00:07:55,685 --> 00:07:58,245
Speaker 7:  it's very weird that it's this particular camera.

117
00:07:58,435 --> 00:08:01,885
Speaker 6:  Yeah. The Nikon cool picks is like very cheap, right? This is an

118
00:08:01,975 --> 00:08:05,685
Speaker 6:  $800 camera normally, right? And right now the

119
00:08:05,755 --> 00:08:09,525
Speaker 6:  secondary market or the demand for It is so high that

120
00:08:09,905 --> 00:08:12,285
Speaker 6:  on Amazon it's like over a thousand dollars. Well

121
00:08:12,285 --> 00:08:15,045
Speaker 7:  That's the thing. It's weird that it's like this, this would actually make

122
00:08:15,045 --> 00:08:18,525
Speaker 7:  more sense to me if this camera was from 2009 than from

123
00:08:18,525 --> 00:08:22,285
Speaker 7:  2019. Do you know what I mean? Like, it's, it's, it doesn't satisfy

124
00:08:23,185 --> 00:08:27,085
Speaker 7:  the, this is a sort of pre-internet digital camera. And that's fun

125
00:08:27,085 --> 00:08:31,025
Speaker 7:  to play with, with my high school friends. It's also too expensive

126
00:08:31,025 --> 00:08:34,705
Speaker 7:  to, and like for $1,500 you can buy a way better camera than the G seven

127
00:08:34,825 --> 00:08:38,585
Speaker 7:  X. Like, do not let TikTok lie to you. You can do much, much

128
00:08:38,585 --> 00:08:42,505
Speaker 7:  better than a G seven X mark three. So It is, it's very strange to me that

129
00:08:42,505 --> 00:08:45,465
Speaker 7:  It is this camera in particular that people are obsessed with.

130
00:08:45,645 --> 00:08:46,585
Speaker 6:  It is a phenomenon.

131
00:08:46,655 --> 00:08:47,505
Speaker 7:  Yeah. Oh, it really is.

132
00:08:47,645 --> 00:08:51,625
Speaker 6:  We looked at an unboxing TikTok that has almost a million likes Oh

133
00:08:51,625 --> 00:08:54,985
Speaker 6:  my gosh. Of a 5-year-old camera. And this unboxing was just recently posted.

134
00:08:55,625 --> 00:08:58,365
Speaker 6:  So I don't know what's going on if you know what's going on. This is a gadget

135
00:08:58,365 --> 00:09:01,325
Speaker 6:  mystery. Like I said, we're reporting on it. We're gonna, you know, talk

136
00:09:01,325 --> 00:09:04,805
Speaker 6:  to people who are using it and figure out if Target is actually restocking

137
00:09:05,155 --> 00:09:08,645
Speaker 6:  like six G, seven x mark threes at 5:00 AM every morning, which would be

138
00:09:08,845 --> 00:09:12,485
Speaker 6:  just deeply hilarious. So we're, we're poking at it. If you know anything

139
00:09:12,665 --> 00:09:16,585
Speaker 6:  or you have one or you're, you know, just

140
00:09:16,585 --> 00:09:19,945
Speaker 6:  like foaming at the mouth trying to get a 5-year-old camera, like let us

141
00:09:19,945 --> 00:09:23,745
Speaker 6:  know. Hit up the, hit up the hotline, send us an email. Like I'm,

142
00:09:23,765 --> 00:09:27,065
Speaker 6:  I'm sort of very curious about it. 'cause it, you are right. Sometimes old

143
00:09:27,065 --> 00:09:30,625
Speaker 6:  gadgets come back and certainly cameras go through these waves,

144
00:09:31,355 --> 00:09:35,265
Speaker 6:  right? Like X one hundreds. This is a thing like Right.

145
00:09:35,335 --> 00:09:38,305
Speaker 6:  It's hard to get them, but those are kind of bleeding edge. They're hard

146
00:09:38,305 --> 00:09:41,945
Speaker 6:  to manufacture. This is a five-year-old camera that Canon should just be

147
00:09:41,945 --> 00:09:42,905
Speaker 6:  able to fart out at will.

148
00:09:43,525 --> 00:09:47,345
Speaker 7:  And if memory serves, this was not like a set the world

149
00:09:47,345 --> 00:09:51,065
Speaker 7:  on fire kind of camera. Like the, that not 100 makes sense to me. Right.

150
00:09:51,065 --> 00:09:54,465
Speaker 7:  That is like an unusual camera with a particular style that people really

151
00:09:54,465 --> 00:09:58,305
Speaker 7:  gravitate to. Like that I fully understand. Even like the RX 100 was like

152
00:09:58,325 --> 00:10:01,825
Speaker 7:  the first of this kind of thing. And so it got a lot of shine for it. And

153
00:10:01,895 --> 00:10:05,465
Speaker 7:  they've had got what, seven or eight revs of that thing since like, I

154
00:10:05,745 --> 00:10:08,505
Speaker 6:  Honestly think, I think one of the answers why the canon is succeeding right

155
00:10:08,505 --> 00:10:11,945
Speaker 6:  now is it's impossible to understand which RX 100 to buy.

156
00:10:12,615 --> 00:10:16,105
Speaker 6:  They sell all of them at once and then the zv ones also exist right next

157
00:10:16,105 --> 00:10:19,465
Speaker 6:  to them, which are the same camera in a shittier case for cheaper with better

158
00:10:19,465 --> 00:10:23,185
Speaker 6:  video features. And none of that makes sense to anyone. And the only difference

159
00:10:23,185 --> 00:10:26,465
Speaker 6:  is between like the four and the five and the six and the seven

160
00:10:27,125 --> 00:10:30,825
Speaker 6:  is like focal lengths of lenses. And so like

161
00:10:31,265 --> 00:10:34,705
Speaker 6:  I, any normal person just like looks at the available set of RX one hundreds

162
00:10:34,705 --> 00:10:37,505
Speaker 6:  and is like, nope. And then someone's like, have you thought about the G

163
00:10:37,505 --> 00:10:38,825
Speaker 6:  seven X mark three? And it's like right there.

164
00:10:38,965 --> 00:10:40,625
Speaker 7:  You're like, thank God I'll have it. Yeah.

165
00:10:40,735 --> 00:10:43,665
Speaker 6:  Like this, this is way simpler than whatever. So did. Yeah. Anyway, it's

166
00:10:43,665 --> 00:10:47,585
Speaker 6:  a mystery. It, I will, I drove our team crazy with

167
00:10:47,585 --> 00:10:50,345
Speaker 6:  this mystery this week because I was like, why is this camera going viral?

168
00:10:50,365 --> 00:10:52,265
Speaker 6:  And they were, I've already had all these reactions. Well, we're gonna do

169
00:10:52,265 --> 00:10:54,945
Speaker 6:  it. We're gonna do, we're gonna figure out this story, let us know. I'm,

170
00:10:54,945 --> 00:10:56,425
Speaker 6:  it's, it's like a culture story. It's not

171
00:10:56,425 --> 00:10:58,825
Speaker 7:  A tech story. Yeah. So there is, I should just say before we move on that

172
00:10:58,825 --> 00:11:02,545
Speaker 7:  like, there's a, there's a sort of truism in journalism that like news

173
00:11:02,685 --> 00:11:06,545
Speaker 7:  is whatever your editor cares about that day, there's a real thing

174
00:11:06,545 --> 00:11:09,945
Speaker 7:  that happens in our Slack in particular where Neli will just parachute in

175
00:11:09,945 --> 00:11:12,985
Speaker 7:  after hanging out on TikTok for an hour and be like, the kids are into this.

176
00:11:13,005 --> 00:11:16,865
Speaker 7:  What's that about? And it, it brings up some of my

177
00:11:16,865 --> 00:11:18,265
Speaker 7:  very favorite stories that we do.

178
00:11:18,545 --> 00:11:22,065
Speaker 6:  As you can imagine, my TikTok for you algorithm is ludicrous.

179
00:11:22,255 --> 00:11:26,145
Speaker 6:  It's like trucks jumping over stuff. We're doing a story on like AI

180
00:11:26,145 --> 00:11:29,905
Speaker 6:  image enhancers. So now it's just weird AI porn that makes me feel bad

181
00:11:29,905 --> 00:11:33,665
Speaker 6:  about myself. Like literally like we've done

182
00:11:33,945 --> 00:11:37,425
Speaker 6:  searches for some of these tools that are available and

183
00:11:37,765 --> 00:11:41,465
Speaker 6:  I'm just sending some of the worst shit in the world to these songs. So

184
00:11:41,465 --> 00:11:45,265
Speaker 6:  that's really bad. And then it's like, ca like 5-year-old cameras that have

185
00:11:45,265 --> 00:11:49,025
Speaker 6:  gone viral. I need to reset my a maybe they're gonna ban it in time

186
00:11:50,155 --> 00:11:52,055
Speaker 6:  for me to regain some semblance of

187
00:11:52,255 --> 00:11:55,485
Speaker 7:  Identity. Yeah. What you need is for it to go dark, new algorithm,

188
00:11:56,305 --> 00:11:57,765
Speaker 7:  new neli, just start it all over.

189
00:11:58,085 --> 00:12:01,245
Speaker 6:  I wanna be a different person. I just, I'd like to get back to Trump's jumping

190
00:12:01,245 --> 00:12:04,485
Speaker 6:  over stuff. Please. That feels good. It's gone. All right. There's actual

191
00:12:04,485 --> 00:12:08,325
Speaker 6:  gadget news this week, not just 5-year-old mysteries. What's sort with

192
00:12:08,525 --> 00:12:10,805
Speaker 6:  Facebook, which is also in an identity crisis.

193
00:12:12,355 --> 00:12:16,125
Speaker 6:  Have I told you my, my deepest theory of meta as a company

194
00:12:16,125 --> 00:12:20,045
Speaker 6:  right now? No. Which is Mark Zuckerberg just desperately wants

195
00:12:20,045 --> 00:12:20,805
Speaker 6:  to beat Elon Musk.

196
00:12:21,385 --> 00:12:22,165
Speaker 7:  Oh, interesting.

197
00:12:22,335 --> 00:12:25,645
Speaker 6:  Right? Like Elon has everything Mark wants, like he's got the president,

198
00:12:25,825 --> 00:12:29,805
Speaker 6:  he gets to say stuff whenever he wants

199
00:12:29,905 --> 00:12:33,445
Speaker 6:  and people are scared of him. He's got fans, like

200
00:12:33,505 --> 00:12:37,325
Speaker 6:  actual fans. He's the rockets and cars guy, whether you like the rockets

201
00:12:37,325 --> 00:12:40,965
Speaker 6:  and cars or not. Like some people perceive him to be deeply evil and some

202
00:12:40,965 --> 00:12:43,685
Speaker 6:  people perceive him to be ruling the world. And Mark Zuckerberg just has

203
00:12:43,685 --> 00:12:47,665
Speaker 6:  like a gold chain and mostly people yelling at him.

204
00:12:47,665 --> 00:12:49,825
Speaker 6:  And I don't think he has the other set of things.

205
00:12:50,605 --> 00:12:54,105
Speaker 7:  That's interesting. And he is, I mean, it, it's a, it's a funny segue into

206
00:12:54,305 --> 00:12:57,585
Speaker 7:  Facebook because like going all the way back to the beginning of Facebook,

207
00:12:58,095 --> 00:13:02,065
Speaker 7:  mark Zuckerberg has always been like big and powerful, but never cool. Yeah.

208
00:13:02,065 --> 00:13:05,945
Speaker 7:  Like even when Facebook ruled the world, Twitter was cooler. It was

209
00:13:05,945 --> 00:13:09,065
Speaker 7:  like culturally central in a way that Facebook never was.

210
00:13:09,335 --> 00:13:12,905
Speaker 6:  Twitter was cooler, Instagram was cool, but Instagram had distance.

211
00:13:13,415 --> 00:13:17,025
Speaker 7:  Yeah. And I weirdly, I don't think people associate Mark with

212
00:13:17,025 --> 00:13:20,865
Speaker 7:  Instagram the same way. Like, do you remember I, this was years

213
00:13:20,865 --> 00:13:23,705
Speaker 7:  ago at this point, so I'm sure it's different now, but there was a pretty

214
00:13:23,965 --> 00:13:27,865
Speaker 7:  strong subset of people in a study that they did that had no idea that Facebook

215
00:13:27,865 --> 00:13:28,385
Speaker 7:  owned Instagram.

216
00:13:28,405 --> 00:13:29,985
Speaker 6:  Oh yeah, we did that. We did that. That was us.

217
00:13:30,045 --> 00:13:30,945
Speaker 7:  Oh, that was us. There you go.

218
00:13:31,255 --> 00:13:32,585
Speaker 6:  Cool. Ages ago. It's a good study.

219
00:13:32,615 --> 00:13:36,305
Speaker 7:  Love that. Yeah. But that is like, I don't think, I don't think he got the

220
00:13:36,305 --> 00:13:39,825
Speaker 7:  shine from Instagram in the way that he would've wanted to.

221
00:13:41,325 --> 00:13:43,825
Speaker 7:  And so, and there was a minute where he was like, he thought Threads was

222
00:13:43,825 --> 00:13:46,945
Speaker 7:  gonna make him really shiny. And then he was like, I'm gonna be a personality.

223
00:13:46,945 --> 00:13:50,825
Speaker 7:  And then Elon Musk just ate all of that airspace completely out away

224
00:13:50,825 --> 00:13:54,635
Speaker 7:  from him. That's a good theory. I feel like if I mark, I want

225
00:13:54,665 --> 00:13:58,555
Speaker 7:  that less and less every single day, but I don't know.

226
00:13:58,905 --> 00:14:01,875
Speaker 6:  Yeah. The baggage of that is enormous, right? Yeah.

227
00:14:03,175 --> 00:14:07,055
Speaker 6:  But you kind of look at a bunch of these moves. We, we'll talk

228
00:14:07,055 --> 00:14:10,335
Speaker 6:  about the, what they're doing with Facebook, the Blue app proper, but you're

229
00:14:10,335 --> 00:14:14,115
Speaker 6:  like, oh man, A lot of this is like positioning him as an inventor or like

230
00:14:14,175 --> 00:14:17,675
Speaker 6:  the product person, not the guy who just steals all the ideas from Snapchat.

231
00:14:18,665 --> 00:14:19,765
Speaker 6:  All right, what are they doing with Facebook?

232
00:14:20,345 --> 00:14:24,125
Speaker 7:  So it was, was it last fall or earlier

233
00:14:24,195 --> 00:14:27,485
Speaker 7:  this year that Mark first was like, we want to get back to

234
00:14:28,165 --> 00:14:31,845
Speaker 7:  OG Facebook. We wanna bring back the thing that makes Facebook great, which

235
00:14:31,845 --> 00:14:35,725
Speaker 7:  is people hanging out with their friends. That was a long time

236
00:14:35,745 --> 00:14:38,565
Speaker 7:  ago, and I'm not sure that was ever actually what made Facebook great, but

237
00:14:38,565 --> 00:14:41,765
Speaker 7:  that was like, there is that, that is the perception of old Facebook. And

238
00:14:41,765 --> 00:14:45,285
Speaker 7:  they're making the first turn of that now, which is

239
00:14:45,285 --> 00:14:48,805
Speaker 7:  essentially they've added a tab to Facebook

240
00:14:49,545 --> 00:14:52,685
Speaker 7:  so far, at least for me, it's just in the mobile app, not on the website

241
00:14:53,635 --> 00:14:57,525
Speaker 7:  that It is just your friends and it gets rid of all of the other,

242
00:14:57,825 --> 00:15:01,685
Speaker 7:  you know, publications and AI slap and all of the like, recommended

243
00:15:01,715 --> 00:15:05,325
Speaker 7:  crap that is all over everybody's feeds now and is designed to just be, it's

244
00:15:05,325 --> 00:15:08,885
Speaker 7:  literally called a Friends tab. And It is designed to just be the people

245
00:15:08,885 --> 00:15:12,845
Speaker 7:  you are friends with on Facebook, which for me is a very funny experience

246
00:15:12,845 --> 00:15:16,725
Speaker 7:  because none of my friends on Facebook are still using Facebook except the

247
00:15:16,725 --> 00:15:20,565
Speaker 7:  people who need it for like branding reasons. Yeah. Like

248
00:15:20,565 --> 00:15:23,965
Speaker 7:  it's, if you wanna like be an influencer on Facebook, you keep posting to

249
00:15:24,125 --> 00:15:26,845
Speaker 7:  Facebook. So I like, I have a few friends who are like outing themselves

250
00:15:28,115 --> 00:15:32,085
Speaker 7:  with like many, you know, large albums of photos of them and

251
00:15:32,085 --> 00:15:35,725
Speaker 7:  their family at Disney World. But other than that, my Friends tab is totally

252
00:15:35,795 --> 00:15:39,525
Speaker 7:  dead. But this is like, this is the first turn in, I think

253
00:15:39,665 --> 00:15:43,545
Speaker 7:  trying to make Facebook in particular, the thing, it was

254
00:15:43,625 --> 00:15:47,305
Speaker 7:  a long time ago and I'm just not convinced A, that's gonna work.

255
00:15:47,565 --> 00:15:50,625
Speaker 7:  And b, that's even possible. Like, it just feels like we've left that idea

256
00:15:50,625 --> 00:15:53,865
Speaker 7:  of like, I'm hanging out with my friends on a social network. We left that

257
00:15:53,865 --> 00:15:55,105
Speaker 7:  behind a long time ago, didn't we?

258
00:15:55,425 --> 00:15:59,305
Speaker 6:  I think we did. But this is, this is, by the way, I'm gonna relate this to

259
00:15:59,455 --> 00:15:59,745
Speaker 6:  Elon.

260
00:16:01,385 --> 00:16:05,325
Speaker 6:  Elon made X into whatever cesspool It

261
00:16:05,325 --> 00:16:09,265
Speaker 6:  is, but It is very influential in its tiny sphere.

262
00:16:09,665 --> 00:16:12,785
Speaker 6:  I actually believe that X is less influential outside of its sphere.

263
00:16:13,695 --> 00:16:17,585
Speaker 6:  Like it used to be influential in culture. It used to be influential in sports

264
00:16:17,585 --> 00:16:21,345
Speaker 6:  media. It used to be influential in tech. Now, now it's kind of influential

265
00:16:21,365 --> 00:16:23,785
Speaker 6:  in MAGA politics and that's it. But It is

266
00:16:23,785 --> 00:16:25,345
Speaker 7:  Very influential in that world.

267
00:16:25,645 --> 00:16:29,345
Speaker 6:  It is, it it It is expanded, its influence in, in one way and reduced it

268
00:16:29,345 --> 00:16:32,625
Speaker 6:  in another way. And that's a trade. And you can have a lot of feelings about

269
00:16:32,625 --> 00:16:35,385
Speaker 6:  that trade and I certainly do. But It is a trade. You can just look at it

270
00:16:35,385 --> 00:16:38,385
Speaker 6:  and evaluate it however you want. But people are posting to it,

271
00:16:39,365 --> 00:16:43,075
Speaker 6:  right? Like that's a still a thing that's happening. Yeah. Like people are

272
00:16:43,075 --> 00:16:46,675
Speaker 6:  posting to it, they're sharing stuff. It is not a creator

273
00:16:47,155 --> 00:16:50,755
Speaker 6:  platform. And like it has creator platform dynamics. It doesn't have any

274
00:16:50,755 --> 00:16:53,755
Speaker 6:  revenue. So it's like, it hasn't been gamed quite in that way.

275
00:16:55,195 --> 00:16:58,635
Speaker 6:  Facebook is turn turned into a creator platform. This is what you're saying

276
00:16:58,635 --> 00:17:01,475
Speaker 6:  about people are acting like influencers on it, right? Like it's all pages.

277
00:17:02,025 --> 00:17:05,435
Speaker 6:  Most of the individual activity happens in groups, like every school,

278
00:17:05,845 --> 00:17:09,835
Speaker 6:  every parents group at every school's on Facebook in groups, and

279
00:17:09,835 --> 00:17:12,635
Speaker 6:  no one's just like posting their regular stuff. Right? And so this turned

280
00:17:12,665 --> 00:17:16,595
Speaker 6:  back to, oh, I just want people to share with their friends is like, it's

281
00:17:16,595 --> 00:17:20,475
Speaker 6:  a product decision, but it's also a where do you post decision?

282
00:17:20,865 --> 00:17:24,345
Speaker 6:  Like where should this stuff live? And I

283
00:17:24,825 --> 00:17:28,625
Speaker 6:  I, you get the sense that like if Facebook turns into

284
00:17:29,515 --> 00:17:33,465
Speaker 6:  Yahoo email lists, that's the end of that. You know,

285
00:17:33,465 --> 00:17:33,705
Speaker 6:  like,

286
00:17:34,135 --> 00:17:34,425
Speaker 7:  Yeah.

287
00:17:34,565 --> 00:17:37,465
Speaker 6:  If it's just where people go to complain about their car, like I'm in a lot

288
00:17:37,465 --> 00:17:41,105
Speaker 6:  of car groups on Facebook and it's just people being like, the tires fell

289
00:17:41,105 --> 00:17:42,505
Speaker 6:  off my car. Right. Does

290
00:17:42,505 --> 00:17:45,025
Speaker 7:  This happen to anyone else? And that's the thing, Facebook I think is, is

291
00:17:45,025 --> 00:17:48,665
Speaker 7:  desperately trying to rectify here, right? Like groups is still hugely

292
00:17:48,665 --> 00:17:52,045
Speaker 7:  successful. Marketplace by all accounts is hugely successful. Yeah.

293
00:17:52,665 --> 00:17:56,435
Speaker 7:  But the like core Facebook thing is just

294
00:17:56,585 --> 00:17:58,635
Speaker 7:  kind of gone. And I feel like for me,

295
00:18:00,695 --> 00:18:04,615
Speaker 7:  I think I am me plus like plus or minus four

296
00:18:04,615 --> 00:18:08,575
Speaker 7:  years I would say. So like sort of our age range and a little younger was

297
00:18:08,575 --> 00:18:11,975
Speaker 7:  like, that was the Facebook generation, right? Like there are albums that

298
00:18:12,015 --> 00:18:15,855
Speaker 7:  I posted that are just like 125 pictures of me drunk because I

299
00:18:15,855 --> 00:18:19,415
Speaker 7:  took every picture on my camera and put it on Facebook and tagged all my

300
00:18:19,415 --> 00:18:23,135
Speaker 7:  friends. And that was like peak Facebook that's

301
00:18:23,245 --> 00:18:27,215
Speaker 7:  gone. Like we learned the hard way that that is a

302
00:18:27,495 --> 00:18:30,575
Speaker 7:  terrible way to interact online with our friends. And that there are all

303
00:18:30,575 --> 00:18:33,815
Speaker 7:  kinds of problems that come with that. And that like, the whole

304
00:18:34,325 --> 00:18:38,215
Speaker 7:  idea of sort of harmless low stakes hanging out with your friends on

305
00:18:38,375 --> 00:18:41,655
Speaker 7:  Facebook is just wrong. Younger generations never learned that in the first

306
00:18:41,655 --> 00:18:44,335
Speaker 7:  place, right? Like they, they gravitated to Snapchat because stuff disappears.

307
00:18:44,335 --> 00:18:47,495
Speaker 7:  Because it doesn't stick around because It is this like ephemeral thing.

308
00:18:47,785 --> 00:18:51,695
Speaker 7:  Group chats have become really important. Like to me, the, the very idea

309
00:18:51,875 --> 00:18:55,695
Speaker 7:  of like posting what I'm doing that day on Facebook so my

310
00:18:55,695 --> 00:18:58,815
Speaker 7:  friends can see it just feels weird now. Yeah. Like I don't, that muscle

311
00:18:58,875 --> 00:19:01,135
Speaker 7:  is gone and I don't think it's ever coming back for people.

312
00:19:01,395 --> 00:19:04,255
Speaker 6:  And that's what I mean by just like turning this product knob, like

313
00:19:05,005 --> 00:19:08,895
Speaker 6:  wanting to be influential in this specific way. You can

314
00:19:08,895 --> 00:19:12,295
Speaker 6:  see that it's getting away from like Threads is not that platform they lost

315
00:19:12,295 --> 00:19:15,615
Speaker 6:  to Blue Sky in a very real world. They have a much bigger number. More people

316
00:19:15,615 --> 00:19:18,375
Speaker 6:  are using it 'cause of the juiced against their entire graph. But like

317
00:19:19,365 --> 00:19:23,055
Speaker 6:  Blue Sky is where a whole conversation is happening. X is where a whole conversation

318
00:19:23,055 --> 00:19:26,895
Speaker 6:  is happening. Yeah. Instagram is a creator platform.

319
00:19:27,125 --> 00:19:30,535
Speaker 6:  Like I, I know lots of regular people who just like do stories or whatever,

320
00:19:30,995 --> 00:19:34,375
Speaker 6:  but the vast majority of the content is creators doing stuff that gets shared

321
00:19:34,375 --> 00:19:38,095
Speaker 6:  into a group chat or a dm. And then that's, that's the

322
00:19:38,095 --> 00:19:41,935
Speaker 6:  action there. So you just see like, oh, these platforms are really

323
00:19:41,935 --> 00:19:45,535
Speaker 6:  big, but they're losing influence in a very specific way. And I'm telling

324
00:19:45,535 --> 00:19:49,215
Speaker 6:  you, there's, there's jealousy there that I think is, you can just see it.

325
00:19:49,235 --> 00:19:50,655
Speaker 6:  He really needs the metaverse to work.

326
00:19:51,365 --> 00:19:55,215
Speaker 7:  Yeah, no, I I I buy it. And I think what's weird about Facebook is it

327
00:19:55,215 --> 00:19:59,135
Speaker 7:  seemed like Facebook's fate was to be sort of the,

328
00:19:59,395 --> 00:20:03,295
Speaker 7:  the overall repository for everything, right? Like you make a reel

329
00:20:03,295 --> 00:20:06,095
Speaker 7:  on Instagram and it, it accrues back to Facebook and becomes a thing. But

330
00:20:06,095 --> 00:20:09,415
Speaker 7:  you didn't make it for Facebook. Yeah. Threads posts show up on Instagram

331
00:20:09,415 --> 00:20:12,855
Speaker 7:  and Facebook. So it was like Facebook became this thing with a huge

332
00:20:13,765 --> 00:20:17,455
Speaker 7:  kind of unengaged user base that is doing other stuff on the platform. And

333
00:20:17,695 --> 00:20:21,595
Speaker 7:  then you just show them a bunch of content and that it's not an exciting

334
00:20:21,595 --> 00:20:24,955
Speaker 7:  product, but as a product strategy actually kind of makes sense to me. And

335
00:20:24,955 --> 00:20:28,635
Speaker 7:  so the only reason I can think that this friends thing is so

336
00:20:28,635 --> 00:20:31,195
Speaker 7:  important is because it's important to Mark for exactly the reasons you're

337
00:20:31,195 --> 00:20:35,155
Speaker 7:  describing. It's like, It is, It is the thing people like about

338
00:20:35,475 --> 00:20:39,275
Speaker 7:  Facebook. Yeah. Not the thing that people tolerate in order to get to

339
00:20:39,425 --> 00:20:43,395
Speaker 7:  marketplace. And like, especially as they, they continue

340
00:20:43,395 --> 00:20:46,755
Speaker 7:  to promote AI stuff all over the feed. They continue to juice

341
00:20:46,895 --> 00:20:50,045
Speaker 7:  engagement and publisher stuff like the, the,

342
00:20:50,675 --> 00:20:54,525
Speaker 7:  they have to have something that people like about being on Facebook or it's

343
00:20:54,525 --> 00:20:58,005
Speaker 7:  just gonna keep getting worse. And it really does feel like, this is Mark

344
00:20:58,025 --> 00:21:01,445
Speaker 7:  in particular saying, this is what I want back. Yeah. It's like that he is

345
00:21:01,445 --> 00:21:03,605
Speaker 7:  nostalgic for this thing so he can build it into Facebook.

346
00:21:04,245 --> 00:21:07,685
Speaker 6:  I can't wait to start posting ai. I slap to all my friends. I'm just gonna

347
00:21:07,705 --> 00:21:11,565
Speaker 6:  AI generate some new children. Like here they're first day of school for

348
00:21:11,565 --> 00:21:12,365
Speaker 6:  my AI generated

349
00:21:12,365 --> 00:21:13,565
Speaker 7:  Baby. That, that would do numbers.

350
00:21:13,715 --> 00:21:17,285
Speaker 6:  Huge engagement. Did I mention this last week? Last week there was the Nvidia

351
00:21:17,285 --> 00:21:20,285
Speaker 6:  conference we talked about a little bit and there was one segment in it that

352
00:21:20,365 --> 00:21:24,085
Speaker 6:  I think will form the basis of yet more editor in chief assigning

353
00:21:24,205 --> 00:21:27,125
Speaker 6:  stories in Slack. 'cause he saw, oh, goodie. But Nvidia is part of their

354
00:21:27,125 --> 00:21:31,045
Speaker 6:  conference demoed this thing that I've been calling Infinite Creative. I

355
00:21:31,045 --> 00:21:34,845
Speaker 6:  think they actually titled it Infinite Creative. See ad agencies make creative,

356
00:21:35,415 --> 00:21:38,685
Speaker 6:  right? That's, that's what Mad Men does. They're like, here's an ad. Like

357
00:21:38,685 --> 00:21:41,965
Speaker 6:  here's the ad, the creative of an ad and the creative of ad is like

358
00:21:42,635 --> 00:21:46,565
Speaker 6:  made of components. It's made of photos of the product, it's made of tagline

359
00:21:46,585 --> 00:21:50,335
Speaker 6:  and copy. And then there's targeting, right? So like

360
00:21:50,755 --> 00:21:54,495
Speaker 6:  you send it, you open Facebook Ads Manager and you're like

361
00:21:54,975 --> 00:21:58,935
Speaker 6:  18 to 30 5-year-old men who like shampoo get 'em. Right? You

362
00:21:58,935 --> 00:22:00,535
Speaker 6:  know, and like it shows people those ads

363
00:22:02,115 --> 00:22:05,975
Speaker 6:  Nvidia demoed infinite creative. So they th they AI

364
00:22:06,215 --> 00:22:10,015
Speaker 6:  generate the shampoo bottle. Then they AI generate the copy. They AI

365
00:22:10,175 --> 00:22:13,335
Speaker 6:  generate the tagline. They AI generate the photo shoot of the shampoo bottle.

366
00:22:13,915 --> 00:22:17,455
Speaker 6:  And then they, when you say target these people, it starts remixing all of

367
00:22:17,455 --> 00:22:20,975
Speaker 6:  the elements in an infinite way until it starts converting.

368
00:22:21,315 --> 00:22:25,245
Speaker 6:  So everyone gets their own ad and like if you thought AI slap on

369
00:22:25,445 --> 00:22:29,205
Speaker 6:  Facebook was bad, now wait until there's real money

370
00:22:29,205 --> 00:22:32,905
Speaker 6:  behind it and it's already here. Like they're, that this is why these big

371
00:22:33,145 --> 00:22:36,985
Speaker 6:  companies like Google and and Meta in particular are chasing AI video generation

372
00:22:37,005 --> 00:22:40,865
Speaker 6:  so hard, right? Because that's the promise is as you're scrolling

373
00:22:41,445 --> 00:22:45,025
Speaker 6:  an AI generated video ad will pop up that's perfectly targeted to you.

374
00:22:45,445 --> 00:22:46,705
Speaker 6:  And then you will buy the shampoo.

375
00:22:47,425 --> 00:22:51,185
Speaker 7:  I I wonder a lot about whether that actually becomes

376
00:22:51,365 --> 00:22:55,085
Speaker 7:  the thing that sort of viscerally turns

377
00:22:55,085 --> 00:22:58,845
Speaker 7:  everybody against ad targeting. 'cause until now it's been like,

378
00:22:59,505 --> 00:23:01,925
Speaker 7:  you know, we're gonna serve you better ads. And I I think

379
00:23:02,255 --> 00:23:04,045
Speaker 6:  Until now it's been, are you listening to me?

380
00:23:04,355 --> 00:23:08,245
Speaker 7:  Well, fair. But like the, the promise of ad targeting

381
00:23:08,245 --> 00:23:10,965
Speaker 7:  is we will, we will send you an ad for something you want instead of something

382
00:23:10,965 --> 00:23:14,885
Speaker 7:  you don't. Right. Which is fine, but I don't think people actually want that.

383
00:23:14,945 --> 00:23:18,405
Speaker 7:  But like, if you're gonna serve me an ad, serve me a good one. Sure. But

384
00:23:18,405 --> 00:23:21,525
Speaker 7:  the idea of like an ad coming up in my feed that's like, Hey David, do you

385
00:23:21,525 --> 00:23:25,125
Speaker 7:  want these shoes? That's it's coming frightening. And like that,

386
00:23:25,315 --> 00:23:29,245
Speaker 7:  that makes a thing plain to people in a way that I think has not

387
00:23:29,245 --> 00:23:33,045
Speaker 7:  been clear before. Like, that is all the data about me that you have

388
00:23:33,135 --> 00:23:37,045
Speaker 7:  being spoken aloud to me and that feels really different from, oh,

389
00:23:37,045 --> 00:23:40,885
Speaker 7:  this ad is a thing I like. Yeah. Do you know what I mean? There's, it connects

390
00:23:40,885 --> 00:23:43,925
Speaker 7:  different neurons somehow in a, in a way that I think might backfire.

391
00:23:44,525 --> 00:23:47,805
Speaker 6:  I would just argue that it already has ad targeting either works

392
00:23:48,545 --> 00:23:52,485
Speaker 6:  so well that you think Mark Zuckerberg is listening to you or

393
00:23:52,485 --> 00:23:55,845
Speaker 6:  it works so poorly that it shows you an ad for something you've already bought.

394
00:23:55,915 --> 00:23:59,725
Speaker 6:  Like there's no middle ground. That's everyone's experience to with advertising

395
00:23:59,725 --> 00:24:03,605
Speaker 6:  today. Right. And most people think Mark Zuckerberg is listening to you

396
00:24:03,985 --> 00:24:07,365
Speaker 6:  and it's like, well not really. 'cause you you bought those shoes

397
00:24:08,105 --> 00:24:10,005
Speaker 6:  and he was like, show 'em more shoes.

398
00:24:12,315 --> 00:24:13,805
Speaker 6:  Like I don't think he's listening to you closely

399
00:24:13,905 --> 00:24:14,805
Speaker 7:  So he closely enough

400
00:24:14,805 --> 00:24:18,085
Speaker 6:  So he like shoes, but sure. Like he he's listening to you. This is gonna

401
00:24:18,235 --> 00:24:21,685
Speaker 6:  turn the he's listening knob all the way like

402
00:24:21,685 --> 00:24:22,445
Speaker 7:  Yeah, that's fair.

403
00:24:22,945 --> 00:24:26,685
Speaker 6:  All the way up to not only is he listening to me, he's talking back to me.

404
00:24:26,935 --> 00:24:27,285
Speaker 6:  Right.

405
00:24:27,795 --> 00:24:29,485
Speaker 7:  Mark Zuckerberg is in my house. Yeah.

406
00:24:29,765 --> 00:24:32,525
Speaker 6:  Yeah. And maybe, you know, and you make all the arguments for why they're

407
00:24:32,525 --> 00:24:35,005
Speaker 6:  not listening. Like Apple would never allow it. It's actually easier to do

408
00:24:35,005 --> 00:24:38,285
Speaker 6:  the, the data fusion about what wifi network you were on at what time and

409
00:24:38,285 --> 00:24:41,765
Speaker 6:  what location because you're not as special as you think, which is really

410
00:24:41,765 --> 00:24:44,325
Speaker 6:  the core argument to they're not listening to you is you're not as special

411
00:24:44,325 --> 00:24:45,045
Speaker 6:  as you think you are.

412
00:24:46,675 --> 00:24:50,565
Speaker 6:  Like every other 18-year-old in this city Googled these shoes

413
00:24:50,625 --> 00:24:54,525
Speaker 6:  at this time. Like we we just guessed that you probably did too. Yeah.

414
00:24:55,105 --> 00:24:58,125
Speaker 6:  And that's, that's the heart of the argument is it's simpler to guess Right.

415
00:24:58,125 --> 00:25:01,805
Speaker 6:  And be right some of the time. This is gonna be something else entirely.

416
00:25:01,805 --> 00:25:05,445
Speaker 6:  Yeah. The all the cost curves have, but Nvidia did demo.

417
00:25:05,765 --> 00:25:09,285
Speaker 6:  Infinite creative TikTok has demoed variations on infinite creative.

418
00:25:09,835 --> 00:25:13,805
Speaker 6:  It's coming. And I, I think some of this push to do friends again is very

419
00:25:13,805 --> 00:25:17,565
Speaker 6:  much, we need more surface area for these experiments

420
00:25:18,075 --> 00:25:19,645
Speaker 6:  that is out of the creator's end. They

421
00:25:19,645 --> 00:25:22,845
Speaker 7:  Need people to look at their feeds again. And if I'm in, if I'm in marketplace

422
00:25:23,065 --> 00:25:27,045
Speaker 7:  and groups, I'm not looking at my feeds. Yeah. That's like, it's that's

423
00:25:27,045 --> 00:25:27,445
Speaker 7:  a real thing.

424
00:25:28,455 --> 00:25:31,545
Speaker 6:  It's coming. It's, I we're gonna do a lot of stories in this stuff over the

425
00:25:31,545 --> 00:25:35,465
Speaker 6:  coming year, but you We'll, I'll, I'll link, I'll put a link

426
00:25:35,465 --> 00:25:39,305
Speaker 6:  to this TikTok in the show notes. How's that? Just like everyone else

427
00:25:39,325 --> 00:25:42,865
Speaker 6:  can experience the absolute insanity of my TikTok feed. We'll do it in the

428
00:25:42,865 --> 00:25:43,385
Speaker 6:  show notes by

429
00:25:43,385 --> 00:25:47,105
Speaker 7:  2027. We're gonna have a version of this podcast that is individualized to

430
00:25:47,105 --> 00:25:49,345
Speaker 7:  every single person who listens to it. And

431
00:25:49,345 --> 00:25:50,625
Speaker 6:  It's gonna be, that is the Dream Sick

432
00:25:51,155 --> 00:25:51,505
Speaker 7:  Can't,

433
00:25:51,505 --> 00:25:54,265
Speaker 6:  That's our final subscription tier for

434
00:25:54,775 --> 00:25:57,665
Speaker 6:  $5 million a year. David will just call you, talk

435
00:25:57,665 --> 00:25:59,745
Speaker 7:  To your, does just make you a podcast. Yeah.

436
00:26:00,565 --> 00:26:04,105
Speaker 6:  All right. There's actual Gadget news, some, some a little stuff. A bunch

437
00:26:04,105 --> 00:26:08,025
Speaker 6:  of Apple stuff. Really. AirPods Max with USPC, they're updating it

438
00:26:08,045 --> 00:26:11,945
Speaker 6:  to support Lossless audio, which is way overdue like way, way

439
00:26:11,945 --> 00:26:15,225
Speaker 6:  overdue. And they're gonna cut some latency down. I have no idea why this

440
00:26:15,225 --> 00:26:15,705
Speaker 6:  took so long.

441
00:26:16,205 --> 00:26:19,985
Speaker 7:  It It is a strange one. And the AirPods max

442
00:26:20,085 --> 00:26:23,265
Speaker 7:  for me kind of goes in the category of products that Apple seems to forget

443
00:26:23,265 --> 00:26:27,065
Speaker 7:  about for years at a time. It's like the MAC Pro, the

444
00:26:27,135 --> 00:26:31,025
Speaker 7:  AirPods Max, the iPad mini. It's just like every, every 18

445
00:26:31,025 --> 00:26:34,345
Speaker 7:  months the company forgets it exists. And it was like, oh crap, we should

446
00:26:34,345 --> 00:26:38,225
Speaker 7:  do something with that. And here we are. But yeah, this one, they, they

447
00:26:38,225 --> 00:26:42,025
Speaker 7:  put back wired playback through USBC when they switched to the connector,

448
00:26:42,025 --> 00:26:45,045
Speaker 7:  they dropped wired playback, which I had forgotten about and was so weird

449
00:26:45,145 --> 00:26:48,725
Speaker 7:  at the time, you just couldn't play audio over a wire. Like

450
00:26:48,875 --> 00:26:52,445
Speaker 7:  just weird stuff. But now that's back. So that's good. And like you said,

451
00:26:52,445 --> 00:26:56,285
Speaker 7:  super low latency stuff and when we were hearing rumblings

452
00:26:56,285 --> 00:27:00,165
Speaker 7:  about there maybe being an announcement a along these lines,

453
00:27:00,365 --> 00:27:04,165
Speaker 7:  I was like, oh my God, are we gonna get new AirPods Max? I I

454
00:27:04,395 --> 00:27:07,765
Speaker 7:  want them. And that's very exciting. And what's interesting about this is

455
00:27:07,765 --> 00:27:11,605
Speaker 7:  this suggests to me that Apple is pretty happy with the

456
00:27:11,805 --> 00:27:15,605
Speaker 7:  hardware of the AirPods Max. And It is like this, this is very much

457
00:27:15,605 --> 00:27:19,525
Speaker 7:  like a meaningful software update on top of a

458
00:27:19,525 --> 00:27:22,965
Speaker 7:  piece of hardware. They have no intention of updating. Yeah. Imminently.

459
00:27:23,225 --> 00:27:26,985
Speaker 6:  So I wanna be clear here, the Lossless audio is only if you use the cable.

460
00:27:28,295 --> 00:27:29,105
Speaker 6:  Like I Oh

461
00:27:29,105 --> 00:27:29,665
Speaker 7:  Yeah, that's true.

462
00:27:30,005 --> 00:27:33,985
Speaker 6:  And so you get 24 bit 48 kilohertz lossless audio, you get personalized

463
00:27:34,045 --> 00:27:37,425
Speaker 6:  spa audio musicians cannot mix in it using these headphones. You have to

464
00:27:37,425 --> 00:27:41,065
Speaker 6:  use the cable, which is wild because Apple

465
00:27:41,135 --> 00:27:44,865
Speaker 6:  owns their Bluetooth stack. They own the chips on both sides of this. These

466
00:27:44,865 --> 00:27:48,025
Speaker 6:  headphones cost $550. What are we doing?

467
00:27:49,255 --> 00:27:52,705
Speaker 7:  Yeah, what are we doing? Did you also see the real reason I wanted to talk

468
00:27:52,705 --> 00:27:55,745
Speaker 7:  about It is I wanted to see if you had seen the new

469
00:27:56,865 --> 00:28:00,785
Speaker 7:  USBC to 3.5 millimeter cable that Apple is selling. It's very

470
00:28:00,785 --> 00:28:00,985
Speaker 6:  Nice.

471
00:28:01,695 --> 00:28:05,585
Speaker 7:  It's $40. Yep. So for four, for for $40,

472
00:28:05,725 --> 00:28:09,025
Speaker 7:  you can approximate having a headphone Jack. Again, congratulations to you.

473
00:28:10,565 --> 00:28:13,705
Speaker 7:  And It is, It is 1.2 meters, which is what, like four feet.

474
00:28:14,685 --> 00:28:17,865
Speaker 7:  You can just have a four foot long adapter cable.

475
00:28:18,665 --> 00:28:22,225
Speaker 7:  I think this is just for airplanes, right? Isn't that This is just for plugging

476
00:28:22,225 --> 00:28:23,545
Speaker 7:  into the, the seat back thing on

477
00:28:23,545 --> 00:28:25,265
Speaker 6:  The air. No, this is the thing you need for everything, for all these new

478
00:28:25,505 --> 00:28:28,145
Speaker 6:  features. So you're wanna plug this in your Mac and do Lossless

479
00:28:28,745 --> 00:28:31,585
Speaker 6:  personalized spatial audio mixing. Okay.

480
00:28:32,815 --> 00:28:33,095
Speaker 6:  I don't

481
00:28:33,095 --> 00:28:36,935
Speaker 7:  Know man, I'm calling this right now. iPhone 17 headphone Jack is back.

482
00:28:38,715 --> 00:28:42,645
Speaker 6:  Yeah, there we, there was a entire like blue

483
00:28:42,665 --> 00:28:45,565
Speaker 6:  sky kerfuffle about the headphone jack being in stratification. I was like,

484
00:28:45,565 --> 00:28:49,165
Speaker 6:  like I wrote about this to the iPhone seven. Like this has always been what's

485
00:28:49,165 --> 00:28:52,805
Speaker 6:  coming. They've compressed the audio and now pendulum swing back and people

486
00:28:52,805 --> 00:28:56,485
Speaker 6:  are like, what if my music sounded good? And the answer is Dongle Town.

487
00:28:58,385 --> 00:29:02,245
Speaker 6:  That's just the way it goes. And it's weird. And the the, the bad thing

488
00:29:02,485 --> 00:29:05,845
Speaker 6:  happened. I I, I made a prediction when I wrote that piece, taking the headphone

489
00:29:05,845 --> 00:29:08,685
Speaker 6:  jack off of phones as you draw also and stupid, which Apple hated at the

490
00:29:08,685 --> 00:29:10,485
Speaker 6:  time. I wanna be very clear about this one

491
00:29:10,485 --> 00:29:11,565
Speaker 7:  Of your better headlines. It was,

492
00:29:11,625 --> 00:29:15,485
Speaker 6:  It was very direct, but my prediction was that we would get DRM audio

493
00:29:15,575 --> 00:29:19,285
Speaker 6:  where only approved headphones would work with certain streaming services.

494
00:29:19,355 --> 00:29:23,285
Speaker 6:  That did not happen. What did happen is arguably weirder

495
00:29:23,285 --> 00:29:27,015
Speaker 6:  and worse, which is that every phone

496
00:29:27,585 --> 00:29:31,275
Speaker 6:  works best with the headphones from the same company. Right? So Pixel

497
00:29:31,385 --> 00:29:34,995
Speaker 6:  Buds work best with the Pixel Galaxy Buds work best with your Samsung phone.

498
00:29:35,585 --> 00:29:38,955
Speaker 6:  AirPods work best with your Apple phone. And then they all have weird proprietary

499
00:29:38,955 --> 00:29:42,825
Speaker 6:  extensions like spatial audio head tracking. And that is ar it's

500
00:29:42,985 --> 00:29:46,385
Speaker 6:  arguably worse than here are some headphones that have the DRM trip for Spotify

501
00:29:46,605 --> 00:29:50,575
Speaker 6:  and we're all, maybe it's not. Maybe, maybe it, maybe it's all just

502
00:29:50,575 --> 00:29:54,375
Speaker 6:  equally bad, but that, I got my prediction wrong in that way. But

503
00:29:54,375 --> 00:29:57,895
Speaker 6:  the thing happened where the headphone market contracted Yeah. And now

504
00:29:58,695 --> 00:30:00,615
Speaker 6:  everyone's kinda has the ecosystem locking on headphones.

505
00:30:00,965 --> 00:30:04,815
Speaker 7:  Yeah. It is just purely platform based now, which is, which

506
00:30:04,815 --> 00:30:07,055
Speaker 7:  is a weird outcome. Yeah. In, in a lot of ways.

507
00:30:07,575 --> 00:30:10,055
Speaker 6:  Speaking of ecosystem lockin, there's some Apple Watch rumors. What's going

508
00:30:10,055 --> 00:30:10,295
Speaker 6:  on here?

509
00:30:10,795 --> 00:30:14,695
Speaker 7:  So we've been talking a lot about AI gadgets in general.

510
00:30:14,955 --> 00:30:18,895
Speaker 7:  And a thing that Allison Johnson keeps floating to me is that actually the

511
00:30:19,035 --> 00:30:22,975
Speaker 7:  AI gadget that should exist is the Apple Watch. Like that's

512
00:30:22,975 --> 00:30:26,575
Speaker 7:  the one, forget the humane pin, forget the rabbit. Just make a smart watch

513
00:30:26,575 --> 00:30:30,455
Speaker 7:  that does AI things. And it turns out Apple appears to

514
00:30:30,455 --> 00:30:33,615
Speaker 7:  agree with that. I think it was over the weekend, mark Kerman at Bloomberg

515
00:30:33,815 --> 00:30:37,655
Speaker 7:  reported that Apple is trying to put visual

516
00:30:37,655 --> 00:30:41,095
Speaker 7:  intelligence and thus cameras inside of the Apple Watch

517
00:30:41,995 --> 00:30:45,535
Speaker 7:  in the next two years. And I think I actually need you to help me visualize

518
00:30:45,535 --> 00:30:49,415
Speaker 7:  this because the, the way he reported It is on the regular models, the

519
00:30:49,415 --> 00:30:53,055
Speaker 7:  camera is going to be inside of the display, which I would assume means it's

520
00:30:53,055 --> 00:30:56,815
Speaker 7:  pointing kind of up off your wrist. But then on the

521
00:30:56,905 --> 00:31:00,735
Speaker 7:  ultra it's gonna be on the side between I guess the,

522
00:31:00,835 --> 00:31:04,775
Speaker 7:  the crown and the action button, which would indicate that

523
00:31:04,775 --> 00:31:08,375
Speaker 7:  it's pointing out the side of the watch. So I like, I've been spending all

524
00:31:08,375 --> 00:31:11,055
Speaker 7:  week trying to figure out like, what is this thing gonna be looking at and

525
00:31:11,055 --> 00:31:14,655
Speaker 7:  how are you gonna use it? But I'm like,

526
00:31:14,685 --> 00:31:18,655
Speaker 7:  there's a real energy within Apple, it appears to

527
00:31:18,655 --> 00:31:22,495
Speaker 7:  figure out how to put cameras both in your watch and in your AirPods

528
00:31:22,835 --> 00:31:26,255
Speaker 7:  as a, as a like visual intelligence AI input system.

529
00:31:28,275 --> 00:31:30,855
Speaker 7:  The AirPods one kind of makes sense to me, right? Like you just, you just

530
00:31:30,855 --> 00:31:33,935
Speaker 7:  put a camera here, it faces this way. Like you have the sort of Ray band

531
00:31:33,965 --> 00:31:37,855
Speaker 7:  meta visual and that makes sense. This one I can't quite wrap my

532
00:31:37,855 --> 00:31:41,815
Speaker 7:  mind around. Like I get the idea. I just don't get how you do it in the product

533
00:31:42,315 --> 00:31:44,615
Speaker 6:  Or like truly where it goes. Like fundamentally,

534
00:31:44,615 --> 00:31:45,895
Speaker 7:  Literally. Yeah. What

535
00:31:45,895 --> 00:31:48,135
Speaker 6:  Is it? If you're wearing a watch be like you're gonna beep.

536
00:31:48,765 --> 00:31:52,255
Speaker 7:  Yeah. It really is like a, like an old school like Dick Tracy

537
00:31:52,485 --> 00:31:55,055
Speaker 7:  kind of thing where you're just, you're gonna hold it up and like shoot lasers

538
00:31:55,055 --> 00:31:55,895
Speaker 7:  at the side of your watch

539
00:31:57,495 --> 00:31:59,395
Speaker 6:  And then if you do the face, you're always kind of like,

540
00:32:00,265 --> 00:32:01,235
Speaker 7:  This is, yeah, this is looking

541
00:32:01,415 --> 00:32:04,815
Speaker 6:  At stuff. And then where is the shutter button here? You got a weird problem

542
00:32:04,815 --> 00:32:05,095
Speaker 6:  there.

543
00:32:05,915 --> 00:32:06,205
Speaker 7:  Yeah.

544
00:32:06,405 --> 00:32:06,685
Speaker 6:  Just

545
00:32:06,785 --> 00:32:09,285
Speaker 7:  Can you just imagine like we're two years away from just walking around and

546
00:32:09,285 --> 00:32:12,565
Speaker 7:  just like shoving our wrist and people's faces being like, look at this.

547
00:32:13,285 --> 00:32:17,005
Speaker 6:  I don't even use, I know a lot of people who do use Apple Wallet on their

548
00:32:17,005 --> 00:32:20,325
Speaker 6:  watches and they're just like, beep. And I find that to be awkward and I

549
00:32:20,325 --> 00:32:24,245
Speaker 6:  use my phone all the time. I get it. I, I like if you've got a

550
00:32:24,245 --> 00:32:28,165
Speaker 6:  big wearable and you've, you've got your own proprietary Bluetooth link

551
00:32:28,595 --> 00:32:32,405
Speaker 6:  back to a phone with a big battery or a bigger battery, more processing

552
00:32:32,405 --> 00:32:35,845
Speaker 6:  power, it makes sense to have this thing be the thing.

553
00:32:37,265 --> 00:32:40,565
Speaker 6:  I don't know. I I I don't know why you would put a camera on it. I I think

554
00:32:40,565 --> 00:32:43,485
Speaker 6:  that would, unless you want it to be totally separate from the phone,

555
00:32:44,255 --> 00:32:47,085
Speaker 6:  which runs directly into

556
00:32:48,285 --> 00:32:52,085
Speaker 6:  a classic of the David Pierce genre, which is when you profiled the Apple

557
00:32:52,085 --> 00:32:56,005
Speaker 6:  Watch for Wired and you set it, its goal was to get away from the, the phone.

558
00:32:56,275 --> 00:33:00,165
Speaker 6:  Yeah. And Apple basically yelled and said that wasn't

559
00:33:00,165 --> 00:33:00,285
Speaker 6:  true.

560
00:33:02,475 --> 00:33:02,765
Speaker 6:  Yeah.

561
00:33:02,795 --> 00:33:06,485
Speaker 7:  They told me on the record it was true. And then told me off

562
00:33:06,625 --> 00:33:07,725
Speaker 7:  on the record it wasn't true.

563
00:33:08,635 --> 00:33:11,885
Speaker 6:  Sure. But like, this has been the problem with the watch forever. Like, can

564
00:33:11,885 --> 00:33:14,485
Speaker 6:  it be independent of the phone and maybe AI is the thing that makes it independent

565
00:33:14,485 --> 00:33:14,685
Speaker 6:  of the phone

566
00:33:15,495 --> 00:33:19,085
Speaker 7:  Maybe. But I also think I I I think one of the other stories of the Apple

567
00:33:19,085 --> 00:33:22,845
Speaker 7:  Watch is another thing that I reported in that story that Apple hated, which

568
00:33:22,845 --> 00:33:26,605
Speaker 7:  is that they decided to build a watch way before they decided why to build

569
00:33:26,645 --> 00:33:30,605
Speaker 7:  a watch. And I think you still see that in the watch. Like they

570
00:33:30,605 --> 00:33:34,365
Speaker 7:  figured out one thing, this is a, a health device, but that has a

571
00:33:34,365 --> 00:33:37,805
Speaker 7:  ceiling for both. Yeah. What they can like literally do

572
00:33:37,805 --> 00:33:40,885
Speaker 7:  scientifically and the number of people who are gonna be interested in it.

573
00:33:40,905 --> 00:33:44,885
Speaker 7:  So you need another thing if you want this thing to sort of continue to grow

574
00:33:44,885 --> 00:33:46,885
Speaker 7:  and become a huge business, which it obviously already is.

575
00:33:48,425 --> 00:33:51,485
Speaker 7:  Now the bet obviously is that it's an AI thing and

576
00:33:52,425 --> 00:33:56,005
Speaker 7:  you, you either need a ton more processing inside of it, which is very hard

577
00:33:56,005 --> 00:33:59,805
Speaker 7:  to do on a watch or you need some new input thing that it can do.

578
00:34:00,065 --> 00:34:03,725
Speaker 7:  And I think that's how you get to, oh, the way we do this is to is to make

579
00:34:03,725 --> 00:34:04,645
Speaker 7:  it a camera somehow.

580
00:34:05,505 --> 00:34:07,765
Speaker 6:  Or you give it AI voice

581
00:34:09,245 --> 00:34:12,135
Speaker 6:  Siri, you know, that's confusing

582
00:34:12,215 --> 00:34:13,175
Speaker 7:  Everyone. Yeah. That's going really well for Apple.

583
00:34:13,535 --> 00:34:16,055
Speaker 6:  Yeah. The, the thing that's currently confusing this entire

584
00:34:16,255 --> 00:34:20,015
Speaker 7:  Industry, honestly, if Siri were good, it would've solved an enormous number

585
00:34:20,015 --> 00:34:23,215
Speaker 7:  of problems like this for Apple because, because it, the watch would've been

586
00:34:23,645 --> 00:34:26,655
Speaker 7:  much closer to the thing that Apple originally wanted it to be if Siri was

587
00:34:26,655 --> 00:34:29,615
Speaker 7:  good because you would've been able to do stuff on your watch and thus not

588
00:34:29,615 --> 00:34:32,015
Speaker 7:  do it on your phone, which was the goal.

589
00:34:32,245 --> 00:34:34,895
Speaker 6:  Yeah. But then you run, you run into that serial problem also. You probably

590
00:34:34,895 --> 00:34:37,975
Speaker 6:  have to wear headphones all the time. Right. It's like the combo platter

591
00:34:37,975 --> 00:34:41,895
Speaker 6:  of AirPods and an Apple Watch replaces your phone is a dream I think

592
00:34:41,895 --> 00:34:45,375
Speaker 6:  they've hinted at. Right? You get some AR glasses in the mix there. That's

593
00:34:45,375 --> 00:34:46,015
Speaker 6:  a lot of stuff.

594
00:34:46,515 --> 00:34:47,055
Speaker 7:  It is a lot of

595
00:34:47,055 --> 00:34:50,855
Speaker 6:  Stuff, you know, on, on the, the chart of wearable bullshit that's a lot

596
00:34:50,855 --> 00:34:54,655
Speaker 6:  of gizmos to care for that does not necessarily deliver more value than

597
00:34:54,655 --> 00:34:55,935
Speaker 6:  having a giant phone.

598
00:34:56,785 --> 00:34:57,135
Speaker 7:  Right.

599
00:34:57,755 --> 00:35:00,215
Speaker 6:  In case you're wondering Eli's theory of wearable bullshit is that you have

600
00:35:00,215 --> 00:35:04,205
Speaker 6:  a Y axis that's FIDS and an X axis

601
00:35:04,885 --> 00:35:07,745
Speaker 6:  that's like value, like utility.

602
00:35:09,525 --> 00:35:13,455
Speaker 6:  I change the axes every time we talk about this and you have a a a

603
00:35:14,535 --> 00:35:18,135
Speaker 6:  a huge penalty for a face on fis. Right? And so

604
00:35:18,325 --> 00:35:21,815
Speaker 6:  regular glasses are like a perfect device. You don't have to care for 'em

605
00:35:21,815 --> 00:35:24,695
Speaker 6:  a lot, a little bit. You have to care for 'em, but they're so valuable, they

606
00:35:24,895 --> 00:35:28,675
Speaker 6:  overcome the, it's on your face penalty and then the Apple

607
00:35:28,675 --> 00:35:32,035
Speaker 6:  Watch had to curve up. It wasn't useful enough at the beginning and now it's

608
00:35:32,035 --> 00:35:34,075
Speaker 6:  pretty useful. You don't have to care for it a lot. 'cause battery less a

609
00:35:34,075 --> 00:35:37,605
Speaker 6:  long time. And the Vision Pro is all the way at the bottom where it's really

610
00:35:37,665 --> 00:35:40,685
Speaker 6:  fiddly has an external battery pack, it doesn't have enough apps, it's on

611
00:35:40,685 --> 00:35:42,445
Speaker 6:  your face and it doesn't do a

612
00:35:42,445 --> 00:35:44,365
Speaker 7:  Lot. It is the most penalty. Yeah.

613
00:35:45,625 --> 00:35:49,515
Speaker 6:  And like many people have sent us vari variations on this

614
00:35:49,515 --> 00:35:53,275
Speaker 6:  chart. Please by all means, send us where you think the combo platter of

615
00:35:53,805 --> 00:35:57,315
Speaker 6:  Apple Watch plus AirPods plus glasses lives

616
00:35:58,155 --> 00:35:58,915
Speaker 6:  compared to phone.

617
00:36:01,725 --> 00:36:04,175
Speaker 6:  Because you, you just see that math doesn't math.

618
00:36:05,725 --> 00:36:09,575
Speaker 7:  Yeah. So I just reviewed the, the light phone three this week and we shouldn't

619
00:36:09,575 --> 00:36:11,295
Speaker 7:  talk about it. I'm gonna talk about it a bunch with Allison next week. But

620
00:36:11,715 --> 00:36:15,415
Speaker 7:  one of the things that kept striking me in the course of doing this is like,

621
00:36:15,415 --> 00:36:19,095
Speaker 7:  intellectually I would like to use my phone less, but then every time

622
00:36:19,975 --> 00:36:23,935
Speaker 7:  I am given a, a thing that would maybe make me use my phone

623
00:36:23,935 --> 00:36:26,775
Speaker 7:  less, I'm like, you know what kicks ass is my phone. Yeah. Like my phone

624
00:36:26,795 --> 00:36:28,895
Speaker 7:  is so good at so many things. It's really,

625
00:36:29,035 --> 00:36:32,575
Speaker 6:  And it's a real problem for this whole industry. Yeah. And

626
00:36:32,725 --> 00:36:36,575
Speaker 7:  Like what I like to do some of those things less. Yeah.

627
00:36:36,795 --> 00:36:39,855
Speaker 7:  But I'd, I still wanna do I still wanna look at Reddit sometimes. Yeah.

628
00:36:40,595 --> 00:36:44,535
Speaker 6:  The screen is really big in this internet connection's pretty fast. Yeah.

629
00:36:45,325 --> 00:36:45,615
Speaker 7:  It's

630
00:36:45,615 --> 00:36:49,575
Speaker 6:  Real problem. The camera's pretty good. Speaking of Apple WWC was

631
00:36:49,575 --> 00:36:53,295
Speaker 6:  announced, it starts June 9th. We're expecting a, a big overhaul to

632
00:36:53,515 --> 00:36:57,425
Speaker 6:  iOS. There's some notion that I

633
00:36:57,425 --> 00:36:59,385
Speaker 6:  know inside of Squares will have circles for icons.

634
00:36:59,775 --> 00:37:00,985
Speaker 7:  Yeah. Speaking of the Vision Pro

635
00:37:01,605 --> 00:37:03,185
Speaker 6:  Aas will finally come to iOS.

636
00:37:05,485 --> 00:37:09,005
Speaker 7:  I will say two like old head internet references that we've made

637
00:37:09,785 --> 00:37:13,365
Speaker 7:  in the last couple of weeks. One was Arrow glass on Windows. We got like

638
00:37:13,385 --> 00:37:16,205
Speaker 7:  an email or two from people who were like, oh, I remember that. And then

639
00:37:16,275 --> 00:37:19,765
Speaker 7:  just offhandedly a drinking outta cups references. And we got an absolute

640
00:37:20,295 --> 00:37:24,245
Speaker 7:  outpouring of support from people who were like that you are my people

641
00:37:25,245 --> 00:37:25,965
Speaker 7:  drinking outta cups.

642
00:37:26,645 --> 00:37:27,365
Speaker 6:  I don't, it

643
00:37:27,365 --> 00:37:27,685
Speaker 7:  Was amazing.

644
00:37:28,475 --> 00:37:31,925
Speaker 6:  Many people on YouTube comments record, I'd love to get this joke. What?

645
00:37:32,425 --> 00:37:34,565
Speaker 6:  And I, the video doesn't hold up.

646
00:37:36,425 --> 00:37:40,005
Speaker 7:  It was funny to watch again, but if you've never seen it, it won't be funny.

647
00:37:40,235 --> 00:37:40,525
Speaker 7:  Yeah.

648
00:37:40,675 --> 00:37:44,325
Speaker 6:  Like it's an old video. You can Google it. It's a

649
00:37:44,725 --> 00:37:47,565
Speaker 6:  animated, it's it's, I'm not even gonna explain it. Whatever. It's just,

650
00:37:48,105 --> 00:37:51,405
Speaker 6:  if you were drunk with your friends at a particular moment in your history,

651
00:37:51,405 --> 00:37:53,285
Speaker 6:  this was the funniest thing you'd ever seen in your life.

652
00:37:53,485 --> 00:37:53,965
Speaker 7:  A hundred percent.

653
00:37:54,265 --> 00:37:58,045
Speaker 6:  And that DNAI think is shared with a great many for chess

654
00:37:58,045 --> 00:38:00,805
Speaker 6:  listeners. Yes. Johnny Hammer sticks over here.

655
00:38:02,105 --> 00:38:04,325
Speaker 7:  But on the, on the WWDC front, it, it seems like

656
00:38:05,855 --> 00:38:09,845
Speaker 7:  there there's been a bunch of hype about a huge redesigned iOS 19 and I

657
00:38:09,845 --> 00:38:13,285
Speaker 7:  would say every little bit we learn about it makes it seem like a less and

658
00:38:13,285 --> 00:38:17,205
Speaker 7:  less huge redesign of all the time. It's just gonna

659
00:38:17,225 --> 00:38:21,165
Speaker 7:  be slightly glassier and that'll be

660
00:38:21,165 --> 00:38:21,605
Speaker 7:  it. Well,

661
00:38:21,645 --> 00:38:25,405
Speaker 6:  I mean this is what, three months away, right? Yeah. Not even two months

662
00:38:25,485 --> 00:38:29,405
Speaker 6:  away. Think about all the things that aren't gonna

663
00:38:29,405 --> 00:38:33,245
Speaker 6:  get paid off at this WWC Apple intelligence with Smart

664
00:38:33,355 --> 00:38:37,285
Speaker 6:  Siri not gonna get paid off at this WWE c like we already

665
00:38:37,285 --> 00:38:40,125
Speaker 6:  know they've announced it next generation CarPlay

666
00:38:40,935 --> 00:38:44,805
Speaker 6:  50 years after they announced it. Yep. Not gonna get paid. Last

667
00:38:44,805 --> 00:38:48,765
Speaker 6:  year they were insistent like this is happening. Like big

668
00:38:48,785 --> 00:38:52,685
Speaker 6:  car companies were meant to like didn't happen. Porsche, which actually

669
00:38:52,685 --> 00:38:56,565
Speaker 6:  did announce it, not, hasn't shipped to card yet with the

670
00:38:56,565 --> 00:38:58,685
Speaker 6:  next generation CarPlay. It's still not paid off. What are they gonna say

671
00:38:58,745 --> 00:39:02,325
Speaker 6:  in, in two months? Is will there, will there be an auto industry in America?

672
00:39:02,385 --> 00:39:04,405
Speaker 6:  Who knows? But it's not there.

673
00:39:05,955 --> 00:39:09,485
Speaker 6:  Then iOS is conceptually, if you're reorienting it around Apple

674
00:39:09,485 --> 00:39:13,365
Speaker 6:  intelligence, what are you gonna pay off? Right? Right. Even

675
00:39:13,365 --> 00:39:16,445
Speaker 6:  marketing these phones is having Apple on top. Like I think there's a lot

676
00:39:16,445 --> 00:39:19,975
Speaker 6:  of pressure. And then you've got just angry developers in general,

677
00:39:20,105 --> 00:39:23,535
Speaker 6:  right? Like this has been the story forever. So I'm curious to see what they

678
00:39:23,535 --> 00:39:24,695
Speaker 6:  say actually. Wwc.

679
00:39:24,925 --> 00:39:28,855
Speaker 7:  Yeah, it's, it's very funny because last year right about now there was

680
00:39:28,855 --> 00:39:32,615
Speaker 7:  this sense of, okay, apple has not talked enough

681
00:39:33,025 --> 00:39:36,735
Speaker 7:  about ai. AI is the thing, it's all anybody wants to talk about.

682
00:39:37,035 --> 00:39:40,855
Speaker 7:  But we know WWDC is coming and it's gonna be the time Apple takes its big

683
00:39:40,855 --> 00:39:44,735
Speaker 7:  swing and sort of announces the perfect productization of AI

684
00:39:44,955 --> 00:39:48,695
Speaker 7:  and Apple like attempted to do that. And here we

685
00:39:48,855 --> 00:39:52,455
Speaker 7:  are and I think this time Apple is coming into it

686
00:39:52,915 --> 00:39:56,815
Speaker 7:  on its back foot in a way that It is not accustomed to that it Apple

687
00:39:56,875 --> 00:40:00,295
Speaker 7:  is going to have to explain itself in a way that It is not used to having

688
00:40:00,295 --> 00:40:04,095
Speaker 7:  to explain itself and it's not going to because

689
00:40:04,095 --> 00:40:07,295
Speaker 7:  that's just not what Apple does. Apple is better than any company on Earth.

690
00:40:07,295 --> 00:40:09,935
Speaker 7:  It just pretending everything is fine and posting through it.

691
00:40:11,355 --> 00:40:15,275
Speaker 7:  But, but that is where It is right now. Like this, this,

692
00:40:15,735 --> 00:40:18,995
Speaker 7:  at some point It is going to have to look people in the face and be like,

693
00:40:18,995 --> 00:40:22,515
Speaker 7:  Hey, remember that Siri thing we told you about that That one's on us.

694
00:40:22,795 --> 00:40:26,355
Speaker 6:  Yeah. Sign up for app Intense where we commoditize your app, right.

695
00:40:26,635 --> 00:40:29,675
Speaker 6:  For a product that doesn't work. We'll see, speaking of

696
00:40:30,775 --> 00:40:34,605
Speaker 6:  AI and agents, we're kind of waiting on next

697
00:40:34,605 --> 00:40:37,885
Speaker 6:  generation Alexa to hit. It's like any minute now. And then there's a rumor

698
00:40:37,915 --> 00:40:41,645
Speaker 6:  that Amazon will rename the entire echo line to just Alexa,

699
00:40:42,115 --> 00:40:42,405
Speaker 6:  what

700
00:40:42,405 --> 00:40:45,245
Speaker 7:  Do you think of this? I've been thinking about this for like two days now

701
00:40:45,255 --> 00:40:49,045
Speaker 7:  since Jen Tui first saw this and, and I think got a tip

702
00:40:49,045 --> 00:40:52,725
Speaker 7:  about it and wrote, wrote a story. And some people on our team were seeing

703
00:40:52,975 --> 00:40:56,605
Speaker 7:  Alexa. Like when you go to the Amazon listing, we're seeing Alexa show other

704
00:40:56,605 --> 00:40:57,565
Speaker 7:  people were seeing echo show.

705
00:40:59,285 --> 00:41:02,255
Speaker 7:  What do you think of that switch? If they just go full on Alexa everything,

706
00:41:02,855 --> 00:41:05,015
Speaker 6:  I think they should go full on Alexa everything. I

707
00:41:05,015 --> 00:41:05,495
Speaker 7:  Kind of think so too.

708
00:41:05,525 --> 00:41:09,175
Speaker 6:  It's the name of the pla It's the argument was always that Alexa is Windows

709
00:41:09,735 --> 00:41:12,935
Speaker 6:  and then you had to have this ecosystem of products that run Alexa and

710
00:41:13,335 --> 00:41:15,015
Speaker 6:  Amazon's products would be called Echo.

711
00:41:16,605 --> 00:41:20,135
Speaker 6:  Even just explaining it that sounds stupid, right? Like it was not the right

712
00:41:20,135 --> 00:41:23,895
Speaker 6:  idea and no one knows the difference. No one's like, I better buy an

713
00:41:23,895 --> 00:41:27,785
Speaker 6:  Echo and not a Lenovo voice operated Alexa

714
00:41:27,845 --> 00:41:28,465
Speaker 6:  bot. Well, and

715
00:41:28,465 --> 00:41:31,425
Speaker 7:  I think the other part of It is that stuff never really materialized. Like

716
00:41:31,425 --> 00:41:34,845
Speaker 7:  there isn't a giant third party universe of Alexa

717
00:41:34,885 --> 00:41:38,125
Speaker 7:  speakers. Like they just are echos

718
00:41:38,355 --> 00:41:42,325
Speaker 6:  Because Alexa is not useful enough, right? So just to be, to, to hammer that

719
00:41:42,325 --> 00:41:45,845
Speaker 6:  home that never took off because Alexa itself was never useful enough. Like

720
00:41:46,555 --> 00:41:49,965
Speaker 6:  when car makers are like, there's Alexa integration in the dash, it's like,

721
00:41:50,025 --> 00:41:53,805
Speaker 6:  why? To do what? Set a timer

722
00:41:53,905 --> 00:41:56,765
Speaker 6:  in my car. Like I'm, I'm not doing that. Maybe play music, but you don't

723
00:41:56,765 --> 00:42:00,525
Speaker 6:  need Alexa to do all that. Like the promise of this brand name hasn't meant

724
00:42:00,805 --> 00:42:04,645
Speaker 6:  anything. Now it means something or it's supposed to mean something,

725
00:42:04,645 --> 00:42:08,165
Speaker 6:  which is all this age agentic AI actual assistant stuff. And I think you

726
00:42:08,165 --> 00:42:11,885
Speaker 6:  need to reclaim that. 'cause I don't think they're gonna run this new Alexa

727
00:42:12,065 --> 00:42:15,645
Speaker 6:  on third party devices the way they were in the past. I think they want you

728
00:42:15,645 --> 00:42:18,845
Speaker 6:  to buy a screen. They, like Panas has said, I want you to buy

729
00:42:19,545 --> 00:42:23,205
Speaker 6:  an Alexa show and then there's gonna be new hardware. And I think you gotta,

730
00:42:23,795 --> 00:42:24,575
Speaker 6:  that's the brand.

731
00:42:25,005 --> 00:42:27,655
Speaker 7:  Yeah, no, I think that, I think that's right. And it actually, it makes a

732
00:42:27,655 --> 00:42:31,615
Speaker 7:  lot of sense to me too. 'cause I think, I would bet most people who own

733
00:42:32,555 --> 00:42:36,095
Speaker 7:  an Echo device don't know that it's called an Echo device. It's just, it's,

734
00:42:36,125 --> 00:42:39,815
Speaker 7:  it's the Alexa. That's how almost everyone I know who has one of these

735
00:42:39,815 --> 00:42:42,055
Speaker 7:  refers to it as it's the Alexa. You

736
00:42:42,055 --> 00:42:44,935
Speaker 6:  Don't, you don't talk to your Dell XPS 500 Alexa bot.

737
00:42:46,125 --> 00:42:47,935
Speaker 7:  What was the Harmon Cardin one that had,

738
00:42:49,445 --> 00:42:50,175
Speaker 7:  that was good stuff.

739
00:42:50,975 --> 00:42:54,815
Speaker 6:  I mean, we'll see. It, it, the it Alexa, the new Alexa is gonna start hitting.

740
00:42:55,875 --> 00:42:59,735
Speaker 6:  If you have access to us, let us know. We're everyone on

741
00:42:59,735 --> 00:43:02,175
Speaker 6:  our team is sort of like, are you useful now?

742
00:47:12,215 --> 00:47:16,125
Speaker 6:  We're back. Kylie Robinson's here. Hey Kylie. Hello. Big

743
00:47:16,125 --> 00:47:19,765
Speaker 6:  week of AI News. Big week of AI Feelings. Yes.

744
00:47:20,105 --> 00:47:23,445
Speaker 6:  Mostly centered around OpenAI says the, I would say the most emotional of

745
00:47:23,445 --> 00:47:25,045
Speaker 6:  the AI companies. Right?

746
00:47:25,985 --> 00:47:28,245
Speaker 7:  That's really true, isn't it? I never really thought about it like that,

747
00:47:28,245 --> 00:47:31,125
Speaker 7:  but that is true. There are a lot of feelings inside that company.

748
00:47:31,235 --> 00:47:32,925
Speaker 6:  Just big feelings all over the place.

749
00:47:33,305 --> 00:47:34,005
Speaker 11:  Big feelings

750
00:47:34,835 --> 00:47:38,165
Speaker 6:  Like Microsoft as a company is not like a big feelings company, you know?

751
00:47:38,545 --> 00:47:42,165
Speaker 6:  No, no. Anthropic, you know, they're sort of like dead ahead. They're like,

752
00:47:42,165 --> 00:47:45,245
Speaker 6:  here's some stuff. It writes code and then open eyes. Like, I'm feeling all

753
00:47:45,245 --> 00:47:46,245
Speaker 6:  of 'em. Yeah,

754
00:47:47,575 --> 00:47:49,605
Speaker 11:  We're creating God, please save us.

755
00:47:51,075 --> 00:47:54,405
Speaker 6:  Like it's just like a lot like, it's like I tell my daughter like it's, you

756
00:47:54,405 --> 00:47:57,485
Speaker 6:  have to feel all of your feelings so you can get over them. You know? I was

757
00:47:57,485 --> 00:47:58,525
Speaker 6:  like, I'm feeling all of them.

758
00:48:00,605 --> 00:48:02,685
Speaker 7:  I feel like there's like two things we need to talk about here. But yeah,

759
00:48:02,685 --> 00:48:05,925
Speaker 7:  before we get way into the inevitable

760
00:48:06,465 --> 00:48:10,045
Speaker 7:  Studio, Ghibli Weeds, which I would very much like to do, there was some

761
00:48:10,045 --> 00:48:13,885
Speaker 7:  like business leadership stuff that happened at OpenAI

762
00:48:13,955 --> 00:48:17,485
Speaker 7:  says. And speaking of companies that are unusual,

763
00:48:18,385 --> 00:48:22,045
Speaker 7:  nobody does a C-suite quite like OpenAI says, does a C-suite, right?

764
00:48:22,155 --> 00:48:25,805
Speaker 7:  What do you make of this whole like shift? What, what happened this weekend

765
00:48:25,805 --> 00:48:27,485
Speaker 7:  and do we need to actually care about it?

766
00:48:28,665 --> 00:48:32,245
Speaker 11:  No, I don't think people need to care about it. But for their information,

767
00:48:32,545 --> 00:48:36,205
Speaker 11:  the CEOO, Brad lcap, he's expanding his role.

768
00:48:36,315 --> 00:48:39,885
Speaker 11:  He's taking over day-to-day operations, basically becoming a CEO.

769
00:48:40,265 --> 00:48:44,125
Speaker 11:  And Sam Altman, who is the CEO, is gonna shift to a more

770
00:48:44,125 --> 00:48:47,645
Speaker 11:  product research focus, which to me, my reading is

771
00:48:48,105 --> 00:48:51,805
Speaker 11:  he wants to focus more on Stargate. He wants to more like focus more on

772
00:48:51,955 --> 00:48:55,645
Speaker 11:  shaking hands and making these deals. And he can't exactly

773
00:48:55,785 --> 00:48:59,725
Speaker 11:  do all that and be the leader. Opening AI knees right now is my

774
00:48:59,725 --> 00:49:03,605
Speaker 11:  take, but it's like an ever shifting thing. They're gonna become

775
00:49:03,605 --> 00:49:07,205
Speaker 11:  a for-profit soon. Brad seems like a nice person to put at the top and be

776
00:49:07,205 --> 00:49:10,645
Speaker 11:  like, this is the guy running things right now and Sam is just tinkering

777
00:49:10,645 --> 00:49:12,245
Speaker 11:  with robots and a GI

778
00:49:12,985 --> 00:49:14,605
Speaker 6:  Are they gonna become a for-profit soon?

779
00:49:15,895 --> 00:49:18,925
Speaker 11:  Great question. Great question. I think they would like to,

780
00:49:20,945 --> 00:49:24,845
Speaker 11:  but I don't know how this lawsuit with Elon and all of

781
00:49:24,845 --> 00:49:28,205
Speaker 11:  that will, you know, totally change the trajectory if at all.

782
00:49:29,035 --> 00:49:32,005
Speaker 11:  That remains to be seen. I think they have to get through that first.

783
00:49:32,385 --> 00:49:36,235
Speaker 7:  How far are we into the two year window that they had to do it? We're not

784
00:49:36,255 --> 00:49:38,195
Speaker 7:  at a year yet, right? No.

785
00:49:38,255 --> 00:49:42,155
Speaker 11:  No, I don't think so. Okay. Please fact, check me. But I, I think off

786
00:49:42,155 --> 00:49:45,075
Speaker 11:  the top of my head, it was supposed to happen like mid midway through this

787
00:49:45,075 --> 00:49:46,435
Speaker 11:  year around the summer. Okay.

788
00:49:46,735 --> 00:49:50,395
Speaker 6:  It does seem like the boring part of changing the corporate structure

789
00:49:51,055 --> 00:49:54,955
Speaker 6:  is what they want Brad to do. Yes. Like do this paperwork so

790
00:49:55,015 --> 00:49:58,965
Speaker 6:  Sam can say he's building God. Right? Is that, is

791
00:49:58,965 --> 00:50:00,125
Speaker 6:  that all, is that all that is?

792
00:50:00,555 --> 00:50:04,005
Speaker 11:  I've had a lot of people ask me about this and I think, you know, you know

793
00:50:04,005 --> 00:50:07,925
Speaker 11:  how Elon was like, I'm leading Twitter and now he just put someone as

794
00:50:08,005 --> 00:50:11,725
Speaker 11:  CEO so he doesn't have to fucking deal with it. That's sort of like you've,

795
00:50:11,725 --> 00:50:14,685
Speaker 11:  he's been running this company long enough, clearly he wants to do other

796
00:50:14,685 --> 00:50:18,165
Speaker 11:  things and he's like, can somebody else deal with the logistics?

797
00:50:18,545 --> 00:50:19,125
Speaker 11:  Is my read.

798
00:50:19,845 --> 00:50:23,725
Speaker 7:  I also wonder if Neil, I, we were talking about Mark Zuckerberg in

799
00:50:23,725 --> 00:50:27,685
Speaker 7:  the last segment and one of the pivots that Mark made pretty aggressively

800
00:50:27,825 --> 00:50:31,565
Speaker 7:  was, I don't wanna be the person required to apologize for this stuff

801
00:50:31,565 --> 00:50:35,365
Speaker 7:  anymore. And so he made like Nick Clegg, the person who had to go

802
00:50:35,365 --> 00:50:39,005
Speaker 7:  apologize for Facebook all the time and he just got to go like announce new

803
00:50:39,005 --> 00:50:42,045
Speaker 7:  products. And I, I keep thinking about, and, and this is where we should

804
00:50:42,045 --> 00:50:45,885
Speaker 7:  get to the images and ChatGPT stuff, but there was a, there was a, a Sam

805
00:50:45,895 --> 00:50:49,605
Speaker 7:  tweet this week where he was basically like, you know, be me,

806
00:50:49,775 --> 00:50:53,365
Speaker 7:  build in silence for a long time. Nobody cares. And then it was like, and

807
00:50:53,365 --> 00:50:56,165
Speaker 7:  then we do images and now everybody's being weird and it's like, oh you,

808
00:50:56,705 --> 00:50:58,845
Speaker 6:  You just didn't wanna say the word that he said. What

809
00:50:58,845 --> 00:51:01,525
Speaker 7:  Did he say? I honestly don't remember what that was. Lemme tell

810
00:51:01,525 --> 00:51:05,205
Speaker 11:  You. I know the exact sequence hit me. So, you know, for a few years

811
00:51:05,445 --> 00:51:08,565
Speaker 11:  building in silence then for two and a half everybody hates me and then

812
00:51:08,595 --> 00:51:12,405
Speaker 11:  wake up today and everyone's saying, haha, I made you into Twink Studio

813
00:51:12,685 --> 00:51:14,045
Speaker 11:  Ghibli. That's right.

814
00:51:15,025 --> 00:51:17,965
Speaker 6:  That's exactly what he said. Yeah. To be clear, Sam is a gay man. I think

815
00:51:17,965 --> 00:51:20,685
Speaker 6:  he's allowed to use that word in a way. Yes. That David and I are not. Yes.

816
00:51:22,265 --> 00:51:25,245
Speaker 6:  But that, I mean that this is the thing, like he's the product person now.

817
00:51:25,275 --> 00:51:29,245
Speaker 6:  He's the face of the product. He's still the CEO famously. He was

818
00:51:29,245 --> 00:51:32,925
Speaker 6:  not the CEO EO for about 20 minutes last year. Yeah. Yes. So I don't think

819
00:51:32,955 --> 00:51:35,645
Speaker 6:  he's allowed to step down as CEO EO again after that flight. He has to say

820
00:51:35,645 --> 00:51:38,765
Speaker 6:  the CEO e Right. But the like operate the company, be the corporate person.

821
00:51:38,795 --> 00:51:41,565
Speaker 6:  Okay. We're shift down somewhere else. I will not be the face of the product

822
00:51:41,565 --> 00:51:45,045
Speaker 6:  and the fundraising and the hype and then that face

823
00:51:45,495 --> 00:51:48,125
Speaker 6:  quite literally this week is image generation.

824
00:51:48,695 --> 00:51:52,525
Speaker 11:  Right. Not to take away from Studio Ghibli, but I think the strangest thing

825
00:51:52,525 --> 00:51:56,445
Speaker 11:  is that they don't have a CTO and they've said they have no plans to replace

826
00:51:56,785 --> 00:52:00,645
Speaker 11:  Mirati anytime soon, which would probably help Sam in

827
00:52:00,665 --> 00:52:04,005
Speaker 11:  his product research mode to have a CTO. Yeah.

828
00:52:04,425 --> 00:52:08,245
Speaker 6:  And Sam notably does not have a reputation as like a great

829
00:52:08,245 --> 00:52:11,925
Speaker 6:  product person. Like he has an incredible reputation as a fundraiser

830
00:52:12,345 --> 00:52:15,005
Speaker 6:  and a guy who can build hype and talk about the story of the product and,

831
00:52:15,105 --> 00:52:18,925
Speaker 6:  and, and you know, he used to run Y Combinator like he's a great

832
00:52:18,995 --> 00:52:22,445
Speaker 6:  spotter of things, but like are you the great

833
00:52:22,715 --> 00:52:26,405
Speaker 6:  product visionary? I think there's a reason that Johnny Ive is building

834
00:52:26,725 --> 00:52:29,965
Speaker 6:  whatever he's building for them, right? Like he, he, he hires that talent

835
00:52:29,965 --> 00:52:33,485
Speaker 6:  so he doesn't have the CTO building that, now that said the image generator

836
00:52:33,485 --> 00:52:37,245
Speaker 6:  come out in four Oh this week has led to a wide variety

837
00:52:37,265 --> 00:52:37,965
Speaker 6:  of reactions.

838
00:52:39,265 --> 00:52:39,485
Speaker 11:  Yes.

839
00:52:40,365 --> 00:52:43,245
Speaker 6:  I, there's no other way to say that. A wide variety of reactions. What's

840
00:52:43,245 --> 00:52:43,565
Speaker 6:  going on there?

841
00:52:43,905 --> 00:52:47,045
Speaker 11:  Is this where I bait neli into talking about fair use because I was really

842
00:52:47,045 --> 00:52:47,245
Speaker 11:  hoping

843
00:52:47,325 --> 00:52:50,405
Speaker 7:  I could let, let's do that in a minute. First, what I would like you to do

844
00:52:50,405 --> 00:52:53,765
Speaker 7:  is describe every single interaction that you personally have had within

845
00:52:53,865 --> 00:52:55,125
Speaker 7:  the chat J pt.

846
00:52:55,245 --> 00:52:59,045
Speaker 11:  Yeah, perfect. The first thing I tried to do is to break it

847
00:52:59,185 --> 00:53:03,045
Speaker 11:  and make pictures of CEOs. And that worked pretty

848
00:53:03,045 --> 00:53:07,005
Speaker 11:  well actually. I described, this is in the piece about Ghibli. I described

849
00:53:07,035 --> 00:53:10,245
Speaker 11:  like this social media executive that everybody knows and he went in front

850
00:53:10,245 --> 00:53:14,125
Speaker 11:  of congress and you know, there's this fictional movie about him and he

851
00:53:14,125 --> 00:53:17,005
Speaker 11:  wears gold chains now. And they're like, oh that's interesting. Okay, let

852
00:53:17,005 --> 00:53:20,685
Speaker 11:  me generate that. And it's literally Mark Zuckerberg. So it's not super

853
00:53:20,685 --> 00:53:24,245
Speaker 11:  hard to get around the whole like make a famous person thing. And then I

854
00:53:24,245 --> 00:53:27,365
Speaker 11:  put him in a VERGE brand hoodie and things started to get weird. It, it

855
00:53:27,365 --> 00:53:31,325
Speaker 11:  started amalgamate into average tech CEO across whatever

856
00:53:31,585 --> 00:53:35,445
Speaker 11:  its data is and then the Ghibli trend kicked off.

857
00:53:35,865 --> 00:53:39,525
Speaker 11:  And I did not expect that when this, when I got the

858
00:53:39,645 --> 00:53:43,365
Speaker 11:  embargo and I talked to the team, I thought, okay, this is a cool upgrade.

859
00:53:43,785 --> 00:53:47,525
Speaker 11:  And I really think the auto regressive approach instead of

860
00:53:47,705 --> 00:53:51,405
Speaker 11:  the diffusion approach, so it goes from left to right top to bottom

861
00:53:51,405 --> 00:53:54,805
Speaker 11:  instead of generating it all at once. I think that has made like

862
00:53:55,385 --> 00:53:59,165
Speaker 11:  the craziest difference in the fidelity and the accuracy, which is

863
00:53:59,225 --> 00:54:02,205
Speaker 11:  was wild to see. And people glbb onto that really fast. What you

864
00:54:02,205 --> 00:54:04,685
Speaker 7:  Mean it generates the pixels of the image lesson, right? Yes. Top to bottom

865
00:54:04,685 --> 00:54:05,605
Speaker 7:  instead of Okay.

866
00:54:05,605 --> 00:54:09,085
Speaker 11:  Instead of just like the whole thing at once. So I think that probably helps

867
00:54:09,085 --> 00:54:13,005
Speaker 11:  with hallucinations, the lead for the project. He said

868
00:54:13,165 --> 00:54:16,365
Speaker 11:  I am speculating that that's the case. Which is funny 'cause it's like that

869
00:54:16,365 --> 00:54:19,365
Speaker 11:  unknown to them as well. Right. So

870
00:54:20,325 --> 00:54:23,685
Speaker 11:  I think people globbed onto that pretty quickly and one person was like,

871
00:54:23,955 --> 00:54:27,605
Speaker 11:  I've made a Ghibli version of my wife and my family and then the

872
00:54:27,805 --> 00:54:31,005
Speaker 11:  internet said, oh my god, I can make nine 11 with this and then it got crazy.

873
00:54:32,895 --> 00:54:36,855
Speaker 7:  I do love a that those are the two steps, right? And that b

874
00:54:37,125 --> 00:54:40,975
Speaker 7:  that is precisely of course how it was going to happen. Yeah. So I'll say

875
00:54:41,395 --> 00:54:44,015
Speaker 7:  my impression and I, I have not personally played with this at all. So all

876
00:54:44,015 --> 00:54:47,695
Speaker 7:  I've seen is what other people have made from it. It is remarkably

877
00:54:47,695 --> 00:54:51,335
Speaker 7:  better than yes. Basically any other image

878
00:54:51,335 --> 00:54:55,215
Speaker 7:  generation tool that I've seen, it does text really well. It seems like you

879
00:54:55,215 --> 00:54:59,175
Speaker 7:  have to pretty carefully describe the text that you

880
00:54:59,175 --> 00:55:02,455
Speaker 7:  want where if you're just like, just put a logo, it'll do some of the sort

881
00:55:02,455 --> 00:55:05,735
Speaker 7:  of insane gibberish that we've seen. But if you're like, no, replace that

882
00:55:05,735 --> 00:55:09,535
Speaker 7:  with Marlboro, it'll do it. Yeah. But the stuff is

883
00:55:09,565 --> 00:55:12,735
Speaker 7:  more accurate, it's more realistic. It is more like true to

884
00:55:13,775 --> 00:55:17,495
Speaker 7:  whatever style you ask for than basically anything I have seen Certainly

885
00:55:17,495 --> 00:55:20,415
Speaker 7:  that is like available to people at any kind of scale. And

886
00:55:21,645 --> 00:55:25,375
Speaker 7:  some of the stuff people are making is wild. And it does feel like we're

887
00:55:25,375 --> 00:55:28,215
Speaker 7:  gonna get into all the fair use of this all in a minute because It is the

888
00:55:28,215 --> 00:55:31,535
Speaker 7:  most straightforward. You just say like, make a Rick and Morty and it just

889
00:55:31,535 --> 00:55:34,615
Speaker 7:  makes a Rick and Morty that now looks like Rick and Morty.

890
00:55:35,315 --> 00:55:39,095
Speaker 7:  Yes. And it's, it's nuts. But like Kylie, I know you made a bunch of stuff

891
00:55:39,275 --> 00:55:43,055
Speaker 7:  and I did like was it, was it different to make stuff

892
00:55:43,055 --> 00:55:46,215
Speaker 7:  with this than you've had experience with image generators before? Like you,

893
00:55:46,215 --> 00:55:49,095
Speaker 7:  you used a bunch of selfies to make Yes. Images of yourself.

894
00:55:49,435 --> 00:55:52,815
Speaker 11:  Yes. 'cause I don't wanna feed it pictures of my family

895
00:55:53,375 --> 00:55:56,575
Speaker 11:  actually it's inevitable, but I just don't wanna start there.

896
00:55:57,365 --> 00:56:01,335
Speaker 11:  Yeah. I uploaded this mirror selfie I took in a bar bathroom in San Francisco

897
00:56:01,755 --> 00:56:05,415
Speaker 11:  and was totally shocked by how it got the depth

898
00:56:05,595 --> 00:56:08,815
Speaker 11:  and the lighting correct. And then it got, it didn't get the small text

899
00:56:08,875 --> 00:56:12,735
Speaker 11:  of the graffiti Correct. It said like lost lever instead of like

900
00:56:12,985 --> 00:56:15,375
Speaker 11:  dirty, hairy or something. So,

901
00:56:15,515 --> 00:56:19,375
Speaker 7:  But like the vibe was right? Like it felt like the, you, it was

902
00:56:19,375 --> 00:56:21,015
Speaker 7:  that bathroom that you were in it,

903
00:56:21,035 --> 00:56:24,445
Speaker 11:  It got the style of my shirt. Correct. It knows that it opened on one side

904
00:56:24,585 --> 00:56:28,245
Speaker 11:  and I was, I was totally sur I was totally

905
00:56:28,315 --> 00:56:31,685
Speaker 11:  surprised. And then I asked it to combine some photos. I took an

906
00:56:31,715 --> 00:56:35,525
Speaker 11:  Amsterdam and add Toro, I add a Toro like character

907
00:56:35,585 --> 00:56:39,445
Speaker 11:  to see if it works. Sure. And then sure enough I got Toro and we put that

908
00:56:39,445 --> 00:56:42,365
Speaker 11:  in the article as the lead image and I was like, are we gonna get sued over

909
00:56:42,365 --> 00:56:46,285
Speaker 11:  this? I can't tell. You know, and that's the fair use of it

910
00:56:46,285 --> 00:56:50,085
Speaker 11:  all. My first question is, where the fuck did you get all of

911
00:56:50,085 --> 00:56:54,045
Speaker 11:  this data? Is, is my first question. What are these people going

912
00:56:54,045 --> 00:56:57,645
Speaker 11:  to do when they see Homer Simpson doing, you know,

913
00:56:59,195 --> 00:57:03,165
Speaker 11:  nine 11? Like, like what happens? I don't know. I

914
00:57:03,485 --> 00:57:07,445
Speaker 11:  actually was, this might dismay some listeners and

915
00:57:07,445 --> 00:57:10,605
Speaker 11:  maybe even Neli, but I, I wanted to argue with Chad Petit this morning through

916
00:57:10,605 --> 00:57:14,565
Speaker 11:  voice mode about fair use in this context. And it did not,

917
00:57:14,785 --> 00:57:18,085
Speaker 11:  it did not have really great responses. It was like, oh yeah, this does

918
00:57:18,325 --> 00:57:19,405
Speaker 11:  seem bad. I'm like, oh,

919
00:57:19,405 --> 00:57:22,925
Speaker 6:  Okay. No, that's the right answer. I just wanna be clear. It, it seems

920
00:57:23,185 --> 00:57:26,965
Speaker 6:  bad. Yes. Like the, the fatal flaw

921
00:57:27,065 --> 00:57:30,645
Speaker 6:  at the heart of this entire over-hyped industry is

922
00:57:30,905 --> 00:57:34,205
Speaker 6:  the amount of copyright infringement that is occurring. Yes. Like obvious,

923
00:57:34,825 --> 00:57:38,725
Speaker 6:  not subtle. Right out in the open, did you copy a bunch

924
00:57:38,725 --> 00:57:42,685
Speaker 6:  of Studio GLI stuff so that you can generate Studio gli art y

925
00:57:42,745 --> 00:57:46,645
Speaker 6:  You sure did. It's ob it's obvious, like it's not

926
00:57:46,725 --> 00:57:50,525
Speaker 6:  a question mark. And then you have OpenAI says, and Google going to the

927
00:57:50,525 --> 00:57:53,285
Speaker 6:  United States Congress and the White House being like, can you write us an

928
00:57:53,285 --> 00:57:56,365
Speaker 6:  exception to the copyright law? So this isn't a problem that destroys our

929
00:57:56,525 --> 00:57:59,805
Speaker 6:  business because that's how bad It is. Copyright law is stupid. I say this

930
00:57:59,805 --> 00:58:03,765
Speaker 6:  on the show all the time. It's not, it's not some sophisticated legal

931
00:58:04,085 --> 00:58:07,965
Speaker 6:  doctrine. It's did you make a copy? Did you have permission to make a

932
00:58:07,965 --> 00:58:11,605
Speaker 6:  copy if you didn't have the permission? Does it fall into one of these narrow

933
00:58:11,655 --> 00:58:15,645
Speaker 6:  categories where we say, we know you made a copy without permission,

934
00:58:15,865 --> 00:58:19,845
Speaker 6:  but that's okay because of x. And those categories are like scholarship,

935
00:58:20,535 --> 00:58:24,485
Speaker 6:  right? Like commentary parody. Like it's not make

936
00:58:24,555 --> 00:58:26,845
Speaker 6:  more of it, make more copies of it

937
00:58:28,465 --> 00:58:32,285
Speaker 6:  and like, I don't know, like I truly do not know how any judge looks

938
00:58:32,285 --> 00:58:36,045
Speaker 6:  at all of this and says, oh this is fine. I can shoehorn this

939
00:58:36,045 --> 00:58:37,005
Speaker 6:  into that legal doctrine.

940
00:58:37,035 --> 00:58:39,605
Speaker 7:  Well, and this is why it's become so important for all these companies to

941
00:58:39,635 --> 00:58:42,565
Speaker 7:  obfuscate where the data is coming from. Right? Because if you boil it all

942
00:58:42,565 --> 00:58:46,005
Speaker 7:  the way down to did you make a copy of this? They're trying very hard to

943
00:58:46,165 --> 00:58:47,725
Speaker 7:  convince everybody that the answer is like, we don't know.

944
00:58:48,255 --> 00:58:48,605
Speaker 11:  Right?

945
00:58:48,825 --> 00:58:51,085
Speaker 7:  The internet is a mysterious place. Can

946
00:58:51,085 --> 00:58:55,005
Speaker 6:  I just underline again how stupid copyright law is as a, as a

947
00:58:55,005 --> 00:58:55,725
Speaker 6:  legal doctrine?

948
00:58:56,225 --> 00:58:57,125
Speaker 11:  I'm glad we've gotten here.

949
00:58:58,635 --> 00:59:02,245
Speaker 6:  It's not smart, it's not like sophisticated. We had to

950
00:59:02,245 --> 00:59:06,165
Speaker 6:  litigate as a society moving images across

951
00:59:06,165 --> 00:59:09,885
Speaker 6:  the internet and making all of the copies on the ram of all of the

952
00:59:09,885 --> 00:59:13,735
Speaker 6:  routers is okay. Those are called ephemeral copies. We

953
00:59:13,735 --> 00:59:16,775
Speaker 6:  that, that's lawsuits we had to litigate. This is a real lawsuit.

954
00:59:16,945 --> 00:59:19,535
Speaker 7:  Kylie, you asked for this. I just want you to know, Kylie, you asked for

955
00:59:19,535 --> 00:59:19,855
Speaker 7:  this. I did,

956
00:59:20,495 --> 00:59:20,775
Speaker 11:  I did.

957
00:59:21,395 --> 00:59:25,255
Speaker 6:  MAI versus Peak. This nation had to litigate

958
00:59:25,875 --> 00:59:29,855
Speaker 6:  and decide that loading software from a hard drive onto

959
00:59:29,915 --> 00:59:33,815
Speaker 6:  the ram of the computer was fair use if you didn't have

960
00:59:33,815 --> 00:59:37,615
Speaker 6:  permission to do it. 'cause It is a copy. 'cause it's just a copy. That's

961
00:59:37,615 --> 00:59:40,855
Speaker 6:  how dumb copyright law is as doctrine. It is so

962
00:59:41,305 --> 00:59:45,215
Speaker 6:  blunt as an instrument. It is used for all kinds of

963
00:59:45,215 --> 00:59:49,175
Speaker 6:  purposes because it's so blunt and so stupid. It is the only speech regulation

964
00:59:49,175 --> 00:59:52,175
Speaker 6:  that exists in this country for real. Because you, you don't have the moral

965
00:59:52,525 --> 00:59:56,455
Speaker 6:  component of speech. You're just like, did you make a copy jail? And like

966
00:59:56,475 --> 01:00:00,335
Speaker 6:  that's the end of that. Like you don't have to do all the work, but like

967
01:00:00,355 --> 01:00:04,295
Speaker 6:  that's how dumb copyright law is. And so you, with all of this, like maybe

968
01:00:04,295 --> 01:00:07,975
Speaker 6:  you can get all that fair use, but you start with did you make a copy?

969
01:00:08,115 --> 01:00:11,895
Speaker 6:  And like Yeah, you sure did. Like, without

970
01:00:12,135 --> 01:00:12,615
Speaker 6:  question they did.

971
01:00:13,095 --> 01:00:16,935
Speaker 11:  I was reading this Anthropic lawsuit sent to me by shout out Crans

972
01:00:17,395 --> 01:00:20,735
Speaker 11:  and I was reading through it and it, the judge

973
01:00:21,295 --> 01:00:25,135
Speaker 11:  seemingly threw out the case and their one thing was like, okay, you

974
01:00:25,135 --> 01:00:28,375
Speaker 11:  have to create guardrails to prevent it from spitting out

975
01:00:28,995 --> 01:00:32,975
Speaker 11:  actual copies of copyrighted work. But

976
01:00:32,975 --> 01:00:36,695
Speaker 11:  otherwise it seemed fine. And I thought, okay,

977
01:00:36,875 --> 01:00:40,855
Speaker 11:  I'm not a a legal expert, but in this case I can get

978
01:00:40,855 --> 01:00:44,815
Speaker 11:  Rick and Morty's face on just about anything. So that seems like a problem.

979
01:00:45,755 --> 01:00:49,455
Speaker 11:  OpenAI says, has surprisingly lacks guidelines. I was waiting for them to

980
01:00:49,485 --> 01:00:52,855
Speaker 11:  respond with comment and they gave me a ton of very interesting guidelines.

981
01:00:53,015 --> 01:00:56,855
Speaker 11:  I thought it would be more like we do not depict, you know, copyrighted

982
01:00:56,975 --> 01:01:00,575
Speaker 11:  material. We do not depict Totoro or Donald Trump, but they say

983
01:01:01,365 --> 01:01:04,415
Speaker 11:  it's fine for artistic endeavors. The long and short just don't shrink.

984
01:01:04,575 --> 01:01:07,975
Speaker 11:  'cause you can upload a photo and be like, do this in studio Gibble. Don't

985
01:01:07,995 --> 01:01:11,455
Speaker 11:  upload photos that you don't own the copyright to, which is basically what

986
01:01:11,735 --> 01:01:15,455
Speaker 11:  everyone is doing. But otherwise they say you own

987
01:01:15,855 --> 01:01:19,655
Speaker 11:  whatever it generates. And I, I feel like this is going

988
01:01:19,655 --> 01:01:23,495
Speaker 11:  to end up being a nightmare that that's the only answer. Yeah.

989
01:01:23,835 --> 01:01:27,615
Speaker 6:  And there's some amount of this that I think you depend on the volume,

990
01:01:27,685 --> 01:01:31,535
Speaker 6:  just overcoming the objection, right? There's so much infringement

991
01:01:32,005 --> 01:01:35,735
Speaker 6:  that it can't be stopped. Right. You'll be perceived of as the bad guys

992
01:01:35,785 --> 01:01:39,415
Speaker 6:  music industry, you're gonna shut down Napster, Lars,

993
01:01:39,695 --> 01:01:43,015
Speaker 6:  Lars Ulrich is the bad guy now that, and that's basically how that playbook

994
01:01:43,015 --> 01:01:45,655
Speaker 6:  worked. That's how the playbook works against Google. We scanned all the

995
01:01:45,655 --> 01:01:48,935
Speaker 6:  books and made Google Books search and the evil authors Guild does not want

996
01:01:48,935 --> 01:01:52,535
Speaker 6:  you to have it. And the judges were like, but these goofballs have slides

997
01:01:52,535 --> 01:01:56,215
Speaker 6:  in their office. Right? Like YouTube is okay, even though you copied all

998
01:01:56,215 --> 01:01:59,975
Speaker 6:  of South Park onto YouTube. Right? And the utility was so

999
01:01:59,975 --> 01:02:03,395
Speaker 6:  high and the, the industry was so new and young and cute

1000
01:02:03,745 --> 01:02:07,355
Speaker 6:  that like they, they kind of, like Google in particular wrote a lot of the

1001
01:02:07,355 --> 01:02:10,675
Speaker 6:  fair use law here. Like they just litigated their way through a lot of fair

1002
01:02:10,675 --> 01:02:13,995
Speaker 6:  use law. And you can see that reflected, like Eric Schmidt

1003
01:02:14,465 --> 01:02:18,315
Speaker 6:  like gives talks at Stanford, which he tried to get taken down. We

1004
01:02:18,315 --> 01:02:20,925
Speaker 6:  wrote about it and he got tried to get taken down, but it's still up on YouTube.

1005
01:02:20,925 --> 01:02:24,005
Speaker 6:  We can go find it. He's like, here's what I would do. I would start an AI

1006
01:02:24,005 --> 01:02:27,125
Speaker 6:  company. I would fire out products until one hit and then when the lawsuits

1007
01:02:27,125 --> 01:02:29,525
Speaker 6:  came, I would just pay off the lawsuits. And it's like, oh, that's good.

1008
01:02:29,525 --> 01:02:33,325
Speaker 6:  You did, you would do Google. That's what Google did. And I

1009
01:02:33,325 --> 01:02:36,285
Speaker 6:  just don't think these companies are positioned the same way as Google in

1010
01:02:36,285 --> 01:02:39,565
Speaker 6:  the early two thousands. Right? Like this is,

1011
01:02:40,305 --> 01:02:44,285
Speaker 6:  here's anime nine 11 is just not the same product as

1012
01:02:44,595 --> 01:02:48,285
Speaker 6:  YouTube at first impression. And I, I don't know, I don't know what's gonna

1013
01:02:48,285 --> 01:02:52,085
Speaker 6:  happen here. I will like agree with David. The amount of usage

1014
01:02:52,185 --> 01:02:55,605
Speaker 6:  is off the charts. Like people love this shit. They love it.

1015
01:02:56,225 --> 01:03:00,115
Speaker 6:  And there's something to that that I, we talk about all the time.

1016
01:03:00,935 --> 01:03:04,315
Speaker 6:  You're in it, you're like, you're reporting on the companies, the

1017
01:03:04,735 --> 01:03:08,675
Speaker 6:  gap between how much the audience says they hate the tools and then

1018
01:03:08,675 --> 01:03:11,075
Speaker 6:  how much people are using the tools seems to get bigger every day.

1019
01:03:11,535 --> 01:03:15,115
Speaker 11:  Yes, yes. The commenters on my Ghibli posts are

1020
01:03:15,455 --> 01:03:19,235
Speaker 11:  so fun because they're arguing so intensely about, my favorite

1021
01:03:19,235 --> 01:03:23,115
Speaker 11:  comment is someone talking about fascism and then another

1022
01:03:23,115 --> 01:03:26,995
Speaker 11:  person who's like, I just think it's pretty That's perfect. That's perfect.

1023
01:03:27,225 --> 01:03:31,155
Speaker 11:  It's like the perfect VERGE audience comment section. But I think

1024
01:03:31,155 --> 01:03:34,995
Speaker 11:  that, yeah, it does get wider and you know,

1025
01:03:34,995 --> 01:03:38,955
Speaker 11:  this is a very silly thing, but I was just at a doctor's appointment and

1026
01:03:39,055 --> 01:03:42,915
Speaker 11:  all my doctors now use this AI scribe. So it takes notes and they

1027
01:03:42,915 --> 01:03:46,155
Speaker 11:  ask for permission and it just feels like this has become

1028
01:03:47,105 --> 01:03:51,025
Speaker 11:  truly like, permeated into our society. There's no way to

1029
01:03:51,025 --> 01:03:53,705
Speaker 11:  stop it. That's sort of the argument with the Ghibli thing. Like, artists

1030
01:03:53,795 --> 01:03:57,785
Speaker 11:  can't stop this, this is here, this is now we just

1031
01:03:57,785 --> 01:04:01,385
Speaker 11:  have to adapt. I'm not sure that's a fair argument. I would love Eli's legal

1032
01:04:01,415 --> 01:04:04,385
Speaker 11:  take. I was gonna ask what do you think happens? But you already said you

1033
01:04:04,385 --> 01:04:04,745
Speaker 11:  don't know.

1034
01:04:05,495 --> 01:04:08,745
Speaker 6:  Well, I think two things happen. I, I have this theory every time AI art

1035
01:04:08,745 --> 01:04:12,545
Speaker 6:  comes up this way and people use it and they go crazy for it. Like open eye,

1036
01:04:12,715 --> 01:04:16,185
Speaker 6:  their line is like the GPUs are melting. You know, like yes, they have all

1037
01:04:16,185 --> 01:04:19,805
Speaker 6:  this usage. First of all, they're not making a dollar on this usage, right?

1038
01:04:19,835 --> 01:04:23,685
Speaker 6:  Like they're just giving this away at 20 bucks a month. Like they're losing

1039
01:04:23,685 --> 01:04:26,285
Speaker 6:  money on this. Especially now that they're melting the GPUs. Like they're

1040
01:04:26,285 --> 01:04:29,605
Speaker 6:  already losing money now we're using even more compute.

1041
01:04:29,985 --> 01:04:32,685
Speaker 6:  You're we're losing more money. So I don't know what happens there, like

1042
01:04:33,025 --> 01:04:36,805
Speaker 6:  on its own. I don't know what, what's happening. I do have this like broader

1043
01:04:37,595 --> 01:04:41,365
Speaker 6:  like vergeer thesis that all of this technology is meant for us

1044
01:04:41,425 --> 01:04:45,205
Speaker 6:  to make art with and like the most human possible instinct

1045
01:04:45,585 --> 01:04:49,445
Speaker 6:  is to somehow communicate art and try to get people

1046
01:04:49,665 --> 01:04:53,445
Speaker 6:  to, to feel things. Yeah. Like Becky has like live laugh,

1047
01:04:53,795 --> 01:04:57,725
Speaker 6:  love ants, you know, like you go in the Midwest. Yeah. And I, I always

1048
01:04:57,725 --> 01:05:01,635
Speaker 6:  think about those. It's like, oh, they, this is a command. Like they

1049
01:05:01,635 --> 01:05:04,515
Speaker 6:  just want me to go in their kitchen and they're like laugh and like that's

1050
01:05:04,515 --> 01:05:07,715
Speaker 6:  how they want me to feel. Yeah. Right. Yeah. And like I have a lot of empathy.

1051
01:05:07,855 --> 01:05:10,875
Speaker 6:  I'm like, I, it's kind of blunt force like love, like

1052
01:05:11,695 --> 01:05:14,515
Speaker 6:  do it And they have a sign that says eat over the dining room table.

1053
01:05:14,705 --> 01:05:14,995
Speaker 11:  Yeah.

1054
01:05:15,135 --> 01:05:18,805
Speaker 6:  Eat. These are just instructions on like what to feel and ever

1055
01:05:18,985 --> 01:05:22,925
Speaker 6:  sleepy. You know, like that's all It is. Like it's, it's so human

1056
01:05:22,985 --> 01:05:26,965
Speaker 6:  to try to communicate what you want other people to feel. And

1057
01:05:26,965 --> 01:05:30,885
Speaker 6:  it's pretty hard to do that. Like, I don't have a 6-year-old and

1058
01:05:30,885 --> 01:05:34,005
Speaker 6:  like mostly what she wants me to feel is pain, I guess. Like, I don't know.

1059
01:05:34,005 --> 01:05:36,605
Speaker 6:  Yeah. There's something there. She's very sweet.

1060
01:05:40,385 --> 01:05:41,605
Speaker 6:  She kicks hard. That's all I'm saying.

1061
01:05:44,185 --> 01:05:47,885
Speaker 6:  But like, you see people get access to these tools and they're

1062
01:05:47,915 --> 01:05:51,605
Speaker 6:  able to communicate that emotional thing so much

1063
01:05:51,605 --> 01:05:54,125
Speaker 6:  easier and then they're into it.

1064
01:05:54,635 --> 01:05:54,925
Speaker 11:  Yeah.

1065
01:05:55,225 --> 01:05:58,405
Speaker 6:  And that to me that's the point. That's like you watch the original Steve

1066
01:05:58,405 --> 01:06:02,365
Speaker 6:  Jobs demo of Garage Band. That's the argument he's making about all

1067
01:06:02,365 --> 01:06:06,165
Speaker 6:  this stuff. Right? Right. And so people have cameras on their phones and

1068
01:06:06,165 --> 01:06:09,725
Speaker 6:  so like there's something very important there, but then there's also,

1069
01:06:10,065 --> 01:06:12,925
Speaker 6:  you shouldn't steal it from the people who figured out how to do it well.

1070
01:06:13,215 --> 01:06:16,325
Speaker 6:  Right. And that is by far the tension that I see.

1071
01:06:16,755 --> 01:06:19,685
Speaker 7:  Yeah. I think make it easy for people to make things

1072
01:06:20,805 --> 01:06:24,675
Speaker 7:  is a very different argument from Make Rick

1073
01:06:24,675 --> 01:06:27,595
Speaker 7:  and Morty do nine 11. Right. Like I just, I I think the AI

1074
01:06:27,795 --> 01:06:29,355
Speaker 6:  Companies, I mean, did did make you feel feelings, David? I think

1075
01:06:29,705 --> 01:06:29,995
Speaker 7:  Love,

1076
01:06:32,095 --> 01:06:34,995
Speaker 7:  But I think, I think the AI companies would like you to believe that those

1077
01:06:34,995 --> 01:06:37,515
Speaker 7:  are the same thing. Right? Yeah. And that this is, and I think this really

1078
01:06:37,515 --> 01:06:41,435
Speaker 7:  comes also back to a lot of OpenAI says like running away

1079
01:06:41,435 --> 01:06:45,035
Speaker 7:  from guardrails. Like Yeah. The, all of these companies are basically like,

1080
01:06:45,055 --> 01:06:48,515
Speaker 7:  all we make is is sort of underlying technology. And what you do with this

1081
01:06:48,665 --> 01:06:52,595
Speaker 7:  good and bad is up to you. And that is, that is a thing they have to

1082
01:06:52,595 --> 01:06:55,635
Speaker 7:  put on society so that they don't have to put it on themselves. That like

1083
01:06:55,635 --> 01:06:59,515
Speaker 7:  the results of this black box are not our fault. All we built is the box.

1084
01:06:59,705 --> 01:07:03,315
Speaker 7:  What you do with It is up to you. Right. And that's very clearly what OpenAI

1085
01:07:03,315 --> 01:07:05,835
Speaker 7:  says is doing here. Like they've, they've tried to set up guardrails, they've

1086
01:07:05,835 --> 01:07:08,635
Speaker 7:  tried to do the right thing, they write long documents and then people make

1087
01:07:08,675 --> 01:07:12,075
Speaker 7:  Rick and Morty do nine 11 and now OpenAI says is like, that's your problem.

1088
01:07:12,105 --> 01:07:15,875
Speaker 7:  Yeah. You, you typed in the words what comes out is not our responsibility.

1089
01:07:16,355 --> 01:07:19,195
Speaker 11:  I saw someone say that the larger story isn't that you can make Ghibli.

1090
01:07:19,195 --> 01:07:22,955
Speaker 11:  Now the larger story is that they've just dropped their guardrails. Yeah.

1091
01:07:22,955 --> 01:07:26,875
Speaker 11:  And they're walking away from those guardrails very publicly and

1092
01:07:26,905 --> 01:07:30,515
Speaker 11:  like where does that line stop? Because clearly it hasn't,

1093
01:07:30,935 --> 01:07:34,795
Speaker 11:  you know, we were just talking about grok not that long ago,

1094
01:07:34,795 --> 01:07:38,595
Speaker 11:  making Kamala Harris with a gun and that was a bad thing. And

1095
01:07:39,395 --> 01:07:43,275
Speaker 11:  I just saw like a depiction of Korean Americans in the eighties holding

1096
01:07:43,275 --> 01:07:46,835
Speaker 11:  guns on rooftops, stirring the LA riots. Like,

1097
01:07:47,195 --> 01:07:50,995
Speaker 11:  I mean, like where is the line? It's, it seems

1098
01:07:50,995 --> 01:07:54,875
Speaker 11:  to blur more and more as they think that maybe this will make them

1099
01:07:54,875 --> 01:07:56,955
Speaker 11:  more money and get them more users. I'm not, I'm not sure.

1100
01:07:57,235 --> 01:08:00,115
Speaker 7:  Yeah. I mean I, this is the same thing as meta walking away from content

1101
01:08:00,115 --> 01:08:03,875
Speaker 7:  moderation. Right? Like it's, it's just easier to say, this isn't our problem

1102
01:08:04,065 --> 01:08:06,115
Speaker 7:  than to take any kind of responsibility for it.

1103
01:08:06,445 --> 01:08:06,795
Speaker 11:  Right.

1104
01:08:07,215 --> 01:08:10,635
Speaker 6:  And this is where I will remind you once again, the only functional speech

1105
01:08:10,635 --> 01:08:14,435
Speaker 6:  regulation in this country is copyright law. It's the only thing that

1106
01:08:14,635 --> 01:08:18,085
Speaker 6:  regulates the internet. 'cause when Disney shows up and says, you stole the

1107
01:08:18,245 --> 01:08:21,805
Speaker 6:  Avengers, the court system is like, yep, jail. And it works

1108
01:08:21,935 --> 01:08:25,885
Speaker 6:  every time. Like without question, if, what is

1109
01:08:25,885 --> 01:08:29,645
Speaker 6:  the most, what is the most effective regulation on

1110
01:08:29,645 --> 01:08:32,925
Speaker 6:  YouTube that is built its own culture? Is copyright strikes Disney?

1111
01:08:33,035 --> 01:08:33,325
Speaker 7:  Yeah.

1112
01:08:33,795 --> 01:08:37,365
Speaker 6:  It's, it's copyright strikes. It's using the music wrong. It's why is TikTok

1113
01:08:37,365 --> 01:08:40,845
Speaker 6:  full of weird sped up movies with that line down the center

1114
01:08:41,345 --> 01:08:45,325
Speaker 6:  to avoid the copyright filters that everyone agrees should exist. There's

1115
01:08:45,325 --> 01:08:48,325
Speaker 6:  no moral outrage about that filter. But if you do actual content moderation,

1116
01:08:48,325 --> 01:08:51,845
Speaker 6:  then there is moral outrage. And so you just, here's this problem,

1117
01:08:52,535 --> 01:08:55,685
Speaker 6:  right? Where OpenAI says isn't gonna make these choices,

1118
01:08:56,635 --> 01:09:00,495
Speaker 6:  this government is certainly not gonna make these choices. Or if

1119
01:09:00,495 --> 01:09:03,535
Speaker 6:  they do, it will be in the, the worst possible way and you'll be abducted

1120
01:09:03,535 --> 01:09:07,495
Speaker 6:  to El Salvador in the dead of night. That's weird. That's a thing

1121
01:09:07,495 --> 01:09:11,135
Speaker 6:  that we're comfortable doing. But like content moderation is not, so

1122
01:09:11,475 --> 01:09:15,135
Speaker 6:  the thing that will step in will be these copyright fights and they are

1123
01:09:15,205 --> 01:09:17,375
Speaker 6:  existential and everyone kind of glosses over them.

1124
01:09:17,855 --> 01:09:20,215
Speaker 7:  I had never thought about this until you said YouTube copyright strikes.

1125
01:09:20,215 --> 01:09:24,015
Speaker 7:  But I, I almost wonder if what these AI companies are gonna want is

1126
01:09:24,815 --> 01:09:28,735
Speaker 7:  a version of the same thing for these AI tools that doesn't say

1127
01:09:28,735 --> 01:09:32,655
Speaker 7:  OpenAI says is guilty of copyright problems. I am, when I

1128
01:09:32,755 --> 01:09:36,695
Speaker 7:  upload the, when I ask for a Studio Ghibli that's in their guidelines style

1129
01:09:36,695 --> 01:09:40,015
Speaker 7:  image's, right? I think OpenAI says, would really like that to be my problem.

1130
01:09:40,475 --> 01:09:44,375
Speaker 7:  And, and that Studio Ghibli or Disney or whoever can come after me and

1131
01:09:44,515 --> 01:09:45,455
Speaker 7:  not OpenAI says,

1132
01:09:45,835 --> 01:09:49,455
Speaker 11:  You can go to a copyright page at OpenAI says and report copyright and

1133
01:09:49,745 --> 01:09:53,335
Speaker 11:  infringers mul like people who have done it multiple times, they will

1134
01:09:53,935 --> 01:09:57,735
Speaker 11:  ban their account. That's what it says. So I mean, that is already in

1135
01:09:57,735 --> 01:09:57,975
Speaker 11:  motion.

1136
01:09:58,495 --> 01:10:01,095
Speaker 7:  I was just about to say that's one of the things that has been really successful

1137
01:10:01,095 --> 01:10:04,775
Speaker 7:  on YouTube is the like chilling effect that the possibility of copyright

1138
01:10:04,775 --> 01:10:08,695
Speaker 7:  strikes has every creator worries about this

1139
01:10:08,795 --> 01:10:12,335
Speaker 7:  all the time. Even like, even the possibility of of an incorrect copyright

1140
01:10:12,335 --> 01:10:16,055
Speaker 7:  strike can be a huge problem because it takes a long time to

1141
01:10:16,055 --> 01:10:19,335
Speaker 7:  litigate these things can be overzealous sometimes like it, and it's a huge

1142
01:10:19,335 --> 01:10:22,015
Speaker 7:  problem if you, if you get copyright strikes, you get demonetized, you get

1143
01:10:22,015 --> 01:10:25,615
Speaker 7:  kicked off the platform. Like you lose your living in a lot of ways. And

1144
01:10:27,015 --> 01:10:30,935
Speaker 7:  I, a system like that for AI content, boy

1145
01:10:30,935 --> 01:10:34,415
Speaker 7:  does that benefit the AI companies at the cost of everybody else.

1146
01:10:34,595 --> 01:10:38,295
Speaker 6:  But that's all rooted in the digital Millennium Copyright Act like

1147
01:10:39,155 --> 01:10:42,615
Speaker 6:  the safe, literally it's called the safe harbor in the

1148
01:10:42,695 --> 01:10:46,495
Speaker 6:  DMCA where by having notice and take down and copyright

1149
01:10:46,495 --> 01:10:50,335
Speaker 6:  strikes and all this system, YouTube doesn't have the liability for the

1150
01:10:50,335 --> 01:10:53,095
Speaker 6:  infringement. Right. The law says if you can build this mechanism where things

1151
01:10:53,115 --> 01:10:57,095
Speaker 6:  get taken down, you will avoid the liability for the cop infringement on

1152
01:10:57,095 --> 01:11:00,375
Speaker 6:  your platform. That doesn't make the same sense for OpenAI says, 'cause they

1153
01:11:00,375 --> 01:11:04,215
Speaker 6:  don't distribute it. Right? Like it's, it's weird to be like, I

1154
01:11:04,215 --> 01:11:07,855
Speaker 6:  will report this Mickey Mouse to OpenAI

1155
01:11:07,925 --> 01:11:11,695
Speaker 6:  says like, why you're, you're gonna distribute it on some other platform,

1156
01:11:11,695 --> 01:11:15,125
Speaker 6:  right? Yeah. So like they're using the law in a way, a way that doesn't make

1157
01:11:15,125 --> 01:11:18,525
Speaker 6:  any sense and none of that has anything to do with how much data have they

1158
01:11:18,525 --> 01:11:22,445
Speaker 6:  trained on? How much did they ingest to do the training, which is a real

1159
01:11:22,445 --> 01:11:26,245
Speaker 6:  problem. Yeah. With meta is getting in trouble every day because in

1160
01:11:26,245 --> 01:11:29,805
Speaker 6:  their court cases it's coming out, they tore into books to put

1161
01:11:29,955 --> 01:11:33,205
Speaker 6:  into the in Salas training and like that's just

1162
01:11:33,475 --> 01:11:36,845
Speaker 6:  copyright. Like, did you make a copy? Yes. Did you have permission? Definitely

1163
01:11:36,865 --> 01:11:37,085
Speaker 6:  not.

1164
01:11:37,265 --> 01:11:40,285
Speaker 11:  And the researchers are scared of it in the lawsuit. The researchers are

1165
01:11:40,285 --> 01:11:42,885
Speaker 11:  like, we shouldn't be doing this. Especially on company laptops.

1166
01:11:43,595 --> 01:11:47,565
Speaker 6:  It's like, yeah, I'm just, again, it's a copyright a stupid flow chart.

1167
01:11:47,565 --> 01:11:51,405
Speaker 6:  Did you make a copy? Yeah. Did you have permission? Well, it, you knew

1168
01:11:51,405 --> 01:11:55,205
Speaker 6:  you didn't, does it fit into one of these exceptions in fair use?

1169
01:11:55,585 --> 01:11:59,445
Speaker 6:  The judge has to decide and you're not exactly like a friendly

1170
01:12:00,425 --> 01:12:03,805
Speaker 6:  cuddly little kid anymore. Mark Zuckerberg, you're Mark Zuckerberg.

1171
01:12:04,635 --> 01:12:05,725
Speaker 6:  Like, good luck.

1172
01:12:06,145 --> 01:12:10,085
Speaker 11:  So the move is like, break the law until it changes for us. Is that

1173
01:12:10,085 --> 01:12:10,845
Speaker 11:  sort of the vibe,

1174
01:12:11,265 --> 01:12:15,125
Speaker 6:  The move? And I think I I I think this is just a a, a law of The

1175
01:12:15,245 --> 01:12:18,485
Speaker 6:  Vergecast. If you're young and cute, you should break the law as much as

1176
01:12:18,485 --> 01:12:20,645
Speaker 6:  you can. And then later on in life you should stop

1177
01:12:21,055 --> 01:12:22,165
Speaker 11:  Incredible. Both

1178
01:12:22,165 --> 01:12:24,005
Speaker 7:  As a, as a person and a company. I

1179
01:12:24,005 --> 01:12:25,845
Speaker 11:  Have some felonies I can pick up. Yeah,

1180
01:12:25,845 --> 01:12:29,565
Speaker 6:  Exactly. You got time. If you're under 30, go nuts. You know, like, have

1181
01:12:29,565 --> 01:12:33,485
Speaker 6:  a good time. Perfect. And then later on in life, you know, settle down. I

1182
01:12:33,485 --> 01:12:33,725
Speaker 6:  think that's,

1183
01:12:33,825 --> 01:12:35,925
Speaker 7:  You buy a suit and, and you testify in front of Congress. That's,

1184
01:12:36,105 --> 01:12:38,405
Speaker 6:  And that's why whenever people get caught now they're like, he was young

1185
01:12:38,405 --> 01:12:40,845
Speaker 6:  and you're like, he's 45 years old. I dunno what you're talking about. That

1186
01:12:40,845 --> 01:12:41,325
Speaker 11:  Is a grown man.

1187
01:12:41,995 --> 01:12:45,765
Speaker 6:  Like that's just an adult. Like he, that's a house and

1188
01:12:45,845 --> 01:12:49,285
Speaker 6:  two kids and a mortgage. Like that's an adult. I don't know. I I don't know

1189
01:12:49,285 --> 01:12:52,685
Speaker 6:  what's gonna happen. I I I think the economic pressure on OpenAI says

1190
01:12:53,145 --> 01:12:56,165
Speaker 6:  is high. Like they, they have to be losing more money than they were yesterday

1191
01:12:56,165 --> 01:12:57,445
Speaker 6:  because of this tool. Yes. Right.

1192
01:12:57,825 --> 01:13:00,605
Speaker 7:  Are we sure that's true, Kylie? I I actually wanna know what you think about

1193
01:13:00,605 --> 01:13:02,725
Speaker 7:  this Kylie. Yeah. Because one of the other things that happened this week

1194
01:13:02,725 --> 01:13:06,645
Speaker 7:  was OpenAI says, said it expects to earn, was it $12.7 billion in

1195
01:13:06,645 --> 01:13:10,565
Speaker 7:  revenue? Yeah, that's a big number. I'm sure it comes underneath a much

1196
01:13:10,565 --> 01:13:13,325
Speaker 7:  bigger cost number because that's what OpenAI says is doing.

1197
01:13:14,585 --> 01:13:17,805
Speaker 7:  But this company seems to essentially be able to raise as much money as it

1198
01:13:17,805 --> 01:13:20,405
Speaker 7:  needs whenever it needs for any purposes whatsoever.

1199
01:13:22,545 --> 01:13:25,765
Speaker 7:  And I'm sort of at a point now where I'm like, maybe this company can just

1200
01:13:25,765 --> 01:13:29,405
Speaker 7:  capitalize itself forever. And, and all these numbers are fake and none of

1201
01:13:29,405 --> 01:13:31,965
Speaker 7:  it really matters. Where is your head with this stuff?

1202
01:13:32,565 --> 01:13:36,485
Speaker 11:  I agree because I'm more of an AI expert, the expert than I am

1203
01:13:36,525 --> 01:13:39,445
Speaker 11:  a finance expert. So when I see those numbers, I'm like, how, how is this

1204
01:13:39,445 --> 01:13:43,365
Speaker 11:  company not totally crumbling? Some of these AI leaders

1205
01:13:43,395 --> 01:13:47,365
Speaker 11:  will point to Amazon. They never turned a profit, but I mean they were doing

1206
01:13:47,365 --> 01:13:50,655
Speaker 11:  something more useful. I think

1207
01:13:51,335 --> 01:13:51,495
Speaker 11:  honestly,

1208
01:13:51,795 --> 01:13:55,255
Speaker 7:  Amazon took all of the money that it made and put it back in the company.

1209
01:13:55,325 --> 01:13:58,895
Speaker 7:  They didn't lose money on every single thing that they shifted That was not,

1210
01:13:59,165 --> 01:14:00,655
Speaker 7:  that was not the Amazon way.

1211
01:14:01,945 --> 01:14:05,775
Speaker 11:  Right. And I, I genuinely do think that OpenAI says

1212
01:14:06,555 --> 01:14:10,255
Speaker 11:  is okay with losing this amount of money. Maybe the new CFO

1213
01:14:10,305 --> 01:14:14,165
Speaker 11:  isn't, but it seems like Sam Altman is totally fine with it. And

1214
01:14:14,685 --> 01:14:18,445
Speaker 11:  I, I don't know where that stops and what happens to a company.

1215
01:14:18,715 --> 01:14:22,125
Speaker 11:  I've never seen a company like this. Their, their new valuation would be

1216
01:14:22,125 --> 01:14:25,685
Speaker 11:  $300 billion and they're not even close to profitable. They're losing billions

1217
01:14:25,685 --> 01:14:29,605
Speaker 11:  and billions every year and they only want to keep losing

1218
01:14:29,835 --> 01:14:33,685
Speaker 11:  more money for studio Ghibli renderings. That doesn't

1219
01:14:33,685 --> 01:14:37,125
Speaker 11:  make a ton of sense to me. And it does make me nervous, especially what

1220
01:14:37,125 --> 01:14:40,365
Speaker 11:  we saw with the deep seek moment and what that did to the markets. And Nvidia,

1221
01:14:41,285 --> 01:14:44,125
Speaker 11:  I, I worry that the OpenAI says bubble

1222
01:14:45,495 --> 01:14:49,185
Speaker 11:  will be really destructive, but I don't wanna scare anybody. I'm not sure.

1223
01:14:50,295 --> 01:14:54,065
Speaker 6:  Well, it's funny, I don't like, you know, the new image generation

1224
01:14:54,305 --> 01:14:58,195
Speaker 6:  features, they're not the promise. Right. The the promise is

1225
01:14:58,195 --> 01:15:01,755
Speaker 6:  in the reasoning models and the agentic stuff and the replace all of your

1226
01:15:01,915 --> 01:15:05,835
Speaker 6:  employees with, with this system and image generation might

1227
01:15:05,835 --> 01:15:09,755
Speaker 6:  be some component of that. But you see, it's not like the interest in

1228
01:15:09,755 --> 01:15:13,675
Speaker 6:  it and the hype around it. It has nothing to do with what might ultimately

1229
01:15:13,675 --> 01:15:17,425
Speaker 6:  justify the valuation. Right. And that seems like as big of a

1230
01:15:17,425 --> 01:15:20,265
Speaker 6:  disconnect as anything like, oh, this costs more money than it did yesterday.

1231
01:15:20,685 --> 01:15:23,865
Speaker 6:  It doesn't meaningfully bring us closer to earning all that money back.

1232
01:15:24,515 --> 01:15:28,025
Speaker 11:  Right. You still need users though. And they don't, the average person does

1233
01:15:28,025 --> 01:15:31,265
Speaker 11:  not care about a GI, they wanna make their daughter into a Studio Ghibli

1234
01:15:31,265 --> 01:15:35,185
Speaker 11:  character. Like they, they've gotta get somebody to subscribe and, and care.

1235
01:15:35,245 --> 01:15:38,625
Speaker 11:  And I feel that way about, you know, OpenAI says for a while it's like no

1236
01:15:38,625 --> 01:15:41,945
Speaker 11:  one cares about, you know, a very small sect of people in Silicon Valley

1237
01:15:41,975 --> 01:15:45,865
Speaker 11:  care about it being the best coder and, and getting a PhD in mathematics.

1238
01:15:45,965 --> 01:15:49,905
Speaker 11:  And I think these are important endeavors, but you know, Google is

1239
01:15:49,905 --> 01:15:53,705
Speaker 11:  baking it into every product. Meta is baking it into every

1240
01:15:53,705 --> 01:15:57,425
Speaker 11:  product. Whether that's a good thing or not, I'm not sure, but it, it seems

1241
01:15:57,815 --> 01:16:01,585
Speaker 11:  more real than a GI So make something people

1242
01:16:01,815 --> 01:16:04,305
Speaker 11:  want to use right now. Seems to be studio gli.

1243
01:16:04,415 --> 01:16:08,345
Speaker 7:  It's also like sneakily a really mainstream use case. Yeah. Like

1244
01:16:08,345 --> 01:16:11,305
Speaker 7:  it, it's, we've talked a lot about like none of these products are things

1245
01:16:11,535 --> 01:16:14,145
Speaker 7:  regular people are actually gonna want to do all the time. But you're right.

1246
01:16:14,145 --> 01:16:18,065
Speaker 7:  Like I can take my family portrait and make it into any style I want and

1247
01:16:18,065 --> 01:16:21,925
Speaker 7:  It is like compelling That's interesting to everybody.

1248
01:16:22,155 --> 01:16:25,565
Speaker 7:  Yeah. I don't think a many people will pay 20 bucks a month for that for

1249
01:16:25,565 --> 01:16:29,325
Speaker 7:  the rest of their lives. But that is like, that is a genuinely mainstream

1250
01:16:29,465 --> 01:16:33,045
Speaker 7:  use case for AI in a way that even like helped me write my emails

1251
01:16:33,255 --> 01:16:33,605
Speaker 7:  isn't

1252
01:16:34,395 --> 01:16:37,245
Speaker 6:  Yeah. Also you can see it, right? That's, yeah, It is like we we're doing

1253
01:16:37,245 --> 01:16:40,805
Speaker 6:  the pixels left to right up to down is like a meaningful visual

1254
01:16:40,805 --> 01:16:44,125
Speaker 6:  improvement over regular diffusion. Yeah. I would just compare that to this

1255
01:16:44,125 --> 01:16:47,005
Speaker 6:  other headline which says, Google says its new reasoning, Gemini Am models

1256
01:16:47,005 --> 01:16:50,365
Speaker 6:  are the best one yet. It's like most people can't evaluate slightly better

1257
01:16:50,365 --> 01:16:50,885
Speaker 6:  reasoning.

1258
01:16:51,275 --> 01:16:52,365
Speaker 11:  Yeah. They

1259
01:16:52,365 --> 01:16:55,965
Speaker 6:  Can say that it looks different. Like Microsoft is adding

1260
01:16:55,965 --> 01:16:59,765
Speaker 6:  reasoning to copilot AI for research and data. Like great. Is it

1261
01:16:59,765 --> 01:17:02,005
Speaker 6:  better than it was yesterday? Right.

1262
01:17:02,115 --> 01:17:04,845
Speaker 7:  This is, this is one of the things Kylie and I always enjoy talking about

1263
01:17:04,865 --> 01:17:07,965
Speaker 7:  is this like endless race where every 15 minutes somebody makes the best

1264
01:17:07,965 --> 01:17:11,485
Speaker 7:  model that has ever been made in history. Yes. And It is, it wins

1265
01:17:11,905 --> 01:17:15,845
Speaker 7:  all It is at the top of every leaderboard for somewhere

1266
01:17:15,845 --> 01:17:19,685
Speaker 7:  between an hour and a week and then It is not anymore. And I think

1267
01:17:20,275 --> 01:17:23,365
Speaker 7:  this, this week was there, there were like three of them that they were like,

1268
01:17:23,365 --> 01:17:27,325
Speaker 7:  this is the best model we've ever made. Right. And Google, Google and Open,

1269
01:17:27,325 --> 01:17:30,085
Speaker 7:  is it Google and OpenAI says that are obsessively trying to like front run

1270
01:17:30,085 --> 01:17:32,125
Speaker 7:  each other's announcements. Yes. With an It's so fun.

1271
01:17:32,875 --> 01:17:36,405
Speaker 11:  It's so fun. It's so fun. When I saw, you know, I got the OpenAI says

1272
01:17:36,685 --> 01:17:40,005
Speaker 11:  announcement and then the Google announcement came in, I thought, oh my

1273
01:17:40,065 --> 01:17:43,605
Speaker 11:  God, they're back at it again. What? Like it's the most pointless thing.

1274
01:17:43,625 --> 01:17:47,085
Speaker 11:  And what's even funnier is I have all these people's tweet notifications

1275
01:17:47,085 --> 01:17:50,645
Speaker 11:  turned on and I was watching Demis do some

1276
01:17:50,925 --> 01:17:54,005
Speaker 11:  retweets that were like Subtweeting OpenAI says and saying like, oh, well

1277
01:17:54,005 --> 01:17:56,885
Speaker 11:  we already had image generation and blah blah blah before they had even

1278
01:17:57,125 --> 01:18:01,005
Speaker 11:  properly announced it. And I thought, wow, you guys are petty. And it's,

1279
01:18:01,475 --> 01:18:05,245
Speaker 11:  it's like such a useless, the average consumer does not notice this

1280
01:18:05,245 --> 01:18:06,645
Speaker 11:  does not care, you know?

1281
01:18:06,945 --> 01:18:10,685
Speaker 6:  No, but It is good to know that you can be a billionaire Nobel Prize

1282
01:18:10,685 --> 01:18:14,445
Speaker 6:  winner and still be just furiously subtweeting your competition

1283
01:18:14,445 --> 01:18:16,965
Speaker 7:  On that. Turns out Twitter breaks your brain no matter who you are. Yes.

1284
01:18:16,965 --> 01:18:19,965
Speaker 7:  Like that is, that is the great lesson of our times is that Twitter will

1285
01:18:19,965 --> 01:18:22,245
Speaker 7:  destroy you no matter who or what you are.

1286
01:18:22,515 --> 01:18:25,205
Speaker 6:  When we get to a GI, we'll finally take this app off our phones.

1287
01:18:26,385 --> 01:18:29,925
Speaker 7:  That's the goal. I do think I, there's one trend inside of all of this that

1288
01:18:29,965 --> 01:18:32,685
Speaker 7:  I think is really interesting, which is that everybody is getting really

1289
01:18:32,685 --> 01:18:36,605
Speaker 7:  comfortable making slower products. And we were on this

1290
01:18:36,605 --> 01:18:40,565
Speaker 7:  race for a while where everything had to be fast and everybody was obsessed

1291
01:18:40,565 --> 01:18:44,045
Speaker 7:  with latency and everybody was like, how do we make, you know, smaller, better,

1292
01:18:44,275 --> 01:18:48,205
Speaker 7:  quicker models, everything should be instant. And feedback is, and now everybody's

1293
01:18:48,205 --> 01:18:50,925
Speaker 7:  like, we're gonna build a deep reasoning model that is going to take its

1294
01:18:50,925 --> 01:18:53,725
Speaker 7:  time and it's gonna think and it's gonna show you that it's thinking and

1295
01:18:53,725 --> 01:18:57,125
Speaker 7:  it's gonna take a while and it's gonna happen step by step. But then at the

1296
01:18:57,125 --> 01:19:00,245
Speaker 7:  end it's gonna say true things and not lies. And

1297
01:19:01,115 --> 01:19:05,045
Speaker 7:  that is we've just like hard turned into, it's okay if this takes a

1298
01:19:05,045 --> 01:19:08,765
Speaker 7:  while because the product is going to be better. And that is like so completely

1299
01:19:08,765 --> 01:19:12,485
Speaker 7:  different than the conversations I was having with AI people even like a

1300
01:19:12,485 --> 01:19:15,125
Speaker 7:  year ago who were like, speed is everything. Yes. We have to make this stuff

1301
01:19:15,125 --> 01:19:17,805
Speaker 7:  real time. It has to be fast or else nobody's gonna use it. And It is like,

1302
01:19:17,805 --> 01:19:19,525
Speaker 7:  we've just gone all the way away from that.

1303
01:19:19,815 --> 01:19:23,285
Speaker 6:  Right. 'cause lying fast turned out to not be a great product. Right. Right.

1304
01:19:24,485 --> 01:19:27,845
Speaker 6:  I think they, they we'll come back to speed once they solve the, like, is

1305
01:19:27,845 --> 01:19:28,405
Speaker 6:  it good enough?

1306
01:19:28,855 --> 01:19:29,205
Speaker 11:  Right.

1307
01:19:29,315 --> 01:19:30,285
Speaker 7:  That might be true. Yeah.

1308
01:19:30,345 --> 01:19:33,765
Speaker 6:  But I, I think that this is useless. Does not being useless fast

1309
01:19:34,275 --> 01:19:38,205
Speaker 6:  does not get you what what anybody needs to keep the promises that

1310
01:19:38,205 --> 01:19:38,765
Speaker 6:  they're, they're making.

1311
01:19:39,155 --> 01:19:42,085
Speaker 7:  Yeah. I just always enjoy, every time somebody talks about deep reasoning,

1312
01:19:42,485 --> 01:19:44,125
Speaker 7:  I just hear slow. Yes.

1313
01:19:44,265 --> 01:19:47,925
Speaker 11:  And it's, that's when they, when they ask me about the, like the new reasoning

1314
01:19:47,965 --> 01:19:51,365
Speaker 11:  powering images, my first question was like, is this slower? And they said

1315
01:19:51,425 --> 01:19:54,525
Speaker 11:  Yes, but I think people are gonna enjoy the product and it, it won't matter

1316
01:19:54,745 --> 01:19:57,245
Speaker 11:  was their answer. And it's Yeah. Wild.

1317
01:19:57,465 --> 01:20:01,125
Speaker 7:  It turns out it's fine. Right. Like turns out it's, and and, and they're

1318
01:20:01,285 --> 01:20:04,045
Speaker 7:  learning how to build product that like shows you the slowness, right. Where

1319
01:20:04,045 --> 01:20:06,725
Speaker 7:  they're like, it, it, it shows you the turns in its thinking. And part of

1320
01:20:06,725 --> 01:20:09,325
Speaker 7:  that is, is to like help you understand whatever. But part of that is also

1321
01:20:09,325 --> 01:20:13,245
Speaker 7:  to like give you something to do while it's slow and all of the

1322
01:20:13,245 --> 01:20:17,045
Speaker 7:  sort of affordances these things are getting so that they can be slow so

1323
01:20:17,045 --> 01:20:20,325
Speaker 7:  that they can not lie to you all the time is just fascinating. Yeah.

1324
01:20:20,425 --> 01:20:24,245
Speaker 11:  The the best part though is people putting in a prompt that it

1325
01:20:24,245 --> 01:20:27,925
Speaker 11:  should not generate and it starts generating and you see a sliver of

1326
01:20:28,265 --> 01:20:32,125
Speaker 11:  the, the band content before it stops. And that makes for

1327
01:20:32,125 --> 01:20:34,445
Speaker 11:  incredible means. Unfortunately. That's,

1328
01:20:34,445 --> 01:20:35,005
Speaker 7:  That's very good.

1329
01:20:35,395 --> 01:20:37,485
Speaker 6:  Well that's happening less and less apparently.

1330
01:20:37,935 --> 01:20:38,285
Speaker 11:  Right.

1331
01:20:39,265 --> 01:20:41,765
Speaker 6:  Do you think Open Eyes is gonna react to this? Right? Like Sam Altman is

1332
01:20:41,765 --> 01:20:45,605
Speaker 6:  saying, yeah, this is, I shouldn't be this person. Like

1333
01:20:45,745 --> 01:20:49,565
Speaker 6:  all my life's work has ended with this picture. And then Opening Eye had

1334
01:20:49,565 --> 01:20:52,845
Speaker 6:  some reactions saying we should, we don't want this to happen as much. There

1335
01:20:52,845 --> 01:20:55,885
Speaker 6:  is the looming copyright threat. Do you think they're gonna to pull back

1336
01:20:55,885 --> 01:20:56,565
Speaker 6:  on this a little bit?

1337
01:20:56,815 --> 01:21:00,125
Speaker 11:  Right. Someone made the joke about how he's like doing the legal back flips

1338
01:21:00,125 --> 01:21:03,165
Speaker 11:  He did after the Scar Joe stuff. The Scarlet Johansson stuff.

1339
01:21:04,755 --> 01:21:08,405
Speaker 11:  Yeah. I think that they're seeing, you know, Sam made this, his profile

1340
01:21:08,405 --> 01:21:11,165
Speaker 11:  picture. He made a Studio Ghibli profile picture and he said, can everyone

1341
01:21:11,165 --> 01:21:14,765
Speaker 11:  make me a better one so they're not shying away from it. And, and the email

1342
01:21:14,765 --> 01:21:17,725
Speaker 11:  they sent me, they're like, we encourage and we love to see all these renditions

1343
01:21:17,825 --> 01:21:21,405
Speaker 11:  and something that was really weird and their statement was,

1344
01:21:21,745 --> 01:21:25,485
Speaker 11:  you know, we don't allow the recreation

1345
01:21:25,905 --> 01:21:29,365
Speaker 11:  of art from living artists. And I'm like, the Studio

1346
01:21:29,765 --> 01:21:31,925
Speaker 11:  Ghibli co-founder is still alive.

1347
01:21:32,035 --> 01:21:35,325
Speaker 6:  Yeah. Miyazaki's alive and he hates ai. Yeah. He's so just tweeting about

1348
01:21:35,325 --> 01:21:35,725
Speaker 6:  how much he,

1349
01:21:35,725 --> 01:21:36,005
Speaker 7:  This

1350
01:21:36,715 --> 01:21:40,285
Speaker 11:  It's, so, I so putting that aside,

1351
01:21:41,525 --> 01:21:45,285
Speaker 11:  I think that they are leaning into this until they get a seasoned desist

1352
01:21:45,505 --> 01:21:46,085
Speaker 11:  is my guess.

1353
01:21:46,835 --> 01:21:47,125
Speaker 6:  Yeah.

1354
01:21:47,525 --> 01:21:50,925
Speaker 7:  I mean they have to right, they have to keep saying this is okay. Yes. 'cause

1355
01:21:50,925 --> 01:21:52,645
Speaker 7:  if if they start apologizing it all falls apart.

1356
01:21:53,035 --> 01:21:56,805
Speaker 11:  Exactly. Exactly. It's much like what's happening with signal

1357
01:21:56,805 --> 01:21:59,845
Speaker 11:  messages if we start admitting things. Indeed.

1358
01:21:59,855 --> 01:22:03,205
Speaker 7:  Which, which we'll get to. But there is one, one more AI thing I wanna talk

1359
01:22:03,205 --> 01:22:06,605
Speaker 7:  about and then we should probably take a break because we all need a break

1360
01:22:06,605 --> 01:22:10,125
Speaker 7:  before we get into Signal. Is Perplexity gonna buy TikTok?

1361
01:22:11,025 --> 01:22:14,965
Speaker 11:  Oh wow. You know, I've, I've gotten not

1362
01:22:14,965 --> 01:22:18,885
Speaker 11:  so nice notes saying that I should really believe that they're

1363
01:22:18,885 --> 01:22:22,165
Speaker 11:  doing that. No. Can you imagine TikTok taking

1364
01:22:22,255 --> 01:22:25,445
Speaker 11:  Perplexity seriously? Some analysts think that

1365
01:22:25,905 --> 01:22:29,245
Speaker 11:  TikTok, their US operations will sell between 30 to

1366
01:22:29,245 --> 01:22:33,045
Speaker 11:  $50 billion in TikTok or in Perplexity is valued at

1367
01:22:33,045 --> 01:22:37,005
Speaker 11:  about 18 billion. And also those are

1368
01:22:37,005 --> 01:22:40,605
Speaker 11:  kind of made up numbers when it comes to a startup that's a GPT wrapper.

1369
01:22:40,785 --> 01:22:44,645
Speaker 11:  I'm, I'm not trying to be mean, but like realistically, how, how would they

1370
01:22:45,005 --> 01:22:48,685
Speaker 11:  actually buy TikTok? It would be great for them. Much better for Perplexity

1371
01:22:48,685 --> 01:22:52,565
Speaker 11:  than I think TikTok ultimately. We'll see, I

1372
01:22:52,565 --> 01:22:56,445
Speaker 11:  would be, that would be the best day on my job on

1373
01:22:56,445 --> 01:22:57,805
Speaker 11:  the internet if that actually happens.

1374
01:22:58,515 --> 01:22:58,805
Speaker 6:  True.

1375
01:23:00,645 --> 01:23:03,045
Speaker 6:  I mean I've definitely seen other companies be like, we're gonna put this

1376
01:23:03,045 --> 01:23:06,255
Speaker 6:  on the blockchain. Like, you know, everyone's got dreams.

1377
01:23:06,885 --> 01:23:08,135
Speaker 11:  Sure. Everybody's got dreams.

1378
01:23:08,375 --> 01:23:11,855
Speaker 7:  I mean, It is funny 'cause in a, in a more normal

1379
01:23:11,965 --> 01:23:15,695
Speaker 7:  political moment, it would be much more likely that TikTok

1380
01:23:15,695 --> 01:23:19,335
Speaker 7:  would just quickly buy Perplexity and then make it like it's search

1381
01:23:19,335 --> 01:23:22,735
Speaker 7:  products be like, we're really leaning into search on TikTok.

1382
01:23:23,565 --> 01:23:24,215
Speaker 7:  Just do that.

1383
01:23:24,635 --> 01:23:27,855
Speaker 6:  Do you think the Perplexity CEO is sitting there searching like, how to get

1384
01:23:27,875 --> 01:23:30,735
Speaker 6:  TikTok to do reverse buyout or I become CEO EO of TikTok.

1385
01:23:32,165 --> 01:23:34,935
Speaker 6:  Like if you got the AI product, you might as well just ask the question.

1386
01:23:35,035 --> 01:23:37,015
Speaker 7:  That's true. Yeah. It, it might know

1387
01:23:37,475 --> 01:23:39,095
Speaker 6:  How to end up CEO of TikTok.

1388
01:23:42,335 --> 01:23:45,655
Speaker 7:  I mean it turns out if, if you say things out loud enough, sometimes

1389
01:23:45,655 --> 01:23:48,735
Speaker 6:  They happen. Make Studio Glibly picture of me, A CEO of TikTok.

1390
01:23:49,695 --> 01:23:52,735
Speaker 11:  I too am seeking $40 billion to be this TikTok.

1391
01:23:52,935 --> 01:23:56,575
Speaker 6:  CEOI would, I think I'd be a great TikTok, CEO just based on my algorithm

1392
01:23:56,595 --> 01:24:00,375
Speaker 6:  at one. All right. We should take a break. I, Kyle, I think you're,

1393
01:24:00,375 --> 01:24:02,815
Speaker 6:  you're on the side of this is not happening for Perplexity.

1394
01:24:03,275 --> 01:24:04,215
Speaker 11:  No, I don't think so.

1395
01:24:04,435 --> 01:24:06,575
Speaker 6:  But if you have a document that says It is, if you have a document that says

1396
01:24:06,715 --> 01:24:10,415
Speaker 6:  JD Vance has seriously considered Perplexity buying

1397
01:24:10,475 --> 01:24:13,935
Speaker 6:  TikTok 'cause apparently he's in charge of the deal, Kylie's email is available.

1398
01:24:13,935 --> 01:24:14,895
Speaker 6:  Send send it to us. We're

1399
01:24:14,895 --> 01:24:18,175
Speaker 7:  Just hit, hit up JD on Signal and let us know. Nobody says we gotta

1400
01:24:18,175 --> 01:24:18,495
Speaker 6:  Take a break.

1401
01:24:18,595 --> 01:24:19,735
Speaker 11:  Add me to your group chats.

1402
01:24:20,005 --> 01:24:22,735
Speaker 6:  Yeah, please. All right. We'll be back. Lightning around

1403
01:25:47,465 --> 01:25:50,815
Speaker 6:  All right, we're back. The lightning round unsponsored

1404
01:25:52,295 --> 01:25:52,755
Speaker 6:  for flavor.

1405
01:25:53,565 --> 01:25:56,675
Speaker 7:  Thank you to everybody who has sent us t-shirt designs. Some of them have

1406
01:25:56,675 --> 01:26:00,435
Speaker 7:  been very good. We will make this t-shirt. I like. I don't know when,

1407
01:26:00,875 --> 01:26:04,085
Speaker 7:  I don't know how much it'll cost, but like I, I'm willing to say on the record,

1408
01:26:04,145 --> 01:26:05,725
Speaker 7:  we will make this t-shirt. Yeah,

1409
01:26:06,075 --> 01:26:09,405
Speaker 6:  It's a lot. It's it just the word putting the word flavor on a t-shirt just

1410
01:26:09,405 --> 01:26:13,285
Speaker 6:  opens you up to all kinds of situations and we'll just,

1411
01:26:13,285 --> 01:26:16,125
Speaker 6:  we'll just figure it out. I will say, I would like to continue reminding

1412
01:26:16,125 --> 01:26:19,965
Speaker 6:  people that even when we are sponsored, the whole point of our ethics policy

1413
01:26:19,965 --> 01:26:23,525
Speaker 6:  is you can't tell us what to do. Which I would compare to

1414
01:26:24,105 --> 01:26:27,245
Speaker 6:  the universe of brand deals that happen elsewhere in the media

1415
01:26:28,005 --> 01:26:31,045
Speaker 6:  where people often get told what to do and you just can't do that. You no

1416
01:26:31,045 --> 01:26:34,845
Speaker 6:  amount of money will, will make me say nice things about you. I'm

1417
01:26:34,845 --> 01:26:37,125
Speaker 7:  Sorry. Well, that's the important thing about being unsponsored for flavor

1418
01:26:37,185 --> 01:26:40,725
Speaker 7:  is that it actually doesn't mean anything. Nothing changes. Right.

1419
01:26:40,765 --> 01:26:43,525
Speaker 6:  I I do like that it implies that some people are sponsored for less flavor.

1420
01:26:44,475 --> 01:26:48,405
Speaker 6:  Sure, sure. Right. Like there's an, there's

1421
01:26:48,405 --> 01:26:52,285
Speaker 6:  an implied comparison there that says, oh, less flavor when

1422
01:26:52,285 --> 01:26:54,885
Speaker 6:  you have, when you have the brand deals. So

1423
01:26:54,885 --> 01:26:58,645
Speaker 7:  Maybe our t-shirt comes with stickers that you can, it covers the UN

1424
01:26:58,725 --> 01:27:00,565
Speaker 7:  and then adds a less when you

1425
01:27:00,565 --> 01:27:02,765
Speaker 6:  Do get sponsored. Right. I think instead of, you know, those like boring

1426
01:27:02,825 --> 01:27:06,685
Speaker 6:  social media badges that are like PON may contain

1427
01:27:07,005 --> 01:27:09,405
Speaker 6:  advertising. It should just say sponsored with less flavor.

1428
01:27:10,715 --> 01:27:13,685
Speaker 6:  Like that might communicate more about what's happening to the media industry.

1429
01:27:14,275 --> 01:27:17,445
Speaker 7:  Well I kind of like the idea that actually we go the other way and everything

1430
01:27:17,445 --> 01:27:21,165
Speaker 7:  that's not sponsored has like a blinking red light that says flavor

1431
01:27:21,865 --> 01:27:25,325
Speaker 7:  and everything that is sponsored doesn't get to have cool light that says

1432
01:27:25,325 --> 01:27:26,285
Speaker 7:  flavor. Yeah. Yeah.

1433
01:27:26,435 --> 01:27:29,565
Speaker 6:  Alright, well speaking of flavor and spiciness,

1434
01:27:30,275 --> 01:27:30,885
Speaker 6:  it's time. David.

1435
01:27:31,225 --> 01:27:35,205
Speaker 7:  Oh my god. America's favorite. It's a top two podcaster than a podcast.

1436
01:27:35,805 --> 01:27:38,605
Speaker 7:  Based on the emails that I get from people every single week,

1437
01:27:39,555 --> 01:27:41,725
Speaker 7:  it's time for Brendan Carr Is a Dummy

1438
01:27:41,825 --> 01:27:44,325
Speaker 6:  By the way. People are starting to send us theme music. If you have ideas

1439
01:27:44,625 --> 01:27:48,205
Speaker 6:  for Brendan Carr Is a Dummy theme music, send 'em in, we'll start running

1440
01:27:48,405 --> 01:27:52,125
Speaker 6:  'em. Absolutely. We need some variety. But I'm, I'm, I'm happy to see that

1441
01:27:52,145 --> 01:27:55,205
Speaker 6:  the podcast within a podcast is beginning to take on a life of its own.

1442
01:27:55,205 --> 01:27:58,805
Speaker 7:  And it has real momentum. It does real momentum. Brendan Carr Is a Dummy.

1443
01:27:58,945 --> 01:28:02,885
Speaker 6:  So does Brendan in his unconstitutional nonsense that he does

1444
01:28:02,885 --> 01:28:06,845
Speaker 6:  every single week. Brendan Carr, chairman of the FCC this week

1445
01:28:07,455 --> 01:28:10,205
Speaker 6:  outta nowhere said he was gonna investigate Disney.

1446
01:28:11,605 --> 01:28:12,745
Speaker 6:  That's everybody wants

1447
01:28:12,745 --> 01:28:13,745
Speaker 7:  To investigate Disney.

1448
01:28:14,045 --> 01:28:17,305
Speaker 6:  That's what I got for you. He gave an interview to Punchbowl News.

1449
01:28:18,175 --> 01:28:21,145
Speaker 6:  It's on YouTube, it's just like a podcast interview. And he says

1450
01:28:22,375 --> 01:28:25,745
Speaker 6:  he's, he's putting the finishing touches on a letter announcing his efforts

1451
01:28:26,125 --> 01:28:30,065
Speaker 6:  to investigate Disney's DEI practices, which similar to his

1452
01:28:30,065 --> 01:28:33,785
Speaker 6:  efforts against Comcast and Verizon. This is because Disney

1453
01:28:34,045 --> 01:28:37,785
Speaker 6:  had a shareholder vote on continuing its DEI practices. And the

1454
01:28:37,785 --> 01:28:41,705
Speaker 6:  shareholders voted to continue them because it

1455
01:28:41,705 --> 01:28:45,505
Speaker 6:  turns out having a wide applicant pool for your open roles

1456
01:28:46,045 --> 01:28:49,975
Speaker 6:  at things like parks is good. Just like I

1457
01:28:50,095 --> 01:28:53,055
Speaker 6:  I, and again, I will point this out, Comcast investor in rock mute, they

1458
01:28:53,055 --> 01:28:56,295
Speaker 6:  do not like me. This is not a thing, but there's your disclosure. They invest

1459
01:28:56,295 --> 01:29:00,235
Speaker 6:  in our company, Comcast being remaining committed to

1460
01:29:00,235 --> 01:29:04,005
Speaker 6:  DEI is because it has a huge physical plant. Like it has wires in the ground,

1461
01:29:04,065 --> 01:29:08,045
Speaker 6:  it hires installers like it needs a big net of how it

1462
01:29:08,095 --> 01:29:11,765
Speaker 6:  hires and who it puts in the field is customer service. 'cause its

1463
01:29:11,925 --> 01:29:15,485
Speaker 6:  customers are everyone Disney's customers. The parks are everyone. Like it

1464
01:29:15,485 --> 01:29:19,415
Speaker 6:  makes sense that you would wanna keep the applicant

1465
01:29:19,825 --> 01:29:22,575
Speaker 6:  pools wide open at the top of the company, at the bottom of the company,

1466
01:29:22,715 --> 01:29:26,615
Speaker 6:  all the way up and down. It makes sense. And so Brendan just

1467
01:29:26,615 --> 01:29:30,015
Speaker 6:  wants to chill the speech of Disney, which owns a b, C news. Right.

1468
01:29:31,075 --> 01:29:34,825
Speaker 6:  Which already settled a case with Donald Trump

1469
01:29:34,825 --> 01:29:38,225
Speaker 6:  about definition saying they were gonna give $15 million to Trump's presidential

1470
01:29:38,465 --> 01:29:38,705
Speaker 6:  library.

1471
01:29:39,175 --> 01:29:41,625
Speaker 7:  That was the, was that the George Stephanopoulos case? Yep.

1472
01:29:41,855 --> 01:29:45,385
Speaker 6:  Yeah. Which George Stephanopoulos did not want to settle, but Disney

1473
01:29:45,635 --> 01:29:49,325
Speaker 6:  caved. So you just see, even when you cave to these

1474
01:29:49,355 --> 01:29:53,045
Speaker 6:  bullies, the pressure remains. And that's happening right now with

1475
01:29:53,045 --> 01:29:56,925
Speaker 6:  universities and law firms. Right. You, you see Columbia caved and the pressures

1476
01:29:57,045 --> 01:30:00,965
Speaker 6:  still on you. You can see that Paul Weiss, the law firm caved and

1477
01:30:00,965 --> 01:30:04,725
Speaker 6:  the pressure is still on. It doesn't work. You, you gotta hit the bully

1478
01:30:04,725 --> 01:30:08,605
Speaker 6:  back and Brendan is nothing if not a bully. And

1479
01:30:08,605 --> 01:30:12,305
Speaker 6:  so just saying gleefully, I'm gonna

1480
01:30:12,625 --> 01:30:16,265
Speaker 6:  investigate Disney for no reason and saying gleefully, any

1481
01:30:16,505 --> 01:30:19,705
Speaker 6:  businesses that are looking for FCC approval, I would encourage them to get

1482
01:30:19,705 --> 01:30:23,425
Speaker 6:  busy ending any sort of their invidious forms of DEI discrimination.

1483
01:30:24,045 --> 01:30:25,425
Speaker 6:  So he's already saying is that a,

1484
01:30:25,425 --> 01:30:27,665
Speaker 7:  That's a quote quote that's like a real thing he said out loud.

1485
01:30:27,665 --> 01:30:30,625
Speaker 6:  That's what said, he said that out loud. And what he's saying is,

1486
01:30:31,385 --> 01:30:35,205
Speaker 6:  before I even evaluate the legality of your merger or the ideas

1487
01:30:35,205 --> 01:30:37,605
Speaker 6:  that you might have about what you might wanna do with your companies, if

1488
01:30:37,605 --> 01:30:40,965
Speaker 6:  you need my approval, I need you to do some racism. Right. Like

1489
01:30:41,165 --> 01:30:45,155
Speaker 6:  straightforwardly. Yeah. And that's power. That's chilling speech. That

1490
01:30:45,155 --> 01:30:48,915
Speaker 6:  is just inappropriate. And then on top of that, this week when I say

1491
01:30:48,915 --> 01:30:49,435
Speaker 6:  he's a bully,

1492
01:30:51,305 --> 01:30:55,235
Speaker 6:  he's investigating CBS for the 60 minutes edit, which we have

1493
01:30:55,235 --> 01:30:58,395
Speaker 6:  talked about before. Which is a very straightforward, they did a long interview

1494
01:30:58,545 --> 01:31:02,075
Speaker 6:  with Kamala Harris. They edit it one way for one program, they edit it another

1495
01:31:02,075 --> 01:31:05,835
Speaker 6:  way for another program that's normal. And you now the whole clip is out

1496
01:31:05,835 --> 01:31:08,475
Speaker 6:  and you can see that it didn't actually make any substantive edits and you

1497
01:31:08,475 --> 01:31:09,435
Speaker 6:  can disagreeing on it.

1498
01:31:09,435 --> 01:31:13,235
Speaker 7:  There's a lot of stupid cases on the, in this universe. This

1499
01:31:13,235 --> 01:31:15,595
Speaker 7:  this is one of the stupidest ones if you ask me.

1500
01:31:15,595 --> 01:31:18,795
Speaker 6:  Yeah. And you know, Brendan bullied CBS, they released the full unedited

1501
01:31:18,795 --> 01:31:21,915
Speaker 6:  clip and you can see with your own eyes, you don't have to agree with me.

1502
01:31:22,175 --> 01:31:25,515
Speaker 6:  You can see that no substantive change was made of that answer. Yep. They

1503
01:31:25,515 --> 01:31:27,675
Speaker 6:  just cut it differently for two different programs. Right.

1504
01:31:27,675 --> 01:31:30,155
Speaker 7:  They used one part over here and one part over there. Yeah. Which is called

1505
01:31:30,265 --> 01:31:30,755
Speaker 7:  editing.

1506
01:31:31,555 --> 01:31:31,875
Speaker 6:  That's

1507
01:31:32,025 --> 01:31:34,075
Speaker 7:  Like what if, what if movie trailers were illegal?

1508
01:31:34,515 --> 01:31:37,915
Speaker 6:  I assure you that editing occurs. Right. If only we edited the show

1509
01:31:39,905 --> 01:31:43,675
Speaker 6:  this week because that case is open as a pending investigation at the

1510
01:31:43,715 --> 01:31:46,995
Speaker 6:  FCC. A bunch of former FCC commissioners

1511
01:31:47,545 --> 01:31:51,435
Speaker 6:  from Republican and Democratic presidential administrations have written

1512
01:31:51,475 --> 01:31:55,175
Speaker 6:  a letter they put into the docket. This is inappropriate.

1513
01:31:55,515 --> 01:31:58,375
Speaker 6:  So Oliver Darcy, who's front of The Verge front of the show report on this,

1514
01:31:58,585 --> 01:32:02,485
Speaker 6:  we'll link link to it, but he is got Alfred Sykes. But

1515
01:32:02,625 --> 01:32:06,365
Speaker 6:  he notes that in this letter, Alfred Sykes, who's the former Republican

1516
01:32:06,645 --> 01:32:10,365
Speaker 6:  chairman of the SEC Irvin Dugan, who is a democratic commissioner appointed

1517
01:32:10,385 --> 01:32:14,245
Speaker 6:  by George HW Bush, Gloria Tristani, who Bill Clinton appointed,

1518
01:32:14,465 --> 01:32:18,205
Speaker 6:  and Tom Wheeler, who I know who served under Barack Obama is the chairman.

1519
01:32:18,415 --> 01:32:22,165
Speaker 6:  We've all signed on this letter and they say it's highly unusual for Carr

1520
01:32:22,165 --> 01:32:25,885
Speaker 6:  to take the case up against CBS. He opened the matter without any real

1521
01:32:25,885 --> 01:32:29,565
Speaker 6:  explanation for reconsidering his prior decision and said,

1522
01:32:29,595 --> 01:32:33,125
Speaker 6:  this sets a chilling precedent. Here's a quote by reopening this complaint.

1523
01:32:33,585 --> 01:32:36,725
Speaker 6:  The commission is signaling to broadcasters that it will indeed act at the

1524
01:32:36,725 --> 01:32:40,285
Speaker 6:  behest of the White House by closely scrutinizing the content of news coverage

1525
01:32:40,425 --> 01:32:43,605
Speaker 6:  and threatening the licenses of broadcasters whose outlets produce coverage

1526
01:32:43,755 --> 01:32:47,225
Speaker 6:  that does not pass muster in the president's view. That is

1527
01:32:47,705 --> 01:32:51,565
Speaker 6:  a big bipartisan spanning time set

1528
01:32:51,565 --> 01:32:55,525
Speaker 6:  of former SEC commissioners and chairs saying, you're doing some illegal

1529
01:32:55,805 --> 01:32:59,365
Speaker 6:  chilling of speech, you're doing some censorship. Okay, maybe you disagree

1530
01:32:59,365 --> 01:33:02,815
Speaker 6:  with him. Maybe you don't agree with me. But I'm telling you b Brendan Carr

1531
01:33:02,815 --> 01:33:06,735
Speaker 6:  Is a Dummy and a bully. And this is how he responded to

1532
01:33:06,735 --> 01:33:10,725
Speaker 6:  Oliver Darcy asking him for comment on this. He sent him a Doctor evil

1533
01:33:10,875 --> 01:33:14,525
Speaker 6:  meme that just says, how about no Jesus but Dr. Evil

1534
01:33:14,725 --> 01:33:18,535
Speaker 6:  from Austin Powers. It's just a gif. It says, how about no,

1535
01:33:19,425 --> 01:33:23,125
Speaker 6:  not a substantive response to former chairs of the

1536
01:33:23,125 --> 01:33:26,725
Speaker 6:  FCC former commissioners of the FCC saying, we're worried about this.

1537
01:33:26,905 --> 01:33:30,805
Speaker 6:  The chilling of speech, the precedent you are setting by reopening

1538
01:33:30,805 --> 01:33:34,735
Speaker 6:  this case without any explanation. He's sending out doctor evil memes.

1539
01:33:35,055 --> 01:33:37,775
Speaker 6:  'cause he is a fundamentally unserious piece of shit.

1540
01:33:37,955 --> 01:33:41,495
Speaker 7:  Yep. It's such useful proof that all of this stuff is happening in bad faith.

1541
01:33:41,495 --> 01:33:44,975
Speaker 7:  Right. Like it's, it, I am endlessly willing to

1542
01:33:45,125 --> 01:33:48,455
Speaker 7:  entertain arguments made in good faith. And this administration has made

1543
01:33:48,455 --> 01:33:52,295
Speaker 7:  so abundantly clear that none of this shit is being done in good

1544
01:33:52,295 --> 01:33:55,655
Speaker 7:  faith. No one, they're not trying to do the right thing. No one believes

1545
01:33:55,655 --> 01:33:59,175
Speaker 7:  any of this. They just, it's some combination of their idiots and they think

1546
01:33:59,175 --> 01:34:01,615
Speaker 7:  it's funny and they want to Yeah. And that's it.

1547
01:34:01,995 --> 01:34:05,535
Speaker 6:  And it there there's just this endless search for power and how to use it

1548
01:34:05,535 --> 01:34:09,165
Speaker 6:  and how to make other people feel bad. And I look,

1549
01:34:09,335 --> 01:34:12,725
Speaker 6:  we've wrestled with who should do the content moderation. How should you

1550
01:34:12,725 --> 01:34:16,565
Speaker 6:  do it? How do you keep kids on the internet safe? Is it even appropriate

1551
01:34:16,905 --> 01:34:20,565
Speaker 6:  for the government to have these rules? I have looked Barack Obama in the

1552
01:34:20,565 --> 01:34:23,365
Speaker 6:  eye on decoder. You can go watch the video and said, aren't you trying to

1553
01:34:23,365 --> 01:34:26,405
Speaker 6:  get around the First Amendment? And had him try to answer that question.

1554
01:34:27,125 --> 01:34:28,525
Speaker 6:  I don't think he gave a very good answer.

1555
01:34:30,785 --> 01:34:34,165
Speaker 6:  We we're in it. We've been covering this stuff for years and to

1556
01:34:34,555 --> 01:34:38,445
Speaker 6:  have real concerns about government overreach into

1557
01:34:38,445 --> 01:34:42,005
Speaker 6:  speech regulations met with Dr. Evil memes is

1558
01:34:42,395 --> 01:34:46,245
Speaker 6:  just wildly inappropriate. It is disrespectful to the American people. It

1559
01:34:46,265 --> 01:34:50,125
Speaker 6:  is on the line of you are a trader to the constitution. So Brendan,

1560
01:34:50,895 --> 01:34:54,125
Speaker 6:  every week I say this. I know, I know you're listening. I know, I know. You

1561
01:34:54,125 --> 01:34:57,645
Speaker 6:  get the notes. Come on the show. Come on this show. Come on decoder.

1562
01:34:58,025 --> 01:35:01,245
Speaker 6:  You can argue with an AI if you want to, you will lose because you are wrong.

1563
01:35:01,865 --> 01:35:05,755
Speaker 6:  But the, the door is always open because I,

1564
01:35:05,995 --> 01:35:08,595
Speaker 6:  I think you should be held to account for the decisions you're making and

1565
01:35:08,595 --> 01:35:12,155
Speaker 6:  the fundamental unseriousness with which you are taking the first amendment.

1566
01:35:12,535 --> 01:35:14,185
Speaker 6:  That's been Brendan Carr had done

1567
01:35:14,605 --> 01:35:17,265
Speaker 7:  Hit us up on signal. Brendan we'll be here.

1568
01:35:17,975 --> 01:35:18,465
Speaker 6:  Alright.

1569
01:35:18,485 --> 01:35:21,945
Speaker 7:  We we're like a hundred years into this podcast and we have not talked about

1570
01:35:21,945 --> 01:35:23,905
Speaker 7:  the only thing anyone has been talking about, the joke

1571
01:35:23,905 --> 01:35:24,745
Speaker 6:  That we open the show with.

1572
01:35:25,095 --> 01:35:27,065
Speaker 7:  Yeah. Should we just talk about this briefly?

1573
01:35:27,695 --> 01:35:30,705
Speaker 6:  Yeah. I mean there's, there's a lot to say. There's almost nothing to say.

1574
01:35:31,095 --> 01:35:31,385
Speaker 6:  Yeah,

1575
01:35:31,385 --> 01:35:35,225
Speaker 7:  That's, that's where I'm at too. So just

1576
01:35:35,225 --> 01:35:39,065
Speaker 7:  to like, if I can just very briefly recap what happened here. So Jeffrey

1577
01:35:39,065 --> 01:35:43,025
Speaker 7:  Goldberg, the editor in Chief of the Atlantic, got added in

1578
01:35:43,025 --> 01:35:46,785
Speaker 7:  some way that I am still desperately trying to figure out to a signal

1579
01:35:46,785 --> 01:35:49,225
Speaker 7:  group chat that included a bunch of people very high up in the government,

1580
01:35:49,225 --> 01:35:52,825
Speaker 7:  including Secretary of Defense, Pete Hegseth and Vice President JD Vance,

1581
01:35:52,905 --> 01:35:56,545
Speaker 7:  and a bunch of other people in which they just planned

1582
01:35:57,175 --> 01:36:00,945
Speaker 7:  attacks. And, and Jeffrey Goldberg writes this story being like,

1583
01:36:01,175 --> 01:36:04,665
Speaker 7:  they planned attacks and signal and they added me to it. Everyone in the

1584
01:36:04,665 --> 01:36:07,145
Speaker 7:  government denied it was like, this isn't classified. None of that happened.

1585
01:36:07,145 --> 01:36:10,065
Speaker 7:  We didn't do any attack planning and signal. He's a liar.

1586
01:36:10,965 --> 01:36:13,385
Speaker 7:  So they just posted the chats. Yeah.

1587
01:36:13,525 --> 01:36:16,785
Speaker 6:  By the way, the move here just editor in chief wise

1588
01:36:17,775 --> 01:36:19,265
Speaker 6:  gold. Like gold, right?

1589
01:36:19,265 --> 01:36:19,905
Speaker 7:  Oh, spectacular.

1590
01:36:20,085 --> 01:36:23,465
Speaker 6:  You say I I didn't publish everything. 'cause I know it's classified sensitive

1591
01:36:23,665 --> 01:36:26,265
Speaker 6:  military information. The government says, screw you, nothing sensitive,

1592
01:36:26,265 --> 01:36:29,825
Speaker 6:  nothing classified was shared. And you say here's all of it. And you just

1593
01:36:29,825 --> 01:36:30,945
Speaker 6:  know he had that in his backlog.

1594
01:36:31,005 --> 01:36:34,585
Speaker 7:  Yes. So, which I will give him credit for because one of the things that

1595
01:36:34,625 --> 01:36:37,265
Speaker 7:  a lot of people have been talking about this week is, why did my man leave

1596
01:36:37,265 --> 01:36:41,145
Speaker 7:  the group chat? Which I agree with. Frankly, I would've stayed

1597
01:36:41,145 --> 01:36:45,025
Speaker 7:  in that group chat. I would've just, you just sit quietly and just, just

1598
01:36:45,025 --> 01:36:48,945
Speaker 7:  watch the group chat happen. But he did take some screenshots before he left

1599
01:36:48,945 --> 01:36:51,905
Speaker 7:  the group chat. And, and I I respect the hell outta that. You gotta take

1600
01:36:51,905 --> 01:36:52,505
Speaker 7:  your screenshots.

1601
01:36:52,575 --> 01:36:55,865
Speaker 6:  Yeah. I mean I think you leave once you think that you have information that

1602
01:36:55,865 --> 01:36:56,945
Speaker 6:  would be illegal for you to have,

1603
01:36:57,245 --> 01:36:59,465
Speaker 7:  Is it illegal if they add you to the group chat.

1604
01:36:59,965 --> 01:37:03,745
Speaker 6:  So this is, this is the complication here. Right. And

1605
01:37:04,005 --> 01:37:07,145
Speaker 6:  you know, we're, we're all the way down to this story where the President

1606
01:37:07,155 --> 01:37:10,745
Speaker 6:  Trump is like, maybe signal is defective and he keeps using the word defective,

1607
01:37:10,745 --> 01:37:12,185
Speaker 6:  which is like very funny in

1608
01:37:12,185 --> 01:37:14,665
Speaker 7:  This context. I'll say everything that President Trump says suggests to me

1609
01:37:14,665 --> 01:37:16,265
Speaker 7:  that he has no idea what has actually,

1610
01:37:16,365 --> 01:37:19,385
Speaker 6:  At one point he referred to this group chat as a call. He said he got added

1611
01:37:19,405 --> 01:37:19,865
Speaker 6:  to the call

1612
01:37:20,245 --> 01:37:24,105
Speaker 7:  And he keeps referring to the signal being bad. And it's like my guy that's

1613
01:37:24,105 --> 01:37:24,225
Speaker 7:  not,

1614
01:37:25,055 --> 01:37:28,065
Speaker 6:  This is how he keeps putting black people and his, his secretary of housing

1615
01:37:28,065 --> 01:37:31,995
Speaker 6:  in Urban Development says, urban in It is true. That's real.

1616
01:37:32,015 --> 01:37:35,995
Speaker 6:  He thinks the signal's bad. And why he keeps mentioning Hannibal

1617
01:37:36,155 --> 01:37:38,835
Speaker 6:  Lecter in the context of asylum because he thinks it's saying it's very good.

1618
01:37:40,885 --> 01:37:44,515
Speaker 6:  Trump notably in this context, the most like

1619
01:37:44,905 --> 01:37:48,835
Speaker 6:  dead ahead, trustworthy person talking. 'cause he

1620
01:37:48,835 --> 01:37:52,035
Speaker 6:  is like, that was bad. I had nothing to do. Yeah, it's true. He's like, I

1621
01:37:52,035 --> 01:37:54,275
Speaker 6:  dunno what they're talking about. That wasn't me. And he, you're like, yeah,

1622
01:37:54,275 --> 01:37:57,805
Speaker 6:  that wasn't you. I, I don't think you know how to use signal, but he has

1623
01:37:57,805 --> 01:38:01,445
Speaker 6:  claimed that it's defective. There's some amount of like what Signal hacked

1624
01:38:01,905 --> 01:38:05,845
Speaker 6:  is a deflection. Elon Musk now is gonna investigate how this

1625
01:38:06,005 --> 01:38:09,685
Speaker 6:  happened. And the answer is, Mike Waltz added Jeffrey Goldberg

1626
01:38:10,025 --> 01:38:11,765
Speaker 6:  to the signal chat. Yeah,

1627
01:38:11,835 --> 01:38:12,845
Speaker 7:  It's right there. That's happened

1628
01:38:12,865 --> 01:38:15,405
Speaker 6:  In the screenshots. Yeah. 'cause it's in the screenshot. Yeah. And maybe

1629
01:38:15,405 --> 01:38:18,965
Speaker 6:  he shares in initial roles with somebody else. Maybe he got the wrong

1630
01:38:19,105 --> 01:38:23,085
Speaker 6:  2 0 2 area code number. 'cause everyone's in DC and

1631
01:38:23,085 --> 01:38:26,295
Speaker 6:  that's the area code. I don't know how it happened, but he did it. It's not

1632
01:38:26,335 --> 01:38:26,855
Speaker 6:  a mystery.

1633
01:38:28,845 --> 01:38:29,195
Speaker 7:  Right?

1634
01:38:29,465 --> 01:38:32,875
Speaker 6:  Like that's how Signal works. And maybe signals interface could be clear.

1635
01:38:32,935 --> 01:38:36,155
Speaker 6:  But he did it. He did the thing and then they had that conversation, which

1636
01:38:36,155 --> 01:38:40,115
Speaker 6:  is inappropriate to have in Signal. I think that much is clear as well. There

1637
01:38:40,115 --> 01:38:43,875
Speaker 6:  are are classified systems that are meant for that kind of communication.

1638
01:38:43,905 --> 01:38:46,155
Speaker 7:  Potentially illegal to have in signal No,

1639
01:38:46,155 --> 01:38:49,635
Speaker 6:  Potentially illegal. Right. To not use the classified systems for the,

1640
01:38:50,055 --> 01:38:53,795
Speaker 6:  the kind of conversation they're having now. There's this very tired semantic

1641
01:38:53,955 --> 01:38:56,515
Speaker 6:  argument about the difference between war plans and attack plans. Right.

1642
01:38:57,215 --> 01:39:01,005
Speaker 6:  But all of this is like, yeah, it's a tech story. 'cause it's a, it's signal.

1643
01:39:01,235 --> 01:39:02,445
Speaker 7:  They just did it in a group chat.

1644
01:39:02,445 --> 01:39:04,845
Speaker 6:  Well, I'm saying the, the, the term that I'm worried about is they're gonna

1645
01:39:04,845 --> 01:39:07,605
Speaker 6:  be like, signal's not safe. We're we need to break its encryption.

1646
01:39:07,985 --> 01:39:09,445
Speaker 7:  Oh, interesting. Right.

1647
01:39:10,075 --> 01:39:12,965
Speaker 6:  That, that's the term that I've just been on Red Alert for. That makes it

1648
01:39:12,965 --> 01:39:13,845
Speaker 6:  like really a story.

1649
01:39:14,155 --> 01:39:17,845
Speaker 7:  Well, it's, it's tricky though because It is, It is clearly

1650
01:39:18,365 --> 01:39:21,885
Speaker 7:  a thing that they are using on purpose because A, it's encrypted and b it

1651
01:39:21,885 --> 01:39:25,405
Speaker 7:  disappears. Right? Like they, they had, yeah, they had message retention

1652
01:39:25,405 --> 01:39:28,765
Speaker 7:  limits in the group chat on purpose, which is again, not a thing you're allowed

1653
01:39:28,765 --> 01:39:32,605
Speaker 7:  to do as a government official doing official business. But It is like, clearly

1654
01:39:32,865 --> 01:39:33,885
Speaker 7:  what's going on here.

1655
01:39:34,385 --> 01:39:37,365
Speaker 6:  And there's another part of this that's, you know, you can make the argument

1656
01:39:37,365 --> 01:39:40,325
Speaker 6:  that signals safer than it used to be because they have to keep making the

1657
01:39:40,445 --> 01:39:43,525
Speaker 6:  argument that it was safe because it's encrypted and can't be backdoored.

1658
01:39:43,525 --> 01:39:46,205
Speaker 6:  And, and now you can't go and attack it.

1659
01:39:46,625 --> 01:39:48,925
Speaker 7:  Oh, interesting. So they, they've kind of painted themselves into a corner.

1660
01:39:49,075 --> 01:39:52,165
Speaker 6:  Yeah. So I don't, I don't know. But the, the thing that I'm always nervous

1661
01:39:52,175 --> 01:39:55,765
Speaker 6:  about is how will governments find ways to attack encryption?

1662
01:39:56,585 --> 01:40:00,405
Speaker 6:  And here you have this thing, here's this mess.

1663
01:40:00,665 --> 01:40:04,205
Speaker 6:  And then the person with the big hammer, like the FBI with the big hammer

1664
01:40:04,205 --> 01:40:06,845
Speaker 6:  that says we shouldn't have encryption. Well, it could be up like, do you

1665
01:40:06,845 --> 01:40:08,485
Speaker 6:  have a nail that looks like a huge nail?

1666
01:40:10,135 --> 01:40:12,435
Speaker 6:  That's, it's just always a thing that's on the back of my mind that I should

1667
01:40:12,435 --> 01:40:15,075
Speaker 6:  be worried about. Like, will the government try to get rid of encryption

1668
01:40:15,075 --> 01:40:15,515
Speaker 6:  because of this?

1669
01:40:15,975 --> 01:40:19,635
Speaker 7:  So the argument is like Jeffrey Goldberg hacked his way into signal. Thus

1670
01:40:19,655 --> 01:40:23,515
Speaker 7:  signal is not safe anyway. Thus we

1671
01:40:23,515 --> 01:40:25,395
Speaker 7:  should break the encryption.

1672
01:40:25,535 --> 01:40:29,155
Speaker 6:  Or the argument is we can't know because signal's encrypted and that's the

1673
01:40:29,175 --> 01:40:31,915
Speaker 6:  law enforcement problem. Oh, I, we have, we can't possibly arrest Jeffrey

1674
01:40:31,915 --> 01:40:34,155
Speaker 6:  Goldberg because there's no evidence because of this encryption.

1675
01:40:34,175 --> 01:40:37,115
Speaker 7:  Our buddy big balls at DOS couldn't figure it out because he couldn't get

1676
01:40:37,115 --> 01:40:37,755
Speaker 7:  by the encryption.

1677
01:40:37,985 --> 01:40:41,435
Speaker 6:  It's very bad. The, this whole situation's very bad. And then like the Trump

1678
01:40:41,575 --> 01:40:45,395
Speaker 6:  administration, like just trying to power through it by

1679
01:40:45,395 --> 01:40:49,385
Speaker 6:  saying the things that are obvious are not obvious. Right. It's bad.

1680
01:40:49,495 --> 01:40:53,230
Speaker 6:  Like all of that's bad. And It is all centered on this app. That, that

1681
01:40:53,965 --> 01:40:56,005
Speaker 6:  I hope the Signal Foundation can withstand the pressure.

1682
01:40:56,425 --> 01:40:59,605
Speaker 7:  It, I will say the Signal Foundation so far has responded to this very well.

1683
01:41:00,175 --> 01:41:04,125
Speaker 7:  Which is like, they, they have mostly been very funny about it. And

1684
01:41:04,195 --> 01:41:08,125
Speaker 7:  also it sounds like downloads have spiked. I think I, in a, in

1685
01:41:08,165 --> 01:41:12,125
Speaker 7:  a funny way, one very possible outcome of this is that if the

1686
01:41:12,125 --> 01:41:15,965
Speaker 7:  government's gonna use it to do attack planning, it's probably pretty

1687
01:41:16,035 --> 01:41:20,005
Speaker 7:  safe. Like they're, if if they're in here doing the

1688
01:41:20,005 --> 01:41:23,805
Speaker 7:  group chats, they, that's a lot of trust to have from a lot of very senior

1689
01:41:23,805 --> 01:41:27,325
Speaker 7:  people. But then at the same time, this is a fundamentally unserious group

1690
01:41:27,325 --> 01:41:31,125
Speaker 7:  of people. And the other thing Wired went and found that Mike

1691
01:41:31,195 --> 01:41:35,125
Speaker 7:  Waltz's Venmo friend list was public. And it's like,

1692
01:41:35,125 --> 01:41:38,885
Speaker 7:  this is just, this is not a group of people who is smart and

1693
01:41:38,885 --> 01:41:41,125
Speaker 7:  thoughtful and doing a good job. No.

1694
01:41:41,445 --> 01:41:45,185
Speaker 6:  Like, can we just run the Marco Rubio audio because

1695
01:41:45,245 --> 01:41:49,065
Speaker 6:  you know, they all do press conferences and I think the Atlantic timed

1696
01:41:49,065 --> 01:41:52,985
Speaker 6:  this stuff when they knew that t Gabbard and Mike

1697
01:41:53,075 --> 01:41:55,425
Speaker 6:  Waltz would be in front of Congress. 'cause they had hearings in the Senate

1698
01:41:55,425 --> 01:41:59,185
Speaker 6:  in the house. So there was just all these moments for people to talk.

1699
01:41:59,605 --> 01:42:03,345
Speaker 6:  But it's Marco Rubio who was in the chat, who I think his

1700
01:42:03,985 --> 01:42:07,785
Speaker 6:  response, It is just such a perfect, it just run the audio. You,

1701
01:42:07,785 --> 01:42:08,545
Speaker 6:  you'll see what I'm saying.

1702
01:42:09,625 --> 01:42:13,205
Speaker 15:  And you know, so I can speak to myself or my presence on it. I think my,

1703
01:42:13,625 --> 01:42:17,445
Speaker 15:  my role I on it was just speaking for my role. I,

1704
01:42:18,985 --> 01:42:20,965
Speaker 16:  That's it. That's the, that's just, just

1705
01:42:21,045 --> 01:42:22,525
Speaker 6:  A primal scream of like help,

1706
01:42:22,995 --> 01:42:24,925
Speaker 16:  Like I'm in danger.

1707
01:42:29,595 --> 01:42:33,445
Speaker 6:  Yeah, that's good. Look, the story's gonna keep playing

1708
01:42:33,465 --> 01:42:36,685
Speaker 6:  out. The, the thing that I think we're, we're gonna stay focused on, it's

1709
01:42:36,685 --> 01:42:39,285
Speaker 6:  like encrypted messaging is important. Encryption is important.

1710
01:42:40,395 --> 01:42:44,125
Speaker 6:  Elon Musk is gonna somehow investigate this. Now I do love

1711
01:42:44,195 --> 01:42:47,365
Speaker 6:  that they've landed on, Elon will fix It is though

1712
01:42:48,975 --> 01:42:51,945
Speaker 6:  he's good at computers. It is like that, that's how they're treating it here.

1713
01:42:51,945 --> 01:42:55,625
Speaker 6:  And it's like, I think the military is good at keeping its

1714
01:42:55,625 --> 01:42:55,985
Speaker 6:  information.

1715
01:42:56,215 --> 01:43:00,145
Speaker 7:  This ends with like e Elon telling lies about dms

1716
01:43:00,145 --> 01:43:04,105
Speaker 7:  on X and suddenly that's where the war planning happens. That's where

1717
01:43:04,105 --> 01:43:07,705
Speaker 7:  this goes. I just wanna shout out to our friends at 4 0 4 media who wrote

1718
01:43:07,745 --> 01:43:10,585
Speaker 7:  a story about all this and their headline is when your threat model is being

1719
01:43:10,625 --> 01:43:13,105
Speaker 7:  a moron. And I think that sums it up all pre

1720
01:43:13,465 --> 01:43:17,105
Speaker 6:  Yeah, I think that's correct. Speaking of Elon, we got a big Alex Heath scoop

1721
01:43:17,105 --> 01:43:20,705
Speaker 6:  that we should mention briefly in the lightning round. We kind of knew this

1722
01:43:20,705 --> 01:43:24,665
Speaker 6:  was happening, but Alex nailed it down. Elon is very mad

1723
01:43:24,675 --> 01:43:27,905
Speaker 6:  about content moderation on Reddit. He's mad about two things. One which

1724
01:43:27,905 --> 01:43:30,345
Speaker 6:  is totally justified. He is mad that there are death threats about him on

1725
01:43:30,345 --> 01:43:34,025
Speaker 6:  Reddit. And he apparently has been communicating those

1726
01:43:34,305 --> 01:43:37,665
Speaker 6:  directly to Steve Hoffman, Reddit to you. The other thing he is mad about

1727
01:43:37,765 --> 01:43:41,145
Speaker 6:  is many, many subreddits have banned links to X Right.

1728
01:43:41,435 --> 01:43:45,145
Speaker 6:  Which Reddit hasn't done anything about yet, but there's a lot of, I would

1729
01:43:45,145 --> 01:43:48,905
Speaker 6:  say user drama, moderator drama about what Reddit might be doing, how it

1730
01:43:49,195 --> 01:43:53,085
Speaker 6:  might be promoting these subreddits, how it might be coming and doing moderation

1731
01:43:53,085 --> 01:43:57,005
Speaker 6:  over the top against this perceived pressure. So far it

1732
01:43:57,005 --> 01:44:00,845
Speaker 6:  doesn't seem like they've made any subreddits have links to X

1733
01:44:00,845 --> 01:44:04,805
Speaker 6:  like that hasn't been the line they've crossed, but there's a lot of ing.

1734
01:44:05,195 --> 01:44:08,205
Speaker 6:  Yeah. How much pressure is Elon putting on the top and how much is that pressure

1735
01:44:08,205 --> 01:44:11,865
Speaker 6:  being reflected on the user experience of Reddit? And that's

1736
01:44:12,125 --> 01:44:13,065
Speaker 6:  as always, that's bad.

1737
01:44:13,255 --> 01:44:17,145
Speaker 7:  Yeah. And I would say a, it's not at all surprising and b It is very

1738
01:44:17,145 --> 01:44:20,985
Speaker 7:  funny because this is like, like for a man who styles himself a free

1739
01:44:20,985 --> 01:44:24,905
Speaker 7:  speech warrior, like here It is. Yeah. It's, it's like, this

1740
01:44:24,905 --> 01:44:28,825
Speaker 7:  is so of a piece for me with the thing that Elon Musk has decided,

1741
01:44:29,095 --> 01:44:32,945
Speaker 7:  it's, it should be illegal not to advertise on X. That It is just

1742
01:44:32,945 --> 01:44:36,745
Speaker 7:  like, we're just so deep down all of these rabbit

1743
01:44:36,745 --> 01:44:40,705
Speaker 7:  holes where it's like you can't possibly think what you

1744
01:44:40,705 --> 01:44:44,585
Speaker 7:  just said out loud that it's like every subreddit should be required to

1745
01:44:44,655 --> 01:44:48,545
Speaker 7:  link to X. It's like, no, that's not how

1746
01:44:48,605 --> 01:44:49,105
Speaker 7:  any of this

1747
01:44:49,105 --> 01:44:50,305
Speaker 6:  Works. That's

1748
01:44:50,305 --> 01:44:53,585
Speaker 7:  Just not how any of this works. And, and yeah, and I think

1749
01:44:54,125 --> 01:44:57,945
Speaker 7:  the, the Reddit side of It is interesting because I think there is a sort

1750
01:44:57,945 --> 01:45:01,785
Speaker 7:  of ongoing fundamental mistrust on those three levels. Like the

1751
01:45:01,785 --> 01:45:05,625
Speaker 7:  users don't trust the mods and the admins, the mods don't trust the

1752
01:45:05,645 --> 01:45:08,265
Speaker 7:  admins. And the admins don't trust anybody. Yeah.

1753
01:45:08,365 --> 01:45:08,785
Speaker 6:  And so

1754
01:45:09,175 --> 01:45:12,625
Speaker 7:  It's just like, this has always been a group of people who are all suspicious

1755
01:45:12,645 --> 01:45:16,065
Speaker 7:  of each other. So as soon as things like this come up, everybody is looking

1756
01:45:16,125 --> 01:45:19,465
Speaker 7:  for like any little thing that happens. And

1757
01:45:19,855 --> 01:45:23,785
Speaker 7:  like there was a, was it white people Twitter that that got,

1758
01:45:23,935 --> 01:45:26,385
Speaker 7:  that was like briefly banned. Yeah.

1759
01:45:26,655 --> 01:45:27,625
Speaker 6:  72 hour

1760
01:45:27,785 --> 01:45:31,265
Speaker 7:  Ban because of that was a place that a lot of the, the conversation about

1761
01:45:31,375 --> 01:45:35,025
Speaker 7:  Doge employees and the threats were being made. And there's, there's now

1762
01:45:35,025 --> 01:45:37,785
Speaker 7:  pretty clearly a link back to Elon Musk from that.

1763
01:45:39,095 --> 01:45:42,945
Speaker 7:  This is just, it's, it's both absurd in exactly the way you would expect

1764
01:45:43,125 --> 01:45:46,065
Speaker 7:  and like a truly terrible look for everyone involved.

1765
01:45:46,215 --> 01:45:49,905
Speaker 6:  There's also just some real fear of Elon here. Like if you run a

1766
01:45:49,905 --> 01:45:53,225
Speaker 6:  competitor to Twitter, which by all accounts, you know, Reddit is

1767
01:45:53,885 --> 01:45:56,905
Speaker 6:  you, your answer should be go run your own social network. That's the answer.

1768
01:45:56,915 --> 01:45:59,105
Speaker 6:  We're gonna run ours our way and you run yours.

1769
01:45:59,585 --> 01:46:03,185
Speaker 7:  I mean, I think back to like the first Trump administration when

1770
01:46:03,275 --> 01:46:07,105
Speaker 7:  there was so much happening because people were afraid

1771
01:46:07,105 --> 01:46:10,865
Speaker 7:  that if they didn't do it or did do it or whatever, Donald Trump would tweet

1772
01:46:10,865 --> 01:46:14,705
Speaker 7:  about it. And I think Elon Musk wields that same thing now,

1773
01:46:14,705 --> 01:46:18,545
Speaker 7:  right? Like there there's been a Trump's like

1774
01:46:18,715 --> 01:46:22,585
Speaker 7:  truth social presence, social people pay attention to it, but it doesn't

1775
01:46:22,585 --> 01:46:26,105
Speaker 7:  carry the weight that his, his Twitter account once did. But Elon Musk sure

1776
01:46:26,105 --> 01:46:29,945
Speaker 7:  does. Like yeah, he tweets a million times a day. But It is like, he is

1777
01:46:30,145 --> 01:46:33,985
Speaker 7:  somebody who can direct a lot of attention toward whatever he

1778
01:46:33,985 --> 01:46:37,785
Speaker 7:  wants. And that is like, it, it just feels like a behavior he learned by

1779
01:46:38,065 --> 01:46:41,785
Speaker 7:  watching Donald Trump in his last campaign and administration.

1780
01:46:41,925 --> 01:46:45,465
Speaker 6:  But I feel like if Elon was like Reddit is being mean to me, go attack Reddit,

1781
01:46:45,745 --> 01:46:49,525
Speaker 6:  Reddit's users would be capable of waging their own fight.

1782
01:46:49,585 --> 01:46:53,365
Speaker 6:  So like that's true of all the communities in the United, they would be like,

1783
01:46:53,545 --> 01:46:55,845
Speaker 6:  no, we're we're very capable on our own. Right.

1784
01:46:55,965 --> 01:46:57,365
Speaker 7:  Like come at me bro. Yeah,

1785
01:46:57,565 --> 01:47:01,365
Speaker 6:  Yeah. We're, we're good actually. We'll see. I just, you just see that pressure.

1786
01:47:01,365 --> 01:47:04,005
Speaker 6:  It's good scoop from Alex. Yeah. We'll see how that, those dynamics keep

1787
01:47:04,005 --> 01:47:07,845
Speaker 6:  playing out. Speaking of social networks that have problems

1788
01:47:07,865 --> 01:47:11,605
Speaker 6:  to TikTok still in the news, David, you keep pointing out that

1789
01:47:11,605 --> 01:47:14,205
Speaker 6:  April is is like tomorrow basically. And it's

1790
01:47:14,205 --> 01:47:14,645
Speaker 7:  So soon

1791
01:47:14,645 --> 01:47:18,285
Speaker 6:  Man. A lot of things come due in April including the

1792
01:47:18,305 --> 01:47:19,005
Speaker 6:  TikTok ban

1793
01:47:19,475 --> 01:47:22,405
Speaker 7:  Next Saturday. Yeah. And there It is.

1794
01:47:23,705 --> 01:47:26,445
Speaker 7:  We were talking about with Kylie about the Perplexity thing, which is not

1795
01:47:26,445 --> 01:47:28,165
Speaker 7:  gonna happen. The Oracle thing

1796
01:47:29,795 --> 01:47:33,565
Speaker 7:  Burs along Walmart is gonna, you know, come in off the top rope

1797
01:47:33,665 --> 01:47:37,565
Speaker 7:  and buy it according to Eli. But now

1798
01:47:37,565 --> 01:47:40,205
Speaker 7:  that, the interesting thing is, and I think one of the questions we have

1799
01:47:40,205 --> 01:47:43,765
Speaker 7:  been asking over and over is essentially who gets to

1800
01:47:44,065 --> 01:47:47,565
Speaker 7:  decide what constitutes a deal here, right? Like

1801
01:47:48,025 --> 01:47:51,525
Speaker 7:  it, who, who is the one who gets to say that satisfies the brief,

1802
01:47:52,025 --> 01:47:55,925
Speaker 7:  TikTok gets to remain. And what happens now

1803
01:47:56,265 --> 01:48:00,125
Speaker 7:  is a, a bunch of members of Congress basically reminding the

1804
01:48:00,125 --> 01:48:02,325
Speaker 7:  Trump administration that actually they do.

1805
01:48:04,065 --> 01:48:08,045
Speaker 7:  And I just, I'll just read you the, the quote here. He

1806
01:48:08,045 --> 01:48:11,485
Speaker 7:  says, to the extent that you continue trying to delay the divestment deadline

1807
01:48:11,485 --> 01:48:14,285
Speaker 7:  through executive orders, any further extensions of the TikTok deadline will

1808
01:48:14,285 --> 01:48:17,565
Speaker 7:  require Oracle, apple, Google, and other companies to continue risking ruinous

1809
01:48:17,565 --> 01:48:20,925
Speaker 7:  legal liability. A difficult decision to justify in perpetuity. So they're

1810
01:48:21,205 --> 01:48:24,925
Speaker 7:  basically like, figure this out or it's going to happen

1811
01:48:25,725 --> 01:48:29,705
Speaker 7:  and we are not going to continue to play these shenanigan games with you.

1812
01:48:29,855 --> 01:48:33,065
Speaker 6:  Yeah. So the interesting thing here is the actual text of the bill, which

1813
01:48:33,065 --> 01:48:36,745
Speaker 6:  we should read, which says here's what a

1814
01:48:36,745 --> 01:48:40,385
Speaker 6:  qualified divestiture is. So the bill said you have to sell it or be banned.

1815
01:48:40,555 --> 01:48:43,825
Speaker 6:  Right? Banned. And everyone was like, this is a band 'cause Byan will never

1816
01:48:43,825 --> 01:48:47,345
Speaker 6:  sell and they didn't sell and they did their little stunt and they shut it

1817
01:48:47,345 --> 01:48:50,385
Speaker 6:  down for a minute. Then Trump said, you're fine until next Saturday. Right.

1818
01:48:50,595 --> 01:48:53,385
Speaker 6:  Which is weird 'cause that's not how law enforcement's supposed to work,

1819
01:48:54,445 --> 01:48:55,905
Speaker 6:  but here we are in the Trump administration

1820
01:48:56,005 --> 01:48:59,345
Speaker 7:  And Congress is trying very hard to remind him that that's not how law enforcement

1821
01:48:59,345 --> 01:48:59,625
Speaker 7:  works.

1822
01:48:59,825 --> 01:49:03,345
Speaker 6:  Yeah. You can't just delay. This went to the Supreme Court, this supreme

1823
01:49:03,445 --> 01:49:06,985
Speaker 6:  the, this is weird, but constitutional, you, the government is allowed to

1824
01:49:07,065 --> 01:49:10,225
Speaker 6:  ban TikTok in this way. So what by chance was supposed to do was enact a

1825
01:49:10,225 --> 01:49:14,185
Speaker 6:  qualified divestiture, which is sell TikTok. And so

1826
01:49:14,185 --> 01:49:17,185
Speaker 6:  here's how it's defined in the bill. The term qualified divestiture means

1827
01:49:17,305 --> 01:49:21,105
Speaker 6:  a divestiture or similar transaction that a, the president

1828
01:49:21,385 --> 01:49:25,105
Speaker 6:  determines through an inter-agency process would result in the

1829
01:49:25,305 --> 01:49:28,505
Speaker 6:  relevant foreign adversary controlled application no longer being controlled

1830
01:49:28,505 --> 01:49:32,065
Speaker 6:  by a foreign adversary. So that President Donald Trump has to go through

1831
01:49:32,065 --> 01:49:35,145
Speaker 6:  an inter-agency process that results in

1832
01:49:35,885 --> 01:49:39,795
Speaker 6:  TikTok no longer being controlled by by chance. That's a big

1833
01:49:39,795 --> 01:49:43,715
Speaker 6:  one. What is this process? Is the Trump administration capable of a process?

1834
01:49:45,575 --> 01:49:45,795
Speaker 7:  No.

1835
01:49:46,055 --> 01:49:47,525
Speaker 6:  Who are the agencies? But the

1836
01:49:47,525 --> 01:49:51,005
Speaker 7:  Trump administration seems very confident that an executive order counts

1837
01:49:51,005 --> 01:49:52,125
Speaker 7:  as an inter-agency process,

1838
01:49:52,765 --> 01:49:56,735
Speaker 6:  I guess. Right? I mean PE the they have to a, a bunch

1839
01:49:56,735 --> 01:50:00,015
Speaker 6:  of agencies including our national security agencies have to weigh in. Right?

1840
01:50:00,555 --> 01:50:04,455
Speaker 6:  We don't know if that's gonna happen by Saturday. And we don't know if

1841
01:50:04,475 --> 01:50:08,335
Speaker 6:  the sort of moves that they have on the table like Byan will reduce

1842
01:50:08,435 --> 01:50:12,175
Speaker 6:  its share of this to X amount of dollars and

1843
01:50:12,275 --> 01:50:15,655
Speaker 6:  Oracle will increase its share will satisfy this process.

1844
01:50:16,475 --> 01:50:18,895
Speaker 6:  And all we've got is like JD Vance being like, we'll have the high level

1845
01:50:19,095 --> 01:50:21,655
Speaker 6:  structure of a deal by the deadline. I don't know.

1846
01:50:22,455 --> 01:50:26,055
Speaker 7:  So my guess would be, and we're gonna find out in eight days, but

1847
01:50:27,035 --> 01:50:30,775
Speaker 7:  my guess would be they're going to use that because there was, there was

1848
01:50:30,815 --> 01:50:34,735
Speaker 7:  a provision that if there was a deal in the works, they could do

1849
01:50:34,735 --> 01:50:38,135
Speaker 7:  another 90 day delay in order to finish that deal. But

1850
01:50:38,135 --> 01:50:41,335
Speaker 6:  That's over, that's over. That was in the te original text. There was that

1851
01:50:41,755 --> 01:50:41,975
Speaker 6:  but

1852
01:50:41,975 --> 01:50:45,375
Speaker 7:  Wasn't the 75 day delay that we just did something else?

1853
01:50:45,755 --> 01:50:49,495
Speaker 6:  No. 'cause the deadline passed. This all matters if you believe the rule

1854
01:50:49,495 --> 01:50:53,175
Speaker 6:  of law in America still matters. Right. And what matters is, did

1855
01:50:53,175 --> 01:50:56,375
Speaker 6:  Donald Trump tell the attorney general to enforce this law? And the answer

1856
01:50:56,375 --> 01:51:00,335
Speaker 6:  right now is he said to her, don't do it. So she's not doing it.

1857
01:51:00,335 --> 01:51:04,215
Speaker 6:  Right. And will that matter on, on Saturday? Maybe

1858
01:51:04,215 --> 01:51:07,255
Speaker 6:  he'll, maybe he'll say it again and like that's where you have letters from

1859
01:51:07,455 --> 01:51:11,375
Speaker 6:  Congress saying, Hey, we passed this law. Your job is to enforce it not to

1860
01:51:11,385 --> 01:51:15,295
Speaker 6:  delay it. Right. And we gave you this grace period basically

1861
01:51:15,485 --> 01:51:18,935
Speaker 6:  because the law went into effect on the first day you were in office.

1862
01:51:20,155 --> 01:51:24,005
Speaker 6:  Okay. Grace periods up, enforce the law, don't. Right. And we,

1863
01:51:24,125 --> 01:51:26,765
Speaker 6:  I just dunno, there's gonna be a deal. There's this other part that says

1864
01:51:26,765 --> 01:51:30,565
Speaker 6:  like, maybe you can get the algorithms out. I I the goal of this law is

1865
01:51:30,565 --> 01:51:34,245
Speaker 6:  for them to sell It is to get by dance out of

1866
01:51:34,425 --> 01:51:36,925
Speaker 6:  social media Right. In the United States. And at least

1867
01:51:36,925 --> 01:51:37,765
Speaker 7:  As far as I can tell,

1868
01:51:39,295 --> 01:51:42,925
Speaker 7:  there is not a lot of indication coming from anywhere other than the Trump

1869
01:51:42,925 --> 01:51:45,605
Speaker 7:  administration that a deal is happening

1870
01:51:45,965 --> 01:51:48,695
Speaker 6:  Because I think everyone is except for Perplexity,

1871
01:51:50,185 --> 01:51:53,135
Speaker 6:  which I think tells you a lot about the status of their efforts here. Yeah.

1872
01:51:53,135 --> 01:51:55,815
Speaker 7:  There's the, what is it, Frank McCort thing, project Liberty? Is that what

1873
01:51:55,815 --> 01:51:57,055
Speaker 7:  it's called? Like that's, that's out there.

1874
01:51:57,235 --> 01:52:00,255
Speaker 6:  That's the one where Alexis Hanney and, and Mr. Beast wanna put it on the

1875
01:52:00,255 --> 01:52:01,135
Speaker 6:  blockchain. Oh

1876
01:52:01,155 --> 01:52:05,055
Speaker 7:  God, yeah, sure, sure. Why not put TikTok on the

1877
01:52:05,055 --> 01:52:08,855
Speaker 7:  blockchain? Like it's whatever that's the same as banning it.

1878
01:52:08,865 --> 01:52:09,495
Speaker 7:  It'll be fine.

1879
01:52:10,495 --> 01:52:14,135
Speaker 6:  Everything's in NFT now. Yeah. We're gonna light the ocean on

1880
01:52:14,205 --> 01:52:17,775
Speaker 6:  fire. Some other lightning round items here. This one's real quick.

1881
01:52:18,375 --> 01:52:21,655
Speaker 6:  The Google antitrust trial, the remedies phase

1882
01:52:23,035 --> 01:52:26,225
Speaker 6:  where they already determined that Google had a monopoly in search and now

1883
01:52:26,225 --> 01:52:29,465
Speaker 6:  it's what do we do about it? That's set to kick off in April as well. Right.

1884
01:52:29,925 --> 01:52:33,505
Speaker 6:  The government, including Trump's administration, wants them to get rid of

1885
01:52:33,505 --> 01:52:37,385
Speaker 6:  Chrome. Right. That was the approach of

1886
01:52:37,445 --> 01:52:40,745
Speaker 6:  the DOJ under the Biden administration who won the trial. The Trump DOJ is

1887
01:52:40,825 --> 01:52:44,265
Speaker 6:  sticking to it. Yep. We won the case. The DOJ won the case. The United States

1888
01:52:44,325 --> 01:52:48,125
Speaker 6:  won the case. You're a monopoly. We also think the remedy is you getting

1889
01:52:48,125 --> 01:52:51,725
Speaker 6:  rid of all this stuff. Google has filed an answer. The news here is that

1890
01:52:51,725 --> 01:52:55,565
Speaker 6:  Apple moved to intervene in that case and you should basically be like,

1891
01:52:55,565 --> 01:52:58,645
Speaker 6:  don't take our money away. And the court ruled that they were too late.

1892
01:52:58,795 --> 01:53:00,045
Speaker 7:  Yeah. Which is really interesting.

1893
01:53:00,385 --> 01:53:04,245
Speaker 6:  So they're not allowed. So that's 20 billions of Apple's revenue on a calendar

1894
01:53:04,245 --> 01:53:05,005
Speaker 6:  error. Basically.

1895
01:53:05,215 --> 01:53:08,925
Speaker 7:  Apple was like central to that case. I mean there was so

1896
01:53:09,155 --> 01:53:13,045
Speaker 7:  much Apple in that case and it's, it's fascinating to see it

1897
01:53:13,045 --> 01:53:14,885
Speaker 7:  just totally looped out of the last part of this.

1898
01:53:15,115 --> 01:53:19,045
Speaker 6:  Yeah, well see the, that that case is coming up by the way, the, the FTCs

1899
01:53:19,045 --> 01:53:22,965
Speaker 6:  case against meta in which the government is asking for

1900
01:53:22,965 --> 01:53:25,965
Speaker 6:  Instagram and WhatsApp to sort off also comment in April. You're

1901
01:53:25,965 --> 01:53:26,925
Speaker 7:  Very excited about that case.

1902
01:53:27,445 --> 01:53:31,295
Speaker 6:  I think that is the most legible case to the biggest audience. The

1903
01:53:31,295 --> 01:53:34,895
Speaker 6:  government wants Mark Zuckerberg to get rid of Instagram. There's not another,

1904
01:53:35,095 --> 01:53:38,935
Speaker 6:  I don't have to say more words, right? Yeah. I I everyone

1905
01:53:38,935 --> 01:53:42,815
Speaker 6:  understands exactly what means. Like I I I honestly think Google

1906
01:53:42,915 --> 01:53:45,735
Speaker 6:  is a monopoly in search and so the government wants it to get rid of Chrome

1907
01:53:45,915 --> 01:53:46,895
Speaker 6:  is totally opaque.

1908
01:53:47,445 --> 01:53:47,735
Speaker 7:  Yeah.

1909
01:53:49,155 --> 01:53:50,725
Speaker 6:  Like you have to explain a bunch of hoops.

1910
01:53:50,725 --> 01:53:54,085
Speaker 7:  Well, which is why it's funny, like Google pays Apple

1911
01:53:54,085 --> 01:53:57,885
Speaker 7:  $20 billion a year to be the search engine in

1912
01:53:58,045 --> 01:54:01,765
Speaker 7:  Safari is actually more legible than Google is a

1913
01:54:01,885 --> 01:54:05,845
Speaker 7:  monopoly because of Chrome. And even though I think both are sort of equally

1914
01:54:05,845 --> 01:54:09,805
Speaker 7:  true, but you're right. Like It is meta a monopoly and thus should

1915
01:54:09,805 --> 01:54:13,765
Speaker 7:  it have to get rid of Instagram is it's just, it's It is as perfectly simple

1916
01:54:14,285 --> 01:54:17,125
Speaker 7:  a monopoly case as I think you'll find. But they're gonna spend seven weeks

1917
01:54:17,125 --> 01:54:18,565
Speaker 7:  making it really complicated.

1918
01:54:19,305 --> 01:54:21,845
Speaker 6:  Oh. So you can ask every celebrity in the world if Mark Zuckerberg shown

1919
01:54:21,845 --> 01:54:24,965
Speaker 6:  on Instagram, like I, I think about the local TV news test,

1920
01:54:25,895 --> 01:54:29,315
Speaker 6:  you know, like, is this a local TV segment and it's just like whoever, like

1921
01:54:29,315 --> 01:54:31,995
Speaker 6:  man on the street should mark Zuckerberg on Instagram. They're like, no.

1922
01:54:31,995 --> 01:54:35,835
Speaker 6:  And like there you go. Like that's great TV like that's coming

1923
01:54:35,835 --> 01:54:39,435
Speaker 6:  up as well. It's big April. And then the other thing that's happening is

1924
01:54:39,505 --> 01:54:42,915
Speaker 6:  tariffs hit in April, Trump yesterday announced

1925
01:54:42,915 --> 01:54:46,235
Speaker 6:  25% tariffs on cars and car parts.

1926
01:54:46,855 --> 01:54:49,195
Speaker 6:  If they're not manufactured in the United States, this is gonna be a huge

1927
01:54:49,195 --> 01:54:53,155
Speaker 6:  problem for every car maker. Yep. Most cars are made of parts from

1928
01:54:53,255 --> 01:54:56,595
Speaker 6:  all kinds of places. There are some like

1929
01:54:57,875 --> 01:55:01,715
Speaker 6:  U-S-M-C-A trade carve outs here. The whole audio industry is like

1930
01:55:01,715 --> 01:55:05,435
Speaker 6:  sort of reorganizing around this problem. Hmm. There might be some benefit

1931
01:55:05,615 --> 01:55:08,795
Speaker 6:  to Tesla because Tesla makes so many of its cars in the United States,

1932
01:55:09,535 --> 01:55:13,515
Speaker 6:  but it's coming like tariffs are here. He, he got

1933
01:55:13,905 --> 01:55:16,595
Speaker 6:  held off on them once he got held off on them. Again.

1934
01:55:18,025 --> 01:55:19,945
Speaker 6:  I don't think he's gonna blink this time.

1935
01:55:20,285 --> 01:55:21,105
Speaker 7:  It doesn't seem like it,

1936
01:55:21,125 --> 01:55:25,105
Speaker 6:  The tariffs on Chinese products are like 45%. So I think we're just

1937
01:55:25,105 --> 01:55:27,825
Speaker 6:  gonna see a bunch of consumer electronics prices go up. We're gonna see a

1938
01:55:27,825 --> 01:55:31,745
Speaker 6:  bunch of car prices go up. I will say that there's

1939
01:55:31,745 --> 01:55:35,185
Speaker 6:  a lot of a lot of memes of people trading in their Teslas for rivian,

1940
01:55:35,795 --> 01:55:38,985
Speaker 6:  which is really interesting. Yeah, I've seen a cyber truck with the Rivian

1941
01:55:38,985 --> 01:55:41,625
Speaker 6:  logo on the back. I've now seen a cyber truck with the Toyota logo on the

1942
01:55:41,625 --> 01:55:43,485
Speaker 6:  back that's floating around New York City. That

1943
01:55:43,485 --> 01:55:47,405
Speaker 7:  Has been my favorite meme. The people putting like Honda and BMW

1944
01:55:48,015 --> 01:55:50,445
Speaker 7:  logos where the, the Tesla thing used to be. It's

1945
01:55:50,445 --> 01:55:53,165
Speaker 6:  Very funny. I've also seen a picture of a cyber truck where they just have

1946
01:55:53,625 --> 01:55:56,085
Speaker 6:  put like huge fender flares on it to make it look like a different

1947
01:55:56,145 --> 01:55:59,925
Speaker 7:  Car. I was just gonna say the cyber truck famously easy to make look like

1948
01:55:59,925 --> 01:56:00,285
Speaker 7:  other cars.

1949
01:56:00,525 --> 01:56:04,125
Speaker 6:  It's like no, it's a cyber truck bro. And one big wiper really

1950
01:56:04,215 --> 01:56:08,125
Speaker 6:  sells it every time. And then, you know, there's just the general

1951
01:56:08,445 --> 01:56:12,085
Speaker 6:  angst around Tesla BYD the Chinese company is

1952
01:56:12,085 --> 01:56:13,405
Speaker 6:  outselling Tesla right now.

1953
01:56:14,925 --> 01:56:18,765
Speaker 6:  I did BYD to 107 billion in revenue for 2024, which

1954
01:56:18,825 --> 01:56:21,485
Speaker 6:  is way over Tesla's 97.7 billion

1955
01:56:22,605 --> 01:56:26,365
Speaker 6:  BYD ships more than double the vehicles of Tesla if you can at hybrids. And

1956
01:56:26,365 --> 01:56:30,205
Speaker 6:  then they're investing in charging tech. Their, their new bods, new

1957
01:56:30,325 --> 01:56:33,605
Speaker 6:  charging tech is twice as fast as Tesla's. You can get 250 miles range in

1958
01:56:33,605 --> 01:56:34,605
Speaker 6:  like five minutes. Yeah.

1959
01:56:34,745 --> 01:56:37,565
Speaker 7:  And we talked about like the, the ongoing boom in

1960
01:56:38,875 --> 01:56:41,685
Speaker 7:  like excitement about Chinese EVs coming to the us

1961
01:56:42,795 --> 01:56:46,765
Speaker 7:  Like the cars are not but the, the the like talkers and YouTubers are reviewing

1962
01:56:46,765 --> 01:56:49,645
Speaker 7:  them. A lot of UID stuff starting to show up.

1963
01:56:49,715 --> 01:56:52,005
Speaker 6:  Yeah. Andy Hawkins is working on a story on that. I'm, we should have him

1964
01:56:52,005 --> 01:56:52,805
Speaker 6:  on when he does that story.

1965
01:56:53,025 --> 01:56:56,205
Speaker 7:  Oh, there you go. Yeah. And the, the like supposed

1966
01:56:56,595 --> 01:57:00,285
Speaker 7:  biggest day of Tesla take down is this weekend.

1967
01:57:00,465 --> 01:57:03,285
Speaker 7:  And so I think we're gonna, we'll we'll have Andy on next week maybe. We'll

1968
01:57:03,285 --> 01:57:04,685
Speaker 7:  we'll talk through a lot of this stuff. Yeah.

1969
01:57:05,205 --> 01:57:09,085
Speaker 6:  I know people get, there's whatever noise about Tesla take down where it's

1970
01:57:09,085 --> 01:57:12,245
Speaker 6:  all just like domestic terrorism, first of all, domestic terrorism, very

1971
01:57:12,245 --> 01:57:15,805
Speaker 6:  fuzzy concept in American law. It is just true. It has been for a long time.

1972
01:57:16,395 --> 01:57:20,005
Speaker 6:  It's not illegal on its own. Like the things you do are illegal

1973
01:57:20,345 --> 01:57:24,045
Speaker 6:  and there are some definitions but there's not like a law. So like the

1974
01:57:24,045 --> 01:57:27,805
Speaker 6:  government keeps saying this thing and it's like cheat codes to get

1975
01:57:28,405 --> 01:57:31,605
Speaker 6:  surveillance warrants. It's very, it's very weird

1976
01:57:32,225 --> 01:57:35,685
Speaker 6:  and like a lot of the Trump administration is invoking

1977
01:57:35,825 --> 01:57:39,815
Speaker 6:  powers that are reserved for special circumstances. Like

1978
01:57:40,145 --> 01:57:44,055
Speaker 6:  we're at war with Canada. Say you just like say these words

1979
01:57:44,995 --> 01:57:48,775
Speaker 6:  to, to justify even like tariffs are, they're being justified under emergency

1980
01:57:48,775 --> 01:57:52,575
Speaker 6:  powers. Right. So there's some weird stuff with the Tesla protests that we

1981
01:57:52,575 --> 01:57:56,415
Speaker 6:  will cover. But yeah, there's big protests coming. Tesla's under a lot of

1982
01:57:56,575 --> 01:57:59,695
Speaker 6:  pressure. These tariffs might benefit Tesla. So there's some back and forth

1983
01:57:59,695 --> 01:58:03,615
Speaker 6:  there. And then last little, little one, which I think we should end on

1984
01:58:03,615 --> 01:58:07,535
Speaker 6:  a high note, but Cars Rivian had a secret e-bike

1985
01:58:07,535 --> 01:58:10,375
Speaker 6:  company that it's spun out called also, which is pretty cool.

1986
01:58:10,755 --> 01:58:14,535
Speaker 7:  I'm pumped about this. The, the idea of rivian

1987
01:58:14,755 --> 01:58:18,575
Speaker 7:  taking its tech into like they, at first

1988
01:58:18,575 --> 01:58:21,895
Speaker 7:  they were like micro mobility and I was like dope, we're gonna get some scooters

1989
01:58:22,775 --> 01:58:26,175
Speaker 7:  'cause you, you know, I love a scooter startup, but it sounds like it's gonna

1990
01:58:26,175 --> 01:58:29,215
Speaker 7:  be bikes, but they're gonna use some of the rivian tech. They're gonna, they're

1991
01:58:29,215 --> 01:58:33,135
Speaker 7:  gonna use a lot of rivian like manufacturing capabilities. And

1992
01:58:33,235 --> 01:58:37,175
Speaker 7:  one of the things that RJ, the CEO said is that he thinks It is

1993
01:58:37,175 --> 01:58:41,055
Speaker 7:  ridiculous how expensive these great e-bikes are. And I

1994
01:58:41,125 --> 01:58:44,005
Speaker 7:  also think It is ridiculous how expensive they are. And so that made me very

1995
01:58:44,005 --> 01:58:47,745
Speaker 7:  happy. Yeah. Like if Rivian can figure out how to economies of scale

1996
01:58:47,845 --> 01:58:51,225
Speaker 7:  its way into good e-bikes for like half what they currently cost.

1997
01:58:51,755 --> 01:58:53,865
Speaker 7:  Great. Forget cars. Just do that. I'm good.

1998
01:58:53,865 --> 01:58:57,425
Speaker 6:  Yeah, I'm super down with that. I am looking at an R one s like every day

1999
01:58:57,425 --> 01:59:01,145
Speaker 6:  right now. Yeah. Yeah. I I've,

2000
01:59:01,145 --> 01:59:04,825
Speaker 6:  I've test driven one a million times. They, RJ was on de coder, they gave

2001
01:59:04,825 --> 01:59:08,785
Speaker 6:  me one for a weekend. It was cool. It's like they're not

2002
01:59:08,805 --> 01:59:12,745
Speaker 6:  as nice on the inside as our Jeep, but they're obviously better cars

2003
01:59:12,745 --> 01:59:15,745
Speaker 6:  because Jeeps are made by STIs.

2004
01:59:18,185 --> 01:59:21,545
Speaker 6:  I dunno what else to say about that. Like you want a 3-year-old SUV for your

2005
01:59:21,545 --> 01:59:24,785
Speaker 6:  family and you want a bunch of range. Like the R one s is the thing that

2006
01:59:24,785 --> 01:59:28,275
Speaker 6:  exists. Yeah, but they're pretty spartan on the inside.

2007
01:59:28,915 --> 01:59:31,715
Speaker 6:  They're also expensive. I just, I'm just like, oh these tariffs are coming,

2008
01:59:31,715 --> 01:59:32,915
Speaker 6:  all these cars are gonna get more expensive

2009
01:59:33,175 --> 01:59:35,275
Speaker 7:  So buy it now. That's the only answer. Right. And

2010
01:59:35,275 --> 01:59:38,115
Speaker 6:  Like, I'm like, am I just gonna pull the trigger on this car for kind of

2011
01:59:38,115 --> 01:59:40,675
Speaker 6:  no reason except I think the tariffs will make it expensive a year from now.

2012
01:59:41,575 --> 01:59:44,275
Speaker 7:  I'm sure that's a case Becky will buy. Listen, I don't need this car now,

2013
01:59:44,275 --> 01:59:45,715
Speaker 7:  but it'll be more expensive in two weeks.

2014
01:59:46,015 --> 01:59:48,555
Speaker 6:  By the way, if you have a Jeep four by eight, if you got one of the hybrids,

2015
01:59:48,615 --> 01:59:51,955
Speaker 6:  you need to go to the dealer today. Because they just issued the software

2016
01:59:51,955 --> 01:59:55,715
Speaker 6:  update for the recall. They had where the, they

2017
01:59:55,785 --> 01:59:59,635
Speaker 6:  told everyone to not charge their cars because they might start on

2018
01:59:59,635 --> 02:00:03,515
Speaker 6:  fire. Oh boy. And part of that was not just don't charge your car,

2019
02:00:03,515 --> 02:00:06,355
Speaker 6:  it was leave it outside in case it starts on fire.

2020
02:00:07,225 --> 02:00:10,035
Speaker 6:  This is a real thing. They had defective Samsung batteries in them. So the

2021
02:00:10,035 --> 02:00:13,835
Speaker 6:  recalls out. You could go to the dealer, they do the software upgrade. Part

2022
02:00:13,835 --> 02:00:16,635
Speaker 6:  of the software upgrade is just making the software know if there's gonna

2023
02:00:16,635 --> 02:00:20,515
Speaker 6:  be fire. Like these numbers are outta spec. Like there's gonna be

2024
02:00:20,515 --> 02:00:23,395
Speaker 6:  fire. And then another part of the software upgrade is after they do it,

2025
02:00:23,465 --> 02:00:27,195
Speaker 6:  they drive your car around and there's like some specific set of miles you

2026
02:00:27,195 --> 02:00:28,395
Speaker 6:  have to drive at specific speeds

2027
02:00:28,655 --> 02:00:30,435
Speaker 7:  To see if it does fire, to

2028
02:00:30,435 --> 02:00:34,395
Speaker 6:  See if it hits the threshold that requires a battery call. You can understand

2029
02:00:34,395 --> 02:00:37,355
Speaker 6:  where I'm like, I should get a different car 'cause we did all the things

2030
02:00:38,055 --> 02:00:41,155
Speaker 6:  and a car was fine. But like once that happens you're like, I don't

2031
02:00:41,795 --> 02:00:42,685
Speaker 6:  like you anymore.

2032
02:00:44,885 --> 02:00:48,775
Speaker 6:  I'm, I, you know, I just, I have uncomfortable feelings about you specifically.

2033
02:00:49,095 --> 02:00:49,295
Speaker 6:  Yeah. You

2034
02:00:49,295 --> 02:00:51,295
Speaker 7:  Don't get that trust back, I don't think. Yeah.

2035
02:00:51,515 --> 02:00:55,375
Speaker 6:  And I, yeah, so I dunno if you, if you can talk me into an R one s,

2036
02:00:55,375 --> 02:00:58,935
Speaker 6:  you know, send me a note. There we go. I'm also open to a 2002 Escalade,

2037
02:00:58,935 --> 02:01:00,535
Speaker 6:  which you can buy at any point for $2,000.

2038
02:01:02,165 --> 02:01:05,255
Speaker 6:  It's all, every time we talk about a computer on the show and we're like,

2039
02:01:05,255 --> 02:01:09,135
Speaker 6:  it's $1,500, we're like a new iPad. I'm like, I could get a

2040
02:01:09,135 --> 02:01:10,095
Speaker 6:  2002 Escalade.

2041
02:01:11,995 --> 02:01:14,455
Speaker 7:  That's the dream. All right. We need to get outta here. We

2042
02:01:14,455 --> 02:01:18,055
Speaker 6:  Got it. Let me know if you have a 2002 escalate. That's awesome. If you've

2043
02:01:18,055 --> 02:01:21,495
Speaker 6:  got one, you know, let's make a deal. Let's talk. All right. That's it.

2044
02:01:21,795 --> 02:01:23,095
Speaker 6:  That's Vergecast. Fuck all.

2045
02:01:28,275 --> 02:01:31,615
Speaker 10:  And that's it for The Vergecast this week. And hey, we'd love to hear from

2046
02:01:31,615 --> 02:01:35,295
Speaker 10:  you. Give us a call at eight six six VERGE one one.

2047
02:01:35,435 --> 02:01:38,935
Speaker 10:  The Vergecast is a production of The Verge and the Vox Media Podcast network.

2048
02:01:39,275 --> 02:01:43,095
Speaker 10:  Our show is produced by Will Por, Eric Gomez and Brandon Keefer. And

2049
02:01:43,095 --> 02:01:44,255
Speaker 10:  that's it. We'll see you next week.

