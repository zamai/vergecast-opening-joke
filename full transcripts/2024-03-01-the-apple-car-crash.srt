1
00:00:00,000 --> 00:00:05,000
# Transcript
ID: 369ef2df-6949-434a-9a1b-df398382b1b0
Status: Done
Stage: Done
Audio URL: https://jfe93e.s3.amazonaws.com/4845369631941694826/-5125746174128096082/s93290-US-5598s-1709289488.mp3
Description: The Verge's Nilay Patel and David Pierce talk through the weird and winding history of Apple's "secret" car project now that it's officially dead. And later, senior policy reporter Lauren Feiner makes her Vergecast debut to catch us up on the arguments made this week in the Supreme Court about online speech and the First Amendment.


2
00:01:20,165 --> 00:01:23,675
Speaker 5:  Hello And Welcome to Vergecast. The flagship podcast of the Apple car.

3
00:01:24,575 --> 00:01:27,675
Speaker 2:  In that it, we've spent a lot of money and it doesn't exist.

4
00:01:28,575 --> 00:01:32,475
Speaker 5:  That's The Vergecast everybody. $10 billion in a decade later and

5
00:01:32,535 --> 00:01:36,115
Speaker 5:  we have more to show for it than Apple does with the car project.

6
00:01:36,375 --> 00:01:38,875
Speaker 5:  We did. I'm your friend NELI. David. Pierce is here.

7
00:01:38,895 --> 00:01:40,315
Speaker 2:  Hi. We're in the studio. This is very exciting.

8
00:01:40,315 --> 00:01:43,435
Speaker 5:  We, David and I are together. Like they saying cable news, Alex Cranz is

9
00:01:43,435 --> 00:01:46,675
Speaker 5:  off this week. That's what they say. That's all the time. Yeah, that's good.

10
00:01:46,675 --> 00:01:49,675
Speaker 5:  I like it. That's just the thing they say, you know, to explain it. No further

11
00:01:49,675 --> 00:01:53,635
Speaker 5:  explanation even no Alex is, should we say it?

12
00:01:53,845 --> 00:01:57,475
Speaker 2:  She's sick. She's sick. Yeah. She'll be okay. Yeah. Send Alex nice

13
00:01:57,845 --> 00:01:59,115
Speaker 2:  vibes. Anything else? Yeah,

14
00:01:59,125 --> 00:02:03,075
Speaker 5:  Happy vibes. Alex. But Lauren Feiner, our new policy

15
00:02:03,315 --> 00:02:06,315
Speaker 5:  reporter is gonna join us in the second segment to talk about the Supreme

16
00:02:06,325 --> 00:02:07,635
Speaker 5:  court hearing

17
00:02:09,895 --> 00:02:13,755
Speaker 5:  Parade of justice about free speech

18
00:02:13,755 --> 00:02:16,155
Speaker 5:  on the internet, which is one of, you know, that's what you come to the show

19
00:02:16,155 --> 00:02:16,515
Speaker 5:  for. What

20
00:02:16,515 --> 00:02:18,315
Speaker 2:  Gadgets does YouTube weigh mean? Like how

21
00:02:18,315 --> 00:02:18,675
Speaker 5:  Much does

22
00:02:18,675 --> 00:02:19,115
Speaker 2:  YouTube weigh?

23
00:02:21,025 --> 00:02:23,995
Speaker 5:  It's a real thing that came up. We'll get to that but we should start

24
00:02:24,825 --> 00:02:28,715
Speaker 5:  with news. The news of the week, which is Apple

25
00:02:28,975 --> 00:02:30,315
Speaker 5:  killed their car project

26
00:02:31,085 --> 00:02:34,915
Speaker 2:  After so many almost and kind of

27
00:02:35,135 --> 00:02:38,715
Speaker 2:  and False starts and what? Like almost exactly a decade

28
00:02:38,935 --> 00:02:42,755
Speaker 2:  of work. Right? So it was, it was gonna be

29
00:02:43,035 --> 00:02:46,835
Speaker 2:  a car kind of like Tesla and then it was gonna be a self-driving car kind

30
00:02:46,835 --> 00:02:50,555
Speaker 2:  of like Waymo and then I think it was gonna be a car, kind of like a Tesla

31
00:02:50,845 --> 00:02:52,835
Speaker 2:  again and now it's going to be nothing.

32
00:02:53,015 --> 00:02:54,915
Speaker 5:  Oh there, there are three more twisted. Were there more?

33
00:02:54,975 --> 00:02:55,195
Speaker 2:  I'm

34
00:02:55,195 --> 00:02:58,795
Speaker 5:  Sure there were then they were gonna focus on making the software

35
00:02:58,985 --> 00:03:02,485
Speaker 5:  for other cars. Oh right. That was a, that was a weird twist in there.

36
00:03:02,995 --> 00:03:06,605
Speaker 5:  Then they were gonna buy the platform for like an EV

37
00:03:06,845 --> 00:03:10,685
Speaker 5:  platform from Hyundai and put Apple stuff on top of that and Hyundai would

38
00:03:10,685 --> 00:03:13,685
Speaker 5:  turn into Foxconn and Hyundai was like, no, we are a car maker.

39
00:03:14,825 --> 00:03:18,605
Speaker 5:  We, no, don't do that to us. Yeah. Then Foxconn was like, we're gonna build

40
00:03:18,605 --> 00:03:18,885
Speaker 5:  cars

41
00:03:21,305 --> 00:03:24,485
Speaker 5:  Foxcon there like at least 10 more twists and turns in there.

42
00:03:24,515 --> 00:03:27,285
Speaker 2:  Yeah, I mean it's really fascinating though 'cause I think If you rewind

43
00:03:27,385 --> 00:03:31,125
Speaker 2:  10 years ago to when this was gonna happen. It was the moment

44
00:03:31,155 --> 00:03:34,685
Speaker 2:  that everybody was saying all cars will be self-driving

45
00:03:34,985 --> 00:03:37,605
Speaker 2:  and the world will have completely changed and everything will be on the

46
00:03:37,605 --> 00:03:40,885
Speaker 2:  road by like 2020. Yeah. And that was that people earnestly believed that

47
00:03:40,885 --> 00:03:43,725
Speaker 2:  like Uber was saying that Lyft was saying that all the car companies were

48
00:03:43,725 --> 00:03:47,685
Speaker 2:  saying that Elon Musk was saying that there was this certainty that by

49
00:03:47,685 --> 00:03:51,565
Speaker 2:  2020 the whole idea of owning a car

50
00:03:51,565 --> 00:03:54,205
Speaker 2:  was going to have disappeared and there were going to be fleets of robots

51
00:03:55,005 --> 00:03:58,965
Speaker 2:  roaming the streets everywhere. Yeah. And that just could

52
00:03:58,965 --> 00:04:01,005
Speaker 2:  not be further from the truth. At this particular

53
00:04:01,005 --> 00:04:03,565
Speaker 5:  Moment in time, there are some robots roaming the streets of San Francisco.

54
00:04:03,565 --> 00:04:03,765
Speaker 5:  There

55
00:04:03,765 --> 00:04:07,125
Speaker 2:  Are a few robots, well Andy Hawkins who wrote a couple of great pieces about

56
00:04:07,125 --> 00:04:11,005
Speaker 2:  this this week, made the point that it's actually not that hard at this moment

57
00:04:11,025 --> 00:04:15,005
Speaker 2:  in time to strap a camera to the top of a Toyota Highlander and put it

58
00:04:15,005 --> 00:04:18,485
Speaker 2:  on the roads. And that's true but it's, it is such a

59
00:04:18,725 --> 00:04:22,565
Speaker 2:  fascinating like counterfactual of history. If it had gone differently

60
00:04:22,565 --> 00:04:26,405
Speaker 2:  and we had solved kind of those core tenets of self-driving technology,

61
00:04:27,655 --> 00:04:30,765
Speaker 2:  would Apple have released a car? What would it have been? Mostly? I'm just

62
00:04:30,765 --> 00:04:34,285
Speaker 2:  sad that we don't get to see the thing that Apple was building. Like I feel

63
00:04:34,285 --> 00:04:34,805
Speaker 2:  like Apple, well

64
00:04:34,805 --> 00:04:37,565
Speaker 5:  We should back up because I really want to talk about what Apple may or may

65
00:04:37,565 --> 00:04:39,285
Speaker 5:  not have been building for quite a long time.

66
00:04:39,785 --> 00:04:40,005
Speaker 2:  Yes.

67
00:04:40,985 --> 00:04:44,725
Speaker 5:  But the, the thing we're talking about was Apple sent out a memo to its

68
00:04:44,845 --> 00:04:45,125
Speaker 5:  employees

69
00:04:46,635 --> 00:04:50,485
Speaker 5:  just saying we're shutting down the car project and those employees will

70
00:04:50,485 --> 00:04:52,725
Speaker 5:  be moved to to generative ai

71
00:04:52,925 --> 00:04:56,085
Speaker 2:  I believe 2000 employees, which the number that was reported

72
00:04:56,705 --> 00:04:59,445
Speaker 5:  And that project has been both bigger and smaller than that over time. Right.

73
00:05:00,475 --> 00:05:03,165
Speaker 5:  They've been working on this for 10 years. It's one of the worst kept secrets

74
00:05:03,165 --> 00:05:06,605
Speaker 5:  in Silicon Valley. There are apple cars just driving the streets,

75
00:05:06,735 --> 00:05:09,525
Speaker 5:  collecting training data you might call it.

76
00:05:09,875 --> 00:05:13,245
Speaker 2:  Well we should say there are Apple owned cars. Yeah. Doing that. They are

77
00:05:13,245 --> 00:05:13,925
Speaker 2:  not Apple cars.

78
00:05:14,545 --> 00:05:17,445
Speaker 5:  But then Apple doesn't do Toyota. What is an Apple carers by the way? They

79
00:05:17,445 --> 00:05:18,165
Speaker 5:  buy Lexuses.

80
00:05:18,345 --> 00:05:19,245
Speaker 2:  Oh you're right. That's true.

81
00:05:19,805 --> 00:05:20,405
Speaker 5:  I just wanna put that

82
00:05:20,455 --> 00:05:21,365
Speaker 2:  Fancy Highlanders

83
00:05:21,705 --> 00:05:25,285
Speaker 5:  Do fancier highlanders than than your average self-driving car company.

84
00:05:25,545 --> 00:05:29,285
Speaker 5:  So the projects started in the moment David

85
00:05:29,345 --> 00:05:33,245
Speaker 5:  is describing, which is Apple needed its next big thing

86
00:05:34,105 --> 00:05:38,045
Speaker 5:  and apple size markets are hard to come by. So they,

87
00:05:38,905 --> 00:05:42,805
Speaker 5:  it was healthcare and cars, right? Or the next thing can like move the needle

88
00:05:42,805 --> 00:05:46,725
Speaker 5:  on Apple's revenue. And they decided healthcare, they do a bunch of

89
00:05:46,725 --> 00:05:50,365
Speaker 5:  watch stuff and then cars always

90
00:05:50,365 --> 00:05:54,165
Speaker 5:  seemed like a lark. Like to me it always seemed like what are you doing?

91
00:05:54,395 --> 00:05:57,925
Speaker 5:  Okay. but it has been 10 years of

92
00:05:58,385 --> 00:06:02,245
Speaker 5:  people coming and going from that division of Apple called Project

93
00:06:02,245 --> 00:06:05,965
Speaker 5:  Titan. And this last turn they took

94
00:06:05,965 --> 00:06:09,365
Speaker 5:  Kevin Lynch who ran the watch and said, you're in charge of it. And I think

95
00:06:09,505 --> 00:06:10,525
Speaker 5:  he was like, well

96
00:06:12,915 --> 00:06:16,125
Speaker 5:  what if I'm not, this is nothing. Yeah. What if I don't do this? Yeah.

97
00:06:16,155 --> 00:06:19,165
Speaker 2:  Well I think it it Kevin Lynch being the one in charge is a funny example

98
00:06:19,165 --> 00:06:23,125
Speaker 2:  because he launched the Apple Watch and was very much the one

99
00:06:23,125 --> 00:06:26,965
Speaker 2:  who spearheaded that project. And to his credit was also a big part

100
00:06:26,965 --> 00:06:30,525
Speaker 2:  of how Apple discovered what the Apple Watch was. Yeah. Right. Because we've

101
00:06:30,525 --> 00:06:33,285
Speaker 2:  talked about many times in the show the Apple Watch was made because Apple

102
00:06:33,285 --> 00:06:36,165
Speaker 2:  wanted to make a watch like it. That was the beginning of the story Johnny.

103
00:06:36,325 --> 00:06:40,125
Speaker 2:  I was like, what if watch And that's how it started. And so Apple over time

104
00:06:40,125 --> 00:06:43,325
Speaker 2:  figured out what it could be, what it should be, how it could work, who it

105
00:06:43,325 --> 00:06:47,245
Speaker 2:  would be for. And it has worked like kudos to that team. They, they got

106
00:06:47,245 --> 00:06:51,165
Speaker 2:  there, they figured it out. And my assumption is that the car

107
00:06:51,165 --> 00:06:54,005
Speaker 2:  is sort of the same way where like you're saying Apple looked around and

108
00:06:54,005 --> 00:06:57,965
Speaker 2:  said, okay, what's a huge market? Oh cars, let's make a car. And then

109
00:06:57,965 --> 00:07:01,885
Speaker 2:  at some point somebody had to sit down and go, what do we do?

110
00:07:02,275 --> 00:07:02,965
Speaker 2:  What is that?

111
00:07:03,185 --> 00:07:05,925
Speaker 5:  So there's a story that comes up over and over again. It's in some of the

112
00:07:05,925 --> 00:07:09,885
Speaker 5:  reporting today and the times we had trip nickel on

113
00:07:09,885 --> 00:07:13,845
Speaker 5:  the show a while back. It was in his book about Apple and just sort of the

114
00:07:13,845 --> 00:07:17,205
Speaker 5:  fight between Tim Cook and Johnny. Ive in the post jobs era

115
00:07:17,625 --> 00:07:21,605
Speaker 5:  and the story has come up several times now. Johnny, ive built a

116
00:07:21,605 --> 00:07:25,445
Speaker 5:  model of a car with no steering wheel and he sat Tim Cook down in

117
00:07:25,445 --> 00:07:29,205
Speaker 5:  it and they pretended they were on a drive while an actor, this is true.

118
00:07:29,465 --> 00:07:33,325
Speaker 5:  An actor pretended to be Siri and pretended to talk to them

119
00:07:33,465 --> 00:07:34,365
Speaker 5:  as they went on their pretend

120
00:07:34,365 --> 00:07:36,805
Speaker 2:  Ride and tell them them like the name of restaurants that they were going

121
00:07:36,905 --> 00:07:40,685
Speaker 2:  by and stuff like that. It was supposed to be, that was the interface for

122
00:07:40,685 --> 00:07:42,885
Speaker 2:  the car was Siri. Which is terrifying.

123
00:07:43,005 --> 00:07:45,725
Speaker 5:  I just want everyone to sit with that for a minute and imagine

124
00:07:47,475 --> 00:07:51,125
Speaker 5:  like these, when you think about Tim Cook, imagine him going on a pretend

125
00:07:51,225 --> 00:07:55,045
Speaker 5:  car ride with Johnny. Ive actor actor. It really, the most actor does

126
00:07:55,045 --> 00:07:56,325
Speaker 5:  pretend Siri.

127
00:07:56,915 --> 00:08:00,805
Speaker 2:  It's the most like CES vaporware demo from a company

128
00:08:00,825 --> 00:08:03,805
Speaker 2:  in China you've never heard of and will never hear from again. Thing that

129
00:08:03,805 --> 00:08:04,405
Speaker 2:  I've ever heard.

130
00:08:04,945 --> 00:08:08,325
Speaker 5:  But it's also like, does Johnny, ive not think Tim Cook has any power of

131
00:08:08,325 --> 00:08:12,005
Speaker 5:  imagination because I could be like, here's what it's gonna be like. You're

132
00:08:12,005 --> 00:08:14,325
Speaker 5:  gonna sit in the car and drive around here, we gonna talk to you. Yeah. And

133
00:08:14,325 --> 00:08:18,285
Speaker 5:  I don't need that acted out for me. Like

134
00:08:19,125 --> 00:08:19,845
Speaker 5:  I like I get it. It

135
00:08:19,845 --> 00:08:22,085
Speaker 2:  Doesn't help to have an actor go, that's a Dwayne Reed.

136
00:08:22,595 --> 00:08:26,485
Speaker 5:  It's, I don't need that. Like it's in the mind's

137
00:08:26,485 --> 00:08:30,445
Speaker 5:  eye, you know? I got it. I should close your eyes. Everybody pull over

138
00:08:30,445 --> 00:08:34,325
Speaker 5:  your car. Imagine your car's driving itself and like serious, like

139
00:08:34,435 --> 00:08:36,485
Speaker 5:  there's a hospital. Like that's all you need to do.

140
00:08:38,125 --> 00:08:41,165
Speaker 5:  I, I think about that moment all the time. It's like just Johnny, I think

141
00:08:41,165 --> 00:08:44,845
Speaker 5:  to cook can't imagine the most basic thing you can imagine

142
00:08:44,845 --> 00:08:46,445
Speaker 5:  about a self-driving car. There

143
00:08:46,445 --> 00:08:46,845
Speaker 2:  Are seats.

144
00:08:48,185 --> 00:08:52,165
Speaker 5:  The end, I made a beautiful chair for you. Sit in it and pretend it's a car.

145
00:08:53,145 --> 00:08:56,285
Speaker 5:  So that demo apparently convinced them that this is The future of the car

146
00:08:56,705 --> 00:09:00,485
Speaker 5:  and they wanted to do the big thing which is

147
00:09:00,485 --> 00:09:04,365
Speaker 5:  self-driving Siri car. Right. No steering wheel. Which is

148
00:09:04,685 --> 00:09:07,245
Speaker 5:  I think like that's the apple level innovation leap.

149
00:09:07,825 --> 00:09:11,685
Speaker 2:  Yes. And I think at that time that's where If, you Apple, you say,

150
00:09:11,685 --> 00:09:15,485
Speaker 2:  okay, there is a thing we can do that is different. Right? Yeah. 'cause if

151
00:09:15,505 --> 00:09:18,445
Speaker 2:  if, if you're Apple and you're just saying, oh let's make a pretty good SUV

152
00:09:18,675 --> 00:09:22,485
Speaker 2:  like that, that's not what Apple does. But If you

153
00:09:22,805 --> 00:09:26,765
Speaker 2:  believed as they did at that time that in a decade the

154
00:09:26,765 --> 00:09:30,525
Speaker 2:  whole way we experience cars would be different. You can see how

155
00:09:30,905 --> 00:09:34,845
Speaker 2:  you would talk yourself into oh it's just gonna be screens you sit in

156
00:09:35,195 --> 00:09:36,965
Speaker 2:  instead of screens. Seriously. Like it's, and

157
00:09:36,965 --> 00:09:38,525
Speaker 5:  We can take 30% of those screens too.

158
00:09:38,525 --> 00:09:42,165
Speaker 2:  Exactly. Like literally it's like the services business in a car is gonna

159
00:09:42,165 --> 00:09:42,845
Speaker 2:  be nuts someday.

160
00:09:43,165 --> 00:09:45,685
Speaker 5:  And by the way, all the car makers believe this. Yeah. This is why GM took

161
00:09:45,685 --> 00:09:49,605
Speaker 5:  CarPlay out of its cars. Like down the line. The idea that you'll roll

162
00:09:49,605 --> 00:09:53,445
Speaker 5:  around in a shopping mall. It, everyone's very excited about the

163
00:09:53,445 --> 00:09:53,605
Speaker 5:  city.

164
00:09:53,725 --> 00:09:57,445
Speaker 2:  A hundred percent. And I feel like the tension has been, there has been this

165
00:09:57,445 --> 00:10:01,005
Speaker 2:  incredible sort of whipsawing hype around self-driving

166
00:10:01,505 --> 00:10:05,245
Speaker 2:  and it seems like whipsawing strategy inside of Apple

167
00:10:05,305 --> 00:10:08,085
Speaker 2:  as a result. And every once in a while they've just been like, oh crap, that's

168
00:10:08,085 --> 00:10:10,605
Speaker 2:  not gonna work. We should just build a car. And then somebody's like, why

169
00:10:10,605 --> 00:10:13,565
Speaker 2:  the hell are we just building a car? And then somebody else is like, but

170
00:10:13,565 --> 00:10:14,885
Speaker 2:  what if it drove itself and they're like,

171
00:10:14,885 --> 00:10:18,365
Speaker 5:  We're back big. Just hire 2000 engineers.

172
00:10:18,875 --> 00:10:22,045
Speaker 5:  Yeah. That part, that's the other term that I think is interesting. There's

173
00:10:22,045 --> 00:10:25,685
Speaker 5:  all these rumors they try to buy Tesla, they investigated it. I think

174
00:10:25,875 --> 00:10:29,805
Speaker 5:  Elon has like said things that Apple has denied over the course of

175
00:10:29,805 --> 00:10:30,285
Speaker 5:  the past decade

176
00:10:30,285 --> 00:10:33,765
Speaker 2:  About their interest. Well there was some reporting around this that Elon,

177
00:10:34,915 --> 00:10:38,685
Speaker 2:  Elon and Tim Cook did have some conversation about it Apple

178
00:10:38,705 --> 00:10:41,685
Speaker 2:  wasn't interested in also Elon would only do it if he was made the CEO EO

179
00:10:41,685 --> 00:10:45,165
Speaker 2:  of Apple, which is just an incredible That's great. I have no, I like a

180
00:10:45,355 --> 00:10:48,965
Speaker 2:  perfectly in character for Elon Musk BI have no idea if it's true, but I

181
00:10:49,165 --> 00:10:50,405
Speaker 2:  I am choosing to believe that that is,

182
00:10:50,405 --> 00:10:53,485
Speaker 5:  That's a very, that's a very truthy Yeah. you know, it just feels good. It

183
00:10:53,485 --> 00:10:56,925
Speaker 5:  does feel good to rattle that what is true is that Johnny and I have sat

184
00:10:56,985 --> 00:11:00,485
Speaker 5:  Tim Cook down in a pretend car and they went on a pretend ride,

185
00:11:00,775 --> 00:11:04,285
Speaker 5:  which I just will never, like that's when you know like Tim Cook should like,

186
00:11:04,285 --> 00:11:07,005
Speaker 5:  so this is your No we're not doing this.

187
00:11:07,385 --> 00:11:10,085
Speaker 2:  And it wasn't even a car, it was like a couch. Yeah. By all it was just,

188
00:11:10,085 --> 00:11:10,685
Speaker 2:  it was just a seat.

189
00:11:10,915 --> 00:11:14,405
Speaker 5:  It's just very good. I just want everyone, it's fine. I want you to use your

190
00:11:14,405 --> 00:11:18,245
Speaker 5:  imagination to imagine them using their imaginations. Just think

191
00:11:18,245 --> 00:11:18,525
Speaker 5:  about it.

192
00:11:19,865 --> 00:11:21,205
Speaker 2:  Do you think there was like fake wind?

193
00:11:22,485 --> 00:11:24,645
Speaker 5:  I can't stop thinking about this. Was

194
00:11:24,845 --> 00:11:26,485
Speaker 2:  Somebody scrolling a picture past also like

195
00:11:26,485 --> 00:11:28,365
Speaker 5:  These are not, I would say

196
00:11:29,885 --> 00:11:33,805
Speaker 5:  mirthful personalities. Like they were taking all this very seriously.

197
00:11:33,805 --> 00:11:37,725
Speaker 5:  Oh yeah. These are two very serious people. It just, just imagine it

198
00:11:37,725 --> 00:11:41,685
Speaker 5:  just like, I don't know like act it out with your friends. See If, you

199
00:11:41,685 --> 00:11:42,685
Speaker 5:  can even get through it.

200
00:11:44,925 --> 00:11:48,805
Speaker 5:  I can't get over it. Okay, so then you got, we should buy Tesla. We

201
00:11:48,805 --> 00:11:51,885
Speaker 5:  can't do self-driving technologies there. We should just build a car. And

202
00:11:51,885 --> 00:11:55,605
Speaker 5:  this is when you were like, what does it look like? I cannot stop thinking

203
00:11:55,605 --> 00:11:59,365
Speaker 5:  about the moments when Apple was like, okay,

204
00:11:59,365 --> 00:12:02,925
Speaker 5:  what does a car look like? Because a car in the

205
00:12:02,925 --> 00:12:06,565
Speaker 5:  United States at this point in time looks like a mid-size crossover. Yes.

206
00:12:07,045 --> 00:12:10,365
Speaker 5:  Arguably the least designed forward shape for a car.

207
00:12:10,555 --> 00:12:10,845
Speaker 2:  Yeah.

208
00:12:11,555 --> 00:12:13,365
Speaker 5:  Like here's a shoe. Yeah.

209
00:12:13,635 --> 00:12:17,125
Speaker 2:  It's just a Subaru. Like all cars are Subarus now. It's just

210
00:12:17,125 --> 00:12:20,805
Speaker 5:  What it is. It's like old Toyotas were angry Robot Subarus, new

211
00:12:20,805 --> 00:12:24,165
Speaker 5:  Toyotas are sort of like mean fish Subarus. Like

212
00:12:24,755 --> 00:12:25,765
Speaker 5:  it's all the same slightly

213
00:12:25,955 --> 00:12:29,925
Speaker 2:  Racy Subarus. Yeah. Slightly tall Subarus. It's

214
00:12:29,925 --> 00:12:31,485
Speaker 2:  just Subarus all the way down man.

215
00:12:32,265 --> 00:12:35,965
Speaker 5:  And so the idea that Apple's gonna like deliver a hot

216
00:12:35,965 --> 00:12:39,685
Speaker 5:  looking mid-size crossover, right. Is

217
00:12:39,755 --> 00:12:43,605
Speaker 5:  very funny. Yes. Like very funny. Like mid-size crossovers look so

218
00:12:43,605 --> 00:12:47,365
Speaker 5:  much the same that the new Ferrari SUV looks

219
00:12:47,465 --> 00:12:51,285
Speaker 5:  almost exactly like a Buick. Like there are, there is

220
00:12:51,365 --> 00:12:55,085
Speaker 5:  a new Buick that looks almost exactly like a Ferrari that looks almost

221
00:12:55,085 --> 00:12:58,325
Speaker 5:  exactly like a Mazda. And it's like Apple entering this market and being

222
00:12:58,325 --> 00:12:59,685
Speaker 5:  like design everyone. Right.

223
00:13:00,875 --> 00:13:04,405
Speaker 2:  Well and to some extent that is the result of

224
00:13:04,955 --> 00:13:08,845
Speaker 2:  literal decades of work from these companies. And again,

225
00:13:08,845 --> 00:13:12,285
Speaker 2:  some of the reporting around this has been that Apple basically it

226
00:13:12,375 --> 00:13:16,285
Speaker 2:  infuriated everybody who came to work on the carb because it said we're not

227
00:13:16,285 --> 00:13:19,245
Speaker 2:  doing anything the way the car industry has done things. We're gonna, you

228
00:13:19,245 --> 00:13:21,765
Speaker 2:  know, operate from first principles. Oh God. Which you can imagine means

229
00:13:21,765 --> 00:13:24,325
Speaker 2:  there's a meeting run by Johnny Ive where he's like, what if the wheels weren't

230
00:13:24,325 --> 00:13:28,165
Speaker 2:  round just as like a thought experiment. What other

231
00:13:28,165 --> 00:13:28,965
Speaker 2:  shape could a wheel be?

232
00:13:28,985 --> 00:13:31,805
Speaker 5:  No, it's Johnny Ab. He'd be like, the answer must be so simple. It seems

233
00:13:31,805 --> 00:13:35,245
Speaker 5:  obvious. Everyone's like round wheels dude. Nailed it. Yeah.

234
00:13:35,635 --> 00:13:38,485
Speaker 2:  Like what if it just hovers? And they're like, no Johnny, that's not it.

235
00:13:38,985 --> 00:13:42,885
Speaker 2:  But, and and, and so from all the way down it's like this is what Tesla

236
00:13:42,885 --> 00:13:46,325
Speaker 2:  experienced too. Like it turns out there aren't actually all that many ways

237
00:13:46,325 --> 00:13:50,045
Speaker 2:  to make a car and you can, you can litigate forever whether that's a good

238
00:13:50,045 --> 00:13:52,805
Speaker 2:  thing or a bad thing. The way the regulation works, the way the industry

239
00:13:52,805 --> 00:13:56,725
Speaker 2:  works, the e economies of scale. Like there aren't that many ways to make

240
00:13:56,765 --> 00:13:59,965
Speaker 2:  a car and If, you try to get weird with it, you make a cyber truck,

241
00:14:01,115 --> 00:14:05,005
Speaker 2:  there's that. But Apple basically tried to

242
00:14:05,005 --> 00:14:08,845
Speaker 2:  do a thing that I think it just couldn't do. And again, I wish we had

243
00:14:08,845 --> 00:14:10,725
Speaker 2:  gotten to see what it would look like. I think there's some reporting

244
00:14:10,725 --> 00:14:11,005
Speaker 5:  Someone at

245
00:14:11,005 --> 00:14:14,765
Speaker 2:  Apple is re it is their moral responsibility to just like

246
00:14:15,035 --> 00:14:18,845
Speaker 2:  post the slide deck, post the keynote file. Show me some showing

247
00:14:18,845 --> 00:14:19,325
Speaker 2:  the designs.

248
00:14:19,585 --> 00:14:23,125
Speaker 5:  So there is a little bit of reporting in the Times today. There's actually

249
00:14:23,125 --> 00:14:26,765
Speaker 5:  a lot of great senses in this time space. But there's the Mr I but this paragraph

250
00:14:26,765 --> 00:14:30,165
Speaker 5:  jumped out Mr. I and his team of designers through concepts for a car that

251
00:14:30,165 --> 00:14:34,125
Speaker 5:  would look like a European minivan such as the Fiat multi plus

252
00:14:34,185 --> 00:14:37,165
Speaker 5:  600, which has half a dozen windows in a curving roof.

253
00:14:38,065 --> 00:14:41,165
Speaker 5:  It had no steering wheel and we controlled by Apple's virtual assistant Siri.

254
00:14:42,585 --> 00:14:45,485
Speaker 5:  So it was a minivan they made steering

255
00:14:45,485 --> 00:14:47,965
Speaker 2:  Wheel. Yeah. And there was some reporting in there that one of the like big

256
00:14:47,965 --> 00:14:51,725
Speaker 2:  innovative things they were gonna do is put a, a polymer on the sunroof

257
00:14:51,945 --> 00:14:55,605
Speaker 2:  so that you didn't get heat in the sunroof. And it's like that

258
00:14:55,745 --> 00:14:59,125
Speaker 2:  is the extent to which you are allowed to innovate in cars is polymers on

259
00:14:59,125 --> 00:15:02,965
Speaker 2:  the sunroof. Right. Like otherwise we pretty much did it for what

260
00:15:02,965 --> 00:15:06,205
Speaker 2:  cars are for now. We've pretty much done it.

261
00:15:06,705 --> 00:15:10,645
Speaker 5:  Can I just point out the Fiat 600 multiple looks exactly

262
00:15:10,645 --> 00:15:14,605
Speaker 5:  like a VW bus. Like it is the most Johnny I thing to be like, so it's a

263
00:15:14,605 --> 00:15:16,685
Speaker 5:  VW bus where we picked a weirder fiat version of it.

264
00:15:16,995 --> 00:15:18,165
Speaker 2:  It's a European one.

265
00:15:19,025 --> 00:15:22,725
Speaker 5:  So is the VW bus. No, no. It's a fiat this time.

266
00:15:23,825 --> 00:15:27,565
Speaker 5:  All of that is just the silliest to me. Yes. And I think app, the reality

267
00:15:27,585 --> 00:15:30,565
Speaker 5:  of what a car is to people, like apple's full of car nerds

268
00:15:31,435 --> 00:15:35,405
Speaker 5:  like Eddie Q is on a board of Ferrari. I think Aston Martin

269
00:15:35,625 --> 00:15:39,445
Speaker 5:  has the new version of CarPlay because Phil Schiller just like

270
00:15:39,795 --> 00:15:42,685
Speaker 5:  went there and was like, look I own so many of these.

271
00:15:43,555 --> 00:15:46,245
Speaker 5:  Like just let's let's make it happen. Yeah.

272
00:15:46,425 --> 00:15:49,765
Speaker 2:  And Johnny i's a big car guy. Like it's it is, it is very much in

273
00:15:50,265 --> 00:15:54,245
Speaker 2:  the DNA of that company to love cars. Yeah. Wasn't it jobs who got the, like

274
00:15:54,245 --> 00:15:56,805
Speaker 2:  the new Mercedes every six months so that he didn't have to have a license

275
00:15:56,805 --> 00:16:00,725
Speaker 2:  plate. Yes. That's good. Like that's the make a car without a license

276
00:16:00,725 --> 00:16:04,245
Speaker 2:  plate Apple. Like there, there it is. We did it. But that's

277
00:16:04,645 --> 00:16:07,365
Speaker 2:  like I, I keep coming around to this question of

278
00:16:08,425 --> 00:16:12,165
Speaker 2:  was this ever going to work? It is. So for Apple, a

279
00:16:12,165 --> 00:16:15,405
Speaker 2:  backwards way of making great products, which is just like we found a huge

280
00:16:15,405 --> 00:16:18,805
Speaker 2:  market, let's go do something there. Yeah. But I do also think If you If

281
00:16:18,805 --> 00:16:22,325
Speaker 2:  you twist the world a little differently and self-driving technology is much

282
00:16:22,325 --> 00:16:25,285
Speaker 2:  further along and the regulation is much further along and we're doing better

283
00:16:25,305 --> 00:16:25,925
Speaker 2:  at all of that.

284
00:16:27,805 --> 00:16:31,045
Speaker 2:  I think it's not crazy to imagine that at some point we are due for a big

285
00:16:31,285 --> 00:16:33,005
Speaker 2:  reinvention of what a car is. But

286
00:16:33,005 --> 00:16:36,165
Speaker 5:  Also I just, again, I'm just going to, I wanna come back to this

287
00:16:36,785 --> 00:16:40,525
Speaker 5:  Johnny, I sat Tim Cook down in 2015 Yeah. In a fake

288
00:16:40,625 --> 00:16:44,165
Speaker 5:  car and said, what I'd like to do is build a hundred thousand dollars

289
00:16:44,425 --> 00:16:48,365
Speaker 5:  bus with no steering wheel. And they were like, let's

290
00:16:48,365 --> 00:16:52,165
Speaker 5:  spend 10 years on this. And that is, that's not

291
00:16:52,165 --> 00:16:54,965
Speaker 5:  just backwards, right. That's like we're not even gonna move the market

292
00:16:55,945 --> 00:16:59,645
Speaker 5:  to like, we're not even gonna move enough units to make revenue

293
00:16:59,745 --> 00:17:00,445
Speaker 5:  to make this worth it.

294
00:17:00,455 --> 00:17:03,725
Speaker 2:  Right. I mean we just talked the other week on the show about the fact that

295
00:17:03,785 --> 00:17:07,685
Speaker 2:  the market $400,000 SUVs turns out to not be huge. Yeah.

296
00:17:07,945 --> 00:17:10,765
Speaker 2:  And it's hurting a lot of companies because everyone who wants a hundred

297
00:17:10,965 --> 00:17:14,405
Speaker 2:  thousand dollars electric SUV already got one and there aren't very many

298
00:17:14,405 --> 00:17:14,605
Speaker 2:  of them.

299
00:17:14,605 --> 00:17:16,805
Speaker 5:  They're doing dumb stuff in their hummers. Yeah, exactly.

300
00:17:16,875 --> 00:17:17,365
Speaker 2:  They're

301
00:17:17,365 --> 00:17:21,325
Speaker 5:  Not gonna buy it. They are not gonna buy a bus. No. And the, the real market

302
00:17:21,345 --> 00:17:24,765
Speaker 5:  for cars is coming down, weighed it back down to Earth and to

303
00:17:24,995 --> 00:17:28,165
Speaker 5:  operate at scale, I think you do to sell a hundred thousand dollars car If

304
00:17:28,165 --> 00:17:31,165
Speaker 5:  you Apple, you do need it to drive itself. Yeah. So then you can't get it

305
00:17:31,165 --> 00:17:34,805
Speaker 5:  to drive itself and now you gotta make a $50,000 car or

306
00:17:34,805 --> 00:17:38,725
Speaker 5:  $70,000 car and now you are in just some of the most overheated

307
00:17:39,045 --> 00:17:42,685
Speaker 5:  territory that exists in cars. You're up against luxury, you're up against

308
00:17:42,805 --> 00:17:46,325
Speaker 5:  BMW and Mercedes and Audi like all those companies.

309
00:17:47,015 --> 00:17:50,605
Speaker 5:  Lexus, you got your own self-driving Lexuses already. Yeah. Like if you're

310
00:17:50,605 --> 00:17:54,365
Speaker 5:  Apple and you have a self-driving Lexus, you have to be like, can we make

311
00:17:54,365 --> 00:17:56,405
Speaker 5:  something better than the Lexus we put a camera on?

312
00:17:56,405 --> 00:17:57,085
Speaker 2:  Right. Did we just do it already?

313
00:17:57,595 --> 00:18:01,445
Speaker 5:  Yeah. You could just buy, apple could just buy Lexus's, put an

314
00:18:01,445 --> 00:18:05,405
Speaker 5:  Apple logo on them and resell the Lexus and Apple car. And that I think is

315
00:18:05,405 --> 00:18:09,285
Speaker 5:  like a real problem for them. Yeah. The thing that I

316
00:18:09,285 --> 00:18:12,125
Speaker 5:  keep coming back to that same moment 10 years ago,

317
00:18:13,555 --> 00:18:17,125
Speaker 5:  that is when like the height of their weird TV ambitions.

318
00:18:18,075 --> 00:18:21,205
Speaker 2:  Well it was, it was really when Johnny was feeling himself the most

319
00:18:21,985 --> 00:18:25,925
Speaker 2:  was like peak Johnny Ive, in terms of just having insane

320
00:18:26,005 --> 00:18:29,205
Speaker 2:  ideas about what everything should be. And some of those things were the

321
00:18:29,205 --> 00:18:32,645
Speaker 2:  butterfly keyboard and some of those things were the Apple watch. And it

322
00:18:32,645 --> 00:18:36,325
Speaker 2:  just like Apple had a real weird moment for a couple of years there because

323
00:18:36,325 --> 00:18:39,885
Speaker 2:  Johnny, ive was both kind of disengaging and just sort of

324
00:18:39,955 --> 00:18:43,645
Speaker 2:  lobbing insane ideas into Apple's design studio at all times.

325
00:18:43,645 --> 00:18:44,725
Speaker 5:  Well this is, I believe, and this

326
00:18:44,725 --> 00:18:45,725
Speaker 2:  Is the most expensive one I think,

327
00:18:46,085 --> 00:18:50,005
Speaker 5:  I believe this is has this moment has been described as when

328
00:18:50,005 --> 00:18:52,085
Speaker 5:  Apple thought it was a fashion company. Mm.

329
00:18:52,725 --> 00:18:53,525
Speaker 2:  Interesting. Right? Yeah.

330
00:18:53,525 --> 00:18:57,045
Speaker 5:  And they, they had hired a bunch of fashion executives and the sort of like

331
00:18:57,325 --> 00:19:00,125
Speaker 5:  computer nerds were like, you should make the MacBooks thicker and work well.

332
00:19:00,425 --> 00:19:03,885
Speaker 5:  And they're like, what if they had no ports? Right. We sold all the ports

333
00:19:03,885 --> 00:19:07,605
Speaker 5:  back to you with $39 a piece and the the keyboards didn't work.

334
00:19:07,745 --> 00:19:10,845
Speaker 5:  Oh, also we're gonna make the phone so thin that, that they have no battery

335
00:19:10,845 --> 00:19:14,765
Speaker 5:  life. Huh. And like that fashion conception of the

336
00:19:14,765 --> 00:19:17,205
Speaker 5:  company was, was real. Yeah. Like that was a real thing that happened at

337
00:19:17,205 --> 00:19:21,085
Speaker 5:  Apple. And this turn lately from the company is

338
00:19:21,085 --> 00:19:24,085
Speaker 5:  much more like we make computers. Like we make so much computers, we made

339
00:19:24,085 --> 00:19:27,685
Speaker 5:  a face computer. Like it's the most Yeah. There's nothing fashion

340
00:19:28,015 --> 00:19:31,685
Speaker 5:  about the Vision Pro except that it looks like a pair of expensive ski goggles.

341
00:19:31,685 --> 00:19:35,365
Speaker 5:  But I don't think that is fashion in the way that No, the 2015 Apple

342
00:19:35,365 --> 00:19:36,685
Speaker 5:  would've thought about it as fashion. Right.

343
00:19:37,185 --> 00:19:40,205
Speaker 2:  Hermes is not making one of those for you anytime soon. That

344
00:19:40,205 --> 00:19:44,165
Speaker 5:  Would be incredible. We did, there was a, a tweet today

345
00:19:45,185 --> 00:19:47,325
Speaker 5:  of a man who wore the Vision Pro to his wedding.

346
00:19:48,865 --> 00:19:52,205
Speaker 5:  Oh wow. The song showed it to me and she, and the, the bride is just like,

347
00:19:52,555 --> 00:19:53,165
Speaker 5:  what are you doing?

348
00:19:53,825 --> 00:19:55,645
Speaker 2:  That's love right there. That is love

349
00:19:56,025 --> 00:19:59,685
Speaker 5:  The look on her face. Wow. It is very, it is exactly what you would expect.

350
00:20:00,125 --> 00:20:00,325
Speaker 5:  If.

351
00:20:00,325 --> 00:20:02,965
Speaker 2:  You could read his vows though. They'd be right there right in front of you.

352
00:20:03,505 --> 00:20:05,085
Speaker 5:  Oh my God. That would be so that's what I'm

353
00:20:05,085 --> 00:20:07,685
Speaker 2:  Saying. You could have it on a window. Look at her, look at the vows. She

354
00:20:07,685 --> 00:20:08,965
Speaker 2:  can see her eyes, the googly eyes.

355
00:20:09,865 --> 00:20:13,805
Speaker 5:  I'm in on this. Don't do this. I don't know, don't do this. If you're

356
00:20:13,805 --> 00:20:16,885
Speaker 5:  listening to this, David is joking and he is proposing a bad idea.

357
00:20:17,585 --> 00:20:21,565
Speaker 5:  That's not an instruction. That 2015 Apple, the, the

358
00:20:21,565 --> 00:20:23,445
Speaker 5:  weird fashion Apple,

359
00:20:25,185 --> 00:20:28,965
Speaker 5:  the thing that was happening like right next in the company at the same time

360
00:20:29,545 --> 00:20:30,245
Speaker 5:  was they were,

361
00:20:31,835 --> 00:20:35,805
Speaker 5:  they were still trying to make good on this promise that Steve Jobs had

362
00:20:35,965 --> 00:20:39,645
Speaker 5:  figured out the tv. Right. And so like the thought experiment I've been thinking

363
00:20:39,645 --> 00:20:43,245
Speaker 5:  about since reading about the card getting shut down, it's like what if they

364
00:20:43,245 --> 00:20:44,005
Speaker 5:  just made a tv?

365
00:20:44,545 --> 00:20:47,805
Speaker 2:  Mm. What if they put all those resources at a television? Right?

366
00:20:47,805 --> 00:20:51,405
Speaker 5:  What if they were just like, it's a great TV and it has

367
00:20:51,725 --> 00:20:55,485
Speaker 5:  a built-in cable service and you're gonna pay us a monthly fee to have the

368
00:20:55,485 --> 00:20:59,085
Speaker 5:  sickest TV in your house. And like, and it

369
00:20:59,445 --> 00:21:02,765
Speaker 5:  probably would've taken all the market share from the high end of the TV

370
00:21:02,965 --> 00:21:05,925
Speaker 5:  industry 'cause that's what Apple does and they would've had a business like

371
00:21:05,965 --> 00:21:08,125
Speaker 5:  a real business selling a thing that people like.

372
00:21:08,125 --> 00:21:11,525
Speaker 2:  Well and ironically in the same way that the

373
00:21:11,525 --> 00:21:14,845
Speaker 2:  self-driving car world has not gone the direction Apple expected,

374
00:21:15,665 --> 00:21:19,565
Speaker 2:  the TV world has gone exactly the way you would want it to if

375
00:21:19,565 --> 00:21:23,045
Speaker 2:  you're Apple. Yeah. Like TVs are a services business. TVs are

376
00:21:23,395 --> 00:21:26,085
Speaker 2:  sort of slowly inching towards becoming the center of a smart home, which

377
00:21:26,085 --> 00:21:30,005
Speaker 2:  we know Apple is very interested in. TVs are are a hub device for lots of

378
00:21:30,005 --> 00:21:33,045
Speaker 2:  other devices. Like in a way it actually makes more sense today for Apple

379
00:21:33,065 --> 00:21:36,405
Speaker 2:  to try to build a television than it ever has in the past. Both for Apple's

380
00:21:36,525 --> 00:21:40,125
Speaker 2:  business and just like the world. Yeah. And it is really interesting to think

381
00:21:40,125 --> 00:21:43,925
Speaker 2:  like what if they had spent $10 million trying to build the

382
00:21:44,045 --> 00:21:47,725
Speaker 2:  greatest TV the world has ever known instead of trying to build a car that

383
00:21:47,965 --> 00:21:50,045
Speaker 2:  actually I don't think there was ever really gonna be any real demand for

384
00:21:50,045 --> 00:21:50,405
Speaker 2:  anyway.

385
00:21:51,275 --> 00:21:55,085
Speaker 5:  Well, so you know, the arguments against TV at that time were

386
00:21:55,985 --> 00:21:59,565
Speaker 5:  the TV upgrade cycles are too long and the margins are too low. Right.

387
00:22:00,065 --> 00:22:04,045
Speaker 5:  And it's like those are good arguments at the time now it's like,

388
00:22:04,185 --> 00:22:08,165
Speaker 5:  yo, do you know how long the iPad upgrade cycle is? 1000 years?

389
00:22:08,645 --> 00:22:12,005
Speaker 5:  Yeah. Like no one upgrades and they're like, I have an iPad, it does the

390
00:22:12,005 --> 00:22:13,925
Speaker 5:  three apps. I need Sometimes, I can circle things.

391
00:22:15,785 --> 00:22:16,845
Speaker 5:  I'm gonna have this iPad until I,

392
00:22:16,985 --> 00:22:19,845
Speaker 2:  My mom is finally hitting the point where some of her apps won't update on

393
00:22:19,845 --> 00:22:22,565
Speaker 2:  her iPad. So she's starting to think about it and I was like, okay, how old

394
00:22:22,565 --> 00:22:24,805
Speaker 2:  do you think it is? She was like, I don't know. Like it's probably like four

395
00:22:24,805 --> 00:22:26,685
Speaker 2:  years old, eight and a half years old, her

396
00:22:26,685 --> 00:22:26,965
Speaker 5:  IPad

397
00:22:27,665 --> 00:22:30,365
Speaker 2:  And it's just now to the point where she's like, it's been a little slow

398
00:22:30,365 --> 00:22:34,165
Speaker 2:  for a while but now there's actually a handful of things I can't do. And

399
00:22:34,165 --> 00:22:36,885
Speaker 2:  I was like, honestly the fact that you just made it eight and a half years

400
00:22:36,885 --> 00:22:39,125
Speaker 2:  without even really thinking about it on this device is pretty

401
00:22:39,235 --> 00:22:41,565
Speaker 5:  Yeah. That's a credit apple, right? Yeah, absolutely. They do the software

402
00:22:41,565 --> 00:22:45,325
Speaker 5:  updates, the, the things last like all credit to them, but that's very

403
00:22:45,325 --> 00:22:48,925
Speaker 5:  funny. Yeah. That they were so wor they, this is a reason not to do TV and

404
00:22:48,925 --> 00:22:51,285
Speaker 5:  now it's like people hold on their phones for a million years, they hold

405
00:22:51,285 --> 00:22:55,085
Speaker 5:  on iPads for and all of their revenue is services. Yep.

406
00:22:56,505 --> 00:22:59,645
Speaker 5:  And it's like, oh you could have just had that business this whole time.

407
00:22:59,645 --> 00:23:02,725
Speaker 5:  You could have had that business instead of allowing, I dunno, Roku to become

408
00:23:02,725 --> 00:23:06,085
Speaker 5:  a huge competitor. Right. Instead of allowing Amazon to be the number one

409
00:23:06,285 --> 00:23:08,485
Speaker 5:  provider of cheap TVs to everybody so they could,

410
00:23:08,485 --> 00:23:11,685
Speaker 2:  And the ad business on TV is booming, which Apple has been perpetually interested

411
00:23:11,685 --> 00:23:14,165
Speaker 2:  in building a real ad business. Like it's kind of, it's

412
00:23:14,165 --> 00:23:17,965
Speaker 5:  Also there, it's just very funny to me to be like they all the reasons

413
00:23:17,965 --> 00:23:21,805
Speaker 5:  they didn't do a TV were like worse reasons to not do something

414
00:23:21,805 --> 00:23:25,325
Speaker 5:  than all the reasons they should not have made a car. Like

415
00:23:25,625 --> 00:23:27,605
Speaker 5:  I'm sure people are gonna laugh at us that we started talking about the car

416
00:23:27,605 --> 00:23:30,205
Speaker 5:  and we ended up with Apple should make a tv, I think Apple should make a

417
00:23:30,205 --> 00:23:33,965
Speaker 5:  TV but they should have done it a million years ago. Right. In instead

418
00:23:34,085 --> 00:23:37,885
Speaker 5:  of this thing, which some of the reporting here is fascinating

419
00:23:38,315 --> 00:23:41,565
Speaker 5:  that one of the reasons they had the car project was because the engineers

420
00:23:41,565 --> 00:23:45,285
Speaker 5:  were bored after doing the watch wanted the next big thing and Tim Cook didn't

421
00:23:45,285 --> 00:23:48,405
Speaker 5:  want them to go to Tesla. Right. And it's like, well If, you got a bunch

422
00:23:48,405 --> 00:23:52,245
Speaker 5:  of engineers and you're like do a fake car, like you haven't accomplished

423
00:23:52,525 --> 00:23:52,605
Speaker 5:  anything.

424
00:23:53,495 --> 00:23:55,285
Speaker 2:  Build me a thing Johnny and I considered,

425
00:23:56,035 --> 00:23:59,605
Speaker 5:  It's like there's no, you literally now at this point have not accomplished

426
00:23:59,765 --> 00:24:03,525
Speaker 5:  anything. Right. But even at the time it's like

427
00:24:03,865 --> 00:24:07,445
Speaker 5:  all we all we're spending money and you not going to Tesla is a weird,

428
00:24:07,635 --> 00:24:11,605
Speaker 5:  there's a weird logic in there. Yeah. Because it's not like they

429
00:24:11,605 --> 00:24:15,565
Speaker 5:  were, they were working on the iPhone the whole time. Right. They were working

430
00:24:15,585 --> 00:24:17,205
Speaker 5:  on a car that wasn't shipping. Right.

431
00:24:17,355 --> 00:24:20,765
Speaker 2:  Well and I also, I I wonder how much the

432
00:24:21,315 --> 00:24:24,285
Speaker 2:  part of the story that is all these folks being moved to generative AI

433
00:24:25,265 --> 00:24:29,045
Speaker 2:  is actually about just sort of being smart about resources

434
00:24:29,145 --> 00:24:32,765
Speaker 2:  in a complicated time in the economy. I would believe that in

435
00:24:32,865 --> 00:24:36,765
Speaker 2:  almost every case that it actually makes more sense for Apple to

436
00:24:36,765 --> 00:24:40,205
Speaker 2:  have its highly paid employees working on a thing that

437
00:24:40,605 --> 00:24:43,085
Speaker 2:  everyone agrees is going to be a gigantic growth industry in a lot of ways

438
00:24:43,265 --> 00:24:46,005
Speaker 2:  in the next decade versus kind of continuing on this lark.

439
00:24:47,105 --> 00:24:50,765
Speaker 2:  But it's Apple. Like Apple can afford to do both those things at the same

440
00:24:50,765 --> 00:24:53,925
Speaker 2:  time if it wanted to. So it does mean that at some point there was someone

441
00:24:53,925 --> 00:24:56,005
Speaker 2:  within Apple who was just like, it's time to call this.

442
00:24:56,115 --> 00:24:57,885
Speaker 5:  Yeah, I think that person was Tim Cook. I

443
00:24:57,885 --> 00:24:58,165
Speaker 2:  Think you're

444
00:24:58,285 --> 00:25:01,805
Speaker 5:  Probably right. I think he is like, I'm no closer to this couch moving than

445
00:25:01,845 --> 00:25:02,525
Speaker 5:  I was before.

446
00:25:03,105 --> 00:25:06,285
Speaker 2:  And also Siri potentially worse than 10 years ago.

447
00:25:06,725 --> 00:25:10,485
Speaker 5:  Like, dude, I have put the chat GBT shortcut on my

448
00:25:10,485 --> 00:25:13,845
Speaker 5:  action button and it's just like very obvious what Apple should do with Siri.

449
00:25:14,315 --> 00:25:14,605
Speaker 5:  Yeah.

450
00:25:15,755 --> 00:25:17,245
Speaker 2:  Kill it and give it to chat GBT

451
00:25:18,075 --> 00:25:21,725
Speaker 5:  They should buy open ai. Like, I don't know. And Siri just now makes horrifying

452
00:25:21,725 --> 00:25:23,485
Speaker 5:  videos of people lighting things on fire.

453
00:25:25,075 --> 00:25:28,445
Speaker 5:  Like Siri, show me a picture of my family and it's like, which is,

454
00:25:28,895 --> 00:25:32,125
Speaker 5:  sorry, I, I just keep, I keep looking at videos generated by Soar up by Open

455
00:25:32,145 --> 00:25:35,885
Speaker 5:  AI and I'm like, this shouldn't threaten anyone because all of these videos

456
00:25:35,985 --> 00:25:39,805
Speaker 5:  are slightly terrifying. Yeah. You see the one of the guys

457
00:25:39,805 --> 00:25:43,765
Speaker 5:  smoking No. Where he like magics the cigarette up in his hand, it sort

458
00:25:43,765 --> 00:25:46,525
Speaker 5:  of like lights on fire and then it brings it to his mouth. That's badass.

459
00:25:46,525 --> 00:25:49,405
Speaker 5:  It's very good. I love that. I just wanna read this line from the Times piece,

460
00:25:49,405 --> 00:25:52,925
Speaker 5:  which again, it's full of just cutting aside effectively

461
00:25:53,065 --> 00:25:55,805
Speaker 2:  The story's really good. We'll we'll put it in the show notes. It's, everyone

462
00:25:55,805 --> 00:25:56,245
Speaker 2:  should read it

463
00:25:56,785 --> 00:25:57,805
Speaker 5:  But at the end of it,

464
00:25:59,405 --> 00:26:02,165
Speaker 5:  I mean just, I'm just gonna read it to you. Apple's dead car project will

465
00:26:02,165 --> 00:26:04,005
Speaker 5:  be survived by its underlying technologies.

466
00:26:06,275 --> 00:26:10,205
Speaker 5:  It's like, oh now you're just writing an obituary. Yeah. It's very good.

467
00:26:11,025 --> 00:26:12,285
Speaker 5:  The comedy plants, this is loved by many in

468
00:26:12,285 --> 00:26:12,645
Speaker 2:  The community.

469
00:26:13,285 --> 00:26:16,125
Speaker 5:  No, this is the auto cutting line in your interviews on Wednesday with the

470
00:26:16,125 --> 00:26:18,845
Speaker 5:  New York Times, people who worked on the project praised the decision to

471
00:26:18,845 --> 00:26:20,565
Speaker 5:  shutter it. That's bad.

472
00:26:21,105 --> 00:26:21,525
Speaker 2:  Brutal.

473
00:26:21,665 --> 00:26:25,645
Speaker 5:  That's bad. Yeah, it's a lot. I think it is In is

474
00:26:25,925 --> 00:26:29,005
Speaker 5:  fascinating to consider what if they built the other thing, it's fascinating

475
00:26:29,005 --> 00:26:32,165
Speaker 5:  to consider this decision in the context of deciding to launch the Vision

476
00:26:32,225 --> 00:26:36,045
Speaker 5:  Pro, which by all accounts, you know, I think

477
00:26:36,555 --> 00:26:38,845
Speaker 5:  some people have given it back. There's some reports on the return rates,

478
00:26:38,845 --> 00:26:42,765
Speaker 5:  there's some, but like people know what it is now. No more mystery, no

479
00:26:42,765 --> 00:26:46,645
Speaker 5:  more hype balloon. It's a VR headset that some people really like and some

480
00:26:46,645 --> 00:26:50,205
Speaker 5:  people do not. And they have a long way to go for that to become a mainstream

481
00:26:50,205 --> 00:26:54,125
Speaker 5:  project. And it's like you decided to launch this thing that kind of doesn't

482
00:26:54,125 --> 00:26:57,445
Speaker 5:  work just to learn something about what you need to do. Right.

483
00:26:57,945 --> 00:27:00,605
Speaker 5:  And you never shipped one mid-size crossover.

484
00:27:01,605 --> 00:27:04,125
Speaker 2:  I mean, I actually don't think that's that complicated. And we should move

485
00:27:04,125 --> 00:27:07,965
Speaker 2:  on from this. Yeah. This is, the Apple car is going to loom large

486
00:27:07,965 --> 00:27:11,125
Speaker 2:  in my life forever, so I'm sure we'll talk about it again. But I think with

487
00:27:11,185 --> 00:27:14,805
Speaker 2:  the Vision Pro, I see two big differences. One

488
00:27:15,145 --> 00:27:18,925
Speaker 2:  is that it's a thing Apple can just do, right?

489
00:27:18,925 --> 00:27:21,965
Speaker 2:  Like you can't just put a car on the road and see what happens.

490
00:27:22,695 --> 00:27:26,685
Speaker 2:  We've seen a bunch of that. It's gone real bad. Yeah. And Apple

491
00:27:27,305 --> 00:27:31,085
Speaker 2:  to its credit not interested in playing that kind of game. But it's also

492
00:27:32,165 --> 00:27:36,065
Speaker 2:  a lower stakes thing for Apple to try to

493
00:27:36,065 --> 00:27:39,745
Speaker 2:  figure out in so many ways. Right? It's, it's an ecosystem. Apple can control.

494
00:27:39,775 --> 00:27:43,745
Speaker 2:  It's a game, it can sort of play on its own, at its own pace. You don't have

495
00:27:43,745 --> 00:27:47,545
Speaker 2:  to be responsive to this gigantic car market that is moving

496
00:27:47,545 --> 00:27:51,265
Speaker 2:  in lots of directions all at the same time. Like I, I had the same thought

497
00:27:51,265 --> 00:27:54,905
Speaker 2:  that both of these things feel like unfinished experiments. And it's interesting

498
00:27:54,905 --> 00:27:58,145
Speaker 2:  that one launched and the other didn't. but it also seems like

499
00:27:58,925 --> 00:28:02,785
Speaker 2:  the experiment that is the Vision Pro is a lot easier to tolerate being a

500
00:28:02,785 --> 00:28:04,785
Speaker 2:  mess than a self drive self-driving car.

501
00:28:07,535 --> 00:28:11,385
Speaker 5:  Fair. Yeah. Fair. But I, I just think there's some part of Apple. It's very

502
00:28:11,385 --> 00:28:14,665
Speaker 5:  different, right? That's like, well let's see what happens. Right? Like that's

503
00:28:14,665 --> 00:28:18,585
Speaker 5:  not Apple. And I think with the Vision Pro there's like a, there's

504
00:28:18,585 --> 00:28:20,905
Speaker 5:  a gap between how the company's talking about that product as though it is

505
00:28:21,065 --> 00:28:23,945
Speaker 5:  finished in The future and the reality of that product, which is what is

506
00:28:23,945 --> 00:28:27,345
Speaker 5:  as good as it gets. Yeah. Agreed. Right? Like let's see what happens in like

507
00:28:27,345 --> 00:28:30,825
Speaker 5:  the Apple Watch. Let's see where this needs to go versus the, the car, which

508
00:28:30,825 --> 00:28:32,385
Speaker 5:  is just 10 years of where

509
00:28:32,385 --> 00:28:33,465
Speaker 2:  It needs to go is away

510
00:28:34,435 --> 00:28:37,785
Speaker 5:  Where needs to go. Alright, we should take a break. We're gonna come back.

511
00:28:37,785 --> 00:28:40,225
Speaker 5:  Warren Fine's gonna join us and we're gonna talk about what happened to the

512
00:28:40,225 --> 00:28:40,865
Speaker 5:  Supreme court this week.

513
00:31:43,635 --> 00:31:46,545
Speaker 5:  We're back. Lauren Feiner here. Welcome Lauren. Thanks.

514
00:31:46,595 --> 00:31:48,185
Speaker 1:  First time on The. Vergecast. Yeah.

515
00:31:48,295 --> 00:31:49,185
Speaker 12:  Yeah. Excited about it.

516
00:31:49,325 --> 00:31:52,985
Speaker 5:  Lauren is our brand new policy reporter here at The. Verge, very

517
00:31:53,265 --> 00:31:57,065
Speaker 5:  exciting because the policy news is off and running already

518
00:31:57,405 --> 00:32:01,345
Speaker 5:  in 2024 and it's election year. So welcome. Thank you. Nope, no pressure.

519
00:32:02,605 --> 00:32:05,505
Speaker 5:  And when I say the policy news is already up and going this week, there was

520
00:32:05,585 --> 00:32:09,305
Speaker 5:  a very big Supreme court hearing about two bills in Texas

521
00:32:09,845 --> 00:32:13,545
Speaker 5:  and Florida that purport to regulate how big platforms can

522
00:32:13,805 --> 00:32:17,625
Speaker 5:  do content moderation. I, the listeners are not gonna be

523
00:32:17,625 --> 00:32:20,345
Speaker 5:  surprised to know that. I think both of them are blatantly constitutional

524
00:32:20,705 --> 00:32:23,265
Speaker 5:  'cause they're, they're government speech regulations at their core and that's

525
00:32:23,265 --> 00:32:26,585
Speaker 5:  why they're in front of the Supreme court. Tell us what's going on with that

526
00:32:26,585 --> 00:32:28,225
Speaker 5:  hearing, how it got there where we are.

527
00:32:28,895 --> 00:32:32,465
Speaker 12:  Yeah, so these laws, each of them are a little bit different, but they're

528
00:32:32,465 --> 00:32:36,345
Speaker 12:  both essentially trying to get the tech platforms, you know,

529
00:32:36,345 --> 00:32:40,325
Speaker 12:  think of the major platforms, meta, Google, YouTube, Twitter,

530
00:32:40,355 --> 00:32:43,205
Speaker 12:  Twitter or now x to

531
00:32:44,245 --> 00:32:47,245
Speaker 12:  regulate, to enforce their policies in ways

532
00:32:48,435 --> 00:32:52,165
Speaker 12:  that would be consistent or not discriminate

533
00:32:52,305 --> 00:32:56,245
Speaker 12:  by viewpoint between different users. So they're basically trying

534
00:32:56,245 --> 00:33:00,045
Speaker 12:  to tell the platforms what they have to host on their sites.

535
00:33:00,345 --> 00:33:04,325
Speaker 2:  And the genesis of both of them is like the, the

536
00:33:04,325 --> 00:33:08,245
Speaker 2:  worries about conservative censorship on social media. Right? Is that, is

537
00:33:08,245 --> 00:33:09,285
Speaker 2:  that where both of these came from?

538
00:33:09,395 --> 00:33:13,005
Speaker 12:  Yeah, exactly. Okay. I mean they were enacted in 2021

539
00:33:13,055 --> 00:33:16,845
Speaker 12:  after Donald Trump notably was kicked off of

540
00:33:17,055 --> 00:33:20,325
Speaker 12:  major platforms. Yeah. After January 6th, so Oh,

541
00:33:20,325 --> 00:33:23,365
Speaker 2:  Okay. So it was literally a direct response to that. Yeah.

542
00:33:23,365 --> 00:33:27,045
Speaker 12:  Okay. Pretty direct. But you know, I think these censorship concerns have

543
00:33:27,045 --> 00:33:30,325
Speaker 12:  been existing in the Republican party for a while. So. I. Don't think that's

544
00:33:30,325 --> 00:33:33,605
Speaker 12:  necessarily the only thing, but definitely drove a lot more conversation

545
00:33:33,605 --> 00:33:34,005
Speaker 12:  around that.

546
00:33:34,315 --> 00:33:38,005
Speaker 2:  Okay. And this case seems like it was made to go to the Supreme court.

547
00:33:38,185 --> 00:33:41,205
Speaker 2:  Oh yeah. Right. Like this has kind of always been, you've been saying this

548
00:33:41,205 --> 00:33:44,165
Speaker 2:  sort of all along like it was destined Yeah. To be one of the ones,

549
00:33:44,785 --> 00:33:48,605
Speaker 5:  Not to get too into the procedural history here, but what you have

550
00:33:48,605 --> 00:33:52,405
Speaker 5:  is a law in Texas and a law in Florida. They're in different court

551
00:33:52,405 --> 00:33:56,245
Speaker 5:  systems, like they're in different circuits. So I think it was Texas

552
00:33:56,625 --> 00:34:00,285
Speaker 5:  got stayed and Florida got to go through and then the appeals courts

553
00:34:00,485 --> 00:34:01,725
Speaker 5:  disagreed and then that's how you get to the

554
00:34:01,985 --> 00:34:05,565
Speaker 2:  Oh the Supreme court. So it was like a, who's right about what this should

555
00:34:05,565 --> 00:34:05,885
Speaker 2:  be Yeah.

556
00:34:06,385 --> 00:34:08,365
Speaker 5:  I's in this, in this corner of the country this bill is going through and

557
00:34:08,365 --> 00:34:11,725
Speaker 5:  in this corner of the country, the court saying it's unconstitutional. You've

558
00:34:11,885 --> 00:34:14,365
Speaker 5:  got a circuit split, you've got a first Amendment problem.

559
00:34:16,165 --> 00:34:18,885
Speaker 5:  Clarence Thomas would you like a word? Like, and he's

560
00:34:18,885 --> 00:34:21,205
Speaker 2:  Been begging for this very kind of case for years

561
00:34:21,205 --> 00:34:25,045
Speaker 5:  And now he wants very badly. Yeah. So that actually I think is the,

562
00:34:25,705 --> 00:34:28,485
Speaker 5:  the piece of the puzzle I think is the most important. These are conservative

563
00:34:28,485 --> 00:34:32,245
Speaker 5:  states with the conservative we're governors, conservative attorney generals,

564
00:34:32,565 --> 00:34:35,805
Speaker 5:  they're mad that Donald Trump gets kicked off the platforms and in response

565
00:34:35,805 --> 00:34:39,565
Speaker 5:  they pass these laws that on their face are like, here are some rules we

566
00:34:39,565 --> 00:34:43,445
Speaker 5:  think the platforms should follow. And then on in the details are like,

567
00:34:43,445 --> 00:34:46,525
Speaker 5:  and here's how we would like you to moderate Facebook. Right, right. Which

568
00:34:46,525 --> 00:34:50,085
Speaker 5:  is just again, straightforwardly a government speech

569
00:34:50,085 --> 00:34:54,005
Speaker 5:  regulation. Right. But they're, it's conservatives making the government

570
00:34:54,105 --> 00:34:57,765
Speaker 5:  speech regulation. So in the run up to this, did anybody reckon with that?

571
00:34:58,485 --> 00:35:01,805
Speaker 5:  'cause usually the conservatives don't like a regulation

572
00:35:03,025 --> 00:35:05,885
Speaker 5:  is my understanding of conservative principles. I

573
00:35:05,885 --> 00:35:09,805
Speaker 12:  Mean you've got a lot of really strange alliances or you know, just

574
00:35:09,805 --> 00:35:13,245
Speaker 12:  seeing where people stand on this because even net choice has a lot of

575
00:35:13,245 --> 00:35:13,805
Speaker 12:  conservatives.

576
00:35:13,805 --> 00:35:15,725
Speaker 5:  Net choice is the, the lobbying firm.

577
00:35:15,955 --> 00:35:19,645
Speaker 12:  Yeah. Net Choice is the trade association that represents like pretty much

578
00:35:19,645 --> 00:35:23,445
Speaker 12:  all the major platforms and they're the ones that have brought

579
00:35:23,445 --> 00:35:27,325
Speaker 12:  this to the Supreme court essentially. And you know, they have

580
00:35:27,325 --> 00:35:31,205
Speaker 12:  conservatives high up in their organization and you know, they

581
00:35:31,205 --> 00:35:35,165
Speaker 12:  feel strongly that this shouldn't be law So I think where you see people

582
00:35:35,165 --> 00:35:38,805
Speaker 12:  netting out on this varies a lot. It's,

583
00:35:38,945 --> 00:35:42,565
Speaker 12:  you know, no longer something I think that we really see falling on like

584
00:35:42,565 --> 00:35:46,125
Speaker 12:  conservative or liberal. It's really based on, you know,

585
00:35:46,215 --> 00:35:50,205
Speaker 12:  other forms. Like is it more populist? Yeah. Or is it more just like

586
00:35:50,205 --> 00:35:51,325
Speaker 12:  traditional conservative.

587
00:35:51,825 --> 00:35:54,885
Speaker 5:  That's one of the things that I just think is utterly fascinating about this

588
00:35:55,585 --> 00:35:59,485
Speaker 5:  is these platforms are really big. They have the re they

589
00:35:59,485 --> 00:36:03,085
Speaker 5:  have the revenues of a country. Like they, I think

590
00:36:03,325 --> 00:36:07,165
Speaker 5:  Facebook has more revenue like by GDP than like some countries.

591
00:36:07,165 --> 00:36:10,285
Speaker 5:  Oh yeah, Google definitely does. Like these are just big, rich

592
00:36:11,125 --> 00:36:14,485
Speaker 5:  companies are global. It's hard to think about who or how you could regulate

593
00:36:14,485 --> 00:36:17,765
Speaker 5:  them. They don't have competition, which is weird

594
00:36:18,585 --> 00:36:22,125
Speaker 5:  and someone from YouTube is gonna like jump through the wall and be like

595
00:36:22,125 --> 00:36:25,845
Speaker 5:  TikTok, but like not really, you know? Right. And then in particular in the,

596
00:36:25,845 --> 00:36:28,845
Speaker 5:  in the sort of Donald Trump moment, they all took the same action

597
00:36:30,585 --> 00:36:34,485
Speaker 5:  And so it's like they're not even competing to have like we have the platform

598
00:36:34,485 --> 00:36:37,925
Speaker 5:  with the most Donald Trump. Like they all took the same kind of action

599
00:36:39,185 --> 00:36:41,885
Speaker 5:  and it's like who would regulate them? And it's very, it just, to me it's

600
00:36:41,885 --> 00:36:45,565
Speaker 5:  particularly strange that you have conservative states

601
00:36:46,185 --> 00:36:49,765
Speaker 5:  who are like, we're gonna just pass regulations about these platforms. And

602
00:36:49,765 --> 00:36:53,085
Speaker 5:  it, I think for most people that that's

603
00:36:53,185 --> 00:36:55,965
Speaker 5:  counterintuitive. But like you're saying there's other ways of perceiving

604
00:36:55,965 --> 00:36:59,485
Speaker 5:  these laws like far more populist obviously like

605
00:36:59,705 --> 00:37:02,565
Speaker 5:  hating on big tech companies is very popular in Washington DC

606
00:37:02,715 --> 00:37:06,125
Speaker 12:  Yeah. And I think you're actually seeing like another weird alignment on

607
00:37:06,125 --> 00:37:10,085
Speaker 12:  just like the competition question here because we've seen

608
00:37:10,115 --> 00:37:13,245
Speaker 12:  some like more usually more liberal like

609
00:37:14,395 --> 00:37:18,245
Speaker 12:  competition groups saying like, Hey, you know, we don't necessarily love

610
00:37:18,245 --> 00:37:21,725
Speaker 12:  these laws but we're concerned about them being struck down. And then what

611
00:37:21,725 --> 00:37:25,045
Speaker 12:  that means for just never being able to regulate the tech companies. Yes.

612
00:37:25,045 --> 00:37:28,285
Speaker 12:  Yeah. Interesting. Yeah. So I think you have like another weird alignment

613
00:37:28,445 --> 00:37:32,285
Speaker 12:  where you see like more liberal groups saying something like that and

614
00:37:32,435 --> 00:37:36,205
Speaker 12:  kind of taking the Republican ag side, even if they

615
00:37:36,205 --> 00:37:39,725
Speaker 12:  don't totally agree, but just being more fearful about what would happen

616
00:37:40,065 --> 00:37:42,645
Speaker 12:  if you know these laws are struck down and what would that means for The

617
00:37:42,645 --> 00:37:44,645
Speaker 12:  future of tech regulation in general. Yeah.

618
00:37:44,645 --> 00:37:47,245
Speaker 2:  There's been a lot of that over the last few years I think where yeah, where

619
00:37:47,295 --> 00:37:51,165
Speaker 2:  there are some people who are like, I want regulation but not this.

620
00:37:51,385 --> 00:37:55,245
Speaker 2:  And then others who are like, we can't overregulate but we have to

621
00:37:55,525 --> 00:37:56,885
Speaker 2:  regulate. It's like wait.

622
00:37:57,035 --> 00:37:57,765
Speaker 5:  Yeah, it's a lot.

623
00:37:57,885 --> 00:38:01,205
Speaker 2:  Everybody's trying to find this like maybe impossible

624
00:38:01,465 --> 00:38:04,485
Speaker 2:  middle ground that everyone is convinced exists. And you had a great quote

625
00:38:04,505 --> 00:38:07,885
Speaker 2:  in the piece you wrote leading up to the, the hearing on Monday

626
00:38:08,175 --> 00:38:11,765
Speaker 2:  where they were basically like, I've never seen the regulation that does

627
00:38:11,765 --> 00:38:15,045
Speaker 2:  what I think needs to be done, but I'm still convinced that it exists or

628
00:38:15,045 --> 00:38:18,405
Speaker 2:  is possible. Yeah. And I think we've now spent a long time

629
00:38:19,465 --> 00:38:22,125
Speaker 2:  trying to figure it out if it's possible. And it doesn't necessarily seem

630
00:38:22,125 --> 00:38:25,205
Speaker 2:  like we're any closer to it than we were of like this

631
00:38:25,715 --> 00:38:29,045
Speaker 2:  nuanced perfect thing that addresses the right things and not the wrong things.

632
00:38:29,585 --> 00:38:33,365
Speaker 2:  And a lot of what they talked about in this hearing was all the wrong things

633
00:38:33,365 --> 00:38:36,925
Speaker 2:  that this, these bills might address if they're put into law.

634
00:38:37,345 --> 00:38:40,325
Speaker 2:  And I don't know, it just all feels like every time we try to do this in

635
00:38:40,365 --> 00:38:43,125
Speaker 2:  a small way, it all becomes sort of the size of the universe because that's

636
00:38:43,125 --> 00:38:46,765
Speaker 2:  what happens on these platforms and everybody just kind of like

637
00:38:46,875 --> 00:38:49,925
Speaker 2:  runs and hides and no one knows who they're gonna end up with at the end

638
00:38:49,925 --> 00:38:51,365
Speaker 2:  of it. It's very strange. Yeah.

639
00:38:51,405 --> 00:38:54,885
Speaker 12:  I think that's what you saw a lot of the justices struggling with on Monday

640
00:38:55,105 --> 00:38:58,925
Speaker 12:  was just like, where does the, where does this end? Like if we

641
00:38:58,925 --> 00:39:01,925
Speaker 12:  strike down these laws, can we have no tech regulation then?

642
00:39:02,865 --> 00:39:06,685
Speaker 12:  And I think that's what concerned them about how these cases were

643
00:39:06,685 --> 00:39:10,205
Speaker 12:  brought to them is like, do we have to make like a huge sweeping

644
00:39:10,355 --> 00:39:14,325
Speaker 12:  statement on future regulation or can we like

645
00:39:14,325 --> 00:39:17,245
Speaker 12:  tailor this to certain companies and or like certain

646
00:39:18,095 --> 00:39:21,325
Speaker 12:  forms of companies that are really engaged in like expression.

647
00:39:21,715 --> 00:39:25,005
Speaker 2:  What was the vibe of the hearing? Like I think the, the Supreme court justices

648
00:39:25,005 --> 00:39:28,765
Speaker 2:  have been pretty honest over the years about not being the

649
00:39:28,965 --> 00:39:32,885
Speaker 2:  greatest at understanding technology. Like I think it was Elena Kagan

650
00:39:32,885 --> 00:39:35,565
Speaker 2:  who said like, we're not the nine foremost experts on the internet. Yes.

651
00:39:35,565 --> 00:39:36,205
Speaker 2:  Not that long ago.

652
00:39:37,265 --> 00:39:38,925
Speaker 5:  And I just wanna point out that that's bad.

653
00:39:39,155 --> 00:39:39,445
Speaker 2:  Yeah.

654
00:39:40,875 --> 00:39:44,685
Speaker 5:  Okay. Like it's like you should try harder get Gen Z on the

655
00:39:44,685 --> 00:39:44,885
Speaker 5:  Supreme

656
00:39:44,885 --> 00:39:45,885
Speaker 2:  Court now let's go,

657
00:39:46,245 --> 00:39:49,965
Speaker 5:  I don't know, like you're nine unelected weirdos. You get to decide everything

658
00:39:49,965 --> 00:39:52,965
Speaker 5:  you about American life. Figure out how to use your phone man. Yeah. Like,

659
00:39:53,005 --> 00:39:53,325
Speaker 5:  I dunno,

660
00:39:53,775 --> 00:39:57,525
Speaker 2:  Sorry. No, I agree. I'm with you. But there's still, and I think part of

661
00:39:57,825 --> 00:40:01,805
Speaker 2:  why we've seen the Supreme court not take these huge stances yet is because

662
00:40:02,425 --> 00:40:05,805
Speaker 2:  of that they seem sort of unwilling to make things Yeah.

663
00:40:05,805 --> 00:40:09,685
Speaker 5:  Make big SW gestures. Sorry I just don't by caveman lawyer like figure

664
00:40:09,685 --> 00:40:12,165
Speaker 5:  it out dude. Oh sure. No listen

665
00:40:12,435 --> 00:40:15,445
Speaker 2:  This is, if we litigate, is the Supreme court a good idea?

666
00:40:15,665 --> 00:40:19,205
Speaker 5:  How will we use our abundant power? Like shut up. I dunno. You're there.

667
00:40:19,265 --> 00:40:20,805
Speaker 5:  You're there for life. All right. Sorry.

668
00:40:20,985 --> 00:40:24,445
Speaker 2:  But what was it, what was it like in the room? Like actually kind of watching

669
00:40:24,445 --> 00:40:27,045
Speaker 2:  them ask these questions and try to figure this out in real time? What were

670
00:40:27,045 --> 00:40:27,445
Speaker 2:  the vibes like?

671
00:40:27,595 --> 00:40:31,405
Speaker 12:  Yeah, So I wasn't in the room directly but just listening to the arguments

672
00:40:31,565 --> 00:40:35,525
Speaker 12:  I think, and especially compared to last year when we had

673
00:40:35,725 --> 00:40:39,205
Speaker 12:  Gonzalez v Google, which was a case about section two 30 and that

674
00:40:39,205 --> 00:40:41,685
Speaker 2:  Which Supreme court ended up pretty much just punting on. Yeah. They're like,

675
00:40:41,685 --> 00:40:42,325
Speaker 2:  ah, we don't know.

676
00:40:42,355 --> 00:40:45,965
Speaker 12:  Yeah. And I think it was those arguments where Kagan said that quote that

677
00:40:45,965 --> 00:40:49,805
Speaker 12:  you said, okay. And you know those arguments you could tell they're

678
00:40:49,805 --> 00:40:53,125
Speaker 12:  really struggling with like how does the technology work and what are the

679
00:40:53,125 --> 00:40:56,845
Speaker 12:  implications? And so that was a question I had going in was like, are we

680
00:40:56,845 --> 00:41:00,285
Speaker 12:  gonna see the same thing this time? But what

681
00:41:01,255 --> 00:41:04,685
Speaker 12:  first Amendment experts I talked to before the hearing told me

682
00:41:04,995 --> 00:41:08,525
Speaker 12:  were basically like, you know, this is really a case about the first Amendment.

683
00:41:08,525 --> 00:41:11,765
Speaker 12:  Whereas like that was section two 30 that was a little bit more in the weeds.

684
00:41:12,195 --> 00:41:15,725
Speaker 12:  This court, they know the first Amendment, they're pretty strong on the

685
00:41:15,725 --> 00:41:18,965
Speaker 12:  first Amendment. So you know, I think there was a feeling that they would

686
00:41:18,985 --> 00:41:22,565
Speaker 12:  be on a little bit more stable ground there just like knowing what they

687
00:41:22,735 --> 00:41:25,485
Speaker 12:  would be talking about or the sorts of questions they should be asking.

688
00:41:26,425 --> 00:41:29,845
Speaker 12:  And yeah, it's about the technology and like how things work, but

689
00:41:30,515 --> 00:41:34,325
Speaker 12:  fundamentally it's a first Amendment question about, you know, can you regulate

690
00:41:34,565 --> 00:41:38,325
Speaker 12:  businesses around speech So I. Think we saw more of that

691
00:41:38,325 --> 00:41:41,445
Speaker 12:  confidence in this hearing compared to the Gonzalez arguments

692
00:41:42,335 --> 00:41:45,565
Speaker 12:  where you know, you were more able to see like they're asking questions

693
00:41:45,565 --> 00:41:49,365
Speaker 12:  that get to the heart of the first Amendment and yeah, So I,

694
00:41:49,445 --> 00:41:52,765
Speaker 12:  I think there was more of a sense that they knew what they were coming in

695
00:41:52,765 --> 00:41:56,605
Speaker 12:  here wanting to do, but at the same time I think they

696
00:41:56,835 --> 00:42:00,605
Speaker 12:  were struggling with figurine out how they were going

697
00:42:00,665 --> 00:42:03,605
Speaker 12:  to rule on this because the cases came to them

698
00:42:04,505 --> 00:42:08,485
Speaker 12:  as facial challenges. Meaning that net choices basically

699
00:42:08,485 --> 00:42:12,325
Speaker 12:  asking for kind of an all or nothing ruling

700
00:42:12,355 --> 00:42:16,165
Speaker 12:  here. Like can you just say like, this category of

701
00:42:16,305 --> 00:42:19,285
Speaker 12:  law is bad as opposed to saying like,

702
00:42:20,505 --> 00:42:24,325
Speaker 12:  you know, we think that this, these laws as applied to these

703
00:42:24,685 --> 00:42:28,525
Speaker 12:  companies or these circumstances should not be allowed.

704
00:42:28,715 --> 00:42:32,565
Speaker 12:  Okay. And that would give them a little bit more flexibility. So, I think

705
00:42:32,565 --> 00:42:36,245
Speaker 12:  when what you saw was the justices kind of prodding at

706
00:42:36,395 --> 00:42:40,245
Speaker 12:  what are the like outward bounds of what we could do here. And you know,

707
00:42:40,245 --> 00:42:43,925
Speaker 12:  we heard them asking about Etsy and Uber and it's like what do these companies

708
00:42:43,925 --> 00:42:47,845
Speaker 12:  have to do with anything? But I think that's where you see

709
00:42:47,845 --> 00:42:51,365
Speaker 12:  them trying to figure out like, is there a way to do this where we don't

710
00:42:51,365 --> 00:42:55,085
Speaker 12:  preclude any regulation of the tech companies and like maybe

711
00:42:55,225 --> 00:42:58,405
Speaker 12:  we focus in on the companies that really are engaged in speech.

712
00:42:58,635 --> 00:43:00,405
Speaker 2:  What did Etsy and Uber have to do with anything?

713
00:43:01,035 --> 00:43:04,765
Speaker 12:  Yeah, I mean I think basically they're saying like, you know, can this

714
00:43:04,775 --> 00:43:08,205
Speaker 12:  apply to like any tech company? Like, you know,

715
00:43:08,555 --> 00:43:12,045
Speaker 12:  Etsy has some speech in that, you know, you could leave comments on

716
00:43:13,005 --> 00:43:16,645
Speaker 12:  a, a seller, a seller's site or something like that. Or you know, Uber

717
00:43:16,825 --> 00:43:18,085
Speaker 12:  you can chat with the driver.

718
00:43:18,645 --> 00:43:22,045
Speaker 5:  I would just point out Etsy is one of the most dramatic websites in the entire,

719
00:43:22,155 --> 00:43:25,725
Speaker 5:  like it is constant moderation driver. Oh yeah. Like all the time.

720
00:43:26,765 --> 00:43:30,565
Speaker 5:  Casey Newton a Platformer and Zoe reported out that in

721
00:43:30,565 --> 00:43:34,485
Speaker 5:  the middle of the Israel Palestine war Etsy

722
00:43:34,485 --> 00:43:37,445
Speaker 5:  is like moderating whether or not you can sell stuff that says from the river

723
00:43:37,465 --> 00:43:41,085
Speaker 5:  to the sea on it. Mm. That's a straight content moderation. Right? Like

724
00:43:41,345 --> 00:43:45,125
Speaker 5:  should the government force a store to carry products

725
00:43:45,145 --> 00:43:48,965
Speaker 5:  it doesn't want to sell is like an open question. Right. So, I The justice

726
00:43:49,065 --> 00:43:53,005
Speaker 5:  is being like, what about Etsy is like, no dude. Like Etsy is f full of

727
00:43:53,025 --> 00:43:56,725
Speaker 5:  con it's full of user generated content that the company moderates. Yeah.

728
00:43:56,865 --> 00:44:00,805
Speaker 5:  And it's like, of course this touches Etsy and it, I, there is a

729
00:44:00,925 --> 00:44:04,885
Speaker 5:  a moment in this hearing where there was like some debate about what a

730
00:44:04,885 --> 00:44:08,245
Speaker 5:  website is and I had the reaction like, well this is why The Verge exists.

731
00:44:08,445 --> 00:44:11,365
Speaker 5:  Like the Supreme Court is like, what is a website? I was like, well here

732
00:44:11,365 --> 00:44:14,525
Speaker 5:  we go. That just comes Kool-Aid manning into the room. Like I'll tell you,

733
00:44:14,625 --> 00:44:17,365
Speaker 5:  our publication is founded the inside that knowing what a website is will

734
00:44:17,365 --> 00:44:20,245
Speaker 5:  be very important to The future of the world. Like here we are.

735
00:44:21,185 --> 00:44:24,605
Speaker 5:  And the lawyer for Net Choice was like, we can just set aside

736
00:44:25,055 --> 00:44:28,245
Speaker 5:  Gmail and these other problems you've raised. 'cause this law definitely

737
00:44:28,245 --> 00:44:30,885
Speaker 5:  covers websites. And I was like, Gmail's a website

738
00:44:32,205 --> 00:44:36,165
Speaker 5:  famously a website. Like that's how most people

739
00:44:36,165 --> 00:44:40,005
Speaker 5:  experience Gmail. And you would just have this like really

740
00:44:40,235 --> 00:44:41,165
Speaker 5:  fine sort of

741
00:44:42,715 --> 00:44:46,645
Speaker 5:  philosophical delineation of things that all feel like the same thing

742
00:44:47,305 --> 00:44:51,125
Speaker 5:  and like the Supreme court inventing these categories. And it felt to me

743
00:44:52,155 --> 00:44:56,085
Speaker 5:  like they wanted to find a category that was like, what if we just

744
00:44:56,325 --> 00:45:00,205
Speaker 5:  regulate Facebook? Right. What, what? And it kept, I believe they kept calling

745
00:45:00,205 --> 00:45:03,845
Speaker 5:  them the classic social media networks Mm. As though that was like a

746
00:45:04,205 --> 00:45:05,525
Speaker 5:  well-defined, it's like

747
00:45:05,525 --> 00:45:08,485
Speaker 2:  In the antitrust thing where all of these fights end up being about market

748
00:45:08,485 --> 00:45:12,365
Speaker 2:  definition. Yeah. It feels like with so much of this stuff, it's just

749
00:45:13,105 --> 00:45:16,245
Speaker 2:  how do you draw the boundaries around this thing that we can talk about

750
00:45:17,165 --> 00:45:20,445
Speaker 2:  Facebook and Reddit and x and

751
00:45:21,205 --> 00:45:25,165
Speaker 2:  a couple of other things without talking about any comment,

752
00:45:25,425 --> 00:45:29,205
Speaker 2:  anyone leaves anywhere on the internet and it just seems it's so leaky.

753
00:45:29,275 --> 00:45:32,605
Speaker 2:  Yeah. As you get down and it just gets, it just gets murkier and murkier

754
00:45:32,625 --> 00:45:35,845
Speaker 2:  as you get bigger. And I think the question of like, can the government tell

755
00:45:36,325 --> 00:45:39,845
Speaker 2:  Facebook how to moderate Facebook is again sort of on its face unconstitutional.

756
00:45:39,845 --> 00:45:40,125
Speaker 2:  Right.

757
00:45:40,125 --> 00:45:43,085
Speaker 5:  And that's by the way she Lauren said facial challenge. Literally that is

758
00:45:43,085 --> 00:45:46,725
Speaker 5:  what that refers to. Yeah. Like on its face is this law unconstitutional?

759
00:45:46,725 --> 00:45:50,605
Speaker 5:  Right. And it not as applied or like in this place the government

760
00:45:50,605 --> 00:45:54,205
Speaker 5:  use this law to do this thing that's bad, which is the justices I think we're

761
00:45:54,205 --> 00:45:57,405
Speaker 5:  kind of openly hoping that maybe they could just like kick it back to the

762
00:45:57,405 --> 00:46:00,365
Speaker 5:  court and like do some other stuff or the case we brought in some other way

763
00:46:00,615 --> 00:46:04,405
Speaker 5:  based on some thing that had actually been done as opposed to this big

764
00:46:04,565 --> 00:46:08,405
Speaker 5:  question of like speech regulations, good or bad. Because they

765
00:46:08,405 --> 00:46:10,165
Speaker 5:  kind of wanna be like sometimes good.

766
00:46:10,845 --> 00:46:14,405
Speaker 12:  Yeah. Right. Also, the courts stop these laws from going into effect so

767
00:46:14,405 --> 00:46:18,245
Speaker 12:  early on that there's not really a super developed record for the

768
00:46:18,245 --> 00:46:21,405
Speaker 12:  justices to look at or you know, to see in the real world like, well what

769
00:46:21,405 --> 00:46:24,725
Speaker 12:  sort of bad things happened under this law? What sort of good things happened?

770
00:46:24,965 --> 00:46:28,085
Speaker 12:  Interesting. So, you know, I think some of them were grasping at like, okay,

771
00:46:28,195 --> 00:46:30,845
Speaker 12:  whoa, what do we even do with this? Yeah. We don't really know how this

772
00:46:30,845 --> 00:46:31,365
Speaker 12:  will work out.

773
00:46:32,045 --> 00:46:35,965
Speaker 5:  We have some clips in the argument of various justices asking what

774
00:46:36,005 --> 00:46:39,845
Speaker 5:  I would call searching existential questions. This

775
00:46:39,845 --> 00:46:42,885
Speaker 5:  bookstore is, one is interesting, it's Amy Coney Barrett. What's she, what's

776
00:46:42,885 --> 00:46:43,685
Speaker 5:  she trying to ask here?

777
00:46:44,115 --> 00:46:47,685
Speaker 12:  Yeah, so Amy Coney Barrett was basically trying to get at like our,

778
00:46:48,585 --> 00:46:52,525
Speaker 12:  you know, If you are talking about how these

779
00:46:52,805 --> 00:46:56,645
Speaker 12:  platforms organize content. Isn't that just a judgment? Like

780
00:46:56,655 --> 00:46:59,965
Speaker 12:  isn't all organization judgment? So that's what she was trying to get at

781
00:46:59,965 --> 00:47:03,805
Speaker 12:  here and she was giving the example of a bookstore as kind of

782
00:47:03,805 --> 00:47:06,325
Speaker 12:  an analogy so we could listen to what she said there.

783
00:47:06,475 --> 00:47:07,765
Speaker 5:  Yeah. It's from the clip. They

784
00:47:07,765 --> 00:47:11,365
Speaker 13:  Have to present information to a consumer in some sort of

785
00:47:11,365 --> 00:47:15,045
Speaker 13:  organized way and that there's a limited enough amount of information that

786
00:47:15,045 --> 00:47:18,925
Speaker 13:  the, the consumer can absorb it. And don't all

787
00:47:18,925 --> 00:47:22,725
Speaker 13:  methods of organization reflect some kind of judgment? I mean, could you

788
00:47:22,725 --> 00:47:26,525
Speaker 13:  tell, could Florida enact a law telling bookstores that they have to put

789
00:47:26,525 --> 00:47:30,005
Speaker 13:  everything out by alphabetical order and that they can't organize or put

790
00:47:30,005 --> 00:47:32,885
Speaker 13:  some things closer to the front of the store that they think, you know,

791
00:47:32,885 --> 00:47:35,645
Speaker 13:  their customers will wanna buy

792
00:47:35,985 --> 00:47:39,805
Speaker 5:  So I just wanna point out it is very likely that the Ron DeSantis version

793
00:47:39,805 --> 00:47:41,165
Speaker 5:  of Florida passes along, I was

794
00:47:41,165 --> 00:47:41,845
Speaker 2:  About to say the same

795
00:47:41,855 --> 00:47:45,765
Speaker 5:  Thing. Like that is totally in character for 2024 Florida. Yeah.

796
00:47:46,185 --> 00:47:49,165
Speaker 2:  And eventually they'll ban all books that start with d like just for some

797
00:47:49,165 --> 00:47:49,765
Speaker 2:  reason. Yeah.

798
00:47:50,225 --> 00:47:54,045
Speaker 5:  No, it's Disney. Get 'em outta here. By the way, DeSantis

799
00:47:54,065 --> 00:47:58,005
Speaker 5:  now on the record saying our don't say gay bill one too far because too many

800
00:47:58,005 --> 00:48:00,765
Speaker 5:  books are getting banned. It's like, well that's what happens, dude. Yeah.

801
00:48:00,765 --> 00:48:04,045
Speaker 5:  What's, that's, that's why we don't ban books in America's just a thing that

802
00:48:04,045 --> 00:48:04,245
Speaker 5:  happens.

803
00:48:05,785 --> 00:48:09,365
Speaker 5:  So that to me right, is If. you have any kind of

804
00:48:09,365 --> 00:48:12,845
Speaker 5:  algorithmic If, you have a platform with tons and tons of content. You have

805
00:48:12,845 --> 00:48:16,685
Speaker 5:  any kind of algorithmic filtering you, you need, you have to have it. Otherwise

806
00:48:16,685 --> 00:48:20,445
Speaker 5:  there'll be a fire hose of everything and that will make your platform unusable.

807
00:48:21,025 --> 00:48:24,765
Speaker 5:  And so the, the distinction there that I think I struggle with, everybody

808
00:48:24,925 --> 00:48:28,245
Speaker 5:  struggle with is it makes sense to say you can't tell the bookstore owner

809
00:48:29,145 --> 00:48:32,885
Speaker 5:  how to organize the bookstore when you get to an algorithm saying,

810
00:48:32,885 --> 00:48:36,085
Speaker 5:  here's some stuff that we think you will engage with.

811
00:48:37,105 --> 00:48:40,565
Speaker 5:  Is that like an editorial, first Amendment decision? I think the argument

812
00:48:40,595 --> 00:48:44,565
Speaker 5:  some people make is like, that's not like that should at least be transparent

813
00:48:44,825 --> 00:48:48,245
Speaker 5:  or we should at least give people some control over it. Or these

814
00:48:48,685 --> 00:48:52,085
Speaker 5:  companies are so powerful that the government does have some interest in

815
00:48:52,085 --> 00:48:56,045
Speaker 5:  being like, here's how those should work. But I, I, I don't

816
00:48:56,045 --> 00:48:59,165
Speaker 5:  know, like, it seems like when we talk about this realignment,

817
00:49:00,025 --> 00:49:03,965
Speaker 5:  you, you mentioned liberal antitrust people, that's Tim Wu who's

818
00:49:03,965 --> 00:49:07,285
Speaker 5:  been on the show. He's the one who's like, these are just algorithms, they're

819
00:49:07,285 --> 00:49:10,925
Speaker 5:  not like a newspaper regulate away. Where did the court come down to that?

820
00:49:12,265 --> 00:49:16,005
Speaker 12:  You know, I think we don't know yet where they fall on it. I think,

821
00:49:16,105 --> 00:49:19,885
Speaker 12:  you know, through that questioning, I think we could infer

822
00:49:20,155 --> 00:49:23,405
Speaker 12:  that Amy Coney Barrett is a little bit skeptical of

823
00:49:23,985 --> 00:49:27,565
Speaker 12:  how organization is different from speech

824
00:49:27,585 --> 00:49:30,885
Speaker 12:  itself because she's saying is an organization a judgment?

825
00:49:32,385 --> 00:49:36,165
Speaker 12:  And you know, we saw Florida Solicitor General who was arguing to say like,

826
00:49:36,515 --> 00:49:39,925
Speaker 12:  well you know, the organization is really different here.

827
00:49:40,985 --> 00:49:44,365
Speaker 12:  And he later said, you know, this is, you know,

828
00:49:44,735 --> 00:49:48,565
Speaker 12:  we're not saying how platforms have to organize their content. But then

829
00:49:48,585 --> 00:49:52,565
Speaker 12:  in kind of the same fret is talking about how the law does talk about shadow

830
00:49:52,805 --> 00:49:55,725
Speaker 12:  banning, which is inherently organizing content.

831
00:49:56,405 --> 00:49:59,685
Speaker 2:  Right? Shadow Banning is just taking the book and putting it all the way

832
00:49:59,685 --> 00:50:00,525
Speaker 2:  at the back of the bookstore

833
00:50:00,825 --> 00:50:02,565
Speaker 5:  And importantly not telling the book. Right.

834
00:50:04,055 --> 00:50:07,925
Speaker 2:  Right. Well and I think to your point, one of the things that it seems like

835
00:50:08,545 --> 00:50:11,365
Speaker 2:  is being talked about kind of underneath all of this is that it seems like

836
00:50:11,365 --> 00:50:15,325
Speaker 2:  what these bills want is for it to be a lot more work

837
00:50:15,865 --> 00:50:19,525
Speaker 2:  to ban somebody or remove content. And that it should essentially have to

838
00:50:19,525 --> 00:50:22,685
Speaker 2:  be a person who presses the button and then fills out a form about why they

839
00:50:22,685 --> 00:50:26,645
Speaker 2:  press the button. And I think part of the outcome of that would

840
00:50:26,645 --> 00:50:29,660
Speaker 2:  very clearly be to make it harder to do that moderation. Just because you

841
00:50:29,660 --> 00:50:32,685
Speaker 2:  can't have an algorithm that just pulls stuff out. You essentially have to

842
00:50:33,005 --> 00:50:36,885
Speaker 2:  document every single time that happens. And also to make

843
00:50:36,885 --> 00:50:40,685
Speaker 2:  it so that it is more transparent but sort of more arduous

844
00:50:40,685 --> 00:50:43,685
Speaker 2:  in the name of being more transparent. So I think they wanna make it look

845
00:50:43,685 --> 00:50:47,565
Speaker 2:  less like a series of algorithms that can make these kinds of decisions and

846
00:50:47,595 --> 00:50:51,405
Speaker 2:  more like literally people curating a bookstore. And then I think you're

847
00:50:51,405 --> 00:50:54,285
Speaker 2:  having a very different conversation about what all of this looks like, but

848
00:50:54,285 --> 00:50:57,965
Speaker 2:  that's just so far away from how these systems work now. And I think one

849
00:50:57,965 --> 00:51:00,525
Speaker 2:  of the things that the tech companies have argued, right, is that this thing

850
00:51:00,525 --> 00:51:03,765
Speaker 2:  you're arguing for where there has to be like a person in a room writing

851
00:51:03,925 --> 00:51:07,285
Speaker 2:  a paragraph about every single post that we take off of Facebook is like

852
00:51:07,285 --> 00:51:10,965
Speaker 2:  preposterous and impossible, right? And so it just feels like I, I don't

853
00:51:10,965 --> 00:51:13,085
Speaker 2:  know, I don't know how to connect those ideas in my head that we're talking

854
00:51:13,085 --> 00:51:16,245
Speaker 2:  about algorithms, but we're also talking about a person who is like, let

855
00:51:16,245 --> 00:51:19,445
Speaker 2:  me tell you why I kicked your post off of Facebook because you are, you were

856
00:51:19,445 --> 00:51:20,845
Speaker 2:  mean to everybody. Yeah.

857
00:51:20,905 --> 00:51:24,845
Speaker 12:  The transparency thing, I think they didn't get into as much

858
00:51:24,985 --> 00:51:28,845
Speaker 12:  in these arguments as the, what are called the must carry

859
00:51:28,845 --> 00:51:32,685
Speaker 12:  provisions that platforms have to carry certain content. But

860
00:51:32,725 --> 00:51:36,365
Speaker 12:  I think that's definitely an important element of it too. And I think, you

861
00:51:36,365 --> 00:51:39,405
Speaker 12:  know, you, you could see why someone would wanna know if they're kicked

862
00:51:39,405 --> 00:51:43,285
Speaker 12:  off a platform, why that happened or you know, why something's not being

863
00:51:43,285 --> 00:51:47,165
Speaker 12:  shown to their followers as much. But I

864
00:51:47,165 --> 00:51:50,645
Speaker 12:  think the tech companies would say like that's a huge

865
00:51:51,475 --> 00:51:55,365
Speaker 12:  like resource suck. And also maybe it's gonna make it harder for

866
00:51:55,365 --> 00:51:59,205
Speaker 12:  newer entrants to do to do that because you know, yeah

867
00:51:59,485 --> 00:52:02,285
Speaker 12:  Facebook can tell everyone here's why you were kicked off our platform.

868
00:52:02,385 --> 00:52:05,845
Speaker 12:  But a newer platform is not really gonna have the resources to do that.

869
00:52:06,095 --> 00:52:08,765
Speaker 2:  Right? Yeah. And one of the arguments that has been made kind of around all

870
00:52:08,765 --> 00:52:12,245
Speaker 2:  of this is that any of these regulations just entrench the huge

871
00:52:12,525 --> 00:52:15,165
Speaker 2:  platforms that can afford to do this kind of moderation and take on the litigation

872
00:52:15,165 --> 00:52:18,965
Speaker 2:  risk and pay out the lawyers. Whereas If, you want to start a new

873
00:52:18,965 --> 00:52:22,245
Speaker 2:  thing, good luck, you're gonna get sued by everyone who doesn't like a post

874
00:52:22,245 --> 00:52:22,405
Speaker 2:  the

875
00:52:22,405 --> 00:52:25,685
Speaker 5:  Da. But this is the alignment problem again. But I don't wanna say alignment

876
00:52:25,685 --> 00:52:29,525
Speaker 5:  problem 'cause that's an AI phrase. This is the weird political like

877
00:52:29,715 --> 00:52:33,645
Speaker 5:  shuffle that's happening is like the conservatives are

878
00:52:33,645 --> 00:52:36,645
Speaker 5:  like, what if we had a bunch of regulated monopolies, right? So we'll have

879
00:52:36,805 --> 00:52:39,605
Speaker 5:  like four companies and they'll just like do what we say and then a bunch

880
00:52:39,605 --> 00:52:42,445
Speaker 5:  of liberal academics who are like, we should have more competition are like,

881
00:52:42,445 --> 00:52:46,325
Speaker 5:  what if we also did some speech regulations And it's like, what is

882
00:52:46,325 --> 00:52:46,605
Speaker 5:  happening?

883
00:52:46,845 --> 00:52:48,205
Speaker 2:  Do you hear yourselves? Yeah.

884
00:52:48,435 --> 00:52:52,125
Speaker 5:  Like wait, what is going on here? Like I don't, who are you people Have you,

885
00:52:52,125 --> 00:52:55,805
Speaker 5:  did you, did you all like literally wake up in the wrong house today?

886
00:52:55,875 --> 00:52:59,525
Speaker 5:  Like I don't understand what's going on. I think we heard a lot of

887
00:52:59,525 --> 00:53:03,125
Speaker 5:  echoes of that split on sort of the right flank of the court and it's a six

888
00:53:03,125 --> 00:53:06,445
Speaker 5:  three court. So it's like the whole court is a right flank right now, but

889
00:53:07,105 --> 00:53:10,925
Speaker 5:  you have far more conservative members on that majority and far

890
00:53:10,925 --> 00:53:14,325
Speaker 5:  less conservative members. And Sam

891
00:53:14,855 --> 00:53:18,485
Speaker 5:  Alito and Clarence Thomas were like, these are common carriers. They have

892
00:53:18,485 --> 00:53:22,365
Speaker 5:  to carry everything. They're Thomas kept on bringing up section two 30

893
00:53:22,365 --> 00:53:25,445
Speaker 5:  like over and over again 'cause he hates it so much. Like he was just like

894
00:53:25,605 --> 00:53:29,285
Speaker 5:  sputtering and everyone's like, that has nothing to do with this. And then

895
00:53:29,285 --> 00:53:32,725
Speaker 5:  you heard Brett Kavanaugh say that like to me like,

896
00:53:33,445 --> 00:53:36,965
Speaker 5:  you know, I'm old, like the funniest things that would like get say in the

897
00:53:36,965 --> 00:53:39,805
Speaker 5:  slash do forums when people would get banned on the slash dot forums. You're

898
00:53:39,805 --> 00:53:42,565
Speaker 5:  like, you're censoring me And somebody be like the government, only the government

899
00:53:42,565 --> 00:53:44,965
Speaker 5:  can censor you. You don't have any first Amendment rights. Like this is like

900
00:53:44,965 --> 00:53:48,685
Speaker 5:  old internet forum poster stuff. So Alito is saying,

901
00:53:48,895 --> 00:53:52,445
Speaker 5:  isn't content moderation just another word for censorship?

902
00:53:52,785 --> 00:53:55,725
Speaker 5:  And that's very Orwellian. Like, we've like made this fake name for this

903
00:53:55,925 --> 00:53:59,325
Speaker 5:  horrible thing and we have a clip of Kavanaugh arguing

904
00:53:59,955 --> 00:54:03,205
Speaker 5:  with Alito and saying, hold on, can we play that clip?

905
00:54:03,675 --> 00:54:07,245
Speaker 14:  Just wanna follow up on justice Alitos questions. And I think he

906
00:54:07,555 --> 00:54:11,325
Speaker 14:  asks a good thought provoking important question and

907
00:54:11,325 --> 00:54:15,085
Speaker 14:  used the term Orwellian. When I think of Orwellian, I think of the state,

908
00:54:15,625 --> 00:54:19,605
Speaker 14:  not the private sector, not private individuals. Maybe people have different

909
00:54:19,955 --> 00:54:23,925
Speaker 14:  conceptions of Orwellian, but the state taking over

910
00:54:24,135 --> 00:54:28,085
Speaker 14:  media like in some other countries and in to, we

911
00:54:28,085 --> 00:54:32,005
Speaker 14:  made clear, the court made clear that we don't wanna be that that

912
00:54:32,005 --> 00:54:32,285
Speaker 14:  country

913
00:54:33,265 --> 00:54:35,565
Speaker 2:  In Supreme court parlance that's such a sick bird

914
00:54:36,505 --> 00:54:40,365
Speaker 5:  Is like maybe you have a different definition of Orwellian. It's stupid and

915
00:54:40,365 --> 00:54:40,685
Speaker 5:  wrong.

916
00:54:42,195 --> 00:54:44,045
Speaker 5:  Have you actually read George Orwell, sir?

917
00:54:45,685 --> 00:54:49,365
Speaker 2:  I think starting an insult with so and so asked a very good,

918
00:54:49,365 --> 00:54:51,525
Speaker 2:  thoughtful question is amazing

919
00:54:52,785 --> 00:54:53,685
Speaker 5:  And it's your fellow

920
00:54:53,685 --> 00:54:57,565
Speaker 2:  Supreme fundamentally misunderstands everything about everything. Yeah, it's

921
00:54:57,565 --> 00:54:57,645
Speaker 2:  very,

922
00:54:58,425 --> 00:55:01,805
Speaker 5:  So that's right. There's a, that's weird. Like that's on the right and that

923
00:55:01,805 --> 00:55:05,125
Speaker 5:  you can see that split between should we use some speech regulations? Should

924
00:55:05,125 --> 00:55:08,965
Speaker 5:  we like dress them up or should we be the

925
00:55:08,965 --> 00:55:12,925
Speaker 5:  place with the first Amendment? Did you, do you see that like indicating

926
00:55:12,925 --> 00:55:13,525
Speaker 5:  how it might go?

927
00:55:15,075 --> 00:55:16,605
Speaker 12:  Yeah, I mean I think,

928
00:55:18,445 --> 00:55:22,165
Speaker 12:  I think that, you know, if you're seeing like Kavanaugh and Amy Coney Barrett

929
00:55:22,165 --> 00:55:25,845
Speaker 12:  asking these questions, you know, I think you're probably gonna see the

930
00:55:25,845 --> 00:55:29,565
Speaker 12:  liberal justices aligned in in some way on those kinds of things.

931
00:55:30,625 --> 00:55:34,485
Speaker 12:  you know, I think maybe we'll still see Alito and Clarence

932
00:55:34,605 --> 00:55:37,165
Speaker 12:  Thomas out on their own.

933
00:55:37,165 --> 00:55:41,125
Speaker 5:  Clarence Thomas is gonna issue a one page descent that just says, fuck Section

934
00:55:41,185 --> 00:55:45,085
Speaker 5:  two 30. Yeah. Sign Clarence. you know, just like in

935
00:55:45,185 --> 00:55:45,605
Speaker 5:  Crayon.

936
00:55:47,435 --> 00:55:50,765
Speaker 12:  Yeah, but I mean I think that's kind of, it's a somewhat good sign for net

937
00:55:50,765 --> 00:55:54,565
Speaker 12:  choice that, you know, they're seeing these alignments, but

938
00:55:54,565 --> 00:55:58,365
Speaker 12:  at the same time, you know, from analysis I've seen after

939
00:55:58,625 --> 00:56:02,605
Speaker 12:  the arguments, I think, you know, net choices is maybe not gonna get as

940
00:56:02,605 --> 00:56:06,205
Speaker 12:  sweeping of a decision as they had hoped for because the

941
00:56:06,205 --> 00:56:10,165
Speaker 12:  justices we're looking for like, you know, where can we regulate tech

942
00:56:10,405 --> 00:56:13,765
Speaker 12:  companies or where could we at least leave open the option that that could

943
00:56:13,765 --> 00:56:17,485
Speaker 12:  happen? And like, do we really have to rule in a way that

944
00:56:17,485 --> 00:56:20,885
Speaker 12:  says like any kind of law, like this is like for sure

945
00:56:21,065 --> 00:56:21,925
Speaker 12:  unconstitutional.

946
00:56:22,395 --> 00:56:26,085
Speaker 2:  Yeah. This might be just a stupid procedural question, but is this the kind

947
00:56:26,085 --> 00:56:29,525
Speaker 2:  of thing where net choice can come in and say we'd like

948
00:56:30,085 --> 00:56:33,965
Speaker 2:  a gigantic sweeping epic decision one way

949
00:56:33,965 --> 00:56:37,845
Speaker 2:  or the other? Like this is the moment gavel down on the internet and the

950
00:56:37,845 --> 00:56:41,725
Speaker 2:  Supreme court can say like, no, but we are gonna do this very tiny thing

951
00:56:41,725 --> 00:56:43,005
Speaker 2:  in the middle that way. That's pretty

952
00:56:43,005 --> 00:56:45,645
Speaker 5:  Much exactly what the Supreme court loves to do. That's their favorite move.

953
00:56:45,675 --> 00:56:48,445
Speaker 5:  Okay. By far. Like in the history of the Supreme court.

954
00:56:48,585 --> 00:56:51,365
Speaker 2:  But I mean in this case, especially like net choice is very loudly saying

955
00:56:51,595 --> 00:56:54,005
Speaker 2:  this is gigantic. And the Supreme Court can be like, shut up, we're still

956
00:56:54,005 --> 00:56:55,285
Speaker 2:  gonna do this tiny thing. They can still do that.

957
00:56:55,995 --> 00:56:58,285
Speaker 5:  Yeah. What, what ways do you think they could do that? Lauren? Yeah,

958
00:56:59,235 --> 00:57:03,045
Speaker 12:  From what I've seen it, it seems like they could probably just, you know,

959
00:57:03,285 --> 00:57:06,165
Speaker 12:  remand this to the lower courts and say like, you need to develop a fuller

960
00:57:06,165 --> 00:57:10,005
Speaker 12:  record or, you know, just kind of give more guidance on, you know, how

961
00:57:10,005 --> 00:57:13,845
Speaker 12:  you should be deciding this. So yeah, they can, they can throw it back to

962
00:57:13,845 --> 00:57:17,685
Speaker 12:  the lower courts and just, you know, kind of, or carve out some

963
00:57:17,685 --> 00:57:19,925
Speaker 12:  middle path of how they wanna decide this.

964
00:57:20,075 --> 00:57:23,645
Speaker 5:  Yeah. The justices did in the oral arguments, ask the lawyers for both sides

965
00:57:23,645 --> 00:57:27,085
Speaker 5:  several times. How would you write this opinion? Let's just straight up like,

966
00:57:27,085 --> 00:57:28,445
Speaker 5:  what, what would, what would you do?

967
00:57:29,185 --> 00:57:30,525
Speaker 2:  Here's my laptop. You do it

968
00:57:30,915 --> 00:57:34,725
Speaker 5:  Because it is such a fundamentally complicated thing, right? In

969
00:57:34,725 --> 00:57:38,285
Speaker 5:  particular two state attorneys general, when you're like, actually write

970
00:57:38,285 --> 00:57:41,845
Speaker 5:  the rule and enforce it, it's pretty easy to trip them up and the

971
00:57:41,845 --> 00:57:45,765
Speaker 5:  justices were able to trip them up over and over again. For example,

972
00:57:45,825 --> 00:57:48,885
Speaker 5:  let me come back to noted forum poster Brett Kavanaugh.

973
00:57:50,315 --> 00:57:53,165
Speaker 5:  That man is definitely like a poster in his area. Oh yeah. You can feel it.

974
00:57:53,305 --> 00:57:57,005
Speaker 5:  Oh yeah. There is a great daily beast headline that was like, God, help us,

975
00:57:57,005 --> 00:57:59,845
Speaker 5:  Brett Kavanaugh might save their first Amendment. Like that's, that's his

976
00:57:59,845 --> 00:58:02,965
Speaker 5:  vibe. Literally. He's reminding them

977
00:58:04,075 --> 00:58:07,205
Speaker 5:  that the first Amendment is about the government. Can we play that clip

978
00:58:07,585 --> 00:58:11,325
Speaker 14:  In your opening remarks? You said the design of the first Amendment

979
00:58:11,385 --> 00:58:14,765
Speaker 14:  is to prevent suppression of speech end quote.

980
00:58:15,385 --> 00:58:19,325
Speaker 14:  And you left out what I understand to be three key words in the first

981
00:58:19,325 --> 00:58:23,005
Speaker 14:  Amendment, or to describe the first Amendment by the

982
00:58:23,005 --> 00:58:26,765
Speaker 14:  government. Do you agree by the government is what the first

983
00:58:26,765 --> 00:58:29,125
Speaker 14:  Amendment is targeting? I,

984
00:58:29,165 --> 00:58:32,605
Speaker 15:  I do agree with that, your Honor, but I don't agree that there is no first

985
00:58:32,605 --> 00:58:36,485
Speaker 15:  Amendment interest in allowing the people's representatives to

986
00:58:36,485 --> 00:58:39,405
Speaker 15:  promote the free exchange of ideas.

987
00:58:39,595 --> 00:58:43,245
Speaker 2:  Another sick burn by Kavanaugh though, like seriously as, as like

988
00:58:43,245 --> 00:58:47,005
Speaker 2:  Supreme court shadiness goes like he's tearing it up. Yeah,

989
00:58:47,005 --> 00:58:50,565
Speaker 5:  Tearing it up. He's just like, what are you doing, man? And that's, again,

990
00:58:50,565 --> 00:58:54,485
Speaker 5:  that's on the right. Like I, I think you would expect these kinds of questions

991
00:58:54,485 --> 00:58:58,205
Speaker 5:  from the three liberal justices. They're not obviously as

992
00:58:58,435 --> 00:59:01,965
Speaker 5:  motivated by the banning of Donald Trump as some of the conservative

993
00:59:01,965 --> 00:59:04,725
Speaker 5:  justices. And then Kavanaugh is just like, hold up like

994
00:59:06,065 --> 00:59:09,085
Speaker 5:  by the government. Like that's the thing, right? Like, and this argument

995
00:59:09,105 --> 00:59:12,245
Speaker 5:  in response that he got, which is the people's representatives, like our

996
00:59:12,445 --> 00:59:15,925
Speaker 5:  lawmakers have some, they should pass laws that

997
00:59:16,255 --> 00:59:20,245
Speaker 5:  allow for the free exchange of ideas is like kind of a new argument.

998
00:59:20,545 --> 00:59:23,965
Speaker 5:  Mm. Right. Like the, the idea that we need government speech

999
00:59:23,965 --> 00:59:27,805
Speaker 5:  regulations so that Facebook doesn't suppress, like that's a

1000
00:59:27,875 --> 00:59:31,645
Speaker 5:  winding path of logic that

1001
00:59:32,005 --> 00:59:33,525
Speaker 5:  I think is like a hard sell.

1002
00:59:34,475 --> 00:59:38,445
Speaker 12:  Yeah. Yeah. I mean I think, you know, it seems like

1003
00:59:38,785 --> 00:59:42,525
Speaker 12:  no one has figured out how to do this in a clean way. How to pass

1004
00:59:43,185 --> 00:59:47,125
Speaker 12:  speech laws for the internet that don't run afoul of the first Amendment,

1005
00:59:47,425 --> 00:59:51,325
Speaker 12:  but do get the sort of outcomes that people want on the internet. Yeah.

1006
00:59:52,065 --> 00:59:54,805
Speaker 12:  So I. I mean, I don't really know what the answer will be if there is a

1007
00:59:54,805 --> 00:59:55,885
Speaker 12:  way to do that. Yeah.

1008
00:59:55,945 --> 00:59:56,165
Speaker 5:  But

1009
00:59:56,165 --> 01:00:00,045
Speaker 2:  This is such a new problem, right? I mean I think they, they talked a lot

1010
01:00:00,045 --> 01:00:03,565
Speaker 2:  about newspapers in this case. I think one of the, one of the big open questions

1011
01:00:03,625 --> 01:00:07,165
Speaker 2:  is can you regulate Facebook? Like you would regulate a newspaper?

1012
01:00:08,465 --> 01:00:12,285
Speaker 2:  And I don't know what the answer will be, but it seems like, no, for

1013
01:00:12,285 --> 01:00:16,005
Speaker 2:  lots of reasons, but one of them is that, like, it, imagine if every single

1014
01:00:16,005 --> 01:00:19,565
Speaker 2:  person on earth got the same newspaper, like maybe we would think about it

1015
01:00:19,565 --> 01:00:23,325
Speaker 2:  differently. And that is fundamentally what something like Facebook is.

1016
01:00:24,145 --> 01:00:27,645
Speaker 2:  And it, and, and, and I guess this is where my favorite quote from the whole

1017
01:00:27,645 --> 01:00:30,765
Speaker 2:  thing was, it, was it Alito that asked how much YouTube would weigh if it

1018
01:00:30,765 --> 01:00:31,525
Speaker 2:  was a newspaper? Oh,

1019
01:00:31,605 --> 01:00:32,205
Speaker 12:  I forget who it

1020
01:00:32,205 --> 01:00:35,645
Speaker 2:  Was. I think it was Alito, I could be wrong, but one of them asked if YouTube

1021
01:00:35,665 --> 01:00:39,285
Speaker 2:  was a newspaper, how much would it weigh? Which is on its face a totally

1022
01:00:39,425 --> 01:00:42,765
Speaker 2:  insane question, but actually kind of in context makes sense. Because the

1023
01:00:42,885 --> 01:00:46,765
Speaker 2:  question is like, can we talk about these new things in the way that we talked

1024
01:00:46,765 --> 01:00:50,685
Speaker 2:  about these old things? And I, I, I don't know the answer

1025
01:00:50,685 --> 01:00:52,605
Speaker 5:  By the way is 50,000 pounds, that's

1026
01:00:52,605 --> 01:00:53,325
Speaker 2:  How much YouTube would weigh.

1027
01:00:53,705 --> 01:00:57,605
Speaker 5:  The Washington Post literally has a headline, oh here's a headline. Here's

1028
01:00:57,605 --> 01:00:59,845
Speaker 5:  how much a YouTube newspaper would weigh Justice Alito.

1029
01:01:00,195 --> 01:01:00,485
Speaker 2:  That

1030
01:01:00,485 --> 01:01:04,325
Speaker 5:  Seems low. And they, they calculated it at 50,000 pounds. It seems

1031
01:01:04,345 --> 01:01:06,805
Speaker 5:  low 1 billion words of daily YouTube output.

1032
01:01:08,095 --> 01:01:10,645
Speaker 5:  50,000 pounds newspaper. That's the answer. I mean, fair

1033
01:01:10,645 --> 01:01:12,925
Speaker 2:  Enough. They asked an answer just as Alito,

1034
01:01:13,715 --> 01:01:17,685
Speaker 5:  It's a big newspaper, it's a long article. Like it's

1035
01:01:17,725 --> 01:01:18,485
Speaker 5:  a very long

1036
01:01:20,115 --> 01:01:21,365
Speaker 5:  Okay, continue.

1037
01:01:22,225 --> 01:01:26,085
Speaker 2:  But no, but I think that's, that's kind of been the central question of a

1038
01:01:26,085 --> 01:01:30,045
Speaker 2:  lot of these things, right? Is can we talk about these platforms in the

1039
01:01:30,045 --> 01:01:33,965
Speaker 2:  way that we've talked about anything else before them? And

1040
01:01:33,965 --> 01:01:36,965
Speaker 2:  nobody seems to know the answer to that. Even like what is the precedent

1041
01:01:36,965 --> 01:01:40,045
Speaker 2:  we're looking at here? Nobody quite knows.

1042
01:01:40,435 --> 01:01:44,085
Speaker 12:  Yeah. I I think that was kind of a question with the, you know, concept

1043
01:01:44,105 --> 01:01:48,085
Speaker 12:  of editorial discretion and you know, talking about a, a case that

1044
01:01:48,085 --> 01:01:52,045
Speaker 12:  came before that looked at the Miami Herald and can they be compelled to

1045
01:01:52,045 --> 01:01:52,965
Speaker 12:  print an op-ed?

1046
01:01:54,625 --> 01:01:58,245
Speaker 12:  And you know, on the one hand you could see the similarities

1047
01:01:58,425 --> 01:02:02,165
Speaker 12:  to the Miami Herald and Facebook on the other hand, you might think like,

1048
01:02:02,755 --> 01:02:06,525
Speaker 12:  does what Facebook's doing is that really editorial discretion? There's

1049
01:02:06,585 --> 01:02:10,445
Speaker 12:  so much information. Like they're not really parsing through

1050
01:02:10,505 --> 01:02:14,005
Speaker 12:  all of it. There's just certain things that are flagged or certain things

1051
01:02:14,005 --> 01:02:17,725
Speaker 12:  they're looking at. It's, you know, but does that constitute having like

1052
01:02:17,725 --> 01:02:19,445
Speaker 12:  an editorial vision? Yeah,

1053
01:02:19,665 --> 01:02:22,805
Speaker 5:  And I think we can confidently say that Facebook does not have an editorial

1054
01:02:22,805 --> 01:02:26,685
Speaker 5:  vision. It's just, just people yelling at each other in neighborhoods and

1055
01:02:26,885 --> 01:02:28,485
Speaker 5:  Facebook marketplace. Yeah, those, that's, those are two,

1056
01:02:28,545 --> 01:02:30,845
Speaker 2:  Two things but's how what Facebook is there for, right? Like that's not,

1057
01:02:31,195 --> 01:02:34,485
Speaker 2:  they, they all at various points have tried to do editorial things and it

1058
01:02:34,485 --> 01:02:36,285
Speaker 2:  mostly goes quite badly. Yeah.

1059
01:02:37,985 --> 01:02:40,445
Speaker 5:  We should wrap this up. We dunno what's gonna happen. We're expecting an

1060
01:02:40,445 --> 01:02:41,285
Speaker 5:  answer in June.

1061
01:02:41,635 --> 01:02:44,165
Speaker 12:  Yeah, probably in June or before then.

1062
01:02:44,545 --> 01:02:48,365
Speaker 5:  The one thing that I, that didn't come up that I, I hope someone

1063
01:02:48,365 --> 01:02:51,485
Speaker 5:  mentions to the Supreme court justices is that

1064
01:02:51,875 --> 01:02:55,165
Speaker 5:  there's actually quite a bit of competition. Like

1065
01:02:55,785 --> 01:02:59,765
Speaker 5:  Donald Trump owns a social network, his reaction to being kicked off

1066
01:02:59,765 --> 01:03:03,725
Speaker 5:  the platforms was to start his own platform. If, you

1067
01:03:03,725 --> 01:03:07,605
Speaker 5:  would like to hear from Donald Trump, you, his tweets are available to you.

1068
01:03:08,135 --> 01:03:11,405
Speaker 5:  Right. And like there are TikTok does exist and

1069
01:03:12,445 --> 01:03:15,205
Speaker 5:  whatever our government currently wants to think about TikTok, it certainly

1070
01:03:15,205 --> 01:03:19,125
Speaker 5:  exists right now. Yeah. And that to me is like, well If, you

1071
01:03:19,125 --> 01:03:23,085
Speaker 5:  want the platforms with looser rules. You,

1072
01:03:23,225 --> 01:03:26,845
Speaker 5:  you can go get them, they're available to you. And that should be,

1073
01:03:27,025 --> 01:03:30,565
Speaker 5:  to me it seems like that's the solution. And I think the thing that is probably

1074
01:03:30,885 --> 01:03:34,525
Speaker 5:  frustrating to a bunch of folks is that to build a healthy business with

1075
01:03:34,525 --> 01:03:38,085
Speaker 5:  happy customers, you usually have to ban the assholes.

1076
01:03:38,715 --> 01:03:42,565
Speaker 5:  Like that's just how it goes. Right? But I don't think YouTube is lacking

1077
01:03:42,565 --> 01:03:46,125
Speaker 5:  for conservative voices. I don't think Facebook is lacking for

1078
01:03:46,125 --> 01:03:49,605
Speaker 5:  conservative voices. But I, but it doesn't seem like,

1079
01:03:50,105 --> 01:03:53,685
Speaker 5:  it seems like the, it seems like our government at every level, the Supreme

1080
01:03:53,685 --> 01:03:57,605
Speaker 5:  Court, congress, whatever, all these hearings constantly forget that many

1081
01:03:57,605 --> 01:04:01,565
Speaker 5:  other platforms exist and sometimes they forget that anything but Twitter

1082
01:04:01,565 --> 01:04:04,925
Speaker 5:  exists, right? Yeah. They're like, ask Mark Zuckerberg about Twitter. Do

1083
01:04:04,925 --> 01:04:08,805
Speaker 5:  you see any hint of like a more expansive scope? Is that why they

1084
01:04:08,805 --> 01:04:11,445
Speaker 5:  were talking about Uber and Etsy so much? Or was that just confusion?

1085
01:04:12,165 --> 01:04:15,405
Speaker 12:  I think that's a little bit different, but I think on the question of competition,

1086
01:04:15,585 --> 01:04:19,165
Speaker 12:  you know, it makes me think about Ken Buck, the Republican representative

1087
01:04:19,645 --> 01:04:21,565
Speaker 12:  who's actually on his way out of Congress.

1088
01:04:23,105 --> 01:04:27,045
Speaker 12:  you know, he was a big champion of new antitrust laws

1089
01:04:27,225 --> 01:04:31,205
Speaker 12:  for the tech companies. And he did think about it as

1090
01:04:31,205 --> 01:04:35,125
Speaker 12:  like, you know what, I do care about censorship on the internet, but

1091
01:04:35,205 --> 01:04:38,765
Speaker 12:  I think the way to fix that is to have more competition on the internet

1092
01:04:38,765 --> 01:04:42,725
Speaker 12:  to have more platforms. And so he dealt, he looked

1093
01:04:42,725 --> 01:04:46,165
Speaker 12:  at that question from the lens of competition,

1094
01:04:46,785 --> 01:04:49,565
Speaker 12:  but you know, notably he's leaving Congress

1095
01:04:50,865 --> 01:04:54,165
Speaker 12:  and you know, his view isn't shared by

1096
01:04:55,015 --> 01:04:58,685
Speaker 12:  maybe the majority of Republicans. I think there is a contingent that does

1097
01:04:58,755 --> 01:05:02,685
Speaker 12:  feel like competition might be the answer, but there hasn't been

1098
01:05:02,685 --> 01:05:06,525
Speaker 12:  enough brought on board to make these antitrust reforms

1099
01:05:07,065 --> 01:05:10,365
Speaker 12:  be able to pass in the house or the Senate yet

1100
01:05:11,225 --> 01:05:14,925
Speaker 12:  So I think, you know, there is, there are like seedlings of that

1101
01:05:15,075 --> 01:05:18,325
Speaker 12:  thought, but I don't know how far that's gotten us yet.

1102
01:05:18,395 --> 01:05:21,925
Speaker 2:  Yeah, I mean it's, it's why all of the stuff happening simultaneously

1103
01:05:22,115 --> 01:05:25,445
Speaker 2:  with these cases and the antitrust cases is so interesting because they so

1104
01:05:25,765 --> 01:05:28,005
Speaker 2:  directly speak to each other. Yeah. For exactly what you're talking about.

1105
01:05:28,005 --> 01:05:31,645
Speaker 2:  Like the, the antitrust cases are about the competition that would

1106
01:05:31,645 --> 01:05:34,645
Speaker 2:  theoretically solve some of these problems. And by solving some of these

1107
01:05:35,445 --> 01:05:38,965
Speaker 2:  problems in other ways with moderation and things like that, we're actually

1108
01:05:39,165 --> 01:05:41,805
Speaker 2:  changing the way that the competitive landscape is going to look on the internet.

1109
01:05:41,905 --> 01:05:45,205
Speaker 2:  And so these things are just going to keep spinning around each other it

1110
01:05:45,205 --> 01:05:49,165
Speaker 2:  seems like for a long time until somebody plants some kind of

1111
01:05:49,605 --> 01:05:53,365
Speaker 2:  regulatory or you know, I don't know, legal flag. Yeah.

1112
01:05:54,025 --> 01:05:57,605
Speaker 2:  And then a lot of things are gonna start to fall out it seems like. And what

1113
01:05:57,605 --> 01:06:01,525
Speaker 2:  I wonder for you before we go is does this feel like it has

1114
01:06:01,565 --> 01:06:04,885
Speaker 2:  a chance to be the moment the Supreme court

1115
01:06:05,275 --> 01:06:09,205
Speaker 2:  sort of plants a flag in some way or another? I think Clarence

1116
01:06:09,205 --> 01:06:13,085
Speaker 2:  Thomas in particular has been waiting for his chance for

1117
01:06:13,145 --> 01:06:17,045
Speaker 2:  so long now to have strong, loud, aggressive feelings about this in

1118
01:06:17,445 --> 01:06:21,285
Speaker 2:  official paperwork. And I think there, there was some indication that this

1119
01:06:21,305 --> 01:06:25,165
Speaker 2:  had the chance to be a moment for the Supreme court to really

1120
01:06:25,195 --> 01:06:29,125
Speaker 2:  have a strong opinion where it has been so wishy-washy in

1121
01:06:29,125 --> 01:06:32,765
Speaker 2:  other cases coming out of the hearing on Monday. Do you have any sense leaning

1122
01:06:32,785 --> 01:06:35,325
Speaker 2:  one way or the other about whether this is gonna end up being kind of small

1123
01:06:35,425 --> 01:06:36,365
Speaker 2:  or big?

1124
01:06:37,245 --> 01:06:41,085
Speaker 12:  I think, you know, if they take some sort of middle route, I still think

1125
01:06:41,085 --> 01:06:44,765
Speaker 12:  even that will be pretty significant because anything that says, you know,

1126
01:06:45,295 --> 01:06:48,005
Speaker 12:  these kinds of tech platforms are allowed to

1127
01:06:49,825 --> 01:06:53,525
Speaker 12:  decide what they carry on their platforms, that's a big deal. Yeah. To have

1128
01:06:53,525 --> 01:06:57,445
Speaker 12:  that affirmed by the Supreme court if that's what happens. And you

1129
01:06:57,445 --> 01:07:00,085
Speaker 12:  know, at the same time if the court says, you know, we're gonna deal with

1130
01:07:00,085 --> 01:07:02,925
Speaker 12:  this little piece of it, but we're not gonna deal with these other things,

1131
01:07:03,725 --> 01:07:06,765
Speaker 12:  I think that's also a big deal because they're saying, Hey, we could still

1132
01:07:07,205 --> 01:07:10,365
Speaker 12:  regulate the platforms in these other ways, or we're at least leaving open

1133
01:07:10,665 --> 01:07:14,125
Speaker 12:  the possibility that there's a constitutional law here that could be created.

1134
01:07:14,355 --> 01:07:16,845
Speaker 12:  Yeah, that's right. And you know, we'll see that go up to the Supreme court

1135
01:07:16,845 --> 01:07:20,645
Speaker 12:  at some point too probably. But at least you know,

1136
01:07:21,145 --> 01:07:24,645
Speaker 12:  we have a better, we'll have a better sense of what's in the realm of

1137
01:07:24,645 --> 01:07:26,045
Speaker 12:  possibility or not. Yeah.

1138
01:07:26,045 --> 01:07:29,925
Speaker 2:  It does seem like we're due for one of these a year. Yeah. For the rest of

1139
01:07:29,925 --> 01:07:30,205
Speaker 2:  our lives.

1140
01:07:30,805 --> 01:07:34,765
Speaker 5:  She's Clarence Thomas being like, section two 30, get outta my face.

1141
01:07:34,785 --> 01:07:37,405
Speaker 5:  He hates it so much. She's given speeches about how much she hates it. Yeah.

1142
01:07:37,665 --> 01:07:40,845
Speaker 5:  And it's, oh it's actually like many things with Clarence Thomas. I'm unclear

1143
01:07:40,845 --> 01:07:44,685
Speaker 5:  why I would, that's not like a fully reasoned argument. Alright,

1144
01:07:46,095 --> 01:07:49,005
Speaker 5:  we'll be through 30 extra minutes of Duncan and Clarence Thomas in a bonus

1145
01:07:49,005 --> 01:07:52,845
Speaker 5:  episode. That'll be coming up. Totally kidding. We gotta take a break, but

1146
01:07:52,845 --> 01:07:55,285
Speaker 5:  Lauren's gonna stick around. We got a lightning round. We'll be right back.

1147
01:08:24,205 --> 01:08:24,525
Speaker 16:  safely.

1148
01:09:28,295 --> 01:09:32,245
Speaker 5:  We're back 30 minutes by CL toms.

1149
01:09:32,675 --> 01:09:35,765
Speaker 5:  Here we go. Does anybody ever call him Clary toms

1150
01:09:35,825 --> 01:09:39,725
Speaker 2:  Cl Toms I think that's the last thing you do when you call 'em cl toms.

1151
01:09:39,915 --> 01:09:43,005
Speaker 5:  Look, I am very proud to be American. I think one of the most American things

1152
01:09:43,005 --> 01:09:46,605
Speaker 5:  you can do is make relentless fun of the Supreme Court. They're a bunch of

1153
01:09:46,605 --> 01:09:47,565
Speaker 5:  weirdos and

1154
01:09:47,565 --> 01:09:49,525
Speaker 2:  They'll never see it because they don't know how to use their phones.

1155
01:09:50,235 --> 01:09:53,725
Speaker 5:  It's like, it doesn't matter who they are. Liberal, conservative. Just make

1156
01:09:53,725 --> 01:09:57,405
Speaker 5:  fun of them. Just like, what? What are you nerds doing? It's great.

1157
01:09:58,705 --> 01:10:01,805
Speaker 5:  That's my first Amendment, right? I love that. Bring it Clary.

1158
01:10:02,625 --> 01:10:03,245
Speaker 5:  All right. Like

1159
01:10:06,555 --> 01:10:09,205
Speaker 5:  vote Patel. That's my whole platform.

1160
01:10:10,565 --> 01:10:14,285
Speaker 5:  I will, I will roast these nerds every day of my term. Is your

1161
01:10:14,285 --> 01:10:16,245
Speaker 5:  president? It's beautiful. Vote Patel

1162
01:10:18,465 --> 01:10:19,445
Speaker 2:  The neg platform.

1163
01:10:21,985 --> 01:10:24,565
Speaker 5:  It was, you should go listen the audio of the hearing. It's, it's good. It's

1164
01:10:24,565 --> 01:10:28,525
Speaker 5:  important. I just think it's funny to make fun of nerd. Okay, cut

1165
01:10:28,525 --> 01:10:29,045
Speaker 5:  that part out.

1166
01:10:30,945 --> 01:10:33,205
Speaker 5:  All right. Lightning round. There's actually a lot in this lightning round.

1167
01:10:33,205 --> 01:10:35,005
Speaker 5:  Yeah. Lauren, you're our guest. Do you wanna go first?

1168
01:10:35,475 --> 01:10:39,205
Speaker 12:  Sure. Yeah. So I chose the story about

1169
01:10:39,685 --> 01:10:43,205
Speaker 12:  Google, CEO, saying that the Gemini AI

1170
01:10:43,205 --> 01:10:47,005
Speaker 12:  diversity mishap was completely unacceptable.

1171
01:10:48,145 --> 01:10:51,965
Speaker 12:  So I thought this was an interesting story because it's basically

1172
01:10:51,965 --> 01:10:55,285
Speaker 12:  If, you hadn't seen Google's Gemini

1173
01:10:55,515 --> 01:10:59,325
Speaker 12:  generated racially diverse Nazis. Yeah. Which very good is

1174
01:10:59,675 --> 01:11:01,765
Speaker 12:  kind of offensive for strange reasons.

1175
01:11:03,685 --> 01:11:07,285
Speaker 2:  I just wanna say, we talked about this on last week's show for like 45 minutes

1176
01:11:07,345 --> 01:11:10,085
Speaker 2:  and that is such a better summation than anything else we can,

1177
01:11:11,055 --> 01:11:12,445
Speaker 2:  which is strange for various

1178
01:11:12,445 --> 01:11:12,885
Speaker 5:  Reasons.

1179
01:11:14,915 --> 01:11:18,765
Speaker 5:  It's so good. It's, I'll say it is, it's also, it's okay

1180
01:11:18,825 --> 01:11:22,725
Speaker 5:  to point out. It is hilarious. Oh yeah, yeah. It is the funniest shit

1181
01:11:22,725 --> 01:11:23,325
Speaker 5:  in the entire world.

1182
01:11:23,425 --> 01:11:26,805
Speaker 2:  It, the fact that it happened is funny. The response was funny. Yeah. The

1183
01:11:26,805 --> 01:11:30,045
Speaker 2:  fact that it went all the way up to Sundar Phai having to like send a long

1184
01:11:30,405 --> 01:11:31,365
Speaker 2:  internal memo about it.

1185
01:11:31,965 --> 01:11:34,885
Speaker 5:  Thompson sent out a newsletter that's like, Sundar should be fired. Yeah.

1186
01:11:34,885 --> 01:11:38,845
Speaker 5:  Because of the black pope that he made is like, dude, everybody

1187
01:11:38,915 --> 01:11:42,645
Speaker 5:  just like, just turn the knobs down. Yeah. All of it's funny.

1188
01:11:42,835 --> 01:11:46,605
Speaker 5:  Like open some very smart people being the dumbest

1189
01:11:47,025 --> 01:11:48,365
Speaker 5:  and carry on. Yeah.

1190
01:11:48,445 --> 01:11:52,285
Speaker 12:  I mean I think it is, it's a amusing way that

1191
01:11:52,285 --> 01:11:55,725
Speaker 12:  this happened. It also could have happened in a way less amusing way. So

1192
01:11:55,725 --> 01:11:58,925
Speaker 12:  I think we're fortunate that it's something that's funny to talk about,

1193
01:11:59,025 --> 01:12:02,845
Speaker 12:  but I think Sundar Chi is clearly seeing this as like, you know,

1194
01:12:02,875 --> 01:12:06,045
Speaker 12:  this could have been way worse. And like, we can't keep having these errors

1195
01:12:06,045 --> 01:12:09,925
Speaker 12:  and it just reminds me how like, you know, OpenAI releasing

1196
01:12:09,925 --> 01:12:13,445
Speaker 12:  chat GBT when it did just like spurred all of these

1197
01:12:13,765 --> 01:12:17,165
Speaker 12:  companies like racing to put out their own, you know,

1198
01:12:17,165 --> 01:12:21,045
Speaker 12:  competitors. And like, here we are with this, like kind of,

1199
01:12:21,285 --> 01:12:24,285
Speaker 12:  I don't know, maybe this was a product of being rushed and you know, having

1200
01:12:24,285 --> 01:12:28,125
Speaker 12:  something really strange and like uncomfortable happening as a

1201
01:12:28,125 --> 01:12:31,605
Speaker 12:  result. And you know, maybe this is some kind of reset for Google thinking.

1202
01:12:31,675 --> 01:12:32,565
Speaker 12:  Yeah. I mean, yeah.

1203
01:12:32,865 --> 01:12:36,445
Speaker 2:  It, do you remember that phase where Mark Zuckerberg was just

1204
01:12:36,605 --> 01:12:39,845
Speaker 2:  reflexively apologizing for everything? Yeah. There, there was like a two

1205
01:12:39,845 --> 01:12:42,845
Speaker 2:  year phase where anytime anything happened on the internet, Zuck would like

1206
01:12:42,845 --> 01:12:46,765
Speaker 2:  do a video in which he like very earnestly apologized. That to me feels like

1207
01:12:46,765 --> 01:12:50,365
Speaker 2:  the, the Sundar Phai era we're heading into right now. Where he's just like,

1208
01:12:50,565 --> 01:12:54,445
Speaker 2:  whatever is happening. I'm so sorry. We'll, we're doing our best. I

1209
01:12:54,445 --> 01:12:58,325
Speaker 2:  don't, I don't understand how we got here or what weird thing we

1210
01:12:58,325 --> 01:13:01,205
Speaker 2:  put into Gemini to make it do this, but I'm so sorry. No, they know and we'll

1211
01:13:01,205 --> 01:13:01,685
Speaker 2:  do better. They

1212
01:13:01,685 --> 01:13:05,645
Speaker 5:  Were like, robot be less racist. Yeah. And the robot was like, so

1213
01:13:05,665 --> 01:13:09,125
Speaker 5:  not racist that we're racist again. Like that's what we're doing.

1214
01:13:10,565 --> 01:13:14,405
Speaker 5:  I mean, this is, to me, every part of this is funny. So, right,

1215
01:13:14,405 --> 01:13:18,165
Speaker 5:  like Google famously did not release its chat bots

1216
01:13:18,165 --> 01:13:21,965
Speaker 5:  because they were afraid of bad outcomes and misalignment and bad things

1217
01:13:21,965 --> 01:13:25,925
Speaker 5:  happening. Open AI, as you point out, released it, Google gets kicked in

1218
01:13:25,925 --> 01:13:28,725
Speaker 5:  the butt, they start releasing things left and right And then you have an

1219
01:13:28,725 --> 01:13:32,245
Speaker 5:  entire move of people called effective Accelerationist who are like,

1220
01:13:32,545 --> 01:13:36,125
Speaker 5:  no rules, like all the ai, we can like do it. And they're the people who

1221
01:13:36,125 --> 01:13:39,125
Speaker 5:  are like, Google went too fast here. Their AI is too woke. And it's like,

1222
01:13:39,195 --> 01:13:40,325
Speaker 5:  this is the thing you want,

1223
01:13:42,145 --> 01:13:45,645
Speaker 5:  the thing you want is to just like, but not like that put like, but not with

1224
01:13:45,645 --> 01:13:47,805
Speaker 5:  so many black people. And it's like, you guys should shut up.

1225
01:13:49,995 --> 01:13:53,485
Speaker 5:  Like you should stop saying what you're saying. It's very revealing

1226
01:13:53,595 --> 01:13:56,885
Speaker 5:  because the problem with the other AI is it only made white people and you

1227
01:13:56,885 --> 01:13:57,645
Speaker 5:  were cool with that.

1228
01:13:59,505 --> 01:14:00,805
Speaker 5:  And now it's like, well we, because it's

1229
01:14:00,925 --> 01:14:01,605
Speaker 2:  Progress or something.

1230
01:14:01,605 --> 01:14:04,965
Speaker 5:  Now we, now you're like, the AI that only makes black people is like that.

1231
01:14:05,035 --> 01:14:08,725
Speaker 5:  It's like, you should shut up. Like yeah, you should, you should point out

1232
01:14:08,725 --> 01:14:12,525
Speaker 5:  to Google that a that like a racially diverse picture of

1233
01:14:12,735 --> 01:14:16,085
Speaker 5:  Nazis is very funny and probably not the intended outcome. And then you should

1234
01:14:16,085 --> 01:14:17,965
Speaker 5:  stop talking about how mad you are about it.

1235
01:14:19,925 --> 01:14:21,845
Speaker 5:  I just, I just think you should be quiet.

1236
01:14:22,225 --> 01:14:26,005
Speaker 2:  And also no one is out here with any actual good ideas about what is

1237
01:14:26,165 --> 01:14:28,725
Speaker 2:  supposed to happen in any of these cases. Yeah. They're like, I wanted to

1238
01:14:28,725 --> 01:14:32,685
Speaker 2:  be perfectly historically accurate and also help me dream up

1239
01:14:32,685 --> 01:14:35,565
Speaker 2:  new things and create things that no human could ever come up with. And it's

1240
01:14:35,565 --> 01:14:36,325
Speaker 2:  like, well what? Yeah.

1241
01:14:36,925 --> 01:14:40,005
Speaker 12:  I dunno. It kind of just makes you think like, okay, maybe this is a really

1242
01:14:40,215 --> 01:14:44,085
Speaker 12:  human thing. Like how do you program, like we want diversity, but not

1243
01:14:44,465 --> 01:14:48,245
Speaker 12:  in like historically accurate depictions of Nazis.

1244
01:14:48,245 --> 01:14:48,485
Speaker 12:  Like,

1245
01:14:48,625 --> 01:14:51,045
Speaker 5:  Do you think there's a whiteboard at Google that's just like labeled the

1246
01:14:51,205 --> 01:14:54,445
Speaker 5:  Hamilton conundrum? you know, it's like, should

1247
01:14:54,955 --> 01:14:58,485
Speaker 5:  Lynn Manel Miranda be able to use shed menno to make

1248
01:14:58,685 --> 01:14:59,925
Speaker 5:  Hamilton too? Well, do you

1249
01:15:00,005 --> 01:15:02,845
Speaker 2:  Remember that drill tweet from a million years ago where he's like turning

1250
01:15:02,925 --> 01:15:06,485
Speaker 2:  a huge dial that says racism and looking at and looking at the crowd like

1251
01:15:06,585 --> 01:15:09,445
Speaker 2:  I'm a contestant on the prizes. Right? Like, that's what Suit our vagina

1252
01:15:09,465 --> 01:15:09,885
Speaker 2:  is doing.

1253
01:15:10,955 --> 01:15:14,845
Speaker 5:  It's very good. That's where we are. Tr truly every part

1254
01:15:14,845 --> 01:15:18,725
Speaker 5:  of this is funny. I can't get, and I just, I'm, I I

1255
01:15:18,785 --> 01:15:21,845
Speaker 5:  beg you, if you're like, what I'm mad about is the computer made drawings

1256
01:15:21,845 --> 01:15:25,605
Speaker 5:  of black people just shut up. I it will go better for you

1257
01:15:25,755 --> 01:15:26,485
Speaker 5:  over time. Yeah.

1258
01:15:26,715 --> 01:15:30,085
Speaker 2:  Well, and also to your point about Google being forced to catch up on all

1259
01:15:30,085 --> 01:15:33,765
Speaker 2:  of this stuff, you've got to assume Sundar Phai is also sitting in meetings

1260
01:15:33,765 --> 01:15:37,405
Speaker 2:  going, this is why we did this slowly. Yeah. You idiots like

1261
01:15:37,795 --> 01:15:41,765
Speaker 2:  this. Google has had this technology forever. Like,

1262
01:15:42,065 --> 01:15:46,005
Speaker 2:  and Google loves to remind you that like, oh, what's the T in GPT? Like

1263
01:15:46,065 --> 01:15:50,005
Speaker 2:  we did that. That's, that's a Google thing that we did. It's Transformers.

1264
01:15:50,065 --> 01:15:53,845
Speaker 2:  We made them and Google was taking it very slowly and then

1265
01:15:54,545 --> 01:15:58,485
Speaker 2:  OpenAI just like yoed its way onto the internet. And now here

1266
01:15:58,485 --> 01:16:02,445
Speaker 2:  we are. And so like, and like Mark Zuckerberg made the heel

1267
01:16:02,525 --> 01:16:05,845
Speaker 2:  turn to being like, I don't care. I just make glasses now. Like Sundar's

1268
01:16:05,845 --> 01:16:09,365
Speaker 2:  next move is just to be like, yeah, morons, this is what she wanted. Welcome

1269
01:16:09,385 --> 01:16:10,365
Speaker 2:  to the world you asked for.

1270
01:16:10,665 --> 01:16:14,645
Speaker 5:  It might be amazing. It might be the best move Sundar ever did. That's

1271
01:16:14,645 --> 01:16:17,365
Speaker 5:  what I'm saying. Here's what you wanted Morons. Yeah. But totally out of

1272
01:16:17,365 --> 01:16:20,925
Speaker 5:  character. I can't even imagine him saying the word morons.

1273
01:16:21,825 --> 01:16:25,605
Speaker 5:  I'm gonna go get it like an AI voice cleaner. It's just, it,

1274
01:16:25,605 --> 01:16:28,685
Speaker 5:  it'll just be sundari issuing like devastating burns to people.

1275
01:16:30,595 --> 01:16:33,205
Speaker 5:  He's a very polite man. Yes. We're working to get him on decoder and I hope

1276
01:16:33,205 --> 01:16:36,845
Speaker 5:  that we didn't just kill that. We'll see what we will see what happens. I'll,

1277
01:16:37,195 --> 01:16:39,765
Speaker 5:  that whole episode of decoder will be like, will you say the word morons

1278
01:16:39,765 --> 01:16:39,925
Speaker 5:  to me?

1279
01:16:40,755 --> 01:16:43,765
Speaker 2:  Just get him to say each phone name and then we can put it together and he'll

1280
01:16:43,765 --> 01:16:44,165
Speaker 2:  say it. Yeah.

1281
01:16:44,485 --> 01:16:47,525
Speaker 5:  Okay. I've got a lightning round. I forgot to mention the Lightning round

1282
01:16:47,525 --> 01:16:51,045
Speaker 5:  is sponsored this week. Oh yeah. But not for money because

1283
01:16:51,305 --> 01:16:54,725
Speaker 5:  no one is that dumb. If. you would like to pay us money. This is the only

1284
01:16:54,725 --> 01:16:58,405
Speaker 5:  time I will say the name of a sponsor round. I will sell my integrity If.

1285
01:16:58,405 --> 01:17:02,285
Speaker 5:  you sponsor the lightning round for money. But we are not,

1286
01:17:02,305 --> 01:17:06,085
Speaker 5:  we didn't take money this week. We took a, an excellent, hilarious tip from

1287
01:17:06,085 --> 01:17:09,885
Speaker 5:  our friend Christopher who is the, this week's sponsored Lightning who

1288
01:17:09,955 --> 01:17:10,245
Speaker 5:  said,

1289
01:17:12,225 --> 01:17:15,405
Speaker 5:  how did you miss this Smeg News? Which is an incredible sentence.

1290
01:17:16,225 --> 01:17:18,245
Speaker 2:  My smeg Google alerts really letting me down

1291
01:17:18,785 --> 01:17:22,725
Speaker 5:  If. you recall Smeg is a maker of quirky refrigerators.

1292
01:17:23,745 --> 01:17:25,565
Speaker 5:  you know the retro refrigerator company

1293
01:17:25,745 --> 01:17:26,485
Speaker 2:  As you do? Yeah.

1294
01:17:27,205 --> 01:17:30,725
Speaker 5:  They have a collaboration with fiat. They've made a Fiat

1295
01:17:30,725 --> 01:17:32,765
Speaker 5:  500 with a refrigerator.

1296
01:17:34,875 --> 01:17:38,085
Speaker 5:  What That's true. It's, it's the, the seg 500

1297
01:17:38,905 --> 01:17:41,245
Speaker 5:  on the Smeg five on the Smeg website.

1298
01:17:42,505 --> 01:17:46,125
Speaker 5:  Here's the first line. What is the fridge doing underneath the

1299
01:17:46,245 --> 01:17:47,525
Speaker 5:  bonnet of a Fiat 500

1300
01:17:49,725 --> 01:17:53,565
Speaker 5:  Smeg and Fiat merge form and function to create an extraordinary item for

1301
01:17:53,565 --> 01:17:57,045
Speaker 5:  the Fiat 500 design collection. Because the refrigerator is not just an electrical

1302
01:17:57,045 --> 01:17:59,405
Speaker 5:  appliance and a bonnet is not just a car part.

1303
01:18:00,515 --> 01:18:01,485
Speaker 2:  Hell yeah. This seems like

1304
01:18:01,485 --> 01:18:05,125
Speaker 5:  An April Fool's joke. Then there's a picture of a Fiat 500

1305
01:18:05,465 --> 01:18:09,285
Speaker 5:  and it says, an iconic place for food with an arrow pointing to the trunk

1306
01:18:09,285 --> 01:18:11,525
Speaker 5:  of the car. It's all true.

1307
01:18:12,565 --> 01:18:15,965
Speaker 2:  I would like to make t-shirts that have just on the stomach the phrase an

1308
01:18:15,965 --> 01:18:16,965
Speaker 2:  iconic place for food.

1309
01:18:18,255 --> 01:18:21,125
Speaker 5:  These two made in Italy brands they've already met in 1950s. It's like a

1310
01:18:21,125 --> 01:18:24,045
Speaker 5:  meat. Cute. Aw. And they like, they've been trying to get a fridge that

1311
01:18:24,465 --> 01:18:27,885
Speaker 2:  The snake, the snake 500 sounds like, like the last NASCAR race.

1312
01:18:28,105 --> 01:18:29,205
Speaker 2:  You've, that ever happens.

1313
01:18:29,515 --> 01:18:29,805
Speaker 5:  Yeah.

1314
01:18:30,585 --> 01:18:31,765
Speaker 2:  You win a fridge at the end

1315
01:18:32,025 --> 01:18:35,565
Speaker 5:  For me. Vittorio, this keeps going by the way, actually,

1316
01:18:36,115 --> 01:18:40,085
Speaker 5:  there's a lot here. Thank, thank you Christopher again for sponsoring Walking

1317
01:18:40,085 --> 01:18:43,925
Speaker 5:  Around Indeed with, with this link, CEO of Smeg Vittorio.

1318
01:18:44,455 --> 01:18:48,325
Speaker 5:  Brett Deani. He said, for me, creating the SEG 500 was above

1319
01:18:48,345 --> 01:18:49,845
Speaker 5:  all a matter of personal honor.

1320
01:18:51,955 --> 01:18:52,565
Speaker 2:  Seriously,

1321
01:18:53,785 --> 01:18:55,245
Speaker 5:  Man. This is,

1322
01:18:55,315 --> 01:18:56,805
Speaker 2:  This is about my family's legacy.

1323
01:19:00,755 --> 01:19:04,685
Speaker 5:  Wait, I you're not that you said that as a joke. That is the

1324
01:19:04,685 --> 01:19:08,205
Speaker 5:  rest of the quote. Are you serious? I strongly holy,

1325
01:19:09,285 --> 01:19:12,445
Speaker 5:  I strongly believe in this project that pays tribute to the memory of my

1326
01:19:12,445 --> 01:19:16,245
Speaker 5:  grandfather. When he founded Seg after the Second World War,

1327
01:19:16,745 --> 01:19:20,285
Speaker 5:  one of his first collaborations was creating refrigerators with fiat.

1328
01:19:20,285 --> 01:19:20,845
Speaker 2:  Incredible.

1329
01:19:21,755 --> 01:19:25,125
Speaker 5:  It's, it's just the whole whatever link to it. Thank you Christopher, for

1330
01:19:25,125 --> 01:19:28,805
Speaker 5:  like sponsoring the lightning round with the smeg 500. No thanks to Smeg

1331
01:19:28,805 --> 01:19:31,765
Speaker 5:  who paid us no money for this. Yeah. Alright. David, what's yours?

1332
01:19:32,085 --> 01:19:36,045
Speaker 2:  I don't know anymore. Mine is. So M wc, the Mobile

1333
01:19:36,045 --> 01:19:39,485
Speaker 2:  World Congress in Barcelona. Barcelona. Barcelona

1334
01:19:40,265 --> 01:19:44,205
Speaker 2:  was this week surprisingly newsy. MWC, John Porter and

1335
01:19:44,205 --> 01:19:46,525
Speaker 2:  Allison Johnson are on our team. We're gonna have them on Tuesdays show to

1336
01:19:46,525 --> 01:19:50,405
Speaker 2:  give us the whole rundown. But the, the most personally exciting thing that

1337
01:19:50,565 --> 01:19:54,325
Speaker 2:  happened was we actually got like a first person demo of the Humane

1338
01:19:54,425 --> 01:19:58,125
Speaker 2:  ai Pin, the mysterious gadget that I'm only,

1339
01:19:58,705 --> 01:20:00,925
Speaker 2:  I'm like 90%. It's real. Like

1340
01:20:01,065 --> 01:20:04,045
Speaker 5:  I'm not, it's real now. Sam Scheffer is like making videos on the streets

1341
01:20:04,045 --> 01:20:07,165
Speaker 5:  of New York with it. Sam, by the way, I keep show it to, to me.

1342
01:20:07,695 --> 01:20:09,245
Speaker 5:  Bring it, bring it over. Yeah.

1343
01:20:09,315 --> 01:20:13,285
Speaker 2:  Come on the show Sam. We'd love to have you back. The people need it. But

1344
01:20:13,285 --> 01:20:17,165
Speaker 2:  Alison got a demo. She got to see some at work and had a really interesting

1345
01:20:17,165 --> 01:20:21,005
Speaker 2:  experience with it. Like the thing about this Pin, I'm desperate to

1346
01:20:21,165 --> 01:20:25,085
Speaker 2:  actually test this thing out because it seems like they did a lot of cool

1347
01:20:25,205 --> 01:20:28,125
Speaker 2:  technology to it. Yeah. Like the projector thing seems to work.

1348
01:20:29,065 --> 01:20:32,965
Speaker 2:  She had it take a picture and it identified her as a person with a Verge

1349
01:20:32,965 --> 01:20:35,925
Speaker 2:  lanyard and all the other stuff in the background. Like seriously impressive

1350
01:20:35,925 --> 01:20:39,605
Speaker 2:  technology going in here. But Allison also sort of landed on the question

1351
01:20:39,605 --> 01:20:41,885
Speaker 2:  that I think a lot of us have been asking about this device. It's like, what

1352
01:20:41,905 --> 01:20:45,805
Speaker 2:  on earth is any of this for? Yeah. Like you made a neat thing that

1353
01:20:45,805 --> 01:20:48,885
Speaker 2:  does some of the stuff that you said it was gonna do. It also was super slow

1354
01:20:48,885 --> 01:20:52,365
Speaker 2:  and I think overheated on the show floor. Very good. That's like conference

1355
01:20:52,365 --> 01:20:56,245
Speaker 2:  demo stuff, whatever. I'll leave that alone. But she kind of got to the end

1356
01:20:56,245 --> 01:20:58,645
Speaker 2:  and was like, why, why is this?

1357
01:21:00,515 --> 01:21:04,485
Speaker 5:  Yeah. It does seem so you can watch a video of it, of that

1358
01:21:04,485 --> 01:21:07,245
Speaker 5:  demo while the places we post videos.

1359
01:21:07,435 --> 01:21:09,205
Speaker 2:  Yeah. I think I saw it on our TikTok. Yeah. It's

1360
01:21:09,205 --> 01:21:12,885
Speaker 5:  A good one. Yeah. Find a Scrolly video place and look for the fridge and

1361
01:21:12,885 --> 01:21:16,525
Speaker 5:  it's nine. She asked a question. 19 seconds go by before the answer. That's

1362
01:21:16,525 --> 01:21:18,205
Speaker 5:  tough. Oh, that's brutal. That's tough. That's that's not good

1363
01:21:19,225 --> 01:21:21,485
Speaker 2:  In which she like made small talk with the person

1364
01:21:22,465 --> 01:21:26,125
Speaker 5:  And, and then I started talking and interrupted. Yeah. And I was like, oh,

1365
01:21:26,125 --> 01:21:29,845
Speaker 5:  it's doing, it's like I don't know about this. Yeah. She did say,

1366
01:21:29,985 --> 01:21:33,965
Speaker 5:  and I think this is a lesson to everyone. She's like, this gadget would be

1367
01:21:33,965 --> 01:21:37,445
Speaker 5:  so much cooler if this company wasn't so overbearingly serious about itself.

1368
01:21:37,705 --> 01:21:41,205
Speaker 5:  Mm. Right. Like if you're like, this will change the nature of culture

1369
01:21:41,705 --> 01:21:44,965
Speaker 5:  and it's like you made a laser that says like your name. I,

1370
01:21:45,435 --> 01:21:48,405
Speaker 2:  Well what's funny is like, I think there's been this interesting thing now

1371
01:21:48,405 --> 01:21:51,525
Speaker 2:  where we have the, the rabbit R one and we have the Humane ai Pin and they're

1372
01:21:51,525 --> 01:21:55,485
Speaker 2:  very much like racing each other to be kind of the nifty AI gadget that everybody's

1373
01:21:55,485 --> 01:21:59,405
Speaker 2:  excited about rabbit's. CEO is just like nakedly out

1374
01:21:59,405 --> 01:22:03,165
Speaker 2:  there trying to beat Humane to market, which I think is so funny. Yeah. But

1375
01:22:03,955 --> 01:22:07,245
Speaker 2:  they've just played this so differently Right. Where, where Jesse, the CEO

1376
01:22:07,245 --> 01:22:09,925
Speaker 2:  of Rabbit has been out there basically being like, this is like a silly fun

1377
01:22:09,925 --> 01:22:12,845
Speaker 2:  thing that we made. We think it's cool. Let's see what happens. Maybe this

1378
01:22:12,845 --> 01:22:15,325
Speaker 2:  will be The future. And you're right. And Humane is actually being like,

1379
01:22:15,325 --> 01:22:19,125
Speaker 2:  we have changed the paradigm of your life. Yeah. And they're,

1380
01:22:19,195 --> 01:22:22,365
Speaker 2:  they just like seem sad in a room together. And it's like, what, what are,

1381
01:22:22,365 --> 01:22:23,525
Speaker 2:  what is happening are

1382
01:22:23,525 --> 01:22:26,885
Speaker 5:  Doing. I feel like they, everyone is doing a Steve Jobs impression, but they

1383
01:22:27,335 --> 01:22:31,285
Speaker 5:  never watched any Steve Jobs videos. Yes. Do you know what I mean? Like it's

1384
01:22:31,285 --> 01:22:35,085
Speaker 5:  like they're doing an impression of who was, it was Fast be Steve Jobs,

1385
01:22:35,255 --> 01:22:35,685
Speaker 5:  who's like

1386
01:22:35,685 --> 01:22:35,845
Speaker 2:  Very,

1387
01:22:36,555 --> 01:22:40,285
Speaker 5:  He's like very serious all the time and like very intense and like the real

1388
01:22:40,285 --> 01:22:42,165
Speaker 5:  Steve Jobs is like kind of a goofball.

1389
01:22:42,475 --> 01:22:45,285
Speaker 2:  Yeah. If. you go back and watch. I went back for some reason not that long

1390
01:22:45,285 --> 01:22:48,605
Speaker 2:  ago and watched his first iPad reveal. Yeah. It's the most

1391
01:22:49,065 --> 01:22:52,965
Speaker 2:  casual, relaxed like man sitting on a chair using a web browser

1392
01:22:52,965 --> 01:22:55,765
Speaker 2:  Yeah. Is the whole vibe of the thing. And it was, yeah. He would make jokes.

1393
01:22:55,825 --> 01:22:59,005
Speaker 2:  He was funny. Yeah. He was silly. Like that was their whole vibe. Yeah. And

1394
01:22:59,005 --> 01:23:02,965
Speaker 2:  now everybody remembers him as this like dictator presenter. Yeah. Who

1395
01:23:02,965 --> 01:23:06,020
Speaker 5:  Was like, yeah. It's like he would get on stage like, so these are our competitors

1396
01:23:06,020 --> 01:23:10,005
Speaker 5:  products. They fucking suck. Huh? Look at this piece of shit. You're like,

1397
01:23:10,005 --> 01:23:13,045
Speaker 5:  here's what I made. It's pretty awesome. Yeah. Right. And like every, like

1398
01:23:13,045 --> 01:23:16,525
Speaker 5:  the Humane, I dunno, there's something that's like everyone's doing an impression

1399
01:23:16,525 --> 01:23:17,285
Speaker 5:  of the wrong thing.

1400
01:23:17,955 --> 01:23:21,725
Speaker 2:  Even Apple kind of is increasingly getting serious in, in a way that

1401
01:23:21,735 --> 01:23:24,125
Speaker 2:  feels kind of ajoy. Yeah. Yeah.

1402
01:23:24,585 --> 01:23:27,125
Speaker 5:  And I think it's, they're doing an impression of those like the Johnny Ive

1403
01:23:27,125 --> 01:23:30,445
Speaker 5:  design videos 'cause Johnny, I didn't like to be on stage. Yeah. So they

1404
01:23:30,445 --> 01:23:34,325
Speaker 5:  would make these videos and be like, what is plastic I've invented? It's

1405
01:23:34,325 --> 01:23:38,285
Speaker 5:  very beautiful. you know, it's like, what are you doing? And it's

1406
01:23:38,285 --> 01:23:41,005
Speaker 5:  like, that didn't work. What worked was the other guy who was like, you know

1407
01:23:41,005 --> 01:23:43,965
Speaker 5:  what? I love his records now I put a thousand of 'em in your pocket. Huh?

1408
01:23:44,665 --> 01:23:44,885
Speaker 5:  He

1409
01:23:44,885 --> 01:23:48,125
Speaker 2:  Pulled the MacBook air out of a sleeve. It was sick. But anyway, so the Humane

1410
01:23:48,305 --> 01:23:51,445
Speaker 2:  Pin is supposed to ship, I think at the end of March is the last thing that

1411
01:23:51,445 --> 01:23:55,285
Speaker 2:  we've heard that has slipped a little, but it iss close enough that I think

1412
01:23:55,285 --> 01:23:58,245
Speaker 2:  we're gonna get these things for real soon. I bought one. I'm very excited

1413
01:23:58,245 --> 01:23:58,925
Speaker 2:  about it. I can't believe If,

1414
01:23:58,925 --> 01:23:59,445
Speaker 5:  You bought one. I

1415
01:23:59,445 --> 01:24:03,205
Speaker 2:  Have so many questions about why I spent

1416
01:24:03,205 --> 01:24:05,645
Speaker 2:  $700 on a Humane ai

1417
01:24:05,705 --> 01:24:05,925
Speaker 5:  Pin.

1418
01:24:07,465 --> 01:24:11,285
Speaker 2:  But I am fascinated by this thing. And every, every little step I get

1419
01:24:11,285 --> 01:24:15,125
Speaker 2:  towards being able to make a phone call with a projector when you

1420
01:24:15,235 --> 01:24:18,045
Speaker 5:  Come all it, when you're a kid outta the house at 18 and be like, I don't

1421
01:24:18,045 --> 01:24:20,845
Speaker 5:  have money for college. I hope you hand them the human AI Pin,

1422
01:24:21,635 --> 01:24:23,245
Speaker 2:  I'll frame it be like, this is

1423
01:24:23,245 --> 01:24:26,245
Speaker 5:  Your gift. This is for you in lieu of a college education.

1424
01:24:27,395 --> 01:24:28,165
Speaker 5:  This thing, it takes

1425
01:24:28,325 --> 01:24:30,805
Speaker 2:  19, sell this my son and go on your way.

1426
01:24:31,315 --> 01:24:35,245
Speaker 5:  This thing will describe with medium accuracy what you are currently

1427
01:24:35,245 --> 01:24:37,565
Speaker 5:  looking at. 19 seconds later.

1428
01:24:39,515 --> 01:24:43,445
Speaker 5:  It's like, I am the person wearing The Verge la that's me.

1429
01:24:43,515 --> 01:24:43,805
Speaker 5:  Yeah,

1430
01:24:43,805 --> 01:24:45,285
Speaker 2:  Exactly. All right. NELI, what's yours?

1431
01:24:48,285 --> 01:24:51,685
Speaker 5:  I think I, I'm gonna pick two. I'm gonna pick two. I'm sorry. I'm gonna pick

1432
01:24:51,685 --> 01:24:55,445
Speaker 5:  two. I'm shocked. I one, I'm, I'm gonna start with the fun one. Okay.

1433
01:24:56,355 --> 01:25:00,325
Speaker 5:  Okay. As you all know, Lauren doesn't know, as

1434
01:25:00,325 --> 01:25:03,405
Speaker 5:  David knows Lauren, you're gonna find out I have one dream as editor in chief

1435
01:25:03,405 --> 01:25:07,325
Speaker 5:  of The Verge, which is for David to file a story to me and for

1436
01:25:07,325 --> 01:25:11,165
Speaker 5:  me to mark it up with a red pen. That's all I want. Very old school.

1437
01:25:11,275 --> 01:25:15,005
Speaker 5:  Very old school. I also would love a glass walled

1438
01:25:15,005 --> 01:25:18,245
Speaker 5:  conference room where we print out the layout of the magazine and I just

1439
01:25:18,245 --> 01:25:22,085
Speaker 5:  move the pages around best. We don't the best have a print magazine. So that

1440
01:25:22,085 --> 01:25:25,405
Speaker 5:  dream has long since been, I've just had to let it go. you know, so you,

1441
01:25:25,825 --> 01:25:28,165
Speaker 5:  I'm never gonna play professional basketball. I'm never gonna move pages

1442
01:25:28,165 --> 01:25:31,205
Speaker 5:  of print magazine around a glass wall conference room. The New York Mag people

1443
01:25:31,205 --> 01:25:35,165
Speaker 5:  have a conference room in this office, like stare at it all the time. They're

1444
01:25:35,165 --> 01:25:35,405
Speaker 5:  just moving

1445
01:25:35,405 --> 01:25:38,045
Speaker 2:  Pages. Do you ever go just move pages around just to see if they notice

1446
01:25:38,355 --> 01:25:42,005
Speaker 5:  Blow up the magazine? Yeah. The red pen thing. No.

1447
01:25:42,145 --> 01:25:45,645
Speaker 5:  One thing I could do is I could have David print out his Google Docs and

1448
01:25:45,645 --> 01:25:47,725
Speaker 5:  Circle pen. Sadly, we don't live in the same place.

1449
01:25:49,425 --> 01:25:53,165
Speaker 2:  That's not the reason. Just to be clear, that's not the reason. I don't know

1450
01:25:53,165 --> 01:25:53,445
Speaker 2:  about that.

1451
01:25:54,985 --> 01:25:58,925
Speaker 5:  So every time a new Stylist Gadget comes out, I'm like, is this,

1452
01:25:59,225 --> 01:26:02,325
Speaker 5:  is this it? Is this the thing that will enable me to like circle stuff? Mm.

1453
01:26:02,425 --> 01:26:05,805
Speaker 5:  That's all I want. And in particular, circle David's copy in a red pen.

1454
01:26:07,265 --> 01:26:11,205
Speaker 5:  The answer for a decade now has been no. We've tried almost every style

1455
01:26:11,205 --> 01:26:14,525
Speaker 5:  of gat in this way and they don't work. Google just released an update to

1456
01:26:14,525 --> 01:26:18,365
Speaker 5:  Android at M WC Ooh, where Google Docs

1457
01:26:18,465 --> 01:26:22,365
Speaker 5:  on an Android tablet. It's called Project Inkwell. You can now mark

1458
01:26:22,365 --> 01:26:26,205
Speaker 5:  up with a pen. That's very exciting. The problem is that

1459
01:26:26,205 --> 01:26:29,845
Speaker 5:  this requires you to have an Android tablet. It also

1460
01:26:29,845 --> 01:26:32,085
Speaker 5:  requires David to file a copy to me.

1461
01:26:33,475 --> 01:26:34,365
Speaker 2:  Yeah. That's,

1462
01:26:34,365 --> 01:26:37,765
Speaker 5:  You know, challenging problem. We're getting there. I'm just, I'm just thrilled

1463
01:26:37,765 --> 01:26:38,565
Speaker 5:  about this. Oh

1464
01:26:38,565 --> 01:26:38,685
Speaker 2:  Yeah.

1465
01:26:38,815 --> 01:26:42,365
Speaker 5:  We're getting an Android tablet. We're gonna demo this on the show. It's

1466
01:26:42,365 --> 01:26:42,725
Speaker 5:  going to happen.

1467
01:26:43,385 --> 01:26:44,045
Speaker 2:  I'm excited about it.

1468
01:26:44,205 --> 01:26:48,045
Speaker 5:  I can't look, I can't see Liam right now. I imagine he's vigorously agreeing

1469
01:26:48,045 --> 01:26:49,245
Speaker 5:  with me that we're gonna do a demo

1470
01:26:49,245 --> 01:26:50,445
Speaker 2:  With me. That's, that's, that is what's happening.

1471
01:26:50,445 --> 01:26:54,245
Speaker 5:  He's circling things live on The. Vergecast. That's all I want. This is my

1472
01:26:54,245 --> 01:26:54,485
Speaker 5:  dream.

1473
01:26:55,575 --> 01:26:59,485
Speaker 2:  We're gonna do it. I would say based on the history of

1474
01:26:59,485 --> 01:27:03,005
Speaker 2:  Google Docs being good at things, I would not get your hopes super high.

1475
01:27:03,825 --> 01:27:05,845
Speaker 2:  But I believe in this dream for you and I want you to add it.

1476
01:27:06,305 --> 01:27:10,045
Speaker 5:  If, you could just add up the number of things that have to be good for this

1477
01:27:10,045 --> 01:27:12,725
Speaker 5:  to go. Well, Android tablets, conceptually

1478
01:27:13,425 --> 01:27:14,365
Speaker 2:  The Google Docs app

1479
01:27:15,625 --> 01:27:18,845
Speaker 5:  On the Android tablet. Android tablet stylus.

1480
01:27:19,565 --> 01:27:21,125
Speaker 5:  Actually some of the Samsung stylists are pretty good. Yeah.

1481
01:27:21,125 --> 01:27:21,405
Speaker 2:  They're getting,

1482
01:27:21,405 --> 01:27:25,365
Speaker 5:  They're getting there. But does the Spen support Google Docs markup

1483
01:27:25,385 --> 01:27:25,605
Speaker 5:  on

1484
01:27:25,605 --> 01:27:29,525
Speaker 2:  Android? It's okay. You just import it into Samsung's proprietary document

1485
01:27:29,525 --> 01:27:30,765
Speaker 2:  viewer and then export it

1486
01:27:30,765 --> 01:27:33,685
Speaker 5:  Out. It's such a small dream. Just give it to me. Just figure it out. Take

1487
01:27:33,685 --> 01:27:37,325
Speaker 5:  all that energy. Shut down Gemini. Put all those engineers on a circle problem.

1488
01:27:37,825 --> 01:27:39,565
Speaker 5:  Be like, we'll do the Hamilton conundrum another

1489
01:27:39,565 --> 01:27:42,445
Speaker 2:  Day. Said maybe now that the Apple car's gone. Everybody's working on generative

1490
01:27:42,465 --> 01:27:44,845
Speaker 2:  AI highlights. This could be you. What's your other one?

1491
01:27:46,315 --> 01:27:48,725
Speaker 5:  Like it's TikTok. We gotta talk about TikTok News. We,

1492
01:27:48,725 --> 01:27:49,085
Speaker 2:  We should talk

1493
01:27:49,085 --> 01:27:52,405
Speaker 5:  About TikTok. So they're in Big Fight. Believe tomorrow is, you're listening

1494
01:27:52,405 --> 01:27:55,285
Speaker 5:  to this is the Drop Dead deadline. Mm. So

1495
01:27:56,035 --> 01:28:00,005
Speaker 5:  Universal Music Group, which has Taylor Swift usher, everybody you can think

1496
01:28:00,005 --> 01:28:03,845
Speaker 5:  of Justin Bieber. I believe I looked at David very knowingly. I was like

1497
01:28:03,845 --> 01:28:04,285
Speaker 5:  your favorite.

1498
01:28:04,685 --> 01:28:07,365
Speaker 2:  I can, I can confirm you started Harry Styles. Many others.

1499
01:28:08,005 --> 01:28:11,405
Speaker 5:  David, the lead of one of David's stories this week was Justin Bieber. So,

1500
01:28:11,625 --> 01:28:11,845
Speaker 5:  oh,

1501
01:28:11,985 --> 01:28:12,685
Speaker 2:  It was good stuff.

1502
01:28:13,795 --> 01:28:16,845
Speaker 5:  With a photo of Justin Biebers. I wanna point out what I'm here for. We've

1503
01:28:16,845 --> 01:28:17,845
Speaker 5:  put a lot of the beeb

1504
01:28:18,045 --> 01:28:18,805
Speaker 2:  I spent on The Verge

1505
01:28:18,865 --> 01:28:19,605
Speaker 5:  Com this week. Spent

1506
01:28:19,605 --> 01:28:23,285
Speaker 2:  90 full minutes on Getty Images, looking at pictures of Justin Bieber in

1507
01:28:23,285 --> 01:28:24,525
Speaker 2:  public. It's the best thing I did

1508
01:28:24,525 --> 01:28:26,925
Speaker 5:  All week. It's a good story. Anyway, universal Own had all the biggest artists.

1509
01:28:26,925 --> 01:28:30,365
Speaker 5:  So you can think of, you know, a fight with TikTok over music

1510
01:28:30,395 --> 01:28:33,405
Speaker 5:  licensing. They say TikTok is a bully. They don't wanna pay the higher rates.

1511
01:28:33,535 --> 01:28:37,245
Speaker 5:  There was a first date that date came, the music went away.

1512
01:28:37,745 --> 01:28:41,725
Speaker 5:  The poor woman on TikTok who has all the jobs, her, her sound is gone.

1513
01:28:41,825 --> 01:28:45,765
Speaker 5:  Oh no. And so she's just making sad, like increasingly sad tiktoks about

1514
01:28:45,765 --> 01:28:46,605
Speaker 5:  her sound being gone.

1515
01:28:46,885 --> 01:28:47,405
Speaker 2:  Oh, that's awful.

1516
01:28:48,235 --> 01:28:51,725
Speaker 5:  There's like a lot of this happening on the platform. Yeah. And tomorrow,

1517
01:28:52,045 --> 01:28:56,005
Speaker 5:  I believe That the, the time that you are currently

1518
01:28:56,005 --> 01:28:59,085
Speaker 5:  listening to this Friday, that's what it's called. There you go.

1519
01:29:00,225 --> 01:29:03,325
Speaker 5:  Is the drop dead date when the rest of it goes. And TikTok has already started

1520
01:29:03,325 --> 01:29:07,085
Speaker 5:  putting, pulling not just Universal's like own

1521
01:29:07,085 --> 01:29:10,285
Speaker 5:  music, but anybody who has a publishing deal with Universal, some of the

1522
01:29:10,285 --> 01:29:12,965
Speaker 5:  songwriters that are associated with Universal. Right. So even more music

1523
01:29:12,985 --> 01:29:16,925
Speaker 5:  is coming off the platform, which I think TikTok thinks it has the leverage,

1524
01:29:17,735 --> 01:29:20,485
Speaker 2:  Which is wild. I mean, one of the stats that I read, and I think a lot of

1525
01:29:20,485 --> 01:29:22,965
Speaker 2:  this is really fuzzy, so Who knows, but one of the stats that I read is,

1526
01:29:23,045 --> 01:29:26,085
Speaker 2:  I think it was something like 30% of the songs,

1527
01:29:26,865 --> 01:29:30,805
Speaker 2:  but 80% of the videos with music Yeah. Would suddenly go silent. Like

1528
01:29:30,805 --> 01:29:32,405
Speaker 2:  that's a huge number. That's just

1529
01:29:32,865 --> 01:29:36,285
Speaker 5:  Of TikTok. Yeah. If. you take Taylor Swift off of TikTok, you've, that's

1530
01:29:36,285 --> 01:29:40,085
Speaker 5:  not great. Yeah. And I think what TikTok has learned is like, we

1531
01:29:40,085 --> 01:29:43,965
Speaker 5:  seem to be fine. And I, I don't know if that's true. There are no

1532
01:29:43,965 --> 01:29:47,845
Speaker 5:  numbers. I I I, we were trying to do some more reporting on it, but

1533
01:29:47,845 --> 01:29:49,805
Speaker 2:  It, one thing I've seen that's been really interesting the last few days

1534
01:29:49,865 --> 01:29:53,845
Speaker 2:  has been a bunch of up and coming artists who now can't

1535
01:29:53,845 --> 01:29:56,405
Speaker 2:  promote their own songs on TikTok. Yep.

1536
01:29:57,435 --> 01:30:01,085
Speaker 2:  Basically like a having to figure out a new thing to do and b kind of

1537
01:30:01,285 --> 01:30:05,125
Speaker 2:  pleading with their audience, like, keep watching

1538
01:30:05,125 --> 01:30:08,805
Speaker 2:  my stuff, follow me. Like it so that it stays on your for you page. Because

1539
01:30:09,225 --> 01:30:13,165
Speaker 2:  if I can't put my song on it and you're not following me, the algorithm is

1540
01:30:13,165 --> 01:30:15,805
Speaker 2:  not gonna serve stuff. And this is the thing artists have said for forever

1541
01:30:15,905 --> 01:30:18,805
Speaker 2:  is that like, when they just make a thing where they talk, it doesn't do

1542
01:30:18,805 --> 01:30:22,325
Speaker 2:  as well. So they do covers of, well-known songs so that they get put into

1543
01:30:22,325 --> 01:30:24,605
Speaker 2:  that part of the algorithm. Or they do their own songs that people like,

1544
01:30:24,605 --> 01:30:27,565
Speaker 2:  so they get served to their fans. But if it's just them talking to camera,

1545
01:30:27,955 --> 01:30:30,685
Speaker 2:  that doesn't do as well. Like, just historically, that is a thing I, I've

1546
01:30:30,685 --> 01:30:34,605
Speaker 2:  seen a bunch of artists say on TikTok and now it's like, if you're an up

1547
01:30:34,605 --> 01:30:38,565
Speaker 2:  and coming artist, the place where people find you is TikTok and

1548
01:30:38,565 --> 01:30:41,725
Speaker 2:  So I think it, it's, it's just creating this Interesting, and

1549
01:30:41,725 --> 01:30:44,165
Speaker 5:  That's tiktoks leverage, right? That's what they're basically saying. Right.

1550
01:30:45,155 --> 01:30:48,925
Speaker 2:  Universal. But then Universal's leverage is Taylor Swift. Yeah. And, and

1551
01:30:49,275 --> 01:30:53,165
Speaker 2:  most of the rest of the music people want to listen to and do stuff. But

1552
01:30:53,165 --> 01:30:56,645
Speaker 2:  it's like that that spectrum is going to affect the people in so many messy

1553
01:30:56,755 --> 01:30:57,685
Speaker 2:  ways. Yeah, yeah.

1554
01:30:57,715 --> 01:31:01,605
Speaker 12:  Also though, like not having music on so many tiktoks makes

1555
01:31:01,605 --> 01:31:03,325
Speaker 12:  TikTok like kind of unusable.

1556
01:31:03,645 --> 01:31:06,645
Speaker 2:  Yeah. That's totally been my experience now. Like it's, it's like one out

1557
01:31:06,645 --> 01:31:09,885
Speaker 2:  of every two videos is just awkwardly quiet. And it turned out, watching

1558
01:31:09,885 --> 01:31:13,285
Speaker 2:  people do TikTok dances with no music is

1559
01:31:13,285 --> 01:31:14,845
Speaker 2:  uncomfortable. Like it's not a

1560
01:31:14,845 --> 01:31:17,765
Speaker 12:  Good sign. And it takes you a couple seconds to register that. Like there's

1561
01:31:17,765 --> 01:31:19,485
Speaker 12:  no music. It's not that like your volume is

1562
01:31:19,485 --> 01:31:19,605
Speaker 2:  Too

1563
01:31:19,605 --> 01:31:20,165
Speaker 5:  Low. Yeah, yeah.

1564
01:31:20,555 --> 01:31:22,365
Speaker 2:  Yeah. It's weird. I totally agree.

1565
01:31:22,585 --> 01:31:26,245
Speaker 5:  I'm surprised that, you know, YouTube has a much healthier relationship with

1566
01:31:26,245 --> 01:31:29,605
Speaker 5:  the universal, like, universal is like, we're very mad about fake Drake and

1567
01:31:29,625 --> 01:31:33,325
Speaker 5:  ai and YouTube was like, we made some stuff for you. Like we have a council

1568
01:31:33,325 --> 01:31:37,205
Speaker 5:  about ai. Like, I'm surprised that YouTube is not doing more to promote

1569
01:31:37,465 --> 01:31:41,325
Speaker 5:  shorts in this moment and to say like, all these creators come over here

1570
01:31:41,325 --> 01:31:44,685
Speaker 5:  because that, that is right now their competitive advantage is they have

1571
01:31:44,685 --> 01:31:45,845
Speaker 5:  access to all this music. Yeah. They've

1572
01:31:45,845 --> 01:31:49,565
Speaker 2:  Been kind of quietly saying things like, you know, we

1573
01:31:49,805 --> 01:31:52,205
Speaker 2:  continue to be a good partner to the music industry and stuff like that.

1574
01:31:52,265 --> 01:31:55,965
Speaker 2:  but it is weird that like Lyor Cohen who runs YouTube music is not out there

1575
01:31:55,965 --> 01:31:59,285
Speaker 2:  being like, you want music? We have all the music. Like, this is, you're

1576
01:31:59,285 --> 01:32:01,405
Speaker 2:  kind of right. This is sort of YouTube's best shot.

1577
01:32:01,405 --> 01:32:03,765
Speaker 5:  It's Instagram has the correct deals with the industry. Pull people. Yeah.

1578
01:32:03,765 --> 01:32:07,165
Speaker 5:  They're not, they're not pushing it. It's, I I think Universal is basically

1579
01:32:07,165 --> 01:32:09,685
Speaker 5:  like, all right, like we just won't be on TikTok. That'll be the end of that.

1580
01:32:09,685 --> 01:32:12,125
Speaker 5:  And like, we have to figure out a new way to promote artists. But guess what

1581
01:32:12,125 --> 01:32:13,045
Speaker 5:  we do? We're a record label.

1582
01:32:13,555 --> 01:32:16,165
Speaker 2:  Yeah. That's the, there have been ways before that's

1583
01:32:16,725 --> 01:32:20,645
Speaker 5:  Historically how we provide value to the artist. But we'll see. I tomorrow

1584
01:32:20,645 --> 01:32:23,605
Speaker 5:  I think will be an interesting day. 'cause I, I think it either gets solved

1585
01:32:23,665 --> 01:32:27,165
Speaker 5:  or it, if they pull out even more, I think it actually drags on for quite

1586
01:32:27,165 --> 01:32:27,565
Speaker 5:  a bit longer.

1587
01:32:27,855 --> 01:32:29,165
Speaker 2:  Think you're probably right. Alright.

1588
01:32:29,165 --> 01:32:30,565
Speaker 5:  Do you have another one or should you just read?

1589
01:32:30,665 --> 01:32:31,565
Speaker 2:  We should just get outta here. We

1590
01:32:31,565 --> 01:32:34,325
Speaker 5:  Should get outta here. You don't wanna talk about Wendy's dynamic search

1591
01:32:34,325 --> 01:32:37,565
Speaker 5:  pricing? No. They said they weren't gonna it. All right. We've gone over

1592
01:32:37,665 --> 01:32:40,085
Speaker 5:  as always, Lauren, thank you so much for joining over our test. Yeah, thanks

1593
01:32:40,085 --> 01:32:43,605
Speaker 5:  for having me. Really fun. We'll have to have you back soon. Would love to.

1594
01:32:43,925 --> 01:32:46,045
Speaker 5:  Whenever Larry Toms issues his decision.

1595
01:32:46,115 --> 01:32:49,325
Speaker 2:  It's gonna be a mega chill policy here, so we Oh yeah. We probably won't

1596
01:32:49,325 --> 01:32:50,485
Speaker 2:  have much to talk about it. I don't think

1597
01:32:50,765 --> 01:32:53,525
Speaker 5:  It'll be great. All right. That's it, Alex. We'll be back next week. That's

1598
01:32:53,525 --> 01:32:54,525
Speaker 5:  for Chest Rock Roll.

1599
01:32:58,785 --> 01:33:02,005
Speaker 1:  And that's it for The Vergecast this week. Hey, we'd love to hear from you.

1600
01:33:02,035 --> 01:33:05,725
Speaker 1:  Give us a call at eight six six VERGE one. One The Vergecast is a

1601
01:33:05,725 --> 01:33:09,285
Speaker 1:  production of The Verge and Vox Media Podcast Network. Our show is produced

1602
01:33:09,285 --> 01:33:12,965
Speaker 1:  by Andrew Marino and Liam James. That's it. We'll see you next week.

